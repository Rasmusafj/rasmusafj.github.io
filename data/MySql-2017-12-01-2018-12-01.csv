author,comment,timestamp
r3pr0b8,"tested this using your data --

    SELECT id, SUBSTRING_INDEX(TRIM(TRAILING ';' FROM val),'; ',1) AS spl
      FROM split
    UNION          
    SELECT id, SUBSTRING_INDEX(SUBSTRING_INDEX(TRIM(TRAILING ';' FROM val),'; ',2),'; ',-1)
      FROM split
    UNION          
    SELECT id, SUBSTRING_INDEX(SUBSTRING_INDEX(TRIM(TRAILING ';' FROM val),'; ',3),'; ',-1)
      FROM split
    ORDER
        BY id
         , spl
             
TRIM for a reason

UNION instead of UNION ALL for a reason

separator `'; '` including space because that's what you had

as for the newID, you can get that by feeding the results of the query into a new table with an auto_increment column",1543623569.0
OkieDaddy,"First things first, create a stringsplit function(unless you're 2016+, it's included!)

    CREATE FUNCTION [dbo].[fn_split]
    (
        @param VARCHAR(MAX),
        @delimiter CHAR(1)
    )
    RETURNS @t TABLE
    (
        val VARCHAR(MAX),
        seq INT
    )
    AS
    BEGIN
        SET @param += @delimiter;
        WITH a
        AS (SELECT CAST(1 AS BIGINT) f,
                   CHARINDEX(@delimiter, @param) t,
                   1 seq
            UNION ALL
            SELECT t + 1,
                   CHARINDEX(@delimiter, @param, t + 1),
                   seq + 1
            FROM a
            WHERE CHARINDEX(@delimiter, @param, t + 1) > 0)
        INSERT @t
        SELECT SUBSTRING(@param, f, t - f),
               seq
        FROM a
        OPTION (MAXRECURSION 0);
        RETURN;
    END;
    GO

&#x200B;

Then, using that, you can slice up your multi-value column as such.   


    SELECT ROW_NUMBER() OVER (ORDER BY V.id) AS NewID,
           V.id AS OldID,
           FS.val AS Value 
    FROM #values AS V
    CROSS APPLY LMSDatabase.dbo.fn_split(V.string, ' ') AS FS
    

&#x200B;",1543942634.0
TheZeven,"    plink -i $PathToKey user@0.0.0.0 ""mysql -u $user -p$pass -e 'CREATE DATABASE $DBName;' ""

.. .I think?",1543620718.0
kaydub88,"Eh, I'm no longer that familiar with windows systems and build systems so my suggestion may not be helpful at all.

Just use docker for WP. WP docker image and mysql docker image, then be done with it. Just mount a volume for your persistent data on both of them.",1543623700.0
pforsbergfan9,Why not create a user for the connection and connect it using authentication plugin?,1543605932.0
r3pr0b8,"`CREATE PROCEDURE ()` requires parentheses for parameters, even if there aren't any

also, the AS is not part of the syntax",1543605745.0
SaltineAmerican_1970,What is the error message?,1543605096.0
MMOAddict,"I would just edit the dump file myself.  Just copy the insert sql out of the file into it's own file, then modify it to only insert the rows I want. 

A more scripted way to do it would be to export as csv, then import the data into a script. Then you can choose which rows and how they are inserted. ",1543599258.0
razin_the_furious,"depending on how big the dataset is, MySQLyog can export the results of any select into a SQL insert statement.

So you could run a select for the few rows you want, save it as a .sql file, and run it against other databases",1543599902.0
k7f,"You can export selected data using \`SELECT \* INTO OUTFILE '/tmp/filename.txt' FROM table WHERE …\` [https://dev.mysql.com/doc/refman/8.0/en/select-into.html](https://dev.mysql.com/doc/refman/8.0/en/select-into.html) and then import it using LOAD DATA LOCAL INFILE [https://dev.mysql.com/doc/refman/8.0/en/load-data.html](https://dev.mysql.com/doc/refman/8.0/en/load-data.html)

Or if you have exported data as csv, you can use this tool [https://harelba.github.io/q/](https://harelba.github.io/q/) and extract interesting rows and then import them.",1543623629.0
jericon,"The best way to accomplish this is to split the destinations column.  The LIKE CONCAT will not use an index.

If you change it to be one row per LAB, then you can use simple joins.",1543601881.0
jericon,"To me, Crashed simply means the server crashed.  Especially in InnoDB, crash recovery will be necessary, but it will typically be done on startup.

A corrupted table is a table that has bad information in it for some reason.  Trying to read from a corrupted table typically leads to a server crash.",1543699632.0
ofb,"Will you cover maintaining independent write availability at two+ data-centres experiencing a network split and re-merging them into a single cluster afterwards? 

Cause that's what I've been looking for forever. 

I know big companies do it, but haven't been able to find any good info on it. I imagine it's custom tooling everywhere?",1543547702.0
5of10,The MySql cli can use the defaults file option to store & use a login & password. Maybe that would help.,1543428463.0
SomeGuyNamedPaul,"They removed that header file in MySQL 8 and the Python native connector hasn't patched in the difference.  I suppose you could slip in a dummy my_config.h into the includes path and see if you can placate things, or you could lay down a 5.7 in parallel and build that way.",1543408245.0
MMOAddict,"Did you try this: 

https://stackoverflow.com/questions/12218229/my-config-h-file-not-found-when-intall-mysql-python-on-osx-10-8",1543594046.0
jdursa,"I ended up just wrapping an update with a limit of 5000 in a bash loop with a sleep, did the job ;D",1543406635.0
jericon,"Check out common schema and queryscript. It can do exactly what you want. 

https://github.com/shlomi-noach/common_schema",1543419089.0
crespo_modesto,"damn 12 million rows? out of curiosity is this on a single machine? I can't help you though sorry, just curious like elapsed time dealing with that many rows",1543395975.0
mobsterer,"just curious...why the double id select?
Why not just 

    UPDATE table
      SET key = 'value' 
      WHERE id in (
        SELECT id FROM table 
        WHERE value is NULL 
        ORDER BY id ASC 
        LIMIT 1000
    );",1543443841.0
r3pr0b8,"    SELECT CONCAT(Zoo, ' ', Species) AS ""*""
         , AVG(Price)                AS ""Average Price""
         , MAX(Price)                AS ""Max Price""
      FROM mytable
    GROUP
        BY Zoo
         , Species  
    ORDER 
        BY Zoo
         , Species  
        ",1543331744.0
MMOAddict,"You mentioned you are new to this so just in case you actually have pound symbols in your database, you'll need to remove those before you can run the query above this post.  If they are all varchar with a pound symbol, you can remove them all at once with a query like:

    UPDATE mytable SET Price=SUBSTRING(Price,2)

Then change the fields to integer (or decimal if you need penny/pence values) .",1543333734.0
LincolnshireSausage,"Regional? Do you mean relational?  
Did you read the message and click ok? What happens when you click ok?",1543318920.0
geoffdevitt,"In my course on Udemy for ""[Business and Data Analysis with SQL](https://www.udemy.com/business-and-data-analysis-with-sql)"", I walk though the installation of MySQL.  I've released some of the videos for free on YouTube, such as this one where I show [how to start and stop the MySQL server.](https://youtu.be/GtPmYLr_tLE)  ",1543326290.0
jericon,How did you install MySQL?  That is a major factor for how things work.,1543699840.0
CustardSandwich,"You need to find the date that they first appear, then group by that:

SELECT month(first_date),
Count(*) as new_clients from (
SELECT wpmv_postmeta.meta_value, min(wpmv_posts.post_date) AS first_date
FROM
    database.wpmv_postmeta
        INNER JOIN
    wpmv_posts ON wpmv_postmeta.post_id = wpmv_posts.ID
WHERE
    meta_key = '_customer_user'
        AND wpmv_posts.post_status = 'wc-completed'
GROUP BY wpmv_postmeta.meta_value)
As w
GROUP BY month(first_date)",1543281944.0
Ipecactus,"How do you define a ""new client""?",1543268694.0
r3pr0b8,"> clients without orders

i'm not going to attempt to solve that part, because that meta shit (wordpress?) drives me crazy

but i'll give you a heads-up on your grouping

you need your SELECT clause to mirror your GROUP BY clause

you can't select a date and then goupr by month, you have to select the month and group by that

also, always try to count rows instead of column values


    SELECT COUNT(*) AS 'NEW CLIENTS'
         , MONTH(wpmv_posts.post_date) AS 'Month'
    ...
    GROUP 
        BY MONTH(wpmv_posts.post_date)",1543272573.0
apaethe,"Could you create a http://sqlfiddle.com/ with the schema in there?

If so someone will be able to help you out really quickly.",1543276371.0
MMOAddict,"Can you back up all of the databases, remove both mysql and the installer, and then try it fresh? ",1543260172.0
apaethe,I think you are looking for the JOIN syntax.  Here is the manual page on that. https://dev.mysql.com/doc/refman/8.0/en/join.html,1543230802.0
MMOAddict,"I'm a little confused. Are you trying to use Mysql like you would Excel, where you input data directly into tables through an interface like Workshop?  It's okay if you are, it's just IMO a little easier to use Excel for spreadsheets. ",1543205134.0
geoffdevitt,"I cover data normalisation in my course on ""[Business and Data Analysis with SQL](https://www.udemy.com/business-and-data-analysis-with-sql/?couponCode=WHATDATA10)"".

In addition, it's also useful to understand how to capture data requirements and why we normalise data in the first place.  You can watch my video [here](https://youtu.be/RPcOyhTXqjc) for this.",1543237914.0
MMOAddict,"I haven't used Workbench in a really long time. In most GUI programs, a red key means unique key.  Did you add a unique key to one and not the other?",1543246859.0
razin_the_furious,"Dumps are basically generated DML statements. Realistically, you would have typed the same or similar to make the tables.  I guess I see his point: a dump doesn’t prove whether or not you made the tables yourself. But realistically, the DML you typed yourself could be identical to those produced in a dump",1543168921.0
MMOAddict,"One way you can avoid this in the future: always use aliases for everything. The Create Table syntax doesn't really allow for that, but in any inserts, updates or selects, use an alias for all tables and subqueries. Another way would be to only use backticks for reserved words, but I always like it looking consistent, so I'd have to sacrifice that for a picky professor.  Also don't tack on the default charset or engine.. those have defaults in mysql and can be set automatically. ",1543247839.0
Bitter_Bridge,"You could do it as a UNION ALL:

SELECT date, orders_received, orders_attended
FROM table
UNION ALL
SELECT ‘Total’, SUM(orders_received), SUM(orders_attended)
FROM table",1543159256.0
razin_the_furious,"WITH ROLLUP at the end will do this, but wont make a nice label",1543161477.0
brwtx,Is the last column being imported something that can support decimal?,1543113589.0
mcstafford,"Make sure the data's line breaks are consistent.

The UI is walking you through writing a [load data](https://dev.mysql.com/doc/refman/5.7/en/load-data.html) statement, part of which specifies how a line/record is identified.",1543094180.0
DrChuTang,Make sure you have no timeout limits in php.ini or other config files. I had to import 37K records into mysql and was having timeout issues. Resolved for me,1543100053.0
holdamster,I'd also check the MySQL net_read_timeout and net_write_timeout values and increase them temporarily if needed. ,1543109444.0
messburg,"Ok, never mind, I had to take the delimiters seriously. Thought it was needed only when working directly from the commandline. Now this works:

    use integration;
    
    DELIMITER //
    CREATE PROCEDURE `integration`.`GetLastETLCutoffTime`(IN TableName VARCHAR(255))
    BEGIN
      SELECT `Cutoff Time` AS CutoffTime
      FROM `Integration`.`ETL Cutoff`
      WHERE `Table Name` = TableName;
    END //
    DELIMITER ;

And while testing it, I found out that the proper way to call is:

    call GetLastETLCutoffTime('City');

and not with the  \` character.

Oh well, not telling how many hours wasted on this. Hoping that the parsing of the parameter is seamless as well, when done from the SSIS-package :D

&#x200B;

Never understood the error message though, despite the issue being fixed.",1543052671.0
MMOAddict,"It looks like you've done everything right so far.  To get the results of the view in a query at this point, you just use something like:

    SELECT 
        CONCAT(ev.firstName, ' ', ev.lastName) fullname, 
        ev.`Employees Reporting To`
      FROM `employeeView` ev
      WHERE 
        ev.`Employees Reporting To` = (SELECT MAX(`Employees Reporting To`) FROM `employeeView`);

That's one way to get all the max employee managers. Not the best way, but a way :) ",1543072886.0
SomeGuyNamedPaul,"Where in subquery fires that subquery once per row in the outer query.  Basically, never do a where in unless you're explicitly listing PK values.

---

Recommended replacements:

* Flatten your query (make it all joins)

* Hide your inner queries in ""inline views"": join (select ... from ...) as abc

* CTEs in MySQL 8

---

I would recommend flattening the query first, but in cases where you have an aggregator like max(), group by, or limit then you can make performance miracles happen with the inline view.  They're my favorite.",1542990227.0
SomeGuyNamedPaul,MySQL is crashing it some kind of GUI or preferences box?  I don't see anything in there about mysqld.,1542979538.0
AmusementAlley,Part Two Of Crash Log Here - [https://dba.stackexchange.com/questions/223274/mysql-keeps-crashing-im-running-macosx-10-9-5-2018-part2](https://dba.stackexchange.com/questions/223274/mysql-keeps-crashing-im-running-macosx-10-9-5-2018-part2),1542973004.0
MercurialNerd,"Not a solution to your specific issue, but have you considered running MySQL in a VM on your Mac? That way it's sandboxed from OSX and you can easily recreate the VM if it ever gives you grief. 

I tend to use Vagrant, with an underlying Ubuntu VM running on VirtualBox, but there's other alternatives out there like Docker, Lando, DDEV...",1542985673.0
MMOAddict,"You might have had the database in mysql prior to 5.0 before.. I believe it was in mysql 5.0 that they added that restriction.  You can either modify the columns to use DATETIME and only use the 1 TIMESTAMP column for the updates (not sure why you'd need 2 of them tbh), or you can switch back to an older mysql version.  I would recommend fixing the database to only use 1 TIMESTAMP with CURRENT\_TIMESTAMP default value per table.

Edit: Also Mysql past 5.6.5 has this restriction taken out again.  So upgrading can fix it too. ",1542944672.0
lovattows,"Sorry didn't post image link

[https://i.imgur.com/9VAM2xO.png](https://i.imgur.com/9VAM2xO.png)",1542926471.0
wake6n9bake,"""It doesn't work""...

Post the actual error if you want any hope of help from the internet",1542928967.0
Anterai,"Too many DB connections? Are you not closing them or something? Which is silly.  

How long is ""A lot to execute""?   
Where's the code?   
How big is the DB?   
Show us the SHOW CREATE of the table.  
Where's the config file dump?    
What's the Hardware?    

Etc etc. Lots of questions need to be answered before we can help you.
",1542933564.0
Bitter_Bridge,"1. What language are you using?

2. Please post your queries

3. Please post the errors you’re receiving, if any. ",1542935434.0
MMOAddict,"Are all the queries updating or inserting into the same tables?  If so, you can simplify it by combining all of them into 1 (or a few queries if it bypasses the max query size).  Don't just add the query strings together, for example you can write:

INSERT INTO table1 (\`name\`, \`somefield\`) VALUE ('name1', 'value1'), ('name2', 'value2').... (you can add as many as you want).

&#x200B;

If you are doing update queries it gets a little more complex but this is how I do it:

    UPDATE table 1 SET 
        `name`= CASE ID
            WHEN 1 THEN 'name1'
            WHEN 2 THEN 'name2'
            ELSE `name`
        END,
        `somefield` = CASE ID
            WHEN 1 THEN 'value1'
            WHEN 2 THEN 'value2'
            ELSE `somefield`
        END
      WHERE ID IN (1, 2);",1542944326.0
CrazyNateS,"~~Try using the local IP (192.168.X.X) rather than the loopback address(~~[~~127.0.0.1~~](https://127.0.0.1)~~).  You may be able to get away with using ""localhost"" for the address rather than~~ [~~127.0.0.1~~](https://127.0.0.1)~~.~~

&#x200B;

~~Also, if you're using MySQL 8, they changed some default security settings that may be conflicting if Mode hasn't been updated for the new version.~~

&#x200B;

Never mind everything I just said....it looks like Mode is a web hosted service, which means your MySQL server has to be accessible from the internet, which I'm guessing your router is not set up for.  You will need to open port 3306 on your router, forward it to your laptop, and give Mode your public IP ([www.whatismyip.com](https://www.whatismyip.com) can help with this)",1542835495.0
MMOAddict,"[127.0.0.1](https://127.0.0.1) is a loopback address. You need to instead use your public IP address to that mysql server. 

If you check the IP address of your mysql server running on your mac, it will most likely be a private ip address (either starts with 192.168.x.x, 172.16-31.x.x, 10.x.x.x).  One of the easy ways to find out what your public IP address is just to google ""what is my IP"" and it'll tell you.  You will also need to forward port 3306 on your router to your mac.. this is a little harder to explain how to do because every router is different..  Usually it's called ""Port forwarding"" or ""firewall settings"" or ""NAT settings"", in that order of most common to least common.  You can just google ""<your router brand/model> port forward"". ",1542836636.0
jericon,What were you doing before this happened?  Has the table every been visible?  Did anything change?,1542827205.0
MMOAddict,try restarting mysql.  if it's running debian you can use: sudo /etc/init.d/mysql restart,1542828556.0
MMOAddict,"Pretty sure you can't do the + javascript-like concatenate in mysql. If you want to concatenate a series of strings you have to use CONCAT(<string1>, <string2>, etc).. so try CONCAT('%', @Artist, '%')",1542814318.0
justintxdave, set @foo=rand();,1542810304.0
jericon,More information would be useful.  What exactly are you trying to do?,1542827548.0
MMOAddict,"I'm assuming there are no pick entries for an employee that hasn't used any picks yet.. if so, change the p.type=1 to:

    (p.type = 1 OR p.type IS NULL)

in the WHERE clause..

&#x200B;

also change all of the COUNT(p.start) to COUNT(IFNULL(p.start,0))

You could also move the p.type=1 to the ON clause of the LEFT JOIN (instead of my first suggestion) and that might make the query use less memory.",1542777064.0
r3pr0b8,"fixed it for you --

    SELECT e.eid
         , CONCAT(e.first , ' ', e.last)
         , COUNT(p.start)/2 as Used
         , e.vacation_days
         , e.vacation_days - COUNT(p.start)/2
      FROM employee e
    LEFT OUTER 
      JOIN picks p
        on p.eid = e.eid
       AND p.type = 1 
       AND YEAR(p.start) = 2019
    GROUP 
        BY e.eid
         , e.first 
         , e.last
         , e.vacation_days
    HAVING e.vacation_days <> COUNT(p.start)/2 

first, moved the WHERE conditions to the ON clause, so you get an actual outer join, not inner join

second, fixed the GROUP BY clause to conform to sql standards",1542796535.0
CrazyNateS,"`INSERT INTO webuser (p_id,username) (SELECT person_id,CONCAT(SUBSTRING(f_name,1,1),SUBSTRING(l_name,1,5)) FROM person WHERE person_id=TARGETID)`

Should do the trick.  Replace TARGETID with the person\_id that you want to create a name for, or remove the entire WHERE clause if you want to create users for everyone",1542745411.0
WampM,ALSO: p\_id is a foreign key in 'webuser' referencing person\_id in 'person',1542745043.0
dr_goodweather,"What do you mean?

Between two rows?",1542718760.0
crapslock,"Something like 'update table set x=y, y=x' ?",1542734455.0
MMOAddict,"The column attributes are all variables so you can just define the attributes again for each column if you're just wanting to change names of a column or whatever.. I can't think of a reason to want to do something like that live.  Well, I can't think of a good reason to want to do something like that live.",1542750618.0
jericon,Removing this post. User has posted multiple threads without any information and is not responding to comments. ,1542767305.0
jericon,Need more info. Please give an example of what you are trying to do. ,1542767262.0
Bitter_Bridge,"Just a heads up, mysql_connect() is deprecated. Look into PDO. ",1542671565.0
mudclub,"https://my.bluehost.com/hosting/help/2209

dude.  google first.",1542663316.0
maetthu,"I stopped using Workbench (it's so incredibly buggy it got beyond usable for me), but regarding generated columns: I guess the expression Workbench is expecting here is the same as MySQL expects for generated columns. However, you can't use values from other tables, so it wouldn't work for your use case anyway, see here for more details what generated columns are: https://dev.mysql.com/doc/refman/8.0/en/create-table-generated-columns.html. ",1542712496.0
justintxdave,You would could use a view as as view can draw from multiple tables while generated columns can not use multiple tables.,1542727716.0
angusmcflurry,Do you have an index on the productname column?,1542652366.0
emsai,Select * should be regarded as an error.,1542657319.0
peixinchen,"What is your explain command's output?

If you have a index on the productname column, the query operation would hit the index but your select \* could cause MySQL sort using filesort, in the case of big table, disk maybe use.",1542706429.0
mobsterer,the fetching of data once the record is found hardly makes a difference in my experience. sometimes it is even worse to do the extra filtering by certain fields.,1543016684.0
r3pr0b8,your error is simple -- do not return 4.1 gigs of data all at once!!!,1542659792.0
freygl,"OK, i found a solution myself. I created a BEFORE INSERT trigger that shoots INSERT IGNORE statements over to the parent table for every ROW that is inserted in the child.

    INSERT IGNORE INTO Holarbleikja.UniqueIDs (UniqueID) VALUES (NEW.UniqueID);

But is this best practice? Shoot Insert statements at the parent table and just let it ignore all the once that don't work because the value is already existent?  ",1542635702.0
mischiefunmanagable,percona toolkit - pt-query-digest,1542601275.0
Johnnymonny1991,"Update table t
Set URL = concat(""http..."", URL)

Or did I understand you wrong?",1542487431.0
jynus,"The equivalent to the feature you mention- transaction log archiving, would be `innodb_log_archive` which was only present on XtraDB (Percona and MariaDB in the past): https://www.percona.com/doc/percona-server/5.6/management/log_archiving.html

For binary logs, the ""MySQL"" model is to archive remotely using [`mysqlbinlog -R --raw`](https://dev.mysql.com/doc/refman/8.0/en/mysqlbinlog-backup.html), as doing it locally would be considered a bad practice as it is supposed to be used for backups (on an external host). Of course, you can still do that on a local instance...

I am not familiar too much with the Enterprise Backup utility to tell you if that is integrated on that product.",1542485286.0
chowderl,"You can use percona xtradb backup tool for this purpose. 
https://www.percona.com/doc/percona-xtrabackup/2.2/xtrabackup_bin/working_with_binary_logs.html 

Note: I don't work for percona 
",1542559159.0
jericon,"Honestly, if you just want to keep a copy of the binlogs the easiest way to do it is to just make a copy of them.  Use rsync or something like that.",1542827463.0
MiltonsBitch,"No, it's not needed but you can use it to point to a configuration file instead of entering the options on the command line.

https://dev.mysql.com/doc/mysql-enterprise-backup/4.1/en/mysqlbackup.html",1542472695.0
jericon,High performance MySQL is kind of the MySQL bible. It covers up to 5.5 but is still highly relevant if you want to get into how MySQL works “under the covers”. ,1542405957.0
,[deleted],1542393488.0
kaydub88,"The best book for MySQL is MySQL by Paul Dubois.

After you get through that go check out High Performance MySQL.

Now that I'm actually reading your post though it seems you're just looking for info on writing queries... which isn't really covered in those books so much.",1542410229.0
neefrankie,I learned by reading the official documentation.,1542433971.0
MarkusWinand,"WITH is a prefix to the SELECT keyword. Read about it here: [https://modern-sql.com/feature/with](https://modern-sql.com/feature/with)

However, I wonder why you would even use WITH for that (as you don't have a SELECT)? Your VALUE clause has only constant values. You could skip the WITH clause and the INSERT should work.

In case the third column is supposed to get the result from the query in the WITH clause (it doesn't because you put image\_theme under quotes), I'd rather go for this:

    INSERT INTO imageproperties(Imagename, ImageLink, ThemeID, Date)
    VALUES ('Aqua', '/portfolio/images/Aqua.png',(SELECT ThemeID FROM imagethemes WHERE Theme ='Culture Clash'),'01/05/2018');

I kept the subquery so it just becomes \`NULL\` if there is no matching 'Culture' theme. If you'd like to not insert anything in that case, just go for this:

    INSERT INTO imageproperties(Imagename, ImageLink, ThemeID, Date)
    SELECT 'Aqua', '/portfolio/images/Aqua.png', ThemeID ,'01/05/2018'
    FROM imagethemes WHERE Theme ='Culture Clash'",1542374949.0
Bitter_Bridge,"I think this is going to be a syntax problem. If you'll notice, the error is actually telling you what's wrong (although in an unhelpful way). ""WITH is not valid at this *position*...""

From the MySQL manual, these are the valid positions of a WITH clause (Taken from https://dev.mysql.com/doc/refman/8.0/en/with.html):

At the beginning of SELECT, UPDATE, and DELETE statements:

WITH ... SELECT ...
WITH ... UPDATE ...
WITH ... DELETE ...

At the beginning of subqueries (including derived table subqueries):

SELECT ... WHERE id IN (WITH ... SELECT ...) ...
SELECT * FROM (WITH ... SELECT ...) AS dt ...

Immediately preceding SELECT for statements that include a SELECT statement:

INSERT ... WITH ... SELECT ...
REPLACE ... WITH ... SELECT ...
CREATE TABLE ... WITH ... SELECT ...
CREATE VIEW ... WITH ... SELECT ...
DECLARE CURSOR ... WITH ... SELECT ...
EXPLAIN ... WITH ... SELECT ...

Where I think your problems lie is that you are doing WITH ... INSERT. The first problem is that improper statement patterning (according to the manual), and the second problem is that you have no SELECT. The WITH statement creates a temp table in the current scope, but you are attempting to reference a column in that temp table as though it's a variable. I *believe* you'll need to do it as an INSERT...SELECT, only with a WITH in the middle. 

Please reference this SO conversation for more specifics on that last part: https://stackoverflow.com/questions/3306096/combining-insert-into-and-with-cte",1542373921.0
Irythros,[https://stackoverflow.com/questions/9780637/how-can-i-restrict-a-mysql-user-to-a-particular-tables](https://stackoverflow.com/questions/9780637/how-can-i-restrict-a-mysql-user-to-a-particular-tables),1542371179.0
apaethe,"I haven't tried this but I can imagine a stored procedure that took in parameters to create a table, created it, then assigned privileges to the user who created it.

Presumably you wouldn't want to make a procedure that let them execute abstract sql, but I think if you parameterized the procedure   to generate the create statement, then used the current_user() function, you might be able to do that dynamically.",1542486077.0
ssmagin,"Hi, I am the author of this tool. Thank you for your thoughtful review :)

BTW, feel free to visit [the FlySpeed SQL Query homepage](https://www.activedbsoft.com/overview-querytool.html).

If you have questions, feel free to ask me right here.",1542346041.0
mkomar,"You could, using PHP go directly from mysql into Google Sheets ... but otherwise, going from a static query to cvs would be very, very simple.",1542298481.0
tuantutran,"I feel the mysql command line client could do it.
https://stackoverflow.com/a/2601837

You'd have to install it on that person's computer.
Then you write a small batch script that this person double clicks and it will dump the csv to disk.

If you do need sed as proposed in this post, you'll need some kind of bash, so git for windows might provide you that.

On the other hand writing a small python script works too and should not be too complicated",1542359556.0
Anterai,Is the person using Linux? If yes I can prolly whip up something quickly for them,1542472437.0
KrevanSerKay,"I recently made an internal tool that's basically just a wrapper around mysqldump and/or select into outfile. Let's my customer support team enter some parameters and it spits out a CSV file for them.

Should be easy to throw something together that does similar. ",1542301129.0
devofallthingsjs,"[https://www.sequelpro.com/](https://www.sequelpro.com/)

You have to connect to the database, but easy to run a query. Once you have query results, click File, Export",1542308326.0
ssmagin,"FlySpeed Data Export has a GUI to set up MySQL connection, define a query and the needed export file type. Having set it up, you save the configuration to a file and can run it with the console app. After that you can make a .bat file and place a shortcut on the desktop - voila. Seems like this is exactly what you need. 

[http://www.activedbsoft.com/overview-dataexport.html](http://www.activedbsoft.com/overview-dataexport.html)

You need the Professional version, it has a command-line utility along with the scheduler and OLE Automation.",1542367832.0
Johnnymonny1991,"What about mysql for excel? 
It's an add-on for Excel and the consultants in my company (literally idiots when it comes to other software than MS Office) can use it.
It's probably the easiest way, as I see it.

Edit:
Create some views, grant access for these views, ez win",1542654142.0
Laurielounge,"Ah

&#x200B;

No. To both.

They're completely different animals and don't really talk to each other.

I guess this is like saying, if I give food to my dog will my cat get too fat or will I have to feed the cat to make my dog fat.

&#x200B;

Good luck, shout if you need ideas",1542251029.0
apaethe,"The ODBC connector between the two will let you do some things, like copy data and tables from one to the other from inside Access.  But not triggers, stored procedures, or functions.",1542257727.0
r3pr0b8,"there are quire a number of issues with your solution

here's your current query, slightly reformatted --

    SELECT DISTINCT 
           COUNT(OP.qtyOrdered) as TotalOrdered
         , P.prodName
         , MAX(OP.paidPrice) as highestpaid
         , MIN(OP.paidPrice) as lowestpaid
         , MONTH(CO.orderDate) as ordermonth
         , YEAR(CO.orderDate) as orderyear
      FROM Product P
      JOIN OrderedProduct OP 
        ON P.productId = OP.ProductId
      JOIN CustOrder CO 
        ON OP.orderId = CO.orderId
     WHERE P.active = 'A' 
       AND P.categoryname = 'Dairy Products' 
    GROUP 
        BY OP.qtyOrdered
         , CO.orderDate
         , OP.paidPrice
    ORDER 
        BY CO.orderDate

here's what it should look like --

    SELECT YEAR(CO.orderDate) as orderyear
         , MONTH(CO.orderDate) as ordermonth
         , MONTHNAME(CO.orderDate) as ordermonthname
         , COUNT(OP.qtyOrdered) as TotalOrdered
         , MAX(OP.paidPrice) as highestpaid
         , MIN(OP.paidPrice) as lowestpaid
         , SUM(OP.paidPrice) as totalamount
      FROM Product P
    INNER  
      JOIN OrderedProduct OP 
        ON OP.productId = P.ProductId
    INNER  
      JOIN CustOrder CO 
        ON CO.orderId = OP.orderId
     WHERE P.active = 'A' 
       AND P.categoryname = 'Dairy Products' 
    GROUP 
        BY YEAR(CO.orderDate)
         , MONTH(CO.orderDate) 
         , MONTHNAME(CO.orderDate)
    ORDER 
        BY YEAR(CO.orderDate)
         , MONTH(CO.orderDate) 

notice the changes --

- added MONTHNAME() to SELECT clause
- added SUM() to SELECT clause
- removed P.prodname from SELECT clause
- minor peccadillo, added INNER keyword to JOINs
- changed GROUP BY to only non-aggregate SELECT clause columns (retaining MONTHNAME in GROUP BY keeps it compatible with standard sql, but mysql doesn't need it)
- changed ORDER BY to use SELECT clause columns",1542236997.0
apaethe,"Here are the aggregate mysql functions,
https://dev.mysql.com/doc/refman/8.0/en/group-by-functions.html
which have standard deviation and variance.  Statistics was never my 
bag and I don't know what NORM.DIST  is calculating, but you can probably
get there with these.",1542257965.0
z4x_,"I would like to save ""1.testatuebung"", how do I do this? ",1542129753.0
,[deleted],1542126831.0
knifebork,"The dump folder has to be on the system where you are running Workbench. Essentially, a dump file contains a bunch of SQL commands that will be run either by Workbench or command line SQL.",1542126750.0
knifebork,"In Workbench, after you connect to your database server, ""Data Import/Restore"" is under ""Management"" in the menu.
",1542126866.0
Voidheart625,Sorry for all the noobish questions again!  I really appreciate everyone's help!,1542131545.0
ajanty,"Make another dump, catch exceptions in the dumping process and if successful make a diff as they are text files.

&#x200B;",1542121275.0
,[deleted],1542149791.0
Nk4512,"To add to this, As others might have this issue. 

When you shutdown mysql it also shuts down its sock connection and removes the file, So when you start it in safe mode it will just die and not say anything but it will act like you cannot connect. 

For this you can find your sock file (While it's running), Copy it to a separate temp file. Kill mysql, rename the temp file to the name of the original and restart. Such a pita in some cases.",1542065054.0
jericon,We need more info about what you are trying to do if you want help. ,1542076441.0
,[deleted],1542053501.0
simonced,"UUIDs should be unique enough so you don't need to check for duplicates.
Especially if generated by the same machine.

Some numbers and articles for your understanding:

- https://en.wikipedia.org/wiki/Universally_unique_identifier
- http://www.h2database.com/html/advanced.html#uuid",1542080358.0
SomeGuyNamedPaul,"Try **UNION ALL**


The more heavy-handed approach is:

    Select * from (SELECT age,sex, ethnicity FROM db.form11 LIMIT 3) as x
    UNION ALL
    Select * from (SELECT age, sex, ethnicity FROM db.form12 LIMIT 3) as x

",1542005350.0
r3pr0b8,"oh.. my.. god..

here, i reformatted your query for you

    SELECT p.ID
         , e.personaId
         , e.ID AS event_id
         , e.EVENTID
         , e.carId
         , e.alternateEventDurationInMilliseconds
         , p.iconIndex
         , p.name AS p_name
         , cc.name AS c_name
         , cc.carClassHash AS c_hash
         , ev.carClassHash
         , ccs.full_name AS cs_name
         , b.user_id AS banned_status
         , ev.legitTime
         , e.bustedCount
         , e.distanceToFinish
         , e.finishReason
         , e.fractionCompleted 
      FROM EVENT_DATA e 
    INNER 
      JOIN ( SELECT personaId
                  , MIN(alternateEventDurationInMilliseconds) AS alternateEventDurationInMilliseconds 
               FROM EVENT_DATA 
             GROUP 
                 BY ID ) f 
        ON f.personaId = e.personaId 
       AND f.alternateEventDurationInMilliseconds = e.alternateEventDurationInMilliseconds 
    INNER 
      JOIN PERSONA p 
        ON p.ID = e.personaId 
    INNER 
      JOIN CUSTOMCAR cc 
        ON cc.ownedCarId = e.carId 
    INNER 
      JOIN EVENTSAVED ev 
        ON ev.ID = e.EVENTID 
    INNER 
      JOIN CAR_CLASSES ccs 
        ON ccs.store_name = cc.name 
    INNER 
      JOIN USER u 
        ON u.ID = p.USERID 
    LEFT 
      JOIN BAN b 
        ON b.user_id = u.ID 
     WHERE e.finishReason = '1' 
        OR (  e.EVENTID = '28'
          AND e.alternateEventDurationInMilliseconds > '0' 
          AND e.bustedCount < '1' 
          AND e.distanceToFinish = '0' 
          AND e.fractionCompleted > '0.9' 
          AND cc.carClassHash = ev.carClassHash 
          AND e.alternateEventDurationInMilliseconds > ev.legitTime 
          AND e.finishReason != '2'
           ) 
    ORDER 
        BY e.alternateEventDurationInMilliseconds ASC
    
i suggest you look long and hard at your subquery",1541952067.0
jericon,"It’s really hard to read your post. Use some of the markup language to clean it up. 

That being said, I think your best bet is going to be a sub select to get the min time, then use that to get the rest of the data. ",1541941469.0
this-is-me-reddit,Use a sub query sort on the column with the times asc and use limit 1 in the sub query. That’s how I pull a value from a table and avoid a one to many relationship. ,1541959367.0
NotTooDeep,"Passwords exist at the level of the user inside the database. Not relevant to anything in the my.cnf. My guess is you created the database but didn't reset the root password, and you're connecting as root from your BI tools.

And you should not be allowing BI tools to use the root account. Create a reporting user similar to this.

Create user reports@'%' identified by password('BigUglyPassword35-#!z') require ssl;

The @'%' will allow the same user to connect from any IP, if the server hosting mysql also allows this (no iptables or similar).

",1541893473.0
beermad,"Is your input wrapped in a transaction?

I have found that large imports that **aren't** in a transaction can be very slow, because each line processed is written to disc, whereas within a transaction the write is deferred.",1541851047.0
jericon,"One trick I use to see how far along this stuff is to pipe it through pv. 

    pv file.sql | mysql -uuser -p database

It will show you the speed it is importing, estimate of completion and percent done. ",1541904501.0
NotTooDeep,"You should be able to join all four tables, using the filters from the web page, and just return the result set in one trip. 

An easy and more efficient way to code this would be to have the app call a stored proc in MySQL, passing it the filter values. The logic for determining which query to run would be based on which parameters were not null.

This puts all the work of filtering the result set on the database, which means the performance most likely depends only on your indexing strategy.

You are correct: SQL is a set based language and MySQL is mostly SQL compliant. You shouldn't run into issues with lock contention if the tables are only being queried.",1541984069.0
kaydub88,30GB is still pretty small. Migrate the data before worrying about optimizations.,1541797678.0
Nk4512,"What kind of data and structure is it? Mysql compress is decent, or compress it before going to the database. Also depending on the dara you could do that and move to a file system storage. Mysql stores a TON of extra data per row depending on your schema ex innodb",1541815645.0
,[deleted],1541896629.0
Anterai,"Side note: Why not move the historic Data into it's own database?

But really, if you wanna talk about this - I'd love to. I dealt with a similar issue on my own projects (Had a 190GB DB)",1541967896.0
,[deleted],1541793459.0
r3pr0b8,"    UPDATE table1 a 
    INNER 
      JOIN table2 b 
        ON b.s = a.s 
       AND b.date > '2018-10-01' 
    SET    a.checked = NULL    ",1541787266.0
jericon,"Change 

    \G

To

    \\G

The first \ will make it so the second \ is actually passed on. ",1541836260.0
drunkferret,"I have a SQL database that I had to write a whole front end for in Python. Wicked pain in the A. 

Watching your thread in case you find something...but I've gone through a similar search (even totally fine with coding) and I didn't find anything that met our needs. Mine needed to be able to edit as well which added a lot....just having something filter and search isn't hard, but kind of like what you said, python's tkinter interface doesn't have excel-like drop downs for filtering...it's typing based. It's one of the things people don't like about what I made...they just want it to be identical to Excel...and I wish I could, but I don't know how and can't find anything useful to accomplish it.",1541781649.0
emsai,"I'm afraid you won't find anything that does what you want, nitty-gritty.

Side note we have a project in this line, but it's a mammoth to build so it's still on the low priority. We're using a system like this internally and in our SaaS apps, but nothing publicly available yet.

Edit: Hmm, might be worth considering pushing this up a little.
",1541799951.0
doi20,Check out phpmaker.,1541800061.0
thinsoldier,"> what I need is a low-code or no-code solution that will allow end-users...

The only people allowed to not use code are the end users. The less code (or other complex thing) you want the end users to have to struggle with, the more code you have to write yourself.",1541811315.0
emsai,"Here's a tip. Do a crc32 on the string and save that to a separate int field. create an index on the int.

When querying, include the crc in query to match the int as well.

An int based index will be way faster than varchar.",1541763667.0
SomeGuyNamedPaul,"An index that wide is kind of bad anyway.  Run an analysis on the cardinality of your data.  Does indexing the first 10 characters give you some hit 99.99% of the time?  That's enough.

Also, those hostnames are long.  I assume that's the FQDN?  What happens if you normalize away the domain?  Storing domains, especially for corporate hosts, can cause you to store an awful lot of redundant characters.",1541773888.0
NotTooDeep,"Dimensional models work well in vanilla MySQL with the InnoDB engine. An appropriate table partitioning strategy of the fact tables can work wonders. You can build one on the InnoDB engine and get your feet wet. Just be sure all the FKs are indexed. It's not mysql's fault if you forget that.

The ad hoc stuff is dominated by the front end. In my experience, reporting GUIs are way oversold in terms of ease of use. The thing is, the ad hoc report writer person still has to understand their data in great detail and make the right choices when they drag and drop stuff around.

Columnar databases like MongoDB are pretty cool if all of your ad hoc reporting has aggregation. As soon as you ask for a specific row, you're no longer ad hoc'ing for fun and enlightenment; you're going to lunch and hoping that row is found before you get back. Columnar databases are totally optimized for aggregation of data, which means there is no compromise. It will sum stuff really fast, faster than InnoDB can possibly go. 

A dimensional model in InnoDB engine on MySQL is a fair trade off. You can find individual rows fast enough. You can sum large data sets fast enough.

Technical details: normal relational databases like the Innodb engine store all the columns of a single row in the same data block. For each disk i/o, you get one or more complete rows. Good indexing points the SQL engine to that specific data block that hosts the desired row.  Columnar databases store as many instances of a single column in a data block. For each disk i/o, you get hundreds of values for that one column, which makes them killer fast at adding those values. But if you want to see all the columns for a single row and that row has 50 columns, then you have 50 disk i/o returning 50 data blocks to parse through each block, find the correct column for the desired row, and stitch them all together before throwing away 99.9% of the data returned that wasn't in the desired row. It's not efficient.",1541732670.0
veldenar,"For Cloud9 use:

mysql-ctl install

[More Info](https://community.c9.io/t/setting-up-mysql/1718)",1541681429.0
Laurielounge,"Have you started the service? Sorry, been a long time since I installed it on a Windows box",1541642430.0
r3pr0b8,"just as fast and just as slow

p.s. ""= NULL"" won't work",1541611585.0
msiekkinen,"Null is meant to mean you don't know the value, it doesn't have one.  0 means you definitely know the value and it is 0. 

You're worrying about a micro, premature optimization that will have nothing to do compared with making sure columns are property indexed. ",1541612023.0
Irythros,"Since this is such a straight forward case... why haven't you benched this yourself?  It's nothing advanced.  

It's also such a small optimization, you spent more time writing this post than what would be spent in a year in timing differences between the two.",1541629881.0
doi20,With the propper index and buffer pool the speed will be the same. The disk size usage for those values will be slightly bigger for 0 than for NULL.,1541630921.0
apaethe,"Yea, there is a function for that.  Check out the documentation here, https://dev.mysql.com/doc/refman/5.7/en/
This is the documenation for mysql 5.7, so check your version number and use the link you'll see there for another version if you need.",1541570283.0
,[deleted],1541533895.0
Bitter_Bridge,I don’t think this is solvable without being able to see the DDL,1541537017.0
msiekkinen,Sounds like you're column is datetime (not just date) and you're running in strict mode. ,1541545779.0
apaethe,"That date hasn't happened yet, so you'll need to wait a bit before you can insert it.",1541558137.0
shug2,https://help.ubuntu.com/community/MysqlPasswordReset,1541523355.0
,[deleted],1541498423.0
Johnnymonny1991,"Why don't you use 3 subqueries, JOIN and an Select again?

Select g.*, u.*, d.* FROM (
SELECT Year, Type, Total_Revenue, Total_Margin, B2B_Margin, B2C_Margin, Group_Margin FROM goal where Year = 2018 and Type = 'g'
) AS g 
JOIN
(SELECT Year, Type, Total_Revenue, Total_Margin, B2B_Margin, B2C_Margin, Group_Margin FROM goal where Year = 2018 and Type = 'u') AS v
ON g.Year=u.Year
JOIN
(SELECT Year, Type, Total_Revenue, Total_Margin, B2B_Margin, B2C_Margin, Group_Margin FROM goal where Year = 2018 and Type = 'd') AS d
ON g.Year = d.Year",1541543764.0
JamalaBear,"You can change your WHERE statement to 

WHERE YEAR = 2018 AND (type = ‘d’ OR type = ‘g’ OR type = ‘u’)
 
For better performance use

WHERE YEAR = 2018 AND type IN (‘d’, ‘g’, ‘u’)",1541498317.0
mischiefunmanagable,"demos and tests should use their own database, your rollback is to wipe out and recreate",1541453918.0
jeffwrule,"This is not your table, but you should be able to get the gist from here

The use xxx (is my database) pre-created and the create table would not be needed if your user table already exists.  I just included them for completeness.

if you are doing this from the mysql command line..

    mysql> use xxx
    Reading table information for completion of table and column names
    You can turn off this feature to get a quicker startup with -A
    Database changed
    
    mysql> create table abc (i integer);
    Query OK, 0 rows affected (0.02 sec)
    
    mysql> insert into abc values (1);
    Query OK, 1 row affected (0.01 sec)
    
    -- rollback won't work b/c autocommit was on by default
    mysql> rollback;
    Query OK, 0 rows affected (0.00 sec)
    
    mysql> select * from abc;
    +------+
    | i |
    +------+
    | 1 |
    +------+
    1 row in set (0.00 sec)
    
    -- turn off autocommit
    -- will reset when you disconnect and reconnect
    mysql> set autocommit = 0;
    Query OK, 0 rows affected (0.00 sec)
    
    mysql> insert into abc values (2);
    Query OK, 1 row affected (0.00 sec)
    
    mysql> select * from abc;
    +------+
    | i |
    +------+
    | 1 |
    | 2 |
    +------+
    2 rows in set (0.00 sec)
    
    mysql> rollback;
    Query OK, 0 rows affected (0.00 sec)
    
    -- row 2 insert is now gone.
    mysql> select * from abc;
    +------+
    | i |
    +------+
    | 1 |
    +------+
    1 row in set (0.00 sec)

&#x200B;

If you are using a programming language all of this will work the same except you should not need to do the 'set autocommit = 0' as autocommit is off by default for most programming languages.

&#x200B;

Just substitute the appropriate table inserts for your example.

&#x200B;",1541465171.0
6ivxx9,"Begin your query with START TRANSACTION;
The query will run as normal and return results. If you’re doing an INSERT you will be able to SELECT * to see the results of the insert. When you’re done run ROLLBACK. This will reset any transactions carried out since START TRANSACTION. (Instead of ROLLBACK you can run COMMIT to commit the changes to the database)",1541594690.0
Neilly1,That’s perfect so do I just use the rollback query I’m not allowed to use the button,1541594857.0
6ivxx9,You mean the execute button? You can use the button but it will execute all the code in the window. If you just want to rollback (and not run anything else) just highlight ROLLBACK in the query window and click the execute button - it will only run the highlighted code. ,1541615651.0
Bitter_Bridge,A stored procedure could handle that ,1541451740.0
msiekkinen,So... you want reddit to do your homework for you?,1541439442.0
ofb,"Just a warning to others, this is not an open source project (nor does it claim to be one). From the license `Don't steal my code` is essentially `all rights reserved`.

I will guess that this is dangerous to do on a live production database, and likely catastrophic on very large live production databases. 

Look into pt-online-schema-change tools instead if you can't take your database offline to do this (it's a lot more work tho).",1541435585.0
KevZero,Ow my disk!,1541423961.0
randomfrequency,This is an awful idea.,1541437555.0
NotTooDeep,"The short answer is it's a many-to-many relationship.

A user may post a video. A user may unpost a video and no one can see it anymore.

A user may see a video. A user may filter which videos they see by an arbitrary window of time.

Some users want to see all of the posts from other users. We'll call this a subscription.

A user may post one or more videos (many) <=> a user may subscribe to all posts from another user (many).

The solution is a bridge table called Channel. It might hold the post_id and user_id to define the left side of the above many-to-many relationship. It will have another user_id representing the subscriber, representing the right side of the above many-to-many relationship. To enable the ""unpost"" feature, the channel table needs a flag; call it ""is_public"" and make it a Boolean. 

All queries will have a where clause ""is_public = true"" and only the authorized posts will appear on a screen for the subscriber.

The concept, ""feed"", is not part of the model. It's part of an abstraction on the UI to hide the complexity of the many-to-many relationship. The user sees ""My feeds"" and has no worries.  All the while, millions of users and subscribe to millions of different channels with just three tables; user, post, channel.

There may be more tables. Videos may be stored on a dedicated device outside of the constraints of a database for performance reasons. There may be HA considerations that require alternate paths to the videos be stored in lookup tables (not sure I'd sanction this as a good idea, but it's one possibility).

But the basic relationship in any subscription model is many-to-many.",1541425789.0
kaydub88,A lot of that stuff probably isn't using MySQL. ,1541469442.0
brkmnd,"If you use set, you are still within the query. Say you allow input on your set, you can end up with the user setting something like: set @id=""123""-- or drop table, where the user input is '123""--or drop table'.

I think this video does a pretty good explanation: [https://youtu.be/ciNHn38EyRc](https://youtu.be/ciNHn38EyRc) . The point is either sanitize rightfully, and that can be even more of an headache, or use prepared statements.  
",1541399106.0
talbahir,Would this still work when I escape every input before put in the set?,1541478475.0
TheSqlAdmin,"Yeah in most cases security group will be the culprit. Can you please share your output of show slave status\G, so can get the exact issue.",1541314255.0
razin_the_furious,"Silly question, but did you run slave start on the RDS? Did the user running it have permissions to do so?",1541336095.0
basc762,"If you're worried about networking/perms, ssh into your ec2 instance and use the mysql command to connect to your rds server from the ec2 instance. If you can login, you know it's not networking but it could still be perms. If you can't login and don't get a prompt, it's networking

mysql --user=(your_user) -p --host=(rds endpoint) --port=3306

",1541339956.0
basc762,"Another thing, flush hosts and privelages JIC you applied perms afterwords and bad ones are cached. ",1541340040.0
Pip_Pip,Did you create a replication user on master?,1541354583.0
r3pr0b8,"we'll need to see the EXPLAIN

by the way, your WHERE condition means you want INNER JOIN, not LEFT OUTER JOIN",1541187398.0
Nk4512,"You would probably want to setup indexing as well as create some foreign keys. 
Once you do that, it should speed the hell out of it. I had an issue similar to this with a couple tables that were ~2 million or more rows. Query (Multiple - like 400 or so) went from 7 minutes total to render data to .3 seconds. ",1541205704.0
apaethe,"Because you are filtering on the post_taxonomy_term_map table in the WHERE clause, you should switch that to a RIGHT join.  You only want to see values where the post_taxonomy_term_map values match the set you have, so doing that in join condition is what you want.

Also then move the where condition into the join, and put a two column index on ""term_id,object_id"" ( try ""object_id,term_id"" too, it's late and I'm drunk ).",1541217937.0
EstimatedPassenger,Check that the collation of both tables matches. I also had a situation where a query was taking far longer than I thought it should and I came across a suggestion to check collations. Once I changed the collation of one table to match the other my query time went from about 20 seconds to under a second. ,1541371853.0
r3pr0b8,"> Is there anyway I can link these tables

they already are linked -- through the foreign keys

however, you aren't really looking for a 3-table join to return what you want, because you said all you want is the ids

    SELECT UserID
         , OrderID
         , PassengerID
      FROM Orders

the moment you need any other columns from either the Users or Passengers table, you will have to join
",1541119514.0
6ivxx9,"I think you want:

SELECT u.UserID
     ,o.OrderID
     ,p.passengerID
FROM Users u JOIN Orders o
     ON u.UserID=o.UserID
JOIN passengers p 
     ON o.OrderID=p.OrderID

(I assume OrderID appears in the Orders table as well as the Passengers table?)

It’s a bit hard to be sure without seeing table definitions for all 3. If UserID is in Users and Orders join on that, if OrderID is in Orders & passengers join on that then all 3 tables should be joined (In fact, you may not need the Users table at all. If UserID is in both Users and Orders you could bring it back from Orders without joining to Users - unless you need other information from Users). Once joined correctly you can select any columns from any of the three tables, just specify them in the SELECT clause. ",1541160588.0
,[deleted],1541086808.0
mudclub,"What do you mean by ""an app"" in this context?",1541065674.0
MMOAddict,"You need to be much more specific.  ""Home screen"" and ""web system"" are very general terms that can mean a lot of different things. ",1541084540.0
Elite_Italian,https://dev.mysql.com/doc/refman/5.7/en/json-creation-functions.html,1541103103.0
SomeGuyNamedPaul,You're gonna need to be a lot more specific than that.,1541075366.0
hangfromthisone,Probably for the public website and forum,1541080577.0
quietidiot,Boring,1541049411.0
OkieDaddy,"Try moving rating\_category\_importance.User = ':session.User' into the LEFT JOIN criteria. Having a where clause statement on a left joined table essentially forces an inner join, which can continue rolling up, converting all your left joins into inner joins, unless you account for NULLs in your Where statements. ",1541095664.0
,[deleted],1541016351.0
,[deleted],1541016841.0
msiekkinen,Add something like elsatic search to your stack that's meant for text search to return PKs you can then look up. ,1541031099.0
jynus,"> My goal is to find proof that one way is better than the other.

Run `EXPLAIN` on the query and profile the `SHOW SESSION STATUS like 'Hand%'`after each execution to get a scientific answer. I have my own opinion about what is better, but you should run the experiment by yourself and tell us the results :-)",1540992404.0
xroni,These are not specific to MySQL but to the shell you're using. Check the documentation for the full list of shortcuts. E.g. if you use bash type `$ man bash`.,1540968164.0
Irythros,"Maybe?  

If you have control over authentication you can create a new user with an empty database.  Restrict the new users credentials to the new database only.  Then the import will only effect that database and they can't use any commands to change database.  

&#x200B;

If you're using the root user to import then you're fucked.",1540950464.0
SaltineAmerican_1970,"Look for a `USE` command in the file. That changes the database. It could be that you enter the database name on the command line, or the script contains the database name. ",1540951388.0
Laurielounge,Any way to set it up on a separate server first? import it and see what the result looks like?,1540950326.0
tuantutran,You could try to create a user with limited privileges and run the script as that user. Depending on what kind of privileges that script requires,1541020429.0
jericon,"Depending on how you installed and setup MySQL, chances are that the socket file is in a different location from the default. ",1540815652.0
ratnose,This is kind of embarrasing... it was an password error. Shame on me.,1540807390.0
SomeOtherGuySits,looks ok to me. Whats your schema look like?,1540761151.0
sandalshagger,yo man fuck isys224 i'm stuck as well dawg,1540968290.0
JamesB41,"Well, take it piece by piece. First, looks like you're mixing types. AccountBal is presumably not a varchar, or at least I'd hope not. How would you check to see if AccountBal is high enough. Assume that's the only question.

Edit: It would also be useful if you posted the schema of the tables with either DESC <table>; or SHOW CREATE TABLE <table>;",1540754157.0
PuzzledImagination,"Your mysql server authentication uses the new authentication plugin which is sha256\_password, and your php connection MIGHT(cause you didn't provide the code) be using mysql\_native\_password.

Heres the link for a detailed explanation.

[https://mysqlserverteam.com/mysql-8-0-4-new-default-authentication-plugin-caching\_sha2\_password/](https://mysqlserverteam.com/mysql-8-0-4-new-default-authentication-plugin-caching_sha2_password/)",1540686895.0
Bitter_Bridge,"After you connect, are you querying the DB at all? If so, what errors do you receive? Since you're learning PHP and MySQL, I _highly_ advise you to learn PDO instead of MySQLi. ",1540681771.0
areti33,"As you are newbie to MySQL  and might be manual procedures is bit tedious for you. 

Another approach to fix such errors in MySQL Database is : [MySQL Database Recovery Tool](http://www.databasefilerecovery.com/mysql-database-recovery.html)",1540811077.0
tuantutran,You should make it a function. Not a variable. Cheers,1540629811.0
Argyle311,"Try flipping the direction of your slashes in the filepath from ""\"" to ""/"".",1540587588.0
EstimatedPassenger,"I believe the variable is ""local-infile"" (hyphen, not underscore). After changing to local-infile=1, restart mysql.

Also, the use of double backslashes seems questionable. I would try using single ""\""s in the file path but that doesn't sound like what the error is referring to.",1540962083.0
mudclub,"> 1380 columns

wtf are you storing in that table?",1540536563.0
SuperQue,"Try InnoDB with file-per-table. I'm guessing it will have a lot better performance and fewer restrictions.

I highly recommend mydumper/myloader to do a clean import of the data.",1540566263.0
aqbabaq,So it looks like the maximum number of coulmns you can have in a table depends on multiple factors. I suspect that mysql 5.7 changes length of some data types (like date for eample) that needs more space and this reduces maximum columns amount you can create. Take a look at this article https://www.percona.com/blog/2013/04/08/understanding-the-maximum-number-of-columns-in-a-mysql-table/,1540534329.0
aqbabaq,No problem and good luck,1540535299.0
razin_the_furious,Put distinct in found of the clause inside the group_concat,1540514650.0
zombiechimp,"SELECT  b.authorid

FROM  db.answers a

,db.questions b

WHERE  a.answerid = 1

AND  a.questionid = b.questionid

;",1540411738.0
Irythros,"Go to console and do: mysql -u root -h localhost -p

If you can login great.  If not change localhost to 127.0.01 and try again.  If it works then localhost was mapped to something other than 127.0.0.1",1540344971.0
MMOAddict,Don't give up.. I actually miss having problems.  Those were when it got interesting for me. What website are you using to log into the database?  phpmyadmin or just a homebrew site?  If it's homebrew can you post the connection code?,1540354803.0
invisiblerabbit,"I was having a similar issue on 18.10 where I could log in on the console using sudo but could not log in from phpmyadmin.  The solution in [this](https://askubuntu.com/questions/766334/cant-login-as-mysql-user-root-from-normal-user-account-in-ubuntu-16-04) article worked for me.  Specifically:
    ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'NEWPASSWORD';
    FLUSH PRIVILEGES; ",1540397955.0
razin_the_furious,Can you post the structure? How can something be set to a value and not have a history of being set?,1540334479.0
apaethe,"If the current value is in the history table as well, then the sudo code would be something like:

    select h.*
    from history_table h
    join (
      select max(h.id) as id
      from history_table h
      group by h.item_id
    ) as mr -- most recent status
    on h.id = mr.id -- filtering to only most recent
    where h.status != 'canceled' -- filtering to !canceled

Assumes you have a h.id autoincrement column and that you mean you want records where the most recent status is not canceled ( rather than never having had a canceled status for example ).  Index it on your item id for the grouping to be fast.",1540336740.0
SaltineAmerican_1970,"\`npm\` isn't a \`mysql\` command. Without the code and a question in the correct sub, your question is as answerable as orange.",1540343154.0
janripke,"return is a reserved word, try using another field name I should think.",1540327988.0
Laurielounge,"O/S?

Might be an selinux problem",1540317586.0
Laurielounge,"Possibly using /root as your start point. Funny feeling user mysql has to have permissions all the way down, or something. 

Rings a vague bell",1540317924.0
Laurielounge,"Any clues here?

https://mariadb.com/kb/en/library/what-to-do-if-mariadb-doesnt-start/",1540320130.0
msiekkinen,[Pastebin](https://pastebin.com/NHb6mbsh) for anyone trying to make sense of this.  ,1540257633.0
kenlubin,"My vote is: use SQL to put data into the database and pull data out of the database.

Use application code to do all of that processing after you've selected the data from the database.",1540267034.0
SomeGuyNamedPaul,It's like it's complaining about CTE syntax from 8.0 drivers but on a pre-8 server.,1540254071.0
Irythros,"[https://datatables.net/](https://datatables.net/)

That's what you could use for the front end.  The back end you'll need to secure in your language of choice (PHP, Node, Go, C languages, Java etc) so it's resistant to SQL Injection.

Styling it is fairly easy.",1540241777.0
jahayhurst,"If you close the screen first, you should get better distance. An open lid adds wind resistance.",1540226290.0
DataVader,"It’s quite easy but first, what did you try so far?",1540221759.0
r3pr0b8,"> made me wonder if having a database table per restaurant would be better?

no, that would be way, way, way worse",1540197421.0
seriousconsult,"Probably 1.  
How many restaurants?  How frequently does the data change? What is reading the data?",1540206968.0
,[deleted],1540218930.0
hangfromthisone,"A lot of rows usually mean more than 50 million or so. At that point, given what kind of app you are developing, consider horizontal partitioning, for example date added. Then you will need to dvelop a custom midware to interact with db, which will have to check which tables to join, then do the query",1540220299.0
SuperQue,"Depends on what you're doing.

Removing all secondary indexes from the import can help a bunch, then `ALTER TABLE ADD INDEX` later. It will also improve how compact the secondary indexes are.

If you're trying to do a backup/restore, use [mydumper/myloader](https://github.com/maxbube/mydumper). It can do the restore multi-threaded.",1540159942.0
angusmcflurry,"What is the ""regular way""?  MySQL should be able to chew through a 40GB dump file from the command line (it will take a while) but obviously no web interface (like PhpMyAdmin) will handle a 40GB file.",1540160584.0
razin_the_furious,How powerful is the machine doing the restore?,1540164524.0
nikoz84,"Import the schema without data, and Export the data to diferents tables

copy (Select * from users) to '/tmp/users.sql' with format=text ,delimiter '*';

And import into psql

copy users from '/tmp/users.sql' delimiter '*';

You need to restart the sequences 

Alter sequence name_sec restart 808708;",1540175716.0
ryosen,"Sounds like the database server shut down. This commonly happens when the server starts to run low on memory. Rather than crash completely, it will look for a process that uses a lot of memory to shut down instead. MySql is likely using the most memory on the server.

To get back up and running asap, restart MySql. If it happens again, you don't have enough RAM on the server or MySql is misconfigured to use an amount of RAM that is inappropriate for the server.",1540095635.0
Laurielounge,Firewall problem?,1540066668.0
lpmusix,mysqldump,1540017688.0
kastauyra,"mysqldump. If you really want or must give the .ibd file instead, then follow https://dev.mysql.com/doc/refman/5.6/en/tablespace-copying.html",1540025059.0
razin_the_furious,Seperate tables for all the things. Have a basic business table that takes the business information. Have an items table that links back to it. Restrict posting items or number of items on the application logic side,1539996187.0
hangfromthisone,"Have a main table and a meta data table, so you can save whatever you need for each case (manufacturer -> props). You can also have a third table to define what are the valid properties (props with cat), and a fourth table to group the properties into different levels (categories)",1540004540.0
dohako,"Others have commented on the concept of normalisation and application logic for features like restricting access.to the number of recorded by user.

With regards to restrictions on database privileges think of it as simply ensuring that people cannot hack your system. 

There will be config files for backups and application access. The passwords stored in these may become compromised. Don't allow a backup user to write data. Don't allow the application user to change your database schema. ",1540010860.0
jericon,"Personally, what I would do is allow a business to input all of the information and store it all the same. Then change the display based on the membership level. That makes it easier for a business to upgrade without having to add more info. 

I do agree with all of the suggestions for normalizing the data into different tables. ",1540025688.0
PalmettoSpur,"Okay, so people seem to favor normalization/separate tables.  How's this?

>Manufacturers:  
>  
>ID (primary key)  
>  
>Name  
>  
>Address  
>  
>Phone  
>  
>Website  
>  
>Subscription Level  
>  
>\----------------------  
>  
>Manufacturers' Items:  
>  
>ID (foreign key)  
>  
>Item1  
>  
>Item2  
>  
>Item3  
>  
>..  
>  
>\-----------------------  
>  
>Manufacturers' Offers:  
>  
>ID (foreign key)  
>  
>Offer1  
>  
>Offer2  
>  
>Offer3  
>  
>\------------------------  
>  
>Manufacturers' Images:  
>  
>ID (foreign key)  
>  
>Pic1  
>  
>Pic2  
>  
>Pic3

Does that look like a smart design?",1540034950.0
finroller,You probably want r/javascript or some such. This really isn't very much mysql related.,1539848220.0
unbihexium,"You need to create a relationship in your model. That establishes the fk as well as enables the API endpoint for you.

Start here: https://loopback.io/doc/en/lb3/Creating-model-relations.html",1539938204.0
TheFormerAstronomer,I tested your code on 8.0 and it works fine for me. What OS/MySQL version are you working on?,1539847446.0
berry_lover96,&#x200B;,1539827398.0
msiekkinen,"I think the query

    Show tables;


Will still work",1539815624.0
jeffwrule,"The mysql schema will have a table that lists the tables in the database.   It is a bit slow, but it will be there.",1539820091.0
assasinine,">  I don't want to have to run this query 12 times every week

Why is this important?",1539805658.0
aheinzm,"This executed for me in 1ms and returns the results for one team.  You could union this thing together 12 times and just change the teamName each instance is looking for.
    
    select *
      /* get highest scoring QB for team 1 */
      from (select *
    		  from `football`
    		 where `teamName` = 1
    		   and `playerPosition` = 'QB'
    		order by `playerScore` desc,
    				`playerName`
    		limit 1) qb
    union
    select *
      /* get two highest scoring RB for team 1 */
      from (select *
    		  from `football`
    		 where `teamName` = 1
    		   and `playerPosition` = 'RB'
    		order by `playerScore` desc,
    				`playerName`
    		limit 2) rb
    union
    select *
      /* get two highest scoring WR for team 1 */
      from (select *
    		  from `football`
    		 where `teamName` = 1
    		   and `playerPosition` = 'WR'
    		order by `playerScore` desc,
    				`playerName`
    		limit 2) wr
    union
    select *
      /* get highest scoring TE for team 1 */
      from (select *
    		  from `football`
    		 where `teamName` = 1
    		   and `playerPosition` = 'TE'
    		order by `playerScore` desc,
    				`playerName`
    		limit 1) te
    union
    select *
      /* get highest scoring RB, WR, TE for team 1 */
      /* exclude the top 2 RB for team 1 */
      /* exclude the top 2 WR for team 1 */
      /* exclude the top TE for team 1 */
      from (select *
    		  from `football`
    		 where `playerPosition` in ('RB', 'WR', 'TE')
    		   and `playerName` not in (select `playerName`
    								  from (select *
    										  from `football`
    										 where `teamName` = 1
    										   and `playerPosition` = 'RB'
    										order by `playerScore` desc,
    												`playerName`
    										limit 2) rb)
    		   and `playerName` not in (select `playerName`
    								  from (select *
    										  from `football`
    										 where `teamName` = 1
    										   and `playerPosition` = 'WR'
    										order by `playerScore` desc,
    												`playerName`
    										limit 2) wr)
    		   and `playerName` not in (select `playerName`
    								  from (select *
    										  from `football`
    										 where `teamName` = 1
    										   and `playerPosition` = 'TE'
    										order by `playerScore` desc,
    												`playerName`
    										limit 1) te)
    		order by `playerScore` desc,
    		       `playerName`
    		limit 1) flex
    union
    select *
      /* get highest scoring K for team 1*/
      from (select *
    		  from `football`
    		 where `teamName` = 1
    		   and `playerPosition` = 'K'
    		order by `playerScore` desc,
    				`playerName`
    		limit 1) k
    union
    select *
      /* get highest scoring DEF for team 1*/
      from (select *
    		  from `football`
    		 where `teamName` = 1
    		   and `playerPosition` = 'DEF'
    		order by `playerScore` desc,
    				`playerName`
    		limit 1) te    ",1540001675.0
riesenarethebest,"Found it: fully implemented in 5.7.5 and 8, but nowhere else",1539790608.0
Murrawhip,"Upgrade your PHP to at least 7.0.19, 7.1.5, or 7.2.0. But based on your post history, you might be stuck on 5.6? In the short term, you could force mysql to use utf8 instead of utf8mb4. While your tables might be utf8, the server itself has its own charset. Add something like the following to your my.cnf and restart the mysql daemon:  

    [mysqld]
    collation-server = utf8_unicode_ci
    character-set-server = utf8",1539714899.0
areti33,"Hi 

To know the causes of such error, you can read this: [https://medium.com/@as0917041/how-to-fix-mysql-issue-error-establishing-a-database-connection-ea7cc6b7e915](https://medium.com/@as0917041/how-to-fix-mysql-issue-error-establishing-a-database-connection-ea7cc6b7e915)

This will really help you.",1539754380.0
Bitter_Bridge,"I don't know how old you are, but it's time to grow up. 


First of all, ""Are"" <> ""Our."" 


Second, the responsibility to learn is on you, regardless of how well your teacher teaches. Throughout your life, you will have a litany of teachers and, later, bosses who communicate poorly, challenge you, frustrate you, and generally do things to a different standard than the one you'd prefer. The more quickly you can learn that, regardless of any of that, you're still responsible for your own life, your own understanding, and your own work, the better off you'll be. 


Third, no one on here is going to do your homework for you. If you want to learn, how about RTFM? If you don't know what RTFM means, then google it and then google everything else you need from there. I'll even help you get started -> https://dev.mysql.com/doc/refman/8.0/en/load-data.html


Lastly, if your project is to create a website that allows a user to upload an external spreadsheet and then have that spreadsheet imported directly into the database, there is more supporting technology needed than just MySQL. I suggest you stop making excuses and get busy. You have an opportunity right now to decide what kind of person you'll be. You can be the kind of person who makes excuses and blames other people, or you can be the kind of person who takes initiative and works hard and excels. 


Good luck. ",1539707707.0
razin_the_furious,"As I understand it, and I'd like to be corrected if I'm wrong, but Triggers cannot affect the same table that they are triggered by.

The risk being if you update the table with a field of value getOrders, and that causes a trigger to update the same field, that would trigger an infinite loop",1539697210.0
NotTooDeep,"SELECT table_schema ""Data Base Name"", 
sum( data_length + index_length ) / 1024 / 
1024 ""Data Base Size in MB"", 
sum( data_free )/ 1024 / 1024 ""Free Space in MB"" 
FROM information_schema.TABLES 
GROUP BY table_schema ; ",1539970024.0
Tickthokk,"MySQL Workbench isn't \_that\_ bad (free).

Recently I've been using Sequel Pro (free) and I like it.",1539655638.0
bla4free,I'm a big fan of SQLyog.,1539650073.0
SomeGuyNamedPaul,DBeaver is good if you have multiple engines to contend with.,1539679669.0
PristineTransition,"If on macOS, Querious is awesome. If on Windows, Navicat. Free ones I have no idea.",1539655759.0
papassinqueso," HeidiSQL, free",1539669946.0
mobsterer,"If you like Visual Studio like UI you can use the free version of Devarts dbforge. Or pay for the full version.
",1539672309.0
DJDarkViper,"Navicat, hands down. But it's not free. 
Best free? I tend to opt for Heidi or DBeaver",1539672341.0
Nk4512,"Love sequal pro over workbench personally
",1539677828.0
umkhunto,GNOME,1539680619.0
unr3al011,i like dbForge Studio,1539684922.0
nick13610,"DataGrip by JetBrains is really good, although it's not cheap, but luckily my company pays.

If you're just looking for free software, I find Workbench and HeidiSQL work well.",1539685050.0
etrnloptimist,"First thing I do whenever I create a table is use id int as my primary key. Second thing I do is think about what the UNIQUE constraint should be. That will define what it means to be an individual piece of data and ensure we don't have duplicates by accident.

The int primary key is surprisingly useful and versatile. I guarantee your most used ad-hoc query will end with ""order by id desc limit 100""

int primary keys are also easy to link against. Any other table that wants to reference your data will just include a ts_id column and they're done.

Your biggest worry may be running out of ints. So use unsigned bigint and never worry again.",1539635397.0
darthmikeyd,"Thanks for the replies. I got the upgrade to run, and it all looks good.",1539633098.0
ElMatze79,"You should upgrade to 5.7 fist, because the changes it complains about were done in 5.7
After that move on to 8.0 and don't forget to run mysql_upgrade after every upgrade.",1539623185.0
areti33,Try to fix it by using [MySQL Database File Recovery](http://www.databasefilerecovery.com/mysql-database-recovery.html) software.,1541222774.0
angusmcflurry,https://dev.mysql.com/doc/refman/8.0/en/innodb-locking-reads.html,1539455297.0
jeffwrule,"Begin 

Select xxx where xxx for update 

Update xxx where xxx

Commit

You must do this w/o disconnecting and make sure auto commit is turned off.  Auto commit is in by default for MySQL command line client and often many query tools.   It is typically off when using a connection from a programming language like Perl or python. 

I don’t remember the syntax for turning off auto commit but it is part of the set syntax 

The select for update will lock the rows you are selecting from being selected until the next commit is issued in the same session that did the update.  

If both session use select for update they will block each other from selecting the same row(a). 

This page should be useful.   

https://dev.mysql.com/doc/refman/8.0/en/innodb-locking-reads.html
",1539470180.0
OkieDaddy,"What are you wanting to accomplish exactly with the two extractions? Are you trying to multi thread it, so you can move more data at once? Be careful with locking, otherwise you could kill that ability. Is there another predicate you could partition the dataset by, so that thread one always looks at partition 1, thread two looks at partition 2, etc?",1539475727.0
91ws6ta,"What column is the 0 supposed to be in for your results? I'm guessing it's supposed to be in your second(count) column.

My guess is you're left joining the TimeDin table on the RentalFact table, meaning there's an ID in RentalFact with 0 sales that for some reason is not in the TimeDim table.

&#x200B;

Try changing the joins around, with the WHERE clause having TimeDim, and the left join having RentalFact. If what I'm thinking is right, you would have the all months listed and either the count or a NULL if there were no sales.",1539411300.0
r3pr0b8,"use a LEFT OUTER JOIN, along with COALESCE",1539383862.0
nonfree,"~~Alright, i got a little further (i think), but now it doesn't display the uid for the undefined usernames. Any help would be greatly appreciated. http://sqlfiddle.com/#!9/d553c7/26~~

Solved: http://sqlfiddle.com/#!9/6cb48a/3",1539389606.0
BRINGtheCANNOLI,"Great job - i have installed and will be using. 

Any plans to support SSH tunnels to hosts? I have several DBs that are only accessible via tunnels.",1539369134.0
Irythros,"Is this a MySQL question?  

Pretty much anything works.  My default go-to stack is CentOS 7, Percona 5.6.  

If it's low throughput: nginx + php  

If it's high throughput: Go + Redis",1539344114.0
sensualcurl,What language do you want to write your restful service in or are you looking for an out the box solution? ,1539344486.0
Laurielounge,"Hi there 

Python probly. Or php.",1539369569.0
jynus,"I found Flask (Python framework) extremely easy to use and with great performance in the past: http://flask.pocoo.org

I implemented a proof of work recently of a key value system with a rest interface answering JSON from a MySQL database in a few lines of code:

https://github.com/jynus/pysessions/blob/master/app.py

Warning! Don't use that code directly, it is bad code just for testing purposes in a few minutes I did to learn the framework, not intended for production usage.",1539423391.0
TheFormerAstronomer,"Select the rows you want to paste (or all of them)

Right click/option click

Select 'copy rows (tab separated)'

Paste to Excel.",1539333492.0
Bitter_Bridge,Is there something missing between RAND() and INTERVAL()? I'm not familiar with that syntax and it won't execute. ,1539267802.0
TheFormerAstronomer,"If you take out each component of the expression (e.g. SELECT INTERVAL(34,2, 17, 21, 56, 69)) and work out what each part evaluates to, you can work out what the formula would be if it was just simple algebra.",1539268427.0
robyk,Reddit formatting ate a * behind INTERVAL should be good now,1539268554.0
Irythros,"No.  

One table for posts, one table for comments.  Have a column in comments which is the ""post\_id"".",1539241440.0
davvblack,"In general, you should avoid dynamically creating tables.  It is rarely better than some column on a tall table to discriminate the data.",1539266787.0
mobsterer,"This might not answer your question directly: I would advise to not do that in MySQL, but in code.",1539215029.0
Irythros,"If you can, never use root.  If you can, always use the most limited account you can use.",1539185996.0
msiekkinen,"This isn't even related to backup and restore, but more widely always run as least privileged possible.

Doy you have one specific part of an automated script that needs different privs?  You can setup your sudoers file for this account to NOPASSWD sudo specific commands.",1539203268.0
Irythros,"They probably use a low latency cache which then sends to a message queue that fans out to all the other regions.  Since it's not a huge issue if they're still unbanned a few seconds later it's also likely they follow a ""eventually consistent"" methodology.

&#x200B;

There's also no guarantee they use MySQL either.  I can't find any Steam/Valve devblog.

If you want to find performance related stuff you can check Cloudflare (for general) : [https://blog.cloudflare.com/tag/performance/](https://blog.cloudflare.com/tag/performance/)

Or specific for MySQL: [http://highscalability.com/](http://highscalability.com/) and [https://www.percona.com/blog/](https://www.percona.com/blog/)",1539142779.0
jynus,"@crypto_mind Your question is good, but it is not really a MySQL question, but rather, an architecture question- MySQL can be part of the solution, of course, but the business logic is rarely implemented on MySQL, and just being used as a building brick of a larger system. In fact, on those sizes, MySQL (or any other transactional DB) is rarely the only ""state"" system, so let me tell you generic solutions I have seen in the field, to later tell you how we are doing it at our specific Multi-DC setup.

It is true that communication within a single datacenter tends to be mostly synchronous, and the one between datacenters more asyncrhonous- this is due to both speed (nothing can beat the speed of light), bandwidth and reliability.

The first misconception is that you think that Steam may have a huge amount of servers- obviously I don't know, but my gut tells me that they need to host a large amount of static files from the developers, which is easily cachable- so they will only need a proper (probably subcontracted) CDN -content delivery network-, which can be fully async. From my own perspective, their main issue is session and authentication handling, purchases and user metadata (library, communications, ...). The immediate ""easy"" but problematic solution would be to just store that on a single location (with within DC redundancy), so no sync is needed between datacenters. Another solution, if users are properly sharded by region, is to authenticate with your closest datacenter, replicate to the others. There are also ways to ""stick"" you to a datacenter by detecting your session, so I can see that working for many of the operations you do with steam (purchases, chat, etc.).

The solution we are planning ourselves to have multiple datacenters, is to have only 1 is the primary one (where POSTs are redirected), and make asynchronous reads and writes. When a critical change happens (user login, important write that could conflict, etc.) the application waits to observe consistency between all datacenters for as long as otherwise the other datacenter would be considered dead. If it takes longer than that, the dc is considered dead and, from the other dc's perspectictice, they go read only because there could be a split brain scenario.

Note I did not mention MySQL a lot, because while we indeed use MySQL replication, metadata is only one part of the system; things like logs, sessions, cache, static files, etc. have to have their own way of replicating changes, with different methods being used. They also have different levels of consistency- a database edit may have higher levels of constrain, but if you upload a file on one browser tab, and then on another browser you are using a vpn to connect to the same application and get a temporary 404, we may not support that. Your specific needs will inform your decision- for example, we are primarily a read only shop, so taxing writes in exchange for consistency is an acceptable solution, but it may not fit other companies. However, thing that 90% of the actions you will do in Steam will be reads (read info, store pages, etc.) while only certain critical ones will be writes (a purchase). Sharding is an art form by itself, and more between datacenters :-)",1539159316.0
unbalancedmindx,I'd like to try to help answer your question but using steam auth as an example is kind of confusing since it would largely be handled at the software level and not really a db scaling type problem.,1539143231.0
SomeGuyNamedPaul,"There's lots of ways of doing this, and MySQL replication might not be one of them.  Heck, they could be doing it by pushing a file on S3 into a bucket of banned people.  They could be doing Cassandra or DynamoDB.  It could be CockroachDB.  It could be a mountain of slave replicas or a master Galera ring with slave rings.

My guess is it's something riding on whatever CDN they're using... which it looks like they use Akamai.

Anyway, there's a lot of ways to skin this cat and I kinda doubt there doing it strictly at the database layer.",1539175082.0
DataVader,"1. Having a server just for authentication and nothing else is a thing, so if you change your password, the changes are not applied on the European server, so they don’t need to be replicated to another.

2. You can set up a replication with multiple masters, set up a cluster or split a table into multiple partitions, which are located at different machines.

3. You can set up a controller, which handles all database requests, broadcasts them to a replication network, keeps track which database finished what request and uses that to avoid redirecting read-requests to a node which hasn’t finished writing that exact data.

4. Most giant systems like Steam are caching much.

If you want to learn about this stuff, you can start by downloading Docker or something like that, run at least three databases (should be identical to each other. I recommend MySQL 8) and set up a replication. Maybe try out something like ProxySQL to manage your replication set up, check out Apache Spark and similar software.

If you are planing to create giant databases, you should consider using MariaDB to have the database engine “Spider”, which allows you to set foreign keys on tables of other databases and servers. That way you can do something like keeping everything game related in one database, everything related to players in another database and still enforce consistency.

Personally I am considering MySQL superior because with CTEs, window functions and “JSON_TABLE()” you can do all kind of crazy stuff, which sometimes is not possible with — for example MariaDB. Every database type has its pros and cons, so checking out PostgreSQL, MongoDB,... won’t hurt ;)",1539185560.0
jynus,"Check the manual page for https://dev.mysql.com/doc/refman/8.0/en/mysqlcheck.html It is mostly an outdated tool, as it made more sense in the times of MyISAM. Not that it does nothing, but you may want to run the actual SQL commands done for InnoDB (analyze, a force of rebuild).

If you just upgraded mysql, you should be running `mysql_upgrade` instead, which will do the safer operations on its own automatically, and tell you if there is something else that needs rebuilding manually on its output.",1539160539.0
mudclub,"You cannot tune that.  Fix your architecture.
",1539121971.0
Irythros,"1.2M tables is absurd.  With such poor life choices already made, you tune it by paying a professional an similarly absurd amount of money to come in and tune it to the best of their ability.",1539128737.0
etrnloptimist,"A good architecture will have somewhere along the order of 100 tables.

So I would suggest fanning out your architecture to 12,000 servers.",1539124670.0
apaethe,Why?,1539132937.0
msiekkinen,"Don't do any of that shit in mysql land.   Definitely don't do any kind of ""custom search"", as in trying to roll your own.   Add elasticsearch to your stack.  Use that for what it's built for, searching.   ",1539113101.0
Bitter_Bridge,"How many solutions do you have to serve up? For each solution, you could have keywords, and then just do an RLIKE with each word entered (ignoring words like a,the,of,etc.) against the keywords field. 

Id | solution | keywords 
1 | Open an Account... | open,account,checking",1539113993.0
Bitter_Bridge,"Let’s say you have three articles for unlocking a workstation, and you want to return the most recent one based on what the user types in the input field. What does the user need to enter to retrieve those results. Is “unlock” enough or do they need to type “unlock a workstation”? What if they just type “workstation” and then decide to add “unlock” and so you end up with “workstation unlock”?

If you want to do this kind of search, you’re going to need to break apart the user input, ignore the words you don’t need, look for common misspellings, and then do a regexp for each word against your keywords. That way if both workstation and unlock are keywords, the search results get narrower when the user adds the second word. 

Please note, this is just one possible approach. But, it could be done. ",1539118007.0
mischiefunmanagable,"solr

&#x200B;",1539118467.0
razin_the_furious,"Did you try running ""mysql_upgrade"" after upgrading to 8.0?",1539094731.0
AllenJB83,"It sounds like you've probably skipped some steps in the upgrade process (see [the MySQL Manual: Upgrading MySQL](https://dev.mysql.com/doc/refman/8.0/en/upgrading.html) or your distros documentation - specifically running [mysql_upgrade](https://dev.mysql.com/doc/refman/8.0/en/mysql-upgrade.html)

You should also read through the [Changes](https://dev.mysql.com/doc/refman/8.0/en/upgrading-from-previous-series.html) manual section - there have been significant changes in MySQL 8.0 you should be aware of",1539095005.0
jynus,"> Storage engine 'MyISAM' does not support system tables. [mysql.user]

I think it was on 5.7 or 8.0 that the user table was migrated into InnoDB format- it is complaining it is missing. Either you didn't run `mysql_upgrade`, or you skipped some run that converted the tables- You need to run it even if you did a logical dump as otherwise they will be created in the old, unsupported format (or alternatively, export the grants logically and import them, e.g. with `pt-show-grants`).",1539094843.0
darthmikeyd,"Thanks for the replies. I didn't actually upgrade. We created a new virtual server and I installed MySql 8.0 and then ran the MySQL Workbench Migration Wizard to copy the data from the old server.

I tried running mysql_upgrade from the shell, but it didn't seem to do anything.",1539097039.0
BorisCJ,"In one cmd window ... 

mysqld --console --skip-grant-tables --skip-external-locking 

In another ... 

mysqlcheck --repair mysql user 

If that fails, you may have to restore the mysql db from a backup. 
",1539156901.0
schoolkillsme,"Hi, I am trying to figure out a query where I can separate the values of 'Y' and 'N' into two different columns and count the number of values for both columns based on a specific id. Any clues on how I may go about achieving that?

The best I've managed to do is to have two queries each for values of 'Y' and 'N' separately but I was wondering if I could join them both into a single query/view?

&#x200B;

\`""SELECT exerciseId, count(frustrated) Frustrated from selfreportfrustration where frustrated = 'Y' group by exerciseId;""\`  


\`""SELECT exerciseId, count(frustrated) NotFrustrated from selfreportfrustration where frustrated = 'N' group by exerciseId;""\`  


These are the two queries that I have. Thank you for your time!",1539074885.0
r3pr0b8,"    SELECT COUNT(CASE WHEN frustrated = 'Y' THEN 'humpty' ELSE NULL END) Frustrated 
         , COUNT(CASE WHEN frustrated = 'N' THEN 'dumpty' ELSE NULL END) NotFrustrated  
      FROM selfreportfrustration 
     WHERE exerciseId = 23        /* based on a specific id */",1540496616.0
gazer3z,"MySQL is the database, where you store user's data (username, password, email, etc.).

PHP is connecting with database, verifying user input, providing data to client and then you display website with UI with HTML, CSS and JavaScript.",1539034093.0
Irythros,"HTML cannot access MySQL.  

You need some actual programming language to do that.  PHP, Python, Go, C, C#, C++, Javascript (using NodeJS)",1539053180.0
analytics_science,"If you decide to learn/use python look at this stack overflow question for a nice guide with ready-made code:
- https://stackoverflow.com/questions/372885/how-do-i-connect-to-a-mysql-database-in-python#622308",1539335875.0
finroller,Maybe there's another user@host mask that blocks it?,1539022159.0
mcstafford,"If your have another way to connect, try running:

show grants for root",1539056842.0
SuperQue,"A couple of thoughts

* You may have slow DNS resolution on the client.
* Is this an encrypted connection? (I sure hope it is) Some of it maybe taken up by TLS round trips.
* This doesn't seem like a useful performance test, most applications use persistent connections.

tcpdump + wireshark should help with the packet timings.",1538981386.0
SomeGuyNamedPaul,"Try where in instead of those or statements, assuming you have the appropriate indexing.",1538999877.0
mobsterer,are the fields in the where clause all indexed? I doubt it. If they are not the query will do a full table scan every time.,1538981993.0
kevank,"Can you provide the indexes you have created?

You at a minimum need one on attachments.id, to_attatchments.attatchments_id and object_id.

",1539000868.0
hangfromthisone,"Kinda hurts my eyes. Read about table aliases, and index. Index that table like if your life depended on it",1539005935.0
Laurielounge,"Firewall? Any activity in mysql-error.log?

Sorry, just re-read. 

My mysqld looks like (no bind):

`[mysqld]`   
 

`# GENERAL #`   
`user                           = mysql`   
`default-storage-engine         = InnoDB`   
`socket                         = /var/lib/mysql/mysql.sock`   
`pid-file                       = /var/lib/mysql/mysql.pid`   
`sql-mode                       = """"`   
 

`# MyISAM #`   
`key-buffer-size                = 32M`   
`myisam-recover-options         = FORCE,BACKUP`   
 

`# SAFETY #`   
`max-allowed-packet             = 16M`   
`max-connect-errors             = 1000000`   
`skip-name-resolve`   
 

`# DATA STORAGE #`   
`datadir                        = /var/lib/mysql/`   
 

`# BINARY LOGGING #`   
`log-bin                        = /var/lib/mysql/mysql-bin`   
`expire-logs-days               = 14`   
`sync-binlog                    = 1`  
 

`# CACHES AND LIMITS #`   
`tmp-table-size                 = 32M`   
`max-heap-table-size            = 32M`   
`query-cache-type               = 0`   
`query-cache-size               = 0`   
`max-connections                = 500`   
`thread-cache-size              = 50`   
`open-files-limit               = 65535`   
`table-definition-cache         = 4096`   
`table-open-cache               = 10240`   
 

`# INNODB #`   
`# innodb_dedicated_server        = 1`   
`innodb-flush-method            = O_DIRECT`   
`innodb-log-files-in-group      = 2`   
`innodb-log-file-size           = 256M`   
`innodb-flush-log-at-trx-commit = 1`   
`innodb-file-per-table          = 1`   
`innodb-buffer-pool-size        = 6G`   
 

`# LOGGING #`   
`log-error                      = /var/lib/mysql/mysql-error.log`   
`log-queries-not-using-indexes  = 1`   
`slow-query-log                 = 1`   
`slow-query-log-file            = /var/lib/mysql/mysql-slow.log`  


&#x200B;",1538951289.0
Irythros,"Why are you doing it this way?  This just sets you up for unnecessary access.  

Use an SSH tunnel and create a local account.",1538957311.0
UncleNorman,"I just set up mysql under slackware. I had to change the skip-networking in my rc.d  (init script) to make remote connections work.

The my.cnf had skip-networking commented out. I kept listing the variables and they all showed skip-networking as true until I realized it was being set on the command line.",1538960888.0
jericon,"1064 is a basic syntax error. 

What is the query you are trying to run and what is the full error message?",1538908741.0
areti33,Yo can try a [MySQL Database Recovery Too](http://www.databasefilerecovery.com/mysql-database-recovery.html)l to resolve all types of issues which occurs in MySQL database.,1538989906.0
finroller,"How real time the data needs to be? Also what do you mean by wanting a better solution? You need to a) get the data out of the database and b) generate HTML from it. You can do this with pretty much any programming language, but not without any parsing and generation.",1538845587.0
bluegrassbiker,"Phpmysqladmin seems like the easiest way to do what you are saying, but not the best way. 

There is nothing built into mysql that would allow itself to be exposed to the web, it is database server software only.  ",1538854022.0
ge0n1,"Cron that runs the query and dumps to a file and then use python http server to host.

[https://blog.anvileight.com/posts/simple-python-http-server/](https://blog.anvileight.com/posts/simple-python-http-server/)

&#x200B;

Hackiest way I can imagine.

&#x200B;",1538876325.0
vsobchuk,"You can use any ready made  front-end for DB like Adminer which is elementary to setup and use or
PhpMyAdmin which is not so elementary ;)
Any php framework that allows to generate CRUD for DB table (yii, Symfony)",1538889769.0
picturepages,"Yes.  So slow, that I went back to Version 11.  Never figured out why.",1538804675.0
NCFlying,You could have stopped at “Is Navicat Slow?”,1538836628.0
ericpp,Here's the direct link to the 5.7 installer: https://dev.mysql.com/get/Downloads/MySQLInstaller/mysql-installer-community-5.7.23.0.msi,1538785919.0
llenders97,Somewhere on the installer screen you can choose to show different versions,1538777747.0
tiben_,which OS ?,1538778947.0
dsn0wman,Just grab [Percona MySQL](https://www.percona.com/software/mysql-database/percona-server),1538845555.0
TheFormerAstronomer,"If you go to the main downloads page:

[https://dev.mysql.com/downloads/mysql/](https://dev.mysql.com/downloads/mysql/)

and then click on the link to the right that says 'Looking for previous GA versions' it should give you the option to download a noinstall ZIP archive for 5.7.23 for Windows.

Then follow the instructions here:

[https://dev.mysql.com/doc/refman/5.7/en/windows-install-archive.html](https://dev.mysql.com/doc/refman/5.7/en/windows-install-archive.html)

to install

Disclaimer: I've never done it this way on Windows myself, so caveat emptor and all that.

\[ETA: version number\]",1539003215.0
llameadrpc1,"Makes it slower, due to the compression/decompression overhead. ",1538773904.0
r3pr0b8,"> I keep getting an error around A.'column 1'

every place where you're using single quotes like this

     'column 1'

you should be using **backticks** like this

    `column 1`",1538751073.0
hambalamba,'column 1' is a string. I dont think you can select like this...,1538746716.0
jericon,"What is the specific error you are receiving?

Also, for future reference, don’t use spaces in database/table/column names. Use an underscore ",1538751998.0
mudclub,Step 1: determine where your bottlenecks are.  It's completely pointless to do anything else until you figure that out.,1538728141.0
jericon,"The number one way to speed queries up is proper indexing. 

The only way we can truly help with this is if you provide examples of the queries you are running and the schema if the database itself. ",1538733733.0
xcjs,Are they too large to merge and upload/import that way?,1538666185.0
pinkdinosauronbowtie,"If you can get CLI access with a bash shell, get back to me. Have several ways.

Second way would be (ssh tunnel) connect via MySQL workbench.

If you can do either of those I can help.",1538666240.0
Mr_Richard_Harrow,Ya need cmd line access and you can script out something !,1538689283.0
NotTooDeep,"OK. You have a memory constraint and you have files to load into memory that are already larger than that constraint. Sorry, you've already failed. Compressing a SQL file makes it unreadable by the mysql client. 

But don't trust internet strangers; test it. Get on the command line in the directory of the files. Issue the command:

    mysql --login-path=my-login-path < BigAssFile-biggerThan2MB.sql

If that doesn't break, then your statement about the constraint is inaccurate. If it breaks, you have to increase memory.

For all the files that will load, simply ls -l the *.sql files in the directory > run_files.sh

Then edit run_files.sh, replacing the start of every line with ""mysql --login-path=my-login-path < "" and the end of every line with "";"". 

Every SQL file should have either ""use *database_name*"" or fully qualified table names in the insert statements.",1538695196.0
beermad,"You don't *execute* SQL files. They're data rather than executables.

Assuming you *do* have command-line access to the files, even though you don't have it for the server, do:

`cat *sql > /tmp/combined.sql`

(Assuming they're all in the same directory)

Then upload the resulting single file through whatever means you want.",1538666339.0
r0oot,"If it is MySql 5.7 then find your root password at

sudo grep ""temporary password"" /var/log/mysqld.log

&#x200B;",1538664292.0
BorisCJ,"Also note that in mysql, localhost is treated differently from 127.0.0.1 so you may have success using 'root'@'127.0.0.1'",1538708754.0
ElMatze79,try `sudo mysql`,1538663662.0
nthdesign,"A super easy way to do this is to run them both in Docker, exposing port 3306 for one, and 3307 for the other. ",1538654353.0
Abazagorath,"Totally doable, I do this at work. I'll take a deeper look tomorrow to see if I can give an idea of the setup",1538644464.0
jynus,"You need to setup certain configuration different on each instance: socket, datadir, port, tmpdir, etc.

For example, this is how we do it at work (it requires some extra config for each instance):

https://phabricator.wikimedia.org/source/operations-puppet/browse/production/modules/mariadb/templates/instance.cnf.erb

(all code, including puppet, is free to reuse)

You only have to include that on your common configuration file, e.g by doing:

> !includedir /etc/mysql/mysqld.conf.d/

And then start the server with:

> mysqld --defaults-group-suffix=@<instance name>

(we handle that with systemd on Linux, not sure how mac does it)",1538651906.0
NoxiousNick,"I don't know about doing it in OSX, but I know the basics of what to look for on a typical linux server. With the standard mysql instance, you have things like:

* data directory (/var/lib/mysql)
* config file (/etc/my.cnf)
* service script (/etc/init.d/mysqld, or systemd)

So what you would have to look into is creating another data directory with the proper permissions, another configuration file, and another startup script. Your end result might be something like starting /etc/init.d/mysqld2 which references the /etc/my2.cnf file and points to the /var/lib/mysql2 directory.

If you're using systemd you might have an easier time because you can add the specifics of the 2nd instance in the default my.cnf file and can reference them by starting up with ""systemctl start mysql@newinstance"".",1538661654.0
xXxLinuxUserxXx,"Checkout [https://dev.mysql.com/doc/refman/5.7/en/mysqld-multi.html](https://dev.mysql.com/doc/refman/5.7/en/mysqld-multi.html)

I wouldn't recommend it too - why not just put all databases in the same instance?",1538669922.0
finroller,Seriously? There's a kazillion hits on google that give you step by step instructions.,1538645100.0
Nk4512,"Depends on what your use case is. Do you want to store it for later on? What framework / language are you using etc. 

You can do whatever you want with sql databases. Or with php for example you can store to a local file, variable, etc. 

Depending on what you want to do and how many tables you want, you would setup multiple tables for different data sets / architecture.",1538623181.0
finroller,"The wording of this questions hints that you need to get familiar with some basics first. This whole data storage thing is a pretty complex thing.

[https://www.webucator.com/tutorial/learn-sql/relational-database-basics.cfm](https://www.webucator.com/tutorial/learn-sql/relational-database-basics.cfm) might help with sql, mysql and such are a popular choice.  
Mongodb is what we call a nosql database: [https://docs.mongodb.com/manual/introduction/](https://docs.mongodb.com/manual/introduction/)

&#x200B;

After you got your front thingie and your database ready comes the interesting part of gluing these together and as Nk4521 points out, we don't know about what sort of a system you are building there. It's possible to use a database with pretty much any programming language ever invented :)  


Then again, maybe you don't need a database in which case you could maybe leverage localstorage: [https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage](https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage)

&#x200B;

And yet again, maybe you don't really even want to store this data for a long time, but just want to do some calculations and javascript variables are enough for you so you probably want to google some javascript tutorials too.  
",1538643304.0
cruyff8,"SELECT a.* FROM mytable as a JOIN (SELECT col1 as username, col2 as email,  COUNT(*) FROM mytable as users  GROUP BY username, email HAVING count(*) > 1 ) b ON a.username = b.username AND a.email = b.email ORDER BY a.email

Should sort you.. If not, leave a comment.",1538586033.0
DataVader,"If you are running MySQL 8, use a window function. Check out window function frames and compare your rows via “LAG()”.",1539096961.0
SomeGuyNamedPaul,My guess is you want an index on object_id but without table structures I'm in the dark.  These are InnoDB tables with a primary key on... id?,1538585595.0
jericon,Can you show us your current table structure for the tables involved?  That would help significantly. ,1538592231.0
mischiefunmanagable,"index attachment\_id  


compound on object\_type, object\_id  


index created\_at  


but maybe lets take a step back, what is your idea of slow? how many records? what does an EXPLAIN show? what are the table schemas? what kind of hardware is this? what storage engine?",1538604485.0
jericon,"Your post has been removed.

Reason: Self Promotion

Your post history shows that you have a propensity to post from the same domain or topic often to a wide breadth of somewhat related subreddits without contributing to the communities otherwise.  Please see [Reddit's Self Promotion Policy](https://www.reddit.com/wiki/selfpromotion).",1538767511.0
SaltineAmerican_1970,"It looks like quotes are part of your data. See if this works: `SELECT * FROM video_categories WHERE cat_name='""Comedy""';`

If it does, you're probably entering your data incorrectly.
Edit: stupid ""Fancy Pants Editor.""",1538864167.0
Zaphod_B,"From my understanding you just remove all the MySQL enterprise tools and binaries.  So use your package manager, remove the server, client, libs, and all other dependencies.  Then install community + dependencies via a package manager, and then run the mysql_upgrade script, this should flip all your tables in your database back to the community version.   ",1538530259.0
eredditan,Thank you for the responses.,1538674131.0
roguelazer,"The MySQL JSON type is... okay. It's not quite as naive as some other datastores' JSON types (there is an index of field offsets in every document, for example). You can build indexes but then you have to make generated columns for every field. I wouldn't use JSON for something I intended to query very often.

In this case, you could extract this field with 

    SELECT JSON_EXTRACT(field, '$.Size') FROM table

Or (if you're on a new enough MySQL), the shorthand

    SELECT field->'$.Size' FROM table

Note the case-sensitivity.",1538517313.0
jericon,Is the column you are deleting by indexed?,1538493348.0
doom-o-matic,what's the output of `explain ....`? i guess you're missing an index in the foreign table.,1538497290.0
zardwiz,"Do you have a mysql client on the host machine?

Is 3306 open in your firewall, if you're trying to connect remotely? Consider where to allow traffic from, of course.

Have you configured the mysql instance in the docker container to respond to connections from other than localhost? If memory serves, the default behavior on a non-docker mysql install is to deny remote connections.

[This](https://docs.docker.com/samples/library/mysql/#-via-docker-stack-deploy-or-docker-compose) has several possible answers for you.  See ""Container shell access and viewing MySQL logs"" along with ""Connect to MySQL from the MySQL command line client""",1538446438.0
Infyx,"Do you see the container running?

> docker ps

Can you log into the container if you log in to the shell for the container?

> docker exec -it <container id> /bin/bash

Then type:

> mysql -uroot -p<password>

Does this put you at the mysql prompt?
Chances are the base image for mysql is not accessible remotely and you would have to set that up when you deploy the container or with a script run against the container.",1538448228.0
Bitter_Bridge,"Switch to using Y-m-d format. I also suspect that, if you’re placing your PHP variables directly in your statement, you’re not putting singles quotes around them. Dates require quotes. ",1538398652.0
xXxLinuxUserxXx,"Use date(„Y-m-d“) instead date(„Y/m/d“).

Anyway you should take a look at prepared statements and you can avoid the MySQL function date(field) if you use field = „2018-12-10“ or use between if your field is type datetime :)",1538396207.0
Fiskepudding,"Heads up, you can keep all date stuff in mysql alone. To get yesterday, write `NOW() - INTERVAL 1 DAY`. Keeps time zone same as db, and avoids incompatible date formats from different programming languages. ",1538431003.0
msiekkinen,"What exactly do you have set to utf8mb4\_bin?   Is this part of the schema column/table definition?   That's required to support it to begin with.   Then you need to make sure your client is set to communicate w/ the proper character encoding.

&#x200B;

What happens you try to save?   Do you get an error?  Is the text truncated at the part of the emoji?   Does it get stored as ""weird junk""?",1538278215.0
greenmarsh77,"I have a bunch of eBooks you can look through: [https://app.box.com/s/d0tnkz4vh3g2pbjz9yxsff6n3whhfqw7](https://app.box.com/s/d0tnkz4vh3g2pbjz9yxsff6n3whhfqw7)

&#x200B;

Hope this helps",1538179785.0
dohako,"I think that's pretty counter intuitive. Good queries need a fair few lines. I'd recommend you learn the MySQL command line separately and focus on loading queries stored in files. 
",1538204451.0
xXxLinuxUserxXx,"Not sure how much you have to map but probably

Insert into $new_db.$new_table select old_field from $old_db.$old_table; 

Will do the job.

You will have to import the new schema or a dump of your current data into the instant and then run you migration queries (probably in bash-script).

Depending where you do your steps you could do a mysqldump on $new_db and move it to the new host.",1538150519.0
aradabaugh,5GB isn’t a lot. A tool like Querious or just MySQL CLI to dumpy to a SQL file is the best bet and avoids things like older data files and table definitions variance between versions. You could also try Percona Xtrabackup,1538156297.0
razin_the_furious,"You'd want to create the $url variable as safely as you can:

    $url = mysqli__real_escape_string($conn,$_SERVER['REQUEST_URI']);

And then edit your query to be:

    SELECT * FROM POST WHERE url = $url;

Add this to get an array of data from your result

    $array = mysqli_fetch_assoc($result);

Then add this to the html to echo it:

     <?php echo $array['id']; ?>",1538100134.0
Irythros,"I'm not going to answer the question you have directly since that's an improper way to be doing it.

What you want is a router.  It will read your URLs and then parse them, sending them to the correct controller and rewriting the URL and such as needed.",1538102829.0
nikoz84,"`
your_url.com?id=155
`


In your file php is important sanitized the data create a function for sanitized, this is for prevent SQL injection

`if(isset($_POST['id'])){
        $id = sanitized ($id);
        $sql= ""select * from post where id={$id}""
}`
",1538133643.0
sandy_patel,"dont worry, done this by my self",1538141245.0
BRINGtheCANNOLI,"SOLVED! Thanks to /u/aqbabaq and /u/kividiot. It does look like an issue with the memory allocator - and configuring MySQL to use TCMalloc seems to have resolved the issue!

I was possibly hitting this bug, https://bugs.mysql.com/bug.php?id=83047&error=lp.

Thank you to everyone for your helpful suggestions.",1538163290.0
Irythros,"Post your config  

Also I would recommend using Percona MySQL Monitoring and Management to gain insight into your nodes.",1538094774.0
clooy,Can you post my.jni file for your server? Make sure to redact anything sensitive. In short MySQL will conform to the memory requirements you have specified and should not be exceeding those. ,1538095547.0
BRINGtheCANNOLI,"My /etc/my.cnf:

    [mysqld]
    datadir=/var/lib/mysql
    socket=/var/lib/mysql/mysql.sock
    log-error=/var/log/mysqld.error.log
    user=mysql
    symbolic-links=0
    character-set-filesystem=utf8mb4
    character-set-server=utf8mb4
    collation-server=utf8mb4_unicode_ci
    init_connect=""SET NAMES utf8mb4 COLLATE utf8mb4_unicode_ci""
    sql-mode=""NO_ENGINE_SUBSTITUTION""
    bind-address=0.0.0.0
    innodb-buffer-pool-size=500M
    log_bin_trust_function_creators=1
    slow_query_log = ON
    log_output = TABLE
    long_query_time = 3
    wait_timeout = 600
    interactive_timeout = 600
    log_slave_updates = ON
    server_id = 1
    relay_log_info_repository = TABLE
    master_info_repository = TABLE
    transaction_write_set_extraction = XXHASH64
    binlog_format = ROW
    report_port = 3306
    binlog_checksum = NONE
    enforce_gtid_consistency = ON
    log_bin
    expire_logs_days = 2
    gtid_mode = ON
    innodb-flush-log-at-trx-commit=0
    sync-binlog=1
    innodb-flush-neighbors=1
    innodb-io-capacity=200
    innodb-flush-method=O_DIRECT
    innodb-autoinc-lock-mode=2
    innodb-page-size=16384
    loose-group_replication_local_address= ""127.0.0.1:33061""
    loose-group_replication_group_seeds= ""X.X.X.X:33061,X.X.X.X:33061,X.X.X.X:33061""
    loose-group_replication_ip_whitelist = ""X.X.X.X,X.X.X.X,X.X.X.X""
    !include /etc/my.cnf.d/cluster.cnf
    [mysqld_safe]
    log-error=/var/log/mysqld.log
    pid-file=/var/run/mysqld/mysqld.pid
    [client]
    default-character-set=utf8mb4

And my /etc/my.cnf.d/cluster.cnf

    [mysqld]
    group_replication_allow_local_disjoint_gtids_join = OFF
    group_replication_allow_local_lower_version_join = OFF
    group_replication_auto_increment_increment = 7
    group_replication_bootstrap_group = OFF
    group_replication_components_stop_timeout = 31536000
    group_replication_compression_threshold = 1000000
    group_replication_enforce_update_everywhere_checks = OFF
    group_replication_flow_control_applier_threshold = 25000
    group_replication_flow_control_certifier_threshold = 25000
    group_replication_flow_control_mode = QUOTA
    group_replication_force_members
    group_replication_group_name = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
    group_replication_group_seeds = X.X.X.X:33061,X.X.X.X:33061
    group_replication_gtid_assignment_block_size = 1000000
    group_replication_ip_whitelist = X.X.X.X,X.X.X.X,X.X.X.X
    group_replication_local_address = X.X.X.X:33061
    group_replication_member_weight = 50
    group_replication_poll_spin_loops = 0
    group_replication_recovery_complete_at = TRANSACTIONS_APPLIED
    group_replication_recovery_reconnect_interval = 60
    group_replication_recovery_retry_count = 10
    group_replication_recovery_ssl_ca
    group_replication_recovery_ssl_capath
    group_replication_recovery_ssl_cert
    group_replication_recovery_ssl_cipher
    group_replication_recovery_ssl_crl
    group_replication_recovery_ssl_crlpath
    group_replication_recovery_ssl_key
    group_replication_recovery_ssl_verify_server_cert = OFF
    group_replication_recovery_use_ssl = OFF
    group_replication_single_primary_mode = ON
    group_replication_ssl_mode = DISABLED
    group_replication_start_on_boot = ON
    group_replication_transaction_size_limit = 0
    group_replication_unreachable_majority_timeout = 0
    auto_increment_increment = 7
    auto_increment_offset = 2",1538097391.0
msiekkinen,"Are open connections going through the roof?  There is some overhead per connection.

Look at your temporary table usage.  Even if not explicit in queries they can be created under the hood on things like joins, sorts.  Stats like  shit should be visualizable over time with things like PPM,  Datadog, or other monitoring tools.",1538100263.0
aqbabaq,There is a problem with MySQL 5.6 5.7+ with not releasing memory when using default memory allocator. For me switching to jemalock fixed the problem on many installations running 5.7+. I will send you link to bug report and info how to fix. ,1538106278.0
kividiot,"With mysql you should be using tcmalloc, had the same issues without it.

**mysql 5.x**

    # yum install gperftools-libs

Create a file /etc/systemd/system/mysqld.service.d/override.conf with the content

    [Service]
    Environment=""LD_PRELOAD=/usr/lib64/libtcmalloc_minimal.so.4""

**mysql 8.x**

    # yum install gperftools-libs

Add to config

    [mysqld_safe] 
    malloc-lib=tcmalloc

edit, missing return..",1538122970.0
Nk4512,"Is you app actually persisting the data to the database completely? And i've seen where some servers need to be restarted depending on the case because it wasn't actually writing to the database, just keeping data in ram until it dies. ",1538097348.0
irukesu,"First, is there a reason you're joining the dates table? It appears you are selecting and grouping by the same field as you are joining which is trade\_detail.Date

&#x200B;

Second, what is the structure of this trade\_detail table, are there multiple listings for the same coin on the same day and you are just summing them?

&#x200B;

Third, going with some assumptions based on the example query, here is a way to do it for a single coin using a subquery and temp variables

&#x200B;

    SELECT
    sub2.*,
    MIN(sub2.Date) AS start_date,
    MAX(sub2.Date) AS end_date
    FROM
    (
        SELECT
        sub.*,
        CASE
            WHEN @prev != sub.Quantity AND @prev:=sub.Quantity
            THEN @level:=@level+1
            ELSE @level
        END AS level
        FROM
        (
            SELECT
            list_coin.ID,
            Coin,
            SUM(Quantity) AS Quantity,
            dates.Date
            FROM
            list_coin
            JOIN trade_detail ON list_coin.ID = trade_detail.CoinID
            JOIN dates ON trade_detail.Date = dates.Date
            WHERE list_coin.ID = 1
            AND trade_detail.Date < '2018-09-01'
            GROUP BY list_coin.ID, dates.Date
        ) AS sub
        JOIN (SELECT @prev:='')
        JOIN (SELECT @level:=0)
    ) AS sub2
    GROUP BY sub2.level

So what we are doing here is first getting all of the coin records and their quantity in the `sub` query. Then we are joining in two temp vars `@prev` and `@level` which will track our previous rows quantity and the level we are at which is just tracking when the previous value is different then we increment that value. Lastly we roll that into `sub2` which we then group on the level. This will flatten and repeat dates and that is when we will pull the min and max dates to get the first and last day in the series. Here is example date at each level:

&#x200B;

Query sub would output:

|ID|Coin|Quantity|Date|
|:-|:-|:-|:-|
|1|BTC|200|2018-09-01|
|1|BTC|150|2018-09-02|
|1|BTC|150|2018-09-03|
|1|BTC|150|2018-09-04|
|1|BTC|126|2018-09-05|
|1|BTC|200|2018-09-06|

&#x200B;

Query sub2 would output:

|ID|Coin|Quantity|Date|level|
|:-|:-|:-|:-|:-|
|1|BTC|200|2018-09-01|1|
|1|BTC|150|2018-09-02|2|
|1|BTC|150|2018-09-03|2|
|1|BTC|150|2018-09-04|2|
|1|BTC|126|2018-09-05|3|
|1|BTC|200|2018-09-06|4|

&#x200B;

Outer query would output:

|ID|Coin|Quantity|Date|level|start\_date|end\_date|
|:-|:-|:-|:-|:-|:-|:-|
|1|BTC|200|2018-09-01|1|2018-09-01|2018-09-01|
|1|BTC|150|2018-09-02|2|2018-09-02|2018-09-04|
|1|BTC|126|2018-09-05|3|2018-09-05|2018-09-05|
|1|BTC|200|2018-09-06|4|2018-09-06|2018-09-06|

&#x200B;",1538104996.0
davvblack,"You will need loops for this, since you want 150 to appear twice and 200 to appear once, there's no group by operation that will get you what you want.

&#x200B;

[https://dev.mysql.com/doc/refman/5.7/en/loop.html](https://dev.mysql.com/doc/refman/5.7/en/loop.html)",1538072603.0
justintxdave,MySQL 8 has windowing functions that will let you do this.  You will 'partition' you data by date and get sums on that partition,1538139703.0
justintxdave,You need to update your client programs and libraries to 8.0.12 and I suggest using the MySQL apt repo,1538061231.0
Old13oy,"When you say ""front end tools"", do you mean you want to run the script from MySQL workbench, or something else? Typically, you would connect to the server via some type of command line interface and run the script.  


&#x200B;",1538063528.0
Irythros,"I'm not sure where the issue is?

You say you know how Python and MySQL work but don't know how to use them together?  I believe this is what you need?  [https://stackoverflow.com/questions/51975342/accessing-mysql-from-python-2-7-programmingerror-1045-28000](https://stackoverflow.com/questions/51975342/accessing-mysql-from-python-2-7-programmingerror-1045-28000)

&#x200B;

If you have to make a website backed by a database you'll probably want [https://www.djangoproject.com/](https://www.djangoproject.com/) assuming you won't get dinged for using a premade framework.

&#x200B;

You will also likely have issues with Godaddy shared hosting.  Most shared hosts only support PHP.  Python may not work.  You'd have to go with a VPS / Cloud server.  However I have not really used python all that much in regards to hosting so may be wrong.",1538094956.0
EnterpriseNCC1701D,Sorry. I don't really know how to explain it. I guess I'm trying to say that I know syntax. I don't know how to use tools/systems/frameworks/etc together. On the town now. Will check these tomorrow morning thanks.,1538095365.0
msiekkinen,"If you really, really, really, want to do this it can be done with a bunch of horrible to maintain ugly, inefficient nested replace calls.  Basically doing what your comments say.

&#x200B;

If this is for inserts you should use your languages api's param binding to take care of escaping.   

&#x200B;

Selecting out then should be fine as is to the application.",1538045267.0
jericon,You are probably out of luck. Did you have any other kind of backup?,1537926711.0
jahayhurst,"a) being helpful:

idk how mission critical this data is, but let's assume it's amazingly mission critical.

* restoring backups is cheap. It's always cheap. That's why you have backups.
* shut down the server now. Period. Clone the drive to a fresh drive, and use a write blocker. God, I hope you haven't overwritten innodb blocks, cause you might lose stuff. You only work with the clone, and while using a write blocker at that. If you don't have a write blocker, clone once, and clone the clone - then look at the clone of the clone with r\\/o stuff - the clone is your original backup, the original drive may be needed for DR recovery if you want to send it off.
* Now that you're looking at a clone, and looking at it r\\/o are there .ibd files? If there are, they likely contain the rows for those tables.
* If you have those .ibd files, you can try recovering data from them yourself using [https://launchpad.net/percona-data-recovery-tool-for-innodb](https://launchpad.net/percona-data-recovery-tool-for-innodb). It's fun.
* TBH, if you have those files, I'd look at hiring Percona consulting or maybe MySQL consulting. It ain't cheap, but they'll give you a full picture of what they can get and they'll get it.
* If you haven't modified the blocks where the ibdata1 and ib\_logfile were, aka haven't written to that disk and happened to land there, Percona consulting may likely even run over the blank space on the disk and extract data from a removed ibdata1 file.

So, like, did you have innodb\_file\_per\_table enabled? Is the server and the disk still online or did you pull it right away? Do you see \*.ibd files?

&#x200B;

b) if you're looking to do the above, don't ask anyone how, just hire someone to do it - like Percona or something, idk. They are great. If the data's worth <200k, restore backups. If the data's worth <200k and you don't have backups, it's a loss and get over it and get some backups.

&#x200B;

b) lol why would you drop the tables from innodb and remove them, when you can convert to myisam (assuming compatible tables, very likely) then convert back later? that just seems unnecessary.

&#x200B;

d) watch out with MySQL 8.0, the system tables are InnoDB - so even moving that folder back won't do it.",1537927745.0
SomeGuyNamedPaul,"Do you still have the individual .ibd files?

Edit: yikes!  I just read the instructions on that link... They're missing the fact that ibdata1 still has the undo space in it plus the insert buffer, so it will definitely still grow.  It even says so in the diagram they forklifted from Percona.  They also describing a full reorg, but done in a way that isn't practical for anything of any size.",1537924580.0
RX_AssocResp,Actually I can find none of the functions mentioned on that page ...,1537901511.0
msiekkinen,"We would have some production replicated to a read only development slave.   This wasn't 100% production to hide sensitive things like user information/PII.   Since it was kind of a shared resource though we would get team X complaining about replication behind behind because team Y was running a really horrible select with subselects not using indexes or what ever that ended blocking replication....  

&#x200B;

Eventually implemented an auto kill policy for queries running over a certain amount of time.   

&#x200B;

This is a reason you don't want to give them even read only access to real production.   Certainly not write....   Our production databases were network segmented to production app network interfaces at the layer 3 level so even if a DBA wanted to say grant select on db.\* to 'joe'@'[192.168.1.1](https://192.168.1.1)' or what not for a dev, it wouldn't work.  ",1537897845.0
apaethe,https://stackoverflow.com/questions/1828948/mysql-function-to-find-the-number-of-working-days-between-two-dates,1538008226.0
SomeGuyNamedPaul,How did you take the archive in the first place?,1537842443.0
gmuslera,"What means reduplicate? Having duplicated lines in the csv input file? A sort | uniq filter can get rid of them pretty fast if the order in the DB will be for key anyway. And load data is usually pretty fast.

Turning off indexes, changing transactional level, tricks with the IO subsystem and a few other strategies may speed up innodb insertions, and you probably will have to work with that data with innodb on production. ",1537826544.0
SomeGuyNamedPaul,If you're inserting one row at a time when m with auto-commit for InnoDB then you're gonna have a bad time.  Each individual insert that was starts a transaction and commits a transaction with each transaction getting allocated a new LSN.  Try batches of like 1000 and you'll see a big difference.  I tell my developers that you can get 30 inserts in a batch in the same time as two in separate transactions.,1537826947.0
r3pr0b8,"> i fear the entries are too many

no, they aren't",1537628668.0
kaydub88,"I had an instance of MySQL we would load 30GB+ per day into. I remember one table with a total size around 14TB.

100MB per month is nothing.",1537629544.0
jynus,"Everybody here is right in saying that as long as you have no more than a few terabytes of data per server, MySQL will not have any problem to be used as a statistics backend.

However, it is possible to hit some issues based on your needs and type of operations you want. For example:

* you may need to alter your schema in the middle of the logs, and that can be painful with a very large table- in that case split your table manually partition by time and only alter the new tables, or make it so your schama is flexible (e.g. json + functional indexes)

* You may run out of disk space or it starts to be too costly- consider compression (InnoDB, RocksDB, TokuDB), more efficient formats/types, user-level compression, or, as a last option, using alternative storage engine software, like Prometheus or Elastic (but only when your storage goes beyond the 1TB mark or you need specific functionalities). MySQL is not the most efficient disk-wise, but you don't wont to diversify your data store unless you have a really good reason

* You may want to get faster results like ""count all results with X"" or ""calculate the average retention"". Again, those can happen, just will get linearly slower with records, but even in a terabyte table they should not take longer than a few seconds. There are cases where you go beyond that size or need faster results, so you can create your own analytics system on top of mysql by creating summary tables and filling them with triggers (real time, heavy on writes) or events/cronjobs (non real time, but less impact on writes). Whenever that starts to become too complex you can think of alternative storage engine, both using the MySQL interface or externally. In many cases those can extract the raw information and compute on top of MySQL, so they don't fully substitute them.

You may have seen ""MySQL is not good for analytics"", and that is true, it was not created thinking on that, but mostly on fast web requests. However, the operational and resource cost overhead of maintaining multiple storage backends has to be justified by the a need that would make impossible to work with the existing storage systems. There is an architecture and operational cost that usually requires extra human resources and skills. Worry when you have to scale, but until then, stick to the minimum amount of technologies possible.

To give you an example, at Wikipedia we are in a size where Hadoop and Elastic (and higher level tools on top of that) for events and logs respectively are more than justified, but people keep using MySQL and plain log files (and we maintain those formats) because they are simpler tools for some of the users.",1537694958.0
davvblack,"> Find all the users that performed Event A this month or

This is easily covered by an index on event_id

> Find all the events with column X and Y

I need to know more about the schema to describe how to do this performantly.",1537683394.0
this-is-me-reddit,"Try it with the 5th line removed 
I’m not at a pc right now  so results may vary. ",1537573206.0
ElMatze79,Try where balance > avg(balance);,1537543050.0
jb11784,"Unless you foresee a need to be able to independently stop/start instances of mysql, I don't know that you would gaining anything running two instances. 

You'd possibly be making it more complicated to manage and wasting hardware resources, though. ",1537534453.0
NotTooDeep,"I like to start this kind of analysis from the failure recovery mode. AWS makes this easy.

If you have two schema in two instances and an instance fails, you restore from the latest snapshot (if you use temp tables) or the latest point in time (if you don't use temp tables). This is good. Restore snapshot of db1 to db1-restored, assign security group and parameter group, rename db1 to db1-old, and rename db1-restored to db1.

If you have two schema in one instance and one schema experiences data corruption (mysql gets lost and can't find a specific tablespace), then you have to restore from snapshot or latest to db1-restored, mysqldump the corrupted schema or table (might not work with just the table), and load the dump file into db1.  

Now you have a vision of the work for recovering the two schema in one instance. You can extend this to two instances with one schema each. More instances will increase your costs, so that's another tradeoff.

",1537537998.0
dsn0wman,"If your databases get big, you might not want both on the same instance. Large databases generally need something faster for backup and recovery than mysqldump.  
  
If you were using percona xtra backup or some other physical backup mechanism you'd not be able to independently recover a single schema. So in the case where only 1 schema gets messed up you have to restore the entire database and incur downtime on all applications.",1537579215.0
primeviltom,"OK, just after I posted this, I found a solution using to_days..

SQL: 

SELECT (SELECT to_days(timestamp) FROM posts WHERE id=(select max(id) from posts)) - 
(SELECT to_days(timestamp) FROM posts WHERE id=(select (max(id)-100) from posts))",1537522337.0
msiekkinen,Are you talking about something like passwords or some thing like reddit comments? ,1537500165.0
Irythros,"Encoding does nothing to make them secure.

Encryption does.  That should happen in your code.  Encryption can be decrypted with guaranteed reliability.

If it's passwords they should be hashed so they cannot be decrypted / read as what the user provided.",1537501220.0
jb11784,"Also, encrypting is only a part of an overall security plan. 

You still need to have good access control, etc. ",1537537336.0
NotAnExpertWitness,"Just alter the user you are using to connect to the DB with.

&#x200B;

ALTER USER 'user'@'localhost' IDENTIFIED WITH mysql\_native\_password BY 'yourpass';

&#x200B;

Explanation: [https://stackoverflow.com/questions/50387952/how-to-resolve-unable-to-load-authentication-plugin-caching-sha2-password-issu](https://stackoverflow.com/questions/50387952/how-to-resolve-unable-to-load-authentication-plugin-caching-sha2-password-issu)",1537499063.0
r3pr0b8,"> This works fine

try it again, this time with a GROUP BY clause",1537465032.0
etrnloptimist,"The syntax is very similar. All SQL is. I would write your queries in workbench, see where it breaks, then just look up the actual documentation. If you have that much experience, it really shouldn't be hard.",1537468290.0
Bitter_Bridge,"SELECT showID, COUNT(*) AS count
FROM genres_table
WHERE genreID IN (2,3)
GROUP BY showID
HAVING count > 1",1537410809.0
UncleNorman,Can you log into phpmyadmin as the user and see the db? ,1537411623.0
ElMatze79,From first glance try putting a space between stores and (,1537395132.0
msiekkinen,"It looks like you're building up a literal string in the sql variable.  

I'm thinking ${name}, ${email} etc are going in as unquoted literal values.   Mysql want them quoted as string literals at least.  

I'm not familiar w/ react, you'd want to see the proper way to do ""parameterized queries"" or ""bound parameters""  to take care of string quoting/escaping for you.

",1537405390.0
mischiefunmanagable,"a stored procedure has a pre-compiled execution plan and will use the same plan every execution (in theory)

a function is NOT pre-compiled, it is run anew every invocation

other more useful differences are a function returns ONE value as a return from it's invocation, where a stored procedure can return zero or more values via parameters

functions are more often used for ""inline"" computation via invocation from DML statements, stored procedures are meant to BE the statement, example: I can run SELECT get_percentage( MAX(a.sales), COUNT(sales) ) from a; but I'd need to embed the query and logic in a stored procedure stand alone and do CALL calculate_sales_percentage(); to get the same output

different implementations for different use cases, can you kludge them to act the same? more or less, but they fit their distinct niches well


easy rule of thumb; inline computation? function. business logic? stored procedure",1537369761.0
UnCunted36244,I have a few ideas but after looking at your post history you seem to be a uniquely awful person - so no.,1537664788.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/php] [\[x\/mysql\] Cross-Domain On-Demand Database Sync](https://www.reddit.com/r/PHP/comments/9gtue6/xmysql_crossdomain_ondemand_database_sync/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1537268930.0
SelectCompare,"If you run the installer on command line with /? parameter, it will display list of available parameters.

&#x200B;",1537223457.0
mich4elp,Cool article! Some performance implications of long transactions that I'd never seen before in there.,1537322906.0
ckofy,"Are you talking about self join? I just can not imagine a situation when knowing the origin table for a row may be important, give an example.",1537104154.0
brkmnd,"I found a solution for those with the same problem.

Use UNION ALL instead and pad with null like this

table\_A : {id,name}

table\_B : {id,name}

&#x200B;

SELECT id,name as name\_A, null as name\_B

UNION ALL

SELECT id,null as name\_A,name as name\_B

&#x200B;

Now check the result for name\_A != """" and so on. There might be a better solution. But JOIN does not seem appropriate here.",1537103848.0
ckofy,"Let’s agree about terms, what you telling about is not rows, but columns (fields), rows in table do not have names. Then, if you have the same named columns in two (or more) tables, you can give them aliases in join. 
Say: Select id , picture_one, picture_two from (Select id, picture as picture_one from table_A) a_selection join (select id, picture as picture_two from table_B) b_selection using(id);
",1537121986.0
neofreeman,I think SQLite should be sufficient. If records are always going to be 1000 I would ask myself if I can keep everything in memory and flush it out periodically to a file on disk. It won't cost you much to iterate over 1000 elements and do all sort of filtering that one might imagine. ,1537075186.0
SomeGuyNamedPaul,sqlite was made for this use case.  Be sure to include at least weekly or daily automatic backups of the database.  It's so small it'd be silly not to.,1537107053.0
sh_tomer,"SQLite is the easiest solution to start with, especially because you mentioned that you're a beginner. I would start with SQLite and see if there is anything missing that I can't fulfil with that technology, before migrating to a different one.

The main advantage of SQLite is the ease of installation and deployment (it's just a file), while MySQL requires a more complicated installation & maintenance, though it provides much more flexibility. ",1537210266.0
aamfk,I'd guess you need the x64 download npt the x86 one. Also are you getting the bundle or just the database installer? It sounds like you grabbed the client tools not the full bundle.,1537073947.0
Jedi_Trumpimus_Prime,I ended up installing WAMP. Much easier.,1537963273.0
KrevanSerKay,"In general most subreddits I've seen consider giving answers to ""homework help"" requests as cheating. So you're more likely to find people saying ""consider X"" or ""look into Y"" instead of giving direct answers.

With that in mind: 

If you imagine that both inputs and outputs in SQL can be represented as tables, then **basically anything you can create with a SELECT query can be turned into a table**. In fact, MySQL has syntax just for this:

> CREATE TABLE `new_table` SELECT `field_1`, `field_3` FROM `old_table` WHERE `field_1` > value;

Would create a new table with two columns, each named after field_1 and field_3 respectively, and only containing those values - so literally the output of the select statement.

So with that in mind, **can you think of any ways to create a single select query that only returns the values you want?** HINT: If your course hasn't already covered it, look into how JOINs work.",1537049259.0
msiekkinen,"Don't know much about triggers but on the insert query instead of book year you could do

    Max(1950, collase(book_year, 1950))",1536903322.0
trashpantaloons,"Yeah it’s impossible to alter a row you’re inserting/updating with a trigger. A trigger is definitely the wrong thing to do here.

You can also do an in-line if statement in your insert: `insert in tbl(...) values(..., if (BOOK_YEAR < 1950, 1950, BOOK_YEAR)...; `",1536914628.0
msiekkinen,"https://dev.mysql.com/doc/refman/8.0/en/trigger-syntax.html

You doin't up a whole new Update... query, simply say

instead of

> update BOOK set BOOK_YEAR= 1950 where BOOK_ID= new.BOOK_ID;

simply say 

>  SET new.BOOK_YEAR=1950;",1536938558.0
clooy,"Why overwrite and not just delete; this query worked for me;

    delete from logtbl 
    where logid not in 
              ( select * from 
                (select logid 
                 from logtbl 
                 order by logid desc 
                 limit 10000) alias);

You can add this as an [insert trigger](https://www.techonthenet.com/mysql/triggers/after_insert.php) so it's run after every insert.
---
Note: `limit` in subquery doesnt work, the alias hack is a work around for this limitation which made the query more complicated then it needed to be.",1536878384.0
apaethe,"Fun question.  I think using the modulo function might do this for you.

What if you make the table with `id` as primary key `int` **not** auto_increment?

Then when you insert you would do something like 

     REPLACE INTO mytable
     VALUES ( MOD((SELECT COUNT(*) FROM mytable),1000)+1, logval1, logval2, ...)

I think you could use `MOD` to to loop through the values as you are wanting to do if you use the `REPLACE` function.

This query isn't tested and probably has a mistake in there, but I think this is a legit way to go.

        ",1536896191.0
KevZero,"""bind-address"" is talking about binding the mysqld process to one or more IPs *on the server*, i.e. the interfaces where mysqld will listen. It's not a list of hosts which are allowed to connect. If you want to connect from a certain set of hosts into mysqld, you need to bind to one of the host's network-facing IPs, and then use a firewall (like iptables) or restrictive grants (or both!) to limit connections to the hosts you want.",1536854501.0
veldenar,"It's either one address or all addresses. 

https://serverfault.com/q/139323",1536837097.0
s3_gunzel,You can set from IP permissions on a per-user basis. ,1536838963.0
Wouto1997,"Typing on mobile so excuse the formatting etc.

Normally from my experience you should be able to login with root on localhost after installation (note that this means you have to either be on the machine mysql is on or ssh into that machine). Since you're already connecting to localhost I assume you're already hosting it on your own machine so this shouls be covered.

Another solution I've used before if all went wrong was to google how to restart mysql in safe mode, this shouldn't allow any outside connections and should allow root from localhost to connect without password. From here you can create new user(s) and eventually (after flushing privileges) you should be able to restart and login with one of the new logins (or an updated root password if you decide to just update that)

Hope any of this helps!",1536853613.0
bobjohnsonmilw,Are you trolling?  You can't be serious.,1536804296.0
DataVader,"SET @myLimit = 7;

SELECT *
FROM myTable
LIMIT @myLimit;",1536765043.0
thecoolbrian,What version of MySQL are you using?  https://stackoverflow.com/q/245180,1536751852.0
cYzzie,the work around usually is that the variable is coming from the programming language that you use before you fire the statement towards mysql,1537222415.0
davvblack,"Can you be more specific about how the order is different?  That does look correct.

As for your second question, read up on SELECT DISTINCT():

https://www.w3schools.com/sql/sql_distinct.asp",1536724040.0
de_argh,    you could also run \. new_database_name.sql from within mysql,1536711139.0
Ctwellman,This command is not run from the MySQL shell. But bash or whatever your shell on the server. So after logging in through putty you should run from there. ,1536707985.0
Ctwellman,"Looks at something like INFILE. https://dev.mysql.com/doc/refman/8.0/en/load-data.html

But if you have the file already I would load from outside MySQL shell. And use the command you already have.",1536716444.0
ge0n1,"You should run 

$ sudo rm /*

My bro did this and now he is a real DBA",1536715444.0
jericon,"Your post has been removed.

Reason: Self Promotion

Your post history shows that you have a propensity to post from the same domain or topic often to a wide breadth of somewhat related subreddits without contributing to the communities otherwise.  Please see [Reddit's Self Promotion Policy](https://www.reddit.com/wiki/selfpromotion).",1537206302.0
r3pr0b8,">The checkbox table would then have multiple repeating rows, like below:
>
> userid: 1 | name: bobby | sport: football
> 
> userid: 1 | name: bobby | sport: soccer

okay, first of all, those aren't repeating rows

secondly, you need to normalize the user data into its own table, and keep the checkbox data in a separate table

    USERS
    userid  name
       1    bobby
       2    fred
       3    todd
       
    SPORTS
    userid  sport
       1    football
       1    soccer
       1    darts   
       2    football  
       3    soccer  

> Let's say they choose both football and soccer. Then Bobby's entry would appear.

    SELECT u.userid
         , u.name
         , s.sport
      FROM ( SELECT userid
               FROM sports 
              WHERE sport IN ( 'football'
                             , 'soccer' )      /* note 2 checked off */
             GROUP
                 BY userid
             HAVING COUNT(DISTINCT sport) = 2  /* number checked off */                         
           ) AS c 
    INNER
      JOIN users AS u
        ON u.userid = c.userid
    INNER
      JOIN sports AS s
        ON s.userid = u.userid    

note this shows users who have each of the selected sports (in this case 2), but shows all their sports (in this case 3)

note the comments inside the subquery -- your front end app has to count the number of search checkboxes, and insert both the list of sports and the number of them into the query",1536686456.0
SelectCompare,"I would create the following tables:

    User (id, name)
    Question (id, description)
    Option (id, description) --checkbox

Relations:

    Question_option(question_id, option_id)
    User_option(user_id, option_id)

To find users who selected 'sport' you'll have

    Select a.name from User a inner join User_option b 
            on a.id = b.user_id inner join Option c 
            on b.user_id = c.id 
    where c.description = 'sport'

The Question table allows you to add more questions and manage their options.",1536686565.0
Bitter_Bridge,"Unless you have a prohibitive number of checkboxes, I would just store them as TINYINT columns in your main table. ",1536681419.0
msiekkinen,Are you getting some kind of error message?  Screenshot of what you're getting stuck on?  Link to exactly what you downloaded?,1536690118.0
sh_tomer,"Make sure you uninstall the server using MySQL's official uninstaller.

Once you did that, make sure to reboot your machine, so everything will be properly updated in the registry (as you mentioned these are Windows machines).",1537210417.0
dsn0wman,"Easiest solution. Throw the old slave out, and create a new 5.7 slave from the 5.7 master. ",1536680110.0
dartalley,"InnoDB and MYISAM are totally different engines with different file formats. There might be a way to convert innoDB to MYISAM but you really don't want to do that.

How is it even possible to have a different version slave with different formatted tables?",1536666514.0
frak808,"I think your Un updated Devart dbForge is still gonna beat Workbench. It should still work without subscription right? 

I've not found anything that can match it. 

",1536668983.0
kaydub88,I've always been partial to SQLYog but I believe Jetbrains has come out with their own DB IDE now.,1536673115.0
clooy,"Every SQL application is going to have it's own strength and weaknesses. As you have noted most will have a history log but not as powerful as the one you highlighted.

The MySQL general log may come close through, you can turn it on with the following commands:

        -- Turn on table based logging
        set global log_output = 'TABLE';

        -- Turn on general log
        set global general_log = 'ON';

        -- Get entries
        select * from mysql.general_log

If this isn't enough - I noticed Terradata had row counts, times, etc. The slow query log is more detailed and includes all these and more - you just have to set a low thresh hold for what a slow query is:

        -- Log everything to slow query log
        set global log_slow_queries = 1;
        set global long_query_time = 0.000001;

        --Take a look at the table mysql.general_log
        select * from mysql.slow_log;

Additionally, you can filter out queries to just your current connection:

        --Filter on the current connection
        select connection_id() into @cid;
        select * from mysql.`general_log` where thread_id=@cid;

Simply add the select queries to your favourites and you should be able to access the data any time. Not as convenient as you may want, but has the added benefit that it logs everything - even from your application.
",1536638327.0
r0ck0,"I'm effectively using datagrip now (actually phpstorm, but it has datagrip's functionality built-in).

This has changed how I treat ""history"" for SQL queries... instead of having a text box that contains one query at a time, and using GUI menus etc to browser history... I just have a big .sql file, and copy and paste commands, and use ctrl-enter to contextually select the command to execute each time.  It's actually much more efficent once you get used to it, and means I can have multiple versions of the same command all on screen at once.

",1536641519.0
kastauyra,"Without complete config it’s hard to say whether the setups are even remotely comparable. For starters, if binlog is enabled, sync_binlog default is 0 in 5.6 and 1 in 8.0",1536591912.0
KrevanSerKay,"Seems like there's a lot more going on here. We have comparable system specs on our 5.6 instances, but can load 10-20x as much data in 1.5h loading from CSVs (and almost identical performance loading from .sql files). Unfortunately, I can't really say what's specifically different here, but figured I'd throw my hat in the ring and let you know that at 90 minutes for 5GB, it sounds like something else might be wrong.

I'm still learning about all of the different configuration settings to figure out how to further optimize/how to properly configure new clusters. If you're able to make any headway here, please let me know, it'll really help >.<"" We're looking at migrating to 8.X in the next few months and i'm worried about performance. 

Had a bad first experience with data loading/testing load on a new 5.7 cluster (query execution plans massively changed and slowed queries 200x), most likely because of poorly optimized server configurations. So i'm still learning. ",1536607915.0
j0holo,Did you disable indexing on the database before importing? If not that explains everything!,1537097070.0
MrPurple_,Besides all the small tweaks it seems like i fount THE solution: see UPDATE in original post.,1542177272.0
just5ath,"> *medium* to *big* database (about **5** **GB**)

I lold",1536589480.0
s3_gunzel,Restart the service. ,1536538790.0
mmurphy3,"Is mysql listening on port 3306? Do you have the ip set at 127.0.0.1? It may not configured correctly to be available on tcp. Or firewall is blocking it upstream or it’s not starting. Check the status: service mariadb status and service mysql status
Other third party repos can cause this as well.",1536551887.0
komanchi2,Is /var/run/mysqld owned by mysql:mysql?,1536579352.0
areti33,"Try to evaluate the demo version of the [MySQL Database Recovery](http://www.databasefilerecovery.com/mysql-database-recovery.html) software, which is proficient in dealing such interruptions in MySQL Database.

Demo: [http://www.databasefilerecovery.com/demo/mysql-database-recovery.exe](http://www.databasefilerecovery.com/demo/mysql-database-recovery.exe)",1541404841.0
SelectCompare,"How long do the tests run? Have you checked how long the DB set up and teardown for each test take?
Perhaps the rest of the database is not required for every test. Or maybe you can parallelize execution of tests, either in the same database, or using separate DBs.",1536534222.0
SomeGuyNamedPaul,You're only cheating yourself by asking Reddit to do your homework for you.,1536509268.0
jericon,What kind of issues are you having?  It’s very hard to help without knowing the problems. ,1536468202.0
,[removed],1536443873.0
Bitter_Bridge,Create a history table and place a trigger on your main table to move the existing row to history before update. ,1536433775.0
ge0n1,"Have the schema include revision number and timestamp on the row. When you go to save check the revison number and if you hit a duplicate you can either show the diff or just go with the most recent and a warning.

As far as data retention you just dump revisions past a certain number once you hit scale (probably never). No one is going to write war and peace in this thing but if you want prematurely optimize then you can trust  that no-one is going to go back 30 revisions.",1536442350.0
PristineTransition,"Make an ‘audit’ or ‘versions’ table that uses compression. Then make a data retention policy to clean up copies older than say 6 months or whatever is best for your use case. If long time archival is important, could dump to a JSON or some other text file and upload to S3",1536432545.0
PristineTransition,Yeah you can. Just copy it to the quit or versions table first with a time stamp of copy creation. I wouldn’t worry about it after that. Databases can scale really well with large tables.,1536446966.0
louisville1169,"You could do

SELECT \* FROM posts

INNER JOIN post\_taxonomy\_term\_map ON (posts.ID = post\_taxonomy\_term\_map.object\_id)

WHERE post\_taxonomy\_term\_map.term\_id=$term1 AND post\_taxonomy\_term\_map.term\_id=$term2 AND ...

&#x200B;

And just keep ""AND""ing. Or

&#x200B;

SELECT \* FROM posts

INNER JOIN post\_taxonomy\_term\_map ON (posts.ID = post\_taxonomy\_term\_map.object\_id)

WHERE post\_taxonomy\_term\_map.term\_id IN ($term1, $term2 , ...)",1536343699.0
MMOAddict,"I'm a little confused what you're trying to do, but I think I have an understanding..  you want to get all the post ids from the map table that have all of a list of term ids.

    SELECT
        post_terms.post_id
    FROM (SELECT
            p.post_id
            FROM post_taxonomy_term_map pmap
                LEFT JOIN posts p ON p.post_id=pmap.post_id
            WHERE pmap.term_id IN ($term1, $term2)
         ) as post_terms
    GROUP BY post_terms.post_id
    HAVING COUNT(*) >= 2;

&#x200B;

I'm just kind of writing from my head on this so I'm not sure if that'll work but I think that's the gist of what you want. You'll need to change the count part to however many term ids there are.   I can think of a different way using SUM() in the having clause and not require a subquery, it might even run faster than that query but it wouldn't look as pretty. :)",1536355790.0
mischiefunmanagable,build one in parallel and replace the existing table instead of clearing and regenerating the single table,1536273223.0
ElMatze79,"Yes, you can see your cluster (or GRE) as an additional slave to your existing setup.
The procedure of switching would be the same as a slave promotion.",1536261431.0
razin_the_furious,"You CAN implode/explode values. There are some circumstances where that's fine, but it usually requires a lot of very specific rules:

1) You know how many values there can be max, and that will never increase. Comma separated storage does not scale well
2) You never need to search for specific sources using a pager number: you're limited to FIND_IN_SET, which is not at all a quick thing.

You are better off having a Pagers table, a Source Table, and a pager_source table that links them together.",1536240834.0
Bitter_Bridge,"I use JSON extensively in MySQL,  but I’m not following your use case. It sounds like you might be wanting to store an array in a field?",1536240783.0
mwtbdltricp,"You have some syntax errors:

You can do it like this.

&#x200B;

declare @test int

set @test=1

if @test>0 

begin

print @test

end 

&#x200B;",1536212520.0
davvblack,Is this inside of a stored procedure? If so can you paste all of it? ,1536212701.0
NotTooDeep,"Write all of your code for a human being to read first and type second. Table and column names are still just code with a special persistence quality. If a junior programmer can understand your code at 3:00 AM without having to call someone else, the programmer that wrote that code should be congratulated. Yes, there was an error in the logic, but it didn't take until sunup for someone else to find it. That's good coding practice. Be obvious. It costs less.

Don't preface any object with its object type. Just don't. If someone can't use the data dictionary or modern IDEs to figure out they've found a function instead of a stored procedure, they have other issues that need addressing first.  It's a waste of real estate and old school uncool to name a table tblApplicationUser. There may have been a time in the evolution of computers that doing so was helpful, but that time has long passed.

It saves time if you name all PKs just ID.  You always know that the pk for the contact table is contact.id. It's faster to write your joins and easier for someone else to understand your code.

In that same vane, you will have natural unique keys in some tables. The trouble with making them the PK is it mucks up those joins that are so fast to write and easy to read. Just picture a table with a natural key consisting of four columns and joining that bad boy to three child tables; that's twelve lines of code instead of three.

If contact.id is the PK for the contact table, then contact_id is its foreign key in any child tables. This makes modeling faster and easier to understand, which makes writing code against that model faster. This costs less.

So you see I have some different opinions that your post. It's not the best way. It's just my preferences. My recommendation is that everyone create a quick schema of five tables and a few triggers and stored procs, then break them and test how they read in errors and the logs. If you begin from the point of view of you getting the call at 3:00 AM and production is down because of your code, you'll thank yourself many times over for figuring out what's the easiest code to read that you can write. The trade-offs are often ugly but necessary. This is why so called best practices fall short sometimes. They can't be best practices for everything that hasn't been created yet.
",1536179076.0
r3pr0b8,"> use singular forms

you mean for table names?   heartily disagree",1536181923.0
jericon,"Your post has been removed.

Reason: Self Promotion

Your post history shows that you have a propensity to post from the same domain or topic often to a wide breadth of somewhat related subreddits without contributing to the communities otherwise.  Please see [Reddit's Self Promotion Policy](https://www.reddit.com/wiki/selfpromotion).",1537206331.0
winzippy,You could use a virtual machine. It's a lot of overhead for just one program though.,1536160320.0
,[deleted],1536100350.0
trashpantaloons,"Brew uninstall mysql@5.6 —force

Then go rm -rf the data folder which I believe is /usr/share/var/mysql",1536082034.0
zseikyocho,"Another reason:
MariaDB
```
ps -ef | grep -i mysql
1025048086 34292     1   0  1:29PM ??         0:00.02 /bin/sh /usr/local/opt/mariadb/bin/mysqld_safe --datadir=/usr/local/var/mysql
```",1536121958.0
GenerousGestapo,"If your goal is to uninstall brew follow this: 

&#x200B;

[https://i.imgur.com/EwiT5wj.png](https://i.imgur.com/EwiT5wj.png) from :

[https://docs.brew.sh/FAQ](https://docs.brew.sh/FAQ)

Here's the code: 

ruby -e ""$(curl -fsSL r/https://raw.githubusercontent.com/Homebrew/install/master/uninstall)""

If that doesn't work can you post pics of all the steps just so we can help narrow down the error ? 

&#x200B;

&#x200B;",1536037952.0
zseikyocho,"New issue:
```
brew services list
mysql@5.6 started username ~/Library/LaunchAgents/homebrew.mxcl.mysql@5.6.plist

mysql -uroot
ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/tmp/mysql.sock' (2)
```
",1536122849.0
zseikyocho,"Solution:
```
brew uninstall mysql@5.6 —force
# Remove listed folders and files above
brew install mysql56
```",1536123103.0
Irythros,"To me that's actually a horrible setup, sorry :P

​

It appears what you have above is a poorly implemented EAV setup, or ""entity-attribute-value"".  That would be like:

id\_user | attribute | value

1 | password | foo

1 | username | bar

1 | email | [foo@bar.com](mailto:foo@bar.com)

2 | username | truck

2 | password | carssuck

2 | email | [ford@ford.com](mailto:ford@ford.com)

​

Essentially it makes an entity without a schema as the schema is based on whatever attributes are associated to the id\_user (or it could be anything like a post, an upload etc)

​

What I would do:

tbl\_users

id\_user | int | pk, ai

password | varchar

last\_updated | datetime

​

tbl\_users\_facebookTokens

id\_user | int | pk

token | varchar

​

Selecting:

select u.\*, ufb.\* from tbl\_users LEFT JOIN tbl\_users\_facebookTokens ufb ON u.id\_user = ufb.id\_user WHERE u.id\_user = :iduser

​

Updating is simple as ""update tbl\_users set last\_updated = now() where id\_user = :idUser""

You can also make a stored procedure to automatically run the query.

​

\>  there's not a great way with this setup to perform an 'update or insert if not there' query for multiple rows at once?

For multiple rows, no.  For a single row: [https://dev.mysql.com/doc/refman/8.0/en/insert-on-duplicate.html](https://dev.mysql.com/doc/refman/8.0/en/insert-on-duplicate.html)

​

\>  **TL;DR** I'm just wondering if there's some kind of best practice in terms of how to approach formatting the data in a table to minimize future headaches.

Don't bother trying to make it ""scalable"" from the start.  Stick all the basics into a single table.  If it's a unique thing (such as being a users facebook token, a users google token, a users action log) then throw it into a new table.  If it's something where the user can only have one of (one row), PK it.  If they can have multiple, throw an index on it.

​

Edit:

If you want to store update times for specific fields/columns, to me that's a bit insane.  If it's for security/logging purposes use a logging service or application.  If you actually need it, add it as a new column.  If you need to do two  or more columns per table then you can use the EAV model or create unique columns per table for each update and then reason about which to update via code.",1536026796.0
clutchguy84,"First of all, each user should have only one row in the users table.  The id of the users table should be the user\_id.  It shouldn't be it's own column.  That kind of defeats the purpose of normalization.

I would make a users table with these columns:

* name
* email
* phone
* password
* created\_at
* updated\_at  
etc.

I would make a table named strategies, and have columns like:

* strategy\_name
* strategy\_field
* strategy\_value

Then have a user\_strageties table with columns:

* user\_id
* stragety\_id

Your user\_strategies table has the user\_id as a foreign key to users, and the strategy\_id as a foreign key to strategies.",1536069943.0
DataVader,"I would create a user table without any login data, so each user is unique. Then add a table for credentials and maybe one for session tokens.

Updating and/or inserting multiple rows at once is not hard. If you want to insert missing rows and update existing ones at the same time, it's possible via ""on duplicate update"". If you just want to insert missing rows and avoid duplicates, use ""insert ignore into..."".

Anything else? ;)",1536074653.0
duplicateBadger,"The table structure seems fine (as in, a user table with the bare minimum (user_id, username, optionally a password field, unless you're storing previous password hashes to prevent re-use), and having everything else user related in a meta table.

The querying, though.

You're doing too much logic in your query. If you want to update the strategy, then have a SetStrategy function in your application that does that. if you want to set their name, have a SetUserName function to do that. ",1536079649.0
knifebork,"Short answer: yes, absolutely. That's kind of the point.

However, now you've got to get your applications pointing to it instead of the failed master, and you'll probably want to establish replicas using it as the master. There are some techniques to try to automate all of this, but I haven't mastered them.

What I have done is remove the old master from the network so that nothing can reach it and then give the master's IP address to the replica. That's quicker than trying to do it with DNS.",1535980249.0
knifebork,"I guess the off site component adds a twist. You probably have different addressing there, so simply adding an address might not work. If you switch to your off site replica during a disaster, you'll probably want to switch back at some point. It's kind of like the same thing only in reverse: you have a new replica at the rebuilt primary location that uses the server at your off site backup as its master, and then you fail back. 

If you've already gone to the time and effort to come up with an automated system for managing and promoting replicas, this might be easy. It depends on how much down time can you afford.",1535982062.0
de_argh,"This is a complicated question and it's more a function of the application, DNS, what have you than the database.  That being said, stop slave; reset slave; and you're no longer a slave.",1535983654.0
SomeGuyNamedPaul,"Speed up the recovery?  You run ""set @@global.read_only=0;"" and point the app that way.",1536001687.0
Irythros,"\>  1: Is that type of db can be hosted on VPS with 2 gb ram and ssd storage?

You *can* but one thing to keep in mind is that InnoDB benefits (and pretty much any application) from storing all of its data in memory.  By having very little memory compared to the data you will be hitting disk and that's significantly slower.  In this case you would at the minimum need it stored on an SSD.

My recommendation here is to have enough memory to store every in it.

​

\>  We had optmized the db by using innoDB and added index on tables but still it's not seems super faster so what are the techniques we can use to make it work faster with response in some milliseconds.

Just adding indexes does not make everything faster.  Infact if you add bad indexes it will be slower than if you didn't have them.  The queries that hit your DB also need to be optimized to use the indexes in the most efficient manner.  Lastly you need to consider how your query is being executed which can be done by running an **EXPLAIN** or **EXPLAIN EXTENDED** on the query.

I always recommend using Percona MySQL and then installing their monitoring tool called Percona Monitoring & Management .  This is created by Percona and it monitors the health of your database and the queries ran on it.  You can find non-performant queries easily as well as see what is causing the MySQL server to be slow such as hitting disk.

​

\>  Is it worth to continue using mysql ? As one of my friend told me to migrate to Mongodb?

Your database is absolutely tiny, there is no reason to switch.  You'll run into the same issues with Mongo and then even more.

&#x200B;

\---

&#x200B;

Edit: Bad indexes would be adding indexes you don't need.  Also adding a ton of them in a table that is constantly written or updating indexed columns would make it much slower since every write/update would force the index to update.

Optimized queries is using the indexes in the proper order.  The faster you can wittle down the results the better.  For example if you have ""WHERE registered = 1 AND date\_add > YEAR(2018)"" with indexes on both, whichever would result in less should be the first.  Assuming the table has tons of records going back to 2010 one could assume checking by year would be better as the first WHERE check.",1535960146.0
dsn0wman,"You have a tiny amount of ram. Given the OS needs a certain amount to run, you don't have much left to help the DB. You really don't need to much for such a small database, but you will need some.  
  
Take a look at how your memory is being used. Particularly look to see if the system is paging/swapping , and what is causing the paging.     
    
If you don't see any paging you can safely increase innodb_buffer_pool_size. This is your database cache, and a larger cache can speed things along.",1535990135.0
davvblack,"""storing the image"" != ""storing the image name"" and you seem to use these interchangeably.  It does not matter too much if you store any of these three things:

    https://mydomain.com/pictures/posters/stupid_poster.jpg
    /pictures/posters/stupid_poster.jpg
    stupid_poster.jpg

Just think about what trade-offs you're making.  For example, if you want to serve some images from a different place:

    https://myOtherDomain.com/pictures/posters/stupid_poster.jpg

That's only possible if you store the full path. This is kind of flimsy, but if you wanted to move everything from 

    mydomain.com/pictures/posters/

to

    mydomain.com/static/assets/pictures/posters/

It would be very slightly easier if you just stored the filename. That said, it's possible to fix that in sql if you store the fullest path.
 
A final option i want to steer you away from is storing the image data directly in mysql as a blob,  It makes certain types of migrations easier, in return for just not using the correct tool for the job.",1535921870.0
r0ck0,"I've done both.  Stopped storing them in the DB like 15 years ago, so always just keep them on the filesystem now.  I don't want to worry about large database dumps just because of the decision to put images in there over some minor conveniences.

Also much less load on the server if nginx can just serve the file directly without needing to even execute another thread of application code and pull binaries out of the database etc.

Also much easier to do resize operations etc on.",1535944516.0
msiekkinen,"I'm not sure, you can get it independently here though https://dev.mysql.com/downloads/connector/j/",1535909269.0
jericon,"Your post has been removed.

Reason: Self Promotion

Your post history shows that you have a propensity to post from the same domain or topic often to a wide breadth of somewhat related subreddits without contributing to the communities otherwise.  Please see [Reddit's Self Promotion Policy](https://www.reddit.com/wiki/selfpromotion).",1537206318.0
amfournda,It should be: `mysql $database -u $user -p`,1535762315.0
jasunto,I just do ‘mysql -uroot -p’ and type password at prompt.,1535763312.0
nofxpunkguy,"mysql -u <yourdatabaseuser> -p -h <mysqlhostname or mysql IP address>

Keep in mind that in shared hosting the MySQL database is usually on a different server.

Once logged in do:
show databases;

And then:
use <databasename>;

Excuse the formatting. I’m on mobile.",1535763322.0
BrotoriousNIG,"Apologies for the lack of formatting; I’m on mobile.

Select ... from users where id in (select user_id from attributes where field_id = x and value = y);

would get you the users who answered y for question x. ",1535748610.0
ExiZ-Anon,"i have tried to reinstalling it and change screenresolution( using macbook pro retina), searched the web and settings.",1535706606.0
ExiZ-Anon,"update: made it work pushing the ""toggle of wrapping long lines"" in the bar",1535707644.0
jericon,"Your post has been removed.

Reason: Self Promotion

Your post history shows that you have a propensity to post from the same domain or topic often to a wide breadth of somewhat related subreddits without contributing to the communities otherwise.  Please see [Reddit's Self Promotion Policy](https://www.reddit.com/wiki/selfpromotion).",1537206562.0
Irythros,"For the ID column (if it will only ever appear once in the column in the table) and you want to have automatically set IDs, you want a Primary Key with Autoincrement.

​

To make sure the Post ID and Tag ID are both unique, you can set a unique key across both.

&#x200B;

Secondly when doing tags, you could delete all tags on a post when updating and then reset all tags again to ensure there is no duplication.",1535688678.0
movieguy95453,"Shouldn't your tag table include the post ID so it is easy to link back to the post? Without this ID, how are the post and the tags linked? Also, if a particular value is always the same, why is that value being stored - or are there other potential values?

I manage a movie theatre web side each movie has an ID which gets stored with the related information in other tables. For example, the table for movie dates includes a column for movie ID so I can easily identify which date(s) go with which movie. ",1535911657.0
razin_the_furious,"if you have access to javascript, you can make an ajax call to a PHP script hosted somewhere else that can access the mysql database, assuming you have access to another server where you can put PHP that can also talk to your DB",1535657987.0
Laurielounge,"Can't be done with mysqlimport. Could be done with mysql load data local infile, but as you're not able to logon to mysql from the commandline for some reason, you're a bit stuck.

You could do a little more work here... count the lines in the file. Count the number of records inserted. Are they the same? 

Think you said you had blanks in your file... is that right? You'd need to get rid of these first.",1535662041.0
kaydub88,"Set it up in VMs/VPSs. It's not really difficult, just follow the docs.

I worked somewhere that would try to get everyone MySQL certified. My suggestion is, don't do that. It's very rare to find MySQL DBA positions. People looking for these positions or trying to sell courses like to say that MySQL is the most prolific RDBMS, and it's true, but a huge chunk of the people using MySQL don't need DBAs.

You can learn everything you need to from a little homelab or an AWS account. Just go that route.",1535636950.0
MMOAddict,"The best way to learn something for me is to actually use that new thing in a project, on a test server.  It goes live eventually but not after a lot of testing. ",1535637832.0
Laurielounge,"Without knowing the source of the csv, two thoughts:

Dupes

Records which fail validation because of, say, a field being too long (no auto truncate).

It might become clear to you if you create a file with just the first 20 lines in it and see what happens. Most of those will likely fail. Working out why **they** failed is less daunting than the number in front of you currently.

Something you could also try - if possible, does --lock-tables change anything?",1535567227.0
Laurielounge,"Nope. Time to crack open the file and have a look. Could be an unescaped double quote, could be anything.

Drop it down to two lines - or one line if you take out the --ignore-lines - run it again, and see what happens.

How many fields in the table? How many commas in the line? Are they the same?

You'll be able to get your head around this once you look  at the actual file. And also, once you refresh your knowledge of what the table's requirements are.

Unfortunately, csv is not really a ""standard"" in the sense that ""all csv files are formatted this way"".",1535570315.0
Laurielounge,"So, it's running on your computer? Or are you shelling (ssh)

 into the machine remotely?",1535575508.0
Laurielounge,"No worries.

​

Did you try the ""file"" command and the ""less"" command? The answer to just about all of your questions lies in there.

What error did you get when logging into mysql? Did you try

`mysql -p`

It will prompt for a password...

Did you try (shudder)

`mysql`

On it's own? So, no password on that server?

Is that really the database name?  DATABASENAME\_wrdp1 ?

Did you try

`mysql DATABASENAME_wrdp1 -p`

Glad you're feeling more positive. Keep at it.

{EDIT} - hit q to get out of `less`",1535582306.0
LobbyDizzle,Right after the import run `show warnings;` and it should give you some hints.,1535615368.0
Laurielounge,"Yup.

​

Try this:

`wc -l /home4/public/wp_posts.csv`

That's a line counter (""-l"" means lines). It'll tell you the number of lines in the file.

But if you've got all '\\n', the file utility doesn't bother mentioning it.",1535660961.0
shady_mcgee,"I've done Postgres on top of OpenShift in AWS.  Does that count?

> I'm fairly confident that we can successfully run stateless processes in Kubernetes.

Totally doable

> However, I haven't been able to get good information on running
a relational DBMS in Kubernetes.

Also doable, but with a lot less benefits

> 1. how much Kubernetes expertise was required for your installation

Quite a bit.  At the very least you need a good understanding of Docker, building images, and how it uses persistent storage.  You'll also need to know how to use the kubernetes API to do the deployment.  

We used OpenShift, which is a (maybe?) superset of the kubernetes API.  One of the frustrating things was that every configuration is done via yaml, and the documentation was fragmented.  

If you were trying to add a setting to your Pod it was easy to find the documentation that would show you the snippet you needed, but not *where* in the overall yaml document that snippet would go, so there was a lot of 'Try it here.  Did it work? no.' -> repeat.  Once you figured out *where* the config was supposed to be it was easy, but building out a config the first time was a PITA.

Another thing is that (any maybe this was just our environment) all ingress into the pods was controlled by haproxy, which (as far as I know) only supports http/https protocols, so you could only connect to the DBMS from inside the openshift cluster.  There's probably some networking hacks you can do to expose the service publicly (similar to how haproxy gets exposed, but now you have a single point of failure), but we didn't explore it.

> 2. how much custom software you needed.

However much you need.  We were doing something fun using OpenShift's source-to-image capability, where the source could that built the resulting docker image contained all of the create scripts.  Building a S2I from scratch is also a not-well-documented PITA, but the end result was a capability that allowed for consistent and atomic deployments governed by version control.  

If you want to just build a docker image with mysql on it and run some load scripts you don't need much more than you do today

>""failure stories""

Not really a failure story, since the project worked, but there's not a whole lot of benefit in turning a DBMS into a pod.  We were deployed on AWS using EBS for storage.  The problem with EBS is that it only exists in one AZ, so if the AZ fails, your storage goes down with it.  What this means is that you don't get a whole lot of extra resiliency from turning your database into a pod, since you're still tied to an AZ.  You get a little bit of resiliency with a single node failure, but we experienced instances where the failed node didn't properly release the lock on the PV, which meant we had to manually go in and release the lock in order to get PostGres to come up on another node.  This bug might have been fixed in OpenShift 3.7, but I'm not sure.  Even if it's fixed, you still have the downtime while the other pod re-spins up your mysql instance.",1535567622.0
etrnloptimist,"First, you don't worry about performance. Worrying about that now is called premature optimization. Just do your queries, ""performance be damned"". I put that in quotes because it will be highly performant for a long time.

Second, once you start growing you will be in an infinite loop of: find bottleneck-->eliminate bottleneck. This is a good problem to have and if you are growing, it is an inevitable part of growing your service. Don't try to think you can ""get ahead"" of this by planning now. Its is premature and, like I said, inevitable.

Third, [read this article](https://stephenmann.io/post/whats-in-a-production-web-application/). It gives an excellent guide to how many web applications scale out, including ones I've developed as well. It was not written by me, but my experiences are inline with theirs.
",1535484096.0
Irythros,"\>  What is the best / optimal practice here?

​

Depends on the requirements.  For analytics, chances are you don't need accurate to time stats.  The difference between needing accurate stats to the last second vs accepting stats that are several minutes old can give you a huge amount of performance benefit.  Instead of calculating on load you would setup an automatic calculation and store that in a memory cache such as Redis.

In pseudo code and PHP:

`$cached = $redis->getStats($user);`

`if ($cached->expired)`

`$stats->markForUpdate($user);`

`$tpl->assign('stats', $cached);`

&#x200B;

then in the mark for update, set for however long you want stats to be stale for.  If time > stale, allow for update and change the cache.  We can get a little over 30 million hits per day and that's one of the easier optimizations.

&#x200B;

Another one which you should be taking advantage of is indexing and using those indexes for searches.  Using explain will show you if it uses any keys and how many rows it has to search.  The order of the where parameters does matter as well.  Try to narrow as quickly as possible.

&#x200B;

I would recommend using a tool called Percona Monitoring & Management.  It'll show you all the queries hitting the database along with time to execute and CPU required.  It'll be easy to find non-performing queries to optimize.",1535496500.0
clutchguy84,"You'll have to set up a db instance in AWS.  That information can be found [here](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateInstance.html).  

Then once that's done, you have to go into the XAMPP control panel: Apache->config->config.inc.php.  That file holds all of the config info for your db instances in phpmyadmin.  It will look like this:

    /*
     * First server
     */
    $i++;
    
    /* Authentication type and info */
    $cfg['Servers'][$i]['auth_type'] = 'config';
    $cfg['Servers'][$i]['user'] = 'root';
    $cfg['Servers'][$i]['password'] = '';
    $cfg['Servers'][$i]['extension'] = 'mysqli';
    $cfg['Servers'][$i]['AllowNoPassword'] = true;
    $cfg['Lang'] = '';
    
    /* Bind to the localhost ipv4 address and tcp */
    $cfg['Servers'][$i]['host'] = '127.0.0.1';
    $cfg['Servers'][$i]['connect_type'] = 'tcp';
    
    /* User for advanced features */
    $cfg['Servers'][$i]['controluser'] = 'pma';
    $cfg['Servers'][$i]['controlpass'] = '';
    
    /* Advanced phpMyAdmin features */
    $cfg['Servers'][$i]['pmadb'] = 'phpmyadmin';
    $cfg['Servers'][$i]['bookmarktable'] = 'pma__bookmark';
    $cfg['Servers'][$i]['relation'] = 'pma__relation';
    $cfg['Servers'][$i]['table_info'] = 'pma__table_info';
    $cfg['Servers'][$i]['table_coords'] = 'pma__table_coords';
    $cfg['Servers'][$i]['pdf_pages'] = 'pma__pdf_pages';
    $cfg['Servers'][$i]['column_info'] = 'pma__column_info';
    $cfg['Servers'][$i]['history'] = 'pma__history';
    $cfg['Servers'][$i]['designer_coords'] = 'pma__designer_coords';
    $cfg['Servers'][$i]['tracking'] = 'pma__tracking';
    $cfg['Servers'][$i]['userconfig'] = 'pma__userconfig';
    $cfg['Servers'][$i]['recent'] = 'pma__recent';
    $cfg['Servers'][$i]['table_uiprefs'] = 'pma__table_uiprefs';
    $cfg['Servers'][$i]['users'] = 'pma__users';
    $cfg['Servers'][$i]['usergroups'] = 'pma__usergroups';
    $cfg['Servers'][$i]['navigationhiding'] = 'pma__navigationhiding';
    $cfg['Servers'][$i]['savedsearches'] = 'pma__savedsearches';
    $cfg['Servers'][$i]['central_columns'] = 'pma__central_columns';
    $cfg['Servers'][$i]['designer_settings'] = 'pma__designer_settings';
    $cfg['Servers'][$i]['export_templates'] = 'pma__export_templates';
    $cfg['Servers'][$i]['favorite'] = 'pma__favorite';

Just copy that block of code, and add the relevant information from your AWS db instance.  For example:

    $cfg['Servers'][$i]['user'] = '<your_db_user_name>';
    $cfg['Servers'][$i]['password'] = '<your_db_password>';
    /* Bind to the localhost ipv4 address and tcp */
    $cfg['Servers'][$i]['host'] = '<the url to the db provided to you from AWS>';
    $cfg['Servers'][$i]['connect_type'] = 'tcp';

&#x200B;",1535485459.0
msiekkinen,What does your mysql.cnf file have the socket filename and path set to?   Ensure that socket:// path/filename is being set in your PDO connection arguments. ,1535400008.0
ge0n1,"You need some linux/unix basics. Do you have a server or are you still using th noob stuff?

find / -name 'my.cnf'",1535414080.0
edisonlbm,"Another option would be to use [MySQL Workbench](https://en.wikipedia.org/wiki/MySQL_Workbench), which is a GUI tool similar to the MySQL command line client.

Either way, phpMyAdmin is a really useful tool for small DB work, but tends to get overwhelmed with even moderately sized tasks.",1535290647.0
jericon,"Mysql is probably timing out the connection. 

Your best way would be to use the command line interface. ",1535271144.0
tux0010,I think you could scp over the file to your box hosting the server and run the mysql client there to load the file (in a screen session)?,1535485360.0
jericon,"The “Bible” for mysql is “High Performance MySql”. 

It’s a little outdated, but it is still very relevant. 

High Performance MySQL: Optimization, Backups, and Replication https://www.amazon.com/dp/1449314287/ref=cm_sw_r_cp_api_xKSGBbNGMEGND",1535297876.0
davvblack,"This debate is called ""natural vs surrogate key"", there are lots of articles on the topic.  In general, you want the primary key to be relatively small (since it's used in other indices), which would point towards in int, but any query on the primary key itself is more performant, which benefits using the natural key, the date field.

There's no strict best practice here, just some tradeoffs to keep in mind.  Surrogate key is a little more robust to other changes (like for example if you decide to capture two datapoints per day down the line).",1535232402.0
thelongbeachtech,Why would you?,1535232303.0
wampey,Char data type? https://dev.mysql.com/doc/refman/8.0/en/char.html,1535209117.0
CrumpleZ0ne,[Add a trigger to validate the field before inserting](https://dev.mysql.com/doc/refman/8.0/en/trigger-syntax.html),1535210884.0
Xydez,Thank you everyone! I solved the problem by using a `binary(32)` to store my password hash.,1535215964.0
SomeGuyNamedPaul,What's in that error file in /usr/local/var/lib/mysql?,1535110747.0
xXxLinuxUserxXx," `2018-08-24T08:43:06.380616Z 1 [ERROR] [MY-012526] [InnoDB] InnoDB:  Upgrade after a crash is not supported. This redo log was created with  MySQL 5.7.18. Please follow the instructio$` 

Try to start the old mysql version if it has to cleanup something and shut it down cleanly. Then take a look at mysql\_upgrade. After that the new mysql version should be working normally.

If not, start again the old mysql version, export all data (mysqldump). Shut it down, move old datadir to backup and start new mysql server and import the mysqldump.",1535114365.0
galaxycube,"I've had this! On a production machine no less!

It was caused by an incorrect shutdown / read write issues to the disk and corrupted the innodb table.

More than likely ibdata file is corrupted.  If you have a back up uninstall mysql and restore from backup.

Check for incorrect shutdowns and the health of your harddrivd/raid system.",1535179771.0
SomeGuyNamedPaul,I wouldn't think about scaling until you've got maybe a few hundred million orders to play with.  Just do the basics like have compact primary keys on every table and do not use decimal instead of floats for money or else you *will* regret it.,1535073865.0
cYzzie,"Nowadays its most likely better not to use different databases or tables per restaurant as sharding and partitioning scale better than many databases or tables

Ten thousands of entries are nothing, my appointment table for just one company gets more than 10k records per day, my production log about 200k rows per day - this is not amounts where much thought on scaling is necessary appart from thinking about disk types and sizes",1535053974.0
Ordinathorreur,"Scaling a database is exactly that, you scale to the needs you have now and in the near (weeks, months) future. It’s pretty normal to move your data and structure around when you need to. Don’t prematurely optimize for what you think you may need; it’s unlikely that you’ll accurately predict your future needs, so you’ll still have work to do when you actually need to scale. Cross that bridge when you get to it.",1535064023.0
ge0n1,"Dear God. ""Designing""

There is no concept of 'mini table' in mysql. Just tables, rows, and columns.

You are going to fuck this all up lol. I hope no one is paying you :P

&#x200B;

Anyway, Tables you want:

&#x200B;

customers,

paySources,

orders,

orderItems,

transactions,

&#x200B;

\*\*customers\*\* is optional depending on the situation, but if there is carryout it is not! 

rows:  customerId(unique), address, first\_name, last\_name, pay\_source\_id

&#x200B;

\*\*orderItems\*\* is going to be unique menu items and have 

rows: ItemId(unique), price, title, description

&#x200B;

\*\*orders\*\* a collection of order items associated with a transaction and customer

rows: orderId (unique), customer, transaction, pay\_source, items, 

&#x200B;

\*\*transactions\*\* actions performed with paysources (void, sale, auth, refund, capture)

rows transactionId(unique), pay\_source, order\_id, customer Id

&#x200B;

\*\*paySources\*\* paySourceId(unique), first\_name, last\_name, exp, card\_number

&#x200B;

Really doing ANY of this from scratch these days is silly without a large team. You should look into open source CRM frameworks out there and just leverage what you can. Run the thing as a SaaS and then you aren't selling the open source code (because you can't), you are selling the \*\*service\*\*  


You are up against much competition, at least in the west. Good luck!",1535055753.0
,[deleted],1535050138.0
r3pr0b8,"you want CASE instead of IF

also, what were you doing in those ABS functions/

try sticking 4 blank spaces in front

so, not like this --

... abs(var3/var4*var5)

but like this --

    abs(var3/var4*var5)
",1535043028.0
mcstafford,"You probably want to look at something like [load data](https://dev.mysql.com/doc/refman/5.5/en/load-data.html) or  [load xml](https://dev.mysql.com/doc/refman/5.5/en/load-xml.html).

In my experience it's more about your workflow than how to get it in to a table.",1534967408.0
DataVader,"SELECT CAST(yourBlob AS CHAR) as theMightyText
FROM yourTable;",1534946360.0
hangfromthisone,"I don't know how deep you are into your project, but what you are describing sounds a lot like NoSQL. Check elasticsearch and similar (I have close to zero knowledge on NoSQL)",1534955595.0
justintxdave,"The question for you is how do you want to search on that data?  'image', 'text', 'xml' by each or in a combination?  Do you need them in that order or some other order?",1534944786.0
mtocker,"The MySQL team describes this as a multi-value index.  It is currently not supported, but there are plans to do so.",1534958222.0
,[deleted],1534976515.0
Laurielounge,"Well, hard to say. Horses for courses.

&#x200B;

One way to think about them is:

Views behave like a table all of the time.

Temp tables don't behave like a table until they're instantiated - in other words, they don't exist until you create them, and then they're gone. So a bit trickier to fiddle with when things go wrong.

Temp tables might be kinda easier to index though.

The joins thing doesn't really matter so long as you've got indices in the right place in the view/temp table.",1534919437.0
r0ck0,"> Can we have too many views?

Not really.

Overall your questions are kind of hard to answer generally with conclusive answers, it depends on what they're being used for.

As a general practical guideline in terms of not wasting time on things you probably don't need...

* Start off only using views - they're much simpler, and ""always up to date""
* If there's performance issues, then look into temporary tables, but don't worry about it until then

For most typical webdev type projects you probably won't need temp tables.  The only time I've used them was in a machine learning system doing lots of calculations (billions).

> Temporary tables is easier to migrate?

Views are pretty much easier all round, because they're basically just shortcuts to an SQL query.  

Whereas with temp tables you're creating, deleting and updating redundant copies data.  
",1534933839.0
NotTooDeep,"Another trade-off: views don't impact incremental backup and restore ops. Temporary tables eliminate the possibility of doing an point-in-time restore of the database.

Found this out the hard way. In short, mysql cannot determine if transactions have completed that require the use of temporary tables when it detects their presence, so it aborts the restore. At least, that's my experience in AWS.

The error thrown is something like RDS does not support point in time restores for databases that use temporary tables.

Unfortunately, this is not an AWS RDS restriction; it is a MySQL restriction.  Here's the explanation we were given by AWS support:

""Temporary tables created by customers using CREATE TEMPORARY TABLE statement are memory based tables and are NOT written to the disk. The temporary tables created by MySQL as part of sorting operations spill over to the disk if they exceed a certain size. These are internal temporary tables created and used by MySQL. The temporary tables created by customers are not similar to the ones created by MySQL and therefore they are NOT written to disk (so they reside exclusively in memory). Due to the usage of temporary tables the backup taken of your DB instance might fail and you can experience data loss when performing PITR or restore operations.""

Relevant docs:

[1] https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html#Overview.BackupDeviceRestrictions
[2] https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_BestPractices.html#CHAP_BestPractices.MySQLStorage
[3] https://dev.mysql.com/doc/refman/8.0/en/create-temporary-table.html
[4] https://dev.mysql.com/doc/refman/8.0/en/myisam-storage-engine.html
[5] https://dev.mysql.com/doc/refman/8.0/en/memory-storage-engine.html
[6] https://dev.mysql.com/doc/refman/8.0/en/innodb-storage-engine.html",1534942811.0
mtocker,"Temporary tables always materialize, so they will usually perform worse (there are some edge cases were materializing part of a query up front can be faster.)

But Views must be defined up front.. so they are not quite interchangeable.  The real question is temporary tables **vs. CTEs.**  A CTE is like a view but for query level duration.  An example:

`WITH v1 AS SELECT * FROM tbl WHERE a='xxxx';`

I recommend CTEs (a MySQL 8.0 feature) over temporary tables.",1534957867.0
thatsricci,"if it MUST be contained in one query then you can use conditional IF's in your 'ON DUPLICATE KEY UPDATE' statement:

[https://thewebfellas.com/blog/conditional-duplicate-key-updates-with-mysql](https://thewebfellas.com/blog/conditional-duplicate-key-updates-with-mysql)

e.g.:

 INSERT INTO t\_a (A,B,C,time) VALUES (1,2,3,'2018-08-21 09:47:26') ON DUPLICATE KEY UPDATE B = IF(time < VALUES(time), VALUES(B), B), C = IF(time < VALUE(time), VALUE(C), C)

Otherwise, I would do a Select in my code (php/python/perl whatever)... if it's newer or doesn't exist,  write the insert/update from my code.... is a bit easier to decipher and keeps your sql simpler. ",1534876556.0
apaethe,"I would make a table for your new data and load the new data into there using LOAD DATA INFILE.

Then run the INSERT ON DUPLICATE KEY UPDATE comparing those two tables.

Indexed correctly it should be pretty quick.",1534976695.0
nivenkos,"You have to use 2 queries.

To insert missing ones:

    INSERT INTO table (    
    SELECT j1.* FROM    
    (SELECT .... )j1    
    LEFT JOIN table    
    ON (j1.id = table.id) WHERE table.id IS NULL );     

Then to update existing ones (doesn't retrigger on inserted due to using > not >= )

    UPDATE table SET  B=j1.B ...    
    FROM    
    (SELECT .... )j1    
    WHERE j1.id = table.id AND j1.time > table.time;    

",1534874884.0
davvblack,"Read about loops in MySQL:

[http://www.mysqltutorial.org/stored-procedures-loop.aspx](http://www.mysqltutorial.org/stored-procedures-loop.aspx)",1534825399.0
mtocker,"MySQL 8.0 supports auto-sizing configuration for this use case: [https://dev.mysql.com/doc/refman/8.0/en/innodb-dedicated-server.html](https://dev.mysql.com/doc/refman/8.0/en/innodb-dedicated-server.html)

I recommend this over tuners.",1534803477.0
faxattack,"https://tools.percona.com/

Requires a free account but has a pretty decent wizard. ",1534842407.0
JackTheSpot,"Could solve it as:

`SELECT 
R.Date,
R.VES,
R.ConcorrenteVES
FROM rates_history R
INNER JOIN 
(
  SELECT 
   MAX(date) max_time
  FROM rates_history
  GROUP BY Date(`date`)
) AS t
ON R.date = t.max_time`",1534771196.0
,[deleted],1534737927.0
thatsricci,"How many posts are you anticipating?   Just have a table called ""replies"" with a FK to a posts table like post_id....",1534741345.0
NobarTheTraveller,With which command do you access mysql from the CLI?,1534718902.0
jericon,Most of the time that I see this it’s timeout settings. ,1534582989.0
Texas-Wanker,"So, I setup another mysql server on the webserver vm. It has the exact same issue when accessing through localhost.

Are there any logs that could tell why it fails? Like, if it’s a timeout, what it was waiting for?",1534610093.0
de_argh,increase your max_allowed_packet setting high.  what is it set at now?,1534611233.0
thatsricci,Is it on a remote system?  Firewall could be the culprit.  Try and connect to it locally (via port not socket) to rule out anything between.,1534615333.0
Id_Panda_Dat,It went on vacation.,1535329148.0
,"SELECT users.username, SUM(points) AS points FROM users INNER JOIN (SELECT user, MAX(points) AS points FROM submissions GROUP BY problem, user)xxx ON users.id=user GROUP BY users.id;",1534540985.0
davvblack,"does tasks.max\_points mean the max potential points you can earn from that task?

I think you want \`select max(points) from submissions group by user\_id, task\_id\` to find the scores to use. Then you can SUM that table up to find total scores by user or whatever you mean to do with it.

Also please tell me you are not storing raw passwords in the database?",1534537381.0
AllenJB83,"MySQL shouldn't normally get into a state where it's totally inaccessible.

The most obvious possible cause I can think of: Do not use superusers for everything. Only humans should access the server using superuser accounts. This is because superusers get reserved connections to allow for administration if all the normal connection slots are used up.

Properly configure your server. Unless you're using MySQL 8 with the innodb dedicated server option enabled (and MySQL is the only notable thing on the server), it probably isn't well configured. There's plenty of guides on how to do this as well as tools that will give you recommendations based on your system (google ""mysql tuner"").

If you want to monitor your server on an ongoing basis, I can recommend Percona Monitoring & Management.

For reading material, I recommend [High Performance MySQL](http://www.highperfmysql.com/) (while it is written against 5.5, the vast majority of it is still relevant)",1534489672.0
galaxycube,Whats the disk io like?  Ive only had this issue once before and i only have like a gig of ram on my low end set ups.  Turns out it was the disk not providing enough read / write capabilities.,1534530725.0
SecretAgentZeroNine,https://www.w3schools.com/nodejs/nodejs_mysql.asp,1534472984.0
r3pr0b8,"the highest normal form of the data given is SDNF (snoop dogg normal form)

the student is snoop dogg, and the advisor is bob marley

pass de dutchie pon de inner join side 

 ",1534454861.0
davvblack,"Can I see the output of EXPLAIN Query B?

I can think of two things that would potentially do this:

1. The query optimizer is confused by the cardinality of the tables involved  and uses the wrong index/does the join backwards.  (ultimately select \* WHERE IN optimizes down like an inner join). ""OPTIMIZE TABLE"" may fix this, or forcing indices.
2. The inner query may be becoming treated as a dependent/correlated subquery, especially if it has any column names in common with the outer query.  What this may mean is that the inner query is run once in its entirety for each row of the outer table.  A guaranteed way to fix would be to select Query A into a temporary table, then just join it to ilcs for the final result.",1534452873.0
yhetti,"Clarification: MySQL is neither.  MySQL is really an SQL front-end for a plugin-based set of database storage engines.  So InnoDB, the default storage engine MySQL uses, is row-oriented. However, InfiniDB (now renamed ColumnStore by Maria) is column oriented and runs behind the MySQL frontend.",1534438215.0
etrnloptimist,row-oriented,1534427558.0
brandondunbar,"A lot of promises, can anyone vouch for this course?",1534424844.0
mischiefunmanagable,"whats explain give you for that query?

product_s_desc  have a fulltext index?",1534425027.0
apaethe,"Yea, sure thing.

    select t.*
    from thisTable t
    join (
      select t.item_nmber, max(t.timestamp) as latest_time 
      from thisTable t
      group by t.item_nmber
    ) latests
    on latests.item_nmber = t.item_nmber
    and latests.latest_time = t.timestamp

Basically find the latest time of the table per item and rejoin that on the table to get only the latest rows.

Index the table with a two column index ( item_number, timestamp ) or you will make the baby jesus cry.",1534377463.0
LordZer,"The POST data needs to be turned into a variable so that instead of 'startdate' you have $startdate

    $startdate = $_POST[""startdate""];    ",1534376054.0
whatalegend89,Select count (*) from table where column = ‘value’,1534366064.0
mischiefunmanagable,"what makes you think it isn't working?

you're telling us what you're doing but not what you're doing afterwards that makes you think it isn't working

good odds that since you're setting it as a session variable that phpmysql isn't keeping the session between queries

what does show variables like 'auto_increment_increment'; return?",1534370967.0
gizram84,"This value determines how many numbers to skip when incrementing an auto_increment field.

So with auto_increment_increment=5, So if your first entry is 1, then your second should be 6, then 11, then 16, and so on.

You should really set this in your configuration file.

But it looks like you're setting a user-defined variable, not the actual mysql setting to do what you want.",1534386289.0
r3pr0b8,">  I need to list employees, grouped by timesheets with the time for each one and each day added up and divided by 3600

you forgot the GROUP BY clause

    SELECT employee_id
         , timesheet_id
         , reported_date
         , SUM(duration) / 3600 AS time_worked
      FROM hs_hr_time_event 
     WHERE employee_id BETWEEN 4000 AND 4003
    GROUP
        BY employee_id
         , timesheet_id
         , reported_date",1534347404.0
jajajajaj,"I don't think they deprecated SQL connections, did they? I haven't been doing php for a while, and just googled it but i  can't be sure what you meant. I do know they at least one set of functions, quite some time ago, though. The original mysql\_\* functions were a dangerously bad design, but there are a couple newer approaches. They just have different names. I think it's mysqli_* and dbo or something.

To answer your question, I have never heard of anything like this, and I believe there must be some other factor confusing things. If you're in doubt that Oracle isn't messing with you, try installing Maria instead of mysql, if that's not too disruptive to your plan.",1534345242.0
razin_the_furious,"PHP did not deprecate mysql connections: PHP 7.0 removed the mysql\_* functions, but the newest version of wordpress does not use those functions, otherwise it wouldn't run at all. PHP 7.0+ wouldn't allow you to turn back on the mysql\_* functions anyways: they literally aren't there anymore.

You said the service is on ""auto"". After a reboot, can you open a telnet connection to 3306 on 127.0.0.1? That would confirm the service is running and listening.  

I'm wondering if workbench is starting the service for you when it's opening somehow.",1534346306.0
,[deleted],1534342295.0
cYzzie,"Whats the reason you dont use ELK for that? Its a much more suitable stack

Otherwise you could add a date dimension column and partition by it, but mysql will always be week for logging compared with logstash and a suitable backend like elastic",1534357774.0
,[deleted],1534342999.0
SomeGuyNamedPaul,"You might want to look at normalizing some of these columns.  Also, few people seem to remember they an IP address isn't actually a varchar(15), but it's really an unsigned 4 byte integer.  Please treat it as such, or normalize that as well since you'll probably be normalizing other columns, and likely have host info in another table anyway.",1534387375.0
dohako,"You really should worry about the write performance over the read performance. 2 minutes for a query on a massive table is nothing. But if a production server experiences a delay or deadlock due to an index still updating you may have the real mess. 

So partitioning as you mentioned by server may be a solution, but better could be partitioning by timestamp eg. 31 days for the month or 24 tables for each hour of the day could be more efficient as I guess you typically know where you need to look timewise. 

You could also move the transactional data into a fully indexed table for analysis the next day if what you query on is not time critical. If that's the case you could potentially get away with just two tables: One for even and one for odd hours. 

If you need more realtime logging, I think you have the wrong tool, using a system like elastic search or hosted solutions like splunk, loggly or greylog may be your best choises. Or at least a non-transactional database which does not need to index on the same transaction as the insert. ",1534359346.0
justintxdave,You are on the right track. JSON\_TABLE does let you turn the schema less data into temporary structured table for occasions like this.,1534333687.0
jericon,You’d have to break it out into individual columns to get that data. ,1534323047.0
Laurielounge,"For the lurkers, the solution:

`SELECT tranno,` 

`row_number() over(partition by tranno ORDER BY tranno) as line_no,` 

`prodno,` 

`qty,` 

`price from blah`

Works like a boss.",1534365898.0
IUseRhetoric,"There's actually a third choice, which is ""don't set up a database in Docker"".  Then behaviour-wise you'd treat it like your second choice.  

An argument in favour of the first choice is that you shouldn't be mixing databases from different services.  You'll be minimizing the memory utilization but with two or more clients using different schemas heavily you'll end up thrashing the database's buffers and caches, and thus affecting performance.  And if you don't have enough data to cause thrashing, then your separate database containers wouldn't be using that much memory in the first place.  Plus having separate containers would make it easier to clean up if you decide to remove or migrate the service.  Each database has its own storage, and you won't need to dump/reload the schemas and users, just move the at-rest data. ",1534284802.0
ncsupheo,"Consider that the idea with docker is to operate on services as explicit and  immutable artifacts. So if you have a service that requires a SQL datastore, you can ship mysql as one of the container dependencies.

SQL DBs however tend to be difficult to scale horizontally, especially in an automated way. There is the [autopilot pattern](https://www.joyent.com/blog/dbaas-simplicity-no-lock-in), but even it's not simple.

If you `docker-compose scale n=5 <my sql service>`, what does that mean for your application? Is your DB read only, and freely scalable? Is it okay if the individual instances fall out of sync? I would guess not. Typically, you want scaling _and_ consistency. SQL DBs tend to be stateful, and it's not an easy problem to spin up a consistent cluster in an automated way.

Something like Cassandra fits the ""Cloud DB"" model better, and you can query it with SQL. You can even [automate it](https://medium.com/merapar/deploy-a-high-available-cassandra-cluster-in-aws-using-kubernetes-bd8ba07bfcdd), but it requires orchestration tooling beyond docker itself.

I think if you are working an application with a stable architecture, and you need a consistent DB, you should consider treating it as a part of the infrastructure until you need more from it.",1534294934.0
magicalakrus,"Well, Unix timestamp is seconds, so you can just do unixtimestamp-31557600 (if you don't care about leap years).",1534252422.0
hangfromthisone,I'm pretty sure SUBDATE works with timestamp and not epoch time. So you should also use FROM_UNIXTIME before attempting to substract dates,1534253390.0
NotAnExpertWitness,Why are you trying to subtract the year in the where clause?  You have the modified year in the select.,1534257503.0
razin_the_furious,Can you show the current create table syntax?,1534254650.0
mcstafford,"Which of those ASCII-looking characters are you calling special?

I'm guessing that you expected to get just 'AAAA' and aren't accounting for either rows , or columns.",1534172731.0
beermad,"Your example doesn't make it clear, but my guess is that your Python script is using a different character set to what the database table is set to.

I've had a similar problem with a Perl script which was accessing a UTF-8 table but wasn't set up to handle UTF-8 characters. I assume Python must have some similar directive to what I do in Perl:

`use utf8;`

`use open ':utf8';`

`binmode STDOUT, "":utf8"";`

`binmode STDERR, "":utf8"";`",1534177504.0
thebardingreen,"Does user not have an INT Primary Key like `id` or `id_user`?  You should be using that as the foreign key in the followers table.  If your username field is actually your key is the user table, MySQL won't like that.

This error generally indicates that MySQL can't verify the uniqueness of the column you're trying to use as a key.",1534129798.0
NotAnExpertWitness,"Master Master ""circular replication"" only works on paper.  You can do Master/Master if you make one of them a read-only master and do all the writes to the other one.

To get started, look into xtradb backup ""innobackupex"", you can use this tool to snapshot a live master while its running.  Then you move the snapshot and load the new replicant with it.",1534103126.0
eredditan,"Thank you all for the reply. I understand now that group replication need innodb engine and some of my tables are using myisam because of incrementals feature.

I looked on circular replication but if i have three nodes :
A master of B
B master of C
C master of A

what happens if B fails ? there is no replication any more ? i didn't see anyone mentioning it that way :()",1534183235.0
NotTooDeep,"So you have a single entity, which is people. These people you may call users or whatever makes sense for your social media application.

Users can do stuff, like create content. Users can follow other users because they like the content they create.

Your question is how to track which users are following any given user and who are all the users that a given follower is following. This is the classic many-to-many relationship.

The common answer is a simple relationship table (AKA bridge table or intersection table). This is how all many-to-many relationships are resolved in relational databases.  

Related and more Typical Example: An employee can work on one of more projects. A project can have one or more employees assigned to it.  This many-to-many relationship gets resolved in a third table, called something like employee_project. The PK from the employee table points to a FK in the employee_project table. The PK from the project table points to a FK in the employee_project table. Joining the three tables, you can filter on a single project to see everyone assigned to it, or filter on a person to see all the projects to which they are assigned.

The difference in your case is there are just two tables; user and follower.  A row in follower consists of a minimum of two columns named user and follower. Both columns are FKs from the PK of the user table. It's the same principle as the example except the many-to-many relationship is folded back onto the one user table. You can filter on a follower.user to find all of the followers for a single user, or you can filter on a follower.follower to find every user that person is following.

This model works well because it is normalized. There is never more than one row for a single user, which means it's simple to maintain the data for that user. A single instance for a user means the data is either right or wrong; it can never be out of sync with itself. This is a key test for normalization and the key benefit. A side effect of having one and only one place where a unique piece of data is stored is storage is smaller, and smaller storage begets faster performance.

A useful enhancement to the follower table would be start and end dates; this would answer questions about how long followers follow a particular person, etc.",1534046886.0
jynus,"Could you provide the current structure or at least one table as example. E.g. Zero data on a DATE, DATETIME, TIMESTAMP data type? You can check more about defaults at: https://dev.mysql.com/doc/refman/8.0/en/data-type-defaults.html

> should I be concerned about causing conflicts by converting all tables to InnoDB from MyISAM?

Yes and no. MySQL would be logically the same (e.g. syntax), but if you relied on broken non-transactional functionality, or very specific MyISAM behaviour, things could be different. For example, this is one subtle difference in configuration that your DBA should be aware of: https://dba.stackexchange.com/questions/76653/match-against-one-character-words-returns-empty-rows-with-ft-min-word-len-1/76666#76666 but you would need extensive converage testing to identify that. For most cases, InnoDB would do the sane thing, though, like not get completely corrupted on crash.",1534027790.0
AllenJB83,"Please add the table schema (CREATE TABLE, including indexes) for the tables involved in this query, and the output of ""EXPLAIN <query>"".

It would also be helpful to know what distribution (Sun/Oracle, MariaDB or Percona)  and version of MySQL you're using.",1533984768.0
p1x3l3d,"Hi :) I'm assuming this is true, but asking doesn't hurt:

Are the fields `machineIpId` and `machineId` in their respective tables `pinger_machine_ip_addresses` and `pinger_machines`,  also keys/indexes?",1534000118.0
liberaltech,"If you’re looking for fake data then look into Faker, it generates fake data for you. 

Php: https://github.com/fzaninotto/Faker/blob/master/readme.md#installation

Python: https://github.com/joke2k/faker/

Ruby: https://github.com/stympy/faker

Javascript: https://github.com/Marak/faker.js/blob/master/Readme.md",1533911770.0
Irythros,"INSERT INTO \`users\` (\`column1\`, \`column2\`) VALUES (1, 'name'), (2, 'name2'), (3, 'someName')",1533911572.0
hangfromthisone,"Disable keys, bulk insert, open beer and relax",1533919588.0
r3pr0b8,"yeah, you cannot use the LIKE wildcard inside the REPLACE function

try `REPLACE(definition_raw, '#', '# ')`

thing is, this will replace hashes inside the definition too, not just at the beginning",1533888291.0
Ipecactus,I think it's a good idea.,1533863607.0
doenietzomoeilijk,"I think it's a great idea, but unless there's a couple of people actively perusing the questions and deflecting the more silly ones, we won't see too much improvement. The ""I get a connection error, please fix this for me"" crowd will not read a sidebar note, IMO.",1533910828.0
BrotoriousNIG,UPDATE mysql.user SET `password` = PASSWORD(‘new password here’) WHERE `user` = ‘username here’ AND `host` = ‘host here’;,1533864168.0
payphone,"MySql used to use a weird regex library. Check out this link and scroll to ""Regular Expression Compatibility Considerations"". [https://dev.mysql.com/doc/refman/8.0/en/regexp.html](https://dev.mysql.com/doc/refman/8.0/en/regexp.html)

Also the accepted answer here: [https://stackoverflow.com/questions/18317183/1139-got-error-repetition-operator-operand-invalid-from-regexp](https://stackoverflow.com/questions/18317183/1139-got-error-repetition-operator-operand-invalid-from-regexp)",1533867652.0
jous,"Man we definitely should have the rules in the right bar. If you do not give the full query and some sample data it is unfathomably difficult to help you. 

In this case you should at least give the full regular expression that you're using instead of the error message.  This post will also be buried when you possibly/eventually give the necessary information.",1533861279.0
tektektektektek,"Please surround in-line code in questions with backticks. Otherwise the [Markdown formatter](https://en.wikipedia.org/wiki/Markdown) gobbles up very important characters from your regular expression such as the asterisk and caret characters!

e.g. *with* backticks: \`\^\s\*\S\*\` looks like `^\s*\S*`

*Without* backticks: \^\s\*\S\* looks like ^\s*\S*
",1534032731.0
payphone,"Documentation is ridiculous, but maybe try this, found it here: [https://dev.mysql.com/doc/dev/connector-nodejs/8.0/module-Table.html](https://dev.mysql.com/doc/dev/connector-nodejs/8.0/module-Table.html)

It might be the order of the chain of commands

    table.update()
        .where('`user`=:preuser')
        .bind('preuser','ADMIN')
        .set('column', value)",1533870850.0
jynus,"> However with windows server the size of db increases post optimisation

You should share the version, and configuration (specially InnoDB's configuration), as well as a summary of the size of the files within your datadir. Without that information it is difficult to establish a cause. MySQL on windows should be mostly equal than on Linux, with the exception of some minor missing features. You should provide more details of what you are doing and what you expect vs. what you are getting to be able to advise you propery.

E.g. maybe your optimization is happening in a single transaction, while large number of writes create a large UNDO space, making the space problem worse. The size of tables may not shrink, but it should not increase under normal circumstances. The details are important.",1533826467.0
BrotoriousNIG,"So the issue seems to be, that these dates are being stored in colonial format in a VARCHAR, rather than a DATETIME, which would handle format for you.  I’m not sure about the 22:00 thing, but I wouldn’t write off it being some separate instance of silliness, that is coinciding here.

I would migrate the data into a new DATETIME field, because this is utter nonsense. Obviously you’ll need to evaluate the impact on your application, how well it will be able to play ball, and how easily you’ll be able to adapt it to use the new field, if at all.",1533761837.0
crackanape,"'04-17-18' is not a valid date.

Do you mean 2017-04-18?

Or maybe 2018-04-17?

Anyway, follow the standard format for dates (YYYY-MM-DD) and it'll work fine.",1533765002.0
trippyd,"https://dev.mysql.com/doc/refman/8.0/en/datetime.html

The mysql date and datetime formats look like this:

`2015-02-03` for `DATE`

`2015-02-03 13:44:21` for `DATETIME`, the errors are correct.  

Also, you probably don't want `update_date` to be `varchar(100)`.  You either want `TIMESTAMP` or `DATETIME`.  Both are far more efficient ways of storing a datetime.   `TIMESTAMP` is functionally a 32 bit INT, so 4 bytes per record, and `DATETIME` is about 5 bytes per record.   

Details:  https://dev.mysql.com/doc/refman/8.0/en/storage-requirements.html

edit:  more details.",1533761769.0
jynus,"Complex replication filters and processes are a nightmare to maintain. Assuming the purging process is relatively simple, and not error prone (e.g. delete records where certain timestamp field is lower than some value), and that consistency for old records has to be exact (because the records could or could not exist), I would recommend running the purging on each non-Archive DB MySQL instance independently, but using SET sql_log_bin=0, so the purging doesn't get replicated. Don't do this process on the archiving DB.

If this doesn't work for you, or you have specific needs, there are other tricks to make this work, like running in STATEMENT mode a particular purging stored procedure that is null on the archive db, or some other tricks, but they are also more tricky to maintain. The problem with trying to maintain replication-based differences is that they always come back and kick you because data differences (e.g. new data being generated based on existing data, which breaks consistency). I would recommend to backup the data on a separate medium/storage rather than implementing it purely on storage side.

We use on certain dbs ROW-based triggers (only on MariaDB 10.1+) for replication-only sanitization, but I would not recommend them to everyone.",1533743669.0
ElMatze79,"It depends if you need the data on your master. If you need it only on your archive, you could use the BLACKHOLE engine everywhere else. This would prevent storing the data at all, but it goes to the binlog. On one slave db you can use the ARCHIVE engine and have the data only there. You don't need to delete the data on the master in that case.

If you need that data on the master for some time, you can go for a hacky solution with 2 tables:  
1. Table that holds the data for the master, on your archive you don't replicate it (replicate-ignore-db)  
2. Log table with BLACKHOLE on master  
When it's time to clean up, you copy the data to the log table first and that then goes to your archive. After that you delete it from the original table.  
Be aware that in this case you only get the data to the archive during cleanup runs.

If you want to give this setup a try, I strongly recommend to do many tests, because this is something not widely used.",1533803031.0
jericon,Remove the comma after RECEIPT.PO,1533686988.0
pinkdinosauronbowtie,"I'm not getting receipt.purchase.po 
Which table are you referencing here? Or is that a cross db join with the scheme called receipt and purchase the table and po the column?",1533687405.0
Anterai,"The first and forthemost question is: 
What version of MySQL are you using?",1533683765.0
Laurielounge,"Hi there ,

Depends on your mysql version 

`mysql - V`

`mysql  Ver 15.1 Distrib 10.2.16-MariaDB, for Linux (x86_64) using readline 5.1`  
in my case I have mariadb 10.2.16 as you can see. From 10.0 mariadb has regexp\_replace. Mysql 8 also has it.

[Regex docs for mariadb here](https://mariadb.com/kb/en/library/regexp_replace/)

`^( )*` 

...grabs spaces at the start to get you going.",1533589819.0
asshelmet,"TRIM already works the way you want it to:

    SELECT TRIM('\n' from  ' \n \n foo bar \n \n ') AS TrimmedString;

Returns

    foo bar",1533590433.0
Laurielounge,"Bugger. You're back to square one I think - an iterative function, seeing as how there's no way to know how many characters you're trying to get rid of.

it does speak to the possibility, as has been suggested elsewhere, that you get the data cleaned up before it hits your database, for example. If it comes from a different RDBMS, maybe there's an opportunity to regex it there. If it's coming from something that looks like a file, a couple of minutes of python could fix it.

If no go, I'd be looking at turning this into a function. Once again, define your list of characters that would mark the start and end of the string you want to generate - \\n doesn't count because, although you want to keep it if it's in the middle of a string, you'd discard it at the start or the end. Locate the first instance if each of those characters. Find the minimum and maximum values. Discard everything before or after using a substring function. But turn it all into a function so your actually query code doesn't look so terrible.

This function will not be something that you'll stick on your CV, but it's probably the best way to get yourself out of the hole.

Good luck...

\---=L

\[Edit\] - actually, I've changed my mind, u/asshelmet's trim suggestion does look like path of least resistance...",1533596096.0
r3pr0b8,"WHILE loops are so 19th century...

    SELECT TRIM(REPLACE(_s1,'\n','')) as vwalah",1533589466.0
ilogik,"This is probably what you need :

>	Initializing a fresh instance
When a container is started for the first time, a new database with the specified name will be created and initialized with the provided configuration variables. Furthermore, it will execute files with extensions .sh, .sql and .sql.gz that are found in /docker-entrypoint-initdb.d. Files will be executed in alphabetical order. You can easily populate your mysql services by mounting a SQL dump into that directory and provide custom images with contributed data. SQL files will be imported by default to the database specified by the MYSQL_DATABASE variable.

I assume you can add `create database / user` queries to that directory ",1533408672.0
C0c04l4,I would fork the official mysql image and make one that creates two databases.,1533428735.0
dedorian,">""A character set is a set of symbols and encodings. A **collation** is a set of rules for comparing characters in a character set.""

From [the official MySQL Documentation.](https://dev.mysql.com/doc/refman/8.0/en/charset-general.html)",1533332306.0
msiekkinen,Basically it will say which characters are considered equal.  For example uppercase S and lower case s,1533343845.0
haventohell,Do it in the business logic.,1533319787.0
davvblack,Maybe you can trick the optimizer by aliasing table\_B differently both times,1533329886.0
SomeGuyNamedPaul,"Single transaction with a pure InnoDB DB, as is the case with Galera, is sufficient.",1533310291.0
beermad,"This, unfortunately, is the problem with so many of the useless tossers that run hosting companies.

I had the same problem a couple of years ago, also related to remote access to a database. And despite telling the support drones **exactly** what needed to be done, it was totally impossible to get it through their thick heads that I was telling them how to fix the problem. Unfortunately, unless you can either persuade them to actually run the necessary command, you're probably stuffed.

Because my problem was related to one website I control trying to display data from another of my sites, I ended up having to replicate the data between the two databases via a regular job on my home Linux server. I suspect from what you describe, pretty much your only option is going to be to replicate the database locally (using mysqldump), doing the processing, then dumping the local copy and uploading it to live. Which could be a pain if you can't afford even the shortest break in the remote database (though maybe it could be done in a transaction?)

As you seem to have noticed, it's very much IP address related. Though it sounds like there are other problems if you're seeing different errors from another IP address.",1533238710.0
NotTooDeep,""" asked us to pull out columns, ""

What I think they mean is to normalize the original table by pulling out repeating groups, like professions, into a lookup table.  

Using profession, let's say you have a few contacts that are all dentists. You could have a descriptive text column in the contact table called ""profession"" and be done, but that means you will have a difficult time in the future changing the contacts table when the business requirements change.

Example: Original Biz Req: tell me all the contacts that are dentists.  Original solution: add a ""profession"" column to contact table and go have a beer.

New Biz Req: tell me all the contacts that are medical professionals. Now you have Big Ugly Query with a hand written predicate similar to ""profession in ('dentist','doctor','nurse'...). Now, if someone adds a new value for a medical profession, you're kinda screwed. You have to change your code instead of just updating a lookup table.

Improved solution:  normalize the data into a lookup table from the start. Yes, it will look silly. But, you can now easily create a dropdown list on a GUI with all the valid professions from this lookup table. You can add a field to the lookup called ""profession_type"" and update it one time to show which records are ""medical"". You could also use a flag, but this is less flexible IMO.  

If you find yourself with hundreds of profession_types, you can normalize again into a profession_type lookup off the profession lookup. 

Now you have more joins, but so what? Your code is easier to maintain. If you follow the most basic of physical data modeling rules and index all FKs, you most likely will not be able to measure the performance difference between joining two tables vs. joining three.

Enjoy!",1533227301.0
techpc,"Do you have a dataset of such magnitude that a single machine could not process the query by itself quickly enough?

[https://www.mysql.com/products/cluster/performance.html](https://www.mysql.com/products/cluster/performance.html)",1533172817.0
ncsupheo,"I'm having a hard time imagin_ing_ what possible kind of query could possibly need to be distributed across nodes.

There must be some alternate explanation for why you would need to.

Could you give some more insight into exactly what you're trying to do? Exponential time recursively derived fulltext search rollups across hundreds of millions of rows? 

Consider that by the nature of relational DBs, you having to be doing something particularly zany to need this. ",1533185847.0
ElMatze79,"Short: No

The only engine capable of that would be ndb_cluster, but you are buying that with awful slow DDLs and very imperformant joins. There are high memory requirements for this engine (your data must fit in) and you should use at least 4 nodes for proper HA.",1533188539.0
kaydub88,Don't prematurely try to optimize things.,1533208025.0
SomeGuyNamedPaul,"You could use Spark to parallelize the query.  

https://www.percona.com/blog/2016/08/17/apache-spark-makes-slow-mysql-queries-10x-faster/",1533229241.0
notenoughcharacters9,"Find the my.cnf that the server is using. Add skip-grant-tables below the [mysqld] section, then restart the server. Update that password & run `flush privileges`. 

Then remove the skip-grant-tables from the mysql config file. No need to restart the server then since flush privileges reloads the grant tables. ",1533178127.0
areti33,"In case you are still stuck with this issue and not able to solve the problem. Then try [MySQL Database Recovery Tool](http://www.databasefilerecovery.com/mysql-database-recovery.html), to overcome this problem.",1540979721.0
Irythros,"Probably something like this:  


SELECT wl.\* FROM whitelist wl

LEFT JOIN jobs j ON [wl.name](https://wl.name) = [j.name](https://j.name) 

WHERE wl.identifier = PUTIDENTIFIERHERE

That assumes the identifier column is in the whitelist table.  If it's not and is instead in the jobs table, change from wl.identifier to j.identifier .  If it's elsewhere then it needs a different query.",1533130978.0
STIIP," `MySQL.Async.fetchAll( 'SELECT  j.* FROM whitelist wl RIGHT JOIN jobs j ON` [`wl.name`](http://wl.name/) `=` [`j.name`](http://j.name/) `WHERE wl.identifier = identifier OR j.basic = 1 ',{}`  

thanks /u/Irythros for your help!!!!!",1533136159.0
mcstafford,"It's not a simple thing, really. It sounds as thigh you'll want to look in to a file-per-table configuration.",1533084293.0
jynus,"Re: #2 There are 2 main reasons why ibdata1 may have grown a lot (and that you can do to prevent it):

* One is that tables were created without `innodb_file_per_table`, and even if later tables are deleted or converted to `innodb_file_per_table`, the main namespace cannot be dropped in a hot way (it requires the process you mention). To prevent it in the future, make `innodb_file_per_table` the default and this should no longer happen

* The other main reason is a transaction being open for a long time, increasing the UNDO space for all modifications that happened since the transaction started. To prevent this, monitoring and killing automatically long running transactions would be ideal (also to prevent performance issues). If you are using a newer MySQL version, 5.7 or higher, you can setup separate tablespaces for the UNDO, and those then can be truncated much easier: https://dev.mysql.com/doc/refman/8.0/en/truncate-undo-tablespace.html",1533120699.0
Irythros,"INSERT INTO tableName (\`first\_name\`, \`last\_name\`, \`user\_name\`, \`email\`, \`password\`) VALUES ('test', 'test', 'test', 'test@test.com', 'test');",1533059759.0
,[deleted],1533074490.0
SaltineAmerican_1970,Are you opening the file in Excel or in a text editor?,1533001048.0
CrazyNateS,Are you using Mysql 8?  They changed some authentication settings in 8.0 (to be more secure) that your node plugin may not support quite yet...,1532978685.0
TimIgoe,"Any VPS hosting can let you do what you need there. Just don't install anything other than mysql and you are good.

If you are after something my with a bit of support it may cost more but any VPS will do it to some level or another.",1532978575.0
msiekkinen,I haven't done this but [this post](https://stackoverflow.com/questions/14940910/creating-mysql-events-in-amazon-rds) talks about enabling it on AWS,1532973386.0
lucasjkr,"Why not Digital Ocean or Linode, then?",1532977219.0
jericon,I am not aware of any companies that offer just database hosting.  Your best bet is to get a cheap $5 server with digital ocean or similar companies and install mysql yourself.  That will also give you the power to tune and upgrade it as you see fit.,1532977988.0
aladine123,"If you don’t mind your database schema change, you can consider elephantsql.com . It offers free plan for small size of postgres database.",1532993920.0
deniszh,Use http://maxwells-daemon.io to track changes in MySQL ,1532966955.0
lucasjkr,What process is changing the data? Why not have that process send out a notification when it starts or completes?,1532977173.0
Murwiz,"I know the original question is about MySQL, but I want to point out that this feature is available (and I've used it, some 15+ years ago) in PostgreSQL:

[LISTEN](https://www.postgresql.org/docs/9.0/static/sql-listen.html)

[NOTIFY](https://www.postgresql.org/docs/9.1/static/sql-notify.html)

What we did was to add triggers to the tables for UPDATE, DELETE, and INSERT which would perform a NOTIFY for a pre-determined ""channel"". Then, applications that needed to know this would either wait on the LISTEN, or just check it periodically.

Possibly something like this exists for MySQL, but I don't know.",1533043757.0
etrnloptimist,"Add a timestamp column called modified to your table.

Query for max of modified in your Android app. Keep track of prev value of poll to see when it changes.",1532963639.0
gadorp,"Rather than create my own thread, I'll hijack this one.

I'm looking for a way to do this entirely in Windows using C#/MySQL.",1532969287.0
StPatsLCA,"I recommend merging the tracking three tables into one.

I would have beginning time, ending time, and a type column which indicates if it's vacation, lieu day, or paid leave. I'd add the validation logic in whatever you're using to render your calendar.",1532974912.0
sam_malcolm,"It’s not a tutorial but the documentation for the MySQL node package is really thorough and clear
Maybe you could get insight from it? Hope it helps
https://www.npmjs.com/package/mysql",1532846075.0
AlexanderHorl,The documentation of the official [MySQL npm package ](https://www.npmjs.com/package/mysql) is pretty good if you know what you want to do or have experience with node and MySQL. Otherwise what do you want to archive with node and MySQL?,1532881156.0
Tiquortoo,"Check out ""High Performance MySQL"". I'm not sure about Labs, but the book does a good job of covering degenerate cases in MySQL.",1532799555.0
SuperQue,"Are you sure you're deleting the correct log file? Sometimes the log file path gets changed. Normally on restart you would see a log entry about creating a new log file.

Or, you possibly have the `innodb_log_file_size` entry in the wrong section of your mysql config files. It needs to be under the `[mysqld]` section.",1532667353.0
Bidibodida,"ok finally foun the issue........ this is ridiculous lol

in fact mysql doesn't read my conf file because it had the wrong permission (since I installed it with ansible without setting chmod)

all the other programs tell you when you have a problem with permission but mysql only tells you so when your conf is at 777",1532797023.0
areti33,"Seems to me some sort of error, you need to take assistance of [MySQL Database Recovery Tool](http://www.databasefilerecovery.com/mysql-database-recovery.html).",1539238267.0
mmurphy3,"The problem is that the ib file will not give you back that space. Innodb helps with performance mainly but not memory since the ib file will continued to grow.. Try using aria instead of innodb. You can use that to create additional tables for your logs. These new tables can be deleted in the future and the ib will not grow as fast. Either way you can’t get that space back. Its holding all transactions. Not sure if this helps, just recently went through something like this for the first time. ",1532656380.0
digicow,"Most of this is an r/php question, not an r/mysql question.

> Modifying the query to include DATE_FORMAT(column_name, '%m/%d/%Y %H:%i')

This, though, is on topic, so I'll address it. Assuming that the OpenDate/CloseDate columns are indeed DATE/DATETIME type columns, you can do it this way, but you may have column addressing issues. When you ""`SELECT *`"" you're pulling all of the table columns implicitly into your query, including `OpenDate`. If you then add ""`DATE_FORMAT(OpenDate, '%m/%d/%Y %H:%i')`"" to your column list, this is not going to replace ""`OpenDate`"" in the column list. Instead, it will become a new (essentially unnamed) column. You can name it using the ""AS"" clause, or access it by index rather than name, but either way, you're probably not going to get it with `$librow[""OpenDate""]`",1532616544.0
crackanape,"Not an answer to your question, but from a usability standpoint MM/DD/YY is terrible. Two-digit years are out until 2032. 

If you have a solely USA audience, and know that you always will, then at a minimum use 4-digit years. If your audience is even slightly international then use the normal format (YYYY-MM-DD) or localize 19-Jan-2018. ",1532620150.0
bobjohnsonmilw,"<table>

<tr>

<th>Library</th>

        <th>Reason</th>

        <th>Close Date</th>

        <th>Open Date</th>

        <th>Open Time</th>

</tr>

<?php while ($librow=mysqli_fetch_array($libclosings)) {

    $Library=$librow[""Library""]; // <-- These vars need to match the case of the DB columns

    $Reason=$librow[""Reason""];

    $CloseDate=$librow[""CloseDate""];

    $OpenDate= $librow[""OpenDate""];

    $OpenTime= $librow[""OpenTime""];

?>    
<tr>

    <td><?=$Library?></td>

    <td><?=$Reason?></td>

    <td><?=date('m/d/y', strtotime($CloseDate))?></td>

    <td><?=$OpenDate?></td>

    <td><?=$OpenTime?></td>

</tr>

<?php }?>",1532639130.0
hangfromthisone,Y'all need more epoch in yer liv,1532645648.0
CrazyNateS,"I'd recommend a relationship table, something like checklist_to_user (see below)


record_id | checklist_id | user_id
---------|------------|-------
1 | 1 | 1
2 | 1 | 3
",1532566383.0
Bitter_Bridge,"Using a linking / relationship table is definitely the standard way, but I honestly don’t mind your approach. Although, since you’re using JSON for one field, why not use it for the list of employees? You could then use MySQL’s native JSON functions to extract your data. ",1532569010.0
SuperQue,"I think what you're talking abut is ""man in the middle attacks"".

It sounds like you're building a client application. If you don't trust the client application, it's generally better to build an API service that sits in front of your real database. It's not safe to allow direct SQL access, especially over the Internet.

The client application will communicate with the server application over a separate protocol. For example [gRPC](https://grpc.io/docs/quickstart/csharp.html) would be appropriate for this.

Then the backend converts the API requests to whatever SQL is needed. This is also where you would sanitize the incoming data to avoid [little bobby tables](https://xkcd.com/327/). ",1532431595.0
crackanape,They are completely different things. Your question is like asking what’s the best way to make soup: carrots or a stove?,1532420169.0
trashpantaloons,"It’s going to be in a . Folder in your home directory or somewhere in your ~library.

Install `brew` and then do a `brew install mysql ` the script will tell you how to get brew services to start MySQL on reboot etc

From there you want to run the standard scripts like `mysql_secure_installation` ",1532405098.0
joatis3,The SP will be able to use the temp table IF it is called using the same connection that created the temp table. The only concern I have is how python manages the connection across the threads. ,1532393837.0
r3pr0b8,"take the pain, run the query 47 times

you have to handle 47 outputs anyway, so how were you planning to do that?

i think it's possible to do it all with a single query using a **numbers table** but the number (2 through 48) will be a column in the result, and you'll get 47 rows of posN_open, posN_high, etc.",1532376711.0
beermad,"I reckon you could probably do it in a stored procedure. They can do all kinds of weird & wonderful things that can't easily be done with ""simple"" queries.",1532387819.0
GenerousGestapo,You could do your MySQL commands by way of jdbc. You’d be working with java getting resultsets and metadatasets but most importantly you can loop via for loop and store in an array for later use in your code. ,1532380943.0
jericon,You would need to use a stored procedure or queryscript (part of https://github.com/shlomi-noach/common_schema ). ,1532421107.0
bowersbros,"Can you post us what the resulting query is?

To do this in Laravel, you can add a query filter that will log it to a file https://laravel.com/docs/5.0/database#query-logging

and also can you run that query independently of Laravel to see if the performance issue is MySQL itself, or the ORM trying to fit the data into its structure.

If you run the query itself, can you also run an `EXPLAIN` on it, so we can see if there are any missing indexes that would be benefitial to have",1532351197.0
payphone,"In my experience count() with joins are super slow. IIRC the issue is in the fact that the entire query has to run for each possible variation of the counted column. For instance  thread\_id is counted and used in joins. The count() is run on the result of the full query, so to get it for each thread\_id all of the joins have to be run for each individual thread\_id, then those results can be counted.

If that makes any sense whatsoever.

I think the solution would be to use sub queries as in this article:  [https://lasserafn.com/faster-mysql-counting-on-joins/](https://lasserafn.com/faster-mysql-counting-on-joins/)",1532370623.0
xXxLinuxUserxXx,"I would try if the problem occurs with mysql 5.7 too.

MySQL 8.0 is still too new that I would rely on for a production system :)",1532364642.0
r3pr0b8,"    SELECT d.Id
         , d.Locality
         , d.Name
         , d.RestrictedLocations
         , d.Shortname
         , d.Alert
         , COUNT(*) AS signal_count
      FROM Services AS d
    INNER
      JOIN Signals AS e
        ON e.ServiceId = d.Id 
       AND e.PostedAt > '2018-07-22T10:02:53' 
     WHERE EXISTS 
           ( SELECT 1
               FROM SiteServices AS f
              WHERE f.SiteID = '5D3D3F84-6172-4BFF-BF46-4E10F65B0AA9' 
                AND f.ServiceID = d.Id )
    GROUP
        BY d.Id
    ORDER 
        BY signal_count DESC LIMIT 36

i'm guessing the EXISTS subquery can be rewrtten as a join too,,, but that might depend on cardinalityi",1532275972.0
SaltineAmerican_1970,Pretend `EXPLAIN` before the entire query and run it in the MySQL shell. It will tell you how the database is constructing the query and what indexes it is using. That may give you insight into how to optimize your query.,1532274391.0
mischiefunmanagable,what exactly are you trying to order by? cause whatever you think it is isn't THAT order by clause,1532274509.0
tuantutran,"Can this run quickly?

```
Select serviceid, count(*) 
From signals
Where postedat > ...
Group by serviceid
```

If i understand correctly you're trying to get all the services associated to a given site, and get the 36 ones with the most number of signals.

I'd try something like :
- On one hand get all the services associated to the site.
- On the other hand get the number of signals per service
- Join those two and order by.

```
Select s.*
From (
Select d.*
From siteservices f
Join service d on f.serviceid = d.serviceid
Where f.siteid= ...
) As s
Join (
Select e.serviceid, count(*) as cnt
From signals e
Where e.date >= ...
Group by e.serviceid
) As c on c.serviceid = s.serviceid
Order by cnt desc
Limit 36
```",1532301365.0
dartalley,"You might also want to look into time series databases. They are made for taking in tons of small data points. They also have built-in data retention policies as well as ways to compress / aggregate the data for you behind the scenes.

If you use something like graphite or influxdb you can they also seamlessly hook it up to grafana and get lots of nice real-time graphs for your data.",1532610257.0
mtocker,"Hi!  I don't consider `86400` rows/day as a lot of data, but having said that there are known algorithms to be able to capture just key changes and project the difference between gaps in the sequence (where there were minimal changes).

I know they use such a technique in Industrial Automation systems, but I can't tell you much more than that sorry :-)",1532200365.0
CrumpleZ0ne,"Disclaimer:  I spent 20 years in the telecom industry doing many things similar to this (capturing call data from switches) so I've had lots of experience with both big tables (billions of rows) and real-time data collection where losing data was not an option. Anyway, here's my take on this: First, the inserts aren't the problem. The deletes are. As the table grows in size, the deletes will take longer and longer (especially if there is a clustered index) and will quite likely begin to block your inserts. It's not unusual for deletes on large tables to take several minutes to complete. Instead of having your python script perform the inserts as the data is received, I would recommend that you spool them to a CSV file instead. Pick an interval (like once per hour or every 30 minutes) for the python script to rotate out the current spool file and rename it to something like DATA-YYYYMMDDHHmm.csv and then continue spooling to a new file. Next, set up a cron job that checks your spool folder for these DATA-xxx files and bulk inserts any that you find into your table. Once you are sure the bulk insert is successful, you can either delete the file or zip it up and move it to another folder (if you're paranoid like me) . Add a daily cron job that reduces the raw data into **a different** table for your summary reporting. You can build a view that pulls from both tables if you need a report that includes any data that hasn't been summarized yet. Finally, set up a cron job that cleans up (or just truncates it if you no longer need it) your raw data.

The big advantage to this method is that it decouples the data collection process from the database. This means your database can now be completely offline for extended periods (maintenance, upgrades, hardware migration, catastrophic failure, etc) without interrupting your data collection.",1532224154.0
Irythros,What is the question / issue?  Computing the totals is not an issue and with proper indexing it wont be anywhere close to slow.,1532224216.0
NotTooDeep,"This math sounds very familiar.  Let me see if I understand the requirements.

At the end of the day, you want a sum. You are not retaining the details that helped you get to that sum, so they must not be important to keep around at all.

If that's the case, then I'd take a simpler, more direct approach. 

Make the date the primary key. The second column is the sum_of_today's_windspeed. Use a merge statement. I think that's it.

""If this row from python has today's date, then merge it with today's row, otherwise, create a new row.""

It's been a long, frustrating week. I may have totally screwed the pooch on this one, in spite of reading your post and all the replies more than once. Or, maybe I got lucky.",1532225020.0
SuperQue,"Have you thought about using a time-series database, something designed for the job?

This is a perfect fit for [InfluxDB](https://www.influxdata.com/).",1532263909.0
Mankriks_Mistress,"This was 12 days ago so I'm sure you've gone in another direction or possibly found the answer elsewhere...

Make it a BEFORE trigger, not an AFTER.

Also, your syntax/approach is wrong. You're attempting to perform an UPDATE on the _table_ as a result of the trigger firing. What you actually want to do is update the _record being inserted_ as a result of the trigger firing:

    IF NEW.amount > 1000 THEN
    SET NEW.success=0;
    END IF;

    IF NEW.success=1 THEN
    # perform rest of the operations

Note that I didn't include the `UPDATE recharge`.",1533302253.0
simonced,"What storage engine and what MySQL version are you using?
Some tables could be partitioned, or your user has restrictions...
The buffer you tried to increase should only impact selects, not inserts.
A place to look for could be the logs. Look in your binary logs if you have any, you should have clues there.",1532146626.0
hangfromthisone,"If the table has indexes, you can try deleting them, then inserting, then creating indexes again",1532141668.0
mischiefunmanagable,"look in the my.cnf, are auto_increment_increment and auto_increment_offset set?",1532119282.0
ckofy,"I’ve done migration of databases many times, between identical schemas and with schema changes, but never at that size as you described. For small databases located at one server that is pretty much running a set of queries/stored procedures to merge tables one by one from the bottom to the top to satisfy foreign keys (or disable foreign key checks and do that in any order). One time when I needed to move/merge a bigger tables (10-20GB), I also used stored procedures as a core engine, but the process was drove by <language of your choice> :) The same approach is if the databases are located at different servers.
You should not have problems with keys collisions if their ranges are separated already, if some internal keys are the same, use autoincrement on the target schema to reassign them.",1532174140.0
r3pr0b8,https://www.reddit.com/r/SQL/comments/90iygg/is_this_the_correct_syntax_for_mysql/,1532122650.0
MercurialNerd,"Hiya, you're going to need the following:

* hostname of the MySQL server in your hosting provider
* port number that MySQL is running on
* database name of the MySQL database on the server
* username of a MySQL user that has access to the database
* password of the MySQL user

Sample values would be like:

* mysql123.myhosting.com
* 3306
* mydatabase
* myuser
* myPassword123

Having this info should allow you to setup a database connection on your local machine.

A couple of gotchas - most hosting companies don't allow access to their database servers from external hosts by default. In other words only the hosting company's webservers can connect to their database servers. 

So if you want to hit the database from your local machine, you'll probably have to explicitly allow access from external hosts, and possibly specify the IP address that your local machine is using to access the internet.

Oftentimes they might change the port number from the default 3306 to another value. 

",1532093059.0
r3pr0b8,"add `ORDER BY username, workout_count DESC`
",1532080821.0
ill_help_you,Is anyone able to assist? Thanks :),1532218499.0
HalexGSd,"Can you provide more information about how you're trying to do the install?  Based on the output provided it looks like something is trying to run mysql\_upgrade, which is a package you use to upgrade to 5.7 from an earlier version. What commands are you running to initiate the install?

Also, it appears that you're running MySQL in windows, which is okay if you're working with a a small development or education purposes, but for production you will want to consider running in linux due to the fact that a lot of the adjacent technologies that MySQL is known to work with will not work in windows natively. You may want to consider working with a linux VM. This blog can help you get started with that. [https://blog.pythian.com/test-lab-using-virtualbox-nat-networking/](https://blog.pythian.com/test-lab-using-virtualbox-nat-networking/)",1532049995.0
TashaEli,"Your syntax appears correct, but it is unnecessary to use a subquery to calculate your avg.",1532015158.0
serunati,"If I was to be honest with anyone with this question the answer is ‘it depends’...

Check out https://www.makeuseof.com/tag/securely-delete-files-hdd-ssd-windows/ for a quick depiction of some of the reasons why you can’t depend on a delete actually deleting your data. 

The only honest answer (especially if you use SSDs anywhere in your systems architecture) is you can’t. 

Instead you should use a whole disk encryption (which puts in some overhead). This way if the disks are ever used outside of the physical system they would be as useless as you could hope for them to be. ",1532030653.0
jahayhurst,"I believe if you're using the table encryption plugin, the encryption key is then stored in the system table and is per table/tablespace. Don't quote me on that though.

If that's the case, when you drop the table, the former file is encrypted and then deleted - and basically secure.... except the key for that table sits in a row and gets marked as deleted I think and not actually removed right away, and even if that system table was emptied out there's a original key too.

Encrypt the whole disk instead.",1532043056.0
supermario182,Get a hammer,1532034771.0
dartalley,"If you have SSH access you should be able to find and copy the `my.cnf` file. If not companies like AWS have a way to get the parameter config whcih you should be able to export.

A last option might be running `SHOW VARIABLES` and converting that into the `my.cnf` file.",1531996935.0
simonced,"I use [https://www.heidisql.com/](https://www.heidisql.com/) at work and I like the Postgres support. All in the same package.

I feel it's lighter than MySQL workbench.",1531799058.0
TimIgoe,Workbench and heidisql are the 2 that I keep coming back to. Neither is perfect but they both do a good job. Certainly better than phpmyadmin etc,1531830510.0
nikhilb_it,MySql Workbench is really nice. Do try that.,1531852055.0
TheMartes,"Why? Sequel pro is by far one of the best. But, if you want more complex solution MySQL Workbench is working fine too :)",1531794360.0
seattlegaucho,Try [DBeaver ](http://dbeaver.io) it supports many others besides MySQL and MariaDB. ,1531807876.0
jynus,"> Right now I use Sequel Pro and I’m not loving it. Anyone have suggestions on clients they like? Thanks !

Could you elaborate what you are needs from a mysql client? Why you don't love the currrent one? What features you think you absolutely need? Which ones you don't?

I am a heavy MySQL user, and I use the mysql command line client + a self-made set of scripts to automate the most common tasks, and that is good enough for me. However, my need will be very different from the usage of an application maintanier, an architect, a sysop, a devops, a data entry person, etc. An OS also makes or breaks your preferred client (I can pipe an process data with standard UNIX tools) or integrate it with our orchestration and configuration management tools, while you may not need or want to do that.",1531852064.0
thatsricci,I use Navicat....  nobody else mentioned it so I might just be strange :),1531868551.0
ChiangRai,Ensure there is room in the file systen where the MySQL server data resides.  Just a thought.  Good luck ,1531746649.0
jynus,"Make sure you are not using MyISAM, or any filesystem with very low file sizes.",1531907012.0
HalexGSd,"In addition to checking the plugin name, you may also want to ensure that the user that you're trying to connect to is not using the old 4.1 password hash format. That can cause the same issue.",1532050182.0
SomeGuyNamedPaul,"Triggers will likely be much slower than doing it in code inside a transaction.  Also, trigger execution will be mostly hidden which leads to management difficulties.

If you absolutely must do it within the domain of the database and outside of application logic then do it as a procedure.",1531609860.0
hangfromthisone,"Select * from knowledge where difficulty < 10 order by subject

There is an error in your query: 'knowledge' table not found",1531485466.0
SaltineAmerican_1970,https://dev.mysql.com/doc/refman/8.0/en/,1531501925.0
LordZer,"SELECT [Data you want] FROM [Source(s)] WHERE [Condition]

Unless this is an admin job, that is what you need to know. It will get crazy complex sometimes (convoluted conditions and weird data requirements) but that's the base of it all.

",1531503153.0
escapewithniko,"[https://www.w3schools.com/sql/](https://www.w3schools.com/sql/)

I'd post more, but with 3 days, this seems like all you have time for. Best o' luck.",1531479714.0
mechthund,"[https://www.youtube.com/playlist?list=PL41lfR-6DnOokmJfmYj2hdVMKVGIcF7\_\_](https://www.youtube.com/playlist?list=PL41lfR-6DnOokmJfmYj2hdVMKVGIcF7__)

[https://www.youtube.com/playlist?list=PLS1QulWo1RIahlYDqHWZb81qsKgEvPiHn](https://www.youtube.com/playlist?list=PLS1QulWo1RIahlYDqHWZb81qsKgEvPiHn)

[https://www.youtube.com/playlist?list=PL\_c9BZzLwBRKn20DFbNeLAAbw4ZMTlZPH](https://www.youtube.com/playlist?list=PL_c9BZzLwBRKn20DFbNeLAAbw4ZMTlZPH)

[https://www.youtube.com/playlist?list=PL32BC9C878BA72085](https://www.youtube.com/playlist?list=PL32BC9C878BA72085)

& coffee.

Good luck.",1531511557.0
OkieDaddy,"Yeah...3 days to learn MySQL? And learn it well?? I mean, w3fools has some basic syntax, but I wouldn't trust them too much. What is your database background like? Do you understand normalization, datatypes, data modeling? What's the job for?",1531487130.0
mischiefunmanagable,"define: fake it til you make it
",1531490571.0
Bitter_Bridge,"Download it, install it, run through as many online tutorials as you can. What position are you actually interviewing for?",1531505787.0
apaethe,Have you by chance recently been extracted from the matrix?,1531532261.0
Sanja261,"There are tons of PostgreSQL tutorials on.yt and the program is free. Also, coursera or udemy.",1531487833.0
MadPhoenix,"If the column is unique, then you already have perfect cardinality for an index which is what you're going for.  However smaller indexes are generally faster, so the real question is whether you can index only a subset of the 32 characters and still get good cardinality.

One way to measure this is discussed in the [High Performance MySQL ](http://shop.oreilly.com/product/0636920022343.do) book in Chapter Five, the section titled ""Prefix Indexes and Index Selectivity.""  Selectivity is the same concept as cardinality - the more unique values in the index, the better the index will perform.

Basically, you can run successive tests using longer and longer index prefixes, calculate how many individual values you get divided by the length of the table, and see what your optimal length is with the goal of getting as close to 1 as possible.  For example:

`select count(distinct left(mycol, 3))/count(*) as sel3,`

`count(distinct left(mycol, 4))/count(*) as sel4,`

`count(distinct left(mycol, 5))/count(*) as sel5,`

`... as many more values as you want up to 31...`

`from mytable;`

Each column in your result will be some decimal number < 1.  If, for example, you can get a selectivity greater than .8 or .9 with a much smaller index prefix than 32, then you may want to go with that.  As always, your mileage may vary, especially depending on whether you frequently write to the table or it's mostly read-only, and the only way to know what really works best for your data set is to test.",1531495198.0
etrnloptimist,"One million rows is tiny. You could fit your entire column in 32 megabytes of memory. 

Stop the premature optimization. Just use a normal index.",1531498776.0
Irythros,What is the issue with just using a normal index on the column?,1531470961.0
apaethe,"Probably premature optimization as said in another comment but...

https://dev.mysql.com/doc/refman/8.0/en/index-btree-hash.html

here it the documentation on btree vs hash indexes and you might find some info that applies to your particular case which points you in one way vs the other.",1531527121.0
razin_the_furious,Where is mysqldump.exe on your laptop?,1531513535.0
philipwhiuk,Provide the exception error,1531435315.0
dedorian,"It's been a while since I used JDBC, but I believe part of your problem could be that the statement you're attempting to execute isn't a string; maybe try something like this?   


stmt.execute(""UPDATE \`cves\` SET \`comments\` = 'test test test' WHERE \`vr\_description\` = 'The ntpq protocol in NTP before 4.2.8p7 allows remote attackers to conduct replay attacks by sniffing the network.'""); ",1531430477.0
engineerwolf,This looks like homework. I will give you hint. Having count =1.,1531403745.0
llameadrpc1,"Too many moving pieces to be optimized adequately. Break it out into different queries, each of which can be run optimally by the engine. Yes, multiple hits to the engine will be necessary. Beats the **** out of getting no response at all due to timeout. ",1531398416.0
OkieDaddy,"Here is a better way to write it, getting rid of the select \*'s, and...don't quote me on it because its morning and no coffee, but I believe the join operations may be more effecient than the IN statements? YMMV. Edited for code cleaning. Reddit screwed up the formatting.  


Edit 2: You'll also probably want a Nonclustered Index on (Name, EntryID) at minimum. If you want it covering, INCLUDE Exp...Not sure if INCLUDE is a keyword in MySQL, I've only done indexing in SQL Server.

    SELECT
        [Base.Name],
        MaxEntry.Exp - MinEntry.Exp AS Exp
    FROM
    (
        SELECT 
            Name, 
            MAX(EntryID) MaxEntry, 
            MIN(EntryID) MinEntry
        FROM Person
        WHERE UpdateID >= 1
        GROUP BY Name
    ) AS Base
    INNER JOIN Person MinEntry ON Base.Name = MinEntry.Name AND Base.MinEntry = MinEntry.EntryID
    INNER JOIN Person MaxEntry ON Base.Name = MaxEntry.Name AND Base.MaxEntry = MaxEntry.EntryID
    ORDER BY Exp DESC",1531408865.0
CrudBert,"I don’t think that you need either of those “order by” clauses. I’m not sure, but try taking those out, I think it’s just a lot of extra work. ",1531438410.0
godsman27,"I get the same error I am using visual studio 2015, with the latest MySQL server and connector",1531747554.0
MyDataBes,"try this one:

SELECT spotID, count(distinct time)-1
FROM records
WHERE time BETWEEN '2017-01-01 00:00:00' AND '2017-01-01 05:00:00'
GROUP BY spotID
ORDER BY `spotID`;",1531287628.0
mischiefunmanagable,going to assume you aren't using innodb_file_per_table then?,1531267809.0
mmurphy3,I saw an article on that. I could go that route. Or maybe change the storage engine. I’m new to this and just  want to find a solution to this ibdata problem definitely.,1531268061.0
msiekkinen,"Even with innodb\_File\_per\_table your ibdata1 will only ever grow, never shrink.  It doesn't matter if you run optimize tables to completely rebuild them all.

W/ file per table the largest contributor to idbata1 growth you'll find is long running transactions that are generating large redo logs needed for MVCC in case of explicit rollback or crash recovery.

If your getting hundreds of gigabytes in your idbata1 in the long run you'll want to track down the access patterns and queries contributing to this. A simple first pass would be enabling the slow query log and using pt-query-digest to get a birds eye view summary of activity for heavy hitting queries.

To reclaim the space you'll basically need to rebuild the database into a new instance and then start using that one.

**Naive Simple Case**

If you have a single stand alone instance mysqldump your databases into a new mysqld and start using that one.  Outside of sandbox cases this likely won't be very viable since you'll want a complete point in time snapshot meaning you'll need to stop all writes in the time being.  This likely isn't acceptable in any kind of production use case.

Alternative to mysqldump, which will be slow since all the data is reevaluated as SQL statements, you could use tablespace export/imports to raw copy the .ibd files from one instance to another.  This will be faster since it's limiting factor is your disk or network I/O.   The downside to this approach is if your tables themselves were fragmented they'll remain fragmented until you run an optimize.   The mysqldump approach would have been rebuilding tables from scratch, reclaiming any fragmented data\_free you might have had.

**Slave Promotion**

Most likely you're only seeing idbata1 bloat on your master if you have replication setup.   If you don't have replication setup you really would want to get it going so you can run backups from the slave anyway.

I'm leaving details on how to do a seamless slave promotion out of the scope of this high level description.   In short you would promote your unused slave w/o a bloated ibdata1 to master.   Your old master is now a slave.   You would take that one offline and do a backup restore to that location (assuming your backups have a slim ibdata1), reconnect it to replication and let it catch up so it's ready for the next maintenance fail over.  ",1531498506.0
jericon,"Your post has been removed.

Reason: Self Promotion

Your post history shows that you have a propensity to post from the same domain or topic often to a wide breadth of somewhat related subreddits without contributing to the communities otherwise.  Please see [Reddit's Self Promotion Policy](https://www.reddit.com/wiki/selfpromotion).",1531254793.0
jericon,"Your post has been removed.

Reason: Self Promotion

Your post history shows that you have a propensity to post from the same domain or topic often to a wide breadth of somewhat related subreddits without contributing to the communities otherwise.  Please see [Reddit's Self Promotion Policy](https://www.reddit.com/wiki/selfpromotion).",1531254826.0
sskss73,"I've got the answer on another forum.
SELECT rowtosearch, COUNT(*) FROM table GROUP BY rowtosearch",1531223020.0
jericon,"Your post has been removed.

Reason: Self Promotion

Your post history shows that you have a propensity to post from the same domain or topic often to a wide breadth of somewhat related subreddits without contributing to the communities otherwise.  Please see [Reddit's Self Promotion Policy](https://www.reddit.com/wiki/selfpromotion).",1531254890.0
payphone,"Where is the DNS lookup being done for [www.site.com](https://www.site.com)? 

Do a nslookup on [www.site.com](https://www.site.com) and see if it is somewhere in resolving the site name back to the IP address. ",1531237199.0
spongebob,Sounds like a great job for SQLite!,1531132817.0
Id_Panda_Dat,"If the game is saved entirely on your computer, use SQLite, if it's going to be online and you'll have a server anyway, use MySQL.",1531228616.0
driverdave,"Do yourself a favor and tell someone who knows what they are doing what happened, and let them fix it correctly.

It sounds like that query might solve your issue, but who knows what other things happen when users are deleted. I'd avoid letting internet strangers tell you what to type into your database.

Or run the query and see what happens. Should work.",1530918199.0
,[deleted],1530918534.0
Bitter_Bridge,"Oh nice, glad you got it squared away. ",1530926952.0
bigbozz,"I see that you've got this fixed, but in the future, I'd strongly recommend making a database backup *before* you make any changes outside of the application - just in case something does not go as planned.

I would hope that someone in your organization is already running backups on some sort of schedule - otherwise, your organization is going to have a lot of trouble in the event of a hardware failure on the machine hosting the database!",1530927111.0
Id_Panda_Dat,"Yes, that'll work. You could also set the where clause to ""where deleted <>0"" to be even safer.",1530928732.0
jericon,"Your post has been removed.

Reason: Self Promotion

Your post history shows that you have a propensity to post from the same domain or topic often to a wide breadth of somewhat related subreddits without contributing to the communities otherwise.  Please see [Reddit's Self Promotion Policy](https://www.reddit.com/wiki/selfpromotion).",1530898914.0
jericon,"So for the hostname, localhost is the same server that you are running on. As you are not running on googles servers, this won’t work. 

For an external hostname, you need to make sure that your database is configured to listen for external connections on that hostname/ip and port. A quick google can show you how to do this. I will try to get a link for you when I am not on mobile. 

On top of this, your server needs to be able to receive traffic on that port. That may require changes to your firewall or router to make sure they can get through. 

In addition, you need your user to have grants that allow it to receive connections as well. When doing the grant you will need to do <user>@‘%’ for any host. Or google may be able to provide you with a different hostname or ip to use there. ",1530846907.0
Bitter_Bridge,"I'm assuming you'd need to do it as a prepared statement, but I'm dying to know what your use case is. ",1530824832.0
hangfromthisone,"You should prepare a statement as a string then execute it, so while concatenating you can use the result of current_date in place of column name.

But let me tell you this is the first time I see someone trying to do that",1530821922.0
LobbyDizzle,"Long story short, this is not possible in a single statement, but can be performed with a prepared statement: [https://stackoverflow.com/questions/26795844/mysql-dynamic-column-alias](https://stackoverflow.com/questions/26795844/mysql-dynamic-column-alias)

Here's an example of what you'd need to do to use current\_date as an alias:

    SET @column_alias := DATE(NOW());
    SET @query := CONCAT('SELECT 1 AS `', @column_alias, '`');
    PREPARE dynamic_statement FROM @query;
    EXECUTE dynamic_statement;

Here it is executed:

    [mysql] localhost:test> SET @column_alias := DATE(NOW());
    Query OK, 0 rows affected (0.00 sec)
    
    [mysql] localhost:test> SET @query := CONCAT('SELECT 1 AS `', @column_alias, '`');
    Query OK, 0 rows affected (0.00 sec)
    
    [mysql] localhost:test> PREPARE dynamic_statement FROM @query;
    Query OK, 0 rows affected (0.00 sec)
    Statement prepared
    
    [mysql] localhost:test> EXECUTE dynamic_statement;
    +------------+
    | 2018-07-05 |
    +------------+
    |          1 |
    +------------+
    1 row in set (0.00 sec)
    ",1530822192.0
de_argh,look at the date and time functions,1530820337.0
chinahawk,[nooooooooooooooo](http://www.nooooooooooooooo.com) ,1530834120.0
pease_pudding,"I think Workbench uses mysqldump to do all the work.

As far as I know mysqldump doesn't support this option, so Workbench can't either",1530799150.0
Id_Panda_Dat,"I think you'd have to make a stored procedure for the table creation, then go into mySQL's scripting engine and call that procedure, then do the exports in python or whatever. Then call that script. I don't think there's any other way to coordinate mysqld and mysqldump like that.",1530928924.0
angusmcflurry,"The most common cause of this is a column that is too large for your current configuration.  If you have some large blob or text columns you may need to bump the max_allowed_packet setting in the server config or startup script:
  
https://dev.mysql.com/doc/refman/8.0/en/packet-too-large.html",1530787617.0
razin_the_furious,How are you loading in the database? Is there any printed dB error?,1530786650.0
Id_Panda_Dat,"I've been doing this for a few years and consider myself to be an expert in sql in general. I just learned last week about creating and incrementing variables within a query to give rank orders within grouped data. Blew my fucking mind, and allows me to do some crazy stuff with group concatenation and other analysis projects. ",1530929077.0
jericon,It looks like a dictionary of passwords and their hashes.  No ideas why yours might be on there.,1530814565.0
Bitter_Bridge,"It's called a rainbow table:

[https://en.wikipedia.org/wiki/Rainbow\_table](https://en.wikipedia.org/wiki/Rainbow_table)",1530824956.0
Bitter_Bridge,"I came to this post not knowing what to expect. I laughed. I cried. I was moved and inspired. 10/10, ",1530701531.0
crackanape,"> all our efforts turned out useless

Sorry to hear that. You may want to hire someone who understands MySQL.",1530704343.0
hangfromthisone,"Wow 20 people, such collision, much transaction",1530711971.0
razin_the_furious,This post is kind of vague. Do you want help? Or is this just and epic tale of sadness?,1530698746.0
Etii3964,"Yp.. it's sick.. you should migrate to MS Excel, much more performance.. ",1530731225.0
Laurielounge,"Tough post to comment on as others have commented!

That's not a good situation, but what do you want help with?

If you're wondering why incomplete data, here are some thoughts

* possibly the process/processes that update the data have unexpectedly removed some of the data.
* possibly the query/queries that return the data are wrong
* possibly someone has accidentally deleted the data
* possibly there's something wrong with the database tables 
* possibly there's a problem with whatever tool you are using to access the data
* possibly there's a problem with the server hosting the database (for example, out of disk space)

What to do about it? More info required. Operating system, for example.

* Do you have backups? Is a restore possible?
* Is it possible to rebuild the data from source?
* Have you checked the state of the server?
* have you tried querying the server directly?
* have you checked the logs?
* Can you locate someone with more mysql expertise to lend a hand?

Good luck, bad situation.

\---=L",1530754688.0
Cambake,"Something on this server is not clearing out the temp directory on a regular basis. Tell your host to empty that directory.  Usually the files are created in /tmp (On \*nix based systems)  
 

It's pretty obscure, but I think you might (just might) have a dead SQL temporary table like that one hanging around, and eventually mysqld tries to reuse the same filename and fails because it has internal rules telling it never to overwrite a .MYD.

Another method to use various articles, guides, tricks from Google and Yahoo, that I could find....

[https://dev.mysql.com/doc/refman/5.7/en/repair-table.html](https://dev.mysql.com/doc/refman/5.7/en/repair-table.html)

[http://www.databasejournal.com/features/mysql/article.php/10897\_3300511\_2/Repairing-Database-Corruption-in-MySQL.htm](http://www.databasejournal.com/features/mysql/article.php/10897_3300511_2/Repairing-Database-Corruption-in-MySQL.htm)

[https://www.fixtoolbox.com/repairtoolformysql.html](https://www.fixtoolbox.com/repairtoolformysql.html)

What I would recommend is looking to see where these #sql\_XXXX\_X.MYD files live, shut down mysqld, clean out the temp files and restart it.  
 SSH into the server and type   
 

service mysql stop  
 

delete the #sql\_XXXX\_X.MYD files  
 

service mysql start",1530885152.0
shobbsy,"Heya, I’ve just given this a go in sql fiddle which seems to work ok
http://sqlfiddle.com/#!9/2267e/7/4

Are you getting an error or just not your intended result?",1530653324.0
Tannerleaf,"Further to others' responses, you might also want to grab MySQL Workbench:

> https://www.mysql.com/products/workbench/

Mostly so that you can at least connect to the database that you're working with, in order to be able to see what's in there, without all of the pissing about with Excel at first.

At the very least, that'll help you confirm that you can actually connect to the MySQL server that you need from your machine, and will give you a clear picture of what tables and data are available. Then, use that information to make the connection from the Excel side.

Never used Excel's Power Query thing, though.
",1530662219.0
crapslock,"Have you followed this? : https://support.office.com/en-us/article/connect-to-a-mysql-database-power-query-8760c647-88b9-409d-b312-6ea8f84a269b

",1530632659.0
shobbsy,"My suggestion is find a similar sheet elsewhere in the business and nick the connection details from one of those, otherwise you’ll have to go through setting up the connection string in the vba which always seemed an effort!",1530634554.0
salvador_danny,"Not sure what you mean by pulling data from the server. If you just need data from a table, I'd run a ""SELECT... INTO OUTFILE"" and just create a delimited file. In Excel's Data tab you can import data from CSV and basically see your data table in an Excel tab.",1530666392.0
razin_the_furious,"Put single quotes around the variables in the ""on duplicate key"" part, as they currently have no quotes around them.

Put ""addslashes"" or ""myslqi_real_escape_string"" around the input stuff, this will prevent single quotes in the input (most likely to be in ""reason"" as that one is in english), to keep inserts from breaking.",1530630541.0
omanisherin,"What is the full error you are getting? And if you printed out your SQL what would your query look like. Also, does $usertable need to be a variable?",1530627120.0
kaydub88,"Dude, this is going to be wide open for sql injection.",1530626031.0
apikey,"What sort of key does it have? Can you confirm this? 

To bypass the error you can execute the following prior to the UPDATE:

SET SQL_SAFE_UPDATES = 0;",1530619338.0
Elite_Italian,"You can do this all via the command line. 

You can dump the entire db using mysqldump. then import the sql file into the new mysql install. ",1530570537.0
pease_pudding,"Try this 

https://mediatemple.net/community/products/dv/204403864/export-and-import-mysql-databases",1530570926.0
cknu,Use an etl tool like pentaho data integration. Read from the aws instance and write on the new server. You need to do this for each table but it’s no so difficult. ,1530587810.0
payphone,"If all you need is the schema you can run the query SHOW CREATE TABLE 'tablename'; 

That will give you the actual statement that was used to create the table in the first place. Run it on the new DB and you have a table with the same schema.",1530638822.0
bigbozz,"A few questions:

Do you have command-line access to the old server?  If so, do you have root access on it (root user login for the server itself, not for MySQL)?

Is the database on the old server currently in use (being modified)?

Regarding your permission issue with mysqldump, according to https://dev.mysql.com/doc/refman/8.0/en/mysqldump.html , ""mysqldump requires at least the SELECT privilege for dumped tables, SHOW VIEW for dumped views, TRIGGER for dumped triggers, and LOCK TABLES if the --single-transaction option is not used.""  Try adding ""--single-transaction"" to your mysqldump command-line and see if that gets you farther.",1530644991.0
chinahawk,🍿,1530585984.0
,[deleted],1530573938.0
therealjoshuad,"Assuming this isn’t a troll post, that is a pretty broad question, can you be more specific? 

Also, what level would you say you’re at now? It may be helpful to know where you’re at to know what your wanting to sharpen up on.",1530407214.0
spore_777_mexen,"Fellow n00b here.  


I read the 5.7 manual and a book called High Performance MySQL.  
I will occasionally reference w3schools for syntax I cannot remember.  
I lurk StackOverflow and this subreddit hard too.",1530785341.0
pease_pudding,"From what I can gather, I think you want to be joining to the table twice. Give it a different alias each time (S and S2), and also use a LEFT JOIN rather than just a JOIN (JOIN on its own implies an INNER JOIN)

See if this helps

	SELECT 
		M.id, 
		M.username, 
		S.value as value1, 
		S2.value as value2
	FROM 
		main_table M
		LEFT JOIN secondary_table S ON (M.id = S.user_id and S.field_id = 1)
		LEFT JOIN secondary_table S2 ON (M.id = S2.user_id and S.field_id = 2)
	order by
		M.id",1530387546.0
Bitter_Bridge,Take a look at GROUP_CONCAT() and see if it will give you what you want. ,1530385276.0
msiekkinen,"What does 

> show grants for stoneyhd_write@localhost;

look like?  (From an admin account you can connect as)",1530303475.0
AllenJB83,"1 . This is dependent on table storage engine, and exactly what you mean by ""optimize"". See [OPTIMIZE TABLE](https://dev.mysql.com/doc/refman/5.7/en/optimize-table.html) documentation (read the documentation page appropriate to your MySQL distribution and version - things can differ).

2 . Not something I've dealt with personally yet, but I think you want [PURGE BINARY LOGS](https://dev.mysql.com/doc/refman/5.7/en/purge-binary-logs.html)

3 . I think you need to be explicit about what type of locks you're interested in here (and what table storage engine(s) you're interested in).

4 . Distro dependent, but usually either (where ""mysqld"" is the name of the service - it might be different depending on your distro and MySQL distribution - eg. ""mariadb"" instead) `service mysqld start` (or stop or restart) or `/etc/init.d/mysqld start`.

 It's also possible the service name / management method may differ if you're using a cluster management tool. I have no experience with these at present.

---

Recommended additional resources:

* [High Performance MySQL](http://www.highperfmysql.com/)
* [Percona Monitoring & Management](https://www.percona.com/software/database-tools/percona-monitoring-and-management)",1530298388.0
mischiefunmanagable,"you want to view the unencrypted version of the encrypted data from within mysql? not possible
export it to an application and decrypt it there",1530304965.0
jous,"You could give us something to work with. Like the schema, example of the data and so on...",1530293258.0
pease_pudding,"I don't think there's much you can do. If the column doesn't exist, then its invalid DDL

Develop your stored procs on your dev database (which presumably already includes the new column).

Then you have a deployment step, which adds the column to live database, and then imports all your stored procs via an SQL file",1530223355.0
gg32641224,"Check that, may be useful https://stackoverflow.com/questions/3395798/mysql-check-if-a-column-exists-in-a-table-with-sql",1530244290.0
jericon,"Why move the old data?  Say the current table is called table_current. 

CREATE TABLE table_new LIKE table_current;
RENAME table_current TO table_year, table_new TO table_current;

And you’re done. No inserts or deletes needed. The original table is now your archive for the past year and you have a fresh table to put this years stuff into. ",1530198036.0
dsn0wman,"It would be better to look into partitioning. That way you don't have to deal with different table names, and your queries are not bogged down with data from other partitions.",1530212706.0
LordZer,"MySQL dosen't change root passwords, it sounds like your box is compromised ",1530190189.0
just5ath,"Just, wow...",1530197049.0
hdvlprbi,"For the next reaches, I runned the mysql_secure_installation command and now everything is fine :)",1530250379.0
Bitter_Bridge,"If you’re just needing the result count in a PHP variable, I would use the native method:

http://php.net/manual/en/pdostatement.rowcount.php

If you’re not using PDO, there is a corresponding function for Mysqli. ",1530193593.0
loki0609,Mysqli [http://php.net/manual/en/mysqli-result.num-rows.php](http://php.net/manual/en/mysqli-result.num-rows.php),1530336316.0
paierlep,"I would guess that your my.ini file has an error: lower_case_table_names expects a value of 0, 1 or 2 (see [1]) and it seems that there is the value 0.0 in your configuration file.

[1] https://dev.mysql.com/doc/refman/8.0/en/identifier-case-sensitivity.html",1530172899.0
mattyjraps,"I've been having the same issue. It's a problem with the installer, seems to be trying to write a floating point instead of an integer to the config file, just have to wait until it's fixed I guess. Could also try setting up the service yourself, no idea how to do that though...",1530506545.0
Lucky777Seven,"There is a bug report on [https://bugs.mysql.com/bug.php?id=91476](https://bugs.mysql.com/bug.php?id=91476) and it looks like that this error happens frequently with the new 8.0.11.0 installer. Even changing the my.ini manually doesn't solve the issue.

Some users report that they are able to install and start the server with the previous version.

EDIT: It just worked! I have downloaded the 5.6 installer. Directly after starting the installer, it asks you if you want to update it. Answer ""Yes"" and you will get the newest server version without the error.",1531378702.0
areti33,Another approach to fix it is by [MySQL Database Recovery Tool](http://www.databasefilerecovery.com/mysql-database-recovery.html).,1540979988.0
jericon,"Your post has been removed.

Reason: Self Promotion

Your post history shows that you have a propensity to post from the same domain or topic often to a wide breadth of somewhat related subreddits without contributing to the communities otherwise.  Please see [Reddit's Self Promotion Policy](https://www.reddit.com/wiki/selfpromotion).",1530165744.0
razin_the_furious,"Laravel does use PDO, but it has a very significant abstraction layer via the ORM it uses. You really don’t have to use PDO much / at all to use it",1530142988.0
Bitter_Bridge,"There’s nothing wrong with using mysqli,  but if you’re like me, once you learn PDO, you’ll never look back. ",1530146266.0
Tiquortoo,You won't learn PDO if you use all the built in (Eloquent) stuff in it. mysqli is not deprecated. I would go with the more pragmatic choice of sticking with it.,1530144522.0
k7f,Maybe this will help [https://dev.mysql.com/doc/apis-php/en/apis-php-mysqlinfo.api.choosing.html](https://dev.mysql.com/doc/apis-php/en/apis-php-mysqlinfo.api.choosing.html),1530153403.0
Tannerleaf,"Laravel? Do it the Laravel way. Which generally seems to entail a mix of the Eloquent and ""direct"" database access approach; avoid doing anything that is too MySQL-specific, if possible.


",1530155276.0
DataVader,"Use PDO or a framework like Laravel. Never use mysqli in your life and when you see someone use is, fire that person",1530171495.0
r3pr0b8,4800?   where did this come from?,1530141842.0
jous,"I'm not getting what you mean, so i'm just guessing on the answer...

If you want the difference for minutes, just use TIMESTAMPDIFF(MINUTE, dateIssued, dateCompleted) as described in the [manual](https://dev.mysql.com/doc/refman/8.0/en/date-and-time-functions.html#function_timestampdiff)

If you want to calculate the working days difference, check out this [question](https://stackoverflow.com/questions/1828948/mysql-function-to-find-the-number-of-working-days-between-two-dates) on stack overflow. If you want to calculate how many working hours the order has been waiting, you can add to the answer like so: (workingDaysBetweenIssuedAndCompleted * workHoursInADay + workHoursOnCompletedDate + workHoursOnIssuedDate)",1530146065.0
msiekkinen,">  )  b)

to

>  ) b) c

You're missing the alias for the sub select 

>  SELECT COUNT(*) AS total_days",1530106239.0
jb13,"select userId, max(timestamp) AS lastActivityTime

FROM logs_user_actions

GROUP BY userID",1530061752.0
jericon,You can easily remove the sub queries and replace them with just the joins. ,1530080023.0
SiliconeOvenHat,I think we need more log messages. ,1530045971.0
LobbyDizzle,"Are you currently running WAMP, or separate instances of each Apache, MySQL, and PHP? 

I would follow the instructions in the top comment here to check to see if you have the MySQL (or WAMPMySQL) daemon running: [https://stackoverflow.com/questions/34177605/wampserver-mysql-service-doesnt-start](https://stackoverflow.com/questions/34177605/wampserver-mysql-service-doesnt-start)",1530044975.0
k7f,"MySQL Server 8.0 introduced new default authentication plugin. You need to create user with one of the older authentication method, e.g. CREATE USER xyz@localhost IDENTIFIED WITH mysql\_native\_password BY 'passw0rd'.

More here [https://dev.mysql.com/doc/refman/8.0/en/create-user.html](https://dev.mysql.com/doc/refman/8.0/en/create-user.html) and here [https://dev.mysql.com/doc/refman/8.0/en/authentication-plugins.html](https://dev.mysql.com/doc/refman/8.0/en/authentication-plugins.html)",1530095072.0
jynus,"It is a general advice, but when doing a major version upgrade, and specially, if you are doing a 5.5 to 8.0 upgrade, I would exclude the mysql database (which also has the additional improvement that, for most cases, it will avoid doing multiple steps, unless deprecated or non-compatible options are being used).

Most of the things on that database can be reloaded in a more version-independent way, like replication control or grants. For example- I normally use `pt-show-grants` to get an almost-version-independent backup of the grants that I can later load to other mysql versions or vendors. That avoids issues with the mysql tables changing formatting.

If you still want to reinsert mysql database tables in its original format, remember to run FLUSH PRIVILEGES after loading them, so that the grants are applied (any grant changes without using GRANT/CREATE USER/REVOKE/DROP USER/SET PASSWORD, etc.) require that to reload its cache.",1530026675.0
jericon,"Your post has been removed.

Reason: Self Promotion

Your post history shows that you have a propensity to post from the same domain or topic often to a wide breadth of somewhat related subreddits without contributing to the communities otherwise.  Please see [Reddit's Self Promotion Policy](https://www.reddit.com/wiki/selfpromotion).",1530038894.0
ilogik,"hard to tell without knowing what your table looks like. I assume the exact order changes each time you run the query?

try `ORDER BY notificationID, notif_for_id DESC`",1530002344.0
Tannerleaf,Are you actually connected to a MySQL Server?,1529991279.0
Laurielounge,"Hard to tell from the screen shot... try running it via the commandline maybe.

mysql -p
source sakila-schema.sql

... assuming it's in your working directory",1529991088.0
edward_175,"yes i had this same problem 

the solution is you have to log into your server ",1542290199.0
zanima19,"If you are on a Linux system stop mysqld, copy the mysql data directory, restart mysqld.  Do your testing.  When finished stop mysqld delete the new data directory, rename the copy to be the data directory, restart mysqld.",1529974500.0
not_rico_suave,Try transposing the table (your individual records are your columns) and counting the not nulls in each column?,1529965371.0
Skalyan95,This breaks the first normal form of database design. You should spend a little time and research the normal forms. It will save you tons of headache in the future. [Here's](https://www.youtube.com/watch?v=UrYLYV7WSHM&frags=pl%2Cwn) a youtube video thats pretty good.,1529965455.0
Bitter_Bridge,"I really, really recommend you read up on data normalization and apply it to this design. Your later self with thank you. 

That said, this should work:

SELECT
(IF principal1 <> ‘’,1,0) +
(IF principal2 <> ‘’,1,0) +
(IF principal3 <> ‘’,1,0) +
(IF principal4 <> ‘’,1,0) AS principalCount
FROM table
WHERE blah",1529968991.0
mcstafford,"I like the idea, but suggest that you start developing for python3.",1529961370.0
pskipw,What would be the use case of this vs standard migrations? Or is this in effect a migrations library?,1529968205.0
pease_pudding,"MySQL 8.0 has a JSON data type. It might have been introduced earlier, but I know its not available in 5.6.

I'd really recommend a middleware script to expose your API anyway. 

Whether this takes the form of a PHP script or some other language, just grab the data and then return it as JSON. For PHP this is as simple as json_encode($resultset);

Edit: Turns out JSON was added in MySQL 5.7.8",1529955586.0
gkodinov,"Any reason why you can't simply produce a JSON string out of your procedure ? Lost of concat etc ?   
Of course if you're running 8.0 the server will do this for you: [https://dev.mysql.com/doc/refman/8.0/en/json-creation-functions.html#function\_json-object](https://dev.mysql.com/doc/refman/8.0/en/json-creation-functions.html#function_json-object)

But it's not rocket science anyway.   
Another option is to create a custom user defined function in C to mimic JSON\_OBJECT : [https://dev.mysql.com/doc/refman/8.0/en/adding-udf.html](https://dev.mysql.com/doc/refman/8.0/en/adding-udf.html) ",1530010014.0
pease_pudding,"Not sure I understand. Is the date field in the CSV always empty, or will you now be populating it in the csv for records already imported?

Allow the date_added field in your temporary table to be NULL, then set them to todays date prior to inserting into the real table..

    UPDATE temporary_table SET date_added = CURRENT_TIMESTAMP WHERE date_added IS NULL;

    INSERT INTO contacts SELECT * FROM temporary_table etc
",1529865508.0
bangmygong,"Try using Homebrew, http://homebrew.sh",1529863827.0
mischiefunmanagable,IIRC you can't do an inplace without stepping through 5.7,1529851014.0
Payneshu,"May I ask OP, why are you looking to do an upgrade? What do you gain?",1530749966.0
jsuzack,My hope is better stability and support.,1530836580.0
chinahawk,"Using a wild card SELECT on a JOIN? ew

ALTER TABLE and add indexes to any columns that are in a WHERE, ORDER BY, or GROUP BY clauses. This should be a mandatory practice, imo.

",1529687859.0
dsn0wman,"I could be wrong, but this looks like it does a full table scan because you are considering every row in table_big. The below query might consider every row in default (full table scan), then hit the key for index_column on table_big.

select * from table_big where table_big.index_column in (select primary_key_colum from default);",1529592518.0
SomeGuyNamedPaul,Is the column type for the indexed column the same as the PK?,1529596060.0
pease_pudding,How does the EXPLAIN look if you just SELECT big.primary_key_column rather than SELECT *?,1529602394.0
TBores777,I've found ways to do it on Windows but I'm not using Windows. ,1529521565.0
MrRiotRick,"Install mysql instead of mariadb?
Phpmyadmin is just a web interface.",1529534722.0
TechnocratByNight,No a check is just that. You shouldn't be getting corrupted tables under normal operation unless your server is having other issues,1529522744.0
coderdan,"I know that a repair might make things worse if there is a file system issue.

&nbsp;

Just a speculation: A simple check puts a read lock on the table and might perform a table flush. Maybe the corruption happens in memory and the flush saves the data in time. Is your memory ECC and works fine?",1529523323.0
mischiefunmanagable,"I'd be looking at RDS, mysql 5.7 or aurora, and pt-online-schema-change or gh-ost

the worry here is why are you running into locks, are you running these tables as myisam?",1529516458.0
dsn0wman,"Look into ProxySQL and Percona XtraDB cluster. This is an easy way to split your workloads across different servers. Unfortunately MySQL is pretty bad about how it handles DDL changes, but having a cluster, and tools like gh-ost can make it less painful on the end user.",1529517721.0
just5ath,"MySQL Cluster is almost never the right answer.

If you're hitting write problems and can no longer scale up you are most likely going to have to make some major rewrites and find a different solution. MySQL Cluster will require major rewrites as it is, so make the right move and find what kind of NoSQL solution will fit the bill for what you're doing.

Otherwise continue to scale up, try to balance load out to different DB instances (split tables up logically into what can be separate DBs), and continue to try and tune your app.",1529529732.0
SuperQue,You might want to look into [Vitess](https://vitess.io/) as a sharding layer between your app and your database.,1529531746.0
SomeGuyNamedPaul,"If you think you've found a use case for MySQL Cluster then you need to think harder.   It's difficult to manage and full of so many gotchas it might as well be made entirely of gotchas.  Also, Galera aka XtraDB Cluster is not a write scaling solution either.  Try to hit it hard with writes and you'll quickly learn its limits as nodes fall behind and get ejected.

Have you considered TokuDB tables?  They fulfill your requirement for quick alters and write acceleration.  No really, a column add is a metadata lock, that's it.

They don't do well for updates or deletes, but their insert and select performance is pretty darned good.  They're so far the only compressed table format that lives up to the claims.  Before you ask, no you don't want to use InnoDB table commission.

At some point you're going to run out of how much space you can consume on a single host.  Are you keeping the data forever, or can you fragment by date and drop off old segments?  You could also spread the data across servers where you add a new server and write to the new one like you're fragmenting data.

But look at Toku, it might be what you need.  Archiving it takes some special handling because you have to learn a new way of doing it.  You can also try it out as a replica to see how well it handles versus an InnoDB replica.",1529550193.0
cx3ptr,"Look into tidb([https://github.com/pingcap/tidb](https://github.com/pingcap/tidb)),a distributed HTAP database compatible with the MySQL protocol.",1529565886.0
razin_the_furious,"The syntax you are using is not valid SQL. Those look like query binding strings that you'd use at the code level, not at the SQL level.",1529504846.0
Tiquortoo,"This schema doesn't support that with nothing but sql. You would need some sort of ""parent"" field and possibly a depth field to exclude certain things and relate them in the way you are describing.",1529502188.0
r3pr0b8,"    SELECT t.customer_id 
         , t.order_id 
         , t.date 
         , t.technician
      FROM ( SELECT customer_id 
                  , technician     
                  , MAX(date) AS latest
               FROM yourtable
             GROUP
                 BY customer_id 
                  , technician  ) AS dupes
    INNER
      JOIN yourtable AS t
        ON t.customer_id = dupes.customer_id                
       AND t.technician  = dupes.technician
       AND t.date        = dupes.latest ",1529506570.0
msiekkinen,"I don't have this information but it's GA now so it will only  grow as others are end of lifed.  

Out of curiosity what plugins do you develop? 


",1529461168.0
roguelazer,"Inno tries to do in-place UPDATEs both to the clustered index and to secondary indexes, so there's a lot less fragmentation and less need for VACUUMs than other databases (ahem, postgres). That being said, you can still end up with lots of unused pages and poorly-filled btree nodes, especially if you DELETE a lot or use variable-size columns. If you're on MySQL 5.7+ and exclusively using InnoDB (both of which you ought to be in 2018), you can rebuild a table and its indexes online with `ALTER TABLE foo ENGINE=INNODB` or just `OPTIMIZE foo`. In these modern versions of MySQL, this builds the indexes by sorting instead of by PK order, so you end up with a very well-balanced btree. Bear in mind that while this is online with respect to the master, it will still block replication on single-threaded replicas, so you don't necessarily want to do this in a highly-available environment (you might instead want to do it with `SQL_LOG_BIN=0` on the master and replicas separately).",1529425385.0
Tickford_daniel,"Change your JOIN(s) to LEFT JOIN
One or more of the referenced tables you are joining doesn’t have a match. IIRC MySQL does inner joins by default.",1529407106.0
kaydub88,Skip the rpm and use the binary. Install in /usr/local. ,1529364318.0
msiekkinen,"https://dev.mysql.com/doc/refman/5.7/en/multiple-servers.html

Talks about running multiple versions on the same machine.  I realize you're talking about running maria but I think the process will be similar.  

Look at setting a separate --basedir for the RPM installation as well as when starting the new mysqld.

Sorry for not an exact step by step solution.  

Alternative could be looking at setting up a docker container. ",1529350999.0
de_argh,do you have an auto incrementing column you could select the max value or last value?,1529289106.0
jeremycole,"Select one more than you need, e.g. 21 when you need 20, then check how many you actually get. If you get <21 it’s the last page, even if it’s full. ",1529290308.0
cknu,Check if the conector supports fetch size so you don’t need to process chunks manually. ,1529298515.0
aqbabaq,"You should try 
https://dev.mysql.com/doc/connector-python/en/connector-python-api-mysqlcursor-rowcount.html",1529301298.0
msiekkinen,"Here's my [previous reply](https://old.reddit.com/r/mysql/comments/8lqrwp/help_with_fetching_10_million_records/dzku9ih/)  trying to educate why limit x,y for chunks/pagination is very anit-pattern and becomes very slow very quickly.

If you have an autoincrement you should iterate on increasing ranges between x and y 

Edit:  To know when you  reach the end it's when you try to select and you get 0 rows returned.  This will be quick and avoid using a count(*).   ",1529343298.0
pease_pudding,"You can fetch the total rows which would match, using SQL_CALC_FOUND_ROWS, like this

    SELECT SQL_CALC_FOUND_ROWS field1, field2 from table where crit1 = 1 and crit2 = 2 LIMIT 100, 100


This returns all your results as normal. Now run a 2nd query..

    SELECT FOUND_ROWS();

This will return the total number of matching rows, including all the criteria you specified

A better idea would be to work from ID's. So lets say you pass back 100 records, with ids ranging from 50-150

You next query just passes this 150 id, and asks for the next 100 rows after id 150
    
    SELECT blah blah WHERE id > 150 LIMIT 100

",1529343762.0
r3pr0b8,"     SELECT CEILING(id/100000) AS chunk
          , COUNT(*) AS countexist
       FROM table
     GROUP
         BY chunk     ",1529274046.0
chinahawk,https://media.makeameme.org/created/happy-birthday-chunk.jpg,1529688720.0
mordisko,"If I get it correctly, you want to differenciate the chunks according to the IDs, right?

I think something like this is what you are looking for:

```
SELECT 
    CASE
        WHEN ID <= 100000 THEN 1
        WHEN id > 100000 AND id <= 200000 THEN 2
        WHEN id > 200000 AND id <= 300000 THEN 3
        ELSE 4
    END AS chunk, 
    COUNT(*) AS countexist
FROM        trazabilidad
GROUP BY    chunk
```",1529274137.0
wampey,"Indexes, EXPLAIN, normalization, transactions",1529260338.0
chinahawk,"https://dev.mysql.com/doc/mysql-getting-started/en/

https://www.elated.com/articles/mysql-for-absolute-beginners/

http://www.dummies.com/store/product/PHP-and-MySQL-For-Dummies-4th-Edition.productCd-0470527587.html",1529688614.0
razin_the_furious,"A primary key is a column or collection of columns that identify a single entity in the table. There is never more than one primary key, but the primary key may have one or more columns. The most common primary key is an auto increment integer names for the table (eg, contactId for a table of contacts)

A foreign key means it is a single column that refers to the primary key of another table. For example, if each contact row had a personId in it, and personId is the primary key of a persons table, personId would likely be a foreign key relating to the persons table",1529242145.0
keithslater,"A primary key is nothing more than an identity to make each row unique. A lot of times a primary key will just be an integer field that auto increments. Depending on the schema, the primary key could also be something like a user id. There can only be 1 value per row in the primary key fields. Normally I wouldn't want the primary key to change either.

A table will only have one PK but can have several FK's. A FK is just a field that links to a key on another table. So if you have a primary key on 1 table, there may be a FK on another table that is a link to it.

As far as your screenshot goes, we can't answer much because there isn't enough information. I can guess that contact phone most likely isn't a primary key.",1529242277.0
acrane55,"No, everything does not need to be a PK or FK.

It would actually be weird to make a phone number or last name a PK. It would be very unusual to make a phone number or last name a FK.

For a table called ""Students"", one might have the following columns:

* ID (this would be a PK) (this would be integer because it's easier to autoincrement and just generally handle) (length long enough to hold a number higher than the maximum number of students)
* FirstName (not a PK or FK) (varchar(100), a character data type because it holds alpha chars, and 100 because student might have v long first name)
* LastName (not a PK or FK, might index it tho) (varchar(100), a character data type because it holds alpha chars, and 100 because student might have v long first name)
* Phone (not a PK or FK) (varchar(50), a character data type because it can have leading zeros, and 50 because student might have long foreign phone number)
* CourseID (FK because it is the ID of the course student is following. This FK would match the PK in a separate table ""Courses"") (this would be integer because has to be same data type as PK in Courses table)",1529249654.0
LobbyDizzle,"The issue is that you are trying to refer to a column in `logs_table` that does not exist. The syntax is `FOREIGN KEY ([foreign key]) REFERENCES [foreign table] ([foreign table key])`. Also, you'll need to add `PRIMARY KEY` for the `id` field or else you'll get another error. With that said, this works:

    CREATE TABLE logs_table(
      id INT(11) AUTO_INCREMENT PRIMARY KEY, 
      logged_ip VARCHAR(45), 
      logged_dns_server VARCHAR(45), 
      logged_ip_country_city VARCHAR(200), 
      logged_hostname VARCHAR(200), 
      logged_user_agent VARCHAR(150), 
      logged_referrer VARCHAR(2000), 
      logged_ip_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
      iplogger_redirect_key_FK CHAR(8), 
      FOREIGN KEY (iplogger_redirect_key_FK) REFERENCES iplogger_info_table(iplogger_redirect_key) 
    );",1529224179.0
wstrucke,"You should also be storing IP addresses as integers, not strings.",1529238381.0
llenders97,"Most likely the IP or url of the dB server, if you are running MySQL on the same box as the hosting it could be localhost, assuming you aren’t connecting remotely, if remote then use the dB server ip",1529189481.0
MMOAddict,"If you have a firewall on the live server (you should), make sure you open the port for your IP address so you can connect to it. ",1529194199.0
jericon,"It would help if you indicate what programming language you are writing in. 

Most of the time this will be localhost if you are running the application on the same server as the database. Otherwise it will be the IP address or hostname of the database server. ",1529222164.0
zanima19,"Is Customer2 going to be an ongoing table that receives new information on the regular?  If Customer2 is not getting any new info just put the data into Customer1 and call it a day.

Will customers actually have access to this info?

A potential solution is to refactor the tables to include a column with which avenue this information came from:

`CREATE TABLE Customers (`

`id pk auto_inc,`

`DataFrom int,`

`...);`

DataFrom could then contain a 1 or 2 based on which Customer data set it came from.

`INSERT INTO Customers (DataFrom, FirstName, ...) SELECT 1, FirstName, ... FROM Customer1;`

`INSERT INTO Customers (DataFrom, FirstName, ...) SELECT 2, FirstName, ... FROM Customer2;`

I also wouldn't truncate/recreate the table every time someone opens the app, set it up on some kind of timer, a button, or even a cron.",1529163284.0
razin_the_furious,"use db_name;

Works for MySQL as well",1529095646.0
mordisko,Could you elaborate on what relations and columns you need in order to prevent the [XY problem?](https://meta.stackexchange.com/questions/66377/what-is-the-xy-problem),1529073792.0
mischiefunmanagable,"recommend reading up on https://en.wikipedia.org/wiki/Database_normalization

and then re-approach the problem",1529074070.0
DJDarkViper,"I tend to find that when it comes to tables like these, 98% of the columns are rarely ever indexed, used in conditions, or sorted against. They’re just (rightly) granulated key value pairs of meta information 

One COULD split the records up into multiple tables of related information. That certainly helps with the organization of it all.

Another approach is to take the fields that are doing nothing fancy, serialize them into a single parseable string (like JSON) and store them in a single TEXT field.  If you’re using the latest versions of MySQL (>= 5.7.8), you can take advantage of the native JSON field type which can not only make sure the data is always valid json, but can take advantage of the databases json processing functions (such as selecting or conditioning on a document key, virtual indexes on chosen fields, manipulating json data directly like appending or creating json constructs on the fly, etc etc) 
",1529077186.0
msiekkinen,"You mention ""about 2000"".   Keep in mind you're already half way there to a hard limit of 4096 columns mysql will allow per table.   ",1529080070.0
mordisko,"But how do you identify these genes? Do they really need all those attributes? If so noSQL might be for you, or a EAV pattern (or anti pattern if you will) where you have a meta table where one field identifies a gene attribute and another its value.

Be advised that this is easy to implement but makes it harder to query the information. ",1529083535.0
apaethe,"Graph databases are something I know next to nothing about.  That said you might want to check them out, as this seems like the kind of thing they are designed for.",1529454799.0
hambalamba,"You should start apache and mysql in xamp and go to ""localhost/phpmyadmin""(or if your apache server is served on a port you have to go to ""localhost:portNumber/phpmyadmin"" where portNumber is the number of port for example ""8080""). There you can add a new  database in the list on the left with the specified name. You can then click on the added database and go to structure to add a table. In database.json in the local object replace <username> with 'root' and the <password> with an empty string like this ''. This should get your local enviorment setup with the db you created. Next follow the instructions and add the league you want to the table. Gl and have fun",1529054870.0
davvblack,"DDL changes autocommit, so you can't have multiple inside of one transaction anyway.  

\> DDL statements, atomic or otherwise, implicitly end any transaction that is active in the current session, as if you had done a [COMMIT](https://dev.mysql.com/doc/refman/8.0/en/commit.html) before executing the statement. This means that DDL statements cannot be performed within another transaction, within transaction control statements such as [START TRANSACTION ... COMMIT](https://dev.mysql.com/doc/refman/8.0/en/commit.html), or combined with other statements within the same transaction.  


[https://dev.mysql.com/doc/refman/8.0/en/atomic\-ddl.html](https://dev.mysql.com/doc/refman/8.0/en/atomic-ddl.html)",1528996334.0
Bitter_Bridge,"I don't know specifically how they do it, but I'm guessing Top comments are literally the comments with the most net upvotes, whereas Best comments are comments that have many upvotes in a short amount of time, and/or a high ratio of upvotes to downvotes. The latter would be easy to do in MySQL using a table that stores total upvotes and total downvotes per comment. The former could be done in a few ways, but the easiest would be to add weight to a comment based on upvotes compared with age. Simple use case:

A comment made two hours ago with 500 upvotes and 200 downvotes has a net of 300 upvotes and could be considered the ""top"" comment. But, a comment that was made 20 minutes ago and has 200 upvotes and 5 downvotes could arguably be seen as a better comment. The performance of such a comment extrapolated out over the lifespan of the post could be projected at much higher than the top comment. If you wanted to get very in\-depth with your algorithm, you could attempt to predict where in its lifecycle the post is and add weight to the algorithm if the popularity of the post is expected to continue increasing, or if it is already petering out. Since the comment is also ""less controversial"" because it has a much smaller downvote ratio, it's likely more people will agree with it being considered ""best.""",1528990146.0
HexKrak,"Do some tutorials/training online, for free. For example: https://www.codecademy.com/learn/learn-sql
It won't take very long to master the basics. Then I'd probably suggest finding some sql query puzzles, where they give you a dataset and you have to create a particular outcome, because this is most likely what it sounds like you'd be doing and this sort of brain teaser is a great way to get accustomed to ways to go about it. Also, youtube, watch some people build queries, talk about stuff like query optimization. The difference between a 20ms query and a 2m query is often small simple fixes, keys, indexes, etc.
If you stick with it you can accomplish all this in a week or less. To really master the system can take years, but the problem solving skills you seem to possess by being self taught will take you 90% of the way there.",1528925719.0
,[deleted],1528936212.0
JackOfAllCode,Could you not just add a field to the table that would end date that row and add a new one or do you really need to overwrite that data? If you could just end date a field then whatever field is most recent would have an end date of null.,1528895858.0
Raphomee,Do you only want to update if the category-column matches or if any of your columns are matching?,1528896314.0
baka47,"For starters, one of my favorites https://www.safaribooksonline.com/library/view/mysql-fifth-edition/9780133038552/

Then for in depth, the online docs are pretty good, plus the internals section depending on how far down the rabbit hole you want to go: https://dev.mysql.com/doc/internals/en/",1528807817.0
original_evanator,http://shop.oreilly.com/product/0636920022343.do,1528806911.0
de_argh,"Are you wanting to learn to develop database applications using MySQL, or are you wanting to learn the administrative side?",1528816722.0
SuperQue,"I learned a ton of stuff from the [Percona performance blog](https://www.percona.com/blog/). 

Reading up on the InnoDB format is highly useful. MySQL performance behavior becomes very predictable and easy to understand once you look at how InnoDB and the query engine are connected.",1528876101.0
original_evanator,Can you post the `EXPLAIN`?,1528777406.0
berryer,"have you tried using `CREATE TEMPORARY TABLE`, inserting into it & joining?",1528774925.0
f0ad,"I don't know anything about PHP, so I can't tell you how to do it. And I don't know whats wrong with your script.

But the way you are building a string from variables to issue that UPDATE is a prime SQL injection candidate. I mention this because you said it's part of a woocommerce site which means customer/financial data could be at risk with this script.

It looks like you're just taking in HTTP POST data and directly building a query from it without using proper data binding. [This seems to be the manual entry that can help you with that specific problem.](http://php.net/manual/en/mysqli.quickstart.prepared-statements.php)",1528740999.0
SaltineAmerican_1970,What does the error log say?,1528740538.0
payphone,"userid is never set in your form, so POST['userid'] is empty, so $user is empty in your SQL, so no match in your where clause.

You should probably just do:

    $user = get_current_user_id(); 

instead of 

    $user = $_POST['$userid'];

$_POST['$userid']; is invalid anyway, it would be $_POST['userid'];
",1528744244.0
keithslater,Can you show us the table schema?,1528744295.0
payphone,"You still don't have a valid $user var in your Update/Replace. 

Either add: 
    
    echo('<input type=""hidden"" value=""' . $userid . '"">'); 

to your form, or just add user id after the form is submitted by doing $user = get_current_user_id();

This:
    
    //get user id on login
    $userid = get_current_user_id();

Is doing absolutely nothing since it is not in the form that is being submitted nor is it being changed to $user, which is what is in your query.
",1528771314.0
payphone,"You also don't have a name in this field, so it won't make it into POST either:

    //insert new address
    echo '<br><input type=""text"" value=""Insert new address here"" id=""address"" size=""40"" />';

Should be

    //insert new address
    echo '<br><input type=""text"" value=""Insert new address here"" id=""address"" name=""address"" size=""40"" />';",1528818066.0
r3pr0b8,"i think you meant to write `tb.TableB_ID` instead of `tm.TableB_ID`

add this --

    WHERE t.TableA_lastName <> tb.TableB_lastName

> In the long run, I actually have many fields to compare/update (First Name, Last Name, DOB, phone, address, etc.). 

my advice -- use this simple update query template to do one column at a time

trying to do them all in one large query will mess you up",1528730550.0
msiekkinen,You should not triggers.   The only thing they're useful for is online schema changes like pt-online-schema-change.   ,1528639432.0
SomeGuyNamedPaul,"Only use a trigger if you like pain.

[This is what you're really looking for.](https://mariadb.com/kb/en/library/mariadb-audit-plugin/). Yes it works with vanilla MySQL and Percona Server as well.",1528649892.0
gkodinov,"Depends on what you need audit data for.

For security reasons you need the failed attempts too. So triggers or two inserts in a transaction (that assume you've successfully committed stuff) will not do. 

Change tracking is what you'll get with both of these approaches. ",1529482802.0
,[deleted],1528606685.0
dubeymanish,Try running optimise table on every table. When we delete a record it does not free up the storage ,1528605648.0
TheSqlAdmin,"The reason for the space is, 

In RDS, tables are not only consumed your space.

1. BinLogs will use this. 
2. Error/Audit logs will share this space.
3. Whenever there is delete happens, then bunch of space will not reclaim the space until we run the Optimize table.
4. General / Slow query logs are also utilizing this space. ",1528702555.0
,[deleted],1528569326.0
scorsy63,"Here's what the pages actually look like on the browser


""My Quizzes"" Page (my mouse is hovering over the ""edit"" link on the 1st quiz)


https://imgur.com/luhtF7n


""Edit"" Page


https://imgur.com/a/GFqIbFP



What's weird is that, the 1st quiz, I can update the questions/answers just fine.

On the next quiz, nothing will update. I have the same problem on Mysql workbench.
",1528572416.0
razin_the_furious,Is the data directory in the configuration file writeable by whatever user is running the service?,1528559417.0
tomderudder,"remove all files in ""`{path-to-mysql}\data`"" directory and run:

`mysqld --initialize-insecure --basedir={path-to-mysql}\mysql --datadir={path-to-mysql}\data --console`",1541608748.0
LobbyDizzle,"You are probably receiving an error with the GROUP BY. Remove that and it should run fine. 

The GROUP BY should only be used if you're trying to use aggregate functions (SUM, AVG, etc). Also, when using a GROUP BY, you cannot use the select all (asterisk) function in the select statement, and also need to GROUP BY all non\-aggregate columns.",1528556243.0
keithslater,How are you connecting to it? Through ip? Hostname? Sounds like something changed there. ,1528517241.0
mudclub,"> There is a central computer we all have personal directories on that we can access remotely through terminal so I imagine this would be a good starting point but honestly I have no idea.

That's probably the best way for your scenario.

Write a script that does what you want it to do.  Drop it in a shared folder that everyone has access to.  Tell them about it.",1528504094.0
ge0n1,You have a job? Lols. ,1528504065.0
jynus,SOURCE is the server command you are looking for. Example: https://github.com/mysql/mysql-sys/blob/master/sys_57.sql,1528499849.0
digicow,https://stackoverflow.com/questions/1734334/mysql-length-vs-char-length,1528478219.0
r3pr0b8,"    SELECT offer_id
         , status 
      FROM tab.offers
     WHERE whatever_your_date_column_is = '2017-12-31'",1528451158.0
trashpantaloons,"I think the thing you want to look at is a trigger on the links table - it may be able to update a separate table (users) with a count(*). 

If you ever plan for users to have the option to Delete links this would cleanly solve when they are deleted. Otherwise you would have to manually do +1 or -1 events separately.",1528413925.0
SomeGuyNamedPaul,"MySQL installs include a tool called perror

    perror 28

That will spit out your error.",1529465116.0
SaltineAmerican_1970,"You should really learn to use this magical thing called the Internet. In fact, if you visit the website to the application you’re running, you may find the exact answer you’re looking for. https://dev.mysql.com/doc/refman/5.5/en/error-messages-server.html

Error: 1004 SQLSTATE: HY000 (ER_CANT_CREATE_FILE)

Message: Can't create file '%s' (errno: %d)

Occurs for failure to create or copy a file needed for some operation.

Possible causes: Permissions problem for source file; destination file already exists but is not writeable.",1528385545.0
jous,You didn't include the schema.  ,1528383668.0
mvelasco93,"Well, better ask that on the exchange forums or Google it. I haven't had that problem. It could be incompatibilities. ",1528344922.0
joatis3,"The UPDATE should only be locking the table if it is not an innodb type table. MyISAM puts on a table lock with every update but innodb has row level locking. 

First, is this special column the script checks indexed? If it is, is it being used by the script? I would run the scripts queries with EXPLAIN to see why it is taking so long. 300k isn't really a lot of rows if it's looking at a single column for a single value.

Second, it's hard to say without an example query of the update. Are you updating all rows IF the column has a certain value? I would do 1 query to get all the primary keys of the rows where the column is a certain value and then issue 1 update query where the primary key is in that list.

Third, again hard to say without an example but, if you are doing an update for all the primary keys, are you using OR or IN? I found you will get a great performance boost by changing a where clause from:

    WHERE key=val1 OR key=val2 OR key=val3 ....

to 

    WHERE key IN (val1, val2, val3, ....)",1528336752.0
mischiefunmanagable,example of the half-hour query?,1528323309.0
martin_n_hamel,It's not going to happen easily except if your application is very very small. There are many differences in those two.,1528328851.0
knifebork,"It's possible, but it's not a simple drop in replacement. While the fundamentals of design, normalization, and querying are extremely similar if not the same, there are differences in syntax and features. 

For example, there's slightly different syntax for retrieving the first X records from a table.

MySQL: SELECT * FROM TestTable WHERE id=12 LIMIT 10

MS-SQL: SELECT TOP 10 * FROM TestTable WHERE id=12

There are more examples here: https://www.mssqltips.com/sqlservertutorial/2204/mysql-to-sql-server-coding-differences/ This should just be considered an example. There are other things like quotes vs. back ticks and how comments are marked.

These are things that a decent programmer or DBA could adapt to quickly, but if you've got a large application, you'll need to find, fix, and test all of the changes needed. 

Next there are features outside of SQL syntax like backups and replication. If your system depends on depends on periodic imports and exports using external utilities, those would need a close look.",1529852795.0
TheZeven,"I wait for the cries of users to alert me to an issue. /s 

I do need better monitoring and would love a wonderful solution.  I know there's things out there like Nagios/Icinga but I've never been overly happy with it.   Looking forward to seeing more on this thread.",1528320726.0
Irythros,Percona Monitor.  It does query analyzing so we get that in addition to the server reporting.  It also give mysql specific stats.,1528360983.0
jynus,"Many things, but the main public stuff can be seen here: Prometheus mysql exporter and Grafana:

* https://grafana.wikimedia.org/dashboard/db/mysql-aggregated
* https://grafana.wikimedia.org/dashboard/db/mysql?orgId=1&var-dc=eqiad%20prometheus%2Fops&var-server=db1091&var-port=9104",1528547764.0
mischiefunmanagable,"datadog with some custom checks, kind of overkill but every key with a numeric value in global status and global variables has an entry that we track

most of the custom checks track business numbers, like registrations in the last X minutes, things like that

we alert on a lot more than we should but certainly load and any time swap is touched, disk usage, and disk space growth over X% in a period, query count over X and query time over X",1528323043.0
BrushyAmoeba,Anyone have experience using GitHub’s Orchestrator to manage (and monitor?) replication topologies? ,1528327126.0
razin_the_furious,A sum is the addition of all the numbers,1528288005.0
,[removed],1528292019.0
visiblebus,just need the code tho,1528287318.0
razin_the_furious,What does the explain say for your query?,1528288076.0
r3pr0b8,the answer is no,1528296902.0
jous,"Rule #4: __Provide all relevant information - Whenever a question is asked, as much information as possible should be included. This includes Schema, query, error message, etc.__

Could you give us an example of the data? Is it in the form 123-234-234? What do you mean by area code? Do you have the area code in another form? As an address? What country are you in? What is the name of your column(s)?

Please try to put yourself in our shoes. We are not mind readers. It is certainly not possible to understand your problem with the information you've given so far.",1528230255.0
razin_the_furious,"    SELECT * FROM table WHERE phone_number like ""areaCodeDigits%"" LIMIT 100000;",1528222114.0
justintxdave,A quick article on the MySQL Document Store for opensource.com,1528205004.0
Fiskepudding,"Doesn't matter, I think they open the same installer. It's a program to choose what you actually want to install.
Install both mysql server and mysql workbench. 

If you install mysql 8, you should get the preview edition of workbench. ",1528177596.0
joatis3,"use a join table that consists of class_id, module_id. That way you can list them and use joins to get the details.
SELECT * from class, module, class_module 
where class.class_id=class_module.class_id and module_id=class_module.module_id.",1528174829.0
martin_n_hamel,"Instead of putting a boolean value in the database, you can set a timestamp of when the value was inserted. The you query for the time difference when you need it. I don't know your use case but something like:

`select * 
from table
where timestamp > now() - interval 1 hour`

It depends of what you are trying to do be it should be something like that.",1528193934.0
jericon,You could setup an event that on a scheduled basis runs an update. ,1528175258.0
,[deleted],1528140837.0
apaethe,"One way you might simulate dynamic tables would be to use a single table partitioned on a group id.  [Partitioning](https://dev.mysql.com/doc/refman/8.0/en/partitioning.html) is an advanced feature with it's own caveats, but you might want to check it out.  

There are a few advantages it offers.  From the documentation: 
> Some queries can be greatly optimized in virtue of the fact that data satisfying a given WHERE clause can be stored only on one or more partitions, which automatically excludes any remaining partitions from the search.

and

> Data that loses its usefulness can often be easily removed from a partitioned table by dropping the partition (or partitions) containing only that data. Conversely, the process of adding new data can in some cases be greatly facilitated by adding one or more new partitions for storing specifically that data.",1528151308.0
SomeGuyNamedPaul,If you're just a beginner then now is not the time for optimizations.,1528154661.0
chinahawk,"Set the RDS instance to 'Publicly accessible = no';
Setup a small EC2 instance w/ OpenSSH Server running;
Ensure that the EC2 instance is on the same VPC and/or can talk to the RDS instance;
Connect to the RDS instance thru an SSH Tunnel via your EC2 instance;
Buy me a beer; ",1528148190.0
mtocker,"The most popular proxy for MySQL is ProxySQL.

The configuration you mentioned is not currently supported, but [will be soon](https://github.com/sysown/proxysql/issues/891) in 2.0.",1528171852.0
xenilko,That’s exactly what the TEXT type is for :),1528114563.0
chrisguitarguy,The TEXT data type is what you are looking for: https://dev.mysql.com/doc/refman/8.0/en/blob.html,1528114563.0
jericon,"Your post has been removed.

Reason: Self Promotion

Your post history shows that you have a propensity to post from the same domain or topic often to a wide breadth of somewhat related subreddits without contributing to the communities otherwise.  Please see [Reddit's Self Promotion Policy](https://www.reddit.com/wiki/selfpromotion).",1528242748.0
carpii,"Join to the players table twice (with different aliases of p and p2)

	select 
	  g.home_team, 
	  g.runs0, 
	  g.away_team, 
	  g.runs1, 
	  case when g.starter0 = p.player_id then concat(p.first_name, ' ', p.last_name) else g.starter0 end as 'Home Pitcher', 
	  case when g.starter1 = p2.player_id then concat(p2.first_name, ' ', p2.last_name) else g.starter1 end as 'Away Pitcher', 
	  g.date, 
	  g.winning_pitcher 
	from 
	  games as g 
	  inner join players as p on (p.player_id = g.starter0) 
	  inner join players as p2 on (p.player_id = g.starter1) 
	limit 10;

A better way would be to redesign your schema though, so you have tables like this...

Teams -> Games -> Lineup -> Players

Lineup would be a lookup table to map the gameid to each players id

",1528065746.0
jericon,"Your post has been removed.

Reason: Self Promotion

Your post history shows that you have a propensity to post from the same domain or topic often to a wide breadth of somewhat related subreddits without contributing to the communities otherwise.  Please see [Reddit's Self Promotion Policy](https://www.reddit.com/wiki/selfpromotion).",1528242733.0
,[deleted],1527812356.0
MyDataBes,"true: 1) Don't use limit offsets. They are extremely slow. You should be iterating in the form of: ""where id > ? order by id limit 1000"" -- Your parameter should be the last id from each set.
i only use this for ad-hoc queries.

I had similar situation to you before. since my ids is auto incremental i created a stored procedure and used simple math to limit it. 
",1527813213.0
hungryballs,"A quick and dirty solution which might answer your question would be to just connect to MySQL while the import is running and run “show processlist”.  This will show you what’s running and how long it’s been running for. If you do this a few times you’ll get an idea of what statements are taking a long time. 

Another alternative would be to switch in the slow query log and see what gets logged after the import. ",1527749124.0
jahayhurst,"If you are sourcing a file external to MySQL instead of loading it inside of MySQL \- if you can help it, don't do:

    mysql -e ""LOAD DATA INFILE filename.sql"" dbname

Instead do something like:

    mysql dbname < filename.sql

Cause then you can:

    pv filename.sql | mysql dbname",1527803968.0
ilogik,"or, create a log table, with just a timestamp and text.

and modify the file to add:

INSERT INTO log VALUES(NULL, ""some text"");",1527764727.0
clooy,"If you can run your script using  a privileged account you can enable slow query logging which can specify that a slow query is anything longer then 0s - essentially logging everything.

At the beginning of your script simply run

    SET GLOBAL slow_query_log = 'ON';
    SET GLOBAL long_query_time = 0;
    SET GLOBAL slow_query_log_file = '/path/to/query.log';
    
And to turn it off do

    SET GLOBAL slow_query_log = 'OFF';

This will log all your queries with start time, and total execution time in seconds

To output a got here you can add comments to your SQL, eg

    --- Got Here Comment
    SELECT 'I CAN ALSO DO GOT HERE' As Comment

Alternatively set `long_query_time = 10` and do

    --- Got Here Comment
    SELECT SLEEP(11)

Note, I have added comments to SQLs using python, .net, perl and many more and never had a problem from the many database handlers - it's always a sad thing to never see comments in the SQLs when going through log files.",1527765827.0
razin_the_furious,"Are you trying to add a column to a table?

Or put a value in a column that already exists?",1527693618.0
jericon,This post has been removed.  Please repost with text instead of an image.,1527800512.0
aqbabaq,"The only common thing between those 2 tables is ClubName so this is what you need to build your join on. So the query will look like somethign like this

    select p.PlayerName,p.Age,p.Nacionality,p.ClubName,c.Country from players p, clubs c 
    where    p.ClubName=c.ClubName and p.Nacionality='Portuguese' and c.Country='Portugal' ;

My table data
    
    select * from players ;
    +----+---------------+------+----------+-------------+
    | id | PlayerName    | Age  | ClubName | Nacionality |
    +----+---------------+------+----------+-------------+
    |  1 | Deco          |   28 | Chelsea  | Portuguese  |
    |  2 | Raul Meireles |   26 | FC Porto | Portuguese  |
    +----+---------------+------+----------+-------------+
    select * from clubs ;
    +----+--------------+----------+
    | id | ClubName     | Country  |
    +----+--------------+----------+
    |  1 | FC Porto     | Portugal |
    |  2 | Real Madrird | Spain    |
    +----+--------------+----------+

Result of the query

    select p.PlayerName,p.Age,p.Nacionality,p.ClubName,c.Country from players p, clubs c where    p.ClubName=c.ClubName and p.Nacionality='Portuguese' and c.Country='Portugal' ;
    +---------------+------+-------------+----------+----------+
    | PlayerName    | Age  | Nacionality | ClubName | Country  |
    +---------------+------+-------------+----------+----------+
    | Raul Meireles |   26 | Portuguese  | FC Porto | Portugal |
    +---------------+------+-------------+----------+----------+

PS. read about normalization

https://en.wikipedia.org/wiki/Database_normalization

Have fun
",1527691292.0
Fiskepudding, 8.0 should be backwards compatible. Are you sure this is required? ,1527659226.0
seb-witt,"How about dropping the quote in the where clause? Though I don't know how effective that is performance wise. Probably not that good on big table, but should get the job done

    WHERE REPLACE(webTitle, '\'', '') = posTitle

The postitle I would normalize outside of the db though.",1527632808.0
r3pr0b8,"run this query --

    SELECT COUNT(*) as howmany 
      FROM subscribers
     WHERE email = '$mail'

it'll come back with howmany equal to zero or one

put that in your IF",1527617749.0
davvblack,What adapter/library/language are you calling mysql from?,1527612643.0
jous,What are the warnings?,1527584990.0
jous,"Do you have access to the mysql database through the mysql client? If yes, you could enter the query a little bit at a time. First run line 1, then 3-62, then 64 and finally 66 and 67. 

This will help you find the offending line, which is probably line 1. The DROP PROCEDURE [manual](https://dev.mysql.com/doc/refman/8.0/en/drop-procedure.html) says:

    The IF EXISTS clause is a MySQL extension. It prevents an error from occurring if the procedure or function does not exist. A warning is produced that can be viewed with SHOW WARNINGS. ",1527685557.0
r3pr0b8,"you need only the personID column...

    SELECT personID
         , CONCAT('http://app.mywebsite/database/personURL[',personID,']') AS personURL
      FROM yourtable
",1527542938.0
mrivorey,"Is there code (like a website or app) that goes along with the database?  If so, you don't need an extra column for personURL.  The code should construct it outside of the database.  Storing the same data twice is bad practice if you can avoid it.

        var personURL = URLPrefix + personID;",1527551108.0
mikegarde,"The previous answers would accomplish what you are looking for but the job should get accomplished by your application. Your application is responsible for responding at that url, not your database. So you'll never store the prefix in your database. I know you could but why? Are you going to change it, if so change it once in your app. If however there is a personalization url like /people/{custom}-{userId} then I would suggest your app allow wild cards for the custom part then lookup the userId, if the custom part is wrong you redirect the user, if it is correct you proceed.",1527561168.0
apaethe,"Check out some of the other SQL clients out there.  HeidieSql is nice and free, and SqlYog is good.",1527537240.0
chocodrpep,"Workbench is an application that runs on the computer.  PHPmyadmin is a web app (written in PHP, hence the name) that is accessable from the web.",1527533300.0
crackanape,"One difference is that PHPMyadmin is a flaming pile of dogshit, that you have to religiously keep up to date to keep your server from getting p0wned, whereas MySQL Workbench is at worst only mildly annoying and these days doesn't even crash all the time like it used to.",1527534597.0
chocodrpep,"That I can't answer, I'm sorry.  I only knew of PHPmyadmin when I started and because of that I preferred it when I tried Workbench.  Only used workbench the once",1527536193.0
chocodrpep,https://www.google.com/amp/s/www.07heavendesign.co.uk/insight/blog/mysql-vs.-phpmyadmin-in-web-development%3fhs_amp=true,1527537236.0
cYzzie,I domt even know what “the model designer is” - i create everything with queries,1527528305.0
seb-witt,"Can we see the real query?

I mean the errors are pretty clear, there is an issue. Perhaps a non printable character?",1527544440.0
areti33,To completely resolve all sort of problem in MySQL database. You can use a reliable [MySQL Database Recovery Tool](http://www.databasefilerecovery.com/mysql-database-recovery.html). It will help you to resolve problem present in both storage engines of MySQL database efficiently.,1540190730.0
r3pr0b8,"for those whose screens aren't seven thousand characters wide, i reformatted it for you --

    INSERT 
      INTO log
         ( query_time
         , query
         , script_name
         , user_id ) 
    VALUES 
         ( '20180529 00:43:02 AM'
         , 'SELECT * FROM user WHERE username = \'test\';'
         , 'C:\\xampp\\htdocs\\project\\login.php'
         , 1);

the first value is invalid for MySQL

try it without the ` AM`

if that still doesn't work, consult the manual under [Date and Time Literals](https://dev.mysql.com/doc/refman/8.0/en/date-and-time-literals.html)",1527520930.0
kaydub88,pdo,1527516734.0
razin_the_furious,What mysql error are you getting?,1527514224.0
Idontremember99,"You should be looking at the error messages it gives you. What is the CREATE statement for that table? If query\_time is DATETIME you are using the wrong syntax for it.

Also, use prepared statements instead of escaping the data.",1527519625.0
Sakred,"If you're ever wondering if you have an error in code vs a query, simply run the query directly on the database and see if it works. ",1527527893.0
bike_boy26,"For readability, it’s better to use “ instead of ‘. This will also remove the necessity to escape the strings inside the “. ",1527572741.0
chocodrpep,"I'm still learning myself, but when I did something similar, I had to follow this tut:
https://community.modeanalytics.com/sql/tutorial/sql-subqueries/

Fir your nested SQL, I think your formatting is off and it is mistaking your query for an input.",1527512335.0
mudclub,It's a good version to learn on.  What are you looking to learn?  It's a SQL database.  Learn SQL.  The database doesn't matter unless you want to be a dedicated DB-specific DBA.,1527506429.0
,[deleted],1527529448.0
TheSqlAdmin,"MySQL 8.0 is proving almost all the expected features from its earlier versions. If you want to learn 8.0, then just check the What's new in 8.0. OR if you want to learn entire MySQL then there are so many courses available in the Internet. ",1527509341.0
mcstafford,"Why do you want to mimic, rather than actually cascade?

Depending upon the context I may use status flags, rather than actually remove records.",1527408195.0
seb-witt,Just edit the column and remove the attribute on update current timestamp and it will not change unless you do it yourself,1527419925.0
apaethe,"You **can** use row specific variables.  Local variables can be set per row.  Here is an example of a basic SELECT.

    SELECT @my_var := t.col1 /* the := sets the variable for each row */
      , REPLACE( t.col2, @my_var, '' ) AS col2_with_instances_of_col1_string_removed
    FROM tbl AS t

You can add your joins in and wrap in an update to do what you need.

Although are you certain you need to?  The replace you have there shouldn't have the quotes around the `table3.name`.  Maybe that is the problem.

",1527449232.0
r3pr0b8,"the next part of that error message tells you where the error actually occurred

we might be able to help if you showed that part, as well as the block of sql you were trying to execute",1527326861.0
msiekkinen,"This is going to be something more to do with terminal settings than mysql client itself.

What OS and terminal client are you using?

Could also use a graphical client

",1527274943.0
apaethe,"Looks great!  The narration/videos are very well done and were easy to follow along.

Were you considering adding a section on indexing?  I think people would find that helpful.

Great work, cheers!
",1527211078.0
xproofx,Awesome.  I will take a look.  Thanks!,1527210334.0
Multitasker123,Thank you!! Sharing really helps this community.,1527210677.0
cYzzie,Xammp in 2018? Oh dear,1527218320.0
jmflna,"Thank you, op!",1527222524.0
msiekkinen,"I'd like to explain why limit, offset is bad to see work it's doing under the hood, especially with your order by.

Each iteration it's going to have to scan for the whole result set into a behind the scenes temp table, then sort that from what ever the where clause matches.   Maybe your indexes helped some but that's still work for the sort.

Each limit/offset is doing the same thing but then scanning further and further only returning the tail end.  For example with a limit 5, offset 5*iteration


limit 5, offset 0

    |XXXXX|

limit 5, offset 5

    |-----XXXXX|

limit 5, offset 10

    |----------XXXXX|

Each ""page"" is going to take longer and longer to scan through.   If you have an auto increment PK this can be done much more efficiently by both being indexed and clustered (meaning data is right next to the index, no secondary pointer lookups to the PK to actually grab the data behind the scenes).

    where PK between 0 and 5
    where PK between 6 and 10
    where PK between 11 and 15

etc...

There are some pitfalls though, depending on what you're doing,  once you've done a range, if a change happens to that range it's not going to be point in time reflected to future pages as you iterate.   

",1527281405.0
SaltineAmerican_1970,"If you have a distributed database or master & multiple slaves and 5 containers, you can run 5 queries in parallel, each over a fifth of the records. Then process them in parallel and you will be done 5 times faster.

Or more or fewer, depending on available resources.",1527147129.0
aspvirx,"Im curious about your solution. Since I don't know mich about your table, do you mind sharing the logic behind the optimization you came up with please? :-) It looks nifty, good job .",1527174751.0
apaethe,"If the table is not being written to you can create a copy of the table, add the index on column2 to that, then ""hotswap"" them with with RENAME.

You may also be able to do this even if the table is being written to, depending on your specific use case.  Say for example the table is only taking inserts:  you could then first make a copy, add the index to the copy, left join original with copy to insert the records that may have been added while the index was being created, then run the rename, and finally compare again to insert any missing records.  Remember foreign key considerations, and triggers too if doing something like this.

Finally, I've worked with large tables of similar size and been able to add indexes without that being a blocking ALTER/causing any problems.  Perhaps someone more familiar with the mysql vagaries could confirm if this is correct.  Either that or you could yolo it and report back!

Edit:

One more trick regarding the copy of the table thing.  If the table is taking updates and it is critical that everything stay perfectly in sync, I think you could make a copy of the table and add a trigger to the original table to mirror any updates onto the copy.  Manually run updates to get them in sync then verify that the triggers are working to keep the copy up to date.  Then run the RENAME hotswap.",1527208359.0
mischiefunmanagable,"google pt-online-schema-change
",1527168145.0
matjam,The simplest way would be wrap an INSERT INTO ... SELECT with the UPDATE for the other table into a transaction.,1527055679.0
apaethe,"Triggers are great for storing histories.  Consider using a key / value table for the history table, where the key is the column being updated and the value is the old value.",1527211680.0
elgselgs,">What are some different approaches for tackling this ? I tried unsuccessfully to consolidate into one >query and keep joining on the same tables with different aliases but that wasn't working.

**INNER JOIN**, **LEFTER OUTER JOIN** are most common join types, JOIN doesn't seems to be standard SQL, but it's more like an alias to **INNER JOIN**. You could also use **EXISTS**, **IN**, or subquery. If you have a small dataset, they might not have much difference. The choice is all about index. **JOIN**s, **EXISTS** should be considered first, because there's no derived table created, so any indexes on the tables should be utilized. **IN** and subquery should be avoided if they could be done with **JOIN**s and **EXISTS**.

>Is it possible to write one as a subquery as another and do a street equality comparison in a WHERE clause ?

Yes, it's possible. But due to the fact that subquery creates derived table when your SQL is being executed, it should be avoided.

I'm sorry I had some hard time understand your original question. If you could breakdown your questions and make them simpler to read, it might be easier to target.
",1527048374.0
mshakhs,"There are no extra \*'s in the above code

 [**SELECT**](https://mariadb.com/kb/en/library/select/) FOO.RACE,GENDER, \([**COUNT**](https://mariadb.com/kb/en/library/group-by-functions/#function_count)\(**DISTINCT** OE.PATIENT\_ID\)/RACECOUNT\)\*100 **AS PERCEN**T  
**FROM** [**OBESITY\_ENCOUNTER**](https://km-db.uthsc.edu/adminer/adminer/index.php?username=malshak1&db=obesity&table=OBESITY_ENCOUNTER) **OE,** [**OBESITY\_PATIENT**](https://km-db.uthsc.edu/adminer/adminer/index.php?username=malshak1&db=obesity&table=OBESITY_PATIENT) **OP,** [**OBESITY\_DIAGNOSIS**](https://km-db.uthsc.edu/adminer/adminer/index.php?username=malshak1&db=obesity&table=OBESITY_DIAGNOSIS) **OD,** [**OBESITY\_DIAGNOSIS\_ENCOUNTER**](https://km-db.uthsc.edu/adminer/adminer/index.php?username=malshak1&db=obesity&table=OBESITY_DIAGNOSIS_ENCOUNTER)**DE,**  
**\(**[**SELECT**](https://mariadb.com/kb/en/library/select/) **RACE,** [**COUNT**](https://mariadb.com/kb/en/library/group-by-functions/#function_count)**\(DISTINCT OE.PATIENT\_ID\)** AS RACECOUNT  
**FROM** [OBESITY\_ENCOUNTER](https://km-db.uthsc.edu/adminer/adminer/index.php?username=malshak1&db=obesity&table=OBESITY_ENCOUNTER) OE, [OBESITY\_PATIENT](https://km-db.uthsc.edu/adminer/adminer/index.php?username=malshak1&db=obesity&table=OBESITY_PATIENT) OP  
**WHERE** OE.PATIENT\_ID=OP.PATIENT\_ID  
**GROUP** **BY** RACE\) FOO  
**WHERE** OP.PATIENT\_ID= OE.PATIENT\_ID  
[**AND**](https://mariadb.com/kb/en/library/logical-operators/#operator_and) DE.DIAGNOSIS\_ID=OD.DIAGNOSIS\_ID  
[**AND**](https://mariadb.com/kb/en/library/logical-operators/#operator_and) DE.ENCOUNTER\_ID=OE.ENCOUNTER\_ID  
[**AND**](https://mariadb.com/kb/en/library/logical-operators/#operator_and) FOO.RACE=OP.RACE  
[**AND**](https://mariadb.com/kb/en/library/logical-operators/#operator_and) DIAGNOSIS\_CODE [**LIKE**](https://mariadb.com/kb/en/library/string-comparison-functions/#operator_like) '250&#37;'  
**GROUP** **BY** RACE, GENDER  
**ORDER** **BY** 3 **desc** ",1527041283.0
xilanthro,"...and not playing into the flame-war either, but there are other players out there (like MariaDB, etc) worth considering. My first thought reading that your supervisor is contemplating a change to MS SQL is that literally everything is different: the platform and associated costs, the eco-system of add-ons and plug-ins, the recourse when things crash, the connectors especially. Maybe you would also want to throw in some sort of grid showing how many aspects differ independent of performance as well, since I would think this is a huge part of the equation in most situations and possibly they are not aware of how deep a change that is...",1527014025.0
EvanCarroll,"Moving away from MySQL is totally understandable, but why not move to the most advanced database which is free, like PostgreSQL?

You can't publish benchmarks on SQL Server at all. It's against the EULA.",1527022458.0
kaydub88,"Why is your organization contemplating changing?

There will be a TON of costs in changing, something to think about.",1527089610.0
FoCo_SQL,"I'm curious as to why not Aurora? It's MySQL and PostgreSQL with some Amazon proprietary magic sprinkled in.

I love SQL Server, it will do a lot of things well. It is also very expensive. (I also love Oracle, 

It all comes down to the right tool for the job. What do you NEED? What is your price range? What are deal breakers? Figure out your business needs then pick the tool that best aligns with it. Whether it is a NoSQL solution, Oracle, MySQL, etc, it's always about the best tool to fit the business needs. Don't drive the needs of your business with your tools, let the business drive the tools you use to accomplish your tasks.",1527262519.0
r3pr0b8,"> I'm not very clear on how 'Primary' or 'Unique' keys function

then perhaps do some research before attempting ON DUPLICATE KEY 

let's start by you running this code, but instead of taking a screenshot, please copy/paste the actual text of the results here

    SHOW CREATE TABLE mainclosings",1527002962.0
etrnloptimist,"There isn't a built-in function to do this.

But here is a thread showing all sorts of examples on how to do it.

https://bugs.mysql.com/bug.php?id=2340",1526993438.0
apaethe,"Assuming you are using case insensitive collation ( end in _ci ), you could probably do this by creating a alphabet table with the capital letters.  Then do something like

    update mytable
    join (
       select id, alphabet_table.letter
       from my_table 
       left join alphabet_table on my_table.capital_name like concat(alphabet_table.letter,'%')
    ) sub on my_table.id = sub.id
    set my_table.captial name = [subtable string munging left as excercise]
",1527212330.0
seb-witt,"How are you displaying them on your website? Why restrict data storage like this? You could never restore the original in case you ever need it \( granted in your example you wouldn't \) unless you store a copy.

Not to talk about the unneeded work involved.

Just alter it when presenting the data on your website.

`ucwords(strtolower($title));`  in php for example",1527290912.0
justintxdave,Which version of the installer are you using?,1526992805.0
Zaphod_B,"Not a DBA but I have done many MySQL installs and upgrades.  The first place you should check is in the MySQL error log.  It may tell you what dependency you are missing.  I also do not have much experience on Windows.  On Linux, however, if I am missing say the python libs it will tell me I am missing that dependency.   I have no idea how Oracle builds the software packages for Windows though.

Also, are you running the MySQL upgrade script to upgrade MySQL?",1526959663.0
cYzzie,"Use vagrant and install it inside a virtualbox container, much easier than dealing with windows",1526905252.0
emdash5,"Replace ""group by name"" with ""group by machine""",1526871144.0
bangmygong,I'd write a Python script to check when the modified_ts changes on the txt.file.  If it changes I'd then run the `load data infile` statement.,1526823430.0
,[deleted],1526872790.0
xilanthro,"It's this simple:

    update parts, tasks
    set parts.taskID = tasks.taskID
    where parts.taskID = tasks.taskIDOld;
",1526786686.0
apaethe,"How about doing something like this?

    select p.code
      , p.description
      ...
      least( [code_terms_final_price], [cat_terms_final_price] )
    from products p
    left join terms as code_terms on code_terms.code = p.code
    left join terms as cat_terms on cat_terms.cat = p.cat

where the final prices are your calculations using the values from the alternate copies of the terms.",1526692469.0
xilanthro,"I like the MariaDB idea, but switching without solving your problem won't fix anything unless you're only switching to give yourself an excuse to start with a clean my.cnf. 

There are some tweaks in your config file that don't make a ton of sense together, like turning off doublewrite while setting flush log at trx commit = 1.

Without getting into too much detail, here's some things that will make your problem go away:

1. Reduce your innodb_buffer_pool_size by 1G to free up some RAM right away.
2. set wait_timeout = 600
3. set a cron job to run flush tables periodically - this will release a good bit of RAM",1526648235.0
roguelazer,`mysqlclient` is an up-to-date version of MySQLdb and uses your system libmysqlclient C library; it should work with mysql 8 as long as you have a new enough libmysqlclient,1526594199.0
philipolson,"Consider trying the [official connector](https://dev.mysql.com/doc/connector-python/en/) instead; e.g., pip install mysql\-connector\-python",1526747483.0
PM_ME_YOUR_HIGHFIVE,isn't that the reason why [Views](https://dev.mysql.com/doc/refman/8.0/en/views.html) were added to DBs?,1526564062.0
cYzzie,"Good indexes should be enough, otherwise you might want to think about caching or search technologies (memcache, sphinxsearch, elastic etc)

A cache table nowadays is not a good choice anymore. ",1526573179.0
angusmcflurry,"This is the general idea behind a data warehouse.  If your data is organized well for your business logic, but not necessarily for reporting, you can create a separate DB or table / tables with the same data laid out in a format that's optimized for reporting / searching.  You can then setup triggers, or procedures, or code to update your search / reporting tables when your other tables are updated.

I've done this with sucess in the past - also lets you build way more useful dedicated reports - especially big ones that run on an automated schedule.",1526580704.0
chinahawk,"INDEX any column in the WHERE, GROUP BY, JOIN and/or ORDER BY clause. (pretty much always)
",1527015316.0
apaethe,"Keeping the aggregate totals in a separate table can be a good idea.  Check out Mondrian if you are interested.

If you just want to optimize the one beefy query, the one thing I would say to do is to figure out what your biggest filters are, and then do those first.  You can often do this by putting those into sub queries in your FROM statement.

Often you will have ""data"" table and ""reference"" tables.  With the data tables being the big ones that get written to often.  Put these into subqueries in the FROM statement and apply WHEREs there on your good indexes.

**Edit**

Also if you add a sqlfiddle then we could get down to brass tacks :-)
",1527213304.0
crackanape,"group_concat performs okay in most circumstances, but there's a limit to how much you can pack in there, and you have to change server settings to exceed that. In your scenario I don't think you'd need a subquery.

The best approach depends on what you're really trying to do, how much data you have, how often you will be querying, etc. Maybe it's better to cache the results in a key-value store for a while. ",1526519458.0
ajanty,Maybe transaction isolation fits for your case. But use it carefully: with great power comes great responsibility.,1526468962.0
msiekkinen,"Deadlocks are not bad, they should be expected and your application should retry appropriately.  Meaning start back over and recheck any assumptions made before committing the write ",1526481438.0
xilanthro,"First: It is a terrible idea (a ""loop-cowboy"" idea) to retry a transaction on a loop because it deadlocked. There should always be a wait before retrying, because, obviously, if the retry starts before the transaction A (that caused transaction B to deadlock) completes, you're just creating a bigger and bigger traffic-jam of locked resources that will hold locks on larger extents promoting more deadlocks.

That said, if you look at the relational mapping of transactions, it is mathematically possible to create a systems where intrinsic structural deadlocks won't happen, if you control all queries and isolation levels. Totally not worth it.

The other kind of deadlock is practically impossible to avoid: given sufficiently high transaction volume and any transaction isolation level above 'read committed' (so from 'repeatable read' to 'serializable') transactions use gap-locks. With gap-locks deadlocks are mathematically impossible to predict (since you have no guarantee for independent keys having specific values anyway)

So the vagueness in any assertions about avoiding deadlocks is just honest, since a deadlock is basically a probabilistic phenomenon. You can tune systems to have rapid small transactions, to always lock resources from the general to the particular, to avoid gap-locks, and end up with something that has very few deadlocks, but 0 deadlocks is not a realistic goal for an ACID relational database.

EDIT: For a nice reference implementation on handling deadlocks, have a look at https://github.com/qertoip/transaction_retry",1526870902.0
jericon,Thank you all for participating in this discussion.  I will be posting another one shortly.,1528310890.0
SomeGuyNamedPaul,"xtrabackup, then rsync or ssh pipe",1526391712.0
jwestbrook,Nightly RDS snapshot and change the setting to 7 days of point in time backups. Also a Lambda function that creates a nightly mirror database of yesterdays data.,1526410478.0
de_argh,replication and mysqldump on slave.  rsync backups and binary logs locally as well as offsite.,1526394618.0
SuperQue,"At my last job doing MySQL, everything was backed up using an async replica, even when each cluster was xtradb.  We had lots of problems with backups causing xtradb to block writes.

All backups were daily

We did both xtrabackup, and mydumper.  xtrabackup would stream backups directly to HDFS via a streaming PUT to webhdfs.  mydumper ran remotely, so we could dump to a staging box and upload from there.  All backups were compressed with lz4 and encrypted with AES-256.

One trick we discovered with mydumper was to use the ""rows per dump file"" option.  This wasn't really interesting for backups, but it helped a bunch for doing restores with myloader on large tables.

One feature I still wish for in mydumper is to [add secondary keys after data loading](https://github.com/maxbube/mydumper/issues/4).  But I stopped caring as much since I don't really use MySQL in production anymore.",1526569317.0
Coder_X,on windows unfortunately with a 1.5 TB DB so I use oracles mysql enterprise backup.  only real option that i know of.,1527012538.0
elgselgs,"Delayed replication. We have several slaves, let's say 4 for this purpose. Slave0 starts replication at 00:00 for 1 hour to catch up with the master, and stop the replication, Slave1 starts at 06:00 for 1 hour and so on. So at any point of time, we can go back to several points within 1 day. You can turn the hours as you need.",1527049647.0
kennejima,"https://dumper.io - shameless plug, but I actually use it for offsite MySQL backup myself. As well as replication and VM snapshots, you need extra layers of backups.",1527081718.0
mk_gecko,Why are people not just using `mysqldump  --all-databases > somefile` ?,1528148078.0
jericon,"I have a tool that I wrote that manages our backups.  We take daily backups using mysql enterprise backup.  It saves this to a NFS mount on a dedicated backup filer that is used by other teams as well.  That filer has it's own internal replication to ensure that a failure of the filer does not mean that backups are lost.  

We keep one daily backup and several weekly backups, depending on the requirements that the specific database have.",1528229712.0
2112syrinx,"This is cool but I'd recommend all of you guys - experts - to write a blurb of what that specific topic means before digging into that. For instance, backing what? You could briefly explain what does backup means to users who wants to learn more about MySQL. Thanks for that.",1526411163.0
mischiefunmanagable,"3-2-1 rule

mysqldump by database.table

and innobackupex/mydumper

both stored onsite and in S3


we technically have a 24 hour window either direction for rto/rpo but try and target 1 hour for rto in most cases, 


we have a few databases that are pretty much a bunch of huge append only tables that we just get the schemas up and deal with the data later (or use the event to finally force people to use our otherwise idle data warehouse we're paying way too much for and not putting anything in actively)",1526389769.0
iheartrms,Mylvmbackup w lvm snapshots. ,1526408193.0
jynus,We put our most important dataset in a public ftp and people willingly download it to their computers. Best possible redundancy!,1526415078.0
jericon,Exactly as the others said. This isn’t a mysql issue. Please check Apache related subreddits. ,1526382401.0
beermad,"You'd be better off asking this on a forum dedicated to Apache, which is a web server, not MySQL, which is a database.",1526376104.0
mudclub,You're in /r/mysql...,1526376274.0
cYzzie,Ms access (as a frontend to mysql),1526367368.0
jous,"Editing the database from excel does not seem so hard to set up: https://dev.mysql.com/doc/mysql-for-excel/en/mysql-for-excel-edit.html

Dedicated app will give you possibility to sanitize the data, but if you're sure your coworkers will update the values nicely (tip: they will not), check out heidisql, database browser portable and others on this list: https://alternativeto.net/software/razorsql/

Edit: The installation of mysql for excel is a bitch. The mysql installer is a pos. Do not use it. I had to install [Visual Studio 2010 Tools for Office Runtime](https://download.microsoft.com/download/7/A/F/7AFA5695-2B52-44AA-9A2D-FC431C231EDC /vstor_redist.exe) first and after that installed [ MySQL for Excel standalone MSI](https://dev.mysql.com/downloads/windows/excel/). Editing worked pretty ok when using only single user at a time.

If excel is so sexy amongst your coworkers, you _could_ cook up a simple app with VBA form controls and a ODBC connector to the database.",1526382557.0
LobbyDizzle,"First, I'd clean up the query a bit by using join statements rather than what you're currently using:

    select SUM(DATEDIFF(DISCHARGED_DT_TM, ADMITTED_DT_TM)) as YEAR2000DAYS
    from OBESITY_DIAGNOSIS_ENCOUNTER DE
    join OBESITY_DIAGNOSIS OD on OD.DIAGNOSIS_ID=DE.DIAGNOSIS_ID
    join OBESITY_ENCOUNTER OE OE.ENCOUNTER_ID=DE.ENCOUNTER_ID
    where DIAGNOSIS_CODE like '401%'
      and (ADMITTED_DT_TM between '2000-01-01 00:00:00.000000' and '2000-12-31 23:59:59.00000');

Then I think you can use case statements to perform the summation by year. I also use this method when I need to perform calculations with a subset vs the total record count \(like for percentages of the total\):

    select SUM(case
                when ADMITTED_DT_TM between '2000-01-01 00:00:00.000000' and '2000-12-31 23:59:59.00000' then DATEDIFF(DISCHARGED_DT_TM, ADMITTED_DT_TM)
                else 0
            end) as YEAR2000DAYS,
           SUM(case
                when ADMITTED_DT_TM between '2001-01-01 00:00:00.000000' and '2001-12-31 23:59:59.00000' then DATEDIFF(DISCHARGED_DT_TM, ADMITTED_DT_TM)
                else 0
            end) as YEAR2001DAYS
    from OBESITY_DIAGNOSIS_ENCOUNTER DE
    join OBESITY_DIAGNOSIS OD on OD.DIAGNOSIS_ID=DE.DIAGNOSIS_ID
    join OBESITY_ENCOUNTER OE OE.ENCOUNTER_ID=DE.ENCOUNTER_ID
    where DIAGNOSIS_CODE like '401%';

Or you can be a fancy pants and use a group by with the `year()` function \(I think this'll work\):

    select year(ADMITTED_DT_TM) as year,
           sum(datediff(DISCHARGED_DT_TM, ADMITTED_DT_TM)) as days
    from OBESITY_DIAGNOSIS_ENCOUNTER DE
    join OBESITY_DIAGNOSIS OD on OD.DIAGNOSIS_ID=DE.DIAGNOSIS_ID
    join OBESITY_ENCOUNTER OE OE.ENCOUNTER_ID=DE.ENCOUNTER_ID
    where DIAGNOSIS_CODE like '401%'
    group by 1;",1526319834.0
cYzzie,"I dont understand your question, maybe give code examples of what you mean",1526258108.0
just5ath,... this is a joke right?,1526250403.0
mudclub,"PHP is literally designed to provide a web interface for a database.  That's what is *does*.  Google ""php mysql tutorial"" dude.",1526255361.0
jschnees,You have to use your websites host database server to import the database and connect that way.,1526255062.0
ccb621,"You will find more assistance at one of the web design/development subreddits. You’re on the right track in the sense that you need a database; however, this is not the appropriate place to get help with PHP. 

Also, you might want to research web frameworks that make this work a lot easier. My preference is for Django, built with Python. ",1526310618.0
pease_pudding,"Don't store phone numbers as an int.

Some countries (and mobiles) have dialling codes which begin with 0, and in these cases 077.. would become 77...

Store it as a varchar, and use your middleware to ensure it only contains permissable characters (ie, digits or maybe you want to preserve it exactly as entered for display purposes)",1526248512.0
mudclub,LMGTFY...,1526169627.0
mvelasco93,"Are you using a framework? And inside xampp, where are you saving your files? ",1526193834.0
msiekkinen,"https://stackoverflow.com/questions/16424828/how-to-connect-mysql-database-using-c

More of a basic hello world example",1526220962.0
davidib2001,"Hi Corn\_11,

I have same question.  Sorry I don't have specific answer after having wasted days looking for a good source.  MySQL documentation/support is terrible, I'm ditching MySQL for sure, just not worth the time and frustration of trying to get (in my case binding C++ vars to fields using the crap X Plugin connector) the simplest things working.",1535460019.0
digicow,"Tables with the exact same columns can be combined using UNION

    SELECT * FROM tableA
    UNION
    SELECT * FROM tableB;

This query could then be used to create a view, or used as the input data for an INSERT (e.g., into a third table)",1526153580.0
,[deleted],1526153650.0
xilanthro,"Have you tried:

where ( diagnosis_code like '401%' or diagnosis_code like '250%' )
and od.diagnosis_id = de.diagnosis_id

-- I think 'and' precedes 'or' in the order of operations, so you're only doing the join (id = id ) for codes beginning with 250 unless you use the parentheses.",1526102979.0
epoxxy,"    SELECT DIAGNOSIS_DESCRIPTION,COUNT(DISTINCT ENCOUNTER_ID),
    SUM(CASE WHEN DIAGNOSIS_CODE LIKE '401%' OR DIAGNOSIS_CODE LIKE '250%' THEN 1 END) as `Diag401_250`
    FROM OBESITY_DIAGNOSIS_ENCOUNTER DE
    JOIN OBESITY_DIAGNOSIS OD
    ON OD.DIAGNOSIS_ID=DE.DIAGNOSIS_ID 
    GROUP BY DIAGNOSIS_DESCRIPTION ORDER BY 1
    

You can copy that conditional SUM with whatever different conditions you like,this is called a pivot.



",1526120178.0
mvelasco93,Make the join first. Use join table on t.val = table.val.,1526102828.0
SiliconeOvenHat,You might want to look at SQLite instead. ,1526123406.0
NotTooDeep,"Excel is a front end to a flat file. It makes the ugly bits into human readable form. Excel can be shared through email, public drives, cloud, etc.

MySQL is an abstraction layer on top of a bunch of flat files. It provides a management and development front end but that's not for end users, especially on the business side. How will you share access to the MySQL database? Website? 

I've worked with a few inventory management systems and they can be very sophisticated. You are right to be concerned about best practices, but you really need to understand why they work. Things like normalization aren't as cookie cutter as they sound, and it can entail a lot of work to normalize a spreadsheet of only 3000 records.

If you do your google-fu and come up with a prototype in a month, how will you test it? This is probably the most important thing for you to document up front. An example of a test might be ""show current quantity of each inventory item"". But this is only a snapshot of a point in time. The business will need to know lead times so they can reorder before running out of popular items. This involves setting trigger levels in the inventory for automated reordering. But some items might be seasonal, so the trigger level also needs start and end dates. 

If you get your tests down in a list, that will guide your design of the tables. Research normalization. This will be the key to easily scaling and integrating with other sister organizations. 

PM me as you go. I've been doing databases for 22 years and can give you some good answers to your design problems.",1526068159.0
cYzzie,"Consider ms access, otherwise you might have to learn some coding as well, as you need to build an app / a frontend to the mysql. ",1526070497.0
kaydub88,"Just FYI, you'll learn enough skills during the process of throwing this together that you'll be able to get an actual DBA/Developer/Admin job. More than likely you won't finish the project. 

You would need to build a front-end application as your end users won't use the database directly.

You're biting off a lot and if this is just an option to get paid to learn this stuff and move on you're probably in a good position. If you're actually trying to build this app and do all this stuff for this business than I wouldn't bother. This project will take far longer than 1 month.",1526085682.0
etrnloptimist,"If you try to learn everything about databases especially best practices right away you will become so scared you won't start.

I would suggest starting with one table and working your way from there.

Implement in my sequel the same thing you have in Excel right now.

A table in a database is the same as a sheet in Excel. That should hopefully be all you need to get started.

You can learn about indexes joins normalized form and relations all later.

Also this isn't just Idle advice. The best way to learn something even learn it the right way is by doing something useful in it first.",1526067352.0
pease_pudding,"You don't need to learn MySQL so much as you need to learn RDBMS theory. 
There is a lot to learn, and so doing all this (and also implementing it) within 1 month is probably a little optimistic.

It could be done though, if you are working full time every day.

The schema design is crucial, even at the expense of querying convenience. 
If the schema design is correct, everything else will gradually fall into place (in most cases that includes scalability)

You seem to already know about 4NF, so stick with that as your guide. 

Just be aware, that if you are building some tin-pot system in Access, or Excel, and the company then starts relying on it and wants to scale it up massively, you will probably be doing them a big disservice in the long run and building a rod for your own back (be careful of painting yourself into a corner with this!)

Drop me a message if you get stuck. I like SQL problems so am happy to mentor you now and then

",1526073950.0
debbywins,"it should be around the time right now that community college courses are starting. You could enroll in a course for databases. Alternatively, there are hundreds of online youtube courses. I would recommend one but I don't know your learning preferences, honestly, the best way to learn this stuff is to just do it. Databases aren't terribly difficult and you could learn what you need to know rather quickly",1526067066.0
etrnloptimist,"How much time is this taking? The explain shows a tiny number of rows.

This is a mess. This query is write-only.

In general, I would decompose this query into all the constituent selects that are buried inside. I would make each select query its own ad-hoc table: (select * from blah) as t1.

Do each one separately. See how fast they run and how many rows they return. If there is a slowdown, you will find your culprit.

I would then join them all together using a combination of joins, left joins, and left joins where the second table join id is null.

[Take a look here](https://blog.codinghorror.com/a-visual-explanation-of-sql-joins/) which explains how joins can be used as set operations (set unions, set intersections, and set differences)",1525896129.0
berryer,'Where x in (select...)' tends to perform [very badly](https://stackoverflow.com/a/6157797) in mysql - I'd recommend refactoring that into a join,1525918933.0
randomfrequency,Refactor that into a join - mysql can't (mariadb can) materialize subselects - either that or use temporary tables.,1525971149.0
Hoysurdady,Also have a look at INSERT ON DUPLICATE KEY UPDATE. Sounds more like it would fit your criteria  ,1525916222.0
Tokkemon,"You've got it pretty much right. Should be like this:

    UPDATE`mainclosings` SET `reason` = 'reason_data', `CloseDate` = 'closedate_data', `openDate` = 'opendate_data' WHERE `Library` = 'library_data';

Keep in mind a primary key is usually an integer that's hidden from the user so they can't muck with your database structure unintentionally. Your current structure could benefit from this as maybe one day you *will* want libraries to have multiple mainclosing records. So instead you designate an arbitrary primary key for each record in that table. In a separate table of Libraries, each Library can have relevant info and each record is assigned its own primary key. Then back in the mainclosings table the Library column takes those primary keys, perhaps with a foreign key constraint. ",1525888386.0
,"My company has a self-hosted ERPNext application. This application has a feature for Reports which are written in Python and Javascript. The Data comes from MariaDB. I would like to know if usage of `ORDER BY` clause would sort it on the Disk or in the memory.

Here is one of the queries :

    entries = frappe.db.sql(""""""
    		SELECT 
                 crf.grace,crf.email_id,crf.crf_received_date,
                 crf.sales_person,crf.router_receipt_number,
                 crf.dsa as crf_dsa,crf.package_name,crf.package_price,
    	         job.*,
                 cust.disabled, cust.user_id , cust.static_ip, cust.registration_date,
    	         job.sales_person AS reg_by_emp, 
                 job.supplier AS reg_by_sup, job.registration_by
            FROM 
               `tabCRF Form` as crf LEFT JOIN 
                    `tabJob Order`  as job 
                          ON crf.crf_no = job.crf_number 
                LEFT JOIN  `tabCustomer` as cust 
                          ON job.customer = cust.name  
            WHERE 1 = 1 %s 
            GROUP BY 
                   job.name 
            ORDER BY 
                   crf.creation  asc """""" %(conditions), as_dict=1)",1525846536.0
aqbabaq,"Depends, if the explain output will show you using temporary (in this case i am sure it will) than mysql will need to create tmp table to sort results. Now it can be created in memory or on disk. If the data set that needs to be sorted are bigger than tmp_table_size or contain BLOB/TEXT columns than mysql will create tmp table on disc. You can see this by running show global status and check Created_tmp_disk_tables value.
",1525861337.0
DaemonOperative,You might want to check to see if your tables are using MyISAM or InnoDB engine. As I recall MyISAM is subject to full table locks while InnoDB is not as much.,1525873740.0
shaunc,"It's possible that the implicit join syntax (selecting from multiple tables without the `JOIN` keyword) is confusing the database engine. It might be trying to join on columns that don't necessarily match up between the tables. Try using explicit joins, where you instruct SQL precisely how to join each table, and see if that helps any:

    SELECT 
    OD.DIAGNOSIS_DESCRIPTION, 
    EC.EVENT_CODE_CATEGORY 
    
    FROM OBESITY_CLINICAL_EVENT CE
    JOIN OBESITY_DIAGNOSIS_ENCOUNTER DE
        ON DE.ENCOUNTER_ID = CE.ENCOUNTER_ID
    JOIN OBESITY_DIAGNOSIS OD
        ON OD.DIAGNOSIS_ID = DE.DIAGNOSIS_ID
    JOIN OBESITY_EVENT_CODE EC
        ON EC.EVENT_CODE_ID = CE.EVENT_CODE_ID
        
    WHERE EC.EVENT_CODE_CATEGORY LIKE 'SYMP%'
",1525797657.0
Limatto,"Was able to get a solution from stackoverflow:

select json_extract(json_remove(months, '$.startdate', '$.enddate'), '$.*')
from `Project` 
where `pId` =""6d43c24f-b258-4b75-8524-26873c643748"" 
AND `creationTime` IN (SELECT MAX(`creationTime`) FROM `Project` GROUP BY `id`);

This is will give the expected output : {""2500"",""9000"",""4000""}",1525842785.0
dohako,GROUP_CONCAT is your friend. ,1525735369.0
LobbyDizzle,"You need to share more about the schema of the database. What are the foreign keys of these tables? Also, you'll probably want to use joins and case statements instead of subselects in the select statement, as they'd be more performant. This is what I'm guessing you're looking for:

    select c.company_name,
           c.clientID,
           (case
                when s.showId = '23' then green_sheet
                else null
            end) as '6/17A',
           (case
                when s.showId = '4' then green_sheet
                else null
            end) as '6/17A'
    from companies c
    join shows s on s.clientID = c.clientID
    join client_shows cs on cs.showID = s.showID;

Or you can use this for a cleaner result with one fewer column:

    select c.company_name,
           c.clientID,
           (case
                when s.showId = '23' then 6/17A' when s.showID = '4' then 6/18A'
                else null
            end) as gs
    from companies c
    join shows s on s.clientID = c.clientID
    join client_shows cs on cs.showID = s.showID
    where s.showID in ('23',
                       '4');",1525725384.0
NotTooDeep,Should those slashes be back slashes?  Just my first thought.,1525575346.0
jericon,"It could be that the path is incorrect. Or it could be a permissions issue. If mysql doesn’t have the permissions to read the file it indicates that it’s not there. This is because to mysql, it isn’t. 

Your best bet to eliminate the path is to go to the file, right click and select “copy path as text”. ",1525585372.0
philipolson,"Looks like a bug that was fixed about a year ago in PHP 7.0.19, 7.1.5, and 7.2.0; via [PHP Bug #74461](https://bugs.php.net/bug.php?id=74461)",1525492192.0
DataVader,You also need the new connectors. For MySQL 8.0.11 you need PHP 7.2.4 or higher for example.,1525475103.0
jynus,"I know your issue. The default collation on MySQL 8.0 has an id not recognized by some connectors, except on the latest versions. It happened to me with mysqlpy (fixed on the latest 0.8 version). Upgrade your connector or if you cannot, change the default server collation to another configuration supported by the connector (utf8mb4_bin, utf8mb4_general_ci, etc).

That and the new default authentication plugin will cause issues on older clients/connectors, despite in general the new defaults being better.",1525476802.0
dohako,"assuming by 'newly added timestamp' you mean the record with the largest time value, the query is easy: 

SELECT string_column, timestamp_column, int_colun FROM my_table ORDERY BY timestamp_column DESC LIMIT 1;

Order by will show the newest timestamp first, LIMIT 1 will restrict it to just one row. 

Look into: 

* Indexing, you most likely want to add an index on the timestamp so the order by part of the query will be really fast. 
* Primary Key, you likely want to add another column as an auto increment / primary key, not strictly because you need it now but it will likely be required later and is pretty much best practice. 
* Default value, if the timestamp is the current time, you don't actually need to pass it to the database but you can simply create a default value of now() on that column.

Good Luck. ",1525471325.0
r3pr0b8,"any help?  sure

first of all, the number in parentheses has absolutely nothing to do with the range of values that can be stored in the column

INT(20) holds the exact same range of numbers as INT(2) and INT(99) and even plain old INT or INTEGER

also, you cannot store periods or commas (as suggested by xxx.xxx.xxx) in an integer column",1525446150.0
jous,"If you can, the easiest way is to dump the database, run replace with a text editor, shell command or script. Then load the database from the corrected dump file. 

Another way is to create a second table and [copy](http://www.mysqltutorial.org/mysql-copy-table-data.aspx) the table using [replace()](https://www.w3resource.com/mysql/string-functions/mysql-replace-function.php), check if the data is ok and then rename the new table.

The most hazardous way is not to take any kind of backups and then run 

    UPDATE table1 SET field1 = REPLACE(field1, ""oldtext"", ""newtext"")",1525434922.0
jericon,"You answered your own question in there.  ""no order by"".

When a query is run without an order or group, it is nondeterministic.  This means that data can be selected in different orders, or even different data (for example, if you put limit 1000 on the query, you could get 1000 different rows on each one). 

While the data looks the same on both hosts to you, internally within mysql they could be different, resulting in a different ordering when the results are returned.  If this is innodb, it could be differences in how pages have been split, how reclaimed space has been reused, etc.  

But in short, this is completely expected.  If you want the queries to be exactly the same, include an ""order by"".",1525211194.0
Sakred,Are you referencing WHM/cPanel? What access rights do you have to the server?,1525196791.0
ambrace911,"mysql> GRANT ALL ON foo.* TO user@'IP' IDENTIFIED BY 'PASSWORD';

some more details on viewing current grants

https://dev.mysql.com/doc/refman/8.0/en/show-grants.html",1525198838.0
dohako,"Have you checked slow query logs and locked table issues?

Do you read this report data in a non-locking transaction level read uncommitted query? ",1525179590.0
just5ath,No link posts is pretty stupid. I'll see how it looks in a few months but I believe I'll be unsubscribing. I'd prefer more links and less guys posting their Php problems here.,1525173818.0
TexasWithADollarsign,"Yeah, your ""no link posts"" rule will kill this sub. So thanks for that I guess.

I mean... now that text posts get karma too, what's even the point of this?",1525186854.0
msiekkinen,"Yes it is advisable.  This will place each table in it's own .ibd file on your file system.  Without it every table is stored in a single file, ibdata1, that will grow and grow.

With per file you can reclaim space on disk with a rebuild if you have deleted a large amount of records.

If you want bulk copy one table from one server to another you can do an export/import table space that involves a raw file copy of the ibd file from one server to another.  This is not possible when per table is disabled meaning you'd have to do a much slower mysqldump/import. 

I really cannot think of a single benefit of disabling file_per_table.  

Edit:  If you're on some edge case of 10s to hundreds of thousand tables per database you might start running into thing like file system open handles allowed and mysql side open table cache wanting to be tweaked appropriately.  ",1525105901.0
nthdesign,Another reason to always enable file-per-table is that it is s pre-requisite for InnoDB tablespace encryption. Many compliance scenarios require encryption at rest. ,1525132811.0
r3pr0b8,"> what is the best way to write these forms?

i would have to go with Java and Jasper ",1525037923.0
bcl79,"Once I was tasked with generating aggregate stats from 4000+ MySQL databases. All the databases were similar and each of interest tended to have 10,000+ rows. I found that writing one monster query was better. It ran a lot faster and seemed to be less burden on the server. I believe that the database server should handle as much data-related tasks as possible. ",1524961281.0
NotTooDeep,"""Better"" depends on what you need it to do. Go fast? You can test that. It's already going pretty fast, so improvements might not be obtainable.

Be easy to maintain if this code is going to be in production for several years and change over time? That's a lot of lines of code for someone else to sort out, unless its structure is obvious.

""Too much time"" is also relative. Is this a one time or some time query? Then who really cares if it takes a few minutes to run. Is it a frequent query that creates contention with other queries on a large, heavily loaded instance? Tune it into the ground.",1524970271.0
jericon,"It really depends on what the query is doing. If it’s a bunch of or statements, yes, break it up. If it’s a big in list, probably won’t make any real difference. ",1524964735.0
Nicolay77,"In my experience, the fewer queries you run the better.

But in doubt, test. You still need to have a reproducible comparison if you want to convince your coworker.",1524966202.0
NotAnExpertWitness,"
In a high traffic database situation, it is typically better to do small quick grabs of data as to not upset the ecosystem of the application.

.2 to .5 seconds will be hard to beat using smaller queries.  The only reason to break it up into smaller ones is if you have locked an important resource for that .5 seconds that causes a disruption somewhere down stream.  If thats not the case, just run the monster query.",1525020523.0
_Momentum,"Either way, it will still get processed. But if you use multiple smaller queries, it will have a much higher overhead from function calls and everything else. Although, as others have mentioned, splitting them up means more maintainability.

If performance is a problem, I would look into other ways to optimise it.",1525032411.0
Nakasje,"The need of an huge data from database, or an huge database procedure, should trigger the quest for some optimisations. User request, design choice where ever it might be. Desire should be the shortest path between request\-response. For example less need for filters.

Most optimisation tends to prefer smaller queries.

When it comes to a short term optimisation then big queries will perform better simply because of the elimination of extra traffic between the app and database space. As an analogy; sometimes it is better to carry with a big truck at once then a small car at multiple times.

At the end a fluent data stream is the way to go. 

There are times it is better to use multi\-threading for better utilisation of CPU, but this is probably far fetched right now.",1525006445.0
SomeGuyNamedPaul,Whatever API you're using will have something that does this: https://dev.mysql.com/doc/refman/5.7/en/getting-unique-id.html   You can also just use it directly in your subsequent insert. Do both in the same transaction (begin/commit) so if one fails they both get rolled back.,1524952213.0
r3pr0b8,"an alternative, which is genuine and foolproof and guaranteed to work, ~without~ BEGIN/COMMIT... but just watch how this gets downvoted...

it begins by acknowledging that an auto_increment PK is (almost always) a **surrogate key**

this means that there is some other column, or combination of columns, that should be declared UNIQUE

so here's the technique...

1. insert the row

2. use the same values for the UNIQUE key that went into inserting that row, to SELECT that same row back

3. vwalah, you got your auto_increment value

notice that it doesn't matter what else happens between 1 and 2 ",1524955168.0
jericon,"The most likely issue is that the program you are calling doesn’t exist or isn’t in your path. 

What operating system are you running?",1524956723.0
lal00,"It might be that you have an index. 

You could sort your data by the index key before inserting or reduce the amount of sql parsing by LOADing the data or doing a multi insert. ",1524858108.0
dohako,"As others have said likely an index issue. Can you bulk import a few hundred at a time?
That should reduce the number of index updates a lot. ",1524862050.0
WarInternal,"Is an innodb table? If so, wrap the operation in an explicit transaction demarcation. It helps. For millions of records though you'll want to periodically commit (hint`counter % frequency == 0`) in order to prevent overwhelming the log buffer.

Also [here's](https://nbsoftsolutions.com/blog/optimizing-innodb-bulk-insert) some ideas.",1524873506.0
fdarcy,"Its not the worst idea I've ever heard but if you want to tame this MySQL beast you need to be able to look at the log files and find out why a ""clean install"" from a pkg wouldn't start / where your my.cnf is etc etc. ",1524846411.0
r3pr0b8,"no WHERE Clause?

you're going to get a number that means nothing because it conflates all events",1524843108.0
OldSissonDorm,"What about doing it without the variable?:
   IF (NEW.ID IN (SELECT.....))",1524843176.0
Giomex,"Try using

chmod -x directory",1524798583.0
SaltineAmerican_1970,"If you’re using Apache, you may need to make sure you have ` Options FollowSymLinks` in your server configuration.",1524813639.0
jericon,"Glad you got this solved, but this isn’t mysql related at all. ",1524905077.0
jericon,"Please provide more details of what you are trying to do, what you have already attempted and where you are getting stuck. 

We are here to help, not do your work for you. Your post is incredibly vague and doesn’t give us any information to go on. 

I have removed this post. Please create a new one when you can answer the questions I posed above and give more details. ",1524817242.0
jericon,"Your post has been removed.

Reason: Self Promotion

Your post history shows that you have a propensity to post from the same domain or topic often to a wide breadth of somewhat related subreddits without contributing to the communities otherwise.  Please see [Reddit's Self Promotion Policy](https://www.reddit.com/wiki/selfpromotion).",1525113432.0
PM_ME_YOUR_HIGHFIVE,do you mean count?,1524740273.0
r3pr0b8,"    UPDATE tablename 
       SET first = CASE WHEN first = 'XXX 001'
                        THEN NULL
                        ELSE first END
         , second = CASE WHEN second = 'XXX 001'
                        THEN NULL
                        ELSE second END
         , third = CASE WHEN third = 'XXX 001'
                        THEN NULL
                        ELSE third END
     WHERE id NOT IN
           ( SELECT DISTINCT id
               FROM levels
              WHERE s >= 3
                AND l >= 3 )",1524733788.0
myevillaugh,I don't think so. What errors are you worried about?,1524731281.0
jericon,"Your post has been removed.

Reason: Self Promotion

Your post history shows that you have a propensity to post from the same domain or topic often to a wide breadth of somewhat related subreddits without contributing to the communities otherwise.  Please see [Reddit's Self Promotion Policy](https://www.reddit.com/wiki/selfpromotion).",1525113446.0
emdash5,"It's mainly convention, and probably a bit more intuitive to pull all the data from the table that's  mentioned first.",1524727085.0
debbywins,"I'm a little worn out from work right now so I don't know much about that beast of a select you have, but you might benefit by creating indexes on this table. If you are unfamiliar with indexes [here](https://www.youtube.com/watch?v=ITcOiLSfVJQ) and [here](https://www.youtube.com/watch?v=i_FwqzYMUvk&t=4s) are some decent videos that are actually understandable ",1524710237.0
mudclub,Of course you can. How you do it depends on the networking config of the vm.   MySQL will need a network listener reachable from outside the vm. ,1524693442.0
r3pr0b8,"    SELECT SUBSTRING_INDEX(location_description, '.', 1) as Country
         , SUBSTRING_INDEX(
           SUBSTRING_INDEX(location_description, '.', 2)
                                               , '.', -1) as State 
         , SUBSTRING_INDEX(location_description, '.', -1) as City
      FROM Table Name",1524681778.0
msiekkinen,"Looks like you got your answer,  but I'd sanity check every record has exactly 2 periods.  I'd put money on there being some city with a period as part of it's literal name ",1524686043.0
NotAnExpertWitness,"Master master is tricky, out\-dated and prone to problems.  Your gonna want to go down the xtraDB or Galera road for what you want.

Master\-Master is basically circular replication, so two writes to the same table happening on different machines will cause inconsistency.  Most people I know that run master\-master still set one up as read only and direct their selects to one box and their writes to another.",1524666009.0
captainevan2,"https://www.percona.com/doc/percona-xtradb-cluster/LATEST/howtos/centos_howto.html

That’s the link I used to set up my Percona cluster. It’s actually fairly straight forward and the link walks you through it step by step. Let me know if you have any questions about it, hope that’s helpful! ",1524781544.0
r3pr0b8,"looks okay to me

try testing a non-parameterized statement directly in mysql --

    UPDATE fishdata 
       SET caught = IF( 3 <> '', '3', caught)
         , species = IF( 7 <> '', '7', species) 
     WHERE id = '42'

this will tell you whether there's a problem with your parameters",1524581087.0
mischiefunmanagable,"I don't have issues with questions so much as I do that they generally seem to be either using reddit as google, or they're about php not actually directly mysql related",1524577250.0
etrnloptimist,"The only questions I really mind are the ones that are clearly homework problems.

*It's your job to do your homework. That's how you learn!*",1524579057.0
synackSA,"I don't have a problem with this for the most part, but there's a fine line between making a sub that is rather inactive to even further inactive.

*edit* I also feel like a little CSS love would go a long way to making the sub look less like it's inactive.",1524599294.0
Cramd,This is the first one I've missed in a LONG time. I'm not sure I will miss my bills from the lobby bar.,1524592283.0
xenilko,I have a colleague over there dm me your info ill put the both of you in touch :),1524598123.0
justintxdave,Try to make it to Pedro’s Tuesday night for the Pythian Community Dinner. You will have to register/pay before going.,1524616197.0
PM_ME_YOUR_HIGHFIVE,"you can do

    ....
    LEFT JOIN db.b as b ON a.site = b.site
    WHERE a.date BETWEEN b.start_date and b.end_date",1524502308.0
LowSociety,"Now, I’m on mobile but in what way is your original query wrong?",1524506474.0
SomeGuyNamedPaul,"You have to set the end of line delimiter to something other than ;

    delimiter @@
    CREATE TRIGGER atualiza_clientes
    AFTER INSERT ON clientes
    FOR EACH ROW
    begin
    update clientes set endereco02 =  complemento;
    end;@@
    delimiter ;



",1524500329.0
kellenkyros,"Hey, on success callback you should do a get HTTP call to fetch all the widgets. This is the straightforward and clean way to do that.",1524449407.0
mischiefunmanagable,don't store the chat events themselves in an rdbms,1524492524.0
r3pr0b8,"i'm not sure, but i would try replacing IF with a CASE expression",1524429600.0
jericon,"Your post has been removed.

Reason: Self Promotion

Your post history shows that you have a propensity to post from the same domain or topic often to a wide breadth of somewhat related subreddits without contributing to the communities otherwise.  Please see [Reddit's Self Promotion Policy](https://www.reddit.com/wiki/selfpromotion).",1525113537.0
Detach50,"I found it! INNER JOIN!

...FROM agrouprelations INNER JOIN aitems INNER JOIN bitems...

But I'm wondering if I even need the INNER JOINs or if just using the proper fields is all I need.

I'll test it tomorrow!",1524451574.0
davvblack,"They optimize to the same exact query under the hood, so it's just a matter of readability.  It's slightly easier to make absurd mistakes like cross-joining two tables with the syntax you use, but it's fine.   Since it's less popular, it might be offputting to other developers who pick up your code, which is worth considering.  There's no particular problem.",1524290582.0
r3pr0b8,"it's a question of readability -- when there are multiple tables, the join conditions tend to get mixed in with filter conditions in the WHERE clause, making it harder to comprehend what's going on

however, the big killer is when you have to write an outer join -- go ahead, show us how you do it ☺",1524301852.0
MrRiotRick,I learned it the same way you did. For the past years I've been using the inner join syntax though. Makes it easier to modify the join type and I find it easier to read.,1524338050.0
jericon,A very quick google search shows this is likely an issue with how you are instantiating mysqli. If you search for the error message on google you should get s number of results. You can also find php assistance at /r/phphelp,1524281530.0
rudetopoint,"What result do you get if you do this exact query with that username using the MySQL Workbench? Why are you closing the connection in the function, are you opening it again every time?",1524302934.0
alduron,I have no idea what data you're actually storing in here...but it sounds a little like you need normalization with table relationships. ,1524234986.0
dbbeginner,"How big is your table, in terms of size (kb, mb, gb)? What's your indexing strategy? How much RAM on the server?

If you have that much data, then of course it should be OK to store it in a database, it's just making sure that the server has adequate resources to store and access the data.

I'm looking at a DB on my dev machine right now; one table with a bunch of indexes has ~60,000 records, which uses 3.5 MB  of space (but has 6MB worth of indexes). 

At least that's my takeaway from Sequel Pro. 

If it grew 100 times as large, that would be 600,000 records, and apparently 600 MB worth of indexes. That's not anything to be concerned about as long as your server can handle that in RAM ",1524240458.0
kielchaos,Remindme!,1524231207.0
coraxwolf,"I am not clear on what you are asking.  The question may be better with a more concrete example.  From my understanding Performance issues stem more form how many queries you are running at once than the number of records.  You do have to watch the size limits of your database which is determined by your table types and your operating system's file size limits.

[Normalizing](https://en.wikipedia.org/wiki/Database_normalization) your data and using indexes is an important part of making an efficient database.  

In your image I am not sure how you would distinguish between records in tables data_type_1 and data_type_2 as both have duplicate values for ID and Time.  If I wanted to get the first record for user 1 at 10:00 (using a time stamp instead of just time may help as long as two actions at the same time won't be captured) how would I tell which is the first and which is the second record?

The part about how to best sort the data that you query would depend on the usage of the data.  I think that how you make use of the data in your application is important.  Should you only pull the exact data that you need for each task or pull all the data and then leave your application to pull out only the fields that it needs.  

I hope some of this was helpful.",1524240607.0
thenickdude,"I can't believe that we're on version 8 and triggers still aren't triggered by cascading actions on foreign keys, [a problem that was reported in 2005](https://bugs.mysql.com/bug.php?id=11472).",1524173163.0
sicruise,Finally desc indexes,1524169100.0
xenilko,"I'm super psyched about 8.0 finally hitting GA! I'll finally be able to toy around with it at work, woo!",1524180993.0
cryonine,"Given this information I'm not entirely sure what your goal is. I will say that you should take a long look at your table designs though, because right now your foreign keys are defined, but don't actually work / do anything. Draw it out if you have to. I'll also point out that your SELECT query is actually pretty non-sensical and doesn't actually work the way you think it does.

Think about how you're going to relate data across tables or use that data to build a report / information about a trip or trips. Think about normalizing data (your `tripData` table is not normalized at all). Think about how you use data from other tables and the 1:1/1:M/M:M relationships you need. Look at where you're repeating data, and also think about what you need to change if the name / model of a car needs to be changed, or the month of a trip.",1524117328.0
debbywins,"The way to know if you need a foreign key is whether or not you want the data on one table to be connected to another table. When you select a car do you also want to be able to trace the user info of who it belongs to? If so, then take one of the unique fields from users (usually id) and make it a colum on the cars table. 

I’m not *totally* sure what you’re doing with that insert statement as I’m on mobile, but it doesn’t look like it’s going to work the way you want it to",1524117989.0
jericon,You say you are getting an error. But you didn’t provide the error. Please do so to help us. ,1524109022.0
vmrios,I get a syntax error that says check the MySQL version error 1064,1524109340.0
mooncheese69,im noobish with mysql but is this code above different from a trigger?,1524421146.0
vmrios,Yes.,1524421195.0
psy-borg,"Not a mysql question, should be in r/phphelp (or r/webdev since I don't think the problem's php related either) but I don't see what's submitting the form ? Which makes me believe there's something else going on in the HTML part and if you aren't going to use a submit button , it's very likely you will have to do something extra to pass the selection to the php file.",1524075149.0
sh_tomer,What does MySQL's error log file has to say about this?,1524065846.0
msiekkinen,"If you're a book person (either ereader or print) Orielly has a nice [intro book](http://shop.oreilly.com/product/9780596008642.do) from installing to basic usage 

These won't teach you from step 0 , but some good resources to keep on hand as you go along:

dba.stackexchange.com - For asking specific targed questions for specific answers once you get going and get stuck.  

https://www.percona.com/blog/  - More advanced on performance issues and latest changes but nice to follow once you're using it, if nothing else to give you an idea of things you didn't even know to ask about or consider.  ",1524002688.0
Xoor,Codecademy's tutorial is OK & free. It was enough for me to build webapps. After that you want to pick up a good reference book & study joins more carefully. ,1524010445.0
mudclub,"You could start by googling something like ""mysql tutorial"" or ""python mysql tutorial""...",1523999556.0
NotTooDeep,"This isn't specific to MySQL but, ""Data Model Patterns: Conventions of Thought"", by David Hay is my favorite data modeling book. ",1524013020.0
msiekkinen,"This is a very basic example that may impact performance of your source machine, including blocking potential writes until it's complete.


From the command line

    mysqldump -h server1 -uUserName -pPassword --single-transaction databaseName tbl1 tbl2 tbl3 | mysql -h server2 -uUserName -pPassword databaseName

Substitute ""databaseName"" for the actual name of the databse you'll be reading/writing to on server1/2.   substitute tbl1/2/3 for all the tables you want included in the dump.

Mysqludmp will create a sql text output for these tables in the form of create table statements and insert queries.  My example pipes it into a mysql client connecting directly to server2 to execute the dumpped queries directly (so you don't  need to have an extra step to copy over to server2 and load in)

You'll need to make user the userName account you use on server1 has read and lock privilegeds, the userName account on server2 would need create, insert privs at least.  

Edit:  This is assuming you're working on linux systems",1523997539.0
jynus,"This is a good introductory guide to setting up replication by taking a backup with Percona XtraBackup and sending it to separate server before starting a replication channel between both:
https://www.percona.com/doc/percona-xtrabackup/LATEST/howtos/setting_up_replication.html",1524002186.0
lurks74,You can use mysql replication .,1524023796.0
trash1000,"Code a crawler and let it scan the pages linked from certain category pages of the wiki for you. For example you could extract a lot of what you need by starting at the wizards category (http://harrypotter.wikia.com/wiki/Category:Wizards) and extracting the data out of the fact sheets they provide.

You'll have to go through other categories to get all the data you want though. ",1523982814.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/harrypotter] [Doing a database project on HP characters. Any suggestions on how to collect the data?](https://www.reddit.com/r/harrypotter/comments/8cxgmm/doing_a_database_project_on_hp_characters_any/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1523979565.0
making-flippy-floppy,"> What am i doing wrong?

Don't try to write a four table join in one go, there's just too many ways things can go wrong. Start with a single join, probably best to start with the `room` and `building` table so you can use the `where` clause to limit output, which will make it easier to see if things are working right. 

    select room.*
    from room
    inner join building on blah, blah, blah
    where building.building_name='Main Street Building';

When you've got that working, add the `meeting` table, and when you've got that working, add the `person` table. ",1523948012.0
jericon,Can you provide a copy of your schema?  ,1523941021.0
LowSociety,"Looks alright to me, so I’d double check the schema and data. Start by switching to LEFT joins and SELECTing from all tables, and you’ll see where the chain is broken.",1523943948.0
janos42us,"Try changing your where to:

Like ‘%Main%’;

Or

Where building.building_id = 42 — or whatever it is. 


This way you can maybe even toss a building.id in your select if you go the route of the “like”",1524631726.0
jericon,"Your post has been removed.

Reason: Self Promotion

Your post history shows that you have a propensity to post from the same domain or topic often to a wide breadth of somewhat related subreddits without contributing to the communities otherwise.  Please see [Reddit's Self Promotion Policy](https://www.reddit.com/wiki/selfpromotion).",1524112421.0
jericon,"Your post has been removed.

Reason: Self Promotion

Your post history shows that you have a propensity to post from the same domain or topic often to a wide breadth of somewhat related subreddits without contributing to the communities otherwise.  Please see [Reddit's Self Promotion Policy](https://www.reddit.com/wiki/selfpromotion).",1524112435.0
osppro,"hey members....watch my mysql programming videos 
data calculations and the rest..thank you",1523920380.0
lilacit,Your $newUrl variable likely has a single quote in it.,1523911576.0
MrRiotRick,"I'm assuming you're using php. Do yourself a favor and learn to use prepared statements.

http://php.net/manual/en/pdo.prepared-statements.php",1523917319.0
movieguy95453,Based on what you wrote in the description of the error it looks like you could be missing the apostrophe between the equals sign and $id in your WHERE clause. ,1523911732.0
dakruchko,"I exactly know that $newUrl variable has no quotes in it. I have added htmlspecialchars for variables, no result",1523912300.0
jericon,"What error(s) are you receiving, SPECIFICALLY?  The error message will help us diagnose the issue immensely.",1523905847.0
b1ack1323,"When you enter

     which mysql

Does it return a path?",1523893265.0
Risyasin,Are you not able to run any command in terminal or just mysql commands?,1523887087.0
debbywins,Have you tried turning it on and off again,1523891443.0
RedElef,"
I can’t get into the terminal 
Tried using bash and sudo commands but no luck... any even reset my password for sql.

I didn’t uninstall and reinstall twice and still no luck 

I tried restarting my laptop and I also tried stopping and restarting the SQL Server in System panel Preferences but nada ",1523891948.0
RedElef,"I just ran this and still nothing ... omg like I'm going nuts because I need to fix this to do my homework and my final

users-Air:bin user$ sudo /usr/local/mysql/bin/mysqld
2018-04-16 22:42:39 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).
2018-04-16 22:42:39 0 [Note] --secure-file-priv is set to NULL. Operations related to importing and exporting data are disabled
2018-04-16 22:42:39 0 [Note] /usr/local/mysql/bin/mysqld (mysqld 5.6.39) starting as process 778 ...
2018-04-16 22:42:39 778 [Warning] Setting lower_case_table_names=2 because file system for /usr/local/mysql-5.6.39-macos10.13-x86_64/data/ is case insensitive
2018-04-16 22:42:39 778 [ERROR] Fatal error: Please read ""Security"" section of the manual to find out how to run mysqld as root!

2018-04-16 22:42:39 778 [ERROR] Aborting",1523933152.0
RedElef,"I deleted and factory reset my entire Mac, backup my docs and software and redone loaded everything. It works great now!

I wish there was some other way to deal with the issue. ",1524482803.0
davvblack,"For this sort of use, any database of any type is suitable.  The amount of data in an image gallery is inconsequential.  If you want to optimize performance, look into a CDN like cloudflare or cloudront.  And set cache control headers properly.",1523850284.0
kadaan,"Best guess would be a mismatched quote or special character. That string is just json, nothing at all related to MySQL.

What's the full error? Also the full line it's trying to run, and the line before it as well.",1523827961.0
Im_FabuIous,Does anyone have a solution for this? I've tried editing the code_editor.xml file to no avail.,1523766153.0
ohmwattflux,Toy with your system/desktop color scheme lately? Try to set it back to default,1524936249.0
r3pr0b8,"> What approach works best for this problem? Any help is greatly appreciated, or even articles/tutorials to point me in the right direction.

there was a great article on EAV posted in /r/database the other day --

https://www.reddit.com/r/Database/comments/8bz3av/database_modelization_antipatterns/",1523703936.0
synackSA,"* Use underscores for your table names, like `travel_tips`. 
* If you have an id for a table (or a column in general), don't use the table name in the column name. EG: for `reviews`, rename `idreviews` to `id` and `reviewtitle` to `title`, etc.
* City table should have a id column and your postal code should probably be a varchar (a lot of countries use alpha numeric for postal codes).
* User underscores in table column names too, EG: `city`.`postal_code`
* Your category tables have terrible naming, if they're related to another table, use the related table name, EG: `attraction_categories`, `hotel_categories`. `category_1` is a terrible table name and isn't very descriptive on what could be contained in the table.
* I would consider creating an `address` table and a `city` table. You can link your attractions and hotels to the `address` table, rather than the `city` table, while you'll still be able to find all the hotels and attractions in a city, as you can go city->address->hotel/attraction.
* I think `travel_tips` should rather be linked to a city, rather than a tourist attraction
* Depending on how deep you want to go, I would create a `hotel_rooms` table, which would list the price per room, how many beds it has, etc.
* You could also create a many-to-many relationship between a hotel and a attraction. Attractions might be linked to a city and so is a hotel, but perhaps you want to link the hotel and attraction too, as you might want to show what attractions are nearby to the hotel and not all of the ones in the city
",1523627921.0
omanisherin,"Looks pretty good!  My suggestions:
1. Rename your category tables to category_attraction, category_hotels. 2. Make your traveltip table many to many.  I don't know the scope of your project, but you might want to consider an address table and a contact table too. Good luck!",1523621824.0
Xilc,"The model itself is absolutely horrible. Not trying to hurt your feelings or anything but it literally takes no consideration into any of the fundamentals of database design. Some of your attributes are named extremely vague things like description or whatnot. That tells you absolutely nothing about what's going on. Furthermore, your relationships don't really make sense which sort of suggests that you haven't really looked into cardinality or don't really understand how table relationships work at all. You have functional and transitive dependencies literally everywhere. Sure, it might work, but it's terrible",1523625318.0
bluegrassbiker,"There should be a library in python that you can import to allow your code to connect, authorize, and run SQL queries. 

Edit: it's called connector.  [Here is an example](https://dev.mysql.com/doc/connector-python/en/connector-python-examples.html)
",1523619486.0
j_kobrah,"Seems like OP doesn’t do much research.  There’s tons of info on what you’re trying to accomplish. Patience and google are your best tools.

",1523663796.0
hungryballs,This is often caused by closing and reopening the connection in between inserting the row and checking for the last_insert_id(). Your “select last_insert_id()” should be the very next thing you run after running the insert statement.  ,1523611619.0
vmrios,So I have an address table with city state and zip etc. and I have a person table. I want to insert into address first then use the address_Id to insert into person address_id. From my understanding the last_insert_id() holds the last inserted value of it is the primary key auto increment or unique which it is. I was testing the last insert Id function by just performing an insert into address table and the value for last insert is is 0 instead of the next incremented value.,1523607615.0
vmrios,What about testing in phpmyadmin? Should I include the insert into address and select lastinsertId in the same query to show the result? Is that why it shows 0 because I’m not executing in the same statement?,1523611736.0
AllenJB83,"Without more information, it's hard to suggest what could be wrong. I'd suggest reading through the [last_insert_id() documentation](https://dev.mysql.com/doc/refman/5.7/en/information-functions.html#function_last-insert-id)

If you still need help, please provide the table schema (CREATE TABLE), including indexes and the queries you're performing.",1523607174.0
Entrepreneurdan,"My dude, put the servers in a solid data center. ",1523592123.0
glib_gator,"it seems you are sailing in a very leaky ship to start with... just curious, why do these people keep turning off your database machines?",1523602342.0
kadaan,"Yes. By default, InnoDB tables won't get corrupted if the host is powered off. InnoDB writes transactions to log files; after a crash it will execute crash recovery and go through the log files to either apply or roll back any changes that haven't been written to the data files yet.",1523604006.0
msiekkinen,"mysql_upgrade applies schema changes to the internal tables mysqld uses in the ""mysql"" database that the binaries are expecting.

Go ahead and run mysql_upgrade now.  

As far as user data issues, IIRC on 5.1 the docs still recommended reloading a full logical mysqldump of your database into the new version.  ",1523557594.0
Nk4512,"Why do this the messy way, You can format this easier so you can save yourself trouble reading it a bit. 
Then, echo it back to see what the query will look like. 

When you get the query, try it out yourself in your sql program to make sure the query works from there, troubleshoot as needed. 



Quick example 

https://imgur.com/onoDJu7

I didn't do everything, or make it completely neat, or 100% standard. 

Sanitize wise, you could do it via pdo, or something quick like the escape string function for php. 

If the query works in your sql server, then you can enter it via something like 

$device($conn, $query) or die(mysqli_error($conn));

There might be an error in syntax, i have two kids on my shoulders making this irritating to type out. ",1523576346.0
mobsterer,I think if you write down the whole query as you would in pure SQL then you might find the error.,1523556975.0
r3pr0b8,"> Am I using it incorrectly?

yup

see /u/mobsterer's reply for a hint",1523557761.0
jericon,"Your post has been removed.

Reason: Self Promotion

Your post history shows that you have a propensity to post from the same domain or topic often to a wide breadth of somewhat related subreddits without contributing to the communities otherwise.  Please see [Reddit's Self Promotion Policy](https://www.reddit.com/wiki/selfpromotion).",1523554024.0
picturepages,"10000 rows, 144 columns.  Primary key by device id  (int).  Secondary table containing data with autoinc pri key that's placed in table 1 in the appropriate column for that timestamp.

My slowest dev machine (quad core, 16GB ram) will do around 6000+ updates a second.   Am I missing something?  I guess it depends on the data being stored for each device.",1523487278.0
kaysb,"* A large number of tables, or a large number of columns is almost never the right solution.
* A table or column name representing a data point (eg, a device name or a timestamp) is almost never the right solution.

It might seem logical to store data like this, since it is easy to visualize in your head, and especially if you are used to work in spread sheets. But the tables in a relational database does not have to make sens like this. It is the queries you run against the database that should generate this kind of structure, not the tables them self.

I would solve this with a very simple table; a primary id (~~maybe the device_id~~), the value and a timestamp (a datetime field, not a timestamp, unless you have a specific reason for storing it like that). Don't forget to add indexes that covers the queries you will do against the table. To avoid running too many inserts queries, take a look at extended inserts so that you can insert more data per query. (Also, remember to delete the rows you dont need, eg. any row older than x days).

(anyway, a relational database like mysql might not be the correct tool for this. There are time series database that are designed for handling data like this)

",1523552678.0
mudclub,"If you need to store only a 24 hour cycle, I’d consider round robin databases. ",1523491546.0
PM_ME_YOUR_HIGHFIVE,what did you try so far?,1523476259.0
jericon,"Please provide the schema that you have, it is nearly impossible to help you generate a query without the schema.

In addition, show us what you have tried and indicate where you are getting stuck.  We aren't here to do your homework/project for you, but we are happy to help you get past something that's blocking you.",1523485388.0
186282_4,"You are getting no responses because there isn't enough information here to help you. We can look at your query or your schema and make suggestions, but your question as posted isn't answerable. 

You got downvoted probably because this is obviously homework. ",1523485405.0
synackSA,"If your MySQL version doesn't have the functions listed here: https://dev.mysql.com/doc/refman/5.7/en/json-function-reference.html, then it's going to be very hard for you to do what you want in SQL.

I would suggest parsing the json in a programming language. If you're trying to do a WHERE on the fields inside the json, then it's going to be a lot harder, you could try some regex magic and see if that works.",1523470051.0
kadaan,"Collations are just for comparisons, so there's zero concern from the database-side with you changing your connection's collation.

The only time you'd notice the difference is in the application if you get results that did/didn't match the collation before.

utf8_unicode_ci is superior as it's more standards-compliant than uff8_general_ci, but unless you're dealing with a lot of extended characters and foreign language alphabets you shouldn't see any difference.

edit: found this that explains a bit more as well: https://stackoverflow.com/questions/766809/whats-the-difference-between-utf8-general-ci-and-utf8-unicode-ci",1523397353.0
jahayhurst,"MySQL's defaults are pretty good by 5.7.x - to the point where I would not deviate from them until you see a specific issue with one of the settings.

It's a far better idea to enable the slow query log, save that log for a while, run pt-query-digest on it, identify the queries that are hurting you the worst and go back and index/rewrite to correct those queries.

https://www.percona.com/doc/percona-toolkit/LATEST/pt-query-digest.html

https://www.percona.com/downloads/percona-toolkit/LATEST/

I usually install percona-toolkit from an RPM or DEB, depending on my system. If you want, you should be able to run it _from your workstation_ instead of the server, if that interests you.",1523384237.0
r3pr0b8,"case sensitivity, perhaps?   `sponsors` versus `Sponsors`",1523380620.0
Davidhatch3,Whoops! Sorry for the confusion. I did create the table “Sponsors” before I attempted to create the “Events” table.,1523387316.0
r3pr0b8,"    ALTER TABLE ar_distribution
    ADD INDEX triway ( account, id, amount )

this should be a **covering index**

also, maybe add an index on `GLTYPE` in `gl_accounts`

p.s. your SELECT clause has some ""hidden"" columns that aren't in your GROUP BY, so be warned the [results will be indeterminate](http://download.nust.na/pub6/mysql/doc/refman/5.1/en/group-by-hidden-columns.html)",1523373463.0
etrnloptimist,"You shouldn't be using a left join on t2 if you're using data from t2 in your where clause (Id's). This will be the equivalent of a normal (inner) join since those cols would be null in the case of a non-matching left join.

Also, if these types ( 'A1', 'A2', 'L1', 'L2', 'E1', 'E2' ) are a majority of your data, MySQL will not use the index since you're not really limiting the dataset by any significant degree.

Lastly, about that join. It is either incorrect and should be an inner join, or you a trying to limit the amount of data from t2 that you will potentially match with t1. If this is the case, I would suggest ""helping"" out the optimizer by doing a nested query:

    SELECT t1.GLACNT, t1.GLTYPE, t1.GLDESC, SUM( t2.amount ) AS sumar
    FROM
    (select * from gl_accounts where GLTYPE IN ( 'A1', 'A2', 'L1', 'L2', 'E1', 'E2' )) as t1
    
    left join
    
    (select * from ar_distribution where id between 2466747 and 2621796) as t2
    
    on t1.GLACNT = t2.account
    GROUP BY GLACNT 
    ORDER BY GLACNT +0, GLTYPE",1523374394.0
wrinklehead,"Thanks for the help that does indeed perform better.  I do not have an index on GLTYPE because there are so few rows ~300 in gl_accounts that the database won't use an index for that table anyway.  

The t1.GLTYPE and t1.GLDESC will always be the same for a given GLACNT.... so do I still have indeterminate result?

Thanks for your help",1523374663.0
Bitter_Bridge,"SELECT url 
FROM table 
WHERE url RLIKE “http://example.com”",1523382340.0
gizram84,"Users are defined as 'name'@'host'.

Simply define a few users, some which have read only privileges, and some that can write too.

Or for the read only users, make a simple web interface which shows them the info you want to display, and don't give them db credentials at all. ",1523331286.0
Bitter_Bridge,"Three things:

1. Stop using LIKE. Use RLIKE instead, and drop the %. 

2. Use a JOIN to select from both tables. Without seeing your full schema I would guess a LEFT JOIN.

3. Your WHERE clause is forcing the match to exist in both tables or it won’t return results because you’re using the pattern WHERE (first table constraints) AND (second table constraints) which means your search term has to match something in the first table AND something in the second table. 

Edit: for your own sake (and for the sake of security and integrity), use PDO instead of mysqli. ",1523268695.0
mudclub,"You used copy, right?  RIGHT?",1523216550.0
JudBurnet,"In most cases, only the index will be corrupted (the index is a separate, smaller, file with records that point to the main data file) - actual data corruption is extremely rare. Fixing most forms of corruption is relatively easy. As with checking, there are three ways to repair tables. These all only work with MyISAM tables - to repair corruption of the other table types, you will need to restore from backup:  

But you may use Google on phrase how to fix corrupt mysql database 2005 windows, there I found following  
https://dev.mysql.com/doc/refman/5.5/en/forcing-innodb-recovery.html  
https://dev.mysql.com/doc/refman/5.7/en/backup-and-recovery.html  
https://www.fixtoolbox.com/repairtoolformysql.html  

•	The REPAIR TABLE SQL statement (obviously the server must be running for this)  
•	The mysqlcheck command-line utility (the server can be running)  
•	The myisamchk command-line utility (the server must be down, or the tables inactive)  

Repairing a table requires twice as much disk space as the original table (a copy of the data is made), so make sure you are not going to run out of disk space before you start.  ",1523707855.0
xenilko,"If you want a ghetto workaround just copy the text from workbench , open notepad, paste it in the notepad document and save it as script_name.sql ...that s basically what workbench would do :)",1523232165.0
jericon,"Your post has been removed.

Reason: Self Promotion

Your post history shows that you have a propensity to post from the same domain or topic often to a wide breadth of somewhat related subreddits without contributing to the communities otherwise.  Please see [Reddit's Self Promotion Policy](https://www.reddit.com/wiki/selfpromotion).",1523300590.0
mexican_restaurant,"I think what you want at least for the first two items is when you create your foreign keys set the “update” and “delete” constraints to be “cascade”. This means rows in child tables will be deleted and updated accordingly when things change in the parent table. 

I’d probably use an auto incrementing id primary key in the category table and a corresponding category id foreign key in the item table. Just a personal preference and more industry standard. I’d also give a little more thought to the names of your tables, they don’t seem to match the columns on some (to me) and seems like it could be a little cleaner. Table names should be singular of the “entity” type they are storing. So for a table of orders, “Order” is perfect, or “Customer”, etc. 

Can you expand on the third question? I think I might know what you’re asking but don’t want to respond incorrectly. ",1523161613.0
mk_gecko,"I wonder if it has something to do with the ""db.php"" file re-establishing the PDO connection with each new page that is loaded. This method works fine with Mysqli, but maybe I should make the $db2conn variable a session variable then I would just have to open the connection once.",1523143237.0
SaltineAmerican_1970,"Read this: https://phpdelusions.net/pdo and pay attention to named parameters.

It is a great resource and you will learn more there than by reading the PHP docs.",1523144057.0
jericon,You will probably get better results from/r/phphelp. ,1523146192.0
Bitter_Bridge,"For runtime errors, use ini__set('display__errors', '1');

Problems in your code:

1. You’re using LIKE instead of RLIKE. Change that up and drop the %. 

2. You must have a separate PDO parameter for each instance of a variable, even if it’s the same value. You have listed :q three times, but it should be :q1, :q2, and :q3, and you should bind for each of them. 

3. You do not need to bind value and parameter both. Read about the difference and pick one. Personally, for your use case, it doesn’t look like it matters much. ",1523179346.0
Bitter_Bridge,Is this your homework?,1523180989.0
r3pr0b8,"     CONCAT('HW','1')",1523169023.0
etrnloptimist,"I'm gonna go out on a limb and say you're suffering from [premature optimization](http://wiki.c2.com/?PrematureOptimization).

But, to answer your question: do it in one query and parse it in code.",1523144809.0
ishegg,"The answer as always is it depends. For a sales receipt you could use a [read model](http://gorodinski.com/blog/2012/04/25/read-models-as-a-tactical-pattern-in-domain-driven-design-ddd/). This implies possibly using your first suggestion, where you only do one trip to the database. If you index your tables correctly, a JOIN operation is very performant, and, depending on the conditions, it will probably outperform 2 queries.

I would add that you should try both ways and measure the results, so you can see for yourself, with some real data, what's better for your use case.",1523135536.0
SomeGuyNamedPaul,"The only truly safe way to downgrade is with mysqldump.  That said, you may be alright with the different levels in dev vs prod as long as your code is pretty vanilla.  Also, don't forget that it can always instead your own on an EC2 instance.

But otherwise, text dump if you absolutely need to be sure.",1523200589.0
xenilko,"5.5.3 is two major versions away from 5.7.21 (oracle’s way is the second digit is the major version..i know, super dumb...this will stop at version 8)

So I’d be somewhat anxious. Technically if you can Id make a duplicate db and run it againt the 5.5.3 binary and do a checksum table of all your tables in both versions to see how it reacts...and also look at all your triggers/procedures to see if they use something that might not be present at that version.

Good luck",1523144079.0
razin_the_furious,"Calculate the percentage of wins in the select, give that value an alias, then order by it",1523098470.0
r3pr0b8,"    SELECT ...
      FROM ...
    ORDER
        BY wongames DESC
         , 1.0 * wongames / playedgames DESC",1523099922.0
Bitter_Bridge,Could you provide a show create table for both tables as well as your insert statements?,1523063875.0
Bitter_Bridge,"Your first table uses the MyISAM engine, which does not support foreign keys. Convert it to InnoDB. ",1523093837.0
jericon,"So... offsets are bad. 

If you have just a limit, mysql will stop returning results when you hit that limit. But an offset will start finding rows. Read (but skip) your offset and then start counting. 

Say you have a query that with no limit will return 10 million rows. You only want 10,000. So you throw a “limit 10000” on it. The first 10k rows that are found will he returned.

You decide you need to view all the data, though.  But only 10k rows at a time and use offset. Your first query (offset 0) is fine it returns the same* rows as above. Your second query (offset 10000) will read those first 10k rows returned in the offset 0 query, then it returns 10k rows. Total amount of rows mysql has to read there is now 20k. Keep going and your last query (offset 990000) has to read those first 9,990,000 rows, THEN return 10k.

Your best bet is to not use an offset. Instead either make a procedure or in your code do the first select with limit 10k and an order by. if the column you order by is not unique you may miss some data, usually I will add an auto increment for this. Then, you notate the highest value of “order by” (let’s call this ob_cursor) returned and for the next page of results you add “where order_by_value > ob_cursor. Keep the limit and everything else. 

Now mysql can use that, especially if it is an indexed column, to bypass the unneeded rows and read/return only the rows you need. Initially this will perform about the same as the other query. But as you get further and further into the table you will find its execution time will barely change and be far more consistent. ",1523016814.0
MyDataBes,looks good on your explain.,1522973318.0
ajanty,"If there's no index on the where clauses, put one on both and retry (be sure the results are not cached).",1523003295.0
r3pr0b8,"you forgot the join condition, so you're getting a **cross join**

probably explains why you threw that GROUP BY in there

please learn how to write explicit INNER JOIN queries",1522954841.0
xenilko,"Done that this year, consider using gh-ost for this! ( https://github.com/github/gh-ost )

This wont lock the table so people can still work and you can also test the checksum of the new table against the old when the change is done!

Good luck and make sure you used unsigned bigint!

Edit: alter table should also be fine if people can deal with some issues/downtime. For us that was an issue. I would drop that column, thats loooking for trouble.",1522966635.0
faxattack,"Well, thats a few transactions that will be logged. ",1522941433.0
jericon,"You have many places data is written. 

Binlogs
Redo logs (if innodb)
The actual data itself
Indexes
Etc

I’m willing to bet the server is mixed mode or row based replication which means all you insert is being written to the binlogs. ",1522945694.0
jericon,"Your post has been removed.

Reason: Self Promotion

Your post history shows that you have a propensity to post from the same domain or topic often to a wide breadth of somewhat related subreddits without contributing to the communities otherwise.  Please see [Reddit's Self Promotion Policy](https://www.reddit.com/wiki/selfpromotion).",1522975537.0
jericon,"Linked servers is not something that is supported by MySQL.  

Maybe you can tell us what you are trying to do and what your goal is, and we can make some suggestions based on that.",1522889523.0
r3pr0b8,stick a semi-colon in front of that SELECT ,1522873362.0
vutheran,"I'm a DBA and my company has the same database architecture as what you described (company per database, schema is the same but different data). We moved to jemalloc for the memory allocator and it significantly reduced the memory footprint of MySQL.

I'd highly recommend you try that before doing a big lift like changing your database architecture. ",1522861778.0
jericon,"What is your configuration?  The number of databases can be an impact, but if you are ""leaking"" memory, you can likely tweak some of the configuration of the server to decrease the usage.",1522867108.0
SaltineAmerican_1970,Would it be a good idea to consider putting each database (or a group of databases) in their own docker containers? ,1522872606.0
r3pr0b8,"> Not sure if it is the correct way.

it is correct in dealing with the existing design

however, the existing design is not correct, it breaks **First Normal Form**",1522853717.0
faxattack,Try https://www.percona.com/software/mysql-database/percona-xtrabackup,1522760044.0
LiamLuthor,"Why can’t you take the mysqlsump?  This can be done without locking tables.  I do this everyday on my 1.4TB db instance.  If you don’t have replication and don’t do sql dumps wtf are you doing to safeguard your data?


I do the exact process you are attempting, every month as part of our disaster recovery process to ensure our backups are functional.


",1522757828.0
2Tack,"I got you, easy peasy lemon squeezy.

(echo ""SET SESSION SQL_LOG_BIN=0;""; mysqldump --single-transaction --master-data --all-databases -uroot -h$master-host -P$master_port ) | mysql -uroot -S $master_socket

Run from slave. No locks. Going to take a while though, and I hope you have a good stable network connection. Shouldn't lock anything if you're running INNODB. Just make sure you already have the initial replication config (host, user, pass) on the slave. This will update the binlog position for where it needs to pick back up when it's done.

Also, get a freaking disaster recovery solution. You're lucky you haven't been burned yet.",1522767993.0
xilanthro,"Piece of cake - take a consistent point-in-time snapshot using [Percona Xtrabackup](https://www.percona.com/doc/percona-xtrabackup/LATEST/howtos/setting_up_replication.html) without interrupting processing on the master. 

When you restore, take care to ""change master to"" the correct binlog file and position as described in the xtrabackup_info file left in the datadir root of the image",1522760198.0
ajanty,"MySQL Enterprise backup solves your problem. Binlog backup.

And remember, replication != backup(!!!)",1522761120.0
The_Espi,"I didn’t get a chance to read all the comments, but I believe you only need to lock the master table to get the current position and bin file the master is currently in. Then you can unlock your master and dump and import to your slave. 

After the import to the slave and actually start the slave you’ll give it the position to start replicating from. 

So your master should only have to be locked for a few moments while you take a screen cap of your “show master status” results

",1522773019.0
gmuslera,"You can put a slave to start from nearly zero, but you need to have the full binary log since the master was created, and the slave would have to reply all the writing transactions since that beginning of time. 

As was pointed out, you don't have to block the DB/transactions doing a mysqldump, at least if your tables are innodb, not have to do it from the same server the database resides or store the dump there. There are alternatives to mysqldump to setup a slave like using percona xtrabackup.",1522760123.0
Naschoff,"Perhaps something like: 
Select p.name from prod p left join (select ip.prod_id from cust c join incl_prod ip on c.id = ip.cust_id where c.name = ""hanky”) tbl on p.id = tbl.prod_id where tbl.prod_id is null;

Excuse the poor formatting, this is from an iPhone. ",1522694856.0
Naschoff,"Perhaps something like: 
Select p.name from prod p left join (select ip.prod_id from cust c join incl_prod ip on c.id = ip.cust_id where c.name = ""hanky”) tbl on p.id = tbl.prod_id where tbl.prod_id is null;

Excuse the poor formatting, this is from an iPhone. ",1522695348.0
etrnloptimist,[GROUP_CONCAT](https://dev.mysql.com/doc/refman/5.7/en/group-by-functions.html#function_group-concat),1522678551.0
,[deleted],1522679232.0
SKTT1_Bisu,"select * from Recipes r  
inner join Recipe_ingredients ringr on (r.id = ringr.recipe_id)  
inner join Recipe_instructions rinst on (r.id = rinst.recipe_id)  
",1522678859.0
picturepages,This was an exact exercise in my Database 101 class.  Down to the column names.,1522699647.0
jericon,"Your post has been removed.

Reason: Self Promotion

Your post history shows that you have a propensity to post from the same domain or topic often to a wide breadth of somewhat related subreddits without contributing to the communities otherwise.  Please see [Reddit's Self Promotion Policy](https://www.reddit.com/wiki/selfpromotion).",1522703836.0
jericon,"Your post has been removed.

Reason: Self Promotion

Your post history shows that you have a propensity to post from the same domain or topic often to a wide breadth of somewhat related subreddits without contributing to the communities otherwise.  Please see [Reddit's Self Promotion Policy](https://www.reddit.com/wiki/selfpromotion).",1522607543.0
jdya1,Mysql (Q&A guide 2018),1522562089.0
FirstEmployee4,"I figured this out...  I had the issue where an IP address was stored as an integer, however the bytes where in little endian order, not network order so the INET_NTOA returned the wrong answer.

wrong:

    INET_NTOA( 673059850 )
    40.30.20.10

right:

    INET_NTOA( CONV( HEX( REVERSE( CHAR( 673059850 ) ) ), 16, 10) )
    10.20.30.40

If you're really working with hex values, this should work as long as there are no leading 0s (zeros):

these are correct:

    HEX( REVERSE( CHAR( 0x01234567 ) ) )
    67452301
    HEX( REVERSE( CHAR( 0x01004567 ) ) )
    67450001 
    HEX( REVERSE( CHAR( 0x76543200 ) ) )
    00325476

does not work quite right here:

    HEX( REVERSE( CHAR( X'00112233' ) ) )
    332211

Since leading zeros are dropped when converting to INT, CHAR does not see them.  I don't know how to fix that scenario yet.  May be back to the drawing board with a multiline function for that.

",1522443121.0
jericon,"We aren't here to do your tests or homework for you.  

What have you tried?  Where are you getting stuck?  Tell us what you are specifically having an issue with, and we can help.",1522428429.0
making-flippy-floppy,[Here's a tutorial on MySQL subqueries](http://www.mysqltutorial.org/mysql-subquery/). The second example they do is very similar to what  you have. ,1522461852.0
jericon,"Your post has been removed.

Reason: Self Promotion

Your post history shows that you have a propensity to post from the same domain or topic often to a wide breadth of somewhat related subreddits without contributing to the communities otherwise.  Please see [Reddit's Self Promotion Policy](https://www.reddit.com/wiki/selfpromotion).",1522428347.0
NotTooDeep,"If you are the only user of this data, then any database is probably overkill, unless you're studying technology. One of the benefits of any brand of SQL database is that it can be easily shared across an organization of any size. MySQL fits this requirement.  400 records just sounds like a spreadsheet, doesn't it.

Another reason to use a database instead of a spreadsheet is when your inventory of spreadsheets surpasses a few dozen sheets. Trying to correlate data between two or more sheets can become complicated. SQL databases can ease some of this, but there's a tradeoff in that you have more to learn before you begin to see the benefits.

Why not start with Excel? Get all of your data into one or more sheets and see what kinds of relationships between different types of data are important to you. This is a basic data analyst exercise. You'll learn a bit more about Excel, maybe some macros. You'll learn more about your data and why some parts are more important than others. 

If, on the other hand, you are building an application that needs to look up your data and/or maintain your data, then look some basic tutorials in MySQL and have at them. MySQL is free and the parts you'll need to focus on are limited to the install (google will answer all of your questions) and the SQL language. If you want to grow your programming skills, especially around web apps, then knowing how to use SQL will help you.",1522346880.0
JackOfAllCode,If you have Access and you don't plan on this project becoming any larger you might as well use it. I've even used it for a small application where a single user needed to track about 40 items. Nothing wrong with MySQL though if you want to use it just to learn while your at it.,1522348365.0
SomeGuyNamedPaul,For 200-400 rows my thought is that sqllite might better serve you if you even decide they you want to keep the data in a database.,1522349255.0
kaydub88,"I've been there man. If you wrote all those VBA macros why not go look for a development/support/helpdesk type of job and get into the IT/Software industry?

If this is your family's business and they *need* you I get it. If that's the case it's definitely possible. I did the same thing 6+ years ago. Built the business around excel spreadsheets and macros until a lot of work was automated and it freed up time to properly develop a solution (that solution being getting a new job where I was paid properly).

There also might already be a software product out there doing what you need, have you looked into that possibility?",1522340126.0
,[deleted],1522364236.0
razin_the_furious,"    SELECT Movie.EquipmentID, Movie.Make, Movie.Model, Equipment.Description, SUM(IF(Movie.Type LIKE ""%video%"",1,0),IF(Movie.Description LIKE ""%video%"",1,0)) AS findScore
    FROM Movie, MovieLoan  
    WHERE Movie.Damaged = 0 
    AND MovieLoan.Current = 0 
    AND Movie.Type LIKE ""%video%"" 
    OR Movie.Description LIKE ""%video%"" 
    GROUP BY Movie.EquipmentID, Movie.Make, Movie.Model, Movie.Description 
    HAVING findScore < 2;",1522325613.0
r3pr0b8,"here's your current query, just reformatted so it's more legible to humans than a mere computer

    SELECT Movie.EquipmentID
         , Movie.Make
         , Movie.Model
         , Equipment.Description 
      FROM Movie
         , MovieLoan 
     WHERE Movie.Damaged = 0 
       AND MovieLoan.Current = 0 
       AND Movie.Type LIKE ""%video%"" 
        OR Movie.Description LIKE ""%video%"" 
    GROUP 
        BY Movie.EquipmentID
         , Movie.Make
         , Movie.Model
         , Movie.Description

right off the bat, there are several things wrong

**worst offence**  -- you are cross-joining the tables, i.e. you have not supplied a valid join condition

by the way, please learn explicit joins using INNER JOIN syntax

**second** (also a serious error) your ANDs and ORs in the WHERE clause do not work the way you think they do

**third** (also a serious error) your SELECT clause and your GROUP BY clause don't match, and in fact your SELECT clause references a table which does not exist in your FROM clause

once you fix these errors, i'll help you understand how to do the ""either but not both"" part",1522338413.0
jericon,"Your post has been removed.

Reason: Self Promotion

Your post history shows that you have a propensity to post from the same domain or topic often to a wide breadth of somewhat related subreddits without contributing to the communities otherwise.  Please see [Reddit's Self Promotion Policy](https://www.reddit.com/wiki/selfpromotion).",1522428328.0
r3pr0b8,"six fixes --

1/ please learn explicit joins

2/ start your FROM clause with the most restrictive table first (the one which has the WHERE condition applied to it) and then join the other tables from there

3/ remove the skills from the GROUP BY (because ""video coding"" and ""video editing"" are different)

4/ show the entire employee, not just the first name (because there could be more than one employee with the first name ""John"" for example)

5/ LIKE comparisons are case insensitive by default

6/ you want HAVING more than one


    SELECT e.firstname
         , e.lastname
         , e.email
         , e.gradeid
      FROM MovieSkill s
    INNER
      JOIN MovieEmployeeSkills es
        ON es.skillid = s.skillid
    INNER
      JOIN Movie e 
        ON s.employeeid = es.employeeid 
     WHERE s.title LIKE '%video%' 
    GROUP 
        BY e.firstname
         , e.lastname
         , e.email
         , e.gradeid
    HAVING COUNT(*) > 1",1522258369.0
jericon,"Your post has been removed.

Reason: Self Promotion

Your post history shows that you have a propensity to post from the same domain or topic often to a wide breadth of somewhat related subreddits without contributing to the communities otherwise.  Please see [Reddit's Self Promotion Policy](https://www.reddit.com/wiki/selfpromotion).",1522220526.0
Bitter_Bridge,"A few things: you’re not selecting position, but you’re also using count which is an aggregate function with your group by. Unless each code has the same position, you need to group by code,position. ",1522239905.0
r3pr0b8,"you tried 'NaN'?   try NULL, no quotes",1522092478.0
dcun,Change your FROM to be Customers and then join in Addresses twice (just make sure to use an alias on the joins for each address type). Add the new alias into your select and you should be golden!,1522030172.0
mvelasco93,"Post it on pastebin and put the link with the encoding for sql. Pasting code on reddit, it just lose the format.",1522024187.0
mojo3120,"I don't know anything about mobile display, but to fetch just one banner from mysql you can just stick a LIMIT 1 at the end of the query

    $bannerinfo = array();
    // first try to get a random banner from this domain
    $query = ""SELECT banners_wide.bannerID, banners_wide.businessid, filename, banners_wide.link, special 
    FROM banners_wide, banners_regions, clients 
    WHERE banners_wide.active='1' 
    AND clients.active='1' 
    AND regionid='$regionid' 
    AND banners_wide.bannerID=banners_regions.bannerid 
    AND clients.businessid=banners_wide.businessid
    AND banners_regions.bannertype='wide'
    LIMIT 1"";
",1522025725.0
Hoysurdady,Just throwing this out there but bootstrap 4 lets you set visibility to divs based on screen size. I suggest looking there. It sounds like it will help achieve your goal. ,1522028959.0
r3pr0b8,"code two consecutive single quotes for each single quote you want treated as data

    INSERT INTO names VALUES ( 'O''Toole' )",1522016248.0
jericon,"I’m going to let this slide, but please just ask your question here instead of linking to another site where you asked if. Having both the question and the responses here, in one place, is very useful. ",1522034430.0
bonfire09,"Well I fixed the issue. Apparently for the Desktop file, the users group was not added as a group to the security with write permissions on win 10. So now its working if anyone is having the same issue. :) ",1522001317.0
jynus,"Either your mysql path/innodb path changed, the main tablespace got deleted/moved or it got logically or physicaly corrupted.

Check the configuration of your paths and check the right files are there, otherwise, recover from backups. If you just want it to start with no data, wipe the directory, recreate with --initialize and try again.",1521926443.0
b3dazzle,"Can you try specifying the database name as the 4th parameter of you $conn= line?

I think if you don't specify it, it uses the default so that it could be looking at the wrong database.

If that doesn't work, log into phpmyadmin (assuming your install uses) and try running your query manually - pretty simple query in this case but good general troubleshooting",1521879366.0
jericon,"When you say ""Server doesn't start"", give us more details.  What is the behavior it shows when you try to start?  What does the error log say?  We need more information to be able to help.",1521850516.0
Laurielounge,"Sorry, service doesn't start.

    Unregistered Authentication Agent for unix-process:7183:1103210 (system bus name :1.35, object path /org/freedesktop
    Registered Authentication Agent for unix-process:7443:1120117 (system bus name :1.36 [/usr/bin/pkttyagent --notify-f
    Starting MariaDB 10.2.13 database server...
    -- Subject: Unit mariadb.service has begun start-up
    -- Defined-By: systemd
    -- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
    --
    -- Unit mariadb.service has begun starting up.
    2018-03-24  1:43:34 140460651702400 [Note] /usr/sbin/mysqld (mysqld 10.2.13-MariaDB-log) starting as process 7592 ..
    mariadb.service: main process exited, code=exited, status=1/FAILURE
    Failed to start MariaDB 10.2.13 database server.
    -- Subject: Unit mariadb.service has failed
    -- Defined-By: systemd
    -- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
    --
    -- Unit mariadb.service has failed.",1521855977.0
beermad,"Check the systemd unit file for mariadb. It probably has a line:

    ProtectHome=true

Change this to false and it'll work. But be aware that it'll be over-written whenever you get an update from the repos, so you'll have to re-edit it.",1521916468.0
jericon,"Your post has been removed.

Reason: Self Promotion

Your post history shows that you have a propensity to post from the same domain or topic often to a wide breadth of somewhat related subreddits without contributing to the communities otherwise.  Please see [Reddit's Self Promotion Policy](https://www.reddit.com/wiki/selfpromotion). ",1521826420.0
jericon,"How is this script any better than just running a grant/create on the command line?  It seems like it's just a glorified way to do that, full of a bunch of advertisement for yourself.

I am removing this post because it is blatant self promotion and not useful.  You have never contributed in /r/mysql before.  If you wish to continue sharing your ""tools"" here, please contribute to the community in a meaningful way prior to doing so, and please review [Reddit's Self Promotion Policy](https://www.reddit.com/wiki/selfpromotion).",1521840913.0
emdash5,Either Database Analyst or Data Quality Manager could be appropriate titles. ,1521789079.0
pinkdinosauronbowtie,"It sounds like you only work with database and perform no admin tasks. 
I think that would mean you should have data in your title somewhere. Or you could generically take Support Engineer.",1521794069.0
janos42us,Thank you both!!,1521810787.0
jericon,"I agree with the others.  I wouldn't call you a Database Admin or Engineer, as it doesn't seem that you are running or administrating the servers behind the databases.

I'd look up various Data Analyst positions and other stuff like that, read the job descriptions and see what is closest to what you do.

That being said... you could probably gain a bit of free time if you automated the process.",1521827499.0
msiekkinen,"Call yourself a DevOps Data Scientist or a Database SRE (Site reliability engineer)

Those combos of all the hot new buzzwords for a resume, and SRE was coined by google.  ",1521830533.0
jericon,"There is no ""contact"" function.

I believe you are looking for CONCAT

https://dev.mysql.com/doc/refman/5.7/en/string-functions.html#function_concat",1521767220.0
jericon,"There are a number of tools out there that are made to synchronize the schemas between two servers.  

here's one I found on google.
https://www.devart.com/dbforge/mysql/schemacompare/",1521767589.0
tylerpoland,"I haven't worked with it personally, but [mysqldiff](https://dev.mysql.com/doc/mysql-utilities/1.6/en/mysqldiff.html) looks promising. Back when I used GUI tools more I had a license for [SQLYog](https://www.webyog.com/product/sqlyog) which was also good for this.

Of course, it might also make sense to look at your dev process and try to understand where these differences are originating. A good practice would be to rely on framework orm-based migrations (if you are using one) or include SQL migration scripts in your source code repository.",1521813680.0
DataVader,[Oracle lifetime support policy](http://www.oracle.com/us/support/library/lifetime-support-technology-069183.pdf)  page 21. Is that what you are looking for?,1521736801.0
carlwgeorge,https://ius.io/LifeCycle/#mysql,1521774367.0
DataVader,"Or just have ""posts"" and ""comments"". ""comments"" contain at least 4 IDs:
- CommentID (AI PK)
- PostID (NN; FK of ""posts"")
- ReplyTo (FK of `posts`.`CommentID`; default NULL)
- UserID (obvious)

That way you aren't screwed if someone wants to reply to a reply.

",1521738983.0
jericon,"I think you need two tables.

Posts and Comments.

Posts should have a post ID, comments should have post ID and Parent comment ID.  If the comment is a direct response to the post, leave parent comment id blank, null, 0, whatever.  If it is in response to another comment, then put the id of that comment in ""parent comment id"".

This will allow you to easily see what comments are in response to what post, and also maintain the hierarchy of comments if needed.",1521763097.0
razin_the_furious,"Do you want the comments to be nested?

That is, does it go:

    Post -> Comment -> next Comment -> next Comment

or 

    Post -> Comment -> Reply to Comment
    |
    -> Other Comment -> Reply to Other Comment",1521723518.0
davvblack,"I think we need more specifics, there's no silver bullet here. Sometimes it's right to use a traditional view (which is really more like a subquery helper, the data isn't actually stored anywhere), sometimes it's best to materialize that view to store and index it, and sometimes other solutions are better. ",1521699895.0
mischiefunmanagable,only grant them select to the schemas they need access to,1521647550.0
msiekkinen,"From the [docs](https://dev.mysql.com/doc/refman/5.7/en/set-password.html)

> When the read_only system variable is enabled, SET PASSWORD requires the SUPER privilege in addition to any other required privileges. 

However setting read_only makes the entire mysqld readonly in the sense that even if you had permissions to insert/update/delete they would fail.   In this mode only an account with the super priv would be able to actually make changes.
",1521683190.0
kadaan,"Look into the `distinct` keyword, or `group by`. Both will give you what you want, but in different ways.",1521580864.0
jericon,"When you say ""fields"" do you mean columns?  Can you show what your table schema looks like?",1521586451.0
Hawkals,"Google gave me:
```
ORDER BY IF(${order} = 1, 'rand()', 'id')
```
Give that a shot!

EDIT: actually tried this myself, this didn't work, darn.",1521577545.0
etrnloptimist,"Keep the order by, but restrict by a conditional where clause.",1521633468.0
jericon,"Hello.

It looks like this is a question about PHP and not MySQL.  I recommend you check out /r/phphelp for assistance.",1521573754.0
SomeGuyNamedPaul,What does the MySQL error log say?  The above is basically useless for diagnosing anything other then it not starting as there's nothing there from MySQL itself.,1521467782.0
Zutch,"this is the code that that runs for mysql.service in systemcl, anything in it that might be the issue ? 

https://pastebin.com/wb5nDnMt",1521488274.0
0ba78683-dbdd-4a31-a,"Given that it sounds like you don't know the absolute basics I'd recommend googling rather than asking for a comprehensive primer. Try Tutorials Point.

Edit: To be a bit more helpful, you probably want 4 tables: students, subjects, grades and grade. The grade table simple references the IDs of the other three tables and you can use joins to query them.

If that makes sense then feel free to ask a follow-up. If not then you're probably a bit ahead of yourself.",1521282797.0
al-eriv,Try to watch some tutorials on modelling data. Can I link on this subreddit?,1521318004.0
msiekkinen,"What client are you using to connect?   Access denied means password error overall for any connection.

If you're getting results from queries you're getting in with some kind of credentials.

Run

    select CURRENT_USER();

to show what you're getting in under.  ",1521216027.0
mudclub,"Didja try googling something like ""mysql stored procedure permissions""?",1521146204.0
r3pr0b8,"> LEFT JOIN rooms ON pa.pid = rooms.room 

you're joining on the participant id being equal to the room number?

seems legit",1521147211.0
NotTooDeep,"Your data is not normalized. First, some standards. Tables are named in the single tense. PKs are the first column in a table definition. FKs are the parent table_name and its PK.

Change people to person. Here's a more readable structure.

    CREATE TABLE `person`(
         id int,
        `first_name` VARCHAR(40),
         last_name varchar(40),
         `status` CHAR(3)
    );

Room is not an attribute of a person, so we lose the 'office' column.

A person may attend one of more meetings.

A room may host one and only one meeting during a given time period.

A meeting may occur in one and only one room.

If we have a list of rooms and a list of times that they are available, there is not reason to have a table called participants.  Move the meeting.data into the room table.

There are other problems you need to address, but just google ""hotel reservation data model"" and all will become clearer. 

The room is the asset at the center of the model. People show up to the room at a specific time. A special person called, perhaps, a presenter or organizer, shows up, too. This constitutes a ""meeting"".  If these people do not show up to that room at the appointed time, then no meeting is created. This suggests that your meeting table should contain nothing more than the idea of the meeting.  The room has to associate, through FKs, all of the participants of a meeting, including the roles they play; i.e. audience, presenter, registrar, etc.",1521170408.0
mischiefunmanagable,"https://en.wikipedia.org/wiki/Associative_entity
",1520968851.0
etrnloptimist,"All you need is a written by table that contains ISBN and author ID. Two columns, the end.",1520985497.0
KrevanSerKay,"Like /u/etrnloptimist said. Honestly, I think an easier solution would be 

    CREATE TABLE written_by (
        isbn VARCHAR(15) NOT NULL,
        authorID VARCHAR(7) NOT NULL,
        PRIMARY KEY (isbn, authorID), -- Prevents duplicates in this table
        FOREIGN KEY (isbn) REFERENCES book(isbn),
        FOREIGN KEY (authorID) REFERENCES author(authorID)
    );

Then you can easily do

    SELECT 
        book.title as Title, 
        group_concat(author.last_name) AS Authors 
    FROM 
        book
        INNER JOIN written_by ON
            book.isbn = written_by.isbn
        INNER JOIN author ON
            written_by.authorID = author.authorID
    GROUP BY
        book.isbn;

That'll return something like

Title|Authors
:--|:--
The Lord of the Rings|Tolkien
Fundamentals of Physics|Halliday,Walker,Resnick

--------------------------------------

Alternatively, you can answer questions like:


    SELECT 
        author.last_name AS LastName,
        count(written_by.isbn) AS BookCount
    FROM 
        author
        INNER JOIN written_by ON
            author.authorID = written_by.authorID
    GROUP BY
        author.authorID
    ORDER BY
        BookCount DESC
    LIMIT 2;

That'll return something like:

LastName|BookCount
:--|:--
Tolkien|10
Rowling|8

**TL;DR You can handle N:M relationships by having multiple rows instead of trying to guess how many columns to include. Then you can leverage SQL's Group By functions to answer many of the questions you might want**",1520993392.0
NotTooDeep,"An Author may write one or more books.

A book may be written by one or more authors.

It's a simple many-to-many relationship.  You are very close, but you've denormalized the bridge table. Replace all six coauthor columns with a flag, is_primary_author, with values of Y or N.

Now you have this:

    CREATE TABLE writtenby (
        isbn        VARCHAR(15)             NOT NULL,
        authorID    VARCHAR(7)             NOT NULL,
       FOREIGN KEY (bookID) REFERENCES book(ID),
        FOREIGN KEY (authorID) REFERENCES author(ID),
        is_primary_author
        );    

Second issue: the ISBN is an attribute of the book table. Do not use it for the PK; use an artificial key, like bookID, or better, just id. ISBNs are an inventory control artifact. Don't make your system dependent on the PKs of another, legacy system. 

Third: you seem to be misunderstanding how this bridge table will look. There will be one row for every author of a book. Let's say you have five authors and five books. Four of these books were written by all five authors. That requires 20 rows in the bridge table. One book was written by one of the authors, and that requires just one row.

The beauty of this is not mathematical. The beauty is the ease of data maintenance. Let's say a lawsuit determines that one of the five authors didn't write anything at all and must be removed. 

You first choice could be delete from writtenby where authorID = x.  Depending on your business requirements, this may be sufficient. You are, however, losing data, and therefore history. If you need that history, then simply add another flag to the bridge table called is_deleted.

Your queries will all contain is_deleted = Y unless you're researching authors that falsely took credit for a book they did not write. Then you would use is_deleted=N. 

Hope this helps.

Note: why is a PK of id better? It reads better in code. Which is more comfortable to read: author.authorID or author.ID?  This example is trivial so doesn't demonstrate the principle very well, but make up a table name with 30 characters including some abbreviated words and it will become more obvious. Also, if all PKs are ID, the anything in another table that reads like authorID is a FK. ",1521027623.0
r3pr0b8,"no, they don't get re-used

the numbers will go up to 32767 and then fail (double that if you declared UNSIGNED)",1520951566.0
msiekkinen,"It is very possible you will hit maxint on an autoincrement id.  When this happens future inserts will fail.

If you have a table that's going to have less than 32k entries I can't imagine a situation where you'd need to worry about space or memory even using a bigint (unless it's some really constrained embedded device)
",1520954996.0
jericon,"If you hit the max (which is I believe 32464 for a smallint), you will basically not be able to write to the table.

Auto Increment values are designed to ALWAYS increase.  So if you delete rows, if you manually insert a row with a higher value and different options in your config (such as auto_increment_increment, which indicates how many id's to add to the current value to generate the new value) will affect this.

It will never fill in old or removed values.",1520963473.0
HalexGSd,"select LEFT(columnname, 2) FROM ...

https://dev.mysql.com/doc/refman/5.7/en/string-functions.html#function_left",1520952914.0
jericon,"This post has been removed.

Reason: Spam/Self Promotion.

All posts from this account are self promotion.  There is no contributions to the communities being posted to.  

Please see the [reddit Self Promotion Policy](https://www.reddit.com/wiki/selfpromotion) for more details, and please participate in the community before posting links to your own content.",1520974473.0
mischiefunmanagable,"make sure you're using GTIDs, and do NOT use ALL of the available cores like the article says, just for your own sanity later, many, maybe even most, but never all

that said it slave_parallel also very workload dependent when it comes to performance, if you have a fairly even spread of transactions across MANY different tables you'll probably see an improvement, but you are just as likely NOT going to see an improvement, it will not make huge schema changes happen faster and it is possible that you may still bottleneck on that DDL query

you would probably be better off fixing how you do the DDL changes in the first place, a schema change shouldn't be causing massive replication issues when tools like pt-online-schema-change exist",1520893145.0
jericon,"That error is VERY generic.  Do you have any logs or more information to go on?  

That's like going to a mechanic and saying ""My car isn't working right"" but giving no more information.",1520870950.0
lindacupple,union all worked as well,1520856929.0
eta99,"Hey friend,

It must be grouping by because of the sum statement. Try having not only your uid but also the primary key when you're selecting within your nested query. 

Goodluck",1520855647.0
jericon,"I'm not really sure why you are doing it this way.  Can you give some more insight as to why you are using a sum and union?

Based on what you've asked, the following should do the trick.

    SELECT count(DISTINCT(uid)) FROM table where type = ""T""; ",1520867318.0
Laurielounge,"You're thinking about this the wrong way around. You don't create an index and then try and work out how to use it - you define your query needs and then create indexes to suit. 

In this case, you've added an index based on two fields. So, this would be useful to you when you're querying by either someone's name or someone's name plus the office they're in, along the lines of

    WHERE name = 'John Smith' and Office = 'Brooklyn'

or just

    WHERE name = 'John Smith'

The index as described, however, is not particularly useful if you just wanted 

    WHERE office = 'Brooklyn'

because there is no index based on 'office', only one base on name + office. You could think of this as being like an index based on the concatenation of name and office, along the lines of 'John SmithBrooklyn'. To get all of the John Smith records, the table is sorted by the index and all of the John Smith records are together. But the Brooklyn records are not - you'd have to go right through the entire table to ensure you'd gotten all of the Brooklyn records.

To see how this would work, you really need some more tables (like, say salary table and office table) and join those to this table.

Gotta start somewhere. Good luck

---=L",1520824971.0
zero_iq,"[Use the index, Luke!](https://use-the-index-luke.com)",1520842885.0
AllenJB83,"Suggested reading:

* [Rick James' site](http://mysql.rjweb.org/) contains some useful articles on indexes, including a ""cookbook"" for setting up multi-column indexes
* [High Performance MySQL](http://www.highperfmysql.com/) is a good grounding in MySQL performance, including indexes. There's a whole chapter on indexes, including how to use cardinality and selectivity to help you select / set up indexes.
",1520844656.0
jericon,"It looks like you have been asking a lot of questions that appear to be homework or class related.  

While we are here to help, please do some googling before posting.  In addition, when you do post, please post what you have tried and specifically where you are getting stuck so that we can point you in the right direction.  We won't do all the work for you.",1520867747.0
mudclub,"You start by googling something like ""mysql create user"" or ""mysql grant"" or ""mysql privileges"".  If the results don't do it for you, add ""example"" or ""tutorial"" to your search.",1520796949.0
,[removed],1520796295.0
wampey,https://dev.mysql.com/doc/refman/5.7/en/adding-users.html,1520809863.0
bigbozz,"I'd check your config file over very carefully for any typos/syntax errors, especially around the area you were changing.

Perhaps restore a version of the config file prior to when the changes were made.

Good luck!",1520747544.0
ernstae,"I've always been curious about this default setting.  Generally, I've tuned the environments I've worked in to set the default between 1 and 3 after performance tuning and benchmarking.  

Why spend the time coming up with query plans instead of actually running the queries? Generally, the first few are the most efficient (and you can cheat by forcing indexes, etc)",1520826205.0
trashpantaloons,"Can you use moment.js to convert it? In straight MySQL you might be able to do something like select NOW() - col  as days_left?


You need to be careful - might need to add a 1 onto your int value cause you basically are going all way to the next day in your col",1520674306.0
CrudBert,"Update payment p, users u
Set u.has_payed = ""true""
Where u.id = p.id 
And u.username = ""sweet"";",1520648556.0
mudclub,"How are you trying to connect?

Is the database running?

Is the database listening?",1520645301.0
jericon,"Your post has been removed.  

Reason: Spam/Self Promotion

Please see Reddit's Self Promotion Policy: https://www.reddit.com/wiki/selfpromotion

You have not contributed to this, or any other community in any meaningful way.",1520641906.0
AllenJB83,"While the size of the database is a factor, MySQL also generally needs its configuration tuning based on the server spec. and the make-up of the database tables (all using InnoDB, or a mix of InnoDB and MyISAM? Other engines involved?)

Read up on mysql tuning - there's a number of tools available, such as [MySQL Tuner](https://github.com/major/MySQLTuner-perl) and [Percona's configuration wizard](https://tools.percona.com/), that can help recommend some basic settings values to get you started.

See also: https://www.percona.com/blog/2016/10/12/mysql-5-7-performance-tuning-immediately-after-installation/

(MySQL 8 also improves this, when running MySQL on its own server, with the innodb_dedicated_server option, which basically does tell MySQL it can take all the resources available)

If you want to ""go deep"" on MySQL performance, a good starting point is the [High Performance MySQL book](http://www.highperfmysql.com/)",1520635215.0
mudclub,"It uses as much memory as it needs to cache relevant parts of the data, queries, etc.  For a small dataset, that's a small chunk of RAM.  Why would it grab more RAM than it needs?",1520631956.0
msiekkinen,"Can you provide what you have tried, also confirm what mysqld version you're ruining, and finally the error message you get

Edit: and your table schema",1520547004.0
r3pr0b8,"any key (one or more columns) which uniquely defines each row is a candidate key

one candidate key is chosen to be the primary key, and the others are then alternate keys

a secondary key is any key used to access rows, and may not necessarily be unique",1520531425.0
ScottWarner92,"Candidate keys are those keys which fulfil the requirements of primary key (not null & have unique records) and so are the candidates for the same.
There can be more than one candidate keys in a table out of which one will be primary key and rest of the candidate keys will be secondary or alternate keys. Secondary and alternate keys are same.

For example: In an employee table there are multiple fields including empno , SSN . Both are not null and unique. Hence both are candidate keys. Empno is set as primary key, so SSN will be secondary or alternate key.",1522418695.0
latham338833,Thanks!,1526058047.0
pinkdinosauronbowtie,"When you take a mysqldump it adds drop table if exists into the sql.
It's self explanatory and will permanently delete whatever is already there.

Or you can drop the schema, recreate it with the same encoding, and then source it from the file or pipe directly from the mysqldump into the database.",1520525015.0
msiekkinen,"Your insert could include a

    drop database pr_db_1;
    create database pr_db_1;
    use database prd_db_1;

However, in case it's not obvious, this completely deletes the database and ALL tables in it (there will be no prompt).  I'm guessing you would never want to do this outside of a development environment.  

Also, after you drop, there is no way to recover.   So if by oopsy there was something you wanted not present in your import you're going to be SOL unless you had another independent backup that did contain it.

Edit:  mysqldump has --add-drop-database  and --create-db options for you to add those to the export
",1520524473.0
msiekkinen,"https://dev.mysql.com/downloads/workbench/  It's not CLI but if you're using JetBrains it sounds like you'd also be fine with a GUI. 

I'm not sure which JetBrains tool you're using but many, if not all seem to have a mysql plugin, which can be nice if you're already using that for other things.

Sorry for not answering your real question; I don't know of any other CLI alternatives to the standard mysql client",1520526165.0
mudclub,Look into mysql replication.,1520373854.0
IUseRhetoric,"Option 1:  Set up a Galera cluster for high availability, use innobackupex for backups.

Option 2:  [Use the appropriate Azure offering](https://azure.microsoft.com/en-us/services/mysql/).",1520374004.0
SomeGuyNamedPaul,If you need easy and turnkey then you might want to look into something like AWS Aurora.  You're otherwise looking to get into the deep end.,1520377087.0
de_argh,"MySQL MMM is another viable option.  It's simple to manage as it's just replication, but you get HA with automatic failover of read and write roles.",1520377315.0
secsaba,I highly recommend [Continuent Cluster](https://www.continuent.com) solution. It is a MySQL HA & DR solution.,1520403343.0
TheSqlAdmin,"I recommend that use percona xtradb cluster. Use 3 nodes and put a load balancer on top of it.

We have implemented the same setup in Azure and it's really helping one of our customers who is having huge number of traffic. ",1520412385.0
techiesaravana,"You could use Mysql 5.7 InnoDB cluster DB
https://funblockchain.com/index.php/2017/12/23/mysql-innodb-cluster-setup-on-centos-7-x64/",1520425603.0
kaydub88,If you're in Azure use whatever they're offering as a managed MySQL database. It might look expensive but it's something you don't have to manage. You will likely save money though it will mainly come from reducing the cost of setting up or maintaining the instances.,1520449428.0
feirnt,"Read up on 'data normalization'. This refers to a logical way to organize data that has relationships (such as one-to-many). The exercise you will need to go though first is to work out the entities you want to model, what properties they have, and how they relate (or do not) to each other. This will reveal a natural way to implement the model in a database,  which will likely consist of tables, relationships (foreign keys). More advanced concepts like constraints (think: business rules implemented in the database... you can't have a review of a movie without first having a movie, for example) and triggers (automate doing something when something else happens) may resonate with you as well. 

Is this enough to get started with?",1520379018.0
jumjhgf,"You should be able to do something like this:

    SELECT FLOOR(amount / 10), COUNT(*) FROM table GROUP BY FLOOR(amount / 10);

Change `10` to be the grouping you want. This is probably not very efficient, but works fine on the small tables I tested it on",1520280461.0
jeremycole,"Alternately to /u/jumjhgf's fine solution:

```SELECT amount - (amount % 10) as amount_group, COUNT(*) as c FROM table GROUP BY amount_group```",1520282503.0
r3pr0b8,"    SELECT COUNT(*) AS stock
         , CASE WHEN instock < 0               THEN ""out of stock'
                WHEN instock BETWEEN  1 AND 10 THEN "" 1 - 10'
                WHEN instock BETWEEN 11 AND 20 THEN ""11 - 20'
                WHEN instock BETWEEN 21 AND 30 THEN ""21 - 30' 
                                               ELSE ""lotsa stock""
            END as stock_level
      FROM inventory
    GROUP
        BY stock_level                                                     ",1520280705.0
payphone,You probably would want to have the countries be a table within the same database as the form responses. Then you can join the id of the country in the response to the county table. No need to have 2 databases. ,1520304332.0
justjames87,these tables were created in  phpmyadmin by the way ,1520275650.0
razin_the_furious,"Put:

    create database mydb;
    use mydb;
as the first two lines of /path/to/script.sql",1520260685.0
msiekkinen,"FYI, when you're in the mysql client you can run

    mysql> source /path/to/script.sql


",1520262091.0
TestyTestis,Perfect - thank you everyone!,1520264056.0
mischiefunmanagable,"can you give a specific example of queries to create, drop and drop&create?

might check ownership of the files for the tables, might be readable by mysql but not owned by mysql",1520260393.0
mobsterer,"practice imho. 
make something like a movie app and play around with it.",1520197708.0
,"I'm doing this and towards the end. Easy and one topic flows into the next without big hurdles.

https://www.udemy.com/the-ultimate-mysql-bootcamp-go-from-sql-beginner-to-expert/

Free option that is great: https://www.w3schools.com/sql/

There are all types of free books and tutorials online. 

*I do not know the author of that course above and don't think it's necessary to take a course like this to learn, just presented it as an option, because it's taught well, you can ask questions, and you actually build something.",1520198180.0
TestyTestis,"I'm in the same boat. Some modest programming/statistics background, but new to SQL. Currently learning MySQL.

I'm a little over half-way through the w3schools tutorial. It's okay, but the annoying thing is that the northwind database they use is built for MS SQL. There are a handful of people online who have converted a .sql script to import to MySQL, but they all threw errors. One of them I was able to fix by editing a one-line clause. PM me if interested. I can't remember where I found it, but can send you the edited script.

I just decided to take a break from the w3schools tutorial as I feel it could have been done better. Currently I'm reading ""SQL For Mere Mortals,"" which is pretty well-regarded. It focuses on ANSI SQL standards, and from what I've read doesn't really address certain nuances in different SQL versions. Apparently MySQL is notoriously bad about standards compliance, so be prepared to consult the tutorial/documentation on the MySQL website: https://dev.mysql.com/doc/refman/5.7/en/

Also, don't forget about MySQL Workbench. I began on the command line but found it cumbersome to get grips on all the different table layouts. Additionally, if you have a lot of coumns, your output can wrap around and be difficult to read.",1520233537.0
DataVader,"Create the structure of a small management system, like renting cars. When you have a basic structure, check if it fulfills the third normalization. If not, do so. Then check it again if any column or part of a code (bill number or something) can be generated/found by using logic, like do you need the name and age of a customer if you already have the drivers license saved? Then think of how to retrieve data efficiently. Can you do that with SQL alone? Can you make useful statistics? That kind of stuff gives you an insight of what you can do with SQL and where you might need some more knowledge.",1520252542.0
jericon,"This has NOTHING to do with MySQL.  The article you posted has no mention of MySQL in it at all.  Or databases at all, for that matter.",1520164480.0
jericon,"Where are you getting stuck?  What specifically are you having problems with?  

Please give it a try and then come back and indicate the issues you are having. 

Also, keep in mind that this is a mysql community. Not a hibernate community. You might find some better help in a community specific to the type of code you are trying to write. ",1520149859.0
gizram84,"UPDATE user_credits set credits = credits + credits / (select sum(credits) from user_credits) * x;
",1520040064.0
Laurielounge,"    mysqladmin password my_new_fancy_password
...is my normal way to do it.

What may be confusing you is

    Access denied for user 'root'@'localhost'


... doesn't necessarily mean what you think it means. You're reading this as ""my password is wrong"" but it really means root@localhost using your password is not allowed access. Access is provided on a user and hostname combination basis, with the password as the clincher. So while root@localhost may not be allowed access, root@some_other_host may be.

If you examine the contents of mysql.user, the clue may be in tere.

Good luck, you've come to the right place.",1520025259.0
whatalegend89,There is an export option in PHPMyAdmin you can export it as an insert script or CSV etc.,1519823962.0
whatalegend89,That really depends on what you want to use it for. ,1519847033.0
whatalegend89,Different purposes will need different types. Just export it in all the ways it allows. And store it. Feel your over complicating this. ,1519847946.0
TedW,"My first attempt was something like:

    select f_id, id, max(time) as max_time from TestDB group by f_id;

This gives me something that *looks* like what I expected, but id is not from the same row as the timestamp.  I believe this is because group by mashes the rows together and I'm not sure how to preserve id.

My plan was to get the min_time_id and max_time_id, and use those to just query the lat and alt.",1519764917.0
TedW,"I also tried:

    select id, f_id, time from TestDB where time in (select max(time) from TestDB group by f_id);

The results look something like this:

    +------+------+---------------------+
    | id   | f_id | time                |
    +------+------+---------------------+
    |  475 |    2 | 2017-12-25 06:00:51 |
    |  526 |    2 | 2017-12-25 06:00:51 |
    | 1784 |    3 | 2017-12-25 06:00:51 |
    | 3233 |    3 | 2017-12-25 06:25:00 |
    | 3284 |    3 | 2017-12-25 06:00:51 |
    | 4733 |    3 | 2017-12-25 06:25:00 |
    | 7141 |    1 | 2018-01-16 20:46:40 |
    | 7462 |    5 | 2018-01-23 17:55:16 |
    +------+------+---------------------+

I see that I'm getting duplicates when the timestamp for f_id=x is the same as the max for f_id=y.  So that's no good.

Obviously, I would like to match on the id instead of time, but I'm not sure how to do that.  I'll read up and hopefully figure that out.",1519765801.0
TedW,"I tried a join but I'm not getting the right result:

    > select a.f_id, a.id, a.time from TestDB a
    join (
        select id, max(time) from TestDB group by f_id
    ) b on b.id = a.id;
    +------+------+---------------------+
    | f_id | id   | time                |
    +------+------+---------------------+
    |    1 | 5542 | 2018-01-16 20:20:01 |
    |    2 |  425 | 2017-12-25 06:00:01 |
    |    3 | 1734 | 2017-12-25 06:00:01 |   <-- not correct time
    |    5 | 7142 | 2018-01-21 11:48:15 |
    +------+------+---------------------+
    4 rows in set (0.08 sec)
    
    > select id, max(time) from TestDB where f_id=3;
    +------+---------------------+
    | id   | max(time)           |
    +------+---------------------+
    | 1734 | 2017-12-25 06:25:00 |
    +------+---------------------+

So this did NOT return the correct id for the maximum value.  But it might be on the right track.",1519766452.0
razin_the_furious,Are you connecting by hostname or IP?,1519677777.0
NotAnExpertWitness,"Try adding -vA to the mysql connect command.  It will skip the re-hash and be a little more verbose.

Also, check the size of your .mysql_history file on the client.",1519744437.0
shobbsy,"Might be worth disabling any firewall/security software, I had something similar on windows with mcafee

Edit:for testing only of course",1519760416.0
r3pr0b8,"> But how do I involved multiple tick boxes?

client ----< client options >--- options

many-to-many relationship",1519638231.0
cknu,"I think you first need to read a little bit about database design. Not too much, but to get an idea of modeling and data structures.
Read about database design fundamentals, this is the first result in google and seems nice: https://www.sqa.org.uk/e-learning/SiteHomeCD/page_27.htm",1519663362.0
mischiefunmanagable,https://en.wikipedia.org/wiki/Associative_entity is what you're looking for,1519670849.0
wetmarble,"Add two fields to your table, previous_task_id and next_task_id",1519521095.0
Hoysurdady,"You could also have an increment id column. If you are adding a step between two existing steps, say 2 and 3, you could add 1 to the id of all rows with id greater than or equal to 3. Then insert your new row with id of three. Same process would work for removing steps. Just subtract from id column instead. ",1519579261.0
CrudBert,"Leave that alone. It's part of mysql 5.6 and beyond. It's dba views for tuning, etc. You need that, don't get rid of it. Some of those views aren't really views, they are database calls that look like views but really aren't.  Think of it like things you find in /sys, those are kernel parameters that implement a fake file system implementation. The sys schema  can be thought of in a similar way. Just leave it alone and ignore it unless you want to learn some tuning.",1519416853.0
lukaseder,"This is akin to deleting `C:\Windows` because you didn't ""opt in"" to this folder on your operating system :-)",1519460813.0
Bitter_Bridge,"1. Yes, certainly
2. Most definitely
3. You need two tables. One will act as the test header table which will contain the information about the test you listed. The second table will hold the seconds and distance columns along with a testId column which will act as a foreign key to the test header table. 

Message me if you need more info than that. ",1519472080.0
jericon,"You would be better off asking this in /r/php.  This is a PHP issue, not a mysql issue.",1519411901.0
dunnybloke,You spelled DB_PASSWORD wrong,1519429129.0
Bitter_Bridge,"Is MySQL on the same server as PHP? If not, the @localhost portion of the user is going to screw you. ",1519472705.0
Mr_Orbital_Laser,"You could write a stored procedure that runs a SHOW TABLES and then loops over the list of tables. but stored procedures are not really intro material. Also this isn’t really something you would normally do. Your tables normally would all have different structures and layouts.

A single query is expected to have the same columns across all its rows and having one query that spits out all data in all rows in all tables doesn’t really fit that.",1519364847.0
DataVader,"SELECT *
FROM INFORMATION_SCHEMA.TABLES;",1519398658.0
knifebork,"Perhaps you're looking for the mysqldump command. It isn't a query, but it can get you the contents of all the tables in a database. It's usually used to make a backup copy of your data, not make any kind of useful report. ",1519395969.0
MyDataBes,sorry to say but its not possible.,1519355389.0
jericon,Even though you are writing to a mysql database.  You would probably receive better responses if you post in a subreddit more closely aligned with your programming expertise.  This will be solved through code that writes to MySQL and not directly within it.,1519373504.0
Bit_Blitter,"This is more of a web dev question than a MySQL one. Paypal captures all this information so you'll need to hook into their API to get this.

This looks like it would help:
https://developer.paypal.com/docs/classic/paypal-payments-standard/integration-guide/paymentdatatransfer/

The INSERT statement is the simplest part of this process",1519325629.0
longwayahead,"You haven’t provided us with the language you are capable of writing in or the steps you’ve tried so far: this is not a MySQL problem, it’s a PHP/Node/Ruby/etc one. I think you are looking for a panacea, but it doesn’t work like that!",1519335412.0
jericon,"Your post has been removed: Self promotion

All of the posts in your history are from the same domain.  You have also not contributed to this subreddit in any other way.  Please review reddit's [Self Promotion Policy](https://www.reddit.com/wiki/selfpromotion).",1519248524.0
magikaas,More likely to get people to pay to remove that limit.,1519210410.0
bowersbros,"The data is valuable enough to them to store it. Also it’s only going to be a pivot table that’s stored

Something like user_1 direction and user_2 which won’t be that much data per swipe ",1519203935.0
SomeGuyNamedPaul,"Swipes are one of their most valuable forms of data.  It's a user telling you this preferences when your whole job is finding something your users want before they know exactly what they want.

Could you imagine Facebook limiting the number of things you can Like?  This is just a way to monetize the site, similar to how free to play games function.  ""Oh, you like doing this?  Pay to keep doing it.""",1519217584.0
jericon,"This sounds like homework.  Why don't you give it a try, come back and let us know what you have and where you're stuck and then we would be happy to help.",1519248702.0
msiekkinen,Is this a homework question?  Do you have an actual preliminary schema you're wanting critiqued?,1519147156.0
Bitter_Bridge,"This is not a definitive answer, this is just to give you an idea of how it should be structured. 

**Users table:**
id (auto incrementing PK) | 
username (userA, userB, etc.)

**Message Header Table:**
id | 
date_started (timestamp set on insert) | 
started_by (userId foreign key)

**Message Details Table:**
id | 
userId (foreign key to users table) | 
messageId (foreign key to message header table) | 
amount (DECIMAL) | 
status (accepted or rejected) | 
timestamp

**File table:**
id | 
messageDetailId (foreign key to message details table) | 
file",1519163129.0
DataVader,"Your question would receive more love, if you could explain what you actually want. 

As I understand it, you have multiple teams, want every possible combination of two teams and all combinations are used exactly once. In addition every team may only play once a day. So with 4 teams you have 2 games for 3 days, with 6 teams you would have 3 games for 5 days and so on. Am I correct?

If so, you can calculate:
gamesTotal = (teams-1)*(teams/2);
gamesPerDay = teams / 2;
days = gamesTotal / gamesPerDay;

If you can't figure out a mathematical function, you could join your teams with the steps of days and insert them into a temporary table with unique constrains. If you are not bound to a specific database, use MariaDBs window functions and CTE to make a recursive loop.",1519320503.0
MercurialNerd,"Get the [v5.0 book for the old exams](https://www.amazon.co.uk/MySQL-Certification-Study-Guide-Mysql/dp/0672328127/ref=sr_1_2?ie=UTF8&qid=1497890899&sr=8-2&keywords=mysql+certification+study+guide), and use DBA chapters (particularly around Transactions and Isolation Levels which were added to the Developer exam) and practice questions as a starting point, but research the differences between v5.0 and v5.6, there are plenty of resources online. 

This is what I had to do for the 1Z0-882 Developer exam and I managed to get myself and 11 students certified.

PM your email address and I'll send you a study outline which matches the 5.6 exam topics to the 5.0 book (and other resources).",1519082220.0
paranoidelephpant,"Support for SSL/TLS in MySQL is completely independent of support for the same in your web server. Don't expect it to work under shared hosting. 

The error for the SSH connection indicates that the connection is fine but the MySQL server rejected the credentials. Keep in mind that in MySQL authentication is also host-based, so a user connecting from localhost (as would be happening with an SSH tunnel) is not the same as the same username connecting from your home computer.",1519070648.0
r3pr0b8,"can a customer have zero installations?

can a given type of equipment (e.g. fish tank) be used in more than one installation?

can an ""installation staff"" really belong to zero staff?",1519051824.0
NotTooDeep,"Not a bad first iteration. Well done.

This is a classic work order modeling problem, which is very similar to invoicing. Some suggestions: Break Installation into two tables, installation_hdr and installation_dtl. This is basic normalization to prevent copying redundant data for every row of the invoice/work order. Examples of data in the header: various dates, customer_id, total_price, tax. This reduces the size of the data set, which makes things faster and easier to read/maintain.

So now you have installation_hdr with customer being a lookup in support of that table. An installation_hdr has one or more installation_dtl's. Your equipment table is a lookup to populate each line in the dtl table. Equipment will have a one-to-many relationship with installation_dtl. This is the opposite of the relationship you modeled.

Now here's some fun stuff to think about. You can't really track the physical installation on this invoice order. You can track the price, the order date, the customer, all of the items that are ordered, but not the staff who will perform the work OR the date they will perform it. There could be complications, which could generate sub-workorders, each with their own schedules, parts, and skill sets. This means you need a work_order table.  And, this actually means you will need two of these tables; work_order_hdr and work_order_dtl. Same reasons as the invoice. Similar relationships with the many_to_many installation_staff bridge table. 

BUT, you're missing scheduling! And you're missing staff_skills. Some equipment will require specific skill sets. People with those skill sets will have commitments to other installations, which affects their availability. 

To carry this a bit further, some equipment will require larger delivery vehicles. These vehicles will have commitments to other installations, which affects their availability. 

You will know you are done when your model can answer all of these questions with simple queries that will show up as a calendar on a UI to the sales person, who can ask the customer, face to face, ""Will this Friday afternoon work for you?"" Friday is only possible if the equipment is in stock or will be by a known date, if the delivery vehicle will be available after that date, and if the skill set for that vehicle (might require a commercial license) is available, and if the skill set for the equipment is available.

Quick summary: header tables and detail tables are a common model. They provide normalization, which means one piece of data exists in one and only one place (huge data maintenance benefit). This pattern solves many modeling problems. Some modeling synonyms are master-detail and parent-child. Tables, however, are always named for the objects they represent; i.e. invoice-invoice-dtl, work_order-work_order_dtl, assembly-subassembly.

Have fun!",1519064446.0
msiekkinen,"are you getting access denied entries in your mysql error log?  That would confirm at least your client is making a connection.  If you're not even seeing that it could be some sort of networking firewall issue.  

",1518993489.0
NotTooDeep,"There are a ton of data modeling books out there and I've probably read too many of them, but this one teaches you how to think through a logical model.

https://www.amazon.com/Data-Model-Patterns-David-Hay/dp/0932633749

To his credit, he gives full warning that you may not want to implement a physical data model exactly like a logical model. My understanding: a logical model captures the lifts of all the things the business wants to keep track of, organized by their relationships only. A physical model starts at the logical model and sometimes modifies things for data flow management and access management and authentication/authorization and auditing requirements. 

Mr. Hay's writing style uses simple, declarative statements, similar to what you would use in reading a one-to-many relationship on a diagram (crow's foot) in both directions.

The book also has an effective progression from person and organization models all the way to modeling discrete part manufacturing vs. batch processes used in the chemical industry. It will broaden your understanding of both how modeling works and its limitations.",1518966607.0
jericon,"Please post and ask a question here directly, instead of just a vague link to StackOverflow.  Feel free to include a link to the question there, but just posting the link here is not helpful without any context.",1519131048.0
enchufadoo,"> because I already have a newer version installed.

Which version do you have installed? which version does workbench need? ",1518880003.0
msiekkinen,It's not a dynamic setting meaning it needs to be set on during start up,1518890792.0
cknu,"You don't need to stop the master to setup a new slave. You need to restart the slave only.

https://www.percona.com/doc/percona-xtrabackup/LATEST/howtos/recipes_ibkx_gtid.html
",1519148662.0
cknu,Nice. You know you can export a result set in JSON format using MySQL Workbench?,1519150728.0
mudclub,"Create a database somewhere else and use that for testing, schema development, etc. ",1518656588.0
mihirjpatel,"So if I understand correctly, you want to work on website on a work computer, which is not related to your job?  I assume you are not allowed to install anything on your machine due to restrictions?  Do you have any SQL based engines installed already?  If you have Access, use that.  MySQL or postgresql is recommended.  If you go with mssql and your website needs to be put for production use, you won't be able to afford it off the bat.  You can always migrate data if needed (though this can become a separate project if it gets complex).  If you do not have anything except office, just use Excel or csv.  It'll be easier to migrate the data after to a real RDBMS.  Make sure you use GitHub for the data for testing purpose and migrate after.  The problem you may face is having to change the logic you write in retrieving and storing data.  Hope this helps...",1518702243.0
r0ck0,"I don't know anything about ""mysql events"".

And I haven't put too much thought into it, but it feels like you should be able to do something like this with a single query.

The first thing I would do is create an SQL VIEW that already pre-joins the relevant tables together.  Then you should be able to issue one UPDATE command sourcing everything from that view.  This makes things much easier to test and debug.

Regarding the 1-8 numbers, you can probably get it all done in a single command using some more complex WHERE conditions either in the CREATE VIEW or UPDATE command you use.  Probably best done in the view, and only have the view return results that should be updated, then you don't need any conditions in the UPDATE command at all.  Again, makes it very easy to test and debug before actually editing your data.  Bugs in complex UPDATE commands are very hard to notice and then fix.

Not really necessary to go to the effort to figure this out in a single command for something fairly simple like this.  But it will help you develop your SQL skills in general.  So if you have the time, it's a good opportunity to learn some more advanced SQL usages that will help you in the future in general.  I've spent quite a lot of time on developing these SQL skills that I didn't really ""need"" in recent years, and it's made me a much better programmer, and I do way more stuff directly in the DB now that I used to do in app code.

Failing that, if you're in a rush you could always just create a [procedure](https://dev.mysql.com/doc/refman/5.7/en/create-procedure.html) (aka function) that contains all 8 commands.  Then just call that procedure from the event.  [Looks like you could also basically do a FOR type loop like this](https://dev.mysql.com/doc/refman/5.7/en/loop.html) - but I highly recommend having a crack at the VIEW + single UPDATE command approach mentioned above over this if you can.",1518744918.0
llameadrpc1,"I would write queries by hand, possibly in MySQL Client. ",1518622408.0
cknu,"Old but Gold:

1st: https://www.percona.com/blog/2012/06/04/thread_concurrency-doesnt-do-what-you-expect/

2nd: https://www.percona.com/blog/2014/01/28/10-mysql-performance-tuning-settings-after-installation/

Remember this: Innodb Buffer Pool is your most important setting.
",1519151117.0
davvblack,"There is no advantage to moving this unparsed data into MYSQL, you should do it in python on the way in, and generate an actual schema with real columns for columns and rows for rows.  In python you can look for the types of the entire first row to be text and therefore not import them (or use them to name the imported table's columns).",1518541515.0
justcollectingdata,The keyword you are looking for is replication. If the MySQL versions are compatible you can replicate from one to another. Keep in mind you will only be able to write to one node at a time using a standard setup.,1518503837.0
davvblack,any particular reason you want master/master?  Master/slave replication is simpler and less error-prone. always write to one and it gets copied to the other.  Also what's a mibox?,1518504381.0
justintxdave,The entity relationship map from MySQL Workbench provides a pretty dramatic visual overview of schemas,1518451177.0
jallits,"At a low level I add comments describing my schema. However, I build my schema migrations in my application code. It is here that I leave detailed comments. I do this because it allows me to likely stay in one  IDE as I do my work.",1518438055.0
kaydub88,I use a migration tool like flyway or liquibase.,1518440959.0
dsleinen,I like LucidChart ER relational database diagram. It's free and really easy to use.,1518571579.0
kaydub88,"grant <permissions, all> on <database>.<table(s)> to <user>@<cidr block of class c subnet>

Examples:

    grant all on test_db.* to 'myuser'@'192.168.1.0/24';

Grant all rights to myuser from the 192.168.1.0/24 subnet to all in test_db.

    grant all on *.* to 'myuser'@'%';

Grant all rights to all databases to myuser from any host.",1518441265.0
jericon,This post has been removed because: It is not related to MySQL at all.  Please try /r/MSSQL ,1519130975.0
r3pr0b8,">  I'd much rather do something like this:

you mean, like this? 

    UPDATE schedule 
       SET data = CURDATE();
    
    UPDATE schedule 
       SET data = data + INTERVAL day DAY
     WHERE day BETWEEN 1 AND numEnemies",1518418325.0
,"https://dev.mysql.com/doc/refman/5.7/en/loop.html

https://dev.mysql.com/doc/refman/5.7/en/repeat.html

Make it a stored procedure and the event with 

    CREATE EVENT myevent ON SCHEDULE EVERY 5 SECOND DO 
        CALL storedproc();",1518400499.0
MadPhoenix,"> how do I make loops on MySQL

You can only make loops in a [procedure or function](https://www.databasejournal.com/features/mysql/mysql-cursors-and-loops.html).

No idea how PHPMyAdmin's event scheduler interface works, but you should be able to just create events in the SQL Editor by creating CREATE EVENT statements.  Docs are online.",1518400771.0
Elementally,What query editor are you using? ,1518399680.0
NotAnExpertWitness,is data a date column?,1518375822.0
CrudBert,you've got two semi colons in your concatenated string. One at the end of the first select and another at the end where you concatenated that string plus the interval + date part. That's two semi colons in one statement.,1518376795.0
Bitter_Bridge,You need to define a delimiter before the create command. ,1518385180.0
dsleinen,"Maybe try installing XAMPP, MySQL is built into it.",1518370803.0
ryosen,Did you run the install as an administrative user? ,1518408666.0
,"Try [MariaDB 10.2](https://downloads.mariadb.org/), it's a fork of MySQL and I don't think you'd notice much of a difference if any",1518371601.0
r3pr0b8,"is your table name really `table`?  that's a reserved word and probably should be escaped

maybe test all your sql outside of php before parameterizing it",1518373805.0
r3pr0b8,"sorry, not familiar with the ""does not work"" error message",1518368188.0
kellenkyros,Are you getting any error? Or else which conditional echo statement is getting printed?,1518369268.0
msiekkinen,">  $id=$row['ID'];

Is the column really called ""ID"" (upper case)?  Try either stepping through or adding debug logging to show the full query being generated for the update.   

Now that I've answered the question, the obligatory: this is full of sql injection problems, please learn to use parameter binding.  ",1518374194.0
kellenkyros,echo $row value and see what you are getting and proceed from there,1518384043.0
hoskyB,"Guys, the problem of that code it is in the update mysql call. Just changed some words for privacy, so I need only to know why my update call it does not work in my database, why can't replace data.",1518456206.0
hoskyB,BTW thanks for all the answers :),1518456238.0
razin_the_furious,"More often than not when a query is weirdly bad, there is a join that is wrong somewhere. Something like honing a table on itself by accident",1518364487.0
SomeGuyNamedPaul,"Try slinging an analyze table for all the tables in the query.  Otherwise reduce the query into components and try that.

Usually when explain goes horribly wrong it's because something really screwy is happening with your schema like you have integers stored as text, or you're filtering on the output of a function.",1518367551.0
,"It was false that it worked on devs pc... instead, it worked sometimes, seemingly at random.

Btw, solved by lowering the optimizer search depth.   Beware the dangerous default...",1520707847.0
NotTooDeep,"All good ideas and easy to test on a small data set. You probably need to run a series of tests to see what works for your use case.

What's not easy is to offer a recommendation. I don't see enough business logic and technical detail to understand the tradeoffs. So I'll ramble a bit.

If you pull in two data sets, you can loop through one based on the values in the other. This is the classic nested loop processing found in pretty much all databases. 

Consider what data you wish to end up with. Is it something you could pass into the stored procedure as a record? You could loop over the final data set and call the proc. once for each record.

Can cursor_a and cursor_b be combined into one cursor? This is often very effective because SQL is a set based language. It's more of a challenge to write complex cursors, but this can yield performance gains. On the other hand, everyone who follows you will take more time to understand that big ugly cursor that a simple nested loop of two cursors.

You're asking good questions. Sometimes you simply have to draw a state change diagram of what the data looks like at the beginning and what it needs to look like at the end. This means physical rows with the values of the columns spelled out. Whiteboards are you friend. If you cannot get from the first step directly to the last step, then and only then do you insert intermediate rows in the state change diagram. There are exceptions, but fewer steps is generally better. 

Thoughts?",1518307754.0
jericon,"I’m not sure why you would want to disable auto commit for an import that is large. 

However you could use a combination of echo, cat and pipes to prepend the set command. ",1518284378.0
SaltineAmerican_1970,`sed` can add a line to the beginning of the file without loading it all into memory. ,1518285158.0
NotFromReddit,"Most frameworks have a concept of migrations.

In Laravel you write the migrations by hand. Then you run the migration on the database instead of adding columns and data manually. There is migration table that keeps track of which migrations were run and which not.

In Symfony, you define your database structure in your models. Then you can run a command that generate migrations based on the difference between the database structure and what's defined in your models.

If you're using PHP without a framework, or some framework without native migrations, then you can maybe try [Phinx](https://phinx.org/).

If if you're using Rails, Django, Flasks, Express or whatever, I'm sure they all have a concept of migrations too.",1518208024.0
Tiquortoo,Structure synchronization tool in Navicat is awesome. If you need to do this for data you're likely doing something really wrong.,1518210318.0
justintxdave,Look at mysqldbdiff from the MySQL Utilities -- https://dev.mysql.com/doc/mysql-utilities/1.6/en/mysqldiff.html if you really just want just the object diff functionality.   or mysqldbcompare  -- https://dev.mysql.com/doc/mysql-utilities/1.6/en/mysqldbcompare.html -- for object and data,1518230278.0
r3pr0b8,"    SELECT threadwith
         , COUNT(*) AS messagecount
      FROM ( SELECT recipient AS threadwith
               FROM conversations
              WHERE sender = 'abc'
             UNION ALL
             SELECT sender
               FROM conversations
              WHERE recipient = 'abc'
           ) AS q
    GROUP
        BY threadwith           ",1518137382.0
ScottWarner92,"If I get this correct you are trying to count a message as read if abc is a recipient & unread if abc is a sender. In this case you can populate unread & read messages like this. Don’t use sum in your subquery as this may cause less count as total in your main query.

    SELECT thread, COUNT(*) AS total, SUM(msgRead) AS msgRead 
    , sum(msgUnread) AS unreadCount
    FROM (
    SELECT recipient AS thread, 0 AS msgRead, -1 AS msgUnread
    FROM tblMsgs 
    WHERE sender LIKE '%abc%'

    UNION ALL

    SELECT sender AS thread, 1 AS msgRead, 0 AS msgUnread
    FROM tblMsgs
    WHERE recipient LIKE '%abc%'
    ) AS q
    GROUP BY thread ",1522502082.0
r3pr0b8,you don't ~have~ to but it'll be better in the long run if you do,1518083721.0
cknu,"You can control relationship between tables on you application, but i do recommend using FK to enforce data consistency.

For FK to work in MySQL you need to use a transactional storage engine like InnoDB (the standard in last versions). Check your tables are created using this storage engine and not MYISAM. MYISAM do not support FK, but it won't throw an error if you try to add one.

To add a FK on a table check the official MySQL documentation: https://dev.mysql.com/doc/refman/5.7/en/create-table-foreign-keys.html",1518096505.0
Bitter_Bridge,It’s best practice. Is there a reason you don’t want to?,1518097711.0
lskdn054,I want to find a select statement that will grab the bikes distinctly by name and add up quantity of the bikes to the corresponding names.,1518065071.0
Mikister2012,"[here's a pic](https://imgur.com/IPHkkbc)

A little description:

* It can import CSV files where columns are different datatype each
* It can take the first row of CSV as column names (a header row)
* It can also enumerate all the values, good if you you want your first field to be something like id",1518056970.0
Fiskepudding,"https://en.m.wikipedia.org/wiki/Aria_(storage_engine)
First result on Google.

Don't use it. It's still in development and used internally in MariaDB. It doesn't support transactions. ",1518016611.0
xproofx,The SQL instance is running on the same system as the file?,1518003829.0
xproofx,Can you test it by pointing to an invalid resource and see if it returns an error?,1518018864.0
xproofx,What is the datatype of the pic column?,1518021688.0
xproofx,Send me your create table script as well.,1518030751.0
xproofx,"Try moving the file into a different directory that has full permissions, off the root so something like C:/Files/test_pic.jpg



",1518100376.0
Bitter_Bridge,What collation are you using on the suspect tables/columns?,1517960506.0
,[deleted],1517965447.0
dsleinen,"I've done something similar to this with the `MySQLdb` and `CSV` packages on Python. If you adjust the connection details and table names, this script should work for you.


    import MySQLdb, csv

    # Establish MySQL connection
    database = MySQLdb.connect(host = ""localhost"", user = ""root"", passwd = ""1234"", db = ""mydb"", port=3306)

    # Get the cursor, which is used to traverse the database
    cursor = database.cursor()

    # Select fields from MySQL table, store as Python variable
    cursor.execute(""SELECT field1, field2 FROM tabl1"")
    data = cursor.fetchall()
    
    # Define OutFile name
    file_name = 'csvOutFile.csv'

    # Open CSV file to write to
    with open(file_name, 'w') as OutFile:
        
        # Create variable to write with
        MyWriter = csv.writer(OutFile, lineterminator = '\n')
        
        # Iterate through MySQL data and write it to CSV
        for k in range(len(data)):
            
            Row  = data[k]
            MyWriter.writerow(Row)
    
    # Close CSV file
    OutFile.close()

    # Close the cursor
    cursor.close()

    # Close db connection
    database.close()
 
This will produce a file called csvOutFile.csv with your MySQL row data. If you just want to access the data in Python, in this script, it is stored as the variable `data`.
",1517966433.0
gizram84,"The problems isn't with the ""LINES TERMINATED BY '\n'"" part.  That's completely fine.

The problem is the  ""FIELDS ESCAPED BY '\'"".  That slash is an escape character, so it's escaping the closing single quote mark.

Add a second slash like this:

    FIELDS ESCAPED BY '\\'

That should do it.",1518012187.0
dginz,"In the end I've found out I can actually control the created partitions in this manner:

    CREATE TABLE ...
    PARTITION BY HASH( ... )
    (
        PARTITION p0 DATA DIRECTORY = '/another/directory',
        PARTITION p1 DATA DIRECTORY = '/another/directory', 
        PARTITION p2 DATA DIRECTORY = '/another/directory', 
        PARTITION p3 DATA DIRECTORY = '/another/directory', 
        ...
    );

The last part can be easily generated by any scripting language, say, Python:

    max_partitions = 100
    for i in range(0, max_partitions):
      print(""PARTITION p"" + str(i) + "" DATA DIRECTORY = '/another/directory',"")",1517927067.0
SomeGuyNamedPaul,"I regularly test restore my backups to an alternate environment, that's how I check it for corruption.  Daily, weekly, wherever floats your boat.

Why are you doing hourly incrementals?  It sounds like what you really want are probably daily backups and then retain your binary logs to do point in time recovery.  I could see weekly full backups and daily incrementals for really busy seems, but hourly seems like overkill to me.",1517915484.0
davvblack,"I can't completely understand your schema, but I think you would be better served by a join table, tenants__properties with tenant_id, property_id, and for reporting perhaps ""start_at"" and ""end_at"" to track the exact interval the relationship was true for. this lets the same tenant attach to multiple properties, and the same property have multiple tenants, either at the same time or serially.  ",1517874868.0
jediqwerty,"I would add a column ""ActiveTenantID"" to your Property table


",1517873333.0
SubstantialRock,"By the sounds of it, you may want to add the boolean column, although you'd need to filter ex-tenants out in a where statement to show only current tenants.

Also assuming this database isn't very large. ",1517874611.0
NotTooDeep,"Practical is as practical does. It's not a useful word in the context of designing a database. The process is not that simple.

Google ""data modeling examples"". You'll see that, even within the constraints of SQL, there are different design types used for different purposes. Start with a basic entity-relationship model, because it's the most common solution. This is where most students of database design need to start.

Then, learn the design language of relationships: one-to-one; one-to-many; many-to-many. Khan academy has some free resources for this.

Then, design a model on a napkin at a coffee shop. Start with the people behind the counter making and serving the drinks and taking money. Watch everything they do and then generalize the parts that are important to remember; i.e. people play different roles in a business setting. This is assuming that people are important to the business to keep track of.  Which leads to this definition:

A data model is an organized list of everything in a business (or process) that's important enough to keep track of.

That's the internal data structure of your ""practical database"".  The system structure is very different. For this, you need to know how high the transaction rates will be, how large the data set will grow over time, how fast the data set will grow, and the nature of the workload, which falls broadly into one of two opposing types: OLTP and OLAP, which is to say, transactional and reporting. 

This is important to know because transactional workloads favor smaller block sizes. Their performance goal is throughput, meaning getting the maximum number of transactions per second committed to the database. Reporting workloads favor larger block sizes. Their performance goal is response time, meaning getting the most data possible returned to the end user as fast as possible. While there are databases in the world that support multiple, concurrent block sizes, MySQL is not one of them. So knowing the type of workload you're designing for is your entry point into tradeoff land.

Feel free to ask me more questions, even very specific ones, in this thread. I'll get back to you as soon as I can, but sometimes this is several hours do to work.",1517845042.0
JackOfAllCode,"Getting there I guess....

SQLException: No output parameters returned by procedure.

Modified the java a bit.

    CallableStatement cs = con.prepareCall(""{? = call     checkLogin    (?, ?)}"");
    cs.registerOutParameter(1, Types.INTEGER);",1517802574.0
kaydub88,"Allow nulls in the field and default to null?

I wouldn't turn off strict mode. ",1517678574.0
nroadwarriorch,paste the error here. ,1517613094.0
,are you using text data type by any chance?.if so change it to varchar.,1517614264.0
CorruptedReddit,"Also ensure you encrypt passwords if your not, hit me up if you need help",1517630102.0
Nk4512,"if you telnet to your local host ip and port 3306 do you get a prompt? also make sure your login creds are 100% correct. you could possibly be blocking certain connections from local/remote hosts (I don't know your layout without looking to be honest)
",1517691986.0
digicow,"MySQL can listen for connections in various ways. One is UNIX sockets, which are only available for local connections (on the same machine). This method does not require ""networking"".

The other is TCP, where a connection is made to a network port. This works both on the machine and remotely. This does require ""networking"".

When connecting locally, your script or application might use either method, it depends on exactly how it's written and configured",1517593559.0
i-reddit-at-work-too,"Without looking at the csv file or the server config, my guess is that you have STRICT sql mode enabled and one of your values for ""Course"" is larger than 20 characters. You either need to disable strict sql mode, which will truncate the course title to match the type. Or you need to make the varchar bigger than 20.",1517441444.0
One_Ders,"Couldn’t you just use

Select * from cdr where dst like ‘%011%’",1517449428.0
jericon,"In the future, please link the question in the description and use a more cohesive title.",1517432099.0
tywkeene,Are you really asking us to do your homework?,1517440147.0
r3pr0b8,"    SELECT m.id
         , m.name
         , MIN(md.date) AS nearest
      FROM movies AS m
    INNER
      JOIN movies_date AS md
        ON md.movie_id = m.id
       AND md.date >= CURRENT_DATE
    GROUP
        BY m.id
         , m.name     ",1517427653.0
AllenJB83,"My guess would be that there's a control character at the start of the ""Fname"" field. You could select HEX(Fname), then look up the number of the first 2 digits (assuming 2 byte charset / UTF-8) in the ASCII table.

The value in MySQLWorkbench probably isn't missing - it's probably either a line return, so the rest of the value isn't shown, or MySQLWorkbench is deciding it can't sensibly display the value.",1517390516.0
kaydub88,This sounds like the guy is trying to get free labor out of you.,1517369147.0
HeyImAlex,"- manually look at the spreadsheets and determine the table structure for your db. this is that part that’s probably the most difficult since it’s a little bit of an art, but you say you’ve done some db work before.
- create tables
- export spreadsheets as csv
- parse using your favorite language’s csv parsing lib
- process the data, cleaning up discrepancies and denormalizing as you go
- insert processed data into db

You can do it! Just stick with it. Also, 5000 lines is tiny! A python script could run through that csv in a second, so forget about performance and just get it to run. Good luck bud, gumption in the face of having no idea what to do is what makes you improve.",1517369386.0
shady_mcgee,"It's a relatively easy task.  Search for 'mysql infile' to learn about ingesting a regular file into mysql.  You'll need to create the database first, but then can convert the excel file to CSV and ingest it.

As to whether you should accept or decline the position, it depends.  If you'll be a junior staff member with a more experienced individual also on staff that you can learn from then don't let this discourage you.  If you're going to be the only DBA and expected to be the expert then I'd be very concerned about taking the position if you can't complete this task.",1517370251.0
razin_the_furious,"Forgetting about the technical problems initially, it sounds like this job isn't a good fit for you.

If you wanted to be working into front end development, this job is going to be the exact opposite of that. ECommerce Apps are almost always built primarily on backend business logic and database management. If that's not the part of development you are actually interested in, you probably won't enjoy this job at all.",1517411028.0
r3pr0b8,"pretty sure your foreign key error was due to not inserting the rows in the correct sequence

for example, you cannot add row 2 which references row 1 if row 1 hasn't been added yet",1517268915.0
KevZero,"If it's a cloud server, you can take an image of it, and use the image to take a mysqldump which you can then restore onto the new server you've set up. There are a few ways you could sync the new one if you're concerned about losing data between the snapshot and the cutover, like setting up replication.",1517199762.0
NotAnExpertWitness,"If you have root for the machine, then there are several guides out there that will show you how to update the root password on mysql.  Its pretty easy as I have found myself in that spot also.

If it were me, I would figure out how to mount a s3 bucket or other cloud storage product on the server.  Then run Perconas innobackupex or xtradbbackup on the box to make a useable snapshot of it.  Writing the output to the remote drive mount.

You can use mysqldump, just do it for each database listed in ""show databases"".  But using mysqldump will likely lock the system up for a period of time as it writes each table out.  Also if the system is live during the backup, what you get from mysqldump will not be consistent.",1517205667.0
maetthu,"You could reset the root passwort, it's trivial, but needs a service restart - starting the server with skip-grant-tables (or something like that, it's been a while) option bypasses authentication and then you're able to reset the password. Without mysql root account and thus a proper way to flush memory and logs and issue locks, you won't get a consistent backup of a running server through a copy of the data directory (and yes, you need all ibdata* and ib_logfile* files). If the data directory is in a LVM volume, you could make a volume snapshot and copy the files from the snapshot, but then again, without a global lock & flush before creating the filesystem snapshot there is no guarantee the data is consistent or even recoverable. So... use mysqldump. ",1517206232.0
SomeGuyNamedPaul,"ibdata1 is basically your database, unfortunately.  It's a lot of other stuff, and there are other things that are your database, but odds are this system wasn't set up with one file per table so it's all crammed together along with undo logs and all sorts of other crap.

Whatever you do, DO NOT just scp the database out without first shutting it down.  If you do then it will be a scrambled mess of crap.

mysqldump is probably the most sane way to get out of the.  That huge ibdata1 file may be a sticky problem is it's all in there.",1517212826.0
razin_the_furious,Filter the redundant tags at the code level. Building an algorithm to parse the titles and clean them wouldn’t be particularly hard,1517189710.0
kevank,I would look at installing Sphinx search and integrating with MySQL.  ,1517117859.0
jericon,"I doubt you have the grants to do so on shared. 

Best practice is to just use UTC and then change it your code. ",1517072705.0
lpmusix,"> If using mysql with node or php do I only need to run SET time_zone = timezone; once or should I run that immediately before every query?

If you're setting it per session you only need to set it once.  ",1517117214.0
SomeGuyNamedPaul,"There's dialing up this parameter: https://dev.mysql.com/doc/refman/5.6/en/innodb-parameters.html#sysvar_innodb_io_capacity

There's also the write threads parameter.",1517025440.0
r3pr0b8,"GROUP_CONCAT is intended for aggregating columns in a grouping query

you're thinking of simple CONCAT which works on strings",1516916837.0
frysql,"looks like severity is already in your SELECT clause.  So all that's missing is prior to your order by and after moogdb.alerts.alert_id = moogdb.alerts_custom_info_fields.alert_id put:

moogdb.severity.value = moogdb.alerts.severity

I'm guessing all these tables are in the moogdb schema anyway.  if you do not want to provide the schema names in the query you can first ""use moogdb;"".  If you provided the table definitions, I could help you write this query a little better. But absent that, try my suggestion.   ",1516903749.0
xXxLinuxUserxXx,"You are probably fucked if you don‘t have an old dump of the data. 
Probably you should look out for some undelete tool but you should avoid further writes to your hdd.
Another solution could be if you had enabled binlogs, Switch are mostly used if you use replication.",1516917304.0
aflesner,"Mysqldump is your friend, but it's too late if the DB was already blown away. Frms are your schema but I've never heard of a successful restore of data from ibdata1 and logs. You're probably stuck with your last good backup.",1516942227.0
r3pr0b8,"what do you get for this --

    SELECT CAST(CURRENT_DATE AS SIGNED) AS today",1516877346.0
smilbandit,"You'll want to start by getting the date and hour in 24 hour format.  Then switch the hour with this floor(hour/4), in the select and group by clauses.  You'll get interval 0 through 5.

Sorry on mobile and haven't tested, but i think that will do it.",1516852864.0
IUseRhetoric,Do you have the error log enabled?  What does it say?  What are the file permissions on var2/mysql/db2 (and the parent directory permissions)?,1516726692.0
mcstafford,If you have app armor installed then you'll need to adjust it to allow mysqld to access the link's target(s).,1516770873.0
faxattack,I usually use mount - - bind for this instead.,1516856880.0
jericon,"What you are looking for is some kind of code that you can write that will give you an interface and alter it based on your input.  Something like PHP, Python, Java, etc are all options.",1516679759.0
acer3680,I’m pretty sure you can create a GUI in Microsoft Access and link it to your database.  I have never done this but I have heard of people doing this before. ,1516680937.0
_Jackk1337,"Figured out the issues!

I forgot to turn on PHP display_errors so once I did this the problem(s) became clear! First is that the database is case sensitive, in my case the database is called ""dpk"" but the connect.php file was ""DPK"". 

Second is that I have moved this web application from a Windows based machine to Linux, so filepathes such as ""includes\connect.php"" no longer work in Linux. 

Thanks for the help and suggestions guys. Now time to fix the rest of the bugs! ",1516557296.0
NCFlying,Is 3306 port open?,1516553644.0
noplansurvives,Permissions issue perhaps?,1516554392.0
NCFlying,Yea my next think would be privileges and if you had something that referred to an IP Address in the privileges. ,1516554557.0
_Jackk1337,"Just to add some more information here... I created a script that will echo out the mysql server information. I got:

5.7.20-0ubuntu0.16.04.1

So my PHP application is connected. However any queries I run don't work, and also mysqli_error isn't giving me any errors. Even simple queries such as ""SELECT * FROM users"" isn't working. That query in phpmyadmin does work however. ",1516556905.0
Bitter_Bridge,"When you submit the form, is the successful connection string from your connection file echoed out?",1516488874.0
Varlen,"I would check to make sure special characters are not being sent over in your input. You can find out by echoing each input at the bottom of the page. Further, on the ""if(submit)"" section, put an ""else"" clause with an error output. This should help narrow down your search.",1516493304.0
JamesB41,"Just for debugging purposes, put a var_dump of $con in your if statement, as well as a var_dump of the result of mysqli_query(). You'll probably learn a lot from that.

Sidebar: I know you're just trying to get this to work, but I hope you're aware that this approach is extremely insecure. There's nothing wrong with using it as a stepping stool just to learn how it works, however.",1516498408.0
Varlen,"Change include to require. Don't require_once, just require. It'll error out if it's unable to load the script. You can then change it to require_once later, if you'd like.",1516561141.0
jericon,"You might get better results on a PHP based subreddit, instead of a mysql one as this appears to be a code issue, not a Database issue.",1516679836.0
r3pr0b8,"you forgot the GROUP BY clause

also, a hint: never use the dreaded ""select star"" in a GROUP BY query",1516432746.0
TechnocratByNight,"Drop your query cache size to 96mbs. What you have seems too high. Also check your slow query log, see what's actually hurting you. Key buffer size seems a bit low.

64 max user connections probably won't help, are you hitting that?

Seeing a 'show full process list' when the issue occurs would be inordinately useful",1516310522.0
SomeGuyNamedPaul,Are you using MYISAM or are you using InnoDB?  You seem to have it configured like you're using both.  I suspect you kind of are and then running out of memory.,1516311628.0
h3xus,"I would make the following changes to increase performance:

* tmp_table_size=96M
* max_heap_table_size=96M
* query_cache_size=0
* query_cache_type=0
* thread_cache_size=0
* innodb_support_xa=0
* innodb_flush_log_at_trx_commit=0
* thread_cache_size=38

Also depending on what your resources are increase the buffer pool size. Make sure you backup the config before making these changes.
",1516335285.0
micalo1,great,1516342212.0
r3pr0b8,"no inner join or subselect is required

    SELECT x, y, MIN(energy)
      FROM yourtable
    GROUP
        BY x, y",1516303959.0
ambrace911,What did you change and what were the initial values?,1516296192.0
yukaputz,"As a side note, I used superputty and nano to connect to the server when I edited the first time, which was adding characters in erroneously.  I've since stopped using superputty, but still can't find what may be wrong here.  ",1516296359.0
msiekkinen,"Before or after?  Either way i guess it's switching the equality sign. 

I think you should be able to get away with

    date(your_datetimecolumn) >= date(now()) - interval 7 day

I just find now shorter to type the dayofweek call seems weird and wrong to me.  Maybe I don't fully understand what you're trying to to. ",1516292301.0
DataVader,"SELECT *
FROM yourTable
WHERE yourDate < DATE_SUB(CURDATE(), INTERVAL 7 DAY);",1516293030.0
shaunc,"This query gets you most of what you want:

    SELECT TABLE_SCHEMA
        , TABLE_NAME
        , COLUMN_NAME
        , DATA_TYPE
        , COLUMN_TYPE
        , IS_NULLABLE
        , CHARACTER_MAXIMUM_LENGTH 
    FROM information_schema.columns
    WHERE TABLE_SCHEMA NOT IN ('information_schema','mysql','performance_schema','sys','test')

If you need the possible ""length"" of numeric columns, you might have to do some fudging, e.g. 

    CASE 
        WHEN DATA_TYPE = 'int' AND COLUMN_TYPE LIKE '%unsigned%' THEN 4294967295
        WHEN DATA_TYPE = 'int' AND COLUMN_TYPE NOT LIKE '%unsigned%' THEN 2147483647
        ...
        ELSE NULL
    END AS max_value ",1516248939.0
jahayhurst,"You can do it if you need to. You'll want to build with either SSL support enabled in MySQL, and make sure you require TLS (it probably won't by default) and configure your client to PIN the SSL you should be using (otherwise, someone MitM's you and you auth with your proper pw and you're done) and even with both of those, you still want to monitor it.

Even with all of those things though, historically authentication and authorization in MySQL hasn't been the most rock solid - I wouldn't be surprised to see someone post another bug sometime, and if that happened no matter what kind of SSL/TLS you have, you could be in trouble.

Much better to put it on a private network dedicated to database traffic. If you NEED remote connections (like for dev work) you could always build an SSH tunnel to the database server and then connect through that - works pretty well for interactive stuff.",1516157050.0
just5ath,Consider what you're doing and maybe backup your database and restore it somewhere locally to work on it in Visual Studio. Wouldn't want to work on a production database directly.,1516162418.0
msiekkinen,"You don't want to dev against your production database.  The very nature of it means you don't know if it's working how you want it to.

Setup a local mysql install on your dev environment to work against.  Copy down empty schemas to work against and populate them with test dummy data.  

If you must have remote access and can't setup a VPN at the very least make sure you're using SSL connections (which requires a bit of work beyond default install to configure serverside).  Also pick a non default port aside from 3306.  ",1516165079.0
AllenJB83,"I would suggest that accessing MySQL directly using the MySQL protocol is no less secure than installing and using a tool such as phpMyAdmin on the web server to admin the database.

I would even go as far as saying it's more secure - I would be willing to bet that there have been less vulnerabilities in the MySQL protocol (and in particular with regards to authentication and access control) than there have been in phpMyAdmin.

For further security, I would recommend using the firewall (iptables on most Linux servers) to restrict access to only known IPs and enabling SSL.

Avoid using the root user for administration - create a new administrator user and keep the root user restricted to local access only. This does help to mitigate brute force attacks somewhat as both the username and password are needed (rather than knowing the username is ""root"")

I'll also echo warnings about developing against the production database. Also, regardless, you should implement and test MySQL backups in case something does happen.",1516181681.0
mudclub,"In general, your database should never be directly exposed to the internet.  Interacting with it across a secure lan is fine.",1516141453.0
apaethe,The SUBSTRING_INDEX() function could be used in a join condition to do that.  Pretty hacky.,1516159171.0
trashpantaloons,"You’re on the right track, make the field type as TIMESTAMP and then the CURRENT_TIMESTAMP default will work",1516126027.0
SaturdayHeartache,"In my select command I have added:

SELECT [...], LEFT(filename,6) 

to get the first six characters (DDMMYY) of the file name. OK, easy. But that chops out the first zero where applicable.",1516045248.0
shift_or_die,"We need to see

show create table v_s\G",1516028263.0
jynus,"There are several questions you posted here:

* Changing the mysqldump view owner on export- I do not see an option to change it on the fly by mysqldump itself, and probably not possible with PHPMyAdmin. I would setup a filter that ignores or replaces the `/*!50013 DEFINER=<user>@<host> SQL SECURITY DEFINER */` line when creating the view- this is trivial with grep or sed, but I do not know your workflow, and I do not think PHPMyAdmin will let you do that. Setting up a script with mysqldump is very easy, you should try that, and that will give you the flexibility you need.

* regarding ""I'd have to go to the trouble of rebuilding all 10 views every time that happens."", you can do SELECT * on a view definition, if that works for you, that will get updated automatically on table definition change; otherwise, you will have to update each field reference manually.

* CRUD are possible on updatable/insertable views. There are some restrictions on which view can be modified, mostly so that there is a 1:1 correspondence between the view ""rows"" and the real rows. You can have joins, but you must only update one table at a time. You can have a look at the detailed restrictions here: https://dev.mysql.com/doc/refman/5.7/en/view-updatability.html

Note the MySQL doesn't have materialized views, all views are just DDLs with no content, that can be or not materialized at query time. Also altering a view (specially, if done very frequently) can cause you metadata locking problems for long running queries.",1516099571.0
clt829,"Use Decimal

* Decimal has more precision, not less than Double.
* Decimal does decimal math, which is what you want for currencies.
* Decimal does keep track of decimal points.

See [decimal (C# Reference)](https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/keywords/decimal)",1515955191.0
CriticDanger,Two integers. ,1515951468.0
,[deleted],1515953341.0
SomeGuyNamedPaul,"Float, real, and double are approximate representations of the value you've fed in.  If you're storing measurements from sensors in the real world then those are good to use.  If you're storing values that *must* be the same exact number retrieved as you've inserted then stay away.  2+2 can pop out as 4.0000001.  Mac OS X's calculator does this sometimes, it's infuriating and I'm not trying to run a ledger with it.

What you probably want is decimal.  You've got a maximum of 65 positions to play with, so for your use case if you define it as DECIMAL (35, 30) then you're probably going to be just fine.  


https://dev.mysql.com/doc/refman/5.7/en/fixed-point-types.html",1515977563.0
,[deleted],1515922760.0
h3xus,"You should be able to uninstall it, and then make sure that you remove the folders left in C:\program data\MySQL and any that might be left in C:\Program files, the folders might be empty but just remove them to be on the safe side. ",1515881118.0
r3pr0b8,"    SELECT recipe_id
         , ingredient_count
         , total_count
      FROM ( SELECT recipe_id
                  , COUNT(*) AS total_count
                  , COUNT(CASE WHEN ingredient LIKE '%salt%'
                                 OR ingredient LIKE '%socker%'
                                 OR ingredient LIKE '%mjölk%'
                               THEN 1 ELSE NULL END ) AS ingredient_count
               FROM matbanken.recipe_ingredients
             GROUP 
                 BY recipe_id
           ) AS result
     WHERE 100.0 * ingredient_count / total_count > 50
    ORDER 
        BY 100.0 * ingredient_count / total_count DESC",1515857982.0
magiskaoskar,Im sorry the formatting is really bad. Trying to fix it,1515856935.0
razin_the_furious,"Something like:

     SELECT host, date, SUM(IF(state = 1,duration,0)) good, SUM(IF(state = 2,duration,0)) warn, SUM(IF(state = 3,duration,0)) bad FROM host_stats GROUP BY host, date;

Would give you the total int seconds in each of the stat summer by rows. No self joins needed",1515790794.0
NotTooDeep,"select street, count(*) from... group by street;

SQL is a set based language. When it evaluates your select statement, it first decides how to find the data the (hopefully) fastest way. Then it decides what to show you. 

You want a list of unique streets. You could say ""select distinct(street)..."" but this doesn't return all that you want. ""select distinct(street), count(*) ..."" looks logically the same as ""select street, count(*) from... group by street;"", but it's not. I'm not sure of the implementation details in MySQL's SQL engine, but you can see that for the statement with the group by, the engine is probably pulling all of the data, sorting it, counting identical values, *then* summarizing (grouping) the results to eliminate duplicate streets, and displaying the results attached to the counts.

Someone else can probably explain the differences between distinct and group by better than this.",1515756324.0
ScruffMcgruff60692,"SELECT street, COUNT(Street)
FROM Table
GROUP BY Street

Does it have to be in that funky order? IF SO

SELECT street, COUNT(Street)
FROM Streets
GROUP BY Street
ORDER BY ID",1515797472.0
Nurgus,"Delimiter only occurs once in the second one because they can't overlap. 

> '**1**.**1**.**1**'  ^(*delimiter '1' occurs three times.*)

> '**1.1**.1'  ^(*delimiter '1.1' occurs once*)

Does that make sense?",1515665724.0
r3pr0b8,"""If count is negative, everything to the right of the final delimiter (counting from the right) is returned."" -- [da manual](https://dev.mysql.com/doc/refman/5.7/en/string-functions.html#function_substring-index)

your delimiter is '1'... the final delimiter in '1.1.1' has nothing to the right of it

the second one is a lot harder to understand, but that is indeed what it returns



",1515663104.0
r3pr0b8,"> Can I safely use

make a copy of the table and test it ",1515582109.0
Raphomee,"Hi,

I did and it looks fine but I cannot check every row in a table with about 500k ",1515582405.0
r3pr0b8,"load the file into a table with single large VARCHAR column

then use SUBSTRING_INDEX to split out the first two fields --

    SELECT SUBSTRING_INDEX(varcharcolumn,' ',1) AS item_number
         , SUBSTRING_INDEX(SUBSTRING_INDEX(varcharcolumn,' ',2),' ',-1) AS short_name
      FROM varchartable

with more complex nesting of functions including LOCATE, you could also split out the combined long_name and description columns, but i don't see any way to split those two because the long_name will have a varying number of spaces in it, so you will have to do some extra work there... maybe manually updating each row with a special symbol (e.g. '|') between them  

",1515576316.0
locvez,"I don't know if this helps any, but i've worked some magic in Notepad++ to add a , after the first word (ITEM_NUMBER, SHORT_NAME, etc) and a "" enclosing the text after (""book"", ""book of interesting words"", etc)

Trying to get my google-fu on to see if I can import the file with the first word as the column name

EDIT - Just in case anyone else stumbles upon this looking for the same solution. I found it much easier and quicker to just write the column names in SQL, then run magic on wordpad++ to create the CSV file.

I did the following in wordpad++ : (using the reg. ex option)

    SEARCH $ REPLACE (1 press of the spacebar) - REPLACE ALL
    SEARCH (^.*?) () REPLACE \1,\2 (This adds a , after the first word on all lines, even ones with no values)
    SEARCH (^.*?) () REPLACE \1\2"" (This adds a "" after the first space, at the beginning of all values, including no value lines)
    SEARCH $ REPLACE "" (this adds a "" at the end of every line""
    SEARCH  "" (blankspace"") REPLACE "" (no space, just a "", remove all extra spaces before ""'s)

Saved the file as a CSV

Loaded the file into excel

highlighted row 1, delete

This should leave you with just the values and empty spaces for ones with no values. I then imported the file using phpmysql using the following SQL

    LOAD DATA LOCAL INFILE
    'c:/items.csv'
    INTO TABLE items
    FIELDS TERMINATED BY '\n'
    LINES TERMINATED BY '\n'

This pulled all the information into the correct columns (I used the column names shown in the CSV file to replicate, I just made everything VARCHAR 100, null for ease and proof that it worked. 75 columns and 65,400 records later, perfection :) )",1515581836.0
NotTooDeep,"Let's see if I understand this correctly.

A user is anyone that accesses your website and volunteers their email and what type of user they want to be, tenant or landlord.

A listing is any real property that can be leased by a landlord to a tenant.

Notice that those two entities are physical. You can touch them with your hands. 

But there is a problem, isn't there. Listing implies available, but its columns imply an existing lease. You've bot the wrong name for this entity. Listing is what happens to a property when it is put on the market. The real entity is Property, so let's use that name.

So, we have User and Property. Nice and obvious. 

User is somewhat generic, which in this case means less than self documenting, but you've saved yourself somewhat by the Usertype column. A person of average skill could read the table name and run through the column names and be confused. They would have to review the values in the Usertype column to figure out what this entity represents, and that's a weak design.

And, what about other people that join your website but are neither tenants nor landlords, but have some role or type that you haven't thought of yet? Your design is not extensible.

So, let's call it Person. 

Now, a person may join your website. A person may rent a home. A person may own a property that they rent to someone else. This could be the same person. This happens all the time. Your model is in more trouble.

Furthermore, the ownership of properties can change over time. You need an easy way to maintain these many to many relationships.

I'll point out another problem. A listing is sometimes a contract between a property owner and an agent. A listing other times is nothing more than a craigslist posting by an owner, with no third party agent involvement. A lease is always a contract between a landlord and a tenant. Without both signatures, it's just a proposal, even though it's still called a lease. Isn't business language grand? And the terms of the lease can be negotiated. The terms listed on a Listing are necessarily the same as the terms on the lease agreement. Usually, the listing is an abbreviated form of the lease; just the highlights.

This means the columns in the Listing table don't belong there.

So how do you model all this?  You test the relationships with statements like the following.

A person may own one or more properties. True.

A person may rent one or more properties. Also true. Rent a home and rent a studio for creating art.

A property may be rented to one or more persons. True. It's called roommates. Sometimes all the roommates sign the lease.

A lease is an agreement between two or more persons for one or more properties. True. Bands will rent both sides of a duplex, live in one side and rehearse in the other, covered under one lease.

All of the subjects of these true statements are individual base entities; i.e. Person, Property, Lease.  All of the verb phrases are special entities (intersections, bridges, etc) that define the relationships between the base entities.

How many tables do you think you need now?

And after you figure that out, riddle me this: does a person ever have more than one email address?
",1515546279.0
MadPhoenix,"You should think through the relationships between those three entity types first.  Are they one to many, many to many, etc.  (e.g. one user can have many listings).  Once you decide on those relationships, just google how they are modeled within SQL.  You can probably support one-to-many between user and listing with just the tables you have, but if you decide there is a many-to-many relationship you'll need a join table.",1515513181.0
diarmuidn4,"What I am thinking at the moment is to have a null foreign key in the tenants table and have the landlord add the email to that table, then when the tenant comes along and registers it updates the foreign key in the tenants table with the id pk from the users table by doing a search based on email and if it returns true update the tenants table? 

Would this work? 

Thanks",1515513201.0
ruhrohshingo,"Personally I'd do this:

* users: id (PK), username, email, password, usertype, firstname, lastname

* listing: id (PK), manager (FK on users.username), deposit_amt, (available_from)

* listing_tenants: id (PK), tenant_id (FK on users.id), listing_id (FK on listing.id), lease_start, lease_end, deposit_status (bool)

The reason being a listing can have multiple tenants, but allow for breaking lease of an individual if there are multiple. Someone coming in in that case might have different lease terms for the same listing. It's also plausible for an individual to be a tenant of multiple listings (e.g.: ending lease at one listing and starting lease with another). If listing_tenants were just tenant_id and listing_id, there'd be no guarantee of uniqueness, which would be problematic in the event that an individual lives in listing X, moves to listing Y, then moves back to listing X - you'd have duplicate rows and nothing relative to ensure anything historical you're digging up for metrics, rental history, etc. are getting messed up.

For landlords adding a listing, this design makes it very simple to know what user/management group ""user"" presides over which listings. With your original schema there is no such relationship. I also tossed in a suggested available_from column in the listing table above. This would act as an easy way for landlords to make the listing visible in your app and also allow for automated updated, such as if a landlord has a normal timeframe of 2 weeks after vacation of previous tenants before a listing is ready for rental again. Your app could take that default ""time to prepare"" value, tack it on to the max date of a unique listing.id, and then automatically update listing.available_from with no further effort on the end user's part to say a particular listing is available again. ex.: I'm a landlord who says ""it takes 2 weeks normally for a vacated listing to be ready for new tenants"" with a listing whose tenants' lease ends 2018-01-12, which is tomorrow. Your app would see ""ruhrohshingo's listing X lease ends 2018-01-12, there are no new lease_end values for listing X further into the futre. ruhrohshingo has a 2 week turnaround to make the listing available again, so after 2018-01-12 I will set listing X's available_from column to 2018-01-26, two weeks from the lease_end date.""

Also your original design with the tenant's name being in the tenants table complicates things unnecessarily - especially since you later state there are usertypes to distinguish who's who. You should leverage that to simplify the design.

I don't know much about how renting works on the other side of the fence as a landlord, but it may be that column needs to be moved back to the listings table as a listing usually has a lease deposit, not a deposit on each tenant on a lease.",1515804203.0
razin_the_furious,"If you are trying to connect via localhost, and the server is listening on 127.0.0.1, it doesn't have an socket file to connect to.

Try connecting on 127.0.0.1 instead",1515427641.0
winzippy,"Try adding this in your my.cnf file (usually /etc/my.cnf):

    bind-address=0.0.0.0",1515440905.0
kaydub88,"I think MySQL Workbench only works for MySQL, not PostgresSQL.",1515421075.0
___GNUSlashLinux___,"Maybe you can try Datagrip from JetBrains. It's not free but was built to use one tool to connect to many databases. 


If you're doing db dev work its great for PostgreSQL but you will need to use `psql` for the admin work.",1515459374.0
zardwiz,"DBeaver is a decent alternative that will handle postgres and is free.

It's also pretty easy to learn your way around.",1515469073.0
SomeGuyNamedPaul,MySQL 8?,1515410845.0
frequenttimetraveler,"did you check that ""your my.cnf matches the ibdata "" as the error says? you might have misspecified the size of the ibdata file",1515712677.0
r3pr0b8,could you do a SHOW CREATE TABLE for the review table please,1515315809.0
SaltineAmerican_1970,You should definitely be using prepared statements and PDO. Read https://phpdelusions.net/pdo for the best how-to on the web. ,1515513723.0
davvblack,"Thank you so much for editing the answer into the question :)  ""Edit: nvm figured it out"" is one of my pet peeves.  Spatial stuff in mysql is cool.",1515359085.0
mudclub,"InnoDB: is in the future! Current system log sequence number 89689091084.

InnoDB: Your database may be corrupt or you may have copied the InnoDB

InnoDB: tablespace but not the InnoDB log files. See

InnoDB: http://dev.mysql.com/doc/refman/5.6/en/forcing-innodb-recovery.html

InnoDB: for more information.

2018-01-05 12:43:55 4cc InnoDB: Error: page 1 log sequence number 8329252252303

Start with the link in the log you posted.",1515188627.0
nofoo,"Hi,

sakila and world are sample databases that come with a default installation. You can use mysql_secure_installation after installation to drop those.

The others are internal schemas that hold data for the dbms to work.",1515154631.0
tagg99,Did you design it in workbench and forward engineered it?,1515154649.0
AllenJB83,"information_schema, performance_schema, sys and mysql are standard databases that allow SQL(-like) access to things like users, detailed schema and data storage information, performance statistics (such as index usage) and more.

You can find more information about these in the MySQL manual (note that the information available is dependent on your MySQL version):

* [mysql](https://dev.mysql.com/doc/refman/5.7/en/system-database.html)
* [information_schema](https://dev.mysql.com/doc/refman/5.7/en/information-schema.html)
* [performance_schema](https://dev.mysql.com/doc/refman/5.7/en/performance-schema.html)
* [sys](https://dev.mysql.com/doc/refman/5.7/en/sys-schema.html)

The '[world](https://dev.mysql.com/doc/world-setup/en/)' and '[sakila](https://dev.mysql.com/doc/sakila/en/)' databases are sample databases. As far as I know can be safely removed.

A database might not appear on the server even if it exists if the user you're currently connected as does not have any permission to access it. Assuming you're not already, try connecting as a superuser who has permissions on all databases (eg. 'root')",1515154851.0
Bitter_Bridge,"Check out the REPLACE() function. 

https://dev.mysql.com/doc/refman/5.7/en/replace.html",1515129305.0
r3pr0b8,"what are you trying to replace?  also, do you want it replaced when selected, or do you want the table updated?",1515106780.0
razin_the_furious,"I'm not familiar with Magento's ORM, but it seems like you could benefit from eager loading all the relationships between orders and gift cards. That would be one query for orders, one query for gift cards. 

You'd still have to loop to save each item, though.",1515091253.0
justintxdave,That is fine.,1515075982.0
knifebork,"They CAN both use the same user, but I prefer to use a different user for each one. One benefit is you can more easily see which connection is which on the master. If you want to decommission one slave (or you suspect a password compromise) you can disable its user on the master. If you need to make some kind of changes to a slave user's rights or password, you'll only risk breaking one slave at a time.",1515075991.0
thatsricci,It sounds like you're so close with the cli mdb to mysql tool.  If you can repeat the pattern by hand then you should be able to use shell script to do the steps for you.  If it were Unix I'd write a bash script to use sed (text replacement tool) in a loop to run the script on every file it sees.  Heck most languages have decent filetree libraries (even php!) So you can probably write it in a language you're comfortable in.,1514999582.0
smilbandit,If you have any coding experience i know of meza and mdbtools libraries for python that might help.  Other languages probably have libraries that do something similar.,1515024034.0
SomeGuyNamedPaul,"That's basically it.  You of course probably want to configure the innodb storage layer as by default the bufferpool is rather small.  You would also want to decrease the MYISAM key buffer and some other stuff as well, assuming you're going to covert all your tables.

There also testing out the wazzoo, changing your archive method to Percona xtrabackup, doing an analysis of your primary keys.... Oh yeah, all your tables that are InnoDB must have a primary key.  You don't want one being silently created for you.",1514930063.0
msiekkinen,"First, you might want to make an export/otherwise backup before making changes if you're not sure it's going to do what it wants.

>  update wp_se_messages  set audio_url=CONCAT(""/audio-path/"", audio_url)

The syntax isn't just replacing select with update.  ",1514660164.0
mottentier,"I'm not sure if I understand what you want to do, but to me it looks like you could simply Update the value of the text field.
Something like: 
UPDATE wpsemessages SET audio_url = ""your new url""

With CONCAT you are just putting strings together.",1514660293.0
rivo99,"After playing some more I figured it out!  I guess I had to be a bit more specific in the DB name and table names in phpMyAdmin:

UPDATE `db_wrdp1`.`wp_se_messages` 
	SET `audio_url` = concat(""/audio-path/"",audio_url) WHERE `wp_se_messages`.`audio_url` NOT LIKE ""/audio-path%"";

Ripped through all my updates, about 1200 rows, in the blink of an eye!

Thanks for all the redditors who looked at this and maybe it will help another noob like me to figure out something new.",1514662754.0
skinfrakki,Check your update command before proceeding!!,1514660548.0
Bitter_Bridge,"It looks like you’re trying to produce a list of people who meet this criteria:

1. Have not worked more than 8 hours in the past two weeks

2. Did not work yesterday

Is that correct?",1514633092.0
msiekkinen,"If you're going from 5.5 you need to do a mysqldump and reload. Don't go to 5.6, 5.7 is current production ready.  After that you'll be able to do binary drop in replacements but from 5.5 you'll want a ree import",1514502180.0
h4ckth1ssh1t,What you need to do is concatenate the content of the `notifs` column with the `[pm]` value. That's the only way I see to do what you want.,1514364557.0
iheartrms,Why are you using phpmyadmin? ,1514394305.0
TeraBoy,"If I use:

    INSERT INTO users (`notifs`) VALUES ('[pm]')

I get error: #1062 - Duplicate entry '' for key 'username'

username is another column.",1514462826.0
CaptainPunisher,"Why do you want to purchase a subscription? You can download it for free in XAMPP, LAMP, or WINE, and have it on whatever machine you want. You can even run it on a raspberry pi acting as a home server that you can access from anywhere.",1514331107.0
SaltineAmerican_1970,You can probably find a shared hosting provider with a mysql database for a couple of bucks a month. ,1514481675.0
knifebork,"There was a book I got from the library that I thought was quite good. I think it was ""High performance MySQL : optimization, backups, replication, and load balancing, Zawodny, Jeremy D. (2004)."" At that age, it might be rather dated, but might still have some good info. At 276 pages, you can spend just a couple of hours on it and skim past the stuff you don't need yet. You probably don't need the replication and load balancing stuff yet, for example. I don't remember if it has stuff on database design and normalization. 

I just wanted to mention something about backups. Making a conventional copy of the data files where your database is stored can be problematic, especially if the database is up and running. There is a mysqldump utility that, with the correct options, will generate backup file(s) that should be consistent and usable. It's really just a big text file with a bunch of ""create"" and ""insert"" commands. This is really what the Workbench program uses. That's what you want for your backups -- test, of course. ",1514043092.0
etrnloptimist,"You don't need a year. You need barely more than the installation tutorial! There's literally a command in MySQL  that will read in a CSV file then load it into your table.

Just do the installation in tutorial on the side until you feel comfortable with it and then switch over.",1514051063.0
CrispyD,"Earlier this year, I was involved in a project that did something very close to this.

We reverse-engineered a Lotus Notes (!) database that pulled financial data from a central repository using CSV, that manipulated the information into a series of tables. A previous contractor had converted this to Access/Excel. It worked great until the person who created it left, and the next person tried to take over. I don't remember the specifics, but I seem to recall that there were hard-coded pointers to local profile locations and files. Those disappeared when their computer was wiped. (Nobody tested before they left.) We never could get it to work after they were gone.

We developed a series of scripts in PHP and hosted them on our local Intranet server. I created the SQL, but the developer wrapped it all up into scripts. As I recall, the difficult parts had more to do with a floating set of requirements, and strange needs to manipulate the data. Just getting the results of the queries to show up in a table was pretty straightforward. You are probably halfway there already since you already have the logic.

I've found that for the most part, SQL is SQL. If you can do the SQL in Access, you can write the SQL for MySQL. I'm not a coder, but I expect the logic in the VBA should translate to almost any language.

Remember kids; Access / Excel is not the solution, it's just a bigger problem.",1514107949.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/database] [Multi-schema databases in MySQL?](https://www.reddit.com/r/Database/comments/7l84d3/multischema_databases_in_mysql/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1513844110.0
jericon,"Schema in mysql has two meanings. 

1) it can be interchanged with database. One instance of mysql can contain multiple databases or schemas, like the mysql database and your application database. 

A database/schema contains multiple tables. A table is a group of columns, and contains rows. 

2) schema is also the actual structure of a table.  When you do show create table you are looking at the schema of the table. The list of column definitions. 

I agree that it is confusing. Especially considering that In oracle a database is the equivalent of what an instance is in mysql. ",1513848173.0
faggatron0,"A ""database"" in MySQL is actually a schema. there is only one real database in MySQL",1513888527.0
r3pr0b8,"easiest way to do this is to receive all the data from the front end, delete the existing playlist songs, and insert the new ones

handles everything -- re-orders, deletions, new entries, etc.

keep it simple! ",1513798678.0
apaethe,"You should write a stored procedure which will do 2 things inside of a transaction.

1. Increment or decrement, respectivly, all the values between the old and new song position depending on if you decreasing or increasing the song position.
2. Update the song position of the song being moved to the new new song position.  

Triggers are sometimes an option for things like this, and there are arguments for and against them, but in this case it is moot since you can't update the table being updated with a trigger, hence why you should write a stored procedure.

https://dev.mysql.com/doc/refman/5.7/en/stored-routines.html",1513822619.0
SaltineAmerican_1970,"A playlist is a doubly-linked list. The only place `song position` would be relevant is when the list is displayed to the user. HTML has an OL tag to take care of that.

Assuming that `playlist_id` is the individual user's playlist and `unique_song_id` is a foreign key to the table containing Artist, Album, Track Number and other song specific data. Use a table that looks something like this

`playlist_id | unique_song_id | this_ptr | next_ptr | previous_ptr`

Where `this_ptr` would be some sort of unique UUID or a combination of user_id, playlist_id, and unique_song_id. `next_ptr` would be null if the song is last in the playlist, or would be the `this_ptr` of the next song in the list and `previous_ptr` would be the `this_ptr` of the previous song in the playlist, or null if it is the start of a playlist.

When moving one song in the playlist, you only need to [start transaction] adjust the `next_ptr` of the previous song, the `previous_ptr` of the next song, and both `next_ptr` and `previous_ptr` of the inserted song [end transaction], even if your playlist has a million entries.

If you don't have a doubly linked list, and you insert a new song to the front of the playlist, you have to manipulate ALL million playlist entries.
",1514483795.0
r3pr0b8,"> The results I am hoping to get would be

what you're showing is a cosmetic interpretation of the results produced by the query

queries always produce a rectangular table -- rows and columns

your query should look like this--

    SELECT state 
         , name
      FROM registered_users 
    ORDER
        BY state
         , name

and it will return this --

    California  Adam
    California  Geoff
    Florida     Sheryl
    Florida     Steve
    Nevada      Carol
    Ohio        Mike

now, in order to print it out the way you want to see it, you have to use an application language such as php

you should not attempt to do this with sql",1513797834.0
DataVader,"You could write a procedure, which updates and selects the user data.",1513786768.0
razin_the_furious,"Not really. Selects don't cause triggers to fire, only inserts, updates, and deletes.

You'd have to either have the PHP application do the insert after a successful login (which is pretty simple), or have the PHP application do an update on the users table (last login date, maybe?) which then triggers something to do the insert.

The first one is easier.",1513784903.0
ms4720,"Just have successful login php code insert the data in login table, no trigger needed. ",1514293542.0
mischiefunmanagable,"worth it


ORM is incredibly useful in a lot of places, DRY becomes much easier to maintain than pure SQL queries, you get cross DB support, a single entity can be presented as only one object making object models faster to design & use, and most (especially Sequelize) come with big syntactic sugar libraries that keep you from having to write verbose and sometimes horrible SQL to do things that usually require a mix of SQL and code logic, an example being findOrCreate


a bonus for Sequelize over some ORM libraries is you CAN use raw queries through it if you really need to, some libs make that an outright chore, Sequelize has a decent plugin framework that lets you add in benchmarking, caching, hierarchies and so on


downside for ORM is increased memory usage, slightly slower execution time compared to a direct SQL query since often the ORM does a couple additional queries for metadata (pulling the schema, checking field names, etc) and has to construct the main query, both of these performance hits can be designed around or mitigated in other ways, ones that kind of become simpler using ORM anyway, caching, threading, etc


worth using even if you don't use it to its fullest, but if you do drink the koolaid you'll find you develop faster and more efficiently, just don't fall into the micro-optimize every query trap, if an ORM query is slow rethink the query or your approach


proper indexing is still important",1513783693.0
Laurielounge,Can you logon from a mysql client? From the command line?,1513729041.0
SomeGuyNamedPaul,"Either modify the table so they the timestamp is within the primary key, or there's a unique key created that you do not want to violate and describes the condition you're looking to create.

From there you have [insert ignore](https://dev.mysql.com/doc/refman/5.7/en/insert.html) which will attempt to insert a row, but if there's a unique or primary key violating from an existing row then it will just drop the insert.

The alternative is the [replace](https://dev.mysql.com/doc/refman/5.7/en/replace.html) keyword which syntactically you use just like an insert.  It will attempt to insert, but if there is an existing value than due to the primary it unique key then it will update that row rather than insert.

The *replace* mechanism is sometimes called an ""upsert"".  In particular in databases like Cassandra where everything that's an insert or an update is really an upsert with the newest timestamp winning.  Obviously care and planning need to take this into consideration for your precise use case.  If you're looking to just insert rows than ignore. I don't think one of any more work unless there are other columns outside of the unique or primary key that you're looking to update.",1513671480.0
dasteph,"    insert into <table>
    select input.*
    from (select 'valueA' as colA, 'valueB' as colB, 'valueC' as colC) input
    left join <table> check
    	on check.colC==input.colC -- may be round to seconds, dunno
    where check.colA is null

:ugly:",1513705790.0
dirtylifeandtimes,"I'm enrolled in these but have not started them yet. 
https://www.udemy.com/the-ultimate-mysql-bootcamp-go-from-sql-beginner-to-expert/

https://www.udemy.com/the-ultimate-mysql-bootcamp-go-from-sql-beginner-to-expert/

Don't think they offer real certs, but they may get you well on your way.",1513701450.0
justintxdave,HAve you considered extracting the JSON key/values into a generated column (or several) and then indexing that generated column.,1513608135.0
Etii3964,"It depends on your use case, if the data is handled only in one place(lets say some javasript) than i'd go with JSON, if you handle the data on different places(ios app, android, web and maybe even some backend tweaks) use separate fields.",1514397385.0
r3pr0b8,"> Will this model allow me to do so?

yes

> Just SELECT all the rows in the order table that match the current user's id?

yes, but don't forget to INNER JOIN to the Order_Item and Product tables",1513614803.0
aheinzm,"If you're going to have a Product and Order table, I think it would make sense that the intersection table be named Order_Product, not Order_Item. Same goes for Wishlist_Item.",1513628101.0
llameadrpc1,"SELECT importer, warehouse_name, COUNT(whatever_you_want_to_count) FROM table GROUPON BY importer, warehouse_name;
",1513621766.0
r3pr0b8,"you have SERIAL on the PK but BIGINT on the FK

i wasn't aware SERIAL is a valid MySQL datatype, but apparently it translates to BIGINT UNSIGNED

the FK is presumably SIGNED by default, so maybe change that to UNSIGNED explicitly
",1513544061.0
maktouch,"If you're using innodb, it should be fine unless you're writing to the same row.

If you're not writing to the same row and it's locking, looks like you're using MyIsam and you should switch to innodb instead.",1513513983.0
SomeGuyNamedPaul,"Stop using MYISAM tables for this workload.  InnoDB exists for a reason and it's more than happy to take concurrent writes.

When you do spin it up, I recommend converting all your tables to InnoDB, greatly increasing InnoDB bufferpool from the really low default, severely dialing back the MYISAM key buffers and whatnot, and probably setting multiple bufferpools.

You'll also gain the ability to do online archives with xtrabackup.

Converting all your tables is beneficial as joins between tables that use multiple storage engines are not optimal and you can still have your locking issue if even one table is non-,transactional.",1513519355.0
NotAnExpertWitness,"If its MyIsam, convert to Innodb and you are done.  If this is Innodb then post your config.  100 entries per minute is nothing for mysql, unless you have slow drives and no ram.


",1513528179.0
greenman,Answered in [r/mariadb](https://www.reddit.com/r/mariadb/comments/7k5w22/mariadb_blob_fields_in_mysql/).,1514043973.0
MRideos,Is this right date format ? ,1513357766.0
SomeGuyNamedPaul,Was it started as user root before and now it's trying to start as user mysql?  Who owns those .err files that are getting the permission denied?  Did something bad happen to your /var filesystem?,1513175390.0
llameadrpc1,mysql.error log? ,1513173325.0
NotAnExpertWitness,"Make sure you are root or at least issuing the command with sudo.  IE: sudo /etc/init.d/mysql start


Then also try issuing a stop first (even though it is off) then the start.",1513175939.0
geleman,Don’t use unit in 7 use systemctl and change your permissions to 0755 just to see if you can get it to start,1513215102.0
msiekkinen,"> default-storage-engine=MyISAM

Yup, you're gonna have data loss

> Steps tried: - Restore backup - did not work  

Never tested backups before needing them?  Yup you have no backups


Edit: I realize those comments do nothing to address your current situation, but please learn from this.  Never use myisam and test backup restores ",1513182394.0
Laurielounge,"... one approach is a temp table of custid which is added to or deleted from as the case may be... ultimately, the output is a stream of customer data only...",1513118756.0
r3pr0b8,"    SELECT B.*
      FROM Flights AS F
    INNER
      JOIN Bee AS B
        ON B.f_id = F.flight_id
    INNER
      JOIN ( SELECT callsign
                  , MAX(time) AS latest
               FROM Bee
             GROUP
                 BY callsign ) AS M
        ON M.callsign = B.callsign
       AND M.latest = B.time
     WHERE F.status = 'Active'                ",1513106253.0
TedW,"Just to clarify:

    SELECT b1.* FROM Bee b1
    INNER JOIN (SELECT flight_id FROM Flights WHERE status='Active') f1
    ON b1.f_id = f1.flight_id ORDER BY b1.time DESC;
    +----+------+---------------------+--------+---------+-------+----------+
    | id | f_id | time                | lat    | lon     | alt   | callsign |
    +----+------+---------------------+--------+---------+-------+----------+
    |  5 |    1 | 2017-12-12 10:22:16 | 44.501 | 144.818 | 23120 | pickle-P   | < I want this
    |  4 |    1 | 2017-12-12 10:22:00 | 44.512 | 144.816 | 23200 | pickle-P   |
    |  3 |    1 | 2017-12-12 09:55:10 | 44.572 | 144.995 | 23515 | pickle-R   | < I want this
    |  2 |    1 | 2017-12-12 09:54:55 | 44.568 | 144.985 | 23492 | pickle-R   |
    |  1 |    1 | 2017-12-12 09:54:37 | 44.567 | 144.987 | 23456 | pickle-R   |
    +----+------+---------------------+--------+---------+-------+----------+
    5 rows in set (0.01 sec)",1513104995.0
mischiefunmanagable,"simple, they aren't the same

check the length of the data in that field for each row and try comparing each character position with ORD(SUBSTR())

",1513001750.0
halukakin,My usual suspects would be \n or \r\n,1513006928.0
hambalamba,Whats the lenght limitation on the row?,1512985243.0
Black_Hawk1096,"In addition to the length limit, check and make sure there aren’t any characters that would cause issue. Basically make sure you’re escaping it. ",1512986495.0
kevank,What language are you coding in?  Can we see your SQL and your insert code snippet?,1512999071.0
mischiefunmanagable,"generate one, test data is trivial to generate automatically",1512944802.0
tywkeene,"Fake name generator let's you generate fake data with fields you can pick like name, gender, CC info, email etc. In SQL/CSV format 

",1512962446.0
payphone,Where is the logic that says what the next course is?,1512862832.0
trashpantaloons,"You need to modify your courses table to Id each group and then an id for the order in order to remove weak links in trying to string cut and integer update your course names. 

If you didn’t want to modify our table structure you effectively want to do something like use ltrim and then substring by 1 to get the integer of the course name. Then make sure it is less than the maximum integer it can be and increase by one and the string back together. This however would be a weak link disaster waiting to happen",1512929094.0
ckofy,"Yeah, you should add either two new fields to your `courses` table: courseid and ord_no, where courseid can be either integer, or some unique part of your courseName, say first three letters of the name if it is guaranteed unique. 

Then selecting of next course will be: 

Select next_course.courseNumber from `courses` next_course join `courses` current_course using(courseid) where current_course.courseNumber = <input_coursenum> and next_course.ord_no = current_course.ord_no + 1;

Last course in the group will have no pair then and select will return null.

Another option is to create linked list of courses by introducing nextCourseNumber field in the `courses` table, then selection of next course will be like Select nextCourseNumber from `courses` where courseNumber = <input_coursenum>;

All above should be created as function which you will call in your procedure, procedure will be like: 

Select courseNumber as CurrentCourse, nextCourseFunct(courseNumber) as NextCourse from `studentscoursesinfo` where studentID = <input_studentid>;",1512942168.0
AllenJB83,"Use the '-u' option to specify the user when entering the commandline, adding the -p option to specify the user needs a password. eg: `mysql -u allenjb -p`

Create a database, then create the user and grant it privileges for only the databases it needs. You should only use the `root` 'superuser' for admin tasks such as creating databases and users.

You can use either the commandline or a GUI such as MySQL Workbench for administering the database. It doesn't really matter - MySQL Workbench does provide extra features such as the database design views and a number of useful reports easily accessible.",1512838077.0
msiekkinen,"There's a lot of what ifs.

Try nothing for the root password.  That may not work depending on your install b/c w/ 5.7 it defaults to generating a random password it puts in the error log.  The idea is to not have a no password root account and you're expected to change it after initial setup.

As far as I'm concerned command line and shell are synonyms.  Workbench or other clients are just UIs that send your commands.

Is this project a strictly academic on for ""hello world"" type purposes?",1512840307.0
r3pr0b8,"to answer your posted text, you need to do some entity modelling to decide the entities and their attributes

to answer the question in your title, no, don't store metadata, that's an anti-pattern
",1512836646.0
AllenJB83,"Without knowing the exact behavior you're talking about, it's difficult to advise.

Commenting out sql_mode in my.cnf will cause the default value to be used (see the documentation to find out what this is - it may vary depending on whether you're using Percona, MariaDB or ""Oracle"" MySQL). Note that this affects all connections to the MySQL server (a connection can set its own sql_mode value which will override the server default).

My personal recommendation (assuming you can't fix the query causing the issue) would be to keep sql_mode as strict as possible, only disabling / changing the modes which are causing you issues.",1512828490.0
msiekkinen,"Commenting out what ever you have in sql_mode will make it revert to default, which in 5.6 is none of the various sql_mode options.  

In 5.7  strict mode becomes the default.

To answer your question that would effectively disable strict mode in 5.6 ( assuming your definition had things setting it to such).   The cnf settings are only read during start up so for that to take effect you'd need to restart.

sql_mode is dynamic so you can run set global sql_mode=''; at runtime with out a restart.

However global settings are copied into session settings during connect, meaning your set global would not have any effect for existing connections.

That being said to answer your question, I'd advise removing strict mode if possible.  There are a lot of benefits and changes to your code or schemas would be worth it.",1512838421.0
SomeGuyNamedPaul,"Do this with great care.  Bad things can happen to your data without strict mode, and these bad things will happen in subtle and silent ways.

The lack of strict mode is why some people call MySQL a ""toy"" database, because without it on it will have some insane behaviors, behaviors that seem like they were convenient to program or implemented by complete neophytes.",1512839521.0
de_argh,Restart the database with skip-grant-tables and update the password / user.,1512744534.0
nick13610,"The former employee may have changed the root username from 'root' to something else (e.g. 'admin'). I suppose they may also have changed where the root user can log in from (i.e. not from localhost) - I suspect this is less likely, and in fact I'm not even sure if this is possible for the root account.",1512743280.0
NotAnExpertWitness,"Look for a "".my.cnf"" file in your home directory.  Might have a bad password in it.",1512780844.0
nitinks,"Check this, it helped me when I lost my MySQL password:
https://proprogramming.org/simplest-way-reset-mysql-root-password/",1512970270.0
SomeGuyNamedPaul,Just turn the bugger off.  Master won't care.,1512674987.0
mischiefunmanagable,https://www.percona.com/doc/percona-toolkit/LATEST/pt-table-sync.html,1512698876.0
SomeGuyNamedPaul,"Once you're done with pt-checksum and pt-table-sync you probably want to switch to ROW based replication instead of MIXED.  You could also simply push over a xtrabackup and be sure your slave is accurate, assuming you could afford the down time. It may be faster depending on what's going on.",1512706072.0
NotTooDeep,"Strictly from a theoretical point of view, yes, it's wasted space and that wastes disk and memory.

From a practical point of view, you may not be able to measure the performance difference after removing them.

Here's a simple test to find out: take the largest table with a bunch of unused columns and do a create table as select only the used columns from big, hollow table. Add the same indexes and constraints. 

Now you can measure the storage difference and time the difference in your standard application queries between both tables.

Here's a more compelling reason to get rid of the unused columns; every person that looks at a table like this in the future will have to sort out what is used, what is not used, and why, as well as the meanings of what is used. Why waste expensive developer time like this? It adds no business value. It certainly adds frustration. To measure the lost productivity, just keep track of all the time you spend figuring out what's used. Multiply this by some number of new developers per year that roll onto the team and then multiply that by ten years. 

Fun times.



",1512650414.0
SomeGuyNamedPaul,"For InnoDB tables the first 500 bytes of a row are stored on the page, and the rest is hiding in some forward pointer off to some other location on disk.  If you have a select that requires looking at rows in that additional portion then it's more head seeks necessary, hence much slower.  Also, the more rows you can pack into a page then the faster things are going to be because more useful data can be accessed with the same amount of I/O and fit within the same bufferpool allocation.

But your biggest problem as /u/NotTooDeep pointed out is actually using the data becomes a problem because programmer time and errors shoot up dramatically.",1512652100.0
SomeGuyNamedPaul,"Let's assume you do somehow get this to work, congratulations you're now running a setup that nobody else on the planet is running.  If you're lucky it will detonate spectacularly and quickly.  If you're unlucky then it will slowly corrupt the shit out of your data in subtle and undetectable ways, ways which won't get noticed until your data has drifted horribly and there's no sane way to reconcile which instance is the source of truth.  Even if you do have archives that span back far enough to before your experiment in production, you cannot be saved by it because you're not sure what sane way there could possibly be to replay mutations.  You've destroyed your company and everyone involved are now destitute.

Then while living homeless under a bridge you get parasites from eating fetid meat out of a garbage can.  You contract horrifying and medically unclassified venereal diseases selling your body, which will of course go untreated and eventually rob you of your mind and soul, hopefully before the parasites' offspring hatch and burrow themselves throughout your twitching body.

Don't do it.",1512605569.0
kaydub88,This is bad and you should feel bad.,1512623930.0
NotTooDeep,"You losing time. I upgraded a 5.5 data warehouse to 5.6 this year and the performance of the nightly batch loads improved by 20%. No other changes.

The reason for the upgrade was to close critical security holes in 5.5. 

There are some changes to the structure of the data dictionary between 5.5 and 5.6. There are more between 5.5 and 5.7. And I don't believe Oracle will support the setup you're considering. I believe they only support replication from one major release (5.6) as the master to the next major release (5.7) as the slave. Master-master I believe must be on at least the same major release (both 5.7, or whatever).

It's good you're thinking about possibilities. It's a good exercise and can draw you into some interesting insights. Now think about what's useful.",1512609630.0
msiekkinen,I feel like I'm reading this wrong.  Are you saying you want to downgrade from 5.7 to 5.5?  Why?  What ever reasoning you have I'll be happy to tell you why it's misplaced.,1512609774.0
AllenJB83,"Simply saying you got an error running a query doesn't give us enough information to help you. You need to tell us what the query was and what the error was.

It may also help to give the full schema (CREATE TABLE) for each table (including indexes), rather than just a brief overview of the schema.

I also don't think it's entirely clear from the information given how these tables are related? Is ID a foreign key in one of the tables (in function, even if not actually defined as a foreign key in the schema)? Is the relationship 1-1 or 1-n (one to many)?

You may also want to read up on EXPLAIN as this can help you determine what MySQL is doing with a query.",1512552699.0
thenickdude,"What are you using to run the query? Some tools, like MySQL Workbench, have really harsh default timeouts and will give up on waiting for the result. It wouldn't be able to handle displaying the 6GB resultset anyway. ",1512550461.0
r0ck0,"""An error""... actually I know the exact cause here: a problem. ",1512571968.0
AllenJB83,"Having read the supplied extra information, one thing that sticks out is the CREATE TABLE ... SELECT query appears to be overwriting the modelscore table (as I read it, I've never used CREATE TABLE ... SELECT, so I may be incorrect in exactly how it works here) because it uses the modelscore table in the SELECT query.

As you appear to be using a large proportion of fields from both tables in the SELECT, I'm not sure how much help a multicolumn index will be for the query (is there a reason you're duplicating so much information instead of using a JOIN when selecting it for output later?).

With the stated number of records involved, I don't think you can avoid this operation taking some significant time. If it's failing to complete you could batch the operation based on aeid (but don't use OFFSET because it's highly inefficient on large result sets - use WHERE clauses instead) using INSERT ... SELECT

You do mention ""there shouldn't be any more than about a million records for each model score"" - is this the approx. number of records you're expecting in the final table? If so, with it earlier stated that there's 50 million records in each of the source tables, this is unlikely to be the case as there's no GROUP BY / aggregation or WHERE restrictions. Is your CREATE TABLE ... SELECT missing some clauses?",1512578770.0
halukakin,"Starting with mysql 5.5 my personal experience is innodb is better even with simple selects.
If that's an option i would work with mysql 5.7 and innodb.

The tables you are trying to merge will require lots of memory. I usually work with memories around 300gb, i'm not exactly sure what the minimum ram would be for big tables like that, but if i had to guess, no less than 8gb i would say.

Workbench can be a bit of a problem with big data. Lately i'm using datagrip more and more for these kind of queries.",1512586356.0
frank266,Dump the db then import dump file into new db. ,1512527704.0
justintxdave,"Download the MySQL Utilities and use mysqldbcopy.  https://dev.mysql.com/downloads/utilities/

mysqldbcopy --source=root@oldmac --destination=root@newmac olddb:newdb",1512583257.0
NotTooDeep,"Are you using a MySQL database or an Oracle database?

I ask because there isn't to my knowledge a system view in MySQL called user_tables, but there is in Oracle, so I suspect you're on Oracle.  I can help you sort out your problem.

A tablespace is an abstraction layer between the logical objects called tables and the physical objects called data files. You don't need to play with tablespaces to do your homework. 

In Oracle, a user and a schema are identical.  This makes a view called user_tables make sense and is why I think you're on an Oracle database. When you select * from user_tables, you only see the tables you created.  You have not done anything to your instructor's tables. They're in a different schema, which wise and generous Oracle chose to call ""Owner"" in these views.

Adjust your query to fully qualify the schema_name.table_name; i.e. SELECT * FROM BANK.employee; 

This tells the SQL engine that you aren't interested in your tables, but in tables owned by BANK. This should put your worries to rest.

For extra credit: why is a tablespace a good idea? Answer: it makes the database portable between different operating systems.  This is a big deal for customers. They can choose to move their database from a small Windows Server to a Linux server or Sun Solaris or Sequent Dynix server or whatever, and the database will work exactly the same way from the application's point of view. The tables look and act the same. The stored procedures work the same. The only thing Oracle needed to do was port the file management routines that live beneath the tablespace to all of these different OS's. Neat, eh?

Additionally, it can serve to group tables away from other tables on different storage devices or partitions, but that ain't so big a thing these days with virtualization. This used to be necessary for high performance, but hardware is so cheap that most folks just buy more power and get on with life.

",1512517464.0
jeremycole,"It is Open Source. You can use it for whatever you want, commercial or not. The only thing you cannot do is distribute it without also providing source code on request. There are restrictions about how you use the trademark ""MySQL"" and its related trademarks, but those are not relevant to your concerns here.",1512506656.0
IUseRhetoric,MySQL CE is free to use by everyone.  The difference between CE and Enterprise is that the latter comes with support and some features not included in CE. ,1512506617.0
Remeran12,oops license* ,1512506297.0
thatsricci,"What's your mysql version?  Are all rows exactly 5 chars to remove? You can use substr() straight up... If it varies use  instr() inside substr to get the first index of a '-' character...   Ref: https://dev.mysql.com/doc/refman/5.7/en/string-functions.html


Update table set id = substr(id, 5)


Please test it first :) "" select substr(id, 5) from table'",1512463190.0
justintxdave,I had a few folks ask when MySQL was getting JSON_TABLE and built a developer milestone release of the code from Labs.MySQL.Com.  Linked article is my first fumbling with JSON_TABLE.  Please provide feedback if you like the implementation.,1512435757.0
davvblack,"oh no, i'm very worried about fixing this code incrementally.  Please read about sql injection before proceeding with this task.  

What is the overall project here?  I strongly suggest you use a framework like Laravel to help guide you towards modern practices.  Many of these problems are well-solved.",1512408815.0
inzeos,"You should read up on PDO and binding parameters. The code you've posted above dangerous and could lead to security issues via SQL injection as /u/davvblack mentioned already.

Here is a decent recent article that is not just a manual page...

  
 https://shareurcodes.com/blog/pdo%20with%20prepared%20statements",1512412640.0
msiekkinen,"Changing a datatype will require a full table rebuild unless you're running the tokudb engine.  

There's nothing special about the specific lengths you mentioned.  There's too many other unknowns to give you a definite answer.  I'd want to know what's going on in the processlist.  It could be there's a long running query that's preventing one of your alter runs to even start doing it's work just by happenstance.  ",1512401347.0
mischiefunmanagable,"so there's two ways to take the second bullet and following line

* old school - they use consistent hashing along with adding nodes to the cluster to only rebalance the data that needs to move to the new nodes as those nodes would get new edge points between existing edge points

or

* they use some calculated metric of the resource cost of their users as the value fed to the hash to rebalance data in between existing nodes, users that have a static resource cost would stay on the same node but ones that have become more popular would in theory be moved to a node with less overhead, becomes very implementation dependent to actually maintain balance, technically this could work the same with naive hashing so I'm betting they're doing the former",1512402987.0
davvblack,"If any single user is too significant of a load for your system, any strategy will fail.  The point of consistent hashing is that the distribution is completely uniform and each shard will statistically have the same mix of users/objects/whatever.  Mod sharding could have some bias that would make it less robust, or even worse just recency sharding (so, say, old users on one server, users from one month to one week ago on another server, and only the newest users on the third server) would lead to very different performance characteristics on each of those shards.",1512414103.0
halukakin,"If your mysql stops during the startup process, first place you should check is /var/log/mysqld.log . It might say what is wrong.",1512324532.0
Phoenix_Sage,"Do you have two versions of MySQL installed?  You have two services, MySQL and MySQL57, right?  If you stop MySQL can you start MySQL57?

I had run into something similar when I was upgrading from MySQL 5.5 to 5.7.  They both want the same port, and so only one could run until I changed the port in the settings.

Also, check your data directory.  You should have an error file in there, typically with the extension .err.  It sounds like you are on Windows, so it should default to C:\ProgramData\MySQL\MySQL Server 5.7\Data
",1512324584.0
chinahawk,chown?,1513656674.0
mischiefunmanagable,bothered to look in the error.log file yet? cause you aren't giving us much to go on,1512259477.0
r3pr0b8,"doing your ALTER will result in a temporary copy of the table... see https://dev.mysql.com/doc/refman/5.7/en/alter-table.html#alter-table-performance 

perhaps you could do it yourself?  

    CREATE TABLE tr60new ( ... )

    INSERT INTO tr60new ( ... ) SELECT ( ... ) FROM tr60

    DROP TABLE tr60

    RENAME TABLE tr60new TO tr60",1512230179.0
razin_the_furious,"The primary key is a time stamp set to put current time stamp in by default. An alter would lead to every single row getting the same time stamp, which causes the index to block it.

That primary key is a bad idea. Maybe add an automincrement column, and change the time index to non unique",1512231242.0
