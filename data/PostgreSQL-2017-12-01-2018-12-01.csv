author,comment,timestamp
Cer_Visia,"What do you mean with ""combine""? What would the result for your example data be?",1543574148.0
LookAtTheHat,If you are using data volumes you could share that.,1543577037.0
pvh,"Oleksii, nice work but it's a still long way from what you get in a DBaaS... especially when things go wrong. Still, the price is right!",1543548083.0
erewok,This is pretty interesting. What role would network policies play if you had lots of little databases that you wanted to wall off from potentially nosy applications?,1543556291.0
pstef,"I'd make ""id"" and ""type"" plain old columns and store everything else in a ""data"" column of type jsonb.",1543475898.0
smugbug23,"I probably wouldn't use JSONB in the first place, unless the client insists on sending or receiving JSON.  And if that is the situation, shouldn't the client be the one dictating the structure of the JSON?

And what happens if one of the keys for 'data' happens to be 'id' or 'type'?",1543763770.0
terrorobe,"By now already dated but a good top-to-bottom introduction into Postgres in the real world is [PostgreSQL 9.0 High Performance](https://www.amazon.com/PostgreSQL-High-Performance-Gregory-Smith-ebook/dp/B0057G9RUG).

Most of the things Postgres does is exposed via system tables & views - for example [`pg_stat_activity`](https://www.postgresql.org/docs/11/monitoring-stats.html) & [`pg_locks`](https://www.postgresql.org/docs/11/view-pg-locks.html).

The rest of the documentation is great as well, give it a read.

If you are new to system administration & architecture, you may want to put [Designing data intensive applications](https://www.amazon.com/Designing-Data-Intensive-Applications-Reliable-Maintainable/dp/1449373321) on your shopping list as well to broaden your horizon.

If you have Postgres-specific questions you can ask them here or [reach out to the community](https://www.postgresql.org/community/).

edit: fixed links",1543418157.0
jtwyrrpirate,"pg\_top is pretty nice for seeing what's generally going on with your cluster.

You may want to look into some of the gui clients. I'd recommend some but the ensuing flame war isn't worth it, maybe others are more brave :-)",1543418013.0
K3dare,"I think the official documentation should answer most of your questions.

About the monitoring thing you should check this : [https://www.postgresql.org/docs/10/monitoring-stats.html](https://www.postgresql.org/docs/10/monitoring-stats.html)

That should cover most of the standard needs.",1543418021.0
agenderphobe,Something like pgBadger can help you get a handle on what your various databases are doing.  It's free and relatively easy to setup as a cron job. ,1543423381.0
cyberst0rm,http://www.postgresqltutorial.com/import-csv-file-into-posgresql-table/,1543376582.0
reifba,Cast to ::text[] or whatever type the values are.  Then unnest. ,1543359056.0
clearing_sky,"I would like to point out Stolon: https://github.com/sorintlab/stolon

I'm using it and it's great.",1543358329.0
olcrazypete,"So put PAF into place on a dev cluster.  The fencing part is where the docs are unclear to me, and where I've gotten into the most issues.  I have STONITH enabled to kill power to the servers via IP PDUs but in all honesty, I don't' want that heavy of a hit.  PAF forces me to have some sort of fencing in place though.  Any less drastic alternatives there?",1543342661.0
swenty,"> PAF Cons
> PAF doesn’t detect if a standby is misconfigured with an unknown or non-existent node in recovery configuration. Node will be shown as slave, even if standby is running without connecting to the master/cascading standby node.

Is this something that normally happens? 

> No pg_rewind support.

What is the recovery process for the failed master then? Rebuild the database from scratch from the new master? ",1543361613.0
jmswlms,"It might help to think of failover and connection routing as separate problems.

For routing, the application would connect to a proxy, like pgpool, pgbouncer, or even HAProxy, which then figures out the currently active master and forwards it there. You can even do this at the application level. In pg10 and above, you can specify [multiple hosts](https://www.postgresql.org/docs/10/libpq-connect.html#LIBPQ-MULTIPLE-HOSTS) in the libpq connect string and libpq will failover without needing application code logic/change.

&#x200B;

&#x200B;",1543376168.0
cazzer548,"Don't forget about [Postico](https://eggerapps.at/postico/)! The best native client I've used, (macOS only).",1543307541.0
pupeno,"It doesn't do the admin stuff, such as creating new users, for which I default to pgadmin, but I really like DataGrip.",1543311163.0
judasferret,PgAdmin for admin stuff. Dbbeaver for the rest. ,1543324987.0
Hoek,This book here: https://masteringpostgresql.com/,1543247124.0
therealgaxbo,"It's not a tutorial _as such_, but https://pgexercises.com/ is pretty good for practising intermediate level SQL.  It's good because it gives you a semi-realistic requirement that you need to figure out how to answer, so I feel it helps you learn how to actually apply your knowledge.",1543250070.0
MavSidharth,It'll be great if the mods can post the best postgresql resources in the sidebar!!,1543292546.0
flyingmayo,"The contents of your pg_xlog directory (on both primary and standby) are managed by the postgres process itself (e.g. not by the archive_clean_command which instead applies to the archive directory from which WAL files are copied).  

WAL files will be removed from the pg_xlog directory when PG believes it is safe to remove them.

e.g. PG no longer needs a WAL file after a checkpoint has occurred which includes the full contents of a WAL file. 
 Prior to this happening, I believe PG will keep the WAL file around in pg_xlog for crash recovery purposes regardless of what wal_keep_segments is set to.

If PG is hanging on to WAL files in pg_xlog then it thinks it needs them.

Check for old, open transactions and close them if you find any.  

Check for the last time a checkpoint occurred on this standby and/or perform one manually

Check how old the oldest WAL file retained in pg_xlog is, wait a few minutes and check again, are WAL files being removed at all? (albeit slowly?) 

Those checks/remedies should cover 95% of the scenarios that cause this issue but I admit that I have seen a few situations where PG will inexplicably retain WAL files in pg_xlog even after I've done everything I've listed above.  

So barring this being a corner-case bug, I probably having something to learn in this area.

In such situations I typically resort to restarting the PG process and giving it some time to sort itself out.  ",1543258620.0
Chousuke,Maybe you have a stale replication slot hanging around causing the WAL to be kept? ,1543269911.0
jmswlms,"1) What's the value of ""max\_wal\_size"" ?

2) You can do a CHECKPOINT on the hot standby (although this is not an ""actual"" checkpoint) -- does doing this help?

3) Is there an inactive replication slot on the hot standby (SELECT \* FROM pg\_replication\_slots WHERE NOT active)?

4) What's the situation on the primary?

&#x200B;",1543287375.0
MonCalamaro,"Check your archive\_cleanup\_command in your recovery.conf: [https://www.postgresql.org/docs/9.6/archive-recovery-settings.html](https://www.postgresql.org/docs/9.6/archive-recovery-settings.html)

&#x200B;

Edit: as others have mentioned, archive\_cleanup\_command is just for the log archive, not for the pg\_xlog directory.  It does not apply in this situation.",1543238691.0
dark-panda,"You can’t define a trigger with arguments, you need to access any arguments via the special variable TG_ARGV and you can only assign values to that variable when you create the trigger. You can access another schema either by setting the appropriate search_path or use a fully qualified name when you access it. This stuff should be mentioned in the docs — i’m on mobile so I can’t access the docs effectively but poke around and you’ll find it. ",1543023474.0
Triteleia,WITH RECURSIVE is why I started using postgres!,1543010470.0
graycube,"I'm not going to do your assignment for you, but I'll give you a couple of hints:

1) There is postgresql function  ""age()"".  Consider the possibilities with that tool in hand.

2) Consider using the ""order by"" clause and maybe coupling it with a ""limit"" clause.

&#x200B;

You'll get it.  It is so satisfying when you finally solve these sorts of problems and understand it.  That 'Aha!' moment is  fun.

&#x200B;",1542919444.0
mr_thwibble,Smells like a lead/lag/over problem to be me with an outer query to pick up the results. ,1542931544.0
agenderphobe,"[pssstt.... hey kid, over here](https://www.postgresql.org/docs/9.6/tutorial-window.html) ",1542920779.0
therealgaxbo,"There _is_ a way to do it, but I would strongly consider whether you _should_.  This sounds very much like you're cramming presentation logic into the query, which would be better suited in the presentation layer.

With that in mind, one solution would be to use a window function to partition by name and then only print the name if it is the first row in that partition:

    select case when row_number() over (partition by name) = 1 then name end, item, val from somewhere;

Note that if you add an order-by clause to the query it will need replicating in the window frame else the name would no longer be in the first row displayed.  And obviously the first component of the order by has to be `name`:

    select case when row_number() over (partition by name order by value) = 1 then name end, item, val from somewhere order by name, value;

But I'd seriously consider why you want to do this at the DB layer.",1542885266.0
djrobstep,"There are no doubt ways to do this in SQL, but IMO it's more appropriate to do this sort of presentation-layer tweaking outside postgres.

The algorithm is pretty simple. In python it looks something like:

    previous = None
    for r in rows:
        if not previous: continue

        for c in column_names:
            if r[c] == previous[c]:
                r[c] = ''
            else:
                break
        previous = r",1542885840.0
daredevil82,"Without some backend service to sit between the client and the database, this is a _really_ bad idea.  ",1542804388.0
mikaelhg,"What you're looking for is called ""many to many"", and you'll find numerous resources and explanations by searching for that. If your team is going to be doing a lot of work on hierarchical database structures, I recommend the classic book ""Joe Celko's Trees and Hierarchies in SQL for Smarties"".",1542743144.0
zieziegabor,"We have a relationship table that stores 2 ""people"" and a relationship id

```
database=# \d person_relationship
      Column      |  Type   | Collation | Nullable |                      Default
------------------+---------+-----------+----------+----------------------------------------------------
 id                        | integer |           | not null | nextval('person_relationship_id_seq'::regclass)
 person_id            | integer |           |          |
 linked_person_id  | integer |           |          |
 relationship_id     | integer |           | not null |
Indexes:
    ""pk_person_relationship"" PRIMARY KEY, btree (id)
    ""person_relationship_uperson_id_linked_person_id"" UNIQUE CONSTRAINT, btree (person_id, linked_person_id)
Foreign-key constraints:
    ""linked_person_dependent_fk"" FOREIGN KEY (linked_person_id) REFERENCES person(person_id)
    ""lk_relationship_depenendent_fk"" FOREIGN KEY (relationship_id) REFERENCES lk_relationship_types(id)
    ""person_dependent_fk"" FOREIGN KEY (person_id) REFERENCES person(person_id)
```



So there can be many entries for a given relationship, depending.  It's a MANY <-> MANY table relationship.

so we have a relationship table with all the diff. kinds of relationships we care about (spouse, child, etc) and a person table that stores people's information.

Works fine for us.

edit: apologize for the formatting, too lazy to fix, since reddit formatting is miserable.",1542742659.0
jplindstrom,Is one of the developers by any chance called Chris?,1542806861.0
agenderphobe,"`truncate DOT_Contacts;`

I don't know anything about Django or anything, but truncate will effectively give you a new, empty table and the old one will get garbage collected in due time.",1542688923.0
antlife,"Delete from DOT_Contacts;

This is legit. I'm going to assume if it froze, you are having issues with the database itself, perhaps due to some weird key relationship. ",1542703967.0
alinroc,"Did it truly ""freeze"", or was it just taking a long time because of the size of the transaction?",1542719658.0
mage2k,"'*' is usually fine for listen_addresses.

> 0:0:0:0/32

That should be '0.0.0.0/0', dots not colons and 0 for the mask.

> What exactly should ""0.0.0.0/32"" allow?

That would only allow a connection from a host with address 0.0.0.0",1542658106.0
swenty,"    create table work (
        time numeric(8,1) check (time * 2 % 1 = 0)
    );",1542610227.0
simcitymayor,"Look at the `interval` type, as it is specifically designed to handle lengths of time.

From there, you can add check constraints either to allow only specific values, or say if you wanted to allow any multiple of 15 minutes up to a full day
```
CONSTRAINT rounded_to_a_minute CHECK (hours_worked = date_trunc('minute', hours_worked)),
CONSTRAINT gotta_be_a_15 CHECK (extract(minutes from hours_worked) IN (0,15,30,45)),
CONSTRAINT nobody_works_that_hard CHECK (hours_worked <= 24)
```",1542610235.0
mozarcik,I believe you wanted to use `percentile_disc` instead of `percentile_cont`,1542533142.0
h2o2,Why not populate a new table and then just rename-flip it with the old one?,1542453730.0
jringstad,"You could just have two tables, `data_a` and `data_b`, with a materialized view `data` that points to `data_a`. Then you refresh the data into table `data_b`, and in the same transaction change the view to point at table `data_b`.

That way from the outside it'll look like there's just one table `data` that got atomically updated.

Alternatively you could of course also just update the table in-place without ever dropping data, if that's feasible.",1542453828.0
alinroc,"Does Postgres have table partitioning? You could load the data into a staging table, then switch the partition into the main table. In SQL Server, this takes about 1ms.",1542461073.0
swenty,"The name of the product is PostgreSQL or just Postgres for short. In the 70s Mike Stonebreaker designed a database system called Ingres (pronounced ""In Gress""). When he and Larry Rowe began a project to address some of the limitations of earlier systems, it was after Ingres, therefore Post-Ingres — Postgres. The name is also a sort of pun on infix and postfix notation in computer science.

Yes, you can certainly build a CRUD application on top of views over relational tables in Postgres. Views are often used when a particular subset or other manipulation of the data is required for several different queries. If your application doesn't have that requirement, you could have the CRUD application query the relational tables directly. 

It really depends on what you are trying to do as to what is the best design. A good design is one that most simply embodies and expresses your intention.",1542351234.0
KitchenDutchDyslexic,"> creating views for relational tables and making crud over them ?

Yes, look at the postgresql rule system then your crud app will be able to insert into the views as well.

https://www.postgresql.org/docs/current/rules-views.html

> good design

Always debatable with trade offs, pros and cons.

- Inheriting a project and you want to do a clean integration on top of the current db modal; Sure might be viable. 
- Want to scratch a itch after you created your own mustache swagger-codegen templates to generate CRUD interfaces; Maybe not so much.",1542383960.0
snotsnot,I would be careful not to place too much logic into the database. What if you want to run your unit tests in an in-memory database? Or change database altogether?,1542406214.0
graycube,"&#x200B;

I usually separate the file system for the wal logs from the file system for the data.  I also usually mount the statistics as tmpfs.  Make sure to use extensible file systems (ie xfs, ext2, zfs)

&#x200B;

Configure huge pages.

&#x200B;

Don't run it on 5432 if you are in the cloud.  There are too many bots out there polling well known service ports to see what they can get away with.

&#x200B;

&#x200B;",1542323060.0
jlrobins_ssc,"What graycube said, plus developing your upgrade / patching / replication plan. Point releases come out every month or so. How are you going to upgrade? Major releases every year -- how to upgrade?

Without replication and a second server, the point releases usually only need a very brief service discontinuity: shut down, swap binaries, turn back on. But a major version upgrade requires either a full dump / restore, or a run of pg_upgrade.

With binary replication, you can can further reduce minor version upgrade downtimes through first upgrading the replica, then forcing failover later on (along with coordinating with your application layer and / or load balancer like pgbouncer).

With logical replication, you can theoretically do a similar thing but across major versions. I've not ever done that though, so YMMV!",1542330297.0
aka-rider,"EC2 nodes fail rather often. 
Installation and configuration should be automated if you don’t want a big downtime. 

If you care about an availability, you may want to setup replication and failover plus switchover. In this case, you may want to add something like pgpool. 

Backups **MUST** be tested regularly. 
You may want to store backups to S3, clean old backups. 

I get WAL corrupted several times with the default Postgres config on Amazon EBS / XFS file system after EC2 node failures - have no idea why. ",1542335943.0
jimmyco2008,"I mean it sounds like you have it figured out. Basically instead of a web GUI to flip settings through, you are SSH’ing into a Linux server directly and then configuring the various Postgres-related services CLI instead of GUI. It’s not as glamorous as a web GUI, but it allows for more advanced configuration and is considerably cheaper.

MySQL has MySQL Workbench, which is kind of (*kind of*) like having a web GUI like AWS to manage everything “MySQL” through, but Postgres, as far as I know, has nothing as comprehensive as MySQL Workbench. Although in my experience Workbench is buggy af.",1542322077.0
upbeatlinux,"Setup a replication plan. Have a hot-standby in a separate availability zone or multi-availability zones and instrument/automate your failover. pgbouncer + haproxy can be useful here.

If you're backing up to s3 (maybe using wal-e?) ensure backups are purged on an interval and as a fail-safe set an s3 bucket policy to expire object olders than your retention period.",1542343433.0
simcitymayor,It's been proposed: [https://www.postgresql.org/message-id/1487773980.3143.15.camel%40oopsware.de](https://www.postgresql.org/message-id/1487773980.3143.15.camel%40oopsware.de),1542238284.0
therealgaxbo,"I'm not sure what the reasoning is (presumably stems from one of the standards), but I will confess to having created somewhere in the hundreds of subqueries called 'fds' in my life.",1542235420.0
Citizenfishy,It's there for namespacing column names,1542235794.0
boy_named_su,`users`,1542140056.0
tof,"Don't answer ! This is a trap !

This will be used by script kiddies with their sql-injection auto bots shenanigans.",1542144654.0
igncampa,"tenant, customer, person

or simply use 'users' and deal with it",1542137628.0
stlbucket,app_user,1542161089.0
cvboucher,user_profile,1542154795.0
caynebyron,"I use 'account', but that's not great in some situations I've found.",1542161671.0
FallingIdiot,"Why don't you name it `""user""`? Works fine for me.",1542202730.0
dnljsn,"Yep, I’ve been pulled to the dark side. Plural it is. Edit: giving 'account' a try thanks to @canyebyron's suggestion",1542141443.0
akerro,"User is very generic so usually things I design user is a separate class that means username + password + email (some other metadata like account creation date, IP address etc). I always try to keep user minimal and just for authentication purposes, this allows to keep the system minimal and more flexible. Everything else is user details, address, location, tenant, group etc.

Table name is usually principal or user, depending on framework I work with, in SpringSecurity authenticated user is called Principal.",1542139006.0
Fosnez,Minions,1542194755.0
sisyphus,Depends on the app but generally I try to think about what kind of user they are--for example I've used 'agent' inside of a crm and tend to 'customer' in generic saas kind of things.  Funny thing is in my early days I was using an ORM all the time which made 'user' as reserved word transparent to me and I didn't even notice it was a slight pain to work with until later.,1542142111.0
agenderphobe,principal,1542149946.0
etrnloptimist,try to change what makes sense to you,1542137061.0
irrational_design,"I always append a domain specific string to the start of every database table. If I worked for eBay I might have a ebay_user table, a ebay_product table, etc. I can’t tell you how many times that has saved my bacon. ",1542143695.0
andersdd,'usr',1542187445.0
wholt08,UserMaster,1542196243.0
jplindstrom,"Sometimes it's obvious (and useful) to distinguish what type of user it is.

* customer
* staff
* client

Depending on your domain you'll have more or better options.

Or, just use `user`. Quote it when needed. No biggie!

    create table ""user"" ( id int, name varchar );
    insert into ""user"" values (1, 'Johan');",1542198508.0
cyancynic,users - table names are plural.,1542213208.0
theartofpostgresql,"My article at https://tapoueh.org/blog/2018/07/postgresql-listen/notify/ explains how to implement real-time subscription in details, with a full Go implementation of a client.",1542228823.0
logi,"This looks quite interesting. We've got APIs and clients polling for updates (with some knowledge of when they are likely) and Hasura could be a much cleaner approach.

However, we have progress and meta data in postgresql but large and frequently changing data in binary files on disk. Is there any way to make graphql responses from that data as well?

I'd love for clients to subscribe to a query and when the progress changes in DB they get fresh data from the binaries as well. ",1542179177.0
tclineks,You might be interested in https://github.com/tmc/pqstream.,1542352919.0
klaxian0,Can you post the schema of saferdb_dot_contacts table? Does the capitalization match the quoted value?,1541900785.0
fullofbones,"Whoever created this table made the  common mistake of quoting the original column name. In this case, the column can only be addressed as ""DOT_Num_id"" based on the schema screenshot. 

SQL is case-insenitive *unless* names are quoted. As a result, most in the community strongly recommend never double quoting identifiers. Personally I'd drop and recreate the table properly,  before you end up having to quote every column in every query in order to exactly match the table definition. ",1541942022.0
andersdd,"If you are asking for help learning stored procedures in c++, you would be far better off learning plpgsql.  It's not that hard, and there are a tonne of examples already for almost every use case possible.

As plpgsql is native, you don't need to install cpp extensions and they are probably more optimised and performant. ",1541790234.0
agenderphobe,"As someone wearing a DBA hat right now, the thought of the devs I support developing C++ stored procedures sounds like a nightmare. ",1541837987.0
iiiinthecomputer,"Writing reliable, correct procedures in C is not easy. You need to learn a lot about PostgreSQL's internal mechanics. Memory contexts, VARLENA, TOAST, TupleDesc, fmgr, so much more.

Really. Use plpgsql. Or pl/python or something.

But the best procedure language is usually SQL.",1541811684.0
doublehyphen,"There are valid use cases for writing stored procedures in C, but as others have pointed out it requires some familiarity with the internals of PostgreSQL so in general I would advice against it.

There have been several good talks on this subject at conferences but I sadly do not think any of them has been recorded. You can see the slides of one [here](http://hlinnaka.iki.fi/presentations/PGConf.eu-Serverside-Programming-in-C.pdf). What I did myself to learn was to look at the implementations of various extensions (they can be found under contrib/ in the PostgreSQL source code) and PostgreSQL's built-in functions.

I do not know of any examples in C++, but it should basically work the same as writing in C. But since you probably want to use PostgreSQL's own memory allocation functions it will probably be more of a pain to use C++ than the possible gain.",1541816824.0
theartofpostgresql,"Have you tried writing the procedures in plain SQL yet? It's by far the best way to do it, when it's possible, and it's very often possible to solve the API in plain SQL.

Next, sure you can write stored procedures in C or even C++. Have a look at PostGIS for a complex C++ example. What language is your application coded in, anyway?",1541804048.0
threeminutemonta,Just a reminder that the procedure is a new  keyword to PostgreSQL 11 as it allows transactions. Previously I used function interchangeably.,1541841323.0
FunDeckHermit,"I used plpython3u last month on PostgreSQL 11, worked like a charm and was very easy to use. Here are the resources I used:

* [link1](https://dev.nextthought.com/blog/2018/09/getting-started-with-pgsql-plpythonu.html)
* [link2](https://www.fullstackpython.com/blog/postgresql-python-3-psycopg2-ubuntu-1604.html)
* [official documentation](https://www.postgresql.org/docs/11/plpython.html)

Setting it up was difficult because of the u in plpython3**u** means it is untrusted. ",1541793889.0
lbmn,"I'm looking forward to PostgreSQL development with a high-level library in a language like [Nim](http://nim-lang.org/), fully wrapping all the C stuff ([pq](https://www.postgresql.org/docs/current/libpq.html), [xfunc](https://www.postgresql.org/docs/current/xfunc-c.html), [spi](https://www.postgresql.org/docs/current/spi.html)) with much better syntax, safety, and comparable native performance.",1542512648.0
judasferret,"There's a bunch but no c

See them all here
https://www.postgresql.org/docs/current/external-pl.html",1541800779.0
Sedifutka,Looks like you must create an additional role named `app_ldd_readonly`,1541771774.0
obrienmustsuffer,"If you could put your example into an http://sqlfiddle.com so we can reproduce the issue, it might be easier for us to debug it and help you. Debugging code without being able to play around with it is hard \^\^",1541699491.0
therealgaxbo,"The query you posted looks fine at a glance (except for the fact your ARPU calculation is the wrong way round...) and certainly shouldn't give you duplicate country codes within a day.  If you're seeing more than one 'BR' as the result of that query, then it must mean that you have multiple country codes that look similar but are actually different (e.g. `'BR', 'Br', 'BR '` etc)

...or that the query you posted is not actually the same one that gave you duplicate results.",1541759123.0
gengengis,"It's going to take me a long time to get used to the new versioning scheme.

When I think about it, I don't have any real objections to it, their logic basically makes sense.  But I still don't like it.

For those who don't know, 11.1 is just a minor bug fix release to 11, the last major version.  The next feature release will be 12.",1541695736.0
swenty,"This groups the sub-tables using correlated sub-queries. The to_json call at the end puts the whole employee record into a single json object. You get one row in the result set per employee record.

    with f as (
        select e.*, 
        (
            select json_agg(s.*) 
            from skills s 
            join employee_skill_map sm on s.id=sm.skill_id 
            where e.id=sm.employee_id
        ) as skills, 
        (
            select json_agg(r.*) 
            from roles r 
            join employee_role_map rm on r.id=rm.role_id 
            where e.id=rm.employee_id
        ) as roles 
        from employee e
    ) select to_json(f) as employee from f;


",1541673382.0
exhuma,"Jesus christ, that new ""FancyPants"" editor in the new Reddit, really manages to butcher preformatted text...",1541659607.0
jossser,"Ok. here is the query: 

&#x200B;

select 

employee.\*,

json\_agg(skils.\*) as skills, 

   json\_agg(roles.\*) as roles  

inner join employee\_role\_map on employee\_role\_map.employee\_id = [employee.id](https://employee.id)

inner join roles on employee\_role\_map.role\_id = [roles.id](https://roles.id)

inner join employee\_skill\_map on employee\_skill\_map.employee\_id = [employee.id](https://employee.id)

inner join skills on employee\_skill\_map.skill\_id = [skills.id](https://roles.id)

group by [employee.id](https://employee.id)

You may need left joins depending on the schema 

&#x200B;",1541665186.0
antlife,Pgadmin4 is trash.,1541654562.0
mage2k,You're not likely to find a pre-built binary of any sort for a postgres version that old on a modern distribution of any OS but  they keep the source for all the old versions on the [main site](https://www.postgresql.org/ftp/source/) if you want to try building it.,1541613602.0
jenkstom,"Assuming grades is a one-to-many (i.e., there are multiple records in the grades table for each book) then you want to use an aggregate function. Something like this:

select id_book, avg(grade) from grades group by id_book;

For more info, check the [docs](https://www.postgresql.org/docs/9.2/functions-aggregate.html) on aggregate functions.",1541601934.0
MonCalamaro,"I don't think this will handle putting quotes around fields with commas or line breaks in them appropriately.  I think it's easier and safer to just do something like this from psql:

&#x200B;

    \copy (select * from SCHEMA.TABLE) To 'export.csv' With CSV;

You could also put this in a file and use the -f switch from the command line",1541598696.0
casual__addict,"If you are going to put a table to a CSV, presumably you want to pipe it into a follow on command or use it for something like creating a python pandas DataFrame, so having the footer of “(n rows)” is annoying. If you include this in your cli args `-P “footer=off”` then it...turns the footer off. It does keep the header row on though.

Check out https://www.postgresql.org/docs/9.2/app-psql.html, specifically the `\pset` section.",1541643929.0
jakkarth,"update mytable set mycol='new value' where locked='false'

? Not exactly rocket surgery.",1541551013.0
,[removed],1541549418.0
wolf2600,"    select rcid, scssn, count from myTable
    where (rcid, count) in
        (select rcid, max(count) as count from myTable
         group by rcid);
",1541552283.0
KitchenDutchDyslexic,"> as thats the latest version in the repos.

Nope... you will soon learn these pgAdmin4 guys are moving fast!

[v3.5 was released 2018-11-01.](https://www.pgadmin.org/), 6 days ago. 

For interest sake see they moved to Boostrap 4, don't think it is your issue?

From your description I'm not sure what is not working or not 'there'. Maybe a screenshot or two of what is missing?

The book maybe not using pgAdmin3?

",1541571466.0
MonCalamaro,"Try reading this:

&#x200B;

[https://stackoverflow.com/questions/23929707/return-dynamic-table-with-unknown-columns-from-pl-pgsql-function](https://stackoverflow.com/questions/23929707/return-dynamic-table-with-unknown-columns-from-pl-pgsql-function)

&#x200B;

Since you know the table name, using the polymorphic type would be an interesting solution.  The solution that returns a cursor could also work for you, but it might take some extra work on behalf of the caller.",1541526742.0
obrienmustsuffer,"hm, something like this?

    UPDATE main
    SET field2 = lookup.field3
    FROM lookup
    WHERE id = lookup.id

Completely untested :)

https://www.postgresql.org/docs/9.6/sql-update.html",1541509852.0
thelindsay,"The correlated sub-query is fine, except that a LIMIT without a ORDER BY will give you effectively random results on a non-unique id match.

Check your EXPLAIN output to see why it's slow. Are you getting sequential scans? Best you can aim for is index scans. If your two ""id"" columns are primary keys (or unique +/- not null) I'd expect index scans. 

Might be more useful to show your actual problem since you might have abstracted away the real problem in this example.",1541510051.0
nikoz84,"`with data as(
Select id, field3 from lookup where id = 23
) Update main set field2 = ( Select field3 from data where data.id = main.id) `

>UPDATE main m SET field2 = (SELECT field3 FROM lookup l WHERE m.id = l.id LIMIT 1) 

",1541509892.0
KitchenDutchDyslexic,"[**Video Here:** Click on the white icon next to the ""Dependents"" window heading and move it to the tabs between ""Statistics"" and ""Dependencies"".](https://imgur.com/a/9sGQrfN)

It should change ""Dependents"" to a tab and move the window into the tab area.",1541487011.0
gregmnagy, How to index your tables to make your queries perform far more efficiently. ,1541426995.0
simcitymayor,"Sigh.

[https://www.postgresql.org/about/press/faq/](https://www.postgresql.org/about/press/faq/) (how to pronounce postgresql)

[https://www.postgresql.org/files/postgresql.mp3](https://www.postgresql.org/files/postgresql.mp3)

Tried to comment on the article but the comment login crashed.",1541525135.0
r0ck0,"Yes, go ahead!",1541425471.0
tektektektektek,"In Linux try:

    tar -cvzf /tmp/backup.tar.gz $(find /opt/postgresql -mtime +93 -type f)

Let me break down what that does:

* `tar` - is a program used to create an archive
* `-c` - creates a new archive
* `-v` - verbose output
* `-z` - compress using gzip compression method
* `-f /tmp/backup.tar.gz` - file to output the created archive
* `$(...)` - run the command inside the expression and use the output
* `find` - a command that scans the file system for filenames that match criteria
* `/opt/postgresql` - the base directory to start finding from
* `-mtime +93` - only filenames that have been modified 93 days or more ago
* `-type f` - only output file names, not directory names",1541456269.0
cachedrive,"What are you trying to ask? Your post is a statement -vs- question.  


You should use S3 buckets and have a clean up script based on mtime.",1541446283.0
KitchenDutchDyslexic,"How does your query lang compare against [mango](https://github.com/cloudant/mango) or [pouch-find](https://pouchdb.com/guides/mango-queries.html)?

/u/dopperpod url is correct in just learn the pgsql json operator and function(s).",1541515749.0
dopperpod,https://www.postgresql.org/docs/11/static/functions-json.html,1541339275.0
IndianaGunner,"Connection pooling, although I am curious why it absorbs memory and never releases it. 2nd part is side question.",1541363200.0
alephylaxis,"It's pgAdmin running stats queries for the charts and stuff it creates. As far as the ones that are idle and still there, probably still in the same session and that's the last query that session executed. ",1541365117.0
boy_named_su,"Data Scientist in Business Intelligence, which is part of Marketing. Boss is VP Analytics",1541280067.0
jringstad,"I'm confused by your first question -- casts never prevent SQL injection. The Django ORM hopefully uses parameterized queries to do that.

For your second question, you can index on any column (primary or not doesn't really matter), and you can have computed indices on any column (or combination of columns, even). If you want a case-insensitive comparison to be as fast as possible, you can create a computed index on a column `columnA` that doesn't index `columnA` directly but that indexes `LOWER(columnA)` instead.

You can read the documentation on that here: https://www.postgresql.org/docs/9.3/static/indexes-expressional.html",1541245180.0
jrwren,"There is a list of books at postgresql.org

click ""Documentation"" at the top. Click the ""Books"" link on the left.

or click here: https://www.postgresql.org/docs/books/",1541263322.0
theartofpostgresql,If you're an application developer I can recommend [https://masteringpostgresql.com](https://masteringpostgresql.com) ; I wrote it!,1541439155.0
KitchenDutchDyslexic,"I like that pgnotify is in py.

Another solution is to use something like [pg-amqp-bridge](https://github.com/subzerocloud/pg-amqp-bridge)(rust) to send it to rabbitmq and then pick it up with mqttwarn, pika or celery in py land.",1541226266.0
jk3us,I've removed this as spam.,1541193206.0
gregmnagy,Understand what is going on under the hood of your queries.,1541171734.0
grovemau5,"7.5 lakh = 750k rows? How are you creating the csv, through a script? Probably out of memory issue, you should take an iterative approach",1541136265.0
Foodei,Does it work if you limit the rows to one lakh?,1541170280.0
ppafford,Use the CLI for PostgreSQL https://gist.github.com/nepsilon/f2937fe10fe8b0efc0cc ,1541201488.0
softwareguy74,lakh?,1541255716.0
So_average,"Those are not large databases. An index is only created on one table, so the index isn't on that 50 or 80gig. Taking so long could mean some sort of resource issue like disks. ",1541072516.0
flyingmayo,"I'm assuming these indexes are indexing the same type of data (e.g. that these databases all have matched schemata) and that you are accounting for varying table sizes already.  If not then this is not a useful comparison to make.  

If you are comparing apples-to-apples operations, and the performance differences you're seeing can't be explained by a simple caching benefit (e.g. if you load data into a database and then immediately index that data, ya that's going to be fast because it's likely some portion of the newly loaded data is going to be sitting in cache) then there's going to be something interesting to learn from your IO metrics.  

What does your iowait look like when you see fast index creation?  What does it look like when the index creation gets slow?

What's your IOPS?


",1541091489.0
haloweenek,"Index creation time heavily depends on the index type. 
Pg stores the db’s in the datadir. 
If the dB is in operation during index creation that impacts the creation process.
How to make it faster ? SSD’s or NVMe’s (I/O) and loads of RAM.",1541072612.0
SomeGuyNamedPaul,Are you on SSDs that slow down so they fill up?  Are the other databases being accessed at the same time as you're loading the newer ones?,1541073962.0
i-eat-kittens,"As mentioned by others, you probably have some kind of resource contention going on. Most likely related to (mis)configuration.

What are your system specs? Ram/number and type of drives and raid level/cpu?

And what are your postgres settings? (dump the contents from ""show all;"" on some pastebin). Did you read https://www.postgresql.org/docs/current/static/runtime-config-resource.html and modify the defaults to something sensible for your hardware?",1541102655.0
cazzer548,"Sharing the plan with `explain analyze` would be helpful, otherwise we'll just be speculating.",1541027982.0
zomb1e-dust,Do you have any indexes?,1541025694.0
alephylaxis,"I'm not sure exactly if the column names, etc will be valid (hard when you can't see the DB). But here's what I thought to try. At least run it through the planner and let's see if it improves the cost at all. At least it got rid of some of the inline filtering.

&#x200B;

`WITH properties AS`

`(`

`SELECT sid FROM prod.vw_property WHERE status = 'active'`

`),`

`cl_not_expired AS`

`(`

`SELECT expired_at, listed_on, list_price, interior_size_sqft, rew_properties_id, property_type_group,`

`land_size_sqft_lower, land_size_sqft_upper, interior_size_sqft_lower, interior_size_sqft_upper,`

`num_bed_lower, num_bed_upper, location`

`FROM prod.vw_comparable_listing`

`WHERE`

`expired_at IS NULL`

`AND`

`interior_size_sqft > 100`

`),`

`cl_30_day AS`

`(`

`SELECT listed_on`

`FROM prod.vw_comparable_listing`

`WHERE`

`listed_on >= ((now())::date - '30 days'::interval))`

`AND`

`interior_size_sqft > 100`

`)`

`SELECT p.sid,`

`-- active list count means not expired`

`count(cl.expired_at) AS list_count_active,`

&#x200B;

`-- 30d list count DOES include those that are now expired (ie. listed and expired quickly within 30d)`

`count(cl_30_day.listed_on) AS list_count_30d,`

&#x200B;

`-- All other aggregates should only include active (not expired) rew listings`

`avg(cl.list_price) AS list_price_avg,`

`percentile_cont(0.50) WITHIN GROUP (ORDER BY cl.list_price) FILTER (WHERE cl.expired_at IS NULL)::numeric(12, 2) AS list_price_median,`

`min(cl.list_price) AS list_price_min,`

`max(cl.list_price) AS list_price_max,`

`avg((cl.list_price / (cl.interior_size_sqft)::numeric)) AS list_price_per_sqft_avg,`

`percentile_cont((0.50)::double precision) WITHIN GROUP (ORDER BY (((cl.list_price / (cl.interior_size_sqft)::numeric))::double precision))`

`FILTER (WHERE (cl.interior_size_sqft > 100) AND cl.expired_at IS NULL)::numeric(12, 2) AS list_price_per_sqft_median,`

&#x200B;

`-- Show up to 10 closest listings that were used to get the aggregate numbers`

`(array_agg(cl.rew_properties_id ORDER BY (st_distance(p.location, cl.location))))[1:10] AS closest_rew_properties_ids`

`FROM prod.vw_property p`

`INNER JOIN cl_not_expired cl`

`ON`

`((((COALESCE(upper(p.land_area_sqft), 0))::numeric >= cl.land_size_sqft_lower)`

`AND ((COALESCE(upper(p.land_area_sqft), 0))::numeric <= cl.land_size_sqft_upper)`

`AND (((upper(p.interior_area_finished_sqft))::numeric >= cl.interior_size_sqft_lower)`

`AND ((upper(p.interior_area_finished_sqft))::numeric <= cl.interior_size_sqft_upper))`

`AND (((p.num_bed)::numeric >= cl.num_bed_lower) AND ((p.num_bed)::numeric <= cl.num_bed_upper))`

`-- Within 2km`

`AND st_dwithin(p.location, cl.location, 2000) AND (`

`CASE`

`WHEN (p.rew_property_type = ANY (ARRAY['duplex'::prod.rew_property_type_enum, 'fourplex'::prod.rew_property_type_enum, 'triplex'::prod.rew_property_type_enum])) THEN 'townhouse'::prod.rew_property_type_enum`

`ELSE p.rew_property_type`

`END = cl.property_type_group)))`

`JOIN properties ps ON ps.sid = p.sid`

`GROUP BY 1;`

&#x200B;

&#x200B;",1541031821.0
mokadillion,So many filters for the same thing. Maybe try writing them into a single where clause. Not sure why you would use the common table expression ? This is causing lots of loop processing also. ,1541028556.0
FunDeckHermit,Upgrade to PostgreSQL 11,1541024681.0
webkrsna,"Does all the keys in the keyboard work properly. Try typing the password in notepad. 

Sometimes there happens that some keys gets double pressed.",1540976265.0
lostburner,"Here’s another seemingly-obvious thing to rule out: some people have a habit of adding a trailing space when they’re done typing. Double check that there’s no trailing space in the password, and that you’re not accidentally adding a trailing space while typing. ",1540998379.0
flyingmayo,"I'm also thinking that you're dealing with the age old issue of windows helping you out by tacking on extra characters to your text in notepad.

quick fix:

* connect by whatever means works and then change your password 

> ALTER USER foo WITH PASSWORD 'bar';

* while still connected with that session, verify via another session that you are able to connect with a typed password. 

Or, learn something along the way:

* connect by whatever means works
* compare your stored password/username hash to your expected password/username hash using something like the following

> postgres=# create user testuser with password 'testpass';

> CREATE ROLE

> postgres=# \x

> Expanded display is on.

> postgres=# select 'md5'||md5('testpasstestuser') AS expected_hash, passwd AS actual_hash from pg_shadow where usename = 'testuser';

> -[ RECORD 1 ]-+------------------------------------

> expected_hash | md599e5ea7a6f7c3269995cba3927fd0093

> actual_hash   | md599e5ea7a6f7c3269995cba3927fd0093


e.g. if those hashes don't match, then you probably have extra characters in your password",1541014570.0
boy_named_su,"You could put PostgREST(http://postgrest.org/en/v5.1/) in front of Postgres and call that from a JavaScript web page
",1540923525.0
lostburner,"Is your application a JavaScript app running in the browser? If so, then you’ll have to have a server layer to interact with the database. It’s untenable to make ODBC connections straight from the browser, because of the security hole that would open up. 

In your server-side application, any programming language will have libraries for connecting to SQL databases and returning your query results in any representation you like (native objects or arrays, most commonly). Can you describe your app setup more? What language is it and where is it running?

Edit: security hole typo",1540933407.0
swenty,"It's not obvious why the column gene_sequence would be included in the primary key. I'm guessing that's the column for which you're getting values that are too large to index. It would be worthwhile confirming that by looking at the data file.

The only value that index will have is if you are retrieving rows by matching the sequence itself. If you really want to do that you might consider building an index on a left substring of the sequence instead.",1540949303.0
pypt,"The sole index that you’re adding here is the primary key on multiple columns which is not a common practice, also causes problems because the indexed columns together are too “wide” (as per the error message).

Just add a separate SERIAL PRIMARY KEY column and adjust your \copy to insert into correct columns.",1540916425.0
anras,"What is the column ""id""? If it uniquely identifies a row that should be your primary key. If it doesn't, you should give it a more descriptive name than ""id"" and probably follow pypt's suggestion (so add a column called ""id"" as a surrogate primary key).",1540917115.0
Chousuke,"If you're using streaming replication, no. It's probably possible with logical replication, but I've never tried so I can't say for sure.

You could use a VIP that the slaves use to connect to the master so that you don't need to edit recovery.conf though: that way restarting the slaves may not be necessary.",1540905400.0
mgonzo,"First, I'll start by saying that is a weird use case, master restarts with new IP, slave does not restart. 

Second, no I don't think you can with streaming. But if you are ok with not using streaming then there is a way to do replication and the slave not care what ip the master is on. 

[https://www.postgresql.org/docs/11/static/warm-standby.html](https://www.postgresql.org/docs/11/static/warm-standby.html)

&#x200B;

You can  actually have both setup, so it streams from the master when available, and when not it simply reads the log files.

So as long as your master is putting log files to the correct shared location  the slave will read them. This has a slightly longer delay as the master has to archive  the wal file. but it works.

&#x200B;

Hope this helps.",1540914191.0
timmyriddle,"I have been doing performance optimisation recently and have found [Markus Winand's site](https://use-the-index-luke.com/) to be an excellent resource. I also bought his book, which is rather short, but to the point.

Some of the latter exercises on [pgexercises](https://pgexercises.com/) cover window functions and some other more advanced topics. However, I suspect that this won't go far enough, based on your question.

It's a predictable answer and perhaps not very helpful, but when I need info on some of specific features such as triggers I consult the PG website itself. It's very well written: clear, concise, and it usually provides examples too.

I'll second your request and say that I'm also interested in discovering some more advanced material in video format.",1540842159.0
Amaracs,"I am with OP, but i am more interested in the performance related topics to know more about the cache and what is happening under the hood on different OS.",1540836788.0
haloweenek,"I don’t use docker since I find no advantages over barebone vps deployments. It’s just a layer that complicates management, but if others like it it’s fine for me.

Now for the sauce:

Generally HA is supposed to eliminate any single point of failure in your system. So any element you think of should be double and ideally tripled. In different availability zones !

1.	3 swarm managers are a good idea. One etcd with one patroni instance in each swarm. I don’t know what happens when the swarm manager dies but I presume that docker is running without interruptions...
2.	For highest survivability it’s recommended to separate etcd from patroni. The etcd should be on different set of machines. So when your db vps dies key/store is unharmed. Smallest possible instances are more than enough for that. So we’re talking about 6 servers minimum.
3.	Running single HAProxy introduces spof in your configuration. I’d recommend running HAp on application machines. It will lower the connection time and I/O penalties. I have that solution on my setup https://severalnines.com/blog/postgresql-load-balancing-using-haproxy-keepalived HAProxy are deployed on worker instances. HAProxy constantly checks the health/status of dB nodes and connects to proper node.

I don’t know about etcd in docker but etcd should persist ALL data between restarts. When the etcd cluster is initialised it should later work with id’s and ip’s it was initiated with. You might run into issues with that. I’m running a hardwired etcd config, id’s strictly matched to ip’s. ",1540853973.0
haloweenek,I’m running a stack like this - without docker. I’ll reply for some of your questions from my computer.,1540831963.0
bar-a-baz,"You can connect to any server that's reachable for you. The server needs to listen on a port that is not firewalled (open port). The server also needs to use the IP address of the public interface (e.g. the ethernet interface). If the server config contains 127.0.0.1 that's not public.

You can check the connectivity on your local box with nmap:

`nmap -p 5432 remote-address`

If it says the port is open, you're good. If it says filtered, check the firewall or the server config. If it says closed, check the server config.

You can also do it with Zenmap the GUI. Nmap and Zenmap are available for Windows, if you have that.

Also make sure the port is correct. 5432 is the default port.",1540684229.0
nikoz84,"You need config the file postgres.conf and add the listen configuración ""*""",1540693633.0
graycube,"A postgres instance running on the same tablet/laptop/desktop/computer that you are using can be considered ""local"".  Anything else is ""remote"".

Our modern networked computers have at least 2, and sometimes many more, IP addresses.  One of those ip addresses is specifically for internal connections and  has a special hostname associated with it:  ""localhost"" .  Localhost may refer to an IPV4 or IPV6 address, that doesn't really matter.

PostgreSQL, by default, only listens for connections from localhost.  If you want it to listen for connections from elsewhere you have to tell it to do so by changing the ""listen"" option in the postgresql.conf.  ""*"" tells it to listen for connections from every network your machine is directly connected to.

Because many network services on your system use the same IP address, they have to share it somehow.  This is where ""ports"" come in, specific numbers on that IP address that are associated with the service we are running.   By default PostgreSQL listens on port 5432.  It doesn't have to.  It could be anywhere from 1 to 65535.  There are lots of conventions and standards around selecting ports.

A PostgreSQL server may have many databases on it.  You will probably want to know the database name you are connecting to as well.

Username, password, hostname, port, and database name.  Get all those correct and you may still not be able to connect.

PostgreSQL ships with a built-in ""firewall"" feature too.  You will have to explicitly configure ""pg_hba.conf"" to allow access as well.  (and reload or restart your database after updating it).  By default pg_hba.conf will allow some connectivity directly from localhost (even without passwords!).  Chances are, however, that you will need to figure out how to update that file to keep out the bad guys and let yourself in at the same time.

And even with that set up there still may be other firewalls in the network that keep you from connecting.  Windows comes with a built-in firewall of sorts.  Linux can easily be configured with ufw or ip tables.  And the network itself may have one or more dedicated firewall appliances between your client and the database you want to connect to.   Chances are, however, if you are on the same LAN, and if the database has already been configured to listen for nearby requests, there won't be any other firewalls to have to burrow though.
",1540754147.0
tsqd,"That looks like Postgres is having trouble getting a handle on that particular file. Verify the file you are expecting is present relative to the path the command is being run at, and that the file permissions allow for the executing user to read that file.",1540641788.0
zoner14,"What a coincidence. I was trying to load the exact same dataset into PG the other day and was amazed that it wasn't trivial to ingest JSON. Sometimes I use my blog to provide notes to my future self, and I detailed on there what I did there

[https://nickdrane.com/using-jq-to-effortlessly-ingest-newline-delimited-JSON-into-postgres/](https://nickdrane.com/using-jq-to-effortlessly-ingest-newline-delimited-JSON-into-postgres/)

I ended up streaming the data in with jq. Be mindful not to specify foreign key constraints until the entire dataset is loaded, otherwise you'll run into referential integrity problems.",1540658162.0
asdfhomerow,"That error, ""could not stat file: Unknown error"" sounds more like OS configuration issue with user, file, and/or filesystem permissions and config parameters. Initially I was going to say check max file descriptors or somesuch ulimit paramter (/etc/security/limits.conf) and doublecheck file ownership and permissions...

&#x200B;

But closer read of your post I'm now guessing you're using Windows with the '\\' pathing... therefore my first suggestion is to make sure your antivirus/antimalware software is temporarily disabled while performing this operation. Antivirus/malware software is notorious wildcard for messing with file operations like this.",1540672740.0
kringel8,"The question is right there in the log: ""Is another postmaster already running on port 5432"". Is it? You could try to connect to that port and see if there is a response.",1540579745.0
scttmthsn,"Postgres.app is the easier option on MacOS, and it comes with PostGIS.",1540651500.0
regex1884,Try installing it in docker.  ,1540736143.0
mage2k,See [here](https://github.com/postgres/postgres/blob/REL_11_STABLE/src/backend/access/hash/hashfunc.c).,1540575366.0
gregmnagy,How to keep PostgreSQL updated on the status of our tables so it can run queries as efficiently as possible. ,1540570354.0
ivanceras,"try this:

    psql -h localhost -U jake -W --no-password --dbname=shopping -c ""copy (select * from groceries) TO '/home/jake/Desktop/export.csv' CSV HEADER""",1540502680.0
cachedrive,"You need to go to /etc/postgresql/x/main/pg\_hba.conf and set TRUST auth for jake or configure jake with a .pgpass file to enter his pw.   


If you dump your bottom of the hba.conf in here, we can better help you but your database doesn't allow jake to login w/o a password regardless if you use --no-password. That is your issue.",1540493321.0
regex1884,Try using a shell script next time.  Might be a little cleaner and easier to debug.  ,1540736308.0
urcadox,"I've never gotten this. I feel like there is probably something specific with this index that makes it break so regularly.

Can you tell us more? (index type, is it partial, is it an index on an expression, the data type, ...)",1540461210.0
justAnotherCodeGuy,"Is there a virus scanner between PG and the HD?  Maybe exclude the database folder?

Bad raid controller?  Anything else running on the box that would notice?  Maybe enable the crc stuff in PG?",1540478084.0
jam507,"Please, don't use windows for a production database.",1540467457.0
pypt,What filesystem are you running this from?,1540463553.0
gregmnagy,"Hey guys, in this article we focus on understanding a narrow but very critical aspect of performance tuning with PostgreSQL. Specifically, we're going to be discussing Vacuum and Autovacuum. ",1540391714.0
reddimato,"There are some interesting answers.

Some of them need an update:

>What will be the new characteristics of Postgre 9.1?

&#x200B;",1540375344.0
Amaracs,"Nice, but I am missing the security related questions which are definietly will be asked in an interview.",1540367088.0
linuxhiker,You may want to look at Sqitch.,1540324082.0
jk3us,"I've removed this as blogspam.  1) We don't need ""How to set up postgres on <every distribution>"" and 2) it's been a while since you posted anything except links your site. So, please take a break from doing that and participate in the community in other ways.",1540329222.0
FunDeckHermit,"""getting started with postgresql 11 and buy our application"" would be a better title. ",1540325691.0
pypt,"Triggers get run for every row, so yes, 1000 times.

To advise on how to do this better, we’d need more details on what your schema looks like, what are you inserting and what is it that you want to achieve.",1540287840.0
goblando,What is your network layer like?  Are you on the same local lan?  Over vpn?,1540245379.0
vladimirice,"you can avoid DNS call if you use IP addresses, not domains. Try to do this also. 

Also you can measure network roundtrip separately. Measure simple ping request time.

Be sure to test this issue more than once, twice, etc, because production server might be affected by another requests, for example which are blocking record for select. And such delay might not related to the instrument you use. ",1540274188.0
twocentsrworth,"Update: instead of select query, created a function to do select job. It is showing some improvement. 
",1540385598.0
geocompR,Did you build the same indexes on your production server as you did on your local machine? With geographic data / PostGIS it is often important to build an index on your `geom` column.,1540257961.0
i-eat-kittens,"How much data is your query returning? Is 70 ms unreasonable for the (full) transfer across your network?

Perhaps psql reports time until the first row is returned, while your app/pgadmin fetches the full result set first. What does ""explain analyze"" from pgadmin say?

> Table has geometry data, some 34 rows, uses index scan

Columns? Anyways, if the query itself completes in 2-4 ms, I guess your query plan is OK.",1540283408.0
zoner14,It doesn't look like anyone here has mentioned the caching. Is it possible that the cache was warm during the local tests but not warm when testing through the application client? Could the difference just be disk reads?,1540658723.0
pypt,You can EXECUTE a generated SQL query from within your function.,1540235931.0
mage2k,"    foo=# select string_agg(quote_ident(a), ', ') from unnest(ARRAY['Blah', 'huh', 'Bar', 'baz']) as a;
           string_agg        
    -------------------------
     ""Blah"", huh, ""Bar"", baz",1540232995.0
threeminutemonta,"I've had need for this type of dynamicness before. The below did the trick for me. 
```
CREATE OR REPLACE FUNCTION update_user_after_translation(_id integer, _column_name varchar, _new_value varchar, _translation_ref_id integer, _new_import_date timestamp) 
RETURNS integer as $$
DECLARE
_found_column boolean;
BEGIN

	_found_column := (SELECT TRUE
			FROM   pg_attribute
			WHERE  attrelid = 'public.users'::regclass
			AND    attname = quote_ident(_column_name)
			AND    NOT attisdropped);

	IF _found_column is True THEN
		EXECUTE format('UPDATE users SET %I = $1,
				additional_reference_loaded_date = now()
				WHERE id = $2', _column_name)
		USING _new_value, _id;
		UPDATE import_reference_ids SET timestamp_processed	= _new_import_date, user_id = _id WHERE id = _translation_ref_id;
	END IF;
RETURN 1;
END; $$ LANGUAGE plpgsql;
```
Edit: I didn't see all your detail when I viewed on the phone and I didn't address any of your questions. I doubt this is too useful though I'd keep it anyway. ",1540262280.0
justAnotherCodeGuy,"Have you seen https://explain.depesz.com/

Here is your query:

https://explain.depesz.com/s/7jbC

Looks like you have a few seq scan's.  Check those will large row counts, see if you can get it to use an index.
",1540219891.0
mwdb,"Ideally we would have CREATE TABLE statements and sample data, but just giving this a cursory review, it looks like you may have redundant joins - standard\_workitems appears in three separate joins. Can you restructure the query so that it only appears once?",1540237140.0
therealgaxbo,"`insert into test1(test1_md5) select md5(random()::text) from generate_series(1,50000);`",1540208532.0
mrweck,I believe that your primary being a bigserial will create new surrogate keys as needed.,1540204267.0
Foodei,Based on your use case/description I would go with one table.  The restaurant table sounds like a master data table to me.  Now if you run transactions against a restaurant you should have separate table(s) (for the transactions).,1540182705.0
sazzer,"How many rows are you expecting? Have you done any performance testing on it?

Just out of interest, I went and did some 10s Googling, and found a StackOverflow answer that suggested some tweaks (Clustering the table on the appropriate value) that made a difference on a table with *218 million rows*. I'm assuming that you're going to be nowhere near that level here, so almost certainly if you just put an index on your `restaurant_id` column then you'll be fine.",1540198495.0
Cabelitz,"Segment the table by the most general aspect, then segment further with views.  


For example, segment by area (north, south, east, west), or by expertise (mexican, japanese), or by type (fast food, a la carte, dinner), then segment creating views.  
You can create a view for each criteria of segmentation, for example a view for restaurants open during the day. Another for night places.",1540181909.0
dtechnology,"I'm not sure how many restaurants there are in your use case, but I assume it less than 1 million and 10GB of data.

In that case, choose whatever is easier, one big table. It's not a problem to optimize later, e.g. by partitioning the data.

As long as the whole table + indexes fits in the database RAM, you're not going to get much improvements from splitting the data.",1540227488.0
jeasoft,"[https://dbeaver.io/](https://dbeaver.io/) it's a life saver, to generate a ""graph"" from your database structure. It's open source!",1540098801.0
jeffk,"You will definitely be able to find tools that do this:

https://wiki.postgresql.org/wiki/Community_Guide_to_PostgreSQL_GUI_Tools

To narrow down choices, you may want to decide between free vs. commercial software, GUI vs. command line:
",1540081105.0
Mamoulian,"https://pgmodeler.io

It's not free unless you want to compile it yourself (was straightforward on Linux) or want to use a docker image, but it is only $16.50 and has a free demo.

It's got a couple of quirks but it's been good for me with our ~30 table schema. It's got a cli tool which we use do exports as part of CI.",1540114966.0
romaia,I am pretty sure this image you used as example was generated by schema spy: http://schemaspy.sourceforge.net/,1540157789.0
jakkarth,"Looks like several things wrong there. city presumably has more than one column, but you're only defining one column for your insert, so that'll mismatch. Also, you don't use values() with insert...select. See [here](https://www.postgresql.org/docs/10/static/sql-insert.html), you can use either default values, OR values, OR a query.",1540068619.0
elvy399,"Don’t know if you managed to fix you problem. 

Change the “*” to a single column name or increase the columns you want to update to all the possible columns in the “city” table. 

Good luck:)",1540071557.0
jeasoft,"This is not an answer, but maybe you can try this tutorial about SQL from Khan Academy. It's a great point to start in SQL, no matter your SQL flavor (MySQL, PostgreSQL, etc)

[https://www.khanacademy.org/computing/computer-programming/sql](https://www.khanacademy.org/computing/computer-programming/sql)",1540098137.0
i-eat-kittens,"You are aware that what you're (apparently) trying to do here is nonsensical and ill-advised?

Country data really don't belong in your city table. Cities have their foreign key to Country, and [that's where it should stop](https://en.wikipedia.org/wiki/Database_normalization). Whenever you want to list your cities along with some relevant country data, you issue a select statement joining the two tables; no inserts are called for. Remove everything but the ""SELECT ..."" part of your query.

By all means, keep at it if this is purely an exercise.",1540150702.0
Darth_Vaporizer,You should be able to do this fairly easily. Can you post your docker-compose.yml or the Docker commands you’re running?,1540065571.0
threeminutemonta,"I had some luck with using -v for volume when using the docker run command. The following docker uses postgres and installs the postgis extension. 
```
cd /Users/threeminutemonta/Downloads/playpen
git clone https://github.com/appropriate/docker-postgis.git
cd docker-postgis/11-2.5/alpine
docker build -t alpine-pg11-postgis2.5 .
docker run -p 5433:5432 -v /Users/threeminutemonta/Downloads/postgresql-11/data:/var/lib/postgresql/data alpine-pg11-postgis2.5
```

Edit typo",1540162326.0
SomehowAnActualAdult,"Josh Berkus has done some solid writing (and presenting) on this.  

Here is a starting place:

http://www.databasesoup.com/2018/07/should-i-run-postgres-on-kubernetes.html?m=1",1540039464.0
zieziegabor,"In general this is considered a bad idea.  Containerization adds a little overhead, and things like PostgreSQL(PG) tend to be very resource intensive.  You want very resource intensive things to get all the resources they possibly can.

To expand on this, the normal use-case for containers is to put more than 1(usually hundreds/thousands) on a given host.  Given the PG is all about wanting all the resources, you can see a future problem here. ;)

Also, containers are built to bring up/down regularly, also something you really don't want out of your PG instance.

PG wants lots of nice stable fast disk.  Containerization isn't really built for that use-case.  (It can of course be sort of done, but it's not a design goal of things like docker).

Or to put another way, the right tool for the right job. :)  Containers have their sweet spot, packaging up python/ruby projects for instance or for stateless microservices or something.

All that said, if your PG Database is small and your workload light, there isn't necessarily anything wrong with shoving PG in a container for a while, until your usage rises to become a problem.",1540035482.0
simcitymayor,"PostgreSQL COPY stops on the first failure, and the error message is returned to the client.",1539919491.0
i_like_postgres,pgloader is a tool which provides advanced data loading capabilities and deals with this issue. ,1540041459.0
matija2209,"Hey guys, I've exported very big group chat on which I'd like to test my Python skills. But first I need to get the data to Postgre. I've issues when importing the data since JSON is not compatible with SQL. I've tested a tool called JsonToPostgres which work amazing but trial offers only 1000 lines and I cannot afford to pay 60$ for it. Does anyone one have any other suggestion how could one do it?

Thanks,

P.s. The size is about 40mb.

",1540069817.0
simcitymayor,"https://www.prowesscorp.com/computer-latency-at-a-human-scale/
Fast, but still more than 10x slower than the slowest RAM out there.
Hence, caching is still needed.",1539970645.0
smugbug23,"> Just now I remembered that talk and checked,

With whom did you check?

Speaking for myself, the main reason I haven't seen the benefits of PCIe is that I haven't bought one.",1540060035.0
thelindsay,"Maybe you could try postgres' table partitioning, on the customer_id or range of customer_ids. Might have a similar effect but without the extra load taken on by Spark.

https://www.postgresql.org/docs/current/static/ddl-partitioning.html",1539965398.0
So_average,"Excellent article, bravo Apoorva!",1539873779.0
jlrobins_ssc,"True TX-controlling stored procedures are huge. Doing large table maintenance in steps and vacuuming in stages between id ranges can now be done in a nice stored proc inside the database instead of shell script foolery.

Parallelism continues to expand to cover more plan types. And then the new JIT infrastructure can begin to make improvements in performance above and beyond and / or 'in parallel' with parallelism for further gains.

In this day and age of bloat, so nice to see something continue to get faster *and* more featureful with age.

The gap between PostgreSQL and MySQL keeps widening, and those between MSSQL and Oracle keep shrinking.",1539876586.0
kenfar,"Beta 4 is available now on AWS in the RDS developer preview area.

But there's no speculation on when we should see it go GA on aws.   Anyone have an educated guess?",1539891069.0
axman44k,Nice features and still hoping for an increase of the default NAMEDATALEN size of 63 chars. ,1539879068.0
Kiniteq,Hopefully native TDE will be added in the near future. Would bring Postgres in line with other databases when it comes to PCI audit. ,1539888984.0
lebogglez,Is there any good reading material to learn about software design based on server procedures etc? My company insists on keeping the logic out of the database and only use it as storage essentially.,1539951497.0
nickmcl23,Does anyone know how long Homebrew usually waits until they add the latest to their formulae?,1540080217.0
Cer_Visia,"Which part of the homework do you have problems with, writing the [constraints](https://www.postgresql.org/docs/9.4/static/ddl-constraints.html), or the queries?",1539850563.0
exhuma,"Update: Added some more useful stats (like disk sizes) and worked on my Grafana dashboard. Looking promising :)

This dashboard might get messy though on a cluster with many databases. But that would be more an issue of the dashboard config rather than the statistics collection.

[https://imgur.com/a/qzWkCDM](https://imgur.com/a/qzWkCDM)",1539808839.0
,[removed],1539754956.0
ivolimmen,Start here: https://en.m.wikipedia.org/wiki/Database_normalization,1539709968.0
boy_named_su,"In an *operational database*, you want one table for each type of thing, in order to avoid *update anomalies* 

If you need to update a value, you only want to update it in one place

(This does not apply to analytic / data warehouse databases)

I'm not a gamer, so can't help directly. However, in say an e-commerce store database, you'd have a table for customers, one for orders, one for order items, one for products
",1539710419.0
beyphy,"> I don’t see why all of this data could not be contained in one table.

It can, but you run into problems when you need to change or update something. Using the relational model, you can just update the record in one location in one table. Once the update is made, it's reflected through the rest of the database whenever it's queried with other tables. You don't have to change it in every single place it appears, which would take significantly longer. And by only having it in one place, you also use less data.

I don't play fighting games, but I do play other video games. But using your fighting game analogy, you can have a table for characters, a table for moveset, a table for costumes, etc. It's best practice to try to limit the fields (columns) in a table to only the data that's *essential* for that table. If they're not essential to it, it would probably be better for them to go into some other table.",1539732480.0
jhj320,What's the release date?,1539749731.0
C4WTP," The answer:

    IF NOT EXISTS 
            (SELECT 1 FROM pg_type t     
            JOIN pg_class c ON c.oid = t.typrelid     
            JOIN pg_attribute a ON a.attrelid = c.oid
            WHERE t.typname = 'tvms_dseoptitree_routes_type' AND a.attname = 'parentvehicleid') THEN 
        ALTER TYPE tvms_dseoptitree_routes_type ADD ATTRIBUTE parentvehicleid integer; 
    END IF;",1539605095.0
ElectronSpiderwort,"If you are a SQL masochist you could union your AB join and your AC join in an inner query, and prefix each row with two additional columns: Table A's key in its own column we'll call ""column1"", and the table source we'll call ""column2"" which will contain the literal 'A' or 'C' depending on which query the row came from. Order the result set by ""column1"", ""column2"" (or simply 1,2) at the end of the inner query. So now you have all the data in the right order, but with two unneeded columns on the front; just pass them through an outer query that only keeps the columns you want.  ",1539561397.0
HarrityRandall,"Have you tried UNION ALL (c.1, c.2, null, null, null)?",1539559804.0
ammoprofit,"Why are you pulling one query for two unrelated data sets?  AB should be one query.  C should be another query.  The UI should display these accordingly.

&#x200B;

If you are displaying disparate data sets in the same UI, it will appear like the data is related.",1539572963.0
CedricCicada,[This](https://docs.microsoft.com/en-us/sql/odbc/reference/appendixes/interval-literals?view=sql-server-2017) seems to answer my question.  I see I have a bit of work ahead of me.,1539373789.0
Cer_Visia,"psqlodbc has the ability to support SQL_INTERVAL since version 09.00.0100.
However:
> SQL_INTERVAL support is disabled because I found
> some applications which are unhappy with it.

You'd have to recompile the driver.

(I don't know why this isn't a configuration option.)",1539599097.0
CedricCicada,"SQL Server does not support interval types, so I have to go back to our old way of representing intervals as an integer number of minutes.  I was hoping to get away from that.

Sigh.",1539610796.0
CedricCicada,"I asked [nearly the same question](https://www.reddit.com/r/PostgreSQL/comments/9lbf15/how_do_i_delete_duplicate_rows/e75ewcx/?context=3) eight days ago.  I was only concerned about a single column, but maybe the answer I got could help you.",1539372866.0
2strokes4lyfe,Would using the result of SELECT DISTINCT * to alter the table work?,1539381934.0
MakeMe_FN_Laugh,"Your query doesn’t much differ from one in the wiki , except you have 4 columns against 3 in the wiki, so it should work.

Also, you can always start a fresh instance of postgresql in docker with a test table  and populate some random data that matches your criteria for testing purposes.

Secondly, the best way to test your update/delete queries is to make them a select query so you can review the subquery/where clause results before actually modifying them.",1539372645.0
wolf2600,"Are you trying to delete all instances of the record if duplicates exist, or do you want to only delete the duplicates, leaving a single record in the table?

If you want to retain a single record, what is your criteria on which record to keep?",1539447152.0
IJP,"They are very similar and you will be able to do most tree operations with them. Things like cycle detection in graphs and topological sorts are all pretty straightforward. I recently had to do a very complex calculation that involved ordered traversal of a graph, and it ended up being very easy. Have a look at the postgresql docs, they have very nice examples: https://www.postgresql.org/docs/9.1/static/queries-with.html",1539413165.0
johnfrazer783,"I'd like to add that I've recently rewritten a recursive CTE for a tree traversal to use a series of views, each one adding one more column until all branches are exhausted. As written, this means I'd have to manually add more view should changes in the tree data produce more levels than I currently have (only six, as a matter of fact), but this could be avoided using dynamic SQL. 

The one great advantage I see for my current implementation over recursive CTEs is that while the new code is quite repetitive, it is also order of magnitudes easier to reason about. What's more, I get to see and if necessary validate each intermediate step of the tree resolution, which is a huge plus. Performance is about the same, I'd say.",1539432588.0
iiiinthecomputer,"The ""PostgreSQL team"" don't actually write pgadmin at all. It's a separate and independent project.

If you think you can do better, go for it. Many people don't love it but personally I'm just grateful to have something. If you're not contributing then your rights to complain are limited.

Check out OmniDB too.",1539345888.0
nick_danger,"I'm genuinely curious...

> runs in a browser which is a very bad user experience IMO.

Is it the because it's browser based that the UX is bad? If so, how does Electron (which is essentially also browser based) solve that?
",1539347658.0
dns99,No. ,1539344101.0
j03,No. ,1539345588.0
cr4d,"FWIW it wasn't Visual C++ but rather C++ with QT, and was multi-platform. VC++ was used for Windows compilation, but it is/was a true multi-platform app. It's also worth pointing out the source is still @ [https://github.com/postgres/pgadmin3/](https://github.com/postgres/pgadmin3/) and forking it wouldn't be difficult at all.

While I still prefer 3 to 4, it's worth considering why Dave and crew rewrote it for 4. I imagine extendability and maintainability were a big part of that. In addition, i think it makes it easier for EDB and others to use pgAdmin as a platform for enterprise add-ons.

While I wouldn't have chosen the embedded web-server and app approach, I imagine it makes it easier for them to add functionality and grow with Postgres. I think that it probably would feed better and more responsive if they went with a different JS framework for the front-end (think something like React or Vue.js).

It's easy to bikeshed on projects like this. It's worth pointing out that a lot of time and effort went into it, and there are a lot of competing UIs. Many of which are single devs or small teams with either freemium or commercial offerings. Writing an exhaustive UI for something as complex as Postgres is not an easy task and it's easy to underestimate the effort involved.

Finally having done a fair amount of development in Python, Electron, C++ (and many other languages) -- and having developed a pretty extensive app in Electron, there are trade-offs in almost every aspect of building a tool like this. I'd be concerned about responsiveness and memory management in Electron for a Postgres admin app. You think people complain about Slack's resource utilization? I can see it easily being far worse in an Electron database management UI where you are loading lots and lots of data into memory (be it DDL/schema or data).",1539354035.0
rick2g,You kinda don’t get to tell others how to do what they do unless you’re paying them for it. You are free to rewrite it yourself if you genuinely think you know better than they do. ,1539345088.0
PumpkinGourd,"I much prefer the old pgadmin3 over the browser based pgadmin4. Still I've started getting used to it and it's not too bad.

I'm assuming they converted to web based to make it easier to develop for everyone to use vs supporting mac/linux/windows variants.

",1539358784.0
justyb11,No. Electron applications are horrible.  They're the worst of web based applications meeting up with the worst of desktop applications.  I have yet to see anything good come from an Electron app and I work with Atom all the time.,1539354551.0
postgrescompare,"No the team should not rewrite it to target Electron. They should continue til they are happy with the functionality and then, since the app is browser based anyways, package it so it's not a tab in your default browser. Maybe use Electron at that point if that's the best option.",1539378329.0
boy_named_su,"I think they should rewrite it in QT

And build a mobile version in QT as well",1539383822.0
spyhunter99,"Java or use the hypersonic ui. I do like the chart functions of 4 but it's unstable as hell. I depend on it to work and it's been terrible to use. Frequent lock ups, usually end up losing my query window. 9/10 times the ui wouldn't load. ",1539792597.0
,[deleted],1539351869.0
z0rb1n0,"Deliberately throwing SQL exceptions can be relatively expensive (if from a procedural languages there also the PL's exception handling in the way), but you're the only judge for ""fast enough""; more importantly, it's kinda an anti-pattern as it tends to clutter logs (and error statistics if you use some tooling to track them).

Is it not feasible to use an ON CONFLICT DO UPDATE clause (commonly know as ~~insert~~ upsert) or a BEFORE trigger?

You can use the RETURNING to determine if it was an insert or an update (by looking at xmax)

EDIT: wrong word
",1539312025.0
softwareguy74,Sweet!  Wonder how long it will take AWS RDS to offer this after released.,1539302468.0
Token428,Port forwarding/firewall configuration on your router?,1539267394.0
MonCalamaro,You'll need to increase max_replication_slots to at least 2 to have 2 replication slots,1539252130.0
mage2k,"> I have made the following changes to my postgres.conf file

Did you have the server reload its config after you edited the config file?  As a superuser (typically `postgres`): `select pg_reload_conf();`",1539205807.0
muddiedwaters45,"    #max_replication_slots = 0  # max number of replication slots
             # (change requires restart)",1539212452.0
BlueDotMoverArounder,See if this helps: [https://www.reddit.com/r/SQL/comments/9gcsco/postgresql\_cant\_uninstall\_postgres\_because\_the/](https://www.reddit.com/r/SQL/comments/9gcsco/postgresql_cant_uninstall_postgres_because_the/),1539202304.0
dark-panda,"It may be helpful to expand a bit more on the PostGIS extension, because the native Postgres point data type on its own does not include a spatial reference system and therefore doesn’t take into account the curvature of the earth and so forth, so the actual distance being measured will be dependent on latitude. It also cannot accommodate measuring across the antemeridian. PostGIS is able to handle these situations given the proper projection. The geometric data types in Postgres are useful, but they aren’t a replacement for a GIS system like PostGIS. ",1539237007.0
jlrobins_ssc,Well written and illustrative post.,1539186038.0
agenderphobe,"The table is corrupt, this is usually hardware / VM crashing related.  You could likely get to be able to login using the zero_damaged_pages developer option, but that's a system table that's hosed, so who knows what state the DB would be in.   If you have a backup, it'd be easier to recover from that.  

>zero_damaged_pages (boolean)

>Detection of a damaged page header normally causes PostgreSQL to report an error, aborting the current transaction. Setting zero_damaged_pages to on causes the system to instead report a warning, zero out the damaged page in memory, and continue processing. **This behavior will destroy data**, namely all the rows on the damaged page. However, it does allow you to get past the error and retrieve rows from any undamaged pages that might be present in the table. It is useful for recovering data if corruption has occurred due to a hardware or software error. You should generally not set this on until you have given up hope of recovering data from the damaged pages of a table. Zeroed-out pages are not forced to disk so it is recommended to recreate the table or the index before turning this parameter off again. The default setting is off, and it can only be changed by a superuser.

",1539179372.0
brad_was_here,"As a con of Postico, I would add that you can't easily see Postgres functions(Issue 72 on github). Running queries on system tables is the only way.",1539118413.0
mix_room,"You will have to build some of them yourself, unless you have a support contract with someone that provides you with binaries. 

Building them is not that difficult, and in my experience mostly works. 

See [https://blog.2ndquadrant.com/compiling-postgresql-extensions-visual-studio-windows/](https://blog.2ndquadrant.com/compiling-postgresql-extensions-visual-studio-windows/) for instructions. 

&#x200B;

&#x200B;",1539069365.0
jenkstom,"Mostly, yes. I'm sure there are some open source ones that never got compiled for one platform or another.",1539052227.0
wolf2600,So you want to add 3 new columns to table1?  Or do you want to add all the columns from table2 into table1?,1539040415.0
agenderphobe,"Creating a new table is significantly easier than trying to modify it in place.

Just create table as (select * from...); will do fine.  ",1539041094.0
pypt,"Assuming* that you want to add two new columns to a huge table:

1) ALTER TABLE to add your columns, with NULL (as opposed to NOT NULL) and no default value
2) UPDATE your altered table to set the cell values you want using another table (e.g. via subselect)
3) ALTER TABLE once more to add NOT NULL constraint and DEFAULT value, if desired

`*` Your “pd.merge()” example is unfortunately not that useful, we’re not acquainted with every ORM in the world.",1539052651.0
DavidBoone,"Context isn't clear from the original question, but if your need is read-only, perhaps a VIEW is what you want.  ie, create a VIEW with the above SQL and then have your python ORM query that view instead of Table1.",1539062048.0
justyb11,"By default an index stores in ASC order on the indexed tuple\*. [Postgresql doc](https://www.postgresql.org/docs/10/static/indexes-ordering.html).

Per your explain provided, it's using the index **standard\_workitems\_company\_id\_filtered\_by\_deleted\_at** which has the key of (*company\_id*) and a predicate of *deleted\_at* IS NULL.  Which makes complete sense, since that's exactly what's in your query's predicate.

Since the key for that index is on (*company\_id*) it's sorted ASC on that.  However, your query indicates a sort order (*item\_code*) ASC.  Which isn't how that index is sorted to begin with.  You can try creating an index with a (*company\_id, item\_code*) as the key tuple with the same predicate as the original index, but I'm doubtful that you'll get any real gains that way, but like everything, you should test and verify on a real machine instead of relying on what some random Redditor is guessing what the database will do.

Your execution time is \~25ms, I think you're pretty good here on your query.  But hopefully that will explain why the sorting is happening in memory.

**EDIT** \- \*I should clarify before someone points it out, that's only the default when using btree, again, see linked to documentation for all the gory details.",1539013404.0
simcitymayor,"If I'm reading that plan right...

The planner estimated that a quicksort would take (3951.32-3946.05)/3951.32 0.133% of the total query time.

The actual time spent was 0.337ms, which is 0.135% of 24.927ms. So it was pretty spot-on.

At some point, the planner has to decide that Perfect is the enemy of Good Enough, and going down every rabbit hole will only increase planner time with no real benefit to the overall query time.

My guess is that the planner locked in all the indexes which shared the same filter criteria (partial on `deleted_at IS NULL` selection of `company_id`), and then began evaluating them for suitability in order of increasing size on disk. A one column index on (a) will always be smaller than a one column index on (a,b), so I would guess that the planner then did one of two things:

* It judged that both indexes would ultimately fetch the same number of rows and the amount of time spent fetching the larger index would exceed the time spent sorting the rows fetched. Given the costs of a single disk fetch (source: [https://www.prowesscorp.com/computer-latency-at-a-human-scale/](https://www.prowesscorp.com/computer-latency-at-a-human-scale/)), one extra HDD disk block read will sink you, as would a single-digit number of SSD disk I/Os.
* When it finished costing the `standard_workitems_company_id_filtered_by_deleted_at`, it saw that all other indexes under consideration were either larger on disk or potentially fetched rows that didn't meet one of the filter criteria, and it decided to not even go through the rest of the costing process.

Thinking some more on this, the planner itself runs the risk of accessing disk to fetch index statistics, which only further incentivizes it to stop looking after it's found a really good plan.

If anybody has more experience with the planner, I'd be interested to know if my intuition is correct.",1539029999.0
brad_was_here,"Not an expert, but if you swap the where clauses to check for company_id first, you might see some better performance. You will be checking for deleted_at being null for just that company_id",1539012436.0
,[deleted],1539013960.0
NickEmpetvee,"It seems I answered my own question: [https://www.postgresql.org/docs/current/static/plpgsql-statements.html#PLPGSQL-STATEMENTS-SQL-ONEROW](https://www.postgresql.org/docs/current/static/plpgsql-statements.html#PLPGSQL-STATEMENTS-SQL-ONEROW).  PL/pgSQL allows `SELECT INTO` record, row-type variable, or a list of scalar variables.",1538856950.0
threeminutemonta,Please note with postgres 11 (in beta) there is the introduction of stored procedures. Previously I mixed the 2 though best be careful with terminology from now on. See [the differences explained on SO]( https://dba.stackexchange.com/questions/194684/in-postgresql-what-is-the-difference-between-a-stored-procedure-and-other-typ),1538862355.0
pypt,I would consider a materialized view.,1538900288.0
jakkarth,"Possibly English is not your first language. We're missing something here. Why are you entering data by hand? What does ""mix the way of ltree"" mean?",1538829389.0
tsqd,"Copy is faster than insert. Copy does not have to be from a file, but it does have its limitations. Thousands generally aren’t that many, even for an insert, but YMMV based on resources, configuration, table details, and use case.

Basically, you should probably time some examples to get some metrics to see if you can get away with an insert if you have a reason to stick to it, otherwise use copy for bulk loads. There is a reason that pg_dump uses copy commands by default to load all of its tables.",1538820791.0
mlt-,Thousands of rows is not that many. For a very large imports there is also pg_bulkload.,1538836549.0
simcitymayor,"It all depends on what you want to have happen in the case of errors and row conflicts.

If you _know_ that all the rows are going to succeed, which is to say that if one row out of 10,000 fails, you're fine with all 10,000 failing, then `COPY` is the way to go.

If you want to upsert-y things, then `INSERT ... ON CONFLICT` is what you should use. Bear in mind that while you can have multiple rows in a `VALUES` clause, you're essentially putting your data through the SQL parser, and that can get CPU heavy. `COPY` avoids that by treating the data as a stream.

The big thing to avoid is a bunch of single row `INSERT` statements, because the additional network round-trips and parsing steps slow the operation down. Having said that, if each row must succeed or fail on its own, then that's your only option.",1538851109.0
Randommaggy,"Using a function to deserialize JSON into a record set and doing mass inserts or updates actually performs quite well and with some nesting in the structure you might be able to have a small payload considering the amount of data.

It also allows you to do additional logic before inserting without the external application needing to account for it.

Three quarters of a million relatively wide rows in 20 seconds on ~5 year old mid-range desktop hardware is possible with only a single core being utilized. The JSON functions in PostgreSQL are simply amazing.",1539033434.0
ohmypostgres,This was very interesting!,1538845118.0
wolf2600,"You mean like creating a schema of views which have restrictions, then only granting user groups access to those views (and not the underlying tables)?",1538752941.0
thelindsay,"Could add row level security policies to the tables. Then just worry about roles / role groups and their granted table and column level permissions.

https://www.postgresql.org/docs/10/static/sql-createpolicy.html

Which leaves mostly business rules (case/when, functions, etc) or aggregation as reasons to make views.

Otherwise, it's a job for a dynamic SQL app.",1538761128.0
CedricCicada,"Answering one of my questions:  creating the index at the time of creating the foreign key is a feature of pgAdmin, not PostgreSQL.  The pgAdmin's constraint creation dialog box has a tab named SQL.  Clicking that tab shows that SQL statements that will be executed: an ADD CONSTRAINT statement and a CREATE INDEX statement.",1538750153.0
MonCalamaro,Unique and primary key constraints will automatically create indexes.  It's also a good idea to put an index on the column in the referencing table to help with deletes and updates in the referenced table.,1538759238.0
planetworthofbugs,"So you want to create a unique constraint on value? There's probably a million ways to do this, but something like this should do it:

DELETE FROM table WHERE key NOT IN (SELECT DISTINCT ON(value) key FROM table)

If you care about which row is kept around, just add an ORDER BY to the select.",1538658508.0
bar-a-baz,"You can copy the table schema and insert selecting from the original table, with the desired group statement. Then you do the rename dance, to keep the interruption of missing table to a minimum:

ALTER TABLE original
RENAME TO old_and_busted;
ALTER TABLE new_and_shiny
RENAME TO original;
DROP TABLE old_and_busted;",1538658256.0
threeminutemonta,Are you asking this question from [2012](https://www.postgresql.org/message-id/jn5ssi%249qr%241%40reversiblemaps.ath.cx)? If not please elaborate.,1538650344.0
vitaly-t,"There is method [oneOrNone](http://vitaly-t.github.io/pg-promise/Database.html#oneOrNone) for that:

    db.oneOrNone('SELECT * FROM users WHERE item = ${myitem}', obj)
        .then(data => {
            // data = NULL, if no record found, or a single row-object, if one record found
        })

&#x200B;",1538578116.0
neomis,"In Node:

    let sql = ""SELECT * FROM users WHERE item = $/myitem/"";
    let options = {myitem: myitem};
    return db.one(sql, options)
      .then((record) => {return callback(null, record);})
      .catch((err) => {
        console.error(""Error Executing query"", err);
        return callback({message:""Error Executing Query"", error: err, sql: sql}, null);
      });

In SQL:

    select case when count(*) > 0 then true else false end ""exists"" from users where item = $/myitem/;",1538578456.0
SulfurousAsh,"You’re trying to do a cross join (on TRUE) two tables, so every row in a is matched with every row in b? And then you don’t want any type of ordering to choose which row you’re keeping, and you only want one?

I would recommend describing your use case a bit more, because logically it sounds like you just want any random pairing returned.

You could start by two simple select statements UNIONed together, I suppose, but my guess is there are more elegant ways to get the answer you want with a little more information ",1538500989.0
DavidBoone,"I'm not sure there's a way to do what you want with straight SQL.

Here's something I whipped up using PL/pgSQL that seems to do what you want:

    CREATE FUNCTION assign_cards()
      RETURNS TABLE (
        card_id int,
        player_id int
      )
      LANGUAGE plpgsql
    AS $$
      BEGIN
        CREATE TEMPORARY TABLE _x (card_id int, player_id int);

        LOOP
          INSERT INTO _x
            SELECT c.id card_id, p.id player_id
              FROM card c JOIN player p ON (c.victim_id != p.id)
             WHERE p.state = 'idle' AND p.id NOT IN (SELECT _x.player_id FROM _x)
               AND c.killer_id IS NULL AND c.id NOT IN (SELECT _x.card_id FROM _x)
             LIMIT 1;

          IF NOT FOUND THEN
            EXIT;
          END IF;
        END LOOP;

        RETURN QUERY SELECT * FROM _x;
      END;
    $$;

    SELECT * FROM assign_cards();
",1538522475.0
ElginSparrowhawk,"How about...

SELECT DISTINCT ON (player_id) player_id, card_id
...
ORDER BY player_id, card_id",1539611856.0
mgonzo,"Wouldn't you just expect this 

    SELECT id FROM subject WHERE id=$5
to return $5?",1538428046.0
wolf2600,"First I'd determine whether you can update ANY field using a subquery.  Just to narrow down the issue.

    create table testTable1
    (col1 varchar(10));

    create table testTable2
    (col1 varchar(10));
 
    insert into testTable1 values ('test1');

    insert into testTable2 values ('test2');

    update testTable1 set col1 = (select col1 from testTable2);


Does this update work, or does it throw the same error?

Secondly, are you running these statements in PGAdmin or somewhere else?  Could you provide additional information about the context? (Python3 script using psycopg, etc...)

Have you tried getting rid of the $ parameters and hardcoding the value?  Does that work?",1538428304.0
threeminutemonta,"The from statement in the update can be used for this.

>> UPDATE summary s SET (sum_x, sum_y, avg_x, avg_y) =
    (SELECT sum(x), sum(y), avg(x), avg(y) FROM data d
     WHERE d.group_id = s.group_id);

See docs [here](https://www.postgresql.org/docs/current/static/sql-update.html) for where this example can from

Edit: I don’t think this will help. I read the problem wrong on mobile.",1538432676.0
charettes,"Don't you have to order the query by the fields you want to `DISTINCT ON` first? I think you forgot to include `ORDER BY url, ...` here.

    ERROR:  SELECT DISTINCT ON expressions must match initial ORDER BY expressions",1538419213.0
wolf2600,Isn't `timestamp` a reserved word?,1538418552.0
vladimirice,"Let me explain a small trap for newcomers. Sorry if this is too obvious. 

In some cases you might think you can use GROUP BY if you switch off checking that all columns from select must appear in group by. And use order by to ensure the row which will be fetched. 

But ORDER BY is executed after GROUP BY, so this “trick” will not work properly.",1538460777.0
akerro,I wish this blog showed not only queries but also results.,1538468774.0
antlife,"Best I've found so far is dbeaver, even though it runs on Java. I've heard datagrip is good, but it costs and it's made to be more general for SQL I think. I'd be open to suggestions myself.

Pgadmin4 is a tire fire.

",1538409293.0
yawboakye,"It could be that sequential scan is faster than an index scan, especially on small tables. Have you tried the query on a much larger table (say, about a million rows) and still got the same result? Then there will be cause for concern. But it's pretty normal for the optimizer to settle for sequential instead of index scan when it makes more sense.",1538402062.0
tsqd,Did you just add the index? You might need to run `ANALYZE ` on the table.,1538399242.0
QuantumRiff,"I may be missing it, but I see an index for workitems.project\_id, but not one for projects.id",1538401022.0
jroller,"In addition to what other's have said, the additional deleted_at filter either requires rechecking the deleted_at index or going back to the table anyway.  An index containing both fields might change how it's used.  

Assuming you always join on things that haven't been deleted, a partial index might help a lot:

    create index index_workitems_on_project_id_not_deleted (project_id) where deleted_at is null;",1538414380.0
pypt,"Try eliminating some candidate rows in your subqueries, e.g. maybe there's a way to move `WHERE total_transit_time = min_total_transit_time` a little bit up the execution tree (to one of said subqueries), or to add some more conditionals to reduce the number of rows returned by those queries. The less rows the executor has to aggregate, the faster your query will run.

Run your query with `EXPLAIN ANALYZE`, post the generated plan to https://explain.depesz.com/, identify which parts are the slowest.

Worst comes to worst, consider caching the output of your large analytical query somehow, e.g. with a [materialized view](https://www.postgresql.org/docs/current/static/sql-creatematerializedview.html).",1538381281.0
tsqd,"How often are you running this query? If it’s just once, it seems unlikely that you need to optimize it. If it’s many times, you could create a prepared statement that can be passed arguments.",1538317140.0
pypt,"In your case, no matter what you do, you’ll have to pay the price for a massive number of arguments somehow, either as a part of planning time or time spent INSERTing those arguments into a temporary table.

For clarity, I would create a temporary table, INSERT / COPY your UUIDs into it, and then use a subselect to that table as a condition to WHERE.",1538367889.0
ohjaleohhjj,"You have a massive amount of arguments being passed. You can avoid this by passing an array and then use the `@>` operator. See [https://www.postgresql.org/docs/10/static/functions-array.html](https://www.postgresql.org/docs/10/static/functions-array.html). This way, you are only passing one argument.

Also, if the number of arguments vary between different query executions (which is probably the case, because you said the data comes from a UI), you really want to avoid this. You are poisoning your query plan cache with different versions of the 'same' query.

You could optimize your indexes, but let's start with this.",1538380687.0
jlrobins_ssc,"A stored procedure which introspects into the system catalog for all tables  with your ‘date_updated’ column  and then checks and ensures that the corresponding trigger exists is not very hard. It then ensures uniform naming on both your column names, the trigger name,
Etc.

Doing this sort of meta- or higher-order coding is SQL is not too hard once you get into the habit. Am on mobile right now, but will revisit with an example layer on. ",1538319665.0
ihavefilipinofriends,Triggers are the best practice IMO. Just make sure your process involves a review so the team can help ensure each table has the appropriate fields and triggers.,1538313176.0
MonCalamaro,"You might be able to do this with an event trigger: [https://www.postgresql.org/docs/10/static/event-triggers.html](https://www.postgresql.org/docs/10/static/event-triggers.html)

&#x200B;

The event trigger could be set up to run when a table is created and could then create a trigger for the new table that will set the updated\_date.",1538361539.0
leftnode,"If you're using Flyway, can you just make the trigger as part of the migration?

If you're using an ORM in your code, you can probably have a preUpdate() method that sets the updated_date value before a modified object is synced to the database.",1538319102.0
mgonzo,I think security definer is what you are looking for. Option on the create function command.,1538268846.0
DavidBoone,"select distinct on (user_id, game_id) * from scores order by user_id, game_id, score desc;",1538258848.0
mwdb,"Your problem isn't entirely clear to me. Is it that your ""results"" query isn't getting the notes column, since you're doing something like the following?

`select user_id, game_id, max(score) from scores group by user_id, game_id`  

Edit: If what I've got you right, maybe try DISTINCT ON - see this blog for a simple example of it: [https://medium.com/statuscode/the-many-faces-of-distinct-in-postgresql-c52490de5954](https://medium.com/statuscode/the-many-faces-of-distinct-in-postgresql-c52490de5954)",1538254988.0
bytesmythe,"you mean something like:

    select o.* from scores o
     join    (select user_id, game_id, max(score) 
              from scores
              group by user_id, game_id) i
    on i.user_id = o.user_id
    and i.game_id = o.game_id
    and i.score = o.score",1538255131.0
jeffdn,"You want a window function, I think:

    with ranked_games as (
      select user_id, game_id, score, notes,
             row_number() over (
               partition by user_id, game_id
               order by score desc
             ) as rank
      from scores
    )
    select *
    from ranked_games
    where rank = 1;

That should do it nicely!",1538267879.0
ants_a,"Jsonb is going to be much faster for this. Specifically a GIN index with hash ops. It's really space efficient and is able to use patterns in data to skip over portions of the index.

This method easily scales into billions of records and more if you add in partitioning.",1538202686.0
fr0z3nph03n1x,I LOVE postgres but if you just have key value data maybe use redis or another key value tool? If you have no perf requirements you could totally do jsonb in postgres.,1538199568.0
einhverfr,"The first option starts to look far more like EAV.  EAV is usually an anti pattern and you would probably find yourself having a lot of extra work and performance headaches in querying anything more than a single key/value pair.  


JSONB performance is a minefield at scale.  It can scale.  Our largest data environment (basically a debug log) is about 3.5PB of JSONB shared randomly across 300+ instances of PostgreSQL.  A few million rows will not be a problem but the sorts of performance problems you have with JSONB tend to be more complex than you have with standard relational storage.  Performance is also harder to reason about in part because JSONB records are so often TOASTed.

This sort of thing requires a great deal of attention to detail either way you go but I think given what you say, JSONB, warts and all, is probably the bet choice.",1538211099.0
pypt,"I’d go with your option #1. Your data is still relational, you don’t need to support free-form JSON documents, so keeping your metadata in a separate table would be more performant (simpler indexes, no need to rewrite whole JSON values upon updating / adding new metadata, easier querying).

I feel and understand your inclination to store stuff the “NoSQL way” as it seems like an easier thing to do initially, but in that case plan to leave the project that you’re working on just before the point in time when you find yourself needing to figure out what’s stored where and how after a year or two of usage :)",1538161350.0
jtwyrrpirate,"The error message is telling you exactly what's wrong. ""--user"" isn't an option.

[https://www.postgresql.org/docs/10/static/app-psql.html](https://www.postgresql.org/docs/10/static/app-psql.html)

Basically, you need to use ""-U"" or ""--username"" not ""--user""

I'm not familiar with this project, but you wouldn't use a ""eugenio.config"" file to specify a password when connecting directly to a db via psql. You use .pgpass for that.

Read that manual & make sure you UNDERSTAND the commands you're running! I know it's tempting to blindly copy/paste commands off the internet, but it's generally a really bad idea.",1538138788.0
wolf2600,"> When I run the psql command I get this msg:

    psql: illegal option -- user
    Try ""psql --help"" for more information.

The error message is telling you where the problem is: `illegal option -- user`.  

Understand what the command is doing and that will help you troubleshoot.  Usually, an option like `--user` would be used to specify the user account.

>  (assuming both your user and database are named airbnb).
>  `psql --user airbnb airbnb < postgresql/schema_current.sql `

So it's trying to specify the username and password as `airbnb`.  Now that you know what it's doing, you can run `psql --help` to find out what the correct options are to specify the username and password.",1538132314.0
shakesbeardZ,"I thought it's gonna be really an overcoming for the dificulties of using sequelize,  but just another normal poat",1538649750.0
jakdak,"What does this do that https://www.sqlinform.com doesn't?  Been using that for over a decade now across multiple DBMS's.  It's insanely customizable, integrates into IDE/editors, has a standalone desktop version, etc.

The complaints seem to mostly be around newline style issues- and those are configurable in most of the other popular formatters.",1538078268.0
doublehyphen,"I suspect that in your case a btree index would be more suitable, but I have some questions:

1. What kind of queries do you want to speed up? Simple equality (=) or something else? If the only things you will be using are =, <, <=, =>, > then you can use a btree index, and btree indexes are usually faster than gin. The gin indexes are useful if you want to index things not supported by btrees.

2. How many distinct values are there in the name column? If there are few distinct values compared to the total number of rows then a gin index will be smaller than a btree index.

EDIT: Indexes in PostgreSQL are always up to date with the current contents of the table, there is no need to manually refresh them.",1537998586.0
philrhurst,"In my experience, searches on a name column are usually split between exact matches and substring matches. The btree will give excellent performance on exact matches, but cannot help at all for substring searches. This is where the pg\_trgm module will help ([link](https://www.postgresql.org/docs/current/static/pgtrgm.html)), as indicated in the other comments. Postgres has support for GIN and GiST indexes here. A general rule of thumb is that GIN indexes are preferred on static data, while GiST is preferred on dynamic data ([link](https://www.postgresql.org/docs/9.4/static/textsearch-indexes.html)). This may have changed in newer versions of Postgres (for some reason, the documentation changed after 9.4).

The ts\_vector function is part of Postgres Full Text Searching (FTS) capabilities. Their support for FTS is great, but does not really cover the use case of partial matching, especially on names. 

I would recommend creating a btree index, GIN index, and GiST index and then profile how the queries perform on your system. My gut tells me that you would need btree (to handle exact lookups) and GiST (to handle substring lookups). Hopefully that would still be performant on your system.

&#x200B;

&#x200B;",1538052550.0
thelindsay,https://www.postgresql.org/docs/current/static/sql-createindex.html,1537973959.0
RealityTimeshare,https://www.postgresql.org/docs/10/static/pgupgrade.html,1537917186.0
mage2k,"Most people do in-place upgrades using the pg_upgrade tool that /u/RealitTimeshare linked to.  However, going from 8.4 all the way up to 10.5 might not work so well.  Are you able to make a copy of the server to test with?  How large is the database?  If it's not too big then doing a dump/restore might be the easiest option.",1537921776.0
Chousuke,"Sounds like a bad customer. I understand your pain though, having experienced a similar situation before.

Running an old system like that is a big risk by itself. What's your DR scenario for when it breaks?

The biggest worry with the upgrade is the application. It probably uses an ancient database driver, which might not work correctly with 10.5. However, with Java at least you can just drop in a newer driver and it will likely work.

I would not do an in-place upgrade simply because the version is so old. If the database is not very large, set up a new server, do a data dump as SQL, and import it to the new server. With a new server you can test the application before going live. There will be some downtime when you do the switchover as you will have to stop the application before you make the final data dump, but it makes rollbacks easy.",1537940296.0
einhverfr,"I think this is too old for pg\_upgrade to work.  Your best option would be to use bucardo (or Slony though that is more complicated) to migrate all the data and changes over to a new instance (possibly on the same system) and then switch ports.  A few notes:

1.  I assume this is still on 8.4 because maybe they were bitten by the upgrade from pre-8.3 to post-8.3.  That was a really big issue for a lot of users though the pain was worth it I think.  One huge issue here is noting that we have not had breaking changes like we had in 8.3 on the same level so unless you are doing really exotic stuff, everything should ""just work.""
2. With bucardo, you can test your queries before you swap out, to make sure there is no problem.  You could even move back if a problem is found though moving back new writes might require copying the data back in.
3. With this approach you could also do it once, and then set another copy of their application to use to stage the changes (and make sure it all works) before doing it again for real.

The fears probably come from a bad experience (the 8.3 changes caused a lot of headaches) but I have never seen anything like that aside from some things that break some C-language extensions in areas like custom background workers.....",1537984514.0
jsorel,"Something like CHR(number+64) should do the trick :)

&#x200B;

[http://www.postgresqltutorial.com/postgresql-chr/](http://www.postgresqltutorial.com/postgresql-chr/)

[http://www.asciitable.com/](http://www.asciitable.com/)",1537902825.0
einhverfr,"syntax error is because your array syntax should be:

`alphabet = '{A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z}'`

&#x200B;

The closest thing to your syntax is an array constructor:

&#x200B;

`alphabet = ARRAY['A', 'B', 'C'...]`",1538118130.0
SelectCompare,"In the past we used to organize production and nonproduction environments in this way:

DEV - developers DDL and DML rights + SHOWPLAN/EXPLAIN for performance tuning.

QA - developers DML + SHOWPLAN/EXPLAIN, QA DML, DevOps DDL (to release code from DEV)

UAT - developers DML + SHOWPLAN/EXPLAIN, Prod support ReadOnly (SELECT), DevOps DDL(to Release code from QA)

PROD - Prod support DML + SHOWPLAN/EXPLAIN, developers (or some of them) ReadOnly (SELECT). DBA team released code to production.

&#x200B;",1537908504.0
jcriddle4,"1. Depends on if you social security numbers, bank routes, credit card numbers or other PII information in production. You may have to be very mean even with select rights in some cases.
2. How big and skilled is your team? Production needs to have more than one person with DBA rights and skills.
3. Do you have auditors you need to keep happy?

If you can quickly move production data, with anything sensitive striped out, into dev in say under twenty minutes then keeping devs off production is much much easier. Is the dev environments regularly refreshed with data? If you do have devs in production then you need to find out why rather then just declaring it to be prohibited or you may end up with some fake, b.s. where management has you pretend devs don't have access when they actually do. At some companies it can be hard to get the right thing done as you may have choices between cleaning up a long running data and code issues or to develop new features. Unfortunately fake security can be really quick to implement and will often keep auditors happy. An informal meeting with a dev over lunch can sometimes get you a much better solution. As an example lets say devs need to edit some data on production? Well that often means you need a web page or screen written to let them edit that data. As a bonus the job may not need a developer any more but a support person can do it. If you can sell a security fix as saving time and resources you can win.

&#x200B;",1537915979.0
koreth,"This approach seems like a recipe for data loss. Here are the steps it outlines:

1. take snapshot of existing RDS instance
2. create new master based on this snapshot
3. upgrade new master using AWS RDS
4. create multi-master group, add both master servers to replica

But since you're doing this while the application is still running (zero downtime), what happens to data written between steps 1 and 4? Multi-master replication won't be set up yet, so there'll be nothing to copy inserts or updates or deletes to the new master.",1537892135.0
jtwyrrpirate,"Honestly that’s about the simplest & best you’re going to do with a pg_dump/pg_restore process.

I can share some insight on how I do it with a very large database, a SAN, and Ansible Tower. It’s way more work up-front to write all the scripting, but it’s much faster once it’s going.

Process works like this:

0) Have a streaming replica of prod db

1) Kick off dev db refresh script. Script does all of the following steps

2) Tell the streaming replica to pause Postgres transactions 

3) XFS freeze the db & wal disks on replica

4) SAN-level snapshot replica’s db & wal disks

5) XFS unfreeze replica’s db & wal disks

6) Tell replica to resume Postgres transactions 

7) Mount snapshots of db & wal disks to a dev server

8) Overwrite postgresql.conf & pg_hba.conf on the dev server with whatever settings are needed for it to be a stand-alone dev instance

9) Start Postgres on the dev server & let it “recover”

10) Perform any data obfuscation needed for dev work

That’s it. It’s all triggered by a button in Tower so devs never see the ugly guts of the scripting. Works pretty well.

The steps can vary wildly based on the specific technologies you use, but the key ingredients are Postgres streaming replication, a SAN, and some sane way for devs to trigger the refresh script.

For some perspective, my db is large enough that pg_dump/pg_restore would take about a week. This whole process takes about 15 minutes.",1537878725.0
pypt,"You can create your test databases using a pristine database as a template, e.g.:

```
CREATE DATABASE pristine IS_TEMPLATE = true;
-- Import your 150 MB of stuff here

CREATE DATABASE testdb TEMPLATE = pristine;
```

Should be way faster than juggling with dumps.",1537880922.0
Burge_AU,LVM snapshots or VMware snapshots? Depending on your infrastructure configuration you may find restoring to a snapshot far more efficient than recreating the database from dump.,1537877448.0
micheal_sazs,Replication of your database is your best bet ,1537941446.0
jrwren,"rather than restoring many times daily, you could copy the entire cluster and restart postgres using the copied cluster.",1537881845.0
pydry,"I built [this tool](https://github.com/hitchdev/hitchbuildpg) to do exactly that.

The docs aren't really there yet but it should be fairly obvious how to set it up by looking at [this test](https://github.com/hitchdev/hitchbuildpg/blob/master/hitch/restore-from-dump.story).

It works by:

1. downloading and compiling postgres (it doesn't use the system's version because I wanted to be able to fix the version in dev/test environments).
2. building data files,
3. restoring a .sql dump to those data files,
4. rsyncing a copy of those data files to a snapshot dir,
5. rebuilding the db by rsyncing back every time you run a test (for 150mb that's about half a second) and then running migrations.",1537885274.0
cvboucher,"Take a look at the lead and lag window functions.  

[https://www.postgresql.org/docs/current/static/functions-window.html](https://www.postgresql.org/docs/current/static/functions-window.html)

Put that into a CTE then you can filter on anything over a second between the timestamp of the current record and the timestamp of the next (lead) record or the previous (lag) record.",1537826305.0
jk3us,I think we'll need more details.,1537804803.0
CedricCicada,"You're not using a variable in that statement.  You're using the literal string 'ABC'.  

I've never seen that ""->>"" operator.  What language or database system (PostgreSQL, SQL Server, etc.) are you using?",1537809881.0
nick_danger,"Perhaps something along the lines of this?

    select cast(""ColumnName""->>field_name as decimal) from ""MyTable"", ((VALUES ('ABC'), ('XYZ'), ('PQR')) AS V(field_name)",1537821830.0
anras,"Obviously you can't set the column to ""not null"" and allow a default of null at the same time. What is your actual requirement? If you only want to allow the existing rows to be null when you add the column, but no subsequently inserted or updated rows may have null, then create a trigger that checks on insert/update whether the column is null. If so, raise an exception.",1537800498.0
felixge,"Do you expect to be able to store the value `NULL` in the non-nullable column? That's impossible.

But if you just want the new column to default to `NULL` when not specifying a value for it when inserting new rows, that's the default behavior in postgres already.

    CREATE TABLE foo(id int);
    ALTER TABLE foo ADD COLUMN bar text NOT NULL;
    INSERT INTO foo (id) VALUES (1);
-
    ERROR:  null value in column ""bar"" violates not-null constraint
    DETAIL:  Failing row contains (1, null).",1537800705.0
etrnloptimist,"Can't do it. The workaround is to choose an ""in-band"" value that is a magic default. 0 or -1 usually works well.",1537800546.0
pypt,"Tell us what is it that you’re trying to do requirements-wise, then we will be able to propose the most effective way to do it.",1537808902.0
gclough,"/u/food4mybrain, any normal column has a default value of NULL... so just don't specify a DEFAULT, and you'll get what you want.  If any SQL tries to insert a row but hasn't specified a value for that column, or the specified value is NULL, then you will get an error.

BUT, if you're adding a NOT NULL column to an existing data set on a ""live"" database that people are using then you should:

a) `ALTER TABLE foo ADD COLUMN bar;`  (this only takes a momentary lock on the table... so won't noticeably block your users)

b) In small batches run `UPDATE foo SET bar = ""some value"" WHERE bar IS NULL;`  (or better yet, do it in a cursor to save the table scans)

c) Alter the column to be NOT NULL  (will also take a momentary lock)

&#x200B;

    Here's an example:
    postgres=# CREATE TABLE test_add_col(id int);
    CREATE TABLE
    
    postgres=# INSERT INTO test_add_col(id) SELECT generate_series(1,1000000) as id;
    INSERT 0 1000000
    
    postgres=# ALTER TABLE test_add_col ADD COLUMN new_col text;
    ALTER TABLE
    
    postgres=# \d test_add_col
    Table ""public.test_add_col""
     Column  |  Type   | Modifiers
    ---------+---------+-----------
     id      | integer |
     new_col | text    |
    
    postgres=# UPDATE test_add_col SET new_col = 'some_value' WHERE id in (
    SELECT id FROM test_add_col WHERE new_col IS NULL LIMIT 10000 FOR UPDATE SKIP LOCKED); UPDATE 10000
    
    postgres=# SELECT count(*) FROM test_add_col WHERE new_col IS NULL;
    count
    990000 (1 row)

(Repeat the UPDATE, until you have ZERO rows)

&#x200B;

    postgres=# ALTER TABLE test_add_col ALTER COLUMN new_col SET NOT NULL;
    ALTER TABLE
    
    postgres=# \d test_add_col
    Table ""public.test_add_col""
     Column  |  Type   | Modifiers
    ---------+---------+-----------
     id      | integer |
     new_col | text    | not null

&#x200B;",1539611712.0
OUAnalyticsNerd,"after creating a table on the primary and waiting for a few seconds: 

`root@4c69e0c0e9b8:/var/lib/postgresql/data/pg_wal# ps -ef` 

`UID        PID  PPID  C STIME TTY          TIME CMD` 

`root         1     0  0 19:22 ?        00:00:00 bash /usr/local/bin/cluster/entrypoint.sh` 

`root         8     1  0 19:23 ?        00:00:00 bash /usr/local/bin/cluster/postgres/entrypoint.sh` 

`postgres    47     8  0 19:23 ?        00:00:01 postgres` 

`root        48     8  0 19:23 ?        00:00:00 bash /usr/local/bin/cluster/repmgr/start.sh` 

`postgres   126    47  0 19:23 ?        00:00:00 postgres: checkpointer process` 

`postgres   127    47  0 19:23 ?        00:00:00 postgres: writer process` 

`postgres   128    47  0 19:23 ?        00:00:00 postgres: wal writer process` 

`postgres   129    47  0 19:23 ?        00:00:00 postgres: autovacuum launcher process` 

`postgres   130    47  0 19:23 ?        00:00:00 postgres: archiver process   last was 000000010000000000000004 p`

`ostgres   131    47  0 19:23 ?        00:00:00 postgres: stats collector process` 

`postgres   132    47  0 19:23 ?        00:00:00 postgres: bgworker: logical replication launcher` 

`postgres   171    48  0 19:23 ?        00:00:00 /usr/lib/postgresql/10/bin/repmgrd -vvv --pid-file=/tmp/repmgrd.pid` 

`postgres   176    47  0 19:23 ?        00:00:00 postgres: replication_user replication_db 172.24.0.4(39034) idle` 

`postgres   192    47  0 19:23 ?        00:00:00 postgres: wal sender process replication_user 172.24.0.2(47988) streaming 0/5524F60` 

`postgres   213    47  0 19:24 ?        00:00:00 postgres: replication_user replication_db 172.24.0.2(48052) idle` 

`postgres   254    47  0 19:24 ?        00:00:00 postgres: monkey_user monkey_db 172.24.0.1(48010) idle` 

`postgres   255    47  0 19:24 ?        00:00:00 postgres: monkey_user monkey_db 172.24.0.1(48012) idle` 

`postgres   324    47  0 19:25 ?        00:00:00 postgres: monkey_user replication_db 172.24.0.1(48202) idle` 

`postgres   325    47  0 19:25 ?        00:00:00 postgres: monkey_user replication_db 172.24.0.1(48204) idle`

&#x200B;

on replica: 

root@906b45808c72:/var/lib/postgresql/data/pg\_wal/archive\_status# ps -ef 

`UID        PID  PPID  C STIME TTY          TIME CMD` 

`root         1     0  0 19:22 ?        00:00:00 bash /usr/local/bin/cluster/entrypoint.sh` 

`root         8     1  0 19:22 ?        00:00:00 bash /usr/local/bin/cluster/postgres/entrypoint.sh` 

`postgres   153     1  0 19:23 ?        00:00:00 postgres` 

`root       154     8  0 19:23 ?        00:00:00 bash /usr/local/bin/cluster/repmgr/start.sh` 

`postgres   161   153  0 19:23 ?        00:00:00 postgres: startup process   recovering 000000010000000000000005` 

`postgres   162   153  0 19:23 ?        00:00:00 postgres: checkpointer process` 

`postgres   163   153  0 19:23 ?        00:00:00 postgres: writer process` 

`postgres   164   153  0 19:23 ?        00:00:00 postgres: stats collector process` 

`postgres   165   153  0 19:23 ?        00:00:01 postgres: wal receiver process   streaming 0/5526398` 

`postgres   230   154  0 19:24 ?        00:00:01 /usr/lib/postgresql/10/bin/repmgrd -vvv --pid-file=/tmp/repmgrd.pid`

&#x200B;

finally, processed WALs on primary: 

`root@4c69e0c0e9b8:/var/lib/postgresql/data/pg_wal/archive_status# ls -ltr` 

`total 0` 

`-rw------- 1 postgres postgres 0 Sep 21 19:23 000000010000000000000001.done` 

`-rw------- 1 postgres postgres 0 Sep 21 19:23 000000010000000000000002.done` 

`-rw------- 1 postgres postgres 0 Sep 21 19:23 000000010000000000000002.00000028.backup.done` 

`-rw------- 1 postgres postgres 0 Sep 21 19:55 000000010000000000000003.done` 

`-rw------- 1 postgres postgres 0 Sep 21 19:57 000000010000000000000004.done`

&#x200B;

on replicat: 

`root@906b45808c72:/var/lib/postgresql/data/pg_wal/archive_status# ls -ltr` 

`total 0` 

`-rw------- 1 postgres postgres 0 Sep 21 19:55 000000010000000000000003.done` 

`-rw------- 1 postgres postgres 0 Sep 21 19:57 000000010000000000000004.done`",1537560713.0
OUAnalyticsNerd,"More information - After both dbs are started up, it looks like the wal sender and wal receiver are processing as normal.  It even appears that the LSN is changing correctly:

after startup on primary:

root@4c69e0c0e9b8:/# ps -ef

`UID        PID  PPID  C STIME TTY          TIME CMD`

`root         1     0  0 19:22 ?        00:00:00 bash /usr/local/bin/cluster/entrypoint.sh`

`root         8     1  0 19:23 ?        00:00:00 bash /usr/local/bin/cluster/postgres/entrypoint.sh`

`postgres    47     8  0 19:23 ?        00:00:00 postgres`

`root        48     8  0 19:23 ?        00:00:00 bash /usr/local/bin/cluster/repmgr/start.sh`

`postgres   126    47  0 19:23 ?        00:00:00 postgres: checkpointer process`

`postgres   127    47  0 19:23 ?        00:00:00 postgres: writer process`

`postgres   128    47  0 19:23 ?        00:00:00 postgres: wal writer process`

`postgres   129    47  0 19:23 ?        00:00:00 postgres: autovacuum launcher process`

`postgres   130    47  0 19:23 ?        00:00:00 postgres: archiver process   last was 000000010000000000000002`

`postgres   131    47  0 19:23 ?        00:00:00 postgres: stats collector process`

`postgres   132    47  0 19:23 ?        00:00:00 postgres: bgworker: logical replication launcher`

`postgres   171    48  0 19:23 ?        00:00:00 /usr/lib/postgresql/10/bin/repmgrd -vvv --pid-file=/tmp/repmgrd.pid`

`postgres   176    47  0 19:23 ?        00:00:00 postgres: replication_user replication_db 172.24.0.4(39034) idle`

`postgres   192    47  0 19:23 ?        00:00:00 postgres: wal sender process replication_user 172.24.0.2(47988) streaming 0/3000700`

`postgres   213    47  0 19:24 ?        00:00:00 postgres: replication_user replication_db 172.24.0.2(48052) idle`

`postgres   254    47  0 19:24 ?        00:00:00 postgres: monkey_user monkey_db 172.24.0.1(48010) idle`

`postgres   255    47  0 19:24 ?        00:00:00 postgres: monkey_user monkey_db 172.24.0.1(48012) idle`

`postgres   324    47  0 19:25 ?        00:00:00 postgres: monkey_user replication_db 172.24.0.1(48202) idle`

`postgres   325    47  0 19:25 ?        00:00:00 postgres: monkey_user replication_db 172.24.0.1(48204) idle`

after startup on replica: 

`root@906b45808c72:/var/lib/postgresql/data/pg_wal# ps -ef` 

`UID        PID  PPID  C STIME TTY          TIME CMD` 

`root         1     0  0 19:22 ?        00:00:00 bash /usr/local/bin/cluster/entrypoint.sh` 

`root         8     1  0 19:22 ?        00:00:00 bash /usr/local/bin/cluster/postgres/entrypoint.sh postgres   153     1  0 19:23 ?        00:00:00` 

`postgres root       154     8  0 19:23 ?        00:00:00 bash /usr/local/bin/cluster/repmgr/start.sh` 

`postgres   161   153  0 19:23 ?        00:00:00 postgres: startup process   recovering 000000010000000000000003` 

`postgres   162   153  0 19:23 ?        00:00:00 postgres: checkpointer process` 

`postgres   163   153  0 19:23 ?        00:00:00 postgres: writer process` 

`postgres   164   153  0 19:23 ?        00:00:00 postgres: stats collector process` 

`postgres   165   153  0 19:23 ?        00:00:00 postgres: wal receiver process   streaming 0/3000700` 

`postgres   230   154  0 19:24 ?        00:00:00 /usr/lib/postgresql/10/bin/repmgrd -vvv --pid-file=/tmp/repmgrd.pid` 

`postgres   234   153  0 19:24 ?        00:00:00 postgres: replication_user replication_db 172.24.0.2(48816) idle`

&#x200B;",1537561278.0
linuxhiker,"This is not really a good case for Logical Replication. For failover, use binary replication. If you don't want all data on a fail over, have your fail over script truncate the tables that you don't want to have data in as part of the fail over procedure.

&#x200B;

Logical replication can be finicky and tricky due to the limitations it has. It is excellent technology when used correctly.",1537537814.0
a1sher,"You need something like this, assuming you need only the first source in general.

>select id, code, first\_origin\_id, delivery\_id, source  
>  
>from (  
>  
>select id, code, first\_origin\_id, delivery\_id, source,  
>  
>row\_number() over (partition by first\_origin\_id, delivery\_id order by source) rn  
>  
>from test  
>  
>) z  
>  
>where rn = 1

&#x200B;",1537426461.0
alephylaxis,"Best way would probably be:

SELECT DISTINCT ON (first_origin_id, delivery_id) id, code, first_origin_id, delivery_id, source
FROM pricing_supports
ORDER BY first_origin_id, delivery_id, source ASC;",1537456260.0
thelindsay,"Let me just warn you that writing a language binding over SQL is not easy, probably will never be complete, and eventually will hit the grand brick wall built from of dependency resolution issues, object alteration limitations, data migration gotchas, and so on.

But if you are determined, have a look at the pg_catalog schema. It stores all pg object info and how they relate. Each table has a docs entry.

If you're familiar with Python, I'd recommend reading the source code for Pyrseas. It's a remarkably comprehensive treatment for this problem, particularly about identifying objects and identifying alterations to make. It's designed to use Yaml specs but the code looks amenable to library usage. However it does battle the above mentioned brick wall with varying levels of success.",1537459212.0
billrobertson42,"Extensions go into schemas I think, but like other items in schemas they're accessible elsewhere.

There are also replication publications and subscriptions. Those might be database level.",1537412980.0
alephylaxis,"And don't forget that toast tables are children of regular tables, indexes and sequences as well. ",1537415010.0
MonCalamaro,"I just wanted to add that users and tablespaces are defined at the cluster level, which I guess would be level0 in your chart?

&#x200B;

There are also a number of settings that can be altered for a database, but I'm not sure if you would count those as objects:

&#x200B;

[https://www.postgresql.org/docs/10/static/sql-alterdatabase.html](https://www.postgresql.org/docs/10/static/sql-alterdatabase.html)",1537446207.0
postgrescompare,"The objects that implement extensions go into a schema, but the extension itself is considered ""level2"". Casts and Languages are also level2.

Roles and tablespaces are level1.",1537570735.0
jakdak,"That operation is known as an ""Upsert""

http://www.postgresqltutorial.com/postgresql-upsert/",1537388923.0
denpanosekai,On conflict. 9.5 and up,1537389517.0
mokadillion,There is ON CONFLICT also. Acts in a similar way to merge. ,1537394698.0
irrational_design,"As a side note, in other databases this is done via a ""merge into"". ",1537389101.0
rails1412,"Using constraints  
If you using rails try this from activerecord-import gem  
[https://github.com/zdennis/activerecord-import/wiki/On-Duplicate-Key-Update](https://github.com/zdennis/activerecord-import/wiki/On-Duplicate-Key-Update)",1537419217.0
iiiinthecomputer,"There's also PostgreSQL foreign data wrappers to consider, though they definitely aren't suited to all application needs.",1537269087.0
TravisLabs,Do you mean something like data warehouse or something for transaction processing?,1537287848.0
stump82,"Pgbouncer is a popular middle where that routes requests to different postgres based data stores (postgres, redshift, aurora). It would be a single endpoint and would route to the correct database based on a set of rules. ",1537302428.0
einhverfr,"A few thoughts here.

1. Foreign Data Wrappers help a great deal here but they are not a magic bullet.  They have a number of limitations in terms of bulk data transport, query push-down and the like.  You really want to look into them but also be aware you need to really think carefully about how things are done.
2. I would look closely at Postgres-XL and Postgres-XC documentation and tutorials because these make you more aware of a lot of the problems that come with data federation generally and will bite you in cases like this.
3. The sorts of things you are talking about doing are edge cases where the few people doing similar things are not necessarily publishing tutorials on it.  So you need to be prepared to learn a lot on the fly.
4. Get to know the PostgreSQL frontend/backend protocol (and as much data as you can on any other databases you are connecting to) because this will give you an understanding of how things work at scale.
5. PostgreSQL 11's parallel append feature will probably help
6. Familiarize yourself with the PostgreSQL (and libpq!) source if going this direction.  Even if you don't code things in C against it, you will find a basic understanding of it helpful out on these areas.

Note a lot of my work involves very large federated PostgreSQL environments (0.5-4PB in size and growing).  Note here's something about the smaller of our environments: [https://www.youtube.com/watch?v=BgcJnurVFag](https://www.youtube.com/watch?v=BgcJnurVFag)",1538117435.0
to_wit_to_who,"Heh, this makes me feel like a GraphQL fanboy since I just mentioned it in the other thread.  Anyway...  
  
I use GraphQL to solve the issue you're describing.  More specifically, I utilize [Schema Stitching](https://www.apollographql.com/docs/graphql-tools/schema-stitching.html) in the main server to combine APIs from various other services.  In my case, those services are PostgreSQL, ElasticSearch, Redis, S3, & other small custom REST services.",1537265573.0
QuantumRiff,"Why give developers access to the production db?

You can build read only replica (that is also good for HA) so large queries on it won't hurt production.

Or, even better, setup a new ""dev"" server, and on a regular basis, copy the data from prod to dev.",1537301339.0
jmswlms,"One of these might fit your needs (all FOSS):

* Redash - https://redash.io/
* pgweb - http://sosedoff.github.io/pgweb/
* Metabase - https://www.metabase.com/
* Apache Superset - https://superset.incubator.apache.org/
",1537326533.0
jtwyrrpirate,"Change “password” to “ident” then do a pg_ctl reload. Then, “sudo su - postgres ; psql” and fix whatever, then set up a .pgpass file, and then go rtfm!

To get you headed in the right direction: order matters in pg_hba.conf",1537240062.0
cyberst0rm,"postgraphile will let you write all your custom functioning inside pg, so if your app needs to scale one day, its still functional, ie, pg still has all the required accessors.

its production ready, v4 is a api change, not a wholely different product.

the support is building and has dedicated dev. it really just needs a more explicit set of docs. ive been building out an internal service api with a spa framework and its stability in rapid developnet of the api footprint is amazing.",1537308922.0
to_wit_to_who,"Depends on how much existing experience you have, the time you're willing to dedicate, & your goals with respect to your timeline.  
  
REST is an architectural style for designing web APIs.  It's very common and used everywhere, so if you're not already familiar with REST in general, then I'd spend a day or two getting the hang of it as an overall concept. I haven't looked at PostgREST in a long time now and haven't used it in even longer, so I'm afraid I can't really comment there.
  
GraphQL is my primary choice for API architecture, but that's just my personal opinion.  If you're interested in going this route, then I'd suggest running through the PostGraphile Forum tutorial (it's in the documentation).  It'll give you a decent primer on both PostgreSQL & GraphQL.  You'll learn how to design a PostgreSQL database for use by PostGraphile, along with learning GraphQL by experimenting around with GraphiQL (the built-in web interface app for GraphQL servers).

I've been using PostGraphile for a while now.  The latest major version is RC, but it's not too far from stable release.  I haven't had any issues with it and I use it quite a bit during development.  However, if you're not already familiar with PostgreSQL, then you're going to want to get comfortable with that.  PostGraphile relies heavily on stuff like RLS for security & FUNCTIONS for GraphQL mutations, along with connection/session variables for things like JWTs.  
  
Good luck!",1537265111.0
antlife,"Just learn REST. It's is completely agnostic. You're confusing web services and REST.

I am a .net developer and MSSQL/Pgsql admin. My opinion is look into creating web services and consuming web services in asp.net. Look into how MVC works. You can use Postgres just fine with it and you can still even use Entity Framework. No need to have some sort of third party middleware service.

Obviously you'll just have to get used to the fact that you don't have ""stored procedures"" in pgsql, but it's easy to adapt.",1537239709.0
nayrb1523,"So we did that for a while. We spun up a script that would pull a backup or production, restore as a new version of whatever it was replacing (ci, stage, uat, etc.) in the right VPC then altered the sexist groups and permissions.  Really the cli is great for this and isn’t major rocket surgery. 

That worked ok until the prod db was getting too large to lower envs and the process just took too long. Now it’s the same restore but now we run a little “data thinning” script on the db to reduce overall record counts in a smart way (for us). I personally don’t want to run huge dbs in lower envs to this gets us good data to work with and smaller dbs. 

For local work on our Postgres installs we further reduce to a desired data graph and then pull that in and at the same time we scramble defined PII keys in the jsonb cols as well at other data. An ec2 pulls in the data and processes then saves to s3 for a pull down and stream to your local as a rebuild when needed. ",1537235758.0
CedricCicada,"Someday I should learn to use Google before I post.  In case anyone is curious, [here](https://stackoverflow.com/questions/27931516/add-newline-or-carriage-return-directly-in-a-column-to-separator-values-in-postg)'s where I got the answer.",1537206531.0
InfoSec812,"Hmmm... This was obvious to me the first time I used CTEs in PostgreSQL. Always limit as much as possible as early as possible in order to limit the ""view"" which needs to be held in memory. ",1537206770.0
koreth,"PostgreSQL's CTE behavior can be a useful escape hatch when the query planner gets confused, but most of the time I wish it worked the way it does in other databases, where it ends up acting more or less like a regular non-materialized view from the query planner's perspective. I always have to think twice before using it due to the performance implications, even when it is the way I'd want to express my query for clarity's sake.",1537211410.0
alcalde,">The first query took 0.619 ms while the second one took almost 300 times 
>more, 227 ms. Why is that?

Because you created an entirely new table first.

>A lesser known fact about CTE in PostgreSQL is that the database will 
>evaluate the query inside the CTE and store the results.

I'm not sure why that wouldn't be known. What else would a CTE be doing?",1537201994.0
doublehyphen,"Dropping a trigger requires you to take an exclusive lock on the table, I would really not recommend doing that more than strictly necessary because it will have to wait until all queries have completed and during that time all new queries will have to wait on the lock.

There are some solutions for specific cases, e.g. it is safe to drop and recreate all functions which are not used by triggers or similar.
But in my view the only general solution is the write migration scripts manually, and use something like apgdiff to make sure we have no diffs in production.

Worth noting is that I come from online gambling where we have large tables and every bit of downtime can cost us a lot of money.",1537181500.0
djrobstep,"I am the author of a [schema diff tool](https://migra.djrobstep.com/) which handles function and view dependencies. It should generate the drop/create statements for only the views/functions that actually need changing, and put them in the correct order (triggers aren't supported, yet).

I'd be interested to know if it works for your particular use case.",1537186242.0
simcitymayor,"Updating all the rows every time you delete a row is going to make a lot of MVCC churn.

I'd just leave gaps in the number sequence, insert new values from a sequence, and if one day you discover that you're running out of integers, _then_ do a re-numbering.",1537131504.0
pypt,"Try triggers:

https://www.postgresql.org/docs/current/static/sql-createtrigger.html

But be wary of the possibility of race conditions.",1537131282.0
fr0z3nph03n1x,"Here is a hacker news discussion and article talking about this scenario. Lots of insight here on different approaches to tackle this. It seems simple but ends up being a bit complicated. 

https://news.ycombinator.com/item?id=16635440",1537139862.0
therealgaxbo,"How do you query this table?  Do you ever actually need to say ""Get me the 15th entry from this list"", or is the number solely used for ordering?

If you don't actually need the numeric index, then this sounds an awful lot like a doubly linked list.  That allows constant time insert and delete (max 3 DML operations for each), though fetching the whole list in order would become significantly more expensive.

So depending on usage patterns and list length, it may be an option?",1537182082.0
aamfk,Does postgres use temp tables like Ms Sql allows? That is the first place I would look.,1537171070.0
Tostino,"Every single postgres release since I have started using it has had multiple extremely useful (for me) features added. 

This one is no different and I can't wait to migrate to it.",1537065549.0
therealgaxbo,"From the section about _potential Pg12 features:

>>An index skip scan patch has also been proposed, which enables B-Tree index scans that omit the leading column from a particular available index when the column happens to have few distinct values. A similar patch by Alexander Korotkov to recognize that the ordering provided by a B-Tree index can be made to work by performing many small sorts on an omitted trailing column may finally leave patch purgatory in the v12 cycle. There is a duality here: the first enhancement helps the case where we want to do an index scan using the B-Tree sort order ""(B, C)"", but only have an index on ""(A,B,C)"", whereas the second enhancement helps the case where we want to do an index scan with the sort order ""(A,B,C)"", but only have an index on ""(A,B)"". The former enhancement ""skips"", while the latter enhancement ""extends"".

!!!

I think this will really help a whole bunch of queries that people didn't even realise could be improved. Speed-ups due to optimisations and parallelisation are great, but changes to the algorithmic complexity can be huge.

The text (about the first patch) just mentions allowing queries to omit low-cardinality leading columns, but some of the huge stuff would be aggregations over the leading column.  Perhaps this lays the foundation for queries like `select date_trunc('day', reading_date), max(value) from sensors where sensor='temperature' and date_trunc('day', reading_date) > '2018-09-01';` to make full use of an index `create index sensor_values on sensors(sensor, date_trunc('day', reading_date), value);` rather than having to iterate over each individual reading in range.

The second patch is likely to have fewer truly huge speedups, but I can imagine it will provide small improvements to a larger number of queries.",1537090831.0
aamfk,Didn't 10 just get released? ,1537067600.0
joaodlf,I can't wait for 11! So many useful improvements to partitioning.,1537181135.0
thinkwelldesigns,"Has the Write Amplification Reduction patch landed in v11?

Seems like it's [in limbo](https://commitfest.postgresql.org/14/775/) but I hope not...",1537529334.0
henry_hopkins,"Coding in Python with Django, I'd need to give up version control as I know it, use a different Python environment, give up unit testing as I know it (and as it integrates with the rest of my application), authentication (because that's owned by Django), distributed tasks and queueing, performance monitoring (as I know it), etc.

I would really like to see a Proof of Concept of a modern web framework like Django/Rails, but with a strong focus on moving as much to the database as possible - user management, queues, etc, as I think there are many benefits to be had from pushing more into the database.

Until something like that exists though, I think moving logic to the database will lose development tooling and process that we are used to, and end up re-implementing logic in multiple places which isn't great.",1537708614.0
pypt,Nice work!,1537025899.0
thelindsay,"Looks great! This seems to fill the gap between reporting tools, desktop databases, and bespoke apps.

In terms of feedback I'd say add a bit more than a single ""it doesn't crash"" test ;P

Are you planning to continue developing it? If not maybe change from GPL to MIT.",1537030827.0
Randommaggy,"You should collaborate with the guy behind this
https://github.com/ivanceras/diwata
Your ui skills and his backend skills would combine to create a really awesome solution.",1537348612.0
OvidPerl,"If you can run it from a terminal, why not use the [COPY](https://www.postgresql.org/docs/9.2/static/sql-copy.html) command?

    COPY ( SELECT STATEMENT HERE )
      TO '/fully/qualified/path/file.csv' CSV HEADER DELIMITER ',';

With that, you'll get a properly formatted CSV file, with headers, that you can import directly into Excel.",1537017959.0
wolf2600,"I use DBVisualizer, but you have to have the pro version to be able to export results to a CSV file.  It works well though.... I've exported 20M record result sets with it. (it writes the results to a file instead of outputting to the screen... if PGAdmin is outputting to screen instead/also, that could be why it crashed).


Maybe try DBeaver?  

https://dbeaver.com/",1537014533.0
thelindsay,"Use a surrogate identity primary key, map_id. Create a not-null constraint on a_id. Create a unique index on a_id, coalesce(b_id,-1), coalesce(c_id,-1). Create a check constraint for b_id is not null or c_id is not null.

This validates the data at hand rather than forcing it to act like a primary key.

The functional unique index avoids having to do weird stuff with operators.",1536989611.0
mr_thwibble,"Are you able to change whatever populates those columns to write a value in that can never be a valid id - say -1 instead of null?  Then you can just index as you would normally.

A bit more clunky would be to assign a power of two value to each column if its populated, 0 of its not, sum those, and write the integer into a fourth column - then index that.  So value/no value/value would give you 4 + 0 + 1 = 5.

If you know you only want value/value/no value, search the 4th column for 4 + 2 + 0 = 6.

This will give you a fast way of locating viable rows, then you can join whatever you need on to that result-set.",1536991311.0
r0ck0,"I'd just create a separate PK column with regular SERIAL or UUID.

I've found that as systems get more complicated, that composite keys can become limiting, and make the rest of the foreign keys (especially once other tables start pointing to your linking table) and application code more complicated than they need to be.

I don't use them at all any more.",1537006305.0
NickEmpetvee,Thank you for all the great advice. I'm taking it under advisement and thinking through things.,1537038959.0
jk3us,"This is very specific to your example:

    with stuff as (
    	select * from ( values 
    		(1,'A', 'foo'),
    		(1,'B','bar'),
    		(1,'C','baz'),
    		(1,'D','qux'),
    		(2,'A','quux'),
    		(2,'B','quuz'),
    		(2,'C','corge'),
    		(2,'D','grault')
    	) as t (col1, col2, col3)
    )
    select
    	col1,
    	min(col3) filter (where col2 = 'A') ""A"",
    	min(col3) filter (where col2 = 'B') ""B"",
    	min(col3) filter (where col2 = 'C') ""C"",
    	min(col3) filter (where col2 = 'D') ""D""
    from stuff
    group by col1

col1 | A | B | C | D
-|-|-|-|-
1 | foo | bar | baz | qux
2 | quux | quuz | corge | grault",1536941096.0
thelindsay,"Use a procedure to generate the necessary SQL. If you're already writing an app in python or something, use that.",1536947571.0
ammoprofit,"SELECT group\_concat(col3, ',') WHERE col1 in (val1, val2) GROUP BY col1 ORDER BY col2 LIMIT 100;",1536981724.0
djrobstep,"Don't bother doing it in SQL, it's always so painful. Get the results, and process them once out of the database.",1537186365.0
pmart123,"Generally, as others have mentioned, you won't/shouldn't dynamically create columns in SQL, but there are other ways to maybe solve what you are trying to do. Some of the following methods have helped me in my experiences when I had similar needs.

Say we start with

    WITH stuff AS (
        SELECT *
        FROM (VALUES
          (1, 'A', 'foo'),
          (1, 'B', 'bar'),
          (1, 'C', 'baz'),
          (1, 'D', 'qux'),
          (2, 'A', 'quux'),
          (2, 'B', 'quuz'),
          (2, 'C', 'corge'),
          (2, 'D', 'grault')
             ) AS t (col1, col2, col3)
    )

## 1. Using array aggregations

```SQL
SELECT
  col1,
  col2,
  array_agg(col3) as collection
FROM
  stuff
GROUP BY col1, col2;
```

Here we simply collect all the elements in column 3 for each col1, col2 group.

## 2. Using group by rollup, cube

```SQL
SELECT
  col1,
  col2,
  GROUPING(col1, col2),
  array_agg(col3)
FROM stuff
  GROUP BY ROLLUP (col1, col2);
```

Rollup and Cube are very useful for these types of problems typically. Depending on what you want from your final result, these functions can be very good at aggregating across columns. There's a lot of good blog examples to handle specific situations.

### 3. JSON aggregation
```SQL
SELECT
  col1,
  col2,
  jsonb_agg((SELECT x
             FROM
               (SELECT
                  col1,
                  col2,
                  col3) AS x)) AS json_data
FROM
  stuff AS s
GROUP BY s.col1, s.col2
```

Aggregate the results to a JSON column. From here, I think you can transform the JSON fields into column names.
",1537317685.0
cachedrive,Nice info! Thank you.,1536856358.0
TravisLabs,"In ETL I commonly do MD5 sums of all the values in a row to get a row hash in order to compare it with another row.

EDIT:

I would probably do something like this. ideally you want the num_keys and num_distinct_keys to match. If not you may have duplicates or something.

	with known_good_data as (
		select
			key,
			md5(StringCol1 || StringCol2 || StringCol3) as md5_sum
		from
			known_good_table
	), 
	untrusted_data as (
		select
			key,
			md5(StringCol1 || StringCol2 || StringCol3) as md5_sum
		from
			untrusted_table
	), checks as (
		select
			known_good_data.key,
			case when known_good_data.md5_sum = untrusted_data.md5_sum then 'GOOD' else 'BAD' end as data_match,
			case when untrusted_data.key is null then 'BAD' else 'GOOD' end as row_missing,
			case when known_good_data.key is null then 'BAD' else 'GOOD' end as mystery_row_added
		from
			known_good_data
			full outer join
			untrusted_data
				on known_good_data.key = untrusted_data.key
	)

	select
		checks.data_match,
		checks.row_missing,
		checks.mystery_row_added,
		count(*) as num_keys,
		count(distinct checks.key) as num_distinct_keys,
	from
		checks
	group by
		checks.data_match,
		checks.row_missing,
		checks.mystry_row_added
",1536786244.0
zombeaver92,Check out metabase. We use it exactly like that. Easy to deploy in docker too. ,1536743421.0
gosh,"This is a tool that might be of interest: [Gorep Selection](https://www.youtube.com/channel/UCKNeQ96WteazmrcsbZFW7Tg/videos)   

Sample: [Prototype a database](http://goorep.se:1010/changelog/report/rselect/page_result.htm?alias=guest&query=Book%20pages&$TArticleBook1.ArticleBookK=1021)",1537495267.0
CedricCicada,"I think I have an answer.  I just create a little table that stores a column name and its conversion, which will be either the name itself or a call to coalesce().  Then, all I have to do is loop through that table and build my strings.  I don't need to both build the conversion and do the aggregation in the same select query.",1536700402.0
kodifaer,"Aggregation functions support ordering and filtering. Also, string\_agg doesn't add extraneous commas at the end, provided you don't have null values in that position, so there is no need strip the extras. Here's a quick take on your queries, rolled into one:  


    select
            string_agg(attname, ',' order by a.attname),
            string_agg(
                    case when adsrc is null then attname 
                    else format('coalesce(%I, %s)', attname, adsrc) 
                    end, ',' order by a.attname)
        from pg_attribute a
            left join pg_attrdef d on adrelid = attrelid and adnum = attnum 
        where attnum > 0 and attisdropped = false
        and attrelid = table_to
        into column_list, conversion_list;

&#x200B;",1536724846.0
NathanClayton,"Check out an example to have all in one table [here](http://sqlfiddle.com/#!17/11401/3).

What it's doing is using the `published_on` field to figure out whether or not the revision is published. Multiple revisions of the same published post can easily be handled by having multiple records, each with different `published_on` timestamps.

If there are too many records, you could easily store the historical revisions in a `posts_hist` table and have just the currently active posts/drafts in the main table to speed up operational queries.

*Note that* `distinct on` *is a PostgreSQL specific extension to the SQL standard.*",1536769439.0
joaodlf,"Great source of info, as always with pgdash content.

I'd like to add the following: https://pgtune.leopard.in.ua/ - Great way to start with configuration.",1536680797.0
depesz,"Yes, just upgrade binaries (programs), and restart database (so that it will load new backend code). No pg-upgrade necessary.",1536660294.0
denpanosekai,Wow -- that is pretty good,1536700701.0
eugenekropotkin,"Cool! I need to test it!   
I'm about pgDash/pgmetrics :)",1536722622.0
jakdak,"This error has nothing to do with the size of a query, it is complaining that there is a row in the query results that is exceeding the 1MB block size.   This usually is hit by doing string manipulation on very large strings and usually the root cause is subquery string field being left with unspecified precision and therefore being presumed to be varchar(max)


> I did some research on redshift it seems to be designed for logging

Redshift not only is not ""designed for logging"" it would be an extremely poor choice for this scenario.  It is a big data database primarily designed for data warehousing style usage patterns.  (Bulk batch ETL writes, Analytic data mining queries)",1536639552.0
thelindsay,"- import / query / rollback on the foreign catalog to check for type changes
- it's possible to alter foreign tables
- create a copy of the table locally with a different name, compare the type to existing table

In any case it'll require polling the foreign server, or if you have edit control on the foreign server then setting up a dml trigger / notify/listen channel of some sort to do it automatically.",1536418539.0
elmicha,Can't you just add the column and catch or ignore the error?,1536389402.0
this1,"I'm sorry I'm not near my work machine but I thought somewhere I had working ""alter table add column $column_name if not exists;""

We're on 9.6 now, but I was also certain I have had it that way since we were in 9.2

",1536378745.0
iantimothy,"Make sure you're connected to the correct database with `\c <dbname>`. Using `\l` will list all databases regardless of which database you're connected to, but most commands are specific to the connected database.

Use `\dt *.*<tablename>*` to list all tables with name matching  `<tablename>` in all schemas.

Assuming the table exists, it will tell you what schema it's in. Then you can query with `SELECT * FROM <schemaname>.<tablename>;`.

If you want to avoid having to explicitly specify the schema name, make sure the schema that contains the table is in your path: `SHOW search_path` and `SET search_path <schemas>`.",1536370721.0
muddiedwaters45,What's the query that's failing?  You should see it in postgres' logs.,1536422506.0
dopperpod,RTFM,1536259591.0
igncampa,"You need to be more specific about what you're trying to achieve here.

* Are you stuck somewhere?
* Do you not understand something?
* Do you have any kind of experience with relational databases?

psql commands can be easily googled, have you tried that?

here's [a list of 17 practical psql commands](http://www.postgresqltutorial.com/psql-commands/) to get you started (including how to list all the tables in a database)",1536260627.0
jk3us,"I've removed this, mainly because it's off-topic.  I think we can compare database platforms here and debate what's better, but this is just MS bashing.",1536254823.0
tweettranscriberbot,"^The linked tweet was tweeted by [@TheEvanCarroll](https://twitter.com/TheEvanCarroll) on Sep 05, 2018 21:10:39 UTC (0 Retweets | 0 Favorites)

-------------------------------------------------

The most popular SQL Server plugin (which I recommend and use) doesn't publish their testing suite because they're afraid to get sued for a EULA violation against benchmarking. This is why I use \#PostgreSQL. \#SQLServer \#Microsoft \#FOSS 

[Attached photo](https://pbs.twimg.com/media/DmXAy2uV4AUzj1U.jpg:orig) | [imgur Mirror](https://i.imgur.com/fDdpGpB.jpg)

-------------------------------------------------

^^• Beep boop I'm a bot • Find out more about me at /r/tweettranscriberbot/ •",1536181956.0
iiiinthecomputer,"That's just weird. Surely you'd ask MS if you were a dev of a big, popular plugin?",1536191956.0
linuxhiker,Not everyone is even closed to being terrified of Microsoft. Microsoft supports the Postgres community through conferences and is actively working making Postgres a first class platform on Windows/Azure. They are very open to discourse and collaboration on how to be good community citizens and want to contribute back to the community.,1536249774.0
craig081785,"The license debate (about MS SQL Server) could be a fair one but same could be said of Oracle as well. But, it is of note that Microsoft is becoming a bigger and bigger supporter of open source. They've added support on Azure for both MySQL and Postgres, have started to get more involved in both communities and are actively supporting the projects in ways they can. So I'm not sure the broader terrified/hate of MSFT is really a valid point, more that databases used to have a EULA and the world is changing with open source.",1536186932.0
AQuietMan,"> AND P.PostingPeriod BETWEEN convert(varchar(6),dateadd(month,-12,getdate()),112) AND convert(varchar(6),dateadd(month,-1,getdate()),112) 

Something along these lines might work. Hard to tell, since you didn't post the whole SELECT statement, or even the whole WHERE clause.

    p.postingperiod between current_date - interval '12 month' 
    and current_date - interval '1 month';

Also, see [SQLfiddle](http://sqlfiddle.com/#!17/10162/1).",1536180256.0
wolf2600,"Is ""PostingPeriod"" a timestamp/date datatype?",1536239346.0
denpanosekai,Hmm Sounds like you have more of a UI problem than a DB problem... well if it were up to me I'd probably rewrite the whole thing in PHP/jQuery.,1536165838.0
Randommaggy,"https://github.com/ivanceras/diwata

This might give you an okay tool for editing and browsing the basic data a bit more easily than a lot of other tools.

Postgrest or postgraphile might be good options if you need to do some more complex operations without writing a lot of server side code.",1536337076.0
denpanosekai,It would probably be more useful to point users to [this tool](https://pgtune.leopard.in.ua/#/) which takes into account your PostgreSQL version. The article doesn't mention anything about that.,1536166303.0
pypt,"Full of typos, e.g. `shared_buffer`, and no backlink to https://wiki.postgresql.org/wiki/Tuning_Your_PostgreSQL_Server from which the article ""took inspiration"" (so to say).",1536187137.0
simcitymayor,"Things that smell like an `ENUM` but aren't quite a fit. Example: airport codes, US radio stations.",1536120778.0
p_mxv_314,"Storing strings if you know size? A char is 2 bytes smaller than varchar
",1536105413.0
dark-panda,"As a bit of trivia, there's also an internal `""char""` type that is exactly one char wide. The double-quotes are included as part of the type name, and it is distinct from the `CHAR` type. From the docs:

>The type ""char"" (note the quotes) is different from char(1) in that it only uses one byte of storage. It is internally used in the system catalogs as a simplistic enumeration type.

Normally you don't really need to worry about this special internal type, but I did run into some issues once when I was doing some work that got into system tables and needed to work around it with the ORM I was using.",1536174143.0
linuxhiker,I can't think of a single modern or good reason to use CHAR. It is a legacy type for a legacy time.,1536176124.0
mrweck,I mostly use DBeaver and it serves me very well,1536087719.0
JeremySalmon,DBschema work well for my needs.,1536130953.0
lpolstor,"-- a bit unclear, but assuming you want to just insert the image with user_id and path as parameters, and get the parameters + inserted image_id (id in your speak), try this:

with img_insert as
(
   insert into image (user_id, path) values (values) returning 
   image_id, user_id, path
)

select * from img_insert;
",1536066231.0
mw44118,"Do it!  It makes queries easier to read, you can easily set intelligent defaults in referencing tables, and I have never observed a performance penalty.",1536065696.0
,"A quick Google brought up this:
https://stackoverflow.com/questions/15477005/primary-key-as-text

That's basically your question right?
If not let me know and I'll see if I can answer it. ",1536064189.0
cyberst0rm,reason to run postgrss in docker?,1536092619.0
pypt,Great heads-up!,1536187253.0
thelindsay,"Unless I'm misunderstanding the concept of a ""lost update"", this article doesn't seem to be accurate.

Repeatable reads are hosed by conflicting updates or deletes in concurrent transactions that commit first, but not inserts (which aren't seen), and the ""blocking"" is from waiting to see if a conflicting update or delete is committed (if it's rolled back we can commit, but if it's committed we must roll back).

Serializable has the same rules as Repeatable Read but also watches for ""anomalies"" which are still possible within these rules, e.g. an insert in tx1, selected by tx2 used to insert in tx2 in such a way that affects a select done in tx1. Even though concurrent tx1 and tx2 have independent snapshots of the database as of their first statement, they use this info in a way that couldn't be reordered e.g. tx1 then tx2 is not the same as tx2 then tx1. If tx1 finished before tx2 began there's no problem.",1536050483.0
mikaelhg,"The Entity-Relationship Diagram, or ERD. Google for ERD, you'll find hundreds of tools.

If you want to just demonstrate a concept, you can just draw a diagram with the table names, rather than including all the detail.",1535933817.0
ppafford,BDBeaver is free database tool that can also generate ERD https://github.com/dbeaver/dbeaver/wiki/ER-Diagrams it works on all platforms ,1536002993.0
obrienmustsuffer,Why would SQL not be acceptable in a thesis? I'd just put it into the appendix.,1535989528.0
charettes,"First you'll want to make sure your unique constraint is `DEFERRABLE`. If it isn't you'll want to drop it and add a new one that is.

If your unique constraint is `INITIALLY DEFERRED` you should be good from now on.

Else you'll have to make sure to defer your constraint within a transaction

    BEGIN;
    SET CONSTRAINTS constraint_name DEFERRED;
    UPDATE ...
    COMMIT;",1535843244.0
alephylaxis,"I didn't see anyone answer the original question, so I'll hit that one first. Even though each record will be unique at the end of the statement, values will still be duplicated at least once during execution and pg doesn't know what the final state will be, only that there's a conflict during execution.

As far as the remedy, honestly setting the constraint deferrable, etc is making things too complicated. Just start a transaction, drop the unique constraint, make the changes, add the constraint back (if anything messed up and is still a duplicate value you can fix it here or just rollback), then commit the tx. This won't affect your ability to create references to this column, and if you need the child records to change when you do this update, set the foreign key column in the other table to ""ON UPDATE CASCADE"". ",1535923407.0
mrweck,I believe those are queries run by PgAdmin 4 to feed those graphics,1535933191.0
therealgaxbo,That is not the stats collector - the stats collector is what produces that data in the first place.  You must have installed some sort of monitoring tool that is executing those queries.,1535838907.0
pypt,"Go through the “tuning PostgreSQL” wiki page, probably you can get your single server to do 10k requests just by fiddling with some params (ours can do 10k just fine, but it has 192 GB of RAM, 32 vCPUs and it’s bare metal). Look into the queries that you’re running, maybe they can be optimized too. Bottlenecks might be your webserver (not sure what Gazelle is though), you might benefit from asyncio (many languages can do this now) and / or pgBouncer to save some time having to reconnect.

Otherwise, easiest options are to just get a bigger server (or just faster disks or whatever), shard by customer, or use something like Citus (which is sharding too).",1535826397.0
jeffdn,"I may be misunderstanding your use case here, but why not maintain balance separately, and update that in batch? You can have an immutable-row transaction table with timestamps, and keep track of the last-updated time for the balance table, and periodically (once every minute? hour? depends on frequency required) update the balance table with all transactions since the last update. You could even calculate the “pending” transactions to get a probable current balance. Really, the thing that would slow you down with the setup you’ve described is updating the balance table. Minimizing the number of updates and batching them together will reduce the number of locks, thereby speeding up all database transactions.",1535825653.0
pypt,Have you tried EXPLAIN ANALYZE in order to find out what is actually being run?,1535807791.0
metaldark,"Interesting story, I'm sure many of us have had similar experiences.  Instead of some esoteric tricks, it's a tale which reminds us all to check the documentation to make sure it matches our assumptions about what's possible using built in tooling.",1535732916.0
stump82,I use directory format for my pg_dump to backup in multiple threads as well as restore in multiple threads.  You can use the same -j parameter in pg_dump to the number of threads you want. ,1535759514.0
smugbug23,"If there is enough drive space on the dev server to store 2 instances of the database (and at 50GB, it seems unlikely there wouldn't be) they could restore to a server on a different port at leisure while continuing to use the old dev instance, then shut down the old instance and change the port on the new instance in a matter of seconds.

Or set up a warm standby whenever it is convenient, and then stop the old dev instance and promote the warm standby to be the new dev instance upon demand.",1535978708.0
KitchenDutchDyslexic,"Thanks for writing this up, and sharing!

",1535746824.0
pypt,"Couldyou post some schema, queries and the conversion code that you’re trying? Also, are you sure that all of the UTF-8 fields could be “downgraded” to latin1, i.e. they contain only characters within latin1 charset?",1535667876.0
Nunki63,"Your database uses the UTF8 encoding, right.  Now your client needs LATIN1 to show characters like ùè in the right way.  The thing you need to do after connecting to the database is to set your encoding with the statement: 'set client\_encoding to 'latin1';  Then your client should display all special characters as should be.",1535728837.0
billrobertson42,"UTF-8 is a superset of LATIN1. If the server is using UTF-8, then client needs to understand UTF-8 for things to function correctly. Converting you the data to a narrower charset will likely result in a loss of information. AFAIK, postgres clients do this automatically, but you should check.

Also, if you suspect that there are hidden characters, then use a combination of the [length() and substr()](https://www.postgresql.org/docs/current/static/functions-string.html) functions to print them all out one by one. 

It would also help to know which version of the database you're using and what you're using to connect to it with.



If you really suspect",1536505788.0
swenty,"That's sort of a funny way of explaining it. I would think that the pattern matching method (""LIKE %term%"") would be the first approach that occurs to most naive implementers familiar with SQL. Anything using tsvectors would require additional research, at which point you'd immediately pick up on the necessity of creating an index against the vectorized data. So I would probably put my explanation in that order:

* LIKE '%' || term || '%'
* tsvector without an index
* tsvector with an in index

Anyway, it's a minor gripe. The article seems fine otherwise.",1535654418.0
InfoSec812,'database connection error' viewing the article. Oh! The irony! ,1535715870.0
scttmthsn,"For a gui dbeaver gets a lot of praise, else pgcli for an enhanced cli.",1535625858.0
planetworthofbugs,"Datagrip is awesome, well worth the license.",1535627097.0
HolyKappaKappa,"Like I've agreed in a separate post, I use DBeaver (CE).",1536413469.0
braydenjw,"Sounds like the postgres user on the database at host.mydomain.com might not have read access to that file?

Try running as root on host.mydomain.com \`chown -R postgres:postgres /path/to/pgdata\` and \`chmod 664 /path/to/pgdata/pg\_log/postgresql-Tue.log\`.",1535608176.0
pypt,"Try sudo’ing as “postgres” user:

sudo su -i postgres bash

and accessing both that file and directory tree that contains it.",1535668009.0
SelectCompare,Thanks for the post. I lke the enum data type - so clean comparing to CHECK constraints.,1535582942.0
threeminutemonta,[This](https://www.postgresql.org/docs/10/static/different-replication-solutions.html#HIGH-AVAILABILITY-MATRIX) matrix from the docs may help. Unfortunately I use a managed service and have no experience in rolling this out. ,1535533358.0
stympy,"I’ve been very happy with the combination of patroni, pgbouncer, and consul-template for managing auto-failover. ",1535553204.0
magott52,"This is cool.

And another example of a feature that’s only available in Microsoft SQL Server Enterprise Edition ($$$), but effctively free in Postgres.",1535555340.0
EvanCarroll,"psql tips? Uninstalling it and using [pgcli](https://github.com/dbcli/pgcli)

    pip3 install pgcli",1535522617.0
ptman,"If you have a unicode terminal, I would suggest using `\pset null ␀` instead of something that can be typed easily on the keyboard ( http://www.fileformat.info/info/unicode/char/2400/index.htm ).",1535535500.0
fullofbones,"I didn't realize my talk overlapped with Magnus'. Well, at least I can practice with the empty room. :D",1535457242.0
francisco-reyes,"> Each week 4 classes

Is it literally just 4 rows worth of data? It sounds like a volume low enough that it would not matter how you get the data in.

>large chunk of inserts?

Could you describe an estimate number of rows? When I see ""large chunks of inserts"" that gives me a very different picture from "" each week 4 classes"", so it would help if you gave more background.",1535391840.0
afig2311,"It seems like you are trying to ask two questions at once:

> My question is what is the best way to pre-load the db with information about these classes?

What do you mean by ""best"" way? Performance is not a concern with only 4 rows/week, regardless of how you add the data. If you are looking for the easiest way, well that depends on your skills and how the data is currently formatted.

>Should I schedule a cron job or simply write in a large chunk of inserts?

This sounds more like you are asking if you should automate this task to run every week, or to do it manually after a certain number of weeks. That's up to you, if you already know how to work with cron, and would like the data to be in the DB as soon as possible, then a cron task could work.

***

However, there is no need to write in a large chunk of inserts regardless of method. The `COPY` command is designed to make importing data easier. If you can get the information on classes in a format such as `csv`, or even just a text file with a structure in place (e.g. spaces between attributes, new line for every row), the `COPY` command makes importing the data a one-liner, regardless of whether you have 4 rows or 4 000 000 rows. See https://www.postgresql.org/docs/10/static/sql-copy.html for more information.

Edit: If you are working with a remote server, then the psql `\copy` command will likely be much easier to implement. (Using only the SQL `COPY` above requires the Postgres server instance to have access to your input data, whereas the psql `\copy` only requires the psql client, which is usually located on your local computer, to have access to the input data) See https://www.postgresql.org/docs/10/static/app-psql.html#APP-PSQL-META-COMMANDS-COPY",1535417676.0
Amphagory,"Are you trying to import all the data from PostgrSQL to excel?
Are you using python to do this? 
You could use psycopg2 library to execute a query and the result will be a list.
Then, you could use csv library to write the result to a csv file.

Load the csv file into excel.",1535338016.0
dmr7092,"I've used the Actual Technologies odbc connector. For a user who's an Excel poweruser but not much for writing code, its a good option.

&#x200B;

But if you're already using python, and can script and schedule these things, I prefer writing my reports in code. Here's a website with a ton of tutorials that help: [http://pbpython.com/](http://pbpython.com/)

&#x200B;

And if you need to handle formatting, charts, etc., here's what you should brush up on: [https://xlsxwriter.readthedocs.io/](https://xlsxwriter.readthedocs.io/)

&#x200B;

I have around 20 reports that I run regularly for my company this way. ",1535372279.0
threeminutemonta,"Would you consider to use libreoffice. Seems like people have got it working [link](https://ask.libreoffice.org/en/question/76610/connecting-to-postgresql-database-with-libreoffice-base/)

Consider to you a popular python data science called pandas. It uses another library under the hood for excel reading and writing that you can also use directly. What don’t you like about the current approach of importing to python first?

",1535358857.0
pypt,"What’s the reason to have a single function? Given that you’re returning two different things (a list of companies and a list of employees), why not have two different functions?",1535215900.0
thelindsay,"Try RECORD return type: https://stackoverflow.com/a/6085167

Trade-off is that you declare the type during usage, to unpack the columns from this composite type.",1535211325.0
SelectCompare,"Perhaps you should reconsider your approach. While you can do such thing, it is generally considered as a bad practice. The code consuming the data gets more complicated and testing becomes more difficult. 
A function or procedure retuning a single data set gives you an opportunity to implement an asynchronous and reactive UI for example, where you can load parts of the screen independently from each other.",1535242638.0
depesz,"Long story short, you can't. You can return multiple cursors though. ",1535215578.0
bar-a-baz,"There you go, the best document you can find: 
https://www.postgresql.org/docs/",1535198823.0
makif611,thank you :D,1535203662.0
r0ck0,"> difference between a primary key and an unique index

Don't worry about it, just set your primary keys and the index will be created for you.

You only need to manually create unique indexes on other non-PK columns.  You might not need many of them.  

But you generally want to use them on linking tables though (multi-column-index on both of the FK columns) so that you can't create multiple redundant links between the same two records.

> school - varchar(40)

> address - varchar(40)

> city - varchar(30) 

> map_url - varchar(300) 

> ad_name - varchar(40)
> email - varchar(60)


Might be ok, but seem kind of restrictive.  Personally I mostly just use `TEXT` on postgres  (no need to specify a length) and put the limits on the web form validation most of the time.

Other people recommend putting the limits in the database, which is fair enough.  But you might want to raise them a bit, they all look pretty low to me.


> size - character(1)

What's this for?

> phone - smallint

Is this just an internal extension number or something?  More generally with full phone numbers you should usually just store them as text, as entered by the user and filter on output when needed.  Too many different ways users enter phone numbers, and sometimes the punctuation helps you figure it out.  Also some numbers start with zero or +.

Also, unrelated to DB, but don't try to get too strict with validating email addresses in your forms, people fuck this up all the time and release systems into production that refuse to accept valid email addresses.  It's infuriating.

> smallint (auto-increment) *primary key

In general I think using SMALLINT for the PKs is kind of overkill with micro-optimisations.  Especially on the STANDINGS_SPORT_YEAR year table that I assume will have the most records.  I just use UUID on everything these days, I think it's worth it, even if the system will only ever be on one server.  I've stopped using auto-increment altogether pretty much, unless I need to generate some simple human-readable number such as invoice numbers, and I don't make them the PK.

And if you ever get to the scale where the performance would be of concern, you probably want UUIDs anyway, and you'll have bigger concerns and solutions that PK size anyway.

Many of the regretful technical decisions (and heaps of wasted time) I've made over the last 20 years programming have been over silly micro-optimisations that never made any difference in the end.
",1535184482.0
cbunn81,"Look into normalization. I see a lot of duplication of names (for teams, divisions, locations, etc. ) The proper thing to do is use the ID for those as a foreign key if it's a one-to-many relationship. If the relationship is many-to-many, use a separate lookup table of IDs. ",1535221253.0
exhuma,"*edit*.... fscking WYSIWYG editor in new-reddit b0rked the formatting...

To store the documents, simply use a column of type `TEXT`.

To display it again you need a library that converts the markdown format to the format you like (I assume HTML?). Simply get the column from the database and run it through the library to convert it.

Pseudocode (you did not specify any language):

    # Create the table
    CREATE TABLE documents (
        id SERIAL,
        markdown_content TEXT -- Careful here... no size-limit. Subject to DOS attack.
    );
    
    # Saving a document
    store_query = ""INSERT INTO documents (markdown_content) VALUES (?) RETURNING id""
    result = connection.execute(store_query, [my_markdown_data])
    stored_id = result.fetchone()[0]
    
    # Retrieve the stored document
    import markdown_library
    query = ""SELECT markdown_content FROM documents WHERE id=?""
    result = connection.execute(query, [stored_id])
    row = result.fetchone()
    content = row.markdown_content
    html_content = markdown_library.to_html(content)
    print(html_content)",1535151030.0
nikoz84,Save like text ,1535150129.0
etrnloptimist,"Indices only work well when they access O(1) data from your DB. If they need to access O(n) data, the query will suffer regardless of index use.

If you know the predicates beforehand, try creating materialized views that store that data in summary tables.",1535136703.0
francisco-reyes,"Which version of Postgresql? Are you using parallelization? Sequential scans on  the newer versions, I believe 9.6 and 10, can use more than one CPU.


Also, any reason you are using that schema and not a Star schema? If not familiar with Star schema see, as an example [this](http://datawarehouse4u.info/Data-warehouse-schema-architecture-star-schema.html) url. A flat design like that would work, in my opinion, on a columnar database such as the Postgres derived Greenplum.


Also, is this a timeseries? Because if it was TimescaleDB may be an option. See for example [this](https://blog.timescale.com/timescaledb-vs-6a696248104e) post.


Another option, if you can run this in more than one machine, would be [CitusDB](https://www.citusdata.com/)",1535164127.0
riksi,"You could use a search engine / inverted-index. See elasticsearch/solr/vespa.ai, it should be a lot faster.",1535152716.0
thelindsay,"In addition to the indexes you could also increase the statistics target for the table.

If the optimisation goal is to have literally any question answered quickly, and you don't want to apply architectural changes, you could approach this by planning to progressively optimise based on actual usage patterns.

What I mean is to plan to track the table and index usage stats, and maybe even statement logging, for a few weeks / months to build up a picture of which columns get predicates applied more often than others, which columns never get queried, etc., and optimise for the more common cases e.g. with predicate or covering indexes.

At the beginning everyone would be experiencing the same sort of query performance, but over time the common cases would see improvements based on your optimisations.",1535153650.0
pypt,"Could you post a sample schema (the one with 100 columns), a sample query and an EXPLAIN ANALYZE for it?",1535155055.0
ants_a,"Indexes aren't magic, they are just a different sortings of row pointers to make finding a set of rows matching a condition faster. The essence of the problem you are having is called the curse of dimensionality. With a large amount of different predicates with low selectivity there is no effective indexing strategy.

Looking into using GIN indexes over jsonb columns for some equality predicates might be useful. GIN has a couple of optimizations for combining high selectivity and low selectivity predicates. For one, it can skip over batches of rows for the low selectivity predicates, and as the row pointers are sorted and compressed, combining multiple predicates is considerably faster.

Also, maybe lower your planner costs for I/O so the planner knows that low selectivity indexes are not worth the effort. ",1535179423.0
flyingmayo,"Yes, 10 seconds is underwhelming unless you're dealing with a reasonably large data set..

I'm very skeptical that an index on every column is the right solution.

How many rows are in this table?

What is the cardinality of the columns used in your predicates? (I'm guessing it's pretty low?)

There's no seqscans anywhere in your query plans?  Can you post an anonymized query plan? (https://explain.depesz.com/)
",1535137889.0
anras,"It's hard to say without more info, but potential tools that may help are [partitions](https://www.postgresql.org/docs/9.2/static/ddl-partitioning.html) or [clustering your table](https://www.postgresql.org/docs/9.2/static/sql-cluster.html).  
  
Are your criteria pretty much arbitrary based on user input? If any/all of the 100 columns can be in the where clause without any kind of predictability, it may be a tough problem.",1535137969.0
r0ck0,"Awesome work!

I'm usually one of the people a bit dismissive of ""yet another"" competing program when there's already a lot of options out there.  

But good open source SQL GUIs are still lacking I think.  Dbeaver is good, but despite using it a bunch of times, I still can't quite gel with its interface.

I was going to suggest you look at HeidiSQL for some inspiration on making a user friendly interface, but looks like you already are!

>  Goat - something between Heidi (sql) and Toad.

I really liked HeidiSQL when I was on Windows, but never had much luck with it in Wine on Linux.

One fairly simple thing I like about HeidiSQL that is missing from many others is that the results use colors based on column type:

* [Example screenshot, note all sizes of INTs are blue, strings are green, dates/timestamps are red etc.](https://i.ytimg.com/vi/bC3xmUsCBw0/maxresdefault.jpg)

...so that could be something to add, that hopefully isn't much work, but lets you stand out a bit?

Also nice to see you're using the silk icons.  I still think they're great.  So many other more ""modern"" flat/monochrome icon sets are too hard to tell apart.


",1535003826.0
1new_username,"It isn't finding the pg_backup.config file.  Here are two options:

Option 1:  pass the location of the pg_backup.config on the command line when you run the script, like this:

pg_backup.sh /usr/local/bin/pg_backup.config

Option 2:  just edit this section of the script:

> if [ $# = 0 ]; then
>         SCRIPTPATH=$(cd ${0%/*} && pwd -P)
>         source $SCRIPTPATH/pg_backup.config
> fi;

 to be something like this:

> if [ $# = 0 ]; then
>         SCRIPTPATH='/usr/local/bin'
>         source $SCRIPTPATH/pg_backup.config
> fi;
 
Either should work.  Your current error is in that block of code above.  Since you aren't passing arguments to the script (if [$# = 0];), it is trying to figure out where the script is currently running from and then get that directory and look for the pg_backup.config there.  It is not finding it and getting confused and erroring out.


",1534884992.0
hunleyd,You should really look into pgbackrest instead of trying to create a backup solution yourself.,1534962725.0
HighlordDerp,Awesome!,1534900585.0
merican_atheist,Datagrip is really great for this. That said it's rather trivial to local port forward over ssh yourself if your tool of choice does not support it.,1534782131.0
melt_Doc,"DBeaver does it, too",1534786623.0
chripede,Datagrip will let you tunnel through SSH,1534781443.0
pypt,"AFAIK pgAdmin 4 can connect through SSH tunnels, but one has to spend some time getting used to it first.

No one’s preventing you to create an SSH tunnel separately and connect to it using any GUI of your choice!",1534786157.0
respack,"Datagrip looks good. However, it isn't free.",1534783190.0
cachedrive,pgAdmin3/4 and every single GUI client I've seen allows you to tunnel over SSH.,1534786236.0
Cabelitz,SQLectron?,1534790926.0
depesz,"Any client that allows you to setup where you connect can connect over ssh tunnel.

Start ssh tunnel with port forwarding, and then start gui client, and it's done.

I can't imagine having a db client that would *not* work in such case.",1534846179.0
r0ck0,"Every GUI client I've seen (in the last 10 years at least) has supported connecting through SSH.  Would be dumb of them not to include that.

I'm using a jetbrains IDE (so I get the same as functionality as datagrip built-in).  Otherwise dbeaver is the best free one.",1534942457.0
Abstrct,"The ""server programming"" part of postgres is where I had/have [the most fun](https://github.com/abstrct/schemaverse). I would suggest you keep going.",1534775336.0
mage2k,I'd consider chapters 37-42 to be necessary learning for Postgres DBA work.  ,1534789637.0
leandro,Not in junior roles.  You can safely skip it until you read the rest and get some familiarity with systems programming.,1534800533.0
boy_named_su,"what you want is called Optimistic Concurrency Control

add an integer ""version"" column

""check out"" the record by selecting it, including the version

when you try to update it, make sure the version number is the same as when you selected it, else tell the user someone has already made a change
",1534714527.0
flyingmayo,"Read up on select for update: https://www.postgresql.org/docs/current/static/explicit-locking.html

> FOR UPDATE causes the rows retrieved by the SELECT statement to be locked as though for update. This prevents them from being locked, modified or deleted by other transactions until the current transaction ends",1534716367.0
,[deleted],1534643518.0
SulfurousAsh,"Can you instead have a separate table for organizations along with some column such as ‘last_activity_at’?

Querying such a large table just for the organizations is clearly not the most optimal way to do it.. 🤷‍♂️",1534645248.0
threeminutemonta,You can stream WAL to s3 using lambda. Recent [doc](https://aws.amazon.com/blogs/database/stream-changes-from-amazon-rds-for-postgresql-using-amazon-kinesis-data-streams-and-aws-lambda/),1534543847.0
devourment77,"Your on RDS? If so,you can make snapshot backups.  If you want pg_dump you could setup a lambda to run on a schedule.",1534533867.0
aruba86,"Hi,

Yea i did set up snapshots. But now I want also to set backups to S3 but I am out of idea how.

And yes DB is on RDS.",1534535048.0
aruba86,double post,1534537321.0
ful_vio,"You can have a look at pgloader: 

[https://pgloader.readthedocs.io/en/latest/](https://pgloader.readthedocs.io/en/latest/) (Documentation)

[https://pgloader.readthedocs.io/en/latest/ref/mssql.html](https://pgloader.readthedocs.io/en/latest/ref/mssql.html) (Direct link for usage with mssql)",1534518498.0
pypt,"0x0a in ASCII is a linebreak, and JSON doesn't allow those. You have to use ""\n"" instead.",1534455225.0
fullofbones,"From past posts of this type, it looks like you've also got an old installation of the tsearch2 module that's preventing you from upgrading. The current solution appears to be dropping those tables from the public schema and trying again.",1534447605.0
iiiinthecomputer,The explanation is that regproc columns contain the oids off functions. if you dump and restore the oids of many functions may change. pg_upgrade does a dump and reload of the schema internally. But it doesn't rewrite the tables so it doesn't remap the old oids to the new oids via the function name like a dump and restore of schema *and data* would.,1534466220.0
pyconza,"**I got the title wrong, please note that Open Source week runs from 8-14 October. If you are a mod, please fix the title. Thanks**",1534440791.0
vegasmacguy,"pg_dump happens in a transaction (think BEGIN...COMMIT) and sets transaction isolation to serializable. This means that it will only backup any data that was committed prior to the start of the backup. 

https://www.postgresql.org/docs/9.6/static/transaction-iso.html",1534392979.0
flyingmayo,"pg_dump is very definitely made to be a backup utility and will either give you a complete dump of your data as it was when you -started- the pg_dump or it will  error and tell you why it errored.

I use both pg_dump and pg_basebackup on all my servers.  They have completely different use cases but they are both very important to create a proper backup strategy.

In order to understand how pg_dump is able to dump your data in a consistent way despite the database being updated while the pg_dump is running, you must have a basic understand of how transactions and the MVCC architecture work.

That said, pg_dump will still do an excellent job of creating a consistent logical backup of your data whether your understand how it is doing it or not.

",1534393387.0
jottilf,"When your database gets large enough a pg\_dump takes 30 minutes, it is probably a good idea to look elsewhere. I mean, the utility works and produces consistent dumps, but it is not very efficient as a regular backup utility.

I recommend taking a look at [Barman](https://www.pgbarman.org/). It takes a bit of configuration to get it all up and running, but it will make database backups (and more importantly: restores) easy and reliable. You can even restore to a given point in time if you need.",1534405841.0
depesz,"yes. pg_dump backups are fully consistent. it is being used by using ""repeatable read"" transaction isolation level (not serializable, like some other commenter mentioned).

Basically - you will get snapshot of data as it was when backup started.",1534421401.0
mrweck,"Pg_dump isn't made to be a backup utility. You should look into PITR, WAL archiving, and pg_basebackup for that. That being said, AFAIK, when you run pg_dump, it'll begin dumping object by object. So it is possible to have partial or incomplete data on the backup file when dumping multiple tables, for example, as you can insert on and update tables that have already been processed and tables that are yet to be.

Sorry for the formatting, I am on mobile.",1534392450.0
rouen_sk,It sounds like you want SQLite - its good for small datasets (not in billions rows) and it is contained to single file you can save wherever you want and even encrypt. Full Postgres server is probably overkill for you.,1534351650.0
bar-a-baz,"You could try out Libreoffice Base. It's very similar to MS Access and available for Mac. You can store the database in a file (.odb) or even use Postgres as the data storage.

Postgres stores the data in a folder you specify on installation. For security, I'd go for full disk encryption (FileVault on Mac). It has the added benefit of protecting all your data, not just Postgres.",1534354865.0
threeminutemonta,I installed PostgreSQL on my Mac for my dev environment. It is just locally installed and by default won’t allow any network access without changing config. You will be able to set a password during install too. You will be able to use pgadmin to import csvs into a table.,1534365354.0
jarym,"Since you just want to do SQL on CSV files have you looked at:

Q: http://harelba.github.io/q/

TextQL: https://github.com/dinedal/textql

Both work on Mac",1534592797.0
pypt,"It sounds that you have a Windows machine, so I’d just go for BitLocker encryption and be done with that.",1534351167.0
dopperpod,/r/DoMyHomework ,1534346416.0
zieziegabor,"There is no one answer to this.
See my comment on another post(and that post too):
https://www.reddit.com/r/PostgreSQL/comments/8n44wc/new_to_postgresql_just_some_basic_questions/dztoqz5/
",1534344474.0
spinur1848,Try adminer: https://www.adminer.org/,1534345077.0
aruba86,"## Any idea why I am getting error when I want to connect to DB?

## No extension

None of the supported PHP extensions (PgSQL, PDO\_PgSQL) are available.

**ReplyShareSaveEdit**  
",1534538435.0
aruba86,Thank you!,1534360764.0
chripede,Please don't do it like this. Use pg_basebackup as a minimum. Wal-g or wal-e makes it even better,1534351744.0
flyingmayo,"If something is wrong with postgres, it will tell you in the logs.

You haven't posted any logs which makes me wonder if you are reading them.

Please post the last few lines of your log so that we can help.

Also, I would not recommend using pgadmin when you are getting started.

Later, when you are more comfortable with postgres........I would still not recommend using pgadmin.  

pgadmin is a tool that can be used with postgresql.

It is by no means required.

I've been running hundreds of postgres clusters for coming on two decades and I think I've launched pgadmin, maybe 5 times total?

 psql, it is well documented here: https://www.postgresql.org/docs/current/static/app-psql.html

Regarding your missing database error, you're on the right track.  psql will attempt to connect to a database named the same as the connecting username unless -U is specified.

Try specifying -U postgres in your psql connection string.  That should connect you up to the default postgres DB as the default postgres user.

Once connected you can do something like

CREATE USER someuser;
CREATE DATABASE somedb OWNER someuser;

Then you can connect to that db using the new user you just created:
psql -U someuser somedb

(Caveats for possible issues with your pg_hba.conf file which may need to be updated to allow (probably local) connections to the somedb database by the someuser user.)

Details about the pg_hba.conf file can be found here:

https://www.postgresql.org/docs/current/static/auth-pg-hba-conf.html

don't give up.

post logs.

Hug your mom.",1534316100.0
tedroshaile,"EnterpriseDB has a free tutorial, [https://www.enterprisedb.com/free-postgres-training](https://www.enterprisedb.com/free-postgres-training)",1534315880.0
ppafford,"PGAdmin 4 has a server that runs, it could be this

I’d suggest trying to install and run PGAdmin 3

Also what OS did you install PostgreSQL on? Is this for local development?",1534333208.0
warmans,If you're running windows I would consider running Pg in a docker container to alleviate some of the environment issues (just remember to create a volume or you'll lose your data when you kill the container). Pg admin not the best thing ever so you could also consider using a different GUI client. I use valentina studio free and it's pretty good.,1534335531.0
merlinm,"\> FATAL: database ""<myname>"" does not exist

if you got this far, you are all set up and good to go -- database is running, etc.  you are just using the wrong database user name. try postgres, first, if that doesn't work there is way to create database user.",1534345770.0
boy_named_su,"PG Admin 4 is hot garbage.

Use PGAdmin III",1534352086.0
Siltala,zone->>'country_iso'='US',1534302019.0
therealgaxbo,"Your table must not contain the data you think it does - your query is fine as you demonstrated yourself with the second query.

Is `{""country_iso"":""US"",""postal_code"":[""92121"",""92122"",""92129"",""92130"",""92131""]}` the entire content of the field, or is it nested inside an outer object/array?  Are there any typos, whitespace...?",1534322498.0
thelindsay,"It could probably be clearer that these examples avoid pitfalls that are easily stumbled into if adapting this pattern to other problems:

- the CTEs get a certain order of execution by using prior (returning) data, otherwise unchained DML CTEs have no determined order
- the DML data is (mostly, except products query) consistent for the same reason, otherwise even if the CTEs executed in the same order they'd see the same starting snapshot but not each other's updates
- race conditions are avoided (mostly, except products query) by using only insert, not updates or deletes which may be already modified by concurrent transactions (after snapshot was taken but before DML)

Also it's clearer to say ""a single statement"" since whether or not some statement(s) are in a single transaction depends on the design of the app / orm / sql script. The integrity offered by a tx is variable as well, depending on isolation level and how it's used.

Related:

- https://stackoverflow.com/questions/11532550/atomic-update-select-in-postgres
- https://www.depesz.com/2012/06/10/why-is-upsert-so-complicated/
- https://www.postgresql.org/docs/current/static/queries-with.html#QUERIES-WITH-MODIFYING",1534313959.0
chock-a-block,"Create a group, then give that group permissions to do whatever you want on the database.

I haven't actually tried it, but, there is GSSAPI support, so you could use LDAP/Kerberos to do it centrally.",1534255111.0
mage2k,[ALTER DEFAULT PRIVILEGES](https://www.postgresql.org/docs/10/static/sql-alterdefaultprivileges.html),1534273806.0
ppafford,"Just thinking off the top of my head, I’d suggest 

- recursive CTE
- time series every day of the year for the CTE
- in the CTE calculate the year total for that day

",1534202238.0
mrweck,"Maybe something like:

Select user_id, count(distinct user_id) filter (where date between date and date - interval '365 days')

From user
Group by user_id

I haven't tested though.
",1534204148.0
pypt,"You didn’t provide any schema so we can’t come up with a specific query for you to run. I think you’ll need something along the lines of:

* generate_series() to create a list of days for which to generate the aggregates
* WHERE date BETWEEN generated_date AND generated_date - INTERVAL ‘365 days’
* (maybe) date_trunc() to truncate timestamps to dates
 ",1534247429.0
ies7,https://stackoverflow.com/questions/45723931/rolling-7-day-count-distinct-in-postgresql,1534254354.0
thelindsay,"This looks great, thanks for sharing. It is impressive especially that it has a general solution for cycles, rather than relying on custom business rules. On that note have you found this did most of the work already or did you need to apply more db specific tweaks before using the resulting data? If so do you think any of those generalisable too?

I have a couple of suggestions which hopefully are helpful:

- there doesn't seem to be a test db / suite. Testing ETL can be tricky but it can boil down to 2-way ""except"" queries (a not b + b not a = nothing) with expected data loaded by ""copy from"" to temp on-commit-drop tables (or just rollback). More thorough versions can unpivot data & compare types etc which can be done with jsonb. Although this assumes sampling is deterministic which may be against the philosophy of the library.
- maybe have a look at the psycopg2.sql module for safer compositions than f-strings, for the habit of it.",1534254608.0
boy_named_su,https://openexchangerates.org/ ,1534182555.0
jk3us,"https://www.federalreserve.gov/releases/h10/hist/dat00_eu.htm

Looks like there are some download options that are probably useful.",1534186971.0
aruba86,"Wow, thank you. Will try to import it.",1534198009.0
edfreitag,https://www.ecb.europa.eu/stats/eurofxref/eurofxref-hist.zip is a pretty simple csv that you can link via FDW,1536915886.0
pypt,"* Make `PRIMARY KEY` into `SERIAL`, then it will autoincrement.
* `bytea('some_string')` won't read the file and convert it to bytes, instead it will convert any string into blob:

&#8203;

    pypt=# select bytea('hello!');
         bytea      
    ----------------
     \x68656c6c6f21
    (1 row)
    
    Time: 0.290 ms

To read the file and return `bytea`, you need [`pg_read_binary_file()`](https://www.postgresql.org/docs/current/static/functions-admin.html#FUNCTIONS-ADMIN-GENFILE). Do note, however, that it's going to be the server which will be reading the file and not the `psql` utility, so you need to make sure that the server has adequate permissions to access the file that is being read.

* While there are [workarounds](https://stackoverflow.com/a/6731452/200603) for reading `bytea` into a file using the `psql` client utility, the utility itself is unfortunately not well suited for the job. I'd suggest that you read / write your data with some sort of a mock [Python script](https://gist.github.com/illume/8aa7c2d03273da846bf4361236a4f3f9) or something similar.",1534171855.0
dbtgbp,"You could hash the field

select md5(the_file_itself);

and compare with md5'ing the file.


When I need to extract a bytea field into a file, which I was doing last night, I tend to go:

> select encode(bytea_field, 'hex') from whatever where foo=bar \g /tmp/bytea.hex

$ xxd -p -r /tmp/bytea.hex > /tmp/bytea.extension

",1534406048.0
ptman,"    SELECT octet_length(THE_FILE_ITSELF) FROM file_locker_test;

does that match the length of the string `'\\Users\\My Name\\Pictures\\picture.jpg'`?",1534171937.0
KitchenDutchDyslexic,might be interested in https://schemaverse.com/ if you want to learn pg.,1534135415.0
cazzer548,What do you want to use Postgres for?,1534134793.0
chemiey,I took Sololearns SQL-course. Great introduction and good explaining language. https://www.sololearn.com/Course/SQL/,1534145555.0
cachedrive,"You need to install PostgreSQL and set up a project / learn how to do basic things like:  
\- Install PG

\- Move an existing PG install to another drive  
\- Configure pg (postgresql.conf / pg\_hba.conf)

\- add roles / grants

\- learn how vacuum works",1534158502.0
lykwydchykyn,"can't vouch for that site, but I wouldn't worry about what version is being taught.  Things don't change that much between versions, and many sites are still going to be running 9.x.",1534171008.0
rouen_sk,"Also, geospatial applications has been certainly on the rise and postgres is literally unchallenged here thanks to postgis. ",1534058751.0
jakdak,"IMHO, Cloud hosting services like AWS RDS where it is the best open source (i.e. no additional licensing cost) option have a lot to do with that increase.

And the product itself has closed the gap on Oracle/SqlServer substantially in the past few years.",1534045779.0
semigroup,Personally I really appreciate the ‘take time and implement things right the first time’ philosophy. That’s exactly what I want in a DB.,1534046054.0
blinkingm,"I think it was mostly due to MySQL getting bought over. Mariadb is only starting to get widespread acceptance. I tried moving my system to Postgres but moved it back once my company start allowing Mariadb (only a few months ago). Mariadb just feels faster, I don't use the features Postgres has over Mariadb and am annoyed by the little workarounds I had to do on Postgres to get certain things which Mariadb does by default, like auto indexing and programmatically getting varchar lengths, smoe date functions, etc",1534049454.0
cachedrive,I don't even recall writing this when I was drunk. Wow Im a nerd. Better than texting my ex I guess,1534079609.0
mikegold10,You should install PG 11 Beta 3 in production. That might spice up your life a bit.,1534078478.0
stump82,Lol this is another level of love for postgres. I hope this continues to be a thing,1534108423.0
Zeugiram,True,1534096461.0
wrongsage,"I love PgSQL this much as well.

Too bad at work they force me to work with MySQL...",1535809849.0
coder111,"You can store files in BLOB column in a PostgreSQL table. As others said, depending on situation it might be more efficient to store files on disk, but PostgreSQL can totally do it.

How to insert data from a windows shell- that's a bit more difficult. Googling found me this:

psql> \set content `cat my_big_textfile`

psql> INSERT INTO files VALUES (:'content');

Other option is ""insert into category(category_name,category_image) values('tablette', lo_import('D:\image.jpg'));""

I'm not on Windows, won't be able to test these suggestions.",1534022468.0
pypt,"This is totally doable, PostgreSQL is a great object store. Some tips:

1. Store the file in BYTEA column: https://www.postgresql.org/docs/current/static/datatype-binary.html

2. If there’s potential for those files to grow beyond 1 GB (or new files to be added that are 1+ GB), consider Large Object store instead: https://www.postgresql.org/docs/current/static/largeobjects.html

3. Decide on whether it’s worth it to transparently compress those files with TOAST, set the right storage mode: https://www.postgresql.org/docs/current/static/storage-toast.html

4. I would keep the file metadata in a separate “lean” table, and have a one-on-one foreign key reference to it from the table with the BYTEA column.

Be wary of those “files belong in a filesystem” people, they’ve never tried to move / delete 100m files :) Also, there’s no good reason to use base64 and thus needlessly increase your blob size by 4*(n/3).",1534022680.0
jakkarth,Storing files in a database is an antipattern. Store the path to the file on disk. Filesystems are designed for optimally storing files.,1534018921.0
linuxhiker,The two most effective ways to insert batch data in Postgres is COPY and in bulk transactions. COPY is almost always faster but bulk transactions can greatly increase the speed. If you don't want to use COPY then wrap your inserts in a BEGIN; INSERT ... COMMIT; where you are pushing 500 to 5000 (test) inserts per transaction.,1534007058.0
swenty,The fastest way to load a large data set is usually to convert it into a format that can be COPYd into the DB. Doing individual inserts on each row is often significantly slower.,1534005522.0
kringel8,"How big is a single row? Are there indices, triggers, foreign keys on this table? How fast is your hard disk? Are all those resources actually available or under load?
",1534003850.0
core_dumpd,How are you writing the data?,1534003981.0
vividsnow,check disk io with [fio](https://manpages.debian.org/stretch/fio/fio.1.en.html),1534004465.0
mikaelhg,"There is a particular way to insert large amounts of data into a PostgreSQL database that's proven itself to be the most optimal one, `COPY FROM`.",1534005892.0
HotKarl_Marx,What kind of disk(s) and what type of filesystem is on the disk(s)?,1534010570.0
pypt,There’s an official doc on how to do bulk inserts effectively: https://www.postgresql.org/docs/current/static/populate.html,1534023065.0
muddiedwaters45,"> However, the return type will depend on the data type of the requested column.

I don't see how.  You're returning pg_attrdef.adsrc which can only be of one type.",1533928453.0
Nunki63,"Imho take a look at dbeaver.  Less heavy on your hardware and intuitive in use.  A very vivid community.  What more could you want ?
",1533905706.0
denpanosekai,"pgAdmin 4 was such a divisive move. It feels like they're still in catch up mode, and it's still not clear what the advantage was. I'm warning up to it, as I use the debugger quite intensively, but otherwise DBeaver is also getting a lot of use these days.",1533913331.0
watagangsta,"My personal experiences with pgAdmin4 are so negative. I moved away to another tool within one day of trying to use it simply because the ux was so bad even for basic tasks.
I m using dbforge which isnt great.
Dbeaver mentioned sounds lk a good option to try. ",1533987389.0
takito_isumotu,"pgAdmin is kinda good but there are other better options. For administrative purposes you better use Navicat, for SQL development DataGrip. ",1533918698.0
Ellyrio,You could use [Citus](https://www.citusdata.com/).,1533891408.0
threeminutemonta,Is your requirements on [this](https://www.postgresql.org/docs/10/static/different-replication-solutions.html#HIGH-AVAILABILITY-MATRIX) matrix?,1533944961.0
krusty_lab,"I would not use drbd. I used it in the past and it can become rather complex.
I would suggest streaming replication.",1534268266.0
Amaracs,What about RDS Instances? Is there a way to backup tables in the database in an rds hosted db instance?,1533900497.0
SulfurousAsh,"how long are you preparing for the pi to be completely offline for? 

Minutes, hours, or days?",1533880583.0
iiiinthecomputer,"Yes, PostgreSQL does. WAL shipping and streaming replication. But AWS RDS does not let you use it between RDS and non-RDS PostgreSQL instances.

You will need a different solution. Look into logical replication.",1533900841.0
boy_named_su,use null,1533846961.0
linuxhiker,"It sounds like you should have a lookup table. Consider:

`create table log_type (id identity primary key, log_type text);`

`create table log (id identity primary key, logone text, logtwo integer references log_type(id))`

Then you can specify the log type value per id (and that value may be N/A). You would have to join the tables to get the log\_type though.",1533846601.0
depesz,alter table disable trigger user,1533897162.0
wolf2600,"Does disabling the triggers also disable the foreign key constraints?

This guy says it does:

https://stackoverflow.com/questions/38112379/disable-postgresql-foreign-key-checks-for-migrations#


",1533846592.0
igncampa,everybody loves postgres. keep up the great work,1533879519.0
joaodlf,"I can't seem to find the download link for Beta3, only Beta2 available at https://download.postgresql.org/pub/repos/yum/11/redhat/rhel-7-x86_64/ (pgdg-centos11-11-2.noarch.rpm)",1534174121.0
geggo98,"I have a simple solution: Stop the slave and remove some of it's files. This simulates a file system corruption. If you corrupt enough files, the slave will not recover.",1533795615.0
Darkmere,`truncate -s0` on the files in pg_xlog and base  should fix it.,1533823741.0
teilo,"What version of pgAdmin are you using? You need to be on at least pgAdmin 4 2.0 for Postgres 10 support, and ideally on 3.0 or later to fix issues displaying indexes.",1533754234.0
mikegold10,"The big problem with PostgreSQL partitioning, even in PG 11, is that you cannot partition based on a moving target (e.g., a relative time frame) with those transfers of rows (i.e., inserts/deletes between partitions) mentioned in the article happening automatically. A simple real world example that comes up again and again for timestamped event data:

SampleTable -- Partitioned as follows

=> SampeTableCur -- Data for the last 24 hours

=> SampeTableHistory -- Data that is older than 24 hours

You still have to “manually” transfer rows from SampleTableCur to SampleTableHistory as they age, if you want to maintain a rolling 24 hour window in SampleTableCur with older rows moved off to SampleTableHistory.

When a relative dynamic partitioning scheme gets added to PG, then it will have caught up to the big players of the relational world.

Maybe one of the PG experts in here knows how to solve this sort of problem with what is available in/for PG 11, without having to write some sort of script, job, process, whatever that runs periodically and does this.",1533765871.0
cediddi,"In such setup, can I define a foreign key from a partition table on the child server to a not partitioned table in the parent server? ",1533731236.0
therealgaxbo,"Since you seem to be treating the hash as 16 chars rather than 128 bits, you could represent the value as a 16 dimension [cube](https://www.postgresql.org/docs/10/static/cube.html).  Then the distance between two hashes can be pulled directly from a GiST index.

The difference to your original query is that the levenshtein distance only tells you *if* a character has changed, but the spatial distances (Euclidean, taxicab, or Chebyshev) are all sensitive to how *much* it's changed.  Depending on the meaning of your hash that might be desirable or a show-stopper.",1533718435.0
thelindsay,"- use pgtune to configure your instance, in particular ram usage
- try a hash index instead of btree
- try declaring the index as unique if in fact the hashes are unique
- use explain analyze to see what's actually happening
- if you're stuck with seq scans try faster storage, I think u can get nvme on aws now instead of SSD or magnetic
- text search approach is probably a waste since it is a match or no match
- could try additional functional index on first x chars (say 5) and structure the query in 2 stages to first narrow down prefix then exact match
- could use a prefix as logical partition key
- could use hash range as partition key too
- make sure you've run analyze on the table
- try a higher statistics target for the table (although if the hashes are unique so this might not help)
- try periodically clustering the table on the hash column's index

Lots of things to try. Good luck!",1533706601.0
pypt,"Given that your strings (hashes) are of equal length, I think you might want to simplify your task by calculating Hamming distances between those hashes.

PostgreSQL doesn't appear to have native Hamming distance search / indexing support, but there are multiple extensions and whatnot that promise to do what you want, e.g.: [Fast hamming distance queries in postgres](https://dba.stackexchange.com/questions/72134/fast-hamming-distance-queries-in-postgres).",1533746029.0
,[deleted],1533674607.0
MarkusWinand,"Named parameters use the `=>` operator, not just `=`:

    CALL config.sp_add_holiday(holiday_date=>'2018-01-01',name=>'New Years Day');

Btw, psql gives the following error for your example:

    ERROR:  column ""holiday_date"" does not exist
    LINE 1: CALL config.sp_add_holiday(holiday_date='2018-01-01',name='N...
                                       ^",1533664651.0
simcitymayor,"Previous poster's tone aside, I think you have found the best way.

It could be done with a trigger, which wouldn't be any faster, and many people decry triggers for being ""invisible"".

I think there's work afoot to add assertion type constraints to v12.",1533671538.0
obrienmustsuffer,"Maybe it prints to STDERR?

https://stackoverflow.com/questions/1420965/redirect-windows-cmd-stdout-and-stderr-to-a-single-file",1533651042.0
Postgresser,"Hi  obrienmustsuffer,  


your tipp makes my day !  


Instead of calling the program like this  


 pg\_ctl.exe init -D c:\\temp\\database -o ""-E SQL\_ASCII  > c:\\temp\\POSTGRES\\initdb.log    


i must use this syntax:

  
 pg\_ctl.exe init -D c:\\temp\\database -o ""-E SQL\_ASCII  > c:\\temp\\POSTGRES\\initdb.log 2>&1   


When i put   


2>&1

to the end of the command line, the redirect works.

Thank's a lot.  
",1533716463.0
nhymxu,I think docker is better than this 😂,1533613108.0
fullofbones,"I keep saying this and mean it every time: don't use pg_dump for backup purposes. Even if your indexes didn't take forever to recreate, there are a number of problems with a restoration of that kind. 

Use pg_basebackup, Barman, or literally anything else. IMO, pg_dump is only good for extracts.",1533555565.0
geeohgeegeeoh,This advice is buried so deep that nobody like me finds it until it's too late.,1533581012.0
flyingmayo,"Bear in mind that pg_dump and pg_basebackup have very different use cases. A side-by-side speed comparison is not really relevant.

pg_dump gives you a logical backup of your data.

pg_basebackup gives you a file-based backup of your data.

A base backup backup has all of your data and indexes already in place and is therefore much faster to restore but is also much larger to store and is an all-or-nothing proposition (you can't restore one db, or one table from a base backup for example).  For typical disaster recovery scenarios a base backup is what you want but make sure that you are managing your WAL files in such a way that your base backup is actually usable.

using pg_restore to restore a logical backup will be much slower but will also provide you with a lot more flexibility on how you use the data down the road.  It will also provide you with a bloat-free version of your data whereas a base backup carries all your existing bloat with it.

Also note that the -j option to pg_restore will allow you to restore database objects in parallel and will significantly reduce your load time provided you 1. dumped the data in directory or custom format and 2. are using a multi-core server to restore your data

 ",1533631705.0
johndashkelly,"I'd take a look at https://github.com/subzerocloud/postgrest-starter-kit/wiki

There is a lot of good information + design guidelines in there",1533530152.0
NickEmpetvee,"Circling back, I received additional feedback from the PostgREST user community that I will share. The include but are not limited to the below:
1. You can expose tables on an internal schema through a view in the exposed schema.
2. PostgREST allows POST through the view endpoint assuming that security has properly been set up between the view and the underlying table. The community suggested that the best way to do this is through grants and possibly row-level security.

I am trying this out over the next few days. Just sharing in case others have the same question.",1533740058.0
fullofbones,"While this may be relevant, it's actually not always desirable. The effect of cluster is to reorganize the physical table data according to the specified index. In the process, the table is rebuilt and any free space in the table is reclaimed. However, if the table is still active, all new rows will go at the end of the table, including the updated row itself.

When VACUUM runs on the table, old row versions are marked for reuse. So long as these updates are evenly distributed, inserts and updates can exist anywhere through the table. And updates will prioritize the same page as the previous row if possible, so the index doesn't need to be updated. (Indexes point to the page where the row resides. this kind of update is called HOT and saves a lot of overhead.)

This is one reason autovacuum is configured to only trigger at 20% activity. Eventually you'll reach a type of equilibrium where you'll have 20% ""wasted"" space, but it will not increase further. If that's not happening, there are _numerous_ knobs to twist that'll make autovacuum more active to keep up with data turnover. Otherwise, updates will be faster due to HOT, inserts will be faster since there's no hot spot at the end of the table heap, and so on. Generally we recommend only using CLUSTER on a table that's static or mostly static.

Using CLUSTER as illustrated here also has a cost in locking the table, making it unusable during the operation. Active 24/7 systems can't accept that kind of downtime. So yes, you may save 20%-ish of space, but it's usually better to just let the database handle it without micromanaging.",1533567373.0
SomeGuyNamedPaul,"It sounds like you're wanting to form many to many relationships.  The way I do this is with a mapping table between.  Say you have ""person"" and ""team"" tables and a person can be on multiple teams.  Have a third table called people_team_map with at least two columns, person_id and team_id.

Have two composite indexes (person_id, team_id) and (team_id, person_id) with the first one unique, probably since person will have higher cardinality than team.",1533400498.0
pypt,"You can consider using PostgreSQL's [table inheritance](https://www.postgresql.org/docs/current/static/ddl-inherit.html) for that.

Create a single table for forms:

    CREATE TABLE forms (
        forms_id SERIAL PRIMARY KEY,
        name TEXT NOT NULL
    );

Then a base table for form fields with columns that are going to be shared by all fields (e.g. `name`):

    CREATE TABLE form_fields (
        form_fields_id SERIAL PRIMARY KEY,
        forms_id INT NOT NULL REFERENCES forms (forms_id) ON DELETE CASCADE,
        name TEXT NOT NULL
    );

Create child tables for each type of a form field and their custom properties:

    CREATE TABLE form_fields_text (
        -- Prefixed with ""text_"" for the NATURAL FULL JOIN to work
        text_value TEXT NOT NULL
    ) INHERITS (form_fields);
    
    CREATE TABLE form_fields_checkbox (
        -- Prefixed with ""checkbox_"" for the NATURAL FULL JOIN to work
        checkbox_checked BOOLEAN NOT NULL
    ) INHERITS (form_fields);
    
    INSERT INTO form_fields_text (forms_id, name, text_value) VALUES (1, 'Test text input', 'Test value');
    INSERT INTO form_fields_checkbox (forms_id, name, checkbox_checked) VALUES (1, 'Test checkbox', true);

SELECTing from such a setup is a bit weird because you have to manually list and [`NATURAL FULL JOIN` all of the child tables](https://dba.stackexchange.com/a/125028) (but I'd argue that your application will know the child table names anyway). Also, ""custom"" column names in every child table have to be unique for the `NATURAL FULL JOIN` to work as expected (thus the `text_` and `checkbox_` prefixes above):

    SELECT
        -- Name of a child table for you to determine the type of the field
        form_fields.tableoid::regclass AS child_table_name,
        *
    FROM form_fields
        NATURAL FULL JOIN form_fields_text
        NATURAL FULL JOIN form_fields_checkbox
    WHERE form_fields.forms_id = 1;

Sample output:

       child_table_name   | form_fields_id | forms_id |      name       | text_value | checkbox_checked 
    ----------------------+----------------+----------+-----------------+------------+------------------
     form_fields_text     |              1 |        1 | Test text input | Test value | 
     form_fields_checkbox |              2 |        1 | Test checkbox   |            | t
    (3 rows)",1533423897.0
,[deleted],1533407515.0
thelindsay,"Check out https://enketo.org/ or  https://www.limesurvey.org/ which might do everything you were after. I work with a related software called OpenClinica, which uses enketo for presenting forms. These revolve around building and completing dynamic forms, mostly in the (medical) research space.

The enketo data model is XML, which allows a lot of flexibility in the form structure. This gets mapped back to Postgres tables in OpenClinica in an Entity-Attribute-Value style. So e.g. we have form definitions table, and an items definitions table. For actual data, there is a form instance table, and an item data table, both of which refer back to their respective definitions tables. Because the item data table has only 1 column for value, all data must either be able to be converted to text, or be a URI to e.g. a file.

This presents a challenge for reporting, since everybody expects a table for each form with a properly typed column for each item, as if it were a spreadsheet. It's tricky to deal with only because once you consider columns for values then value labels (e.g. drop downs) then multi-choice the table can get very wide and sparse and maybe even hit the column limit ~1500 per table. But in that case it's the user's choice to make a form so big. In contrast, LimeSurvey generates these wide-style tables on the fly for transactional use, which greatly simplifies reporting use.

Anyway, my point is that dynamic form apps are difficult to design and will require numerous trade offs. There are many existing tools to leverage. Whatever you do, don't forget how the data will be used / extracted / reported on!",1533485901.0
mlt-,Did I miss the point? Where is the simplification part? Looks like an example from the docs.,1533310438.0
johnfrazer783,"Building everything yourself is heroic and certainly what [real programmers](https://www.xkcd.com/378/) do, however, it is also failure-prone.

You may want to use the official PPA as per https://wiki.postgresql.org/wiki/Apt; in short,

    sudo apt install wget ca-certificates psmisc
    sudo sh -c 'echo ""deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -cs)-pgdg main"" > /etc/apt/sources.list.d/pgdg.list'
    wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
    sudo apt update
    sudo apt install postgresql-9.6 postgresql-plpython3-9.6 postgresql-contrib-9.6 


Adapt to your preferred version and check back with the wiki page before running the above; you probably want version 10 as of this writing.

The official PPA is great because it has all the supported versions and all the important extensions. Be sure to install the `contrib` package for these.",1533381859.0
pypt,"For the best safety and compatibility, upgrade to the latest 9.x first (there’s not much to upgrade between identican 9.x versions), and then go for the latest 10.x.",1533246810.0
denpanosekai,I have done it a few times but as others mentioned can you stage this upgrade? Don't pull the trigger on a production site.,1533308965.0
threeminutemonta,"I never had to do it a we use a managed service, though I believe you should aim for 1 major version at a time. 9.3.X, 9.4.X, 9.5.X, 9.6.X . With each upgrade testing and checking the release notes.",1533245948.0
SomeGuyNamedPaul,"In theory, yes.  In practice, so you have any extensions?  If you do, you need to either uninstall them, or upgrade them to versions that are also compatible with the PG version you're migrating to.

For example, if you have PostGIS 2.2.2 installed, then when you move to 10.4 it's going to look for PostGIS 2.2.2 to be installed.  If not then your upgrade will fail.  Upgrade your extensions first.  If this means you have to go to say 9.5 first then that's what it will take.",1533302433.0
ElectronSpiderwort,"I learned a thing: generate_series().  I've been using row_number() on any random table for years for that; this is cleaner.  

Consider a rolling-month since that's often more interesting than restarting on a month boundary and that also solves the goofy wide bar problem: 

    select current_date - interval '1 day' * generate_series(0,31) as the_date;

",1533293009.0
cazzer548,"Great article! I especially appreciate that you abstracted the series out to a function. For what it's worth, you can also build [complicated reports](https://medium.com/volta-charging/calculating-hourly-utilization-in-postgresql-29345424deb) with `generate_series`. I love combining it with `overlaps` and `between` in cases where your event data has a `time_start` _and_ a `time_stop`.

Edit: Markdown",1533313908.0
cazzer548,"I wrote this because I love Row Level Security but struggle to find discussions online that expand beyond ""Alice and Bob""-type examples. I'm no expert in all things Postgres but I am learning and appreciate any comments and criticisms!",1533224936.0
boy_named_su,"Nice one

I played with something similar, back before PG had RLS:

http://blog.databasepatterns.com/2014/12/postgresql-row-security-sharing-rls-vpd.html



",1533226326.0
thelindsay,"Thanks for this article, I know what you mean about the lack of worked examples. The current_user tip is great as well.

On the query optimisation front, maybe you can squeeze out a few ms by doing the permissions where clause in two steps, i.e first filter on items.id then do the user-or-group comparison. The idea being to try and limit the  regex work to the subset of items of interest. Maybe a subquery is enough of a hint, or use a CTE to materialize that subset. Or try a session temp table, if the claims list tends to be somewhat long. Could even try a plpgsql function that checks for a comma, returns the string as uuid if not found (i.e. only 1 claim), or if found then does the regex split and cast.

Also a partial index on items.id where items.public, to maybe change the top node seq scan to index scan. Or a multi-column index on both items.id, items.public; or two separate indexes. One of those 3 usually helps. Same for the usage of user-or-group id with permissions.role = write.",1533271221.0
spinur1848,"Look at one or more of

 * https://www.adminer.org/en/
 * https://omnidb.org/en/",1533264682.0
KitchenDutchDyslexic,"Have you tried editing data with pgadmin4? Not pretty forms but a good view into your db.

> I want to do this using Django if possible.

I'm a big sqlalchemy fan so i decided to use [flask-admin](http://examples.flask-admin.org/sqla/simple/admin/) to create my crud screens from my database model.",1533526532.0
outlier_lynn,"In a given connection, postgresql will cache the query plan.  Your command line ""time"" version creates a new connection each time you run it, so no caching.  That will slow it down some.  Your command line version must make a new connection each time it is run, so I'm guessing the wild differences lies there.  The database may be doing something that delays the connection or delays executing the query.  My money is on the delayed connection.

Try marking the exact time you start the psql, then look at the database logs to see what time the connection happened.  You can tell the logs to give you times and timingsfor events, too.  Make the logs as verbose as possible.  At the least, it should then tell you where the extra delay is.",1533224286.0
thelindsay,"Well there can be other things going on in the database, and your computer/server. Is anything else running queries at the same time? Is autovacuum running? Is your IDE reindexing your code? Is your AV running a scan? etc. Point is, it's hard to reproduce query timings precisely, especially at these fractions of a second.

If you make the test table 10x or 100x bigger and still see a large proportional fluctuation in timings then you might have a problem.

All I could recommend is to turn on all statement logging, and run your explain-analyze in a loop to try and catch a ""bad"" run (save results to timestamped files), then have a look at both the plans and db activity at the time of execution.",1533224026.0
lgastako,"I was able to download the zip file from http://www.postgresqltutorial.com/wp-content/uploads/2017/10/dvdrental.zip and then unzip it to get the tar file that was inside, then run the following:

    createuser -s dvdrental
    createdb dvdrental -O dvdrental
    pg_restore -U dvdrental -d dvdrental ./dvdrental.tar

which loaded the data successfully.

FWIW here's my version info:

    [local]:5432 dvdrental@dvdrental # SELECT version();
    +---------------------------------------------------------------------------------------------------------------+
    |                                                    version                                                    |
    +---------------------------------------------------------------------------------------------------------------+
    | PostgreSQL 10.3 on x86_64-apple-darwin17.3.0, compiled by Apple LLVM version 9.0.0 (clang-900.0.39.2), 64-bit |
    +---------------------------------------------------------------------------------------------------------------+


",1533182622.0
leamanc,I was able to load it with PGadmin a few months back. I did not try with the command line.,1533180057.0
cyberst0rm,I think it's line end8ngs. Not sure what Mac apps do it but notepad++ will strip windows line endings,1533181008.0
chock-a-block,"I haven't looked, but you should be able to extract the TAR, then modify the locale.

Or, set up a VM and change the database locale in windows.",1533185051.0
kmatrah,Hi! We use it at Algolia to power our Analytics API. We wrote an article about it: [https://blog.algolia.com/building-real-time-analytics-apis/](https://blog.algolia.com/building-real-time-analytics-apis/). Hope it helps!,1533146997.0
pypt,"We did some testing to evaluate whether we could migrate our Solr-based full-text search to PostgreSQL (PostgreSQL’s FTS is great as long as the number of documents you’re querying against is < 10k, so we were looking for ways to split up our dataset into shards). We’ve tried the open source Citus version.

My takeaway would be that in principle the product works fine, it’s just that there are a bunch of system-level caveats that one has to know about, e.g. disk reads being a bottleneck, network throughput between shards, core / shard proportion, optimal configuration + memory limits for every shard, etc., and those caveats are very project-specific.

We found those tips & tricks not too well documented, but it is understandable given that Citus is a business that wants to make money, so they sell this internal know-how in the form of Citus Cloud. But should you choose to go with their open source version, I’d suggest you test it thoroughly first with a realistic amount of data.

People on their Slack are rather helpful though!

",1533155211.0
linuxhiker,The first thing I see is that you could pk on userid/sessionid. Then you don't need the one row check.,1533092769.0
chock-a-block,"You want to check the state of any transaction that is pending on that row.  https://www.postgresql.org/docs/9.6/static/view-pg-locks.html

I would add a check on the post function.  

If ($user is moderator)

  Call routine to get $user moderator state.

   if ($user is STILL mod)
    Call post routine.

   else

   error

else

Next

Code formatting not working!!",1533099687.0
pypt,Wrap your checks in a transaction (BEGIN; ...; ...; COMMIT). That way you’ll be guaranteed data consistency between your queries.,1533114279.0
obscurant,"You need to check in the where of the update that user still has mod at the time of update.

Update Thread t set body=@newText, flag=ModEdit FROM users u WHERE u.modstatus = 'true' AND u.userid = @user.UserID",1533142768.0
felixge,"I'll bite:

- Even so MySQL has been catching up on advanced features lately (e.g. window functions, CTEs, etc.), PostgreSQL still has more features (e.g. custom types, user defined aggregates, full outer join, etc.).
- MySQL's boolean type is an alias for tinyint(1), and yes you can store the value 2 in it.
- MySQL doesn't support check constraints on table columns and will just silently (!) ignore them.
- MySQL has historically sacrificed data durability for performance, not unlike MongoDB.

That being said, if you have an existing application that depends on MySQL it's probably not worth making the switch.

You should also be able to find more in-depth comparisons online.",1532955393.0
nikoz84,"For me this featured are amazing in POSTGRESQL

* FULL TEXT SEARCH
* JSON AND JSONB (Hibrid database)






 ",1532957266.0
denpanosekai,"For me as an open source integrator, it's all down to the license. PostgreSQL is completely free and MySQL is not free.

Icing on the cake is that PostgreSQL has such an amazing community and dedicated team of core contributors.

The rate at which new features come in is just staggering. I attend PGCon every year but it's still a lot to take in. We're just moving to PG 10 these days and I'm still exploring all of my new options, which of course will be even better with 11.",1532957272.0
jgardner,"Transactions. Real, honest-to-gosh, transactions.

Once your commit is done, you're good to go. If something goes wrong, no harm no foul. This saves you a TON of development work, and is the reason why you'll find PostgreSQL even in distributed solutions.

You can include DML in your transactions as well. Modifying a bunch of tables but they have to be done in a transaction? PostgreSQL has got your covered.

Also, you might look into real, honest-to-gosh serialized transaction isolation. It's an option.",1532971568.0
pypt,"* Very stable, rather conservative development team. They don’t “move fast and break things” like the rest of the industry does nowadays, so features, performance and upgrades are very predictable.

* Advanced, very well developed and comprehensive analytical functions.

* Amazing documentation. Say what you want about PHP, it has one of the best docs around, and PostgreSQL is on par with what PHP tech writers have to offer. The docs even go as far as to explain the tradeoffs and disadvantages of various approaches, which is not what one could say about competing offerings.

* RhodiumToad on #postgresql @ Freenode.
",1532987456.0
mlt-,"Table inheritance and extensions! Foreign Data Wrappers, temporal_tables, lots of stored procedure languages.",1532959581.0
nikoz84,"**FULL TEXT SEARCH**

* THE OBJETIVE OF A SEARCH IS FIND THE DOCUMENTS MORE RELEVANT WHIT YOUR TERMS OF SEARCH
* KNOW WHERE IS THESE DOCUMENTS (web crawler)
* INDEXING THE DOCUMENTS FOR FINDING (inverted index)
* KNOW THE RELEVANCE OF THE DOCUMENT (scoring)
* SHOW THE RANKING FOR RELEVANCE (search) 

**TOKENIZE TEXT IN WORDS** 

* SPLIT TEXT FOR WORDS (split words)
* DELETE STOP WORDS OR INTERRUPTION WORDS (articles, conyunction, etc) (stop words)
* ALL IN LOWERCASE
* REMOVE PUNCTUATION 
* FREQUENCY OF THE QUATITY OF REPETE WORDS IN A DOCUMENT
* CREATE LEXEMAS FROM WORDS

**EXAMPLE**

**CREATE A DOCUMENT COLUMN WHIT GIN INDEX**

ALTER TABLE tv_series ADD ""document_vectors"" tsvector;
CREATE INDEX idx_fts_doc_vec ON tv_series USING gin(document_vectors);

**UPDATE DE TABLE**

    UPDATE 
        tv_series 
    SET 
        document_vectors = (to_tsvector(title) || to_tsvector(description) || to_tsvector(creator));

**INSERT VALUES**

    INSERT INTO tv_series
        (id, title, description, creator, document_vectors)
    VALUES
        (5, 'Better Call Saul', 'Better Call Saul is an American television crime drama series created by Vince Gilligan and Peter Gould. It is a spin-off prequel of Gilligan''s prior series Breaking Bad.', 'Vince Gilligan, Peter Gould', (to_tsvector('Better Call Saul') || to_tsvector('Better Call Saul is an American television crime drama series created by Vince Gilligan and Peter Gould. It is a spin-off prequel of Gilligan''s prior series Breaking Bad.') || to_tsvector('Vince Gilligan, Peter Gould')));

**SEARCH A VALUE**

    SELECT id, title, creator FROM tv_series WHERE document_vectors @@ plainto_tsquery('break');

**YOU CAN USE to_tsquery() or plainto_tsquery() functions for the TERM**


* Example 1
https://hevodata.com/blog/postgresql-full-text-search-setup/
* Example 2
http://rachbelaid.com/postgres-full-text-search-is-good-enough/
* Example 3
https://www.compose.com/articles/mastering-postgresql-tools-full-text-search-and-phrase-search/

OFICIAL DOCUMENTATION 
https://www.postgresql.org/docs/9.5/static/textsearch.html",1532961211.0
stump82,"MVCC helps alot for read heavy applications in reducing blocking.

[https://www.postgresql.org/docs/10/static/mvcc-intro.html](https://www.postgresql.org/docs/10/static/mvcc-intro.html)",1532969696.0
core_dumpd,"The JSON/B features of postgresql are very compelling if you're doing any sort of JS front-end work with it. I was able to take a ~1.7s API call down to ~26ms by coupling lookup queries together into one JSON result set, and that was casting a bunch of normalized table structures into a single JSON/ARRAY structure. So, best of both worlds - normalize data on the back-end, but deliver a document like result to the front-end.",1533001897.0
alcalde,"https://www.cybertec-postgresql.com/en/why-favor-postgresql-over-mariadb-mysql/

https://medium.com/holistics-software/why-you-should-use-postgres-over-mysql-for-analytics-purpose-e3df42df35d7

https://blog.2ndquadrant.com/postgresql-better-mysql-1/?utm_source=postgresweekly&utm_medium=email

https://smartbear.com/blog/test-and-monitor/5-reasons-its-time-to-ditch-mysql/


",1533005262.0
jenkstom,"Honestly one can argue features all day. The real benefit to me is twofold: Firstly I trust PostgreSQL over Mysql a lot more to not lose my data. It is more mature and has better regression testing. Secondly writing SQL statements, stored procedures or anything else for PostgreSQL is much, much nicer. Especially if you compare it to the nightmare that is MSSQL Server. It's a coder's database.",1532981374.0
jk3us,Removing this as a repost: https://www.reddit.com/r/PostgreSQL/comments/90ef39/how_postgresqls_sql_dialect_stays_ahead_of_its/,1532953442.0
pypt,How many milliseconds? And what does EXPLAIN ANALYZE say?,1532903827.0
anonymous_subroutine,"https://www.postgresql.org/docs/current/static/runtime-config-autovacuum.html

> autovacuum (boolean)
>
> Note that even when this parameter is disabled, the system will launch autovacuum processes if necessary to prevent transaction ID wraparound. See Section 24.1.5 for more information.",1532893337.0
SulfurousAsh,"The culprit must be how the data is being saved into PG originally. Take a look at the exact query being made when that column is inserted / updated.

My guess is the write is putting in malformed data.",1532807727.0
cyberst0rm,"I ran into an issue where the jaonb was of type string rather than object.

Fact: you can store a string as jsonb

There's a jsonb object type function

    Jsonb_typeof",1532959798.0
HipsterCosmologist,"I may not have understood the question completely, but here goes.  You've installed XAMPP (which I just looked up), and it installs a nice integrated set of services including an apache webserver with PHP and Perl integration, along with MariaDB (open source version of MySQL.)  Now you want to switch to Postgres, yes?  Not a big deal, but it means working outside the XAMPP stack (BTW, if you look up ""XAMPP postgres"" there are plenty of tutorials)

Postgres can just replace MariaDB.  You install a postgres server via a number of different methods depending on your OS.  Then you create new version of your table schema, porting what you have done in MySQL over to Postgres (not too huge of a jump.)  After that, you have to look into PHP<->Postgres integration and change your PHP code accordingly. 

This is all local development though, yes?  If you want to deploy into the wild, something like Heroku would allow you to spin up a cloud-based postgres server, then upload your PHP code to be hosted by them as well.  This part may actually be easier than trying to self-host!",1532732663.0
pypt,"replace() is a string function, it replaces all occurrences of a needle in a haystack, and you can’t replace part of a string (the needle) into NULL because NULL can’t be a part of a string.

Try instead:

SELECT CASE WHEN column = ‘-‘ THEN NULL ELSE column END AS modified_column FROM ...",1532682530.0
dark-panda,Do you mean `GROUP BY`?,1532652327.0
leftnode,"It sounds like you want to GROUP the results rather than DISTINCT them:

    SELECT user, toy, COALESCE(SUM(amount), 0.0) FROM table_b GROUP BY user, toy

The COALESCE also ensures you get a numeric value rather than NULL which might blow things up depending on the type of the amount column.",1532652313.0
wolf2600,"    INSERT INTO table_a (user, toy, amount)
    (SELECT user, toy, sum(amount)
      from table_b
      group by user, toy);",1532653933.0
pypt,"Why no Flyway?

apgdiff (https://apgdiff.com/), while old, manages to get 80% or so schema diffs right, so that might be worth a try (provided that you intend to verify autogenerated schema diffs by hand).",1532650368.0
postgrescompare,I'll throw the product I'm working on in here for consideration too :) www.postgrescompare.com,1532733691.0
thelindsay,https://pyrseas.readthedocs.io/en/latest/index.html,1532676435.0
zieziegabor,We have good success with Liquibase: http://www.liquibase.org/,1532653167.0
,[deleted],1532664897.0
fr0z3nph03n1x,That's impressive. How do you know what sql changes were made? What about complex data type migrations?,1532674883.0
threeminutemonta,I couldn’t find the link I was searching for though found [pg compare](https://github.com/cbbrowne/pgcmp),1532720019.0
obscurant,"I use sqitch for schema management- https://sqitch.org/. It suits my requirements (no migration scripts, no meta-language, just sql)  and has been quite flexible when combined with certain psql nuances.

For your needs I would recommend taking a look at https://github.com/eulerto/pgquarrel and see if that can be made to fit.",1532827162.0
_mmf_,"/u/cerealLeggings: It's a few months later, and I'm curious to know if you made a decision.",1543601549.0
cruyff8,Heroku or Openshift are most popular for postgres.,1532585068.0
jossser,"RDS has free tier [https://aws.amazon.com/rds/](https://aws.amazon.com/rds/)

But, it's not so hard to install it locally. You will get more opportunities and understanding of how things works internally",1532600023.0
davbeer,Google Cloud is another alternative [https://cloud.google.com/sql/](https://cloud.google.com/sql/),1532585704.0
beyphy,"You can create a linux server. I have mine running on VirtualBox and just connect to it using SSH tunneling with Microsoft Access. I'm really glad I went that route. I switched PCs and just had to restore the appliance, as opposed to reinstalling, backing up, and restoring the database. It's probably more complicated than those other options, but it's completely free.",1532611945.0
vcamargo,[https://www.elephantsql.com/](https://www.elephantsql.com/),1532593892.0
AndreSteenveld,Azure has a hosted PG offering as well,1532604602.0
silva42,"Ask yourself, what features do you want to try ? if don't need the bleeding edge features just get a db from Heroku/rds/Google cloud. Maintaining a Linux VM just to host the latest version of PostgreSQL isn't difficult but it will be more work.  Just a simple older version of the db will be cheaper and easier. ",1532617816.0
pypt,"Store the X and Y coordinates of every cell in separate columns, e.g.:

CREATE TABLE cells (
    id SERIAL PRIMARY KEY,
    x INT NOT NULL,
    y INT NOT NULL,
    value TEXT NULL,
    UNIQUE(x, y)
);

Edit: and upsert cells using PostgreSQL 10’s native atomic upsert:

INSERT INTO cells (x, y, value)
VALUES (..., ..., ...)
ON CONFLICT (x, y) DO UPDATE SET value = ...;",1532580125.0
iiiinthecomputer,"Google ""EAV"". There are lots of problems with this approach though. Also Google ""inner system effect"".",1532609390.0
pypt,"I’m guessing that it’s something weird with your data, i.e. some of the cell values in “lastname” column are actually ‘00000000’.",1532562293.0
wolf2600,"What if you put the lastname list into a separate table and join to that table to do the filtering?  Do you still get the strange results?



    SELECT 
    name, array_agg(lastname) 
    FROM table 
    WHERE lastname in (select lastname from namesList) 
    GROUP BY name;

or...

    SELECT 
    a.name, array_agg(a.lastname) 
    FROM table a
    INNER JOIN namesList b
        on a.lastname = b.lastname
    GROUP BY a.name;",1532551647.0
jk3us,"What happens if you changie it to 

    SELECT name, array_agg(distinct lastname) FROM table WHERE lastname in ('Johnson', 'Wick', etc..) GROUP BY name;

I'm just wondering if it sees all the `000000000` values as the same.  What client are you running these in? I wonder if it could be mangling the results.",1532617653.0
Mamoulian,"Seems odd that Patroni doesn't do this out of the box. There's a mention of it here:
https://github.com/zalando/patroni/issues/547

And a (non Patroni) doc on how to set up haproxy for read replicas here:
https://severalnines.com/blog/postgresql-load-balancing-using-haproxy-keepalived

Alternatively, if there was a way to update the JBoss datasource at runtime direct from etcd that would save having a (very busy) haproxy instance.",1532554252.0
ants_a,"Have two different ports on Haproxy, one for writes and up to date reads, one for load balanced reads. Patroni has an example for that under extras/confd. Then configure two datasources in your application and choose between them for each transaction. There are examples on the net for how to set up Spring to automatically route all transactions marked read only to the load balanced datasource.

There is also pgpool to automatically route queries based on type, but that is pretty complicated to set up properly for HA and there are lots of caveats for the application to adhere to so you can't really use it blindly anyway. Most importantly, there isn't currently a good way to set up an HA cluster with load balanced queries where read after write is guaranteed to see the write.",1532583094.0
linuxhiker,"Howdy,

A couple of things, 9.2.8 is no longer supported. Of course you are welcome to continue using it but you are missing half a decade of improvements. I recommend using 10.x. It sounds like what you are actually asking for is an audit trail, if that is the case I would recommend pg\_audit.

JD",1532461635.0
jk3us,1 database with 2 identical tables? Maybe explain your use case a little so we can get a grasp on what you're looking for.,1532460759.0
mokadillion,"So you want two copies of one table
In the same database ? Why ? ",1532465456.0
iyer_bharat,"Hello 

We have Batch management tool which uses Postgresql .  We have two table Audit and history which keep on updating based on any action . 

Since the developer are accessing the those two tables for checking ( olny view access ) , we see delay happing when the batch management tool want to update those table . 

So we thought why cant we replicate those table and give access to duplicate one rather than the live one. 

Please advice ",1532525273.0
mage2k,"How about something more in the middle: set up the partitions through 2018-08 and have a task that runs on the first of the month that creates the partition for the next month, e.g. on August 1st the partition for September gets created.  That way you don't need any triggers and you also don't have a bunch of empty partitions sitting around doing nothing.",1532458119.0
felixge,"SQLite is a library. The library can be embedded by other programs (e.g. a GUI) to read/write SQLite files and to execute SQL queries against them. The library is used by the sqlite command line utility. SQLite databases are single files, and you need access to the file system they are on in order to access then.

Postgres is a network program. You can run it on any machine, including your local one. Once it is running, multiple clients can connect from it if they have login credentials and network access to the computer where Postgres is running. Of course you always have implicit network access to your own machine.

Postgres also stores your data in files, but there is a whole directory full of them. The location of this directory depends on your OS and method of installing Postgres. If you already managed to connect to a Postgres DB, you can find the location of this directory by executing the following query:

SHOW data_directory;

That being said, you probably don't want to interact with this directory directly for now. If you change or move files in this directory, your database may break.",1532414228.0
swenty,"You can run the PostgreSQL database server locally on your own machine or remotely on a network accessible server. In either case you connect to a particular named database. The files for each database are stored in a directory managed by the server. It is almost always wrong to manipulate those files by hand. All operations should be done through the database server, including creating and deleting databases.",1532416389.0
wolf2600,"The process/terminology is the same for both local/remote.  The only difference will be that you connect to ""localhost"" instead of a remote IP address.",1532434542.0
boy_named_su,"PostgreSQL is a database server. This means that you connect to it over a network socket (usually TCP). The server usually runs on a remote computer, but can also run on your local computer

If it runs on a remote computer, then yes, all the data and schema are stored on that remote computer

The PostgreSQL data on say Ubuntu is usually in `/var/lib/postgresql/$version/main`",1532449761.0
NickEmpetvee,Well it seems I resolved this. I put the files 'pgjwt.control' and 'pgjwt--0.0.1.sql' in the \`share\\postgresql\\extensions\` folder and ran the command `CREATE EXTENSION pgwjwt CASCADE;` and it seemed to do the trick.,1532411960.0
Xirious,"My response is... Why fix something that's already working? And working exceptionally well at that. For a chance to use ever newer features at the cost of possibly diminished code quality? The methodology Postgres has been following clearly works and works extremely well - the upsides presented here do not in any way outweigh the potential downsides. 

And just to add - it's not as if Postgres is slow in any case - numberous new features are added all the time, so much so our production system can't keep up.",1532406628.0
boy_named_su,"1. you can install it locally on your computer

2. ask your boss if they have a server to install it on

3. ask your boss if there's budget for Amazon's AWS RDS. This is their managed database service, and they support PostgreSQL

it's free to try: https://aws.amazon.com/rds/free/ for a small db, for 750 hours",1532363029.0
hardwaresofton,"Also, it may seem like a huge time sink, but reading the [official postgres documentation](https://www.postgresql.org/docs/current/static/datatype.html) will probably tremendously help you out in your career, as well as put you past a lot of developers in the field right now.

Also I want to point out that a ""server"" is just a computer in someone else's house/business. It's not magic -- in the end, a computer process is running on a computer somewhere, listening on a port, and making changes to disk when you send it instructions (SQL) stuff. AWS sells you a ""managed"" service, which means that you don't have to worry if the computer overheats, or you don't have to worry about setting up a computer that must run [`postgres` (the executable)](https://www.postgresql.org/docs/current/static/app-postgres.html)

Also another good tip is that your first day 0/1 concern should probably be [how to do backups](https://www.postgresql.org/docs/current/static/backup.html). Postgres just manages the data but the data is the important thing -- so make sure you protect it. And if you want to be a whiz kid, and actually set yourself up well for your future career, *automate* your backups.",1532439363.0
etrnloptimist,"update your pg_hba.conf to allow tcp connections from ""anywhere"":

host    all             all             0.0.0.0/0               md5


Your company firewall should prevent access from outside. Now anyone inside your LAN can access the db using TCP port 5432.

Give them credentials using an admin tool and you should be good to go!
",1532363024.0
ammoprofit,Sounds like the start up should be paying a DBA six figures.,1532422079.0
reifba,Postgres docker is nice as well. We use it on our testing environments. ,1532381370.0
chock-a-block,"I know I'm not answering your question directly, but, it's wrong to take your MSSQL assumptions and apply them to PostgreSQL.  You will very likely be dissatisfied trying to force Microsoft designs onto Postgres.

Multi-master is probably the most complicated way to accomplish high availability in MSSQL, but people do it because they don't want to buy ""enterprise"" storage.  

Don't say you were never warned.",1532358000.0
SomeGuyNamedPaul,"Postgres-XL or BDR.  There also exists Greenplum.  Each has their positives and negatives, and different use cases.

Are you sure you really need multi-master or will multiple readonly nodes do the trick for you?  Writes generally don't scale with schemes unless your use case is one where you don't need multi-master in the first place.",1532350571.0
zieziegabor,"The short answer is, don't if you can at all avoid it.  It generally creates more problems than it solves. Other responses here have talked about how to do it, if you must. 

If you aren't forced into multi-master, but can handle other forms of multi-node PG, you can check out Citus, and lots of other open-source solutions, or pay one of the large cloud vendors that will give you ""unlimited"" scaling of their custom PG if you pay them enough $$'s.

But in general, with a great backup strategy and stand-by replication nodes, and manual switching on failure, you can get very, very far with basically no downtime(with pgbouncer or friends in front) and without crazy complexity that causes headaches and occasional nightmares (which is what basically all the multi-node solutions for PG will give you).

Scale the hardware on a single node to the absolute extremes before you go to any sort of multi-node solution, and your life will be better off.",1532409280.0
jeffk,"The GUC acronym stands for [Grand Unified Configuration](https://www.enterprisedb.com/node/3383). GUC [appears in Postgres docs](https://www.postgresql.org/docs/current/static/acronyms.html), but the [relevant section](https://www.postgresql.org/docs/current/static/config-setting.html) doesn't mention GUC.
",1532303692.0
smugbug23,"This is an elaborate system for setting configuration variables.  You can set them in the config files (see postgresql.conf), from the command line (""pg_ctl start -o --work_mem=3GB""), at the SQL command prompt ('set work_mem=""3GB""'),  from environment variables on the client (""PGOPTIONS='-c work_mem=3GB' psql""), and more ways.  

Not all GUCs can be set or changed using all of these methods.  For example, it makes no sense to change the port number or data directory while the server is running, so those can only be set at startup.  While it might be nice to change shared_buffers while the server is running, it would be extremely hard to implement that ability, so it is also not allowed except at server start-up.  Others can only be set by the superuser, for safety and security reasons.  But in general they can be set or changed in as many ways and by as many people as it is sensible to implement.

You have almost certainly already used them, just without knowing what they are called.

""current_user"" is not a GUC.  It is funny-looking function mandated to not take parenthesis by the SQL spec.",1532392627.0
chock-a-block,"Imagine a table with 10,000 orders for every day, year to date. 

SALES TABLE: date(smalldate),item_number(char 10), units(integer),price(decimal(10,2))

*select date,mode() within group (order by price), sum(1) from sales where date > '2017-12-31'::date group by date*

That should return the most frequent item cost people bought per day.

*select mode() within group (order by price), sum(1) from sales where date > '2017-12-31'::date having count(price) > 5*

That should return the cost of whatever item most frequently purchased since the start of 2018.

sum(1) is another way to count(). I did this quickly, but it should get you started.",1532360827.0
smugbug23,"You can't use the same character to separate the columns, and also terminate the rows.  The default row terminator is the newline.

    my_string += k + "";"" + v + ""\n""
",1532131451.0
Darkmere,"You're better off setting  `buf`  to an empty stringio object and then use the `csv` module to generate your CSV file.

Set the separator to "";"" and write it to the `buf`, something like this:  

    >>> import io
    >>> import csv
    >>> buf = io.StringIO()
    >>> writer = csv.writer(buf, delimiter="";"", quoting=csv.QUOTE_MINIMAL)
    >>> data = ((1,2),(2,3),('a', 'b'))
    >>> writer.writerows(data)
     
       
In this case, f will now contain a newline-separated, Semicolon delimited CSV of the contents of ""data"":

    >>> print(buf.getvalue())
    1;2
    2;3
    a;b
    1;2
    2;3
    a;b

",1532172676.0
smugbug23,"Can you show the exact error message, preferably gathered after setting log_error_verbosity = verbose ?",1532172040.0
thelindsay,"Thanks for putting this together, very helpful visualisations!

Since you write about the frame exclude feature, I was wondering if you have applied it, or devised practical use cases for it? /u/lukaseder from Jooq wrote about it last week or so [1] but in discussing it I didn't come up with many compelling ideas.

[1] https://www.reddit.com/r/PostgreSQL/comments/8wl15e/postgresql_11s_support_for_sql_standard_groups",1532104753.0
lukaseder,"Looking at most of the timelines, it is fair to say, though, that PostgreSQL catches up with its competitors rather than staying ahead :-) (at least as far as the discussed topics are concerned)",1532108718.0
NoInkling,"Somehow this is the first I've heard of the identity column feature, so thanks for that.",1532144567.0
Cer_Visia,"Google says that the most likely cause is a bug fixed in 2014. How current is your OpenSSL library?

Are these connection attempts coming from the internet, or from one of your trusted clients?",1532083251.0
boy_named_su,Start here: https://wiki.postgresql.org/wiki/PostgreSQL_for_Oracle_DBAs,1532010341.0
iiiinthecomputer,"Download it, try it, play with it.

A lot of your challenge will actually be unlearning Oracleisms. 

PL/PgSQL is not PL/SQL. There's no rich embedded JVM. No autonomous commit. No packages. Replication is more manual to set up and benefits from third party tools. Same with continuous backup.

Some of the extensions are fantastic and it's a much nicer system to use day to day, much simpler too. Still not tuning-free though.

There's definitely a lot to learn.

Read the manual. The entire manual. More than once.

Also read some Oracle to PostgreSQL migration guides.",1532010437.0
chock-a-block,"It's not going to be enormously difficult to transition.  That doesn't mean it will be quick to learn.  It's a very powerful feature-filled DB server.

Be sure to set up a VM and play with replication.",1532013619.0
MuCowNow,"Oracle/PostgreSQL guy here ...

You will find the cross-over to be pretty nice.

The SQL is veritably equivalent (functions will differ).  You will find some deficiencies in PG, and some extras that don't exist in Oracle.

On PL/PgSql, you may note that Oracle PL/SQL is very much a strictly typed statically compiled language, and PL/PgSql is more a dynamic script language, the caveat being fewer errors are caught at compile-time, but more errors at run time.

On the administrative side, PG is behind in features, but is adding more every release!  PG comes with options that are extra $$,$$$/core in Oracle (e.g., open standbys).

You will miss the lack of Flashback.

You will miss stored packages.  The workarounds are hacks.  Stored procedures (with anonymous transactions!) appear to be coming in PG 11.

There are manual solutions to the lack of AWR.  The event interface is still pretty basic but again, adding more every release.

You will wonder at the lack of a shared sql cache, which is so important in Oracle.  The optimizer is faster (i.e., less sophisticated), so it gets away with it ... so far.

Backups are basic - akin to use of Oracle's hot-backup mode and an external file copy of some sort.  Recovery options are limited -- doing fancier recoveries (i.e., recovering files while the rest of the database is running, partial recoveries, etc.) are either not possible or will have to be hacked-up and tested thoroughly beforehand before being considered an option.

There will be an array of behavior differences that you will just have to pick up on the way.  Here is one for free: in Oracle, if there are missing datafiles on a standby, Oracle screams bitterly about it until something is done (this is good).  PG will continue silently, skipping the apply to those missing files (this ... ain't great).  Woe to ye who try to failover to that server -- you get errors only at database-connection time -- basically you have to monitor the standby, by doing test connections to every database in the server.

PG is very solid (compare to MySql, oh lawd make it stop!), but will still feel ""loose"" compared to Oracle ... which may be interpreted as ... less of a pain in the ass to get things working (e.g., querying generated rows from an Oracle stored-function is a byzantine nightmare.  In PG, easy peasey).
",1532183557.0
depesz,Why do you want to use array here anyway? This structure would be much better as normal table. ,1532012935.0
therealgaxbo,"I think you're slightly confusing arrays with records.  Your metadata type is a record, and has an input syntax like:

    test=# select ('video2',2,2,0.66666)::metadata;
             row          
    ----------------------
     (video2,2,2,0.66666)

And your column is of type metadata[] and has input syntax like:

    test=# select '{""(video1,1.0,2,0.33333)"",""(video2,2,2,0.66666)""}'::metadata[];
                        metadata                     
    -------------------------------------------------
     {""(video1,1,2,0.33333)"",""(video2,2,2,0.66666)""}

Or alternatively (and I think this is slightly cleaner):

    test=# select array[('video1',1,2,0.33333), ('video2',2,2,0.66666)]::metadata[];
                          array                      
    -------------------------------------------------
     {""(video1,1,2,0.33333)"",""(video2,2,2,0.66666)""}


As for queries...depends on what you want your query to do!  If you need to query inside the array, take a look at some of the [array operators](https://www.postgresql.org/docs/devel/static/functions-array.html) - but be aware that indexing a custom composite type array may not be very easy at all, so performance could be an issue.",1532010910.0
fullofbones,"I wouldn't consider pgAdmin an authoritative source on that kind of information. It gets its information from the Postgres system catalog like everyone else. I'd recommend opening this as a bug if it's unintended behavior, as that index clearly covers host_id, as the composite nature of the index is irrelevant for the first column.",1531950507.0
InfoSec812,"Yeah, that's an issue sometimes. There are several ways that you can integrate Liquibase: Maven, Gradle, embedded, etc. Different deployment environments determine which is best for you. With more info, I might be able to suggest ideas. ",1531913741.0
rouen_sk,"I doubt it. SQLite is designed for running from single file, but postgres is not. You need whole engine (WAL, system tables..) and dump file is intended to restore DB into this environment, not being opened and manipulated directly.",1531837304.0
jenkstom,"PostgreSQL is a relational database management system (server), not a file format. Whereas sqlite is a library that can read and write a database file format. They are very different things. If you want to just manipulate the dump file use a text editor. Maybe what you need to do is translate it into a SQLite format and use that instead?",1531860446.0
iiiinthecomputer,"Usually you restore it to a PostgreSQL instance and examine it there.

pg_restore can dump the table of contents of a custom format dump, but it doesn't tell you a great deal.",1531869606.0
cyberst0rm,"You can potentially dump the file in a basic format, then run it inside sqlite.

    --column-inserts
    --attribute-inserts

    Dump data as INSERT commands with explicit column names (INSERT INTO table (column, ...) VALUES ...). This will make restoration very slow; it is mainly useful for making dumps that can be loaded into non-PostgreSQLdatabases. However, since this option generates a separate command for each row, an error in reloading a row causes only that row to be lost rather than the entire table contents.

",1531884427.0
boy_named_su,"PGAdmin III is a desktop client. You can still use it

SQL Workbench/J works with multiple database brands",1531930382.0
rouen_sk,"For questions like this:
1. google tutorials, I am sure there are many for PHP+postgres
2. If you cant find it, use StackOverflow - you can tag the question with PHP and postgres; basic questions like this are answered in minutes",1531837491.0
mlt-,"For uninitiated, how is this different from postgraphile?",1531839240.0
francisco-reyes,This one I have used in the past to send Jr DBAs where I work to learn from http://www.postgresqltutorial.com/,1531769589.0
ajatkj,I know this doesn’t answer your question but learning on the job is the best way to learn anything new. Google up stuff as and when required. With your experience you’ll find it pretty easy to learn this way. Not sure if many people agree with this. ,1531754003.0
i-eat-kittens,"With your background any online course is guaranteed to be nothing but a waste of time. You already know all the concepts people take db courses to learn.

What you're missing are the specifics, which you find in the [official docs](https://www.postgresql.org/docs/current/static/index.html). RTFM front to back, and you're almost good to go.

Once you're through the manual, have a look at pgbouncer. Then spend some time investigating (linux?) system administration, containers/virtualization and replication/failover. Deployment/platform related stuff, I guess..",1531779461.0
leamanc,"If you’re an Oracle DBA, you’ll feel right at home with Postgres. 

If you’re handy at PL/SQL, you’ll take to PGL/SQL right away. 

Although I know they have different lineages, I often think of Postgres as the “open source Oracle db.”",1531763501.0
mokadillion,"Working PostgreSQL DBA here. 

The documentation is amazing read it and read it again. From start to finish it’s a great guide on managing a Postgres instance. 

Absolute must know topics. ;
WAL archiving. 
Streaming / logical replication. 
Failover methods. 

Connection pooling.: 
Pgbouncer
Pgpool

Configuration:
Postgres.conf file
Pg_hba.conf file. 

On top of this. The overlap between Linux filesystem and server admin and PostgreSQL admin is pretty blurred so get some Linux in you too. 

Master this and you will be good to take on most installations. ",1531785178.0
alephylaxis,Train me a bit in Oracle and I'll train you in PostgreSQL :) ,1531788881.0
cachedrive,"PostgreSQL is very slim in regards to training. EnterpriseDB offers it but at that price, you can likely hire the whole PG dev team to spoon feed you knowledge.  


Fire up your favorite VMs and install / configure PG 9.6 or 10+.",1531754539.0
spinur1848,https://github.com/laurenz/oracle_fdw,1531788295.0
cyberst0rm,"you probably  need type coercion 
 
    '192.168.1.1'::inet

Inside the %s which I think means string

    %s::inet",1531745715.0
shobble,"as cyberst0rm says, something like:

`cur.execute('insert into moo (ips) values (%s::inet[]);', (['0.0.0.1'],))`",1531765867.0
mage2k,"From reading the psycopg [docs](http://initd.org/psycopg/docs/extras.html#additional-data-types) I think you need to get rid of those str calls.  Try this:

    cursor.execute( ""INSERT INTO test( ip_list ) VALUES ( %s );"", (psycopg2.extras.Inet( ""8.8.8.8"" ),) )

Also, you have called `psycopg2.extras.register_inet()`?

",1531937153.0
wolf2600,"When the user logs into what?  Do you have an application that the user would access?

I'd suggest adding a Notifications table, then create an UPDATE trigger on your Questions table which will insert a record into Notifications containing the userID, questionID, timestamp, etc (or update if a record already exists for that user).

Then add a step to your user login process which would query the Notifications table to see if it contains any records for the user who is logging in, if a record was found, display a popup to the user and delete the record from Notifications.


Actually, the update trigger doesn't even have to be in the DB at all.  Just make it part of your application code to insert/update a record in Notifications whenever a question is edited.

I think the Notify feature listed would be more useful if you had multiple applications updating a shared database, where one could be updating records and the other would need to know about it.  But in your case, its a single application doing all the work, so the notification functionality could be built in to the application itself rather than using database functions.
",1531656071.0
pypt,Have you tried the docs?,1531493103.0
AlexAtFTI,"Hi! Here are links to Parts One and Two of the series if you missed them, let us know what you think!

[https://articles.fti.bi/postgres-10k-connections-part-one](https://articles.fti.bi/postgres-10k-connections-part-one)

[https://articles.fti.bi/postgres-10k-connections-part-two](https://articles.fti.bi/postgres-10k-connections-part-two)",1531420119.0
boy_named_su,"So, there's SQL Table Inheritance and PostgreSQL Table Inheritance. They are not the same

PostgreSQL Table Inheritance is useful for partitioning

SQL Table Inheritance is a useful design pattern to implement subtypes/supertypes

Single Table Inheritance (one table for all subtypes) is performant and simple, but requires NULL columns

Whereas Class Table Inheritance (one table per type) means no NULLs but is slower and more complex",1531418988.0
leandro,"More tables — much more comprehensible.   Table inheritance was just a fad leaving a bad inheritance, as object orientation proved harmful as regards data.  Also, remember always defining natural keys, as surrogate keys do not ensure unicity.",1531443797.0
fullofbones,"Please do, actually. One of the whole points of schemas is to act as a namespace. As a side-effect, that namespace separation _can be utilized_ for multi-tenant scenarios.",1531410901.0
boy_named_su,"PostgreSQL actually calls schemas ""namepaces"" internally, so have at 'er

https://www.postgresql.org/docs/9.3/static/catalog-pg-namespace.html

Personally, I think it's better to store tenants in their own databases, for true isolation
",1531417670.0
pypt,"Sure you can, although I haven’t seen this being done too often, usually everything gets crammed into “public” schema to reduce the amount of typing needed.",1531408514.0
i-w-d-w,"Sure, if you want.  There isn't a huge amount of upside to doing things that way, unless you're using the schemas for access control or such.  What it does mean is that now every query will have to be written to schema qualify many tables (or you will have to set search_path appropriately on all connections).  I'd only do it if there was a compelling reason to do it that couldn't be solved another way.",1531409176.0
spinur1848,"I use them for separating permissions, and version control. 

Ive got slowly changing data that gets updated once a month, and some subset and derivative structures i want to make available externally through an API.

So each monthly extract gets its own schema with a date in the name, and a separate schema called ""current"" that's a set of views that get updated. There's also an api schema, again populated with views to current but with different permissions.",1531419771.0
outlier_lynn,"In my databases, the public schema is only used as an API for accessing functions in other schemas.  I have other schemas that separate different business models and a ""common"" schema for things common to all models.  That would be things like time and session function. 

Sanity checks are performed on function parameters given to public schema functions. The public functions then call the appropriate functions in the correct schema.  The search_path of public functions are set to current when the function is defined. 

More than half by business logic is handled by PostgreSQL.  And if I were slightly less lazy, a good deal more would be in the database rather than in middleware.
",1531421857.0
everlong241,This limits your database to columns only of type jsonb. No thanks. Better to refactor queries manually for Postgres or just stay with Mongo.,1531383450.0
boy_named_su,"It's easier to tell what time it is by looking at it 

You can do date math on timestamps directly ",1531346539.0
mgonzo,"All of those and more! Typing is pretty important. You miss a digit in that epoch, welp good luck, Dev does some crappy math on it during an update... Waaa.. Whereas if it's a tsz well then you get an error on insert/update if it's not a valid ts.

That's pretty handy. I've run across plenty of schema where they just used ints and had things like row creation date: 122346, for their timestamp. Perfectly good int, not so much on timestamp. Along with formatting, manipluation functions etc. 

Want to find out the day of the week that timestamp is, np extract(dow from timestamp) .

See for even more fun you can have with timestamps! https://www.postgresql.org/docs/current/static/functions-datetime.html",1531350612.0
minaguib,"If all you do is do date arithmetic, a unix epoch will do fine.

If you often however have to actually render and present the date/time (whether internally by looking at the data in the db, or externally as part of a UI), timestamp(tz) is a better choice.

Also timestamp fields in PG work naturally with another data type called intervals.  This is quite readable compared to magic numbers/formulas going the other route:

`select CURRENT_TIMESTAMP + '4 months'::interval;`",1531349614.0
lukaseder,I'm very happy with https://dbeaver.io,1531337410.0
merican_atheist,DataGrip by JetBrains I find to be very useful and a more generic solution to more than just postgres.,1531338407.0
Citizenfishy,"pgadmin3 had to be reengineered because the codebase relied on old libraries and was becoming a nightmare to maintain.

The pgadmin team are incredibly receptive to bug reports and feedback. Just look at the mailing list and you'll find Dave Page takes every bug issue on board despite getting a constant panning for all his hard work and dedication.

I use pgadmin4 daily, it's not perfect but it works and it is getting better all the time as the team gets more feedback and issue reports. The new architecture makes it easier for them to make such changes.

Instead of whining like a bitch  on reddit maybe you could offer some bug reports that replicate this and you'll find that they will do something about it. I doubt they have time to trawl the net for petulant comments from the moaning types who usually contribute f-all back.",1531342861.0
wheezl,"It has its shortcomings but it gets better with every release.   I mostly use psql or Datagrip depending on what I'm doing but PgAdmin is improving.   Comments like ""pure garbage"" really aren't helpful to people who are giving up their free time in an effort to make a new tool.

Contribute to the project or start your own before deciding to behave like an asshole.",1531344032.0
sorryihadtosaythis,"It really saddens me when I read posts like this one. Statements as ""pure garbage"" are so harsh. Imagine yourself being an active developer of pgAdmin 4 and doing your best to improve the product and some random stranger degrades your work as ""pure garbage"" without providing any actual feedback...

Anyway, I had to say this. Think about it.",1531342598.0
lykwydchykyn,"I use pgcli for running quick queries, and dbeaver for fancier operations.  I miss pgadmin3 for some things, though.",1531340076.0
Fun2badult,It was surprising to see pgadmin3 go from app to website app,1531356590.0
iiiinthecomputer,"I think the issue was that PgAdmin-III was a pain to maintain and nobody wanted to work on it.

PostgreSQL is a great DBMS, but the tooling is a second-rate afterthought. (I work on some of said tooling, btw, but too few people are interested).",1531360088.0
zsoca8711,Command line > anything,1531342750.0
koalillo,No love for Squirrel SQL?,1531338763.0
mispp1,"If you know some qt5 devs that would like to contribute, this might be a viable option before futurama becomes a reality:

https://github.com/mispp/goat

Have to say, finding people to contribute is damn hard",1531340355.0
fullofbones,May want to consider giving [OmniDB](https://www.omnidb.org/en/) a try as well.,1531342669.0
cyancynic,"I use navicat.  Its generic.  Its java.  It works across all major JDBC supported databases (for some limited definition of works). 

I don't like it but I do get things done with it.",1531342811.0
HeyzeusHChrist,"I use pgAdmin and intellij(datagrip).  They have different uses, but pgAdmin is definitely needed.  The web interface was a little annoying at first, but it's fine and I have grown to enjoy it.  If you use pg10, pgadmin4 is a must.",1531347050.0
qatanah,"I like pgcli, if i need more longer lines and analytical stuff. Try vim + dbext extension. It has steep learning curve but could blow your mind once it's running.

https://mutelight.org/dbext-the-last-sql-client-youll-ever-need",1531358857.0
smugbug23,"Are you using the latest version?  PgAdmin4 had a rough introduction, but it did improve a lot since version 1.0.

> Yes its free, and thank you, but don't fix it if it ain't broke. 

It isn't just free as in beer, it is free as in speech.  The people who maintained PgAdmin3 thought that it was thoroughly broke as far as maintainability goes, which is why they changed, but you are perfectly entitled to keep maintaining PgAdmin3 by yourself or with as many friends as you can muster up.  And some people [are doing just that](https://www.openscg.com/bigsql/pgadmin3/)",1531451700.0
Doctor_Jeep,"Pgadmin 4 is worst fucking piece of shit software Ive ever encountered in 20+ years in the IT business. I rage every single day at it and I have to work with it, which is simple an impossible task. I am basically forced to reinstall it every day I start my desktop - it wont start before I reinstall it and that stupid idiotic launch screen will stare at me without anything happening. The query tool is garbage, the notifications are always in the way garbage, backing up a database is garbage and frequently breaks. Everyone working on it should retire and never touch a computer again as part of a job, as they must be full blown autists with absolutely no feel and understanding of usability and user handling. It makes me soo soo angry to think about the hours of work I wasted with this piece of crap. ugggghhh",1537869565.0
brandit_like123,"The future is web. I had the same thing. Spun up a t2.nano AWS instance with a redash AMI and I'm happy. Query sharing is easy as pie, and I can access my queries and charts no matter where I am. 

Redash and Metabase are both good. ",1531344030.0
leamanc,"It all depends on the security you need in your environment. If you don’t really have any privileges to delegate, then no, there’s nothing wrong with it. ",1531328471.0
boy_named_su,"If you're doing (most) of your DDL using a migration tool, you should create a user for that tool, and use that user. Make them a superuser if need be

That user should own the db and tables too

Actually, I'd use a non-superuser for as much as you can, and only use a superuser role when absolutely required",1531345887.0
dtseiler,"I'd be fine having the postgres user own the DB. From there I would create schemas owned by app_owner roles that own the objects within them. Set up app_read and app_write roles that get the needed default privileges for that schema.

Then create whatever actual app_user logins as needed and assign roles accordingly.",1531496266.0
ants_a,"Probably index scans are causing random IO. IO on a normal index scan is sequential, meaning that for each random IO you will be waiting for average half a turn of the disk. That gives a maximum 10000rpm/60s/0.5 rotations*8192B ~= 2.6 MB/s of IO, less if some of the data is coming from cache. You really should get an SSD for this type of workload.",1531321969.0
jringstad,"How big are the indices? I would suspect they end up not being cached. I don't know what heuristics (if any) postgres uses to determine whether indices should be cached, and how it goes about caching them, but it might have decided that the indices are too large to keep them in memory.",1531331534.0
smugbug23,"> 2MB second when a query kicks off is barely above idle.

If the IO is random, not sequential, then 2MB/s could be as fast as you can go on spinning rust without parallelism.  You should look at metrics of latency time, not throughput, to determine the bottleneck.  track_io_timing, for example, is a good tool for this in PostgreSQL.  At the OS level, maybe something like svctm or %util from sar.

It is probably not the indices, but the table access directed from the indices which is driving the random IO.  So even if all the indices are in RAM, it doesn't help unless the tables are too.

You didn't say which version you use, but parallel query driven off from large index scans weren't introduced until v10. 

Index scans which return lots of rows would often execute as bitmaps scans not regular index scans, and bitmap scans can benefit from large settings of effective_io_concurrency to get some benefit from the RAID.  But the default value of effective_io_concurrency is not large.   If you have not increased it to at least 8, you probably should (unless you expect to have many of these types of queries running simultaneously).  Also, the planner does not take effective_io_concurrency into account, so it might choose regular index scans even though bitmap scans would be faster if effective_io_concurrency where accounted for.  That can lead to tuning frustrations.

In the absence of both parallel queries and bitmap scans with large effective_io_concurrency, then each random read is not started until after the previous one finished; so you get no benefit from the RAID, you might as well just have one very large 10000 rpm HDD, as far as index scans are concerned.

Also, you said you tuned it, but not what you tuned it to.  We don't know what you inputted into the first link, nor which of the pieces of advice from the 2nd link you chose to follow.  Can you share your actual non-default settings?

Finally, an EXPLAIN (ANALYZE, BUFFERS) of an example simple but large and slow query could go a long way to getting better advice, especially if you turn track_io_timings on before running it.",1531451153.0
olcrazypete,"We actually do both a full dump periodically AND have barman in place, although size and load limitations has us leaning toward just using barman, and restoring the barman backup periodically to replace the full dumps.  

",1531323469.0
zieziegabor,"barman and friends is probably the right answer.  It's hard to get those scripts right as there are lots of edge cases you have to manage.  The sad part is, unless you test your backups constantly, you probably won't find out until worst case scenario happens, and life gets super, super bad for you.

That said, even with barman(or pgbackrest or whatever) you still should test your backups!",1531328493.0
obscurant,"We use pgbackrest, and have pitr with that - pgbackrest has replaced our pg_basebackup scripts. 


",1531327763.0
jpastore,"So relational databases serve a different purpose than nosql db. Typically you want to use both. Right tool for the right job. I like to use redis and PostgreSQL. Redis is a fast cache where I keep aggregates for making decisions quickly with Lua scripts doing most of the work server side. PostgreSQL is my authoritative record from which cache is rebuilt. I use decent hardware for PostgreSQL, and provision a ton ram for redis. Scaling PostgreSQL depends on the data and application to get a good sense of which way to go. After talking with commandprompt.com, I'm leaning towards timescale.com, though I hear green plum is nice. ",1531255706.0
zieziegabor,"depends on what you mean by ""scale"".  

In general for PG, you want to scale the hardware, and not to multiple instances.  If you get crazy large(unlikely) that modern hardware can't meet your needs, then it gets more complicated moving PG onto more than one piece of hardware, but it's totally doable.

For instance:

Amazon and other cloud providers offer PG compatible global DB instances, they have no real scale limitations, if you are willing to write a big enough check.

There are tons of companies you can pay to help you, if you want it on-site.  Citus, 2ndQuadrant, commandprompt.com, etc.

Also you can do it yourself, with many diff. solutions.  Many of which are OSS, and each have their unique trade offs.

There is no one right answer, unlike with most NoSQL DB's where they support 1 and only 1 way to do multi-instance scaling, so you have to pick the right NoSQL DB today to handle your scaling problems in the future.  This is unlikely to go well for you, as the chances of picking correctly is hard.  With PG, there are many options, and you can pick the solution when it's time to scale(well hopefully just a bit before, so you don't have to rush it!).",1531258062.0
syslog2000,"Historically NoSQL has offered better performance at the cost of data (and metadata) integrity.

However, a properly tuned RDBMS like PostgreSQL gives you more than acceptable performance for 99% of cases and you get to keep your data integrity (and sanity) intact.

Let me put it this way. If you are not sure you need NoSQL or RDBMS, you need RDBMS.",1531255656.0
leamanc,"You’ll see the benefits of NoSQL when you have as much data as Google does. 

All this nonsense about unstructured data is said by people who hope to have a data set that large someday. 

This is a really great blog post that puts everything into perspective, with humor: https://www.citusdata.com/blog/2018/05/08/its-the-future-for-databases/",1531266397.0
arkrish,"The chief disadvantage in Postgres and other RDBMSes compared to NoSQL when talking about scalability is in the write mode. In acid compliant RDBMSes with transaction support it’s extremely tough to have multiple writers, and hence you may hit some limits if you have to scale writes. In NoSQL it’s easy to scale writes. 

Note that it’s easy to scale reads in both kinds of databases. ",1531299967.0
Darkmere,"Not really that difficult, but there are different kinds of scaling. Without knowing something about the workload, it's hard to discuss scaling.

For example, We have stable load (ingest rate) of data, and slowly growing amounts of data ingested (with each customer).

This means that our loads are pretty predictable, but since it's constantly more and more data, disk size is the main scaling point.  For that, postgres has pretty good pieces (but no pre-created tooling) for offloading some data to other places (servers, storage, etc)

Others might have larger queries (memory intensive)  and large working sets that get queried a lot, which calls for different kind of scaling (fan out) and different optimizations.

",1531313110.0
sparrow_ua,RemindMe! 2 weeks,1531254446.0
therealgaxbo,[pg_repack](https://github.com/reorg/pg_repack)?,1531229655.0
ants_a,"In one transaction create a copy of partition, insert all data, build indexes. Then as a separate transaction swap the partition out, by detaching old one and attaching the rebuilt one. Then you only need to lock out writers for the quick metadata operation at the end.",1531232079.0
Darkmere,"If you have primary keys on the tables, then `pg_repack` can do what you want.

Otherwise, if it's okay that data is _missing_ during periods, you can detach the table, cluster it, and re-attach it.  That way traffic to the primary table works, while sometimes reporting ""missing"" data for that period.

I wrote about [how I solved it](https://www.modio.se/scaling-past-the-single-machine.html) which also includes sourcecode for that part.  That article covers a bit more than just the clustering part.  ( Here's [another](https://www.modio.se/optimizing-disk-usage-of-zabbix-and-postgresql.html) article about similar size issues on databases )

The main problem with time-based data as it comes in is that you usually don't query by the timestamp, but by some other key, causing it to be spread out across the disk compared to query order.",1531238382.0
stympy,Use [patroni](https://github.com/zalando/patroni),1531158832.0
jmswlms,+1 for Patroni. This [blog post](https://www.opsdash.com/blog/postgres-getting-started-patroni.html) might help in getting started.,1531197215.0
btgeekboy,https://www.postgresql.org/docs/current/static/different-replication-solutions.html,1531212044.0
AlexAtFTI,"We're big fans of [Sorintlab's Stolon](https://github.com/sorintlab/stolon) for HA PostgreSQL; it holds up pretty well to load and supports cloud-native and custom/local deployments, as well as both in a hybrid setup.  [This document](https://github.com/sorintlab/stolon/blob/master/doc/syncrepl.md) goes over synchronous replication setup in particular, we've had the most success using Consul to back it.  We've also put together [a little series](https://articles.fti.bi/postgres-10k-connections-part-one) on running HA inbound Postgres connections and handling the efficiency of connects/disconnects which often comes into play with larger deployments.",1531419518.0
ertegix,"The path showed you is /etc/postgresql/, but you're trying to find /var/lib/postgresql

",1531134139.0
Zell0u,https://github.com/JGroselle/pgbackup,1531152208.0
OHotDawnThisIsMyJawn,Second flyway.  Make sure you understand their philosophy for how migrations should run and work (and why backing out a migration generally doesn’t make sense),1531066882.0
mikaelhg,https://flywaydb.org/,1531065316.0
Abstrct,"I hadn't heard of flywaydb, which looks pretty sweet too, but I had always used a fairly simple tool called [Sqitch](https://sqitch.org/)",1531080468.0
chock-a-block,"Two ways to do database changes depending on the scale of your application that are easy/discrete.

1. Low-volume application: Add everything as it runs.  You might run into a couple of locking scenarios if people are using the application, but no show stopper.  And then update the application. 

2. Restore a database backup on a new LAMP instance with the new application and updated database. Change your NAT to the new stack.  

Number 2 can go no downtime at all with a load balancer.",1531086573.0
TiCL,"I never found these so called db change management tools to be effective, especially when project grows beyond simple schema. Large projects still require manual scripting by full time db architect.",1531112434.0
thelindsay,"Also if the column names themselves are sensitive, you could use a separate database + foreign tables where the foreign table does the job of the view by only defining the shared columns, and set either the foreign server or particular tables as option ""updatable 'false'"". It'd still be real time on read. There's no way to mask column names in the same db because any user can query pg_catalog.",1530926332.0
ellmkay,"Yes it probably shouldn't be in the PPA anymore. 

I haven't found anything better, unfortunately. ",1530939133.0
ellmkay,There is a patch in the issue you linked to. The problem is that there are no active maintainers of apgdiff anymore.,1530906851.0
ellmkay, You might want to try this: https://github.com/eulerto/pgquarrel,1530939680.0
postgrescompare,There's the tool I'm working on right now if you want to give something else a try. www.postgrescompare.com,1531085377.0
thelindsay,"Could you use ""exclude"" for things like year-to-date total excluding the current quarter? That could be an exclude current if the input data is quarterly, or exclude group if it's finer (month/week/day etc).

For exclude ties maybe it could be good for finding a distinct count with in window frame e.g. to find which months a customer made any order?

I would have thought the SQL 08/11 standards would have examples but sadly they're behind a paywall.",1530934128.0
9too,"First things first, I'll say it's a bad sign to see conditionals and loops in SQL. It's not that they don't have their use, but in general you don't need them.

To address your actual question, here's what I can suggest :

    UPDATE tasks
    SET (name, description, date) = (COALESCE($1, name), COALESCE($2, description), COALESCE($3, date))
    WHERE name = $4

So if you didn't want to update a field, you would set the corresponding parameter to `NULL`

If you prefer/have to use this in a function, I would suggest to use `LANGUAGE SQL` whenever possible, that way the query can be optimized by the planner unlike plpgsql.",1530854435.0
thelindsay,"https://www.postgresql.org/docs/current/static/transaction-iso.html

https://www.postgresql.org/docs/current/static/explicit-locking.html

It took me a couple of reads to sink in. 

Firstly, it is too broad to just say ""lock"" - there are many lock types that interact in different ways. Some of them conflict and so block an action, mostly that certain DDL blocks certain DML.

Serializable transactions (tx) start with a snapshot of the database state as of the time of the first statement, just like repeatable read. Both isolation levels cannot update or delete data at the same time as another tx, but they can insert new rows.

Serializable goes further than repeatable read to track ""anomalies"" which can happen when you insert data in one tx that affects a statement in another tx, such as adding an audit log record which is being summarised in another tx into a new row. It uses the lock system to achieve this but the locks don't conflict with other lock types.

In terms of concurrency, you can have multiple serializable tx running at the same time. For them to all succeed, they have to not be modifying things that would affect the outcome of each other's tx work. So you could have tx 1 read from table A and insert/update/delete to table B, tx 2 read from table A and insert/update/delete to table C.

However if they conflict, either by anomaly or because you're doing updates, deletes, DDL, etc on shared resources (e.g. both affect table B), then the first tx to start will succeed and all other conflicting serializable tx's will fail and need to be re-tried. It really depends on conflicts, e.g. you could have serializable tx's 1, 2, 3, 4; if 1 and 2 conflict such that 2 is cancelled then 1, 3, and 4 can succeed.

It's possible to have different isolation levels going at the same time too, so you could have some read committed level tx's going while a serializable tx is going, and they could all succeed as long as there are no conflicts. In that scenario, the read committed tx's would not see the serializable tx work until the serializable tx was committed.

I encourage you to, well, not be discouraged by this complexity. Understanding and managing tx isolation can help avoid logical errors as well as a bunch of app code to try and deal with these issues which are readily solvable in the database.",1530836258.0
jakkarth,"That's not how you use long-form flags. Those start with two dashes, not no dashes. It even tells you this with examples in the help text:

    Connection options:
      -h, --host=HOSTNAME      database server host or socket directory (default: ""local socket"")

and it's giving you hints that you're doing it wrong by asking you for a password for ""user=postgres"" instead of ""postgres"".",1530824178.0
lgastako,Set the PGDATABASE environment variable to the name of the database you want to connect to. ,1530769844.0
depesz,"Please note that when doing

    psql

you try to connect as user iruvmattree (i assume it's your shell account name), and then - it tries to connect to database named just like your user.

On the other hand: psql -U postgres makes psql connect as *user* postgres, and then, only because you didn't provide database name - it picks ""postgres"" (based on username) as db name.

So - to change your default database to postgres, you'd set PGDATBASE to postgres in shell, but it's not what you seem to want to do - based on your examples, I would say you want to set PGUSER to postgres.

Also - please note that this is bad idea. Just like noone should be working as root, noone should be working as superuser in database.

Make yourself a pg account, and db, and use it.",1530790101.0
cachedrive,"You should always explicitly connect to your db...

psql -d mydb -U user -W",1530789979.0
thewheelsontheboat,"Technically in postgres, at least as of when I last tested on 9.3.5, `*` can actually be faster.  It can be faster even than selecting a single column.  You will typically only notice it when scanning many rows and returning few, eg. an `OFFSET 1000000 LIMIT 100` query.  The reason for this is when you specify a column list postgres has to do a projection from the raw tuple to whatever the list of columns is, which is more expensive than just reading what is in the table on disk, as you suggested.  I do not know if specifying the full list of columns in the on disk order avoids that penalty, I suspect it doesn't.

If you are actually returning most of the data you are scanning, the cost of doing that that will far outweigh any difference.

You should very seldom if ever make decisions based on that performance difference though, and the adage ""don't select columns you don't need"" is typically very valid.  I know that isn't your question but had to say it.

This, of course, could be optimized by the database in a variety of ways, it just is an edge case that isn't (at least wasn't last I tested).

See https://www.postgresql.org/message-id/10064.1412303977%40sss.pgh.pa.us and its parent.",1530771574.0
jakdak,"Not sure exactly how this is handled in PostgreSQL, but my guess based on how this works in nearly every other DBMS is that the ""*"" will be expanded to the full column list by the query interpreter/parser before it even hits the query execution unit.

So the differences will likely be infinitesimal if they are even measurable. ",1530752938.0
Paratwa,"Depending on the tool you are using I typically see more memory usage when doing a select * vs manually choosing the columns, I can’t tell you why, but I definitely see it. Of course this is when using massive datasets far beyond what most people run into, for normal day to day stuff I’d say the difference is very small.",1530761894.0
pstef,">I did find one bit of relevant text in [this answer](https://stackoverflow.com/a/67380/4698922):   
>>For item 3, there is NO savings as the expansion of the \* has to happen anyway, which means consulting the table(s) schema anyway. Realistically, listing the columns will incur the same cost because they have to be validated against the schema. In other words this is a complete wash.

The context of that answer is item 3, which is ""SQL Server query parse / validation time"". I can believe it's similarly true for parsing in Postgres as well, but it's not the parsing you should be thinking about but the data that the backend will have to fetch from storage in order to be able to yield the whole result.

Of course it's OK to use the \* when you're wrapping a subquery that yields individual columns/expressions.",1530770310.0
johnfrazer783,"I want to second everything that has been said in the other answers, including that it is generally a bad idea to get bogged down in micro- and nano-optimizations before having hard evidence where time is spent in an actual scenario; this applies especially to a declarative and highly optimizing environment like your typical SQL RDBMS. 

That said, there's another dimension worth mentioning when weighing 'select star' against 'select column names', and that is maintainability, reliability / confidence, and intent. 

There's a common rule to the effect that `select *` is only good for dabbling and one-offs, and has no place in production code. I think this is generally the right thing to observe; you should typically hold yourself accountable to each single value / column you deal with, and make that choice explicit. That is particularly important when quering the DB from the application; the lazy way is to 'request everything' and then sort things out in the app, but that will invariably incur higher network costs than being more specific with what you really need. 

*But* there's actually a place for `select *` in production, and that is diagnostic data views. For example, you may want to define a view as `select * from sales order by sale_date desc;`; the *intent* is here to 'show everything, leave out nothing, be accepting with schema changes, only assume column `sale_date`'. It is a little more maintainable to use the star here. If that view was in the middle of a processing chain instead, it's probably a good idea to emphasize confidence ('yes, this explicitly named column is needed and does exist') over ease of maintenance.

BTW all of 'bunch of regurgitators', 'spewing', 'over-beaten horse', 'blah blah', 'we've all heard a million times already', 'irrelevant' are all stop-words when it comes to asking for advice. I hate it as much as the next guy when I ask 'can I do that using technique x' and all I get back is 'why do you want to use x' or even 'you could do something completely different'. Yet saying that 'select * has no place in production' is actually good advice with little reservation; it doesn't say why, and doesn't say whether there are any performance implications, but, for a 'dead horse', it's surprisingly sound.",1530859757.0
SulfurousAsh,"Disk reads and network activity will completely dominate the time of the request.

The usage (or not usage of) ‘*’ should in no way be based on speeding up queries. 

However, if you don’t need every column, there can be significant savings for just specifying the columns you need.  Taken to the extreme, some queries can be served off of indexes only if you’re using columns that are all in an index.",1530751117.0
alinroc,"In SQL Server, there's no significant performance difference between `select *` and naming every field on the table. I would expect the same for most modern RDBMSs

However, it's a rare case that you actually **need** every column on the table and there *is* a performance penalty for selecting columns you have no use for.

Also, if you address the columns by ordinal (position) instead of by name in the application running your query, you'll get unexpected results if the column order on the table ever changes and you're using `select *`.",1530755210.0
spinur1848,Pgmodeler,1530699464.0
ants_a,"Here's [a small script](https://gist.github.com/ants/8f46ce7dd1fb7ef41e88ae87064de988) that renders schema as input for plantuml. Doesn't handle fancy features, but has been good enough to get some automated documentation.

Usage: pg-schema-to-plantuml.sh ""host=localhost dbname=mydb"" > mydb.plantuml && java -jar platuml.jar mydb.plantuml",1530702353.0
pypt,Start off with a simplest solution (a single database with team_id) and change course once you reach 1b or so teams.,1530669065.0
leamanc,"Referencing a team ID is not going to slow down your database or app. Not until you’ve got an insane amount of data, anyway. Keys and indexes are your friends.

Part of why we normalize is to not have redundancies. Creating multiple instances of schemas is definitely redundant.",1530665148.0
boy_named_su,"This doc explains the trade offs

https://docs.microsoft.com/en-us/azure/sql-database/saas-tenancy-app-design-patterns

It's not Microsoft or azure specific ",1530666890.0
Foodei,"Team-id with  Single database. You can save complexity and money (especially if you are in the cloud).  Down the road if you run into a volume bind, you can look into partitioning/sharding/table inheritance to address performance.",1530724578.0
mage2k,Thank you!,1530643225.0
platohut,[SchemaSpy](http://schemaspy.sourceforge.net/) is pretty good.,1530630395.0
professaDE,"If by ""visualize"" you mean ""display query results (or table contents) in interactive configurable graphs/diagrams"", then possibly:

* Redash (https://github.com/getredash/redash) 
* Superset (https://github.com/apache/incubator-superset) 
* Metabase (https://github.com/metabase/metabase)",1530636242.0
chock-a-block,"I'm not sure what exactly you mean by visualize, but, the old open source, Java solutions are  BIRT  and JasperReport.  There are a couple of projects that rewrap those.  Use a JDBC connector to your database.

The most reliable is a Python script.",1530647684.0
JesseJessie0115,"You need a data visualization tool, and I recommend this software ---FineReeport,which I recently used.

Data visualization analysis using FineReport is very simple. After simple configuration, users can simply drag and drop the target data and related dimensions on the B/S side to get the results of analysis from different dimensions. [www.finereport.com/en/](https://www.finereport.com/en/)",1540965946.0
mage2k,Any video of the talk?,1530641104.0
contrarianism,"If you are able, it may be worth looking at [AgensGraph](https://github.com/bitnine-oss/agensgraph) – a graph database built on top of PG.  

Doing graph models within relational is complicated, and with the data volume you're talking about, performance could be prohibitive.  In any case you're reinventing the wheel, which has it's own overhead and risk.  

Agensgraph is nice because you have relational and graph data models side-by-side with seamless integration.  And your graph is a first-class citizen with the Cypher query language.  I just started using it for a project, but they claim strong performance and should hold up for your needs.  And at least you don't have to fight the impedance mismatch of data models.  ",1530570469.0
pypt,"How large do you plan those tables to be? Also, could you post (at least preliminary) schema for some reference?",1530552481.0
greenspans,try neo4j,1530598396.0
mokadillion,"Quit literally in a directory called DATA. 

https://www.postgresql.org/docs/10/static/runtime-config-file-locations.html",1530475071.0
atipugin,This article explains how it all works in PostgreSQL http://rachbelaid.com/introduction-to-postgres-physical-storage/,1530481580.0
depesz,"Others did provide information, but please - NEVER touch database files. NEVER. Nothing good will come out of this, it is never needed for any practical application, and whatever you'd do with the files - will break sooner or later.

Specifically:

copying the files, renaming, zipping - is bound to cause problems and loss of data.",1530521061.0
hardwaresofton,"Hey you might not want to hear this, but you should probably give the postgresql documentation a read-through in your spare time:

https://www.postgresql.org/docs/current/

If you want to jump to the section that would have answered your question, it's the [Server Administration section](https://www.postgresql.org/docs/current/static/admin.html), in particular [file locations](https://www.postgresql.org/docs/current/static/runtime-config-file-locations.html).

You should at least give the documentation of a DB you want to use a once-through -- while you don't necessarily have to become a DBA on the first day, it's important to know your tools",1530536000.0
leamanc,"I don’t think Postgres uses .db files. I think each table is a file, but I’ve never actually gone looking. From what I understand, they should be in the data_directory. But you can also check the output of this query:

    SELECT * FROM pg_tablespace;",1530473663.0
jakkarth,"You can use parentheses to group operands, just like in mathematics:

    ...
    ""public""member.""HealthCarePlan_id"" <> 10 AND
    (""public"".member.""PlanEndDate"" <= (current_date - interval '3 months') OR
    ""public""._member.""PlanEndDate"" IS NULL)",1530451711.0
lindleyw,"Also consider the IN / NOT IN clauses for clarity:

    ...WHERE...
    ""public"".member.""HealthCarePlan_id"" NOT IN (2, 8, 10) AND
    ...

",1530453352.0
mage2k,"> ""public"".member.""PlanEndDate"" <= (current_date - interval '3 months') 

That's going to give you members with end dates *before* the last three months.  That should be switched to >=.  Also, instead of the OR consider using COALESCE, e.g. `COALESCE(""public"".member.""PlanEndDate"", current_timestamp) >= (current_date - interval '3 months')`.  That essentially says ""if the value is NULL, use `current_timestamp`"".",1530561669.0
myurr,"NULL + 1 = NULL

So you'd need to SELECT SUM(COALESCE(task1time, 0) + COALESCE(task2time, 0)...)
",1530191488.0
Darkmere,"pgbouncer is my recommendation.

Especially for applications built on the PHP style of ""many short connections"", it helps a fair bit.  both in that it makes loads more reliable on the server (amount of processes spawned can be kept constant, etc.)  and it improves the performance of the application.
",1530131762.0
mokadillion,"

Have set up pgpool and I would say it’s no fun to do. Documentation is poor and jump from one method of set up to another and following from start to finish won’t work. Google was used extensively. HOWEVER. connection pooling is very useful. 

If your application makes many requests that do not need persistence then each time a new connection will need to be made.  By reusing pooled connections you can reduce the time taken to connect dramatically and subsequently the time taken to return data to your application. Connections which end up idle , like many applications do , are killed automatically and you don’t end up running out of connections thus using less resources.

It also gives you the option of pointing you app tier to a single point of entry , the pooler , which then points to your dB. If you change your dB config ie such as failover to a standby you only need repoint the pooler to it and the app tier will carry on as normal.  That said you would need an auto failover service as pgpool doesn’t offer this. 

After all that we use pgBouncer in our production set up and a separate failover manager. 
",1530127006.0
fr0z3nph03n1x,"I'd like to piggy back on this thread. I'm currently using the built in pool manager of knex.js which is great... but I'm running on heroku which spawns many ""dynos"" which means many instances of pools. Is there any best practice here for that situation. I should probably just be using a single pool across all dynos right?",1530157677.0
QuantumRiff,"I tried to like them, but gave up. Horrible documentation that was really hard to follow at times.  They don't really work if you have long running connections, and utilize transactions. (there is nothing to 'pool' then, so doesn't really save any connections).  But really, having to manage the usernames/passwords outside the database was the part I liked the least.",1530167116.0
smugbug23,"If you don't need it, then you don't need it.

Connections poolers are often a work-around for poorly written apps which needlessly open and close connections in a loop, or open connections and then leak them.

If you use stateless web technology, like classical CGI, then you would need a connection pooler for high-traffic sites.  But classical CGI is pretty much obsolete for high-traffic sites anyway, and its more modern replacements will have built-in poolers.",1530279974.0
MonCalamaro,"I don't really understand why previous_id and balance are stored instead of calculated.  It seems like this requirement is causing a lot of extra complication and potential data integrity issues.  What happens if you ever need to insert a transaction in the past, between two existing transactions?

That said, you could do the calculations with a before insert trigger, but I believe you will still hit concurrency issues unless you start locking the table, which could degrade performance.",1530091358.0
jeffdn,"You could write a function that queries the tables, and then call that in a check constraint.",1530072099.0
cybernd,"Think about which type of metric will actually help you to make a decission. If its just a metric in order to measure something, but will never be used to make a decission: not worth to be monitored in the first place.

So stop thinking about your metrics, start thinking about possible measures. Cache Hit ratio  (block read vs block hits) for example could be used to decide on a memory upgrade.

Also when you think about particular measures, think about your metrics granularity. You want sum(seq_scan) but would this actually help you to find the query causing it? This metric would be usefull on a per table granularity (if there are to many tables, just do it for your top 20 biggest tables).",1530455681.0
jmswlms,"Check out [pgmetrics](https://pgmetrics.io/), which fetches and displays a lot of metrics like these. You can copy out the queries for the ones you want from the [source](https://github.com/rapidloop/pgmetrics/blob/master/cmd/pgmetrics/collect.go), or build on top of the json output generated by pgmetrics.",1530694563.0
smugbug23,"None of these seem particularly useful to me in the first place, other than the database sizes to make sure you aren't running out of disk space.  And even that isn't terribly useful, because you can run out of disk anyway due to things stored outside of any particular database.  

The extensions [pg_stat_statements](https://www.postgresql.org/docs/current/static/pgstatstatements.html) and [auto_explain] (https://www.postgresql.org/docs/current/static/auto-explain.html), and the parameters ""track_io_timing"" and ""log_min_duration_statement"" are useful for monitoring queries.",1530281004.0
johnfrazer783,"tl;dr: The resulting code—ironically back-translated from SQL into Python, it would appear—has ""just use SQL, not an ORM"" written all over it:

    _tree_cte = db.select([
                      Location.id, Location.parent_id,
                      # TODO swap postgresql.array to db.array once SQLalchemy 1.1 releases
                      postgresql.array([Location.id]).label('path')
                      ]).where(Location.parent_id==None
                      ).cte(name='tree', recursive=True)
      _tree_cte = _tree_cte.union_all(db.select([
                      Location.id, Location.parent_id,
                      # TODO swap postgresql.array to db.array once SQLalchemy 1.1 releases
                      (_tree_cte.c.path + postgresql.array([Location.id])
                                                              ).label('path'),
                      ]).select_from(db.join(_tree_cte, Location,
                          _tree_cte.c.id==Location.parent_id)))
      # print(db.select([_tree_cte]).compile(dialect=postgresql.dialect())) # DEBUG
      direct_count = db.select([Route.category_id.label('id'),
                              db.func.count(Route.id).label('item_ct'),
                      ]).group_by(Route.category_id).alias('direct_count')
      _tree_count_cte = db.select([_tree_cte.c.id, _tree_cte.c.path,
                      db.func.coalesce(direct_count.c.item_ct, 0
                                                          ).label('item_ct'),
                      ]).select_from(
                          db.join(_tree_cte, direct_count,
                                  _tree_cte.c.id==direct_count.c.id, isouter=True)
                      ).cte(name='tree_ct')
      # print(db.select([_tree_count_cte]).compile(dialect=postgresql.dialect())) # DEBUG
      _aliased_tree_count_cte = _tree_count_cte.alias('t1')

    gc_mv_query = db.select([
                      _tree_count_cte.c.id.label('id'),
                      _tree_count_cte.c.item_ct.label('count_direct_child_items'),
                      db.func.sum(_aliased_tree_count_cte.c.item_ct
                                      ).label('count_recursive_child_items'),
                      ]).select_from(
                          db.join(_tree_count_cte, _aliased_tree_count_cte,
                              _tree_count_cte.c.path==
                                  _aliased_tree_count_cte.c.path[1: 
                                  db.func.array_upper(_tree_count_cte.c.path, 1)], 
                              isouter=True)
                      ).group_by(_tree_count_cte.c.id, _tree_count_cte.c.item_ct)
 
    class GearCategoryMV(MaterializedView):
        __table__ = create_mat_view(""gear_category_mv"", _gc_mv_query)

    db.Index('gear_category_mv_id_idx', GearCategoryMV.id, unique=True)

    class GearCategoryMV(MaterializedView):
        __table__ = create_mat_view(""gear_category_mv"", _gc_mv_query)
     
    db.Index('gear_category_mv_id_idx', GearCategoryMV.id, unique=True)

The effort as such is certainly heroic and laudable, but the result has little to recommend it over pure SQL. It totally escapes me why people insist on sacrificing readability and thereby maintainability just so they can claim they're writing in 100% Python. True, SQLAlchemy claims, AFAIK, that it is cross-DB compatible, but then when was the last time you migrated from one brand of RDBMS to another? As such, a lot of SQL is either already rather portable or else uses non-portable features. In that case, chances are that either an ORM will not help you much or else you relegate yourself to not using, say, PostgreSQL arrays or user-defined functions.

As for using a recursive CTE for hierarchy resolution, that is certainly the modern and most often recommended way. OTOH I recently replaced exactly such a recursive CTE with a series of views, each doing one single step. That does look quite un-DRY but has the distinct advantage of being less confusing than a recursive CTE, plus each single step is observable, whereas recursive CTEs have a tendency to become big black boxes. Turned out I needed only five steps to fully resolve the entire hierarchy; as a simple sanity check, I added a sixth step just to prove it isn't needed. I'd do it that way anytime again when I know the number of steps is rather small and rather certain not to increase much.
 
Just as an aside, had the author sticked to using `psql`, he would've been rewarded with nicely formatted, tabular, *paged* results for each `select * from` debugging statement; in Python, instead, he has to write a gazillion brackets and wrap that into a `print()` call to spill—correct me if I'm wrong—an unformatted slew of letters, digits and commas into the terminal.
",1530050406.0
linuxhiker,There two most obvious ways are pg\_dumpall and pg\_upgrade. The docs will discuss both and a quick search of Google will provide you with several tutorials.,1529965524.0
this1,If you're going to upgrade go 9.5 or higher. The new features alone make it worthwhile.,1529984438.0
JigSaw1st,"Thank all, i will see what i can do :) ",1529990421.0
obscurant,"More than likely you should be upgrading to 10, rather than 9.4.

man pg\_upgrade, or read [https://www.postgresql.org/docs/current/static/pgupgrade.html](https://www.postgresql.org/docs/current/static/pgupgrade.html)",1530108350.0
dopperpod,[search/RTFM](https://www.postgresql.org/docs/9.6/static/pgupgrade.html),1529956069.0
jlrobins_ssc,"Took me years to notice that there's a preexisting function making the

    array_to_string(array_agg(users.email), ', ')

dance simpler:

    string_agg(users.email, ', ')

",1529942087.0
thematrix307,"docker exec -it your_container /bin/bash

Then you can run psql 

Also, do a docker ps to find the name of the container ",1529888338.0
threeminutemonta,You want a field in the table that is not present in the csv. It’s likely that a blank date_added in the csv is being loaded into the database as null. The error is likely that the date_added is not nullable. You seem that you have a default for date_added though the way to use this field is for it to not exist in the insert statement. There is a slight distinction.,1529878323.0
mikaelhg,"Create an insert trigger on the table that automatically sets the timestamp field when someone inserts data?

That way it doesn't matter whether there is data in the field or not, I wasn't quite able to figure out from your post whether the field would be there in the CSV or not.

https://www.postgresql.org/docs/10/static/sql-createtrigger.html",1529882359.0
mage2k,"If you want to have the index used with the cube operators then use GIST.  From the docs:

> he cube module also provides a GiST index operator class for cube values. A cube GiST index can be used to search for values using the =, &&, @>, and <@ operators in WHERE clauses.

> In addition, a cube GiST index can be used to find nearest neighbors using the metric operators <->, <#>, and <=> in ORDER BY clauses.",1529861185.0
BS_in_BS,What queries are you trying to do with cubes?,1529860524.0
jlrobins_ssc,"Oh man, `CHECK` is parsed then *ignored*? That's, like, wow man.",1529806706.0
scottocom,TIL Text is probably more efficient than varchar in postgres,1529805428.0
eCommerce_2015,"On the one hand, I hate these blogs because they always say the same things. There is no '*VS*'.  It's just a beating for MySQL. Which is fine with me. I moved over to postgres after nearly a decade with MSSQL and never looked back. 

On the other hand i think we should all acknowledge that some software is insignificant. If Joe sixpack wants to put up a blog and legacy software does it with two clicks, for free and uses MySQL, then great! That's already as good as that gets. 

Even new software that can be deployed more cheaply on cloud solutions because the PaaS providers charge less. Those should also be built with MySQL, so long as the data integrity is unimportant or trivially maintained.",1529807331.0
swenty,"> I was thinking on adding a field on the user account that is `is_deleted` ... but i don't know if that is good practice.

This is a very normal way to handle this type of situation.

> The other idea that i had is to add another 2 fields on the table `question` ... But that would violate one of the principles of normalization because i would got duplicate data.

It's not really a normalization violation. You could think of it this way: the name stored in the `questions` table is the name of the user at the time that they asked/answered the question. That's a potentially different piece of information from the current name of the user, which is what's stored in the `users` table.

Nonetheless, I prefer your first solution, as it's cleaner and simpler.

You can also create two views onto the underlying users table: `current_users` and `deleted_users`. You can execute queries against those views, which helps to avoid the mistake of accidentally including deleted users in a query that is supposed to only include current users.
",1529706788.0
aragospot,"Yeah, the first option is really common. I usually use a date/time field instead of a boolean, ex. DeletedOn. It turns out that knowing when something was deleted is sometimes really important.",1529717763.0
Darkmere,"as others have said, I prefer the following setup:

`id: uuid, not null, username:text, not null, deleted:datetime, nullable`   and then a unique constraint  on `username,deleted`

Then using the userid for references, so re-adding a user doesn't re-create the old permissions etc.

",1529747672.0
MetallixBrother,"From reading your use case, I would probably recommend the 1st option if you want the username to persist after account deletion (with the potential benefit of being able to readd that user relatively painlessly if you need to).

That said, if you want to retain the comment after deleting a user and handle a null user (using reddit's [deleted] for example), set the foreign key as nullable & change the cascade rule to set null.",1529707161.0
dark-panda,"I would suggest using a time stamp field instead of a Boolean, and a null value would indicate that the record is active. That way you can track when the record was deleted at the same time. This is also very indexable using a `where not null` clause in the index definition. ",1529715477.0
AlexAtFTI,What's the coolest thing you've done with PgBouncer and friends?,1529625022.0
shobble,"how do you know which row from the `userid` table corresponds to which row from `testusersetup` table?

Once you know that, you can do something like:

`insert into dest_table (col names...) select s.field1, s.field2, ui.id from usersetup s, userinfo ui where s.FIELD = ui.OTHER_FIELD`

(replacing with the correct `FIELD` and `col names` parts accordingly)",1529599762.0
dopperpod,I figured it out too. I stopped using psql and started using a GUI editor. ,1529529258.0
dopperpod,Can't really help with the information provided. What error message do you get after you install [this](https://www.openscg.com/bigsql/oscg_download?file=packages/PostgreSQL-10.4-1-win64-bigsql.exe)?,1529517340.0
nikoz84,Windows 10??,1529537107.0
mokadillion,Seriously I wouldn’t bother with Postgres on windows. Use a Linux VM. it’s life savingly easier. ,1529517202.0
batisteo,"Still better idea than Mongo…

(sorry)",1529520736.0
antirabbit,"If you are not going to query the fields, you are probably fine just using JSON or even TEXT. JSONB should be fine, too.

If you are going to be doing complex queries with the fields, and you aren't really using joins, a document store database may be preferable.

Regardless, if your data isn't massive, then you'll probably be fine. Having too many columns could end up being a pain in the ass.

You may consider breaking the JSON up into a few columns, though, if, for example, one column is ID, another is a name, and then another is a deeply nested JSON field. ",1529513555.0
irony,"Start with jsonb and elevate fields as first order as necessary.  But keep in mind, ""your other fields aren't first order?  Really?""",1529542076.0
riksi,No. Only dynamic columns as jsonb.,1529529655.0
GFandango,"depends on use case really, if it's ""loose"" data it's fine",1529579116.0
mlt-,"As long as you don't need indexing. It all depends on what you plan to query for. Also you can flatten into EAV instead of 500 columns, but that is another can of worms.",1529513491.0
chincyincincy,Google the concat function,1529506724.0
jk3us,"    > select replace(lower('Joe B' || 'Bloggs'), ' ', '') as identifier;
     identifier
    ════════════
     joebbloggs
    
",1529508590.0
outlier_lynn,"update your-table set identifier=lower(regexp_replace(firstmiddlename||lastname, E'\\\s+', '','g')); 

Look up ""string functions"" in the online documentation for an explanation of the functions I used.

This assumes the identifier column is of type text. Also, if you intend to use this identifier as a primary key or with a unique constraint, you might well have a problem.  ""John Anthony Smith"" and ""John Aron Smith"" would have the same identifier.
",1529512293.0
i-w-d-w,"You would likely want to setup an insert/update trigger to fill out the field rather than doing it once with an update.  This is the basic layout for triggers like this:

    DROP TRIGGER IF EXISTS tablename_identifier ON tablename CASCADE;
    
    CREATE OR REPLACE FUNCTION tablename_identifier_insert_update() RETURNS trigger AS $BODY$
    BEGIN
        if NEW.identifier is null then
            NEW.identifier := lower(regexp_replace(NEW.firstmiddlename||NEW.lastname, E'\\s+', '','g'));
        end if;
    RETURN NEW;
    end
    $BODY$
     LANGUAGE 'plpgsql';
    
    CREATE TRIGGER tablename_identifier
        BEFORE INSERT OR UPDATE ON tablename FOR EACH ROW
        EXECUTE PROCEDURE tablename_identifier_insert_update();
    
(Feel free to take that null check out, I just think it's a safe assumption that if the query is providing an identifier then you want to trust it).  ",1529520888.0
jabiko,"My guess is that the query crashes the process on the server.

You should check if the server logfile contains additional information.",1529497139.0
kellenkyros,"Yes, it should be the query. Please check the PG logs.",1529497876.0
Cer_Visia,"There is some difference between these tables.

    SELECT * FROM crystal_ball('LopsidedOrdinary''s table definitions');
    ERROR:  function crystal_ball(unknown) does not exist",1529484421.0
Engival,"Wait a sec, can you post an example where the permission is granted automatically? I've never actually seen that.

The normal process:

* Create table ... something BIGSERIAL ...
* Table is now created, default permissions (my superuser)
* Sequence is now created, default permissions (superuser)
* GRANT permission on table (application user, restricted access)
* Table now has user access, but sequence does not know to magically update itself.
* GRANT permission on sequence (application user)
* All is well in the world

Although, a quick google search shows you can change your default grant permissions for your current login role: https://stackoverflow.com/questions/19309416/grant-permissions-to-user-for-any-new-tables-created-in-postgresql

(I personally don't like to have the DB do automatic things like that. My schema script should contain all the information needed to know what will appear in the final DB)",1529497546.0
thedward,"Not exactly what you are asking for, but relevant: [PostgREST](https://github.com/PostgREST/postgrest).",1529424844.0
,"What do you mean using only?
What type of application?

You aren't going to be able to build a website with just a query language. You aren't going to be able to build an android app with just a query language.

Not sure what exactly you are proposing, so not sure how we are meant to help you ",1529425623.0
chock-a-block,Sure it's possible.  It depends on the problem you are trying to solve.,1529435158.0
spinur1848,"With a few of the extensions and nginx, possibly. Thats kind of like trying to serve a 10 course meal with just a spoon though.",1529449658.0
johnfrazer783,"There are some people out there who are trying to do just this: https://github.com/aquametalabs/aquameta. Also there is https://github.com/loveencounterflow/intershop. Both projects are somewhat unfinished and will require some effort to get started with, but it has been done and I think it can be done. 

The intershop project has some code to read files into cache tables: https://github.com/loveencounterflow/intershop/blob/master/db/060-mirage.sql; this uses `plpython3u`. Without the use of an unencumbered language like this it is not possible to start a web server or read/write from/to the file system. ",1529489753.0
awegge,"Range types are cool. I once fixed a control system where the naive lookup with min and max in three distinct intervals took more than 5 seconds. That was no optimal, as it was supposed to sort postal packages  at a peak rate of 30k/hr. Using custom designed range types made the average lookup time drop to 30-40 ms. 

",1529417072.0
cyberst0rm,"pg_cron and plsh look like fun admin tooling and could make a fully automated platform, if I ever get the time to fully implement ",1529466435.0
softwareguy74,Good stuff.  Thanks!,1529938953.0
cyberst0rm,"I think the standard PG 10 works following this document for logical replication:

https://www.postgresql.org/docs/10/static/logical-replication.html

I've also installed pg_cron:

https://github.com/citusdata/pg_cron

Which lets me run scheduled backups. I use plsh to make it easier to construct foul-proof backup scripts:

https://github.com/petere/plsh
",1529353454.0
chock-a-block,">AWS webs servers can do load balancing between them

Is there so much traffic a load balancer is required?  

>I'd prefer if all the instances could take writes

This makes it very much more complicated.  I'd avoid this for now.

I'd probably start with master-slave replication on AWS.  I'd push table changes out to your AWS instance.  One way is to use a trigger like this: https://dba.stackexchange.com/questions/158022/call-dblink-from-a-trigger-function

Just remember you need a trigger for insert, delete, update.",1529436076.0
dopperpod,"I'm not sure why you're using all those unnecessary double quotes, but it makes it harder to read. Also, use a simple query formatter for more easy-to-read and standardized queries.

    SELECT Id
    	,Time1
    	,Time2
    FROM Process
    WHERE Amount1 = 'Normal'
        OR Amount2 = 'Normal';",1529337713.0
zieziegabor,"It depends on what you mean by anonymized, and how anonymous do you need it?

For instance, if you replace every user-ID 'Rene_408' with 'ANON1234', then sure they can't tie ANON1234 -> Rene_408, but perhaps they know enough about the DB that they can infer that ANON1234 = The person Rene (i.e. by street address or phone number or purchases made or something).  So They may not know it's Rene_408, but they know it's you anyway.

The best thing is to replace everything with totally random values.  This however makes it very hard to use.  i.e. you wouldn't do a map of Rene_408 -> ANON1234, instead, for every place you see Rene_408, you would replace it with a totally new random value.

So it really depends on the use case of the ""anonymized"" data.  If you are planning on making it public, then you best get someone with serious experience anonymizing data for public consumption before you do such a thing.. Or better yet, just create a whole new DB with random entries.",1529335349.0
admiralspark,"I'm curious for the use case of a **relational database** using **anonymous** value maps.

Do you by chance mean encrypted instead?",1529345883.0
swenty,"How are you distributing the anonymized data? If the customer will have access to the DB's filesystem, you will have to worry about cleaning out identifying data from the write ahead log and other system records. That may not be easy to do. If instead you are providing a logical copy of the data, such as the output of pg_dump, you need only worry about anonymizing the contents of the included tables.",1529338171.0
analfabeetti,"Check your logs under /var/log/postgresql.

And that systemd service is probably the one that just runs real services matching different postgresql clusters - something like systemctl status postgresql@9.6-main probably would reveal more - run pg_lsclusters to see correct version and name of the cluster.

If  this is a Debian / Ubuntu, other distributions may vary.
",1529303759.0
Darkmere,`journalctl -u postgresql `  ,1529304447.0
v_jingqiang_zhang,"After upgrade PostgreSQL to 10, it works.",1529308938.0
adila01,"Wow, this looks really great! Good job!",1529259898.0
iiiinthecomputer,"Hint, avoid using Qt's SQL classes for it. They're super primitive and you can't do much useful with them especially for something like an admin tool. Total lack of useful access to error information is a big problem.

Use libpq. ",1529284669.0
chock-a-block,"Current ""Linux"" way to do things is to write in a dot-file in the user's home directory.  Something like a file named .sql-sessions.   If you want to store passwords, warn the user!

Forewarning, there is very likely documentation at freedesktop.org that will give you a better idea of the ""right way"" to write the user data file.

Also, maybe you want to use QT's database connectors to get more than just Postgres working?",1529298026.0
mispp1,"I just opened an account and mistyped the password. Without email. 
This is new account since I can't reset the pass on the '0'.",1529256508.0
a1sher,"Remove the comma from this line -  `END AS ""Process.time 2"",` **<---**",1529233660.0
Dj24680,thank you ,1529236373.0
thelindsay,"I've been working on a project with DDL generation and have pondered this too. Since table vs. column is functionally equivalent, it's been preferable so far to go with the table-level version in order to have only one code path for constraints. The add/alter/drop/validate syntax acts at the table-level so it made sense from that perspective too.",1529247851.0
GFandango,"It's a good question. I don't know either.

I'm jealous of people like web designers who can put shiny portfolios up on a website to clearly demonstrate their skills.",1529098347.0
otheranotherx,I had a nice postgresql exercise when I was using this: https://github.com/PostgREST/postgrest to create a restful API. There is also something for the graphQL people:  https://github.com/graphile/postgraphile ,1529140994.0
chunkyks,"[https://github.com/Abstrct/Schemaverse](https://github.com/Abstrct/Schemaverse)

""What is Schemaverse? The Schemaverse is a space-based strategy game implemented entirely within a PostgreSQL database. Compete against other players using raw SQL commands to command your fleet. Or, if your PL/pgSQL-foo is strong, wield it to write AI and have your fleet command itself!""",1529101828.0
cyberst0rm,Create a replica system using  torrents,1529114746.0
perkules,"I think it it might be difficult to find postgres specific projects to elevate your skills unless you want to get into the really very specific postgres stuff like how postgres works on the backend side of things. Maybe it would be better to look into PL/pgSQL stuff? Perhaps getting into what types of indexes would suit what kind of dataset etc.?

Personally i think datawarehousing and star schema is a really good subject to get into (and data vault as well), but that is more database modeling and not postgres specific per se. ",1529102670.0
hardwaresofton,"Here's one that I've seriously been considering doing for a laugh (and blogging about, the plan was to call it ""why not postgres""): Rewrite a bunch of major database types with Postgres (with or without use/inspiration from plugins.

- KV Store (partial/naive redis)
- A Graph database (partial/naive neo4j)
- A time series data base (check out [timescaledb](http://docs.timescale.com) for inspiration)
- A persistent log (partial/naive kafka)
- Etc.

Here's the even more interesting part -- compare your naive approach to the actual polished products it's emulating and see how close you can get on performance. It will definitely teach you a lot about certain parts of Postgres internals -- like I assume you'll get to learn all the warts of table partitioning in great detail while trying to get the kafka clone working performantly.",1529117470.0
9loabl,"How normalised are your existing projects. Are they normalised on the database or through the backend code (python or whatever) using that code.

Have you ran explain commands to see how optimised your queries are? That's what will matter when you will be looking for a job.",1529147796.0
cbunn81,">I already know how to create a db, schema, tables, delete, as well as query data with SQL.

This sounds quite vague as those are the basic functions one does with SQL.   


As I see it, you have three things to study:

1. Database design
2. SQL (perhaps including PL/pgSQL)
3. Database administration

You don't need to study all of those, of course, depending on your goals. But that might give you an idea of where to go next.  
",1529148147.0
tobbeben,"I’m working on a pure postgres database app for bookkeeping (similar to Ledger). If you want to help out, send me a PM and I can publish/open source it",1529149488.0
mw44118,"The full text search stuff is pretty straightforward once you get into it, and it can be really useful!",1529164403.0
0x2a,"Have you tried anything already? Otherwise start with

    CREATE INDEX cache_bool_idx ON cache (bool_a, bool_b);

Also, the best way to look at this stuff is to `EXPLAIN ANALYZE` and then paste the output into https://explain.depesz.com/, it will nicely show you all the slow bits.",1529058969.0
simcitymayor,"As several have pointed out, indexes are worth their weight only in cases where the rows found are sufficiently rare. Consensus seems to be that ""rare"" is 15&#37; or less, and by definition, this index would never be more than 25&#37; selective.

If the rest of your app allows for it, I would suggest that partitioning might be your answer, because then you'd only be seq-scanning one of the four partitions.  


That may be too much effort just to save one query though.",1529076316.0
rouen_sk,"indexes dont really make sense in low-variety data. if you have only 4 different values, it probably make more sense to seq-scan table, than to make huge amount of ""random seeks"" after using index. That is why PG decided to use seq.scan.
You can try to use BRIN index, and I would also try to cluster the table by index based on these columns (if these select is your main use case of course) so that seq-scan basically have zero overhead.",1529062142.0
vittore29,What are other conditions you are using the most? Seems you need covering index with those two bit columns included. ,1529041385.0
Siltala,"Say the value is a\01\0\0b\02\0\0

Wouldn't that search return true when trying to find rows where the value of a is 2?",1528992455.0
torkild,"I think it would depend on how many key value pairs are in each record. As that value grows I would expect hstore to be faster, but I don't know for sure",1528993302.0
thelindsay,"The PuTTY setup is more realistic, since it'd be better to use a tunneled connection than have the database port open, if you ever were doing remote admin (for whatever reason).

That said, to connect directly you'd need to change postgres.conf listen addresses to '*' to allow non-localhost connections, restart Postgres, make sure 5432 is open in your VM firewall, and possibly also do a port mapping in VirtualBox if you want to avoid even specifying the VM IP in your connection.",1528945701.0
QuantumRiff,"do you need pgadmin4? or do you need a gui to run your queries?

I really like dbeaver. it has ssh tunneling built in, but I have not used it.",1528985935.0
jakkarth,Postgres is a relational database. It does not contain files or folders. It contains relations and rows. Why would you expect to see python scripts inside of a database?,1528934419.0
jeffdn,"You need to transfer the data to the new database. You could write a script using a language like Python or Ruby, or really any language. Alternatively, you could just dump the data to a CSV and load it back if it’s only one table.

Does the SQLite database get checked into your Git repository, or is it stored on the server?",1528940971.0
thelindsay,"By using ""SET work_mem = 'xMB';"" at the start of a transaction, its possible to tune the default up or down based on the complexity of the query being executed. Much easier than trying to find an adequately conservative one-size-fits-all value.",1528865375.0
fr0z3nph03n1x,Has anyone been to this? How is it?,1528912430.0
RealityTimeshare,"You can have access connect to your postgresql server through an ODBC connection. You can keep using access to maintain what you have, other than updating the sources of the various and sundry tables/queries, without having to give your users a set of unfamiliar tools.",1528756842.0
spinur1848,"We've used the following in certain circumstances:

Adminer: https://www.adminer.org/en/
Omnidb: https://omnidb.org/en/",1528803776.0
jrwren,point django at it and use the django admin interface?,1528815762.0
leamanc,"You could build your own web forms, using PHP or something along those lines to interact with Postgres. That’s free if you know how to do it. 

This could be a stressful undertaking. Why are you wanting to move? I prefer Postgres to Access also, but if it works for your users, why fix it?",1528756921.0
simcitymayor,"It seems more efficient because it is. You're using \copy (and COPY) for what it was designed for. Writing your own while-read-csv-line-do-insert-into-db-check-for-error-ok-goto-next-oh-wait-no-more-rows-kthxbai loop is tedious and error prone.

Gotchas: 

* \copy either loads all the rows, or none of them. One error spoils your whole batch.

* The error messages are sometimes helpful ("" 'xyz' is not a number, row 31, etc) and sometimes you get specifics down to the row and column, but sometimes your input is so messed up that your concept of what's a row doesn't match what COPY found.

* \COPY references the local filesystem (and local shell environment when using FROM PROGRAM) using your client's permissions, so if you're writing production code you better make sure the production machine matches your dev box.

* COPY (the SQL command) references the _server_ filesystem, using postgresql's permissions, so the installed environment may not (read: probably doesn't) match your local environment.",1528740769.0
johnfrazer783,"There's one annyoing property of `\copy` that made me stop using it, and that's the unconditional replacement of backslash literals. This means e.g. that legal JSON strings like `{!a"":""foo\nbar""}` get rendered as two lines, rendering them malformed. ISTR that this also happens to `\xNN` escapes which can be ultra-annoying. ",1528752294.0
boy_named_su,"use CSV format, even if importing TSVs (text format can be weird w special characters)

I usually have some sort of character encoding error

\copy does append, it doesn't overwrite

\copy is still affected by triggers, constraints. You can drop those and re-create them to speed things up",1528823004.0
gryba,"I may not have fully understood what you’re trying to achieve, but feels like you’re looking for array_to_string function",1528729184.0
gryba,‘Array_to_string’ - with underscores around “to”,1528729229.0
jk3us,"something like this

    WITH cte as (
        SELECT partnerslistori as a FROM sales.members WHERE id = '"".$id.""'
    )
    SELECT 
        u.id as actualid,
        (SELECT m.company || ' (' || m.id ||')' FROM support.members m WHERE m.id = u.id) AS actualcompany,
        u.itemname,
        DATE_PART('day', CURRENT_TIMESTAMP - u.datein::timestamp) AS daysinstock, 
        TRIM(u.grade)::character varying as condition, 
        sting_agg(u.vstockno, ' ') AS stock,  -- Aggregate stock numbers into a single string
        u.hol AS ic, 
        CASE WHEN u.rprice > 0 THEN 
            u.rprice 
        ELSE 
            NULL 
        END AS price, 
        sum(u.quantity),   -- sum the quantities
        string_agg(u.location, ' ')
    FROM public.net u 
    WHERE u.holl in ("".$ic."")
      AND visibledate <= now() 
      AND u.id = any(regexp_split_to_array('"".$id."",'||(select a from cte), ','))
    GROUP BY u.id, u.itemname, u.datein, u.grade, u.hol, u.rprice; -- this groups the rows where all of the values from these columns are equal


There may be mistakes, I don't have your db to run it against.  This looks like a decent overview of grouping and aggregates: http://www.postgresqltutorial.com/postgresql-group-by/",1528731003.0
MrBaseball77,I have made modifications to the original post. Now need help speeding it up.,1528742825.0
merlinm,"\> When I query the ccvalues table, it takes an unusually long time to run (10s) and comes back w/ 0 records on a count(\*)

Try VACUUM FULL or CLUSTER on the ccvalue table.  It's probably packed with dead records.",1528718835.0
dGhleSBoYXZlIG5vIHdv,Look at `\dtm+` to see sizes and also include materialized views in case you have any. Also `\l+` to see DB sizes as a starting point. Vacuum full too and see it that helps.,1528710170.0
jmswlms,"Your transaction could see only 0 records in the table, but it is possible that there are still open transactions that have inserted/updated/deleted rows from the table. See the [pg_stat_activity](https://www.postgresql.org/docs/current/static/monitoring-stats.html#PG-STAT-ACTIVITY-VIEW) system catalog view to see all running transactions.

If there are no interesting backends listed, try [vacuuming](https://www.postgresql.org/docs/current/static/routine-vacuuming.html).",1528779398.0
kevin____,Try dumping the database and reloading it and seeing if the size is different.,1528686216.0
depesz,"Inside the database you can:

    select oid::regclass, relkind, pg_table_size(oid)
    from pg_class
    where relkind in ('r', 'i')
    order by 3 desc;

To see what tables (relkind == 'r') or indexes (relkind == 'i') are using the most disk.",1528710589.0
Darkmere," Churn.

If you have changes ( deletes, removes, updates) in a table, the files containing the data is expanded to fit.  When you remove data, it's marked as ""deleted"" and unable to be used by the DB internally until it's been marked as ""cleaned"" by the VACUUM process.

VACUUM runs in the background and marks stale objects in the DB files as available, but DOESNT return them to the operating system.


This means that even when VACUUM is run, your database may contain ""bloat"", deleted areas that can be used by new objects, but aren't used right now.

This data can only* be cleared by the `VACUUM FULL` or the `CLUSTER` commands, or by doing a `dump and restore` style operation.

*) Not really only, if you empty the table (truncate) then run vacuum it can return the data, there's a few other special cases, but generally not.

Most of the time, in production, this doesn't _matter_ since the tables may grow or shrink and if it's used that data in the past, it might use it again.  Sometimes though, this really matters and a table may contain tens or even hundreds of gigabytes of bloat.  

Quick reference on:

* CLUSTER,  locks a table completely, then re-writes it in a new order, based on an index, used to lay out data on-disk according to set preference by an index.  Requires 2x-3x the original space (1x for original, 1x for the temp data, 1x for the final data on disk)  and re-calculates indexes and everything.

* VACUUM FULL: completely locks a table, internally copies all the data to another one, and purges the old ones. Also requires a bit more than 2x the space of the original table.

",1528808319.0
mokadillion,Is the size the actual size of your base directory and not just the objects within it ? ,1528671240.0
boy_named_su,Liquibase or flyway,1528571102.0
jakkarth,How about just writing SQL statements?,1528570347.0
iTroll,Sqitch is pretty good.,1528641999.0
thelindsay,"https://pyrseas.readthedocs.io/en/latest/ is a  python project that lets you write your database as yaml, and by comparing to the current state of the database figures out the difference / what needs to change.

In other words if you intended to write things like ""if table x not exists then create table x else exit"" then Pyrseas is taking care of the bulk of it so you can just think about requiring ""table x"". 

Its a different strategy to many migration tools, which typically define some unique ID for each set of database changes and record the ID and timestamp when applied, thereby relying on those records rather than what the database actually contains.",1528657792.0
depesz,"I wrote, and use, versioning system, called imaginatively [Versioning](https://gitlab.com/depesz/Versioning).

You can read more about it in the [blog post](https://www.depesz.com/2010/08/22/versioning/) that I wrote to explain how to use it.

It is a pure-sql solution, so you don't have to install anything to use.",1528710510.0
cyberst0rm,Love this thing. Needs a node client though for maximum ease,1528587870.0
neofreeman,Can’t wait for this to graduate,1528629982.0
r0ck0,"Had my eye on this for a while, and looks interesting.  But I noticed on https://www.graphile.org/postgraphile/pricing/ that it seems that you need to pay to:

* Enforce limits on pagination
* Limit depth of GraphQL queries	

Does this mean that unless you pay for the commercial version, anyone can just run unlimited queries on your database?  As in if you had 1 million rows in a publically accessible table via graphql, anyone can just request the whole table of data?",1528659483.0
koreth,"If you can get a single VPS for $40/month and you need failover, then you're looking at spending (at least) $80/month to run both the primary and the standby/replica instances. RDS may end up being the lower-cost option.",1528531037.0
ypsthelove,"If you plan to run self-hosted RDS, I'd recommend to use AWS/GCP instead of VPS, because VPS is more expensive on Ram. A ec2 r4.large (2cpu+15G), cost $42/mo(RI). DigitalOcean 16G vps is $80/mo.
I'd also recommend to setup a logical rep on a nano instance for real time backup.
",1528578434.0
2strokes4lyfe,psql,1528431824.0
boy_named_su,"PGAdmin III (4 is shit)

or SQL Workbench (Java)",1528474615.0
dotjosh,Datagrip is incredible (not free),1528679548.0
-markusb-,"Just from mobile. Keep in mind single connections are just using single threads. Faster, fewer cores can be better. Depends on your application. ",1528465787.0
cybernd,"Ask your self: do you have many small queries or some long runners. If it is the first, you are most probably not cpu bound. If there the long runners are one of your issues, focus on single core performance.

In case of single cpu performance i would look at xeon w or use a 8700. The xeon equivalent of the 8700 should be the rumored E-2176G, but it is still unkown when they will get released.

Be cautios with attempting to interprete cpu load graphs. They will not tell you if the core was saturated by a cascade of small queries or just by one bigger quier. In the first case an additonal core would have already reduced the pain, but if its the 2nd case an additional core would change nothing.",1528474081.0
ants_a,Pick one of the ones that support ECC memory. Xeon or Ryzen. It is much cheaper than hiring somebody like me to piece together a corrupted database just because a ram stick decided to crap out and silently corrupt your database and backups.,1528884487.0
boy_named_su,more cores is better,1528475126.0
antlife,The fact this is using Rust has me already interested.,1528404704.0
davbeer,Looks really impressive! How was your experience developing with Elm?,1528485263.0
batisteo,"It looks like less pgAdmin and more like the Django admin, right?",1528489480.0
stickman393,"Not an answer to your question, but I've been enjoying dBeaver Community Edition for the last week. Check it out if you just need an IDE and not full management.",1528343993.0
chock-a-block,"To be perfectly frank, there is no difference between what you see in the browser and the old standalone app.  That's pretty amazing.

If you must have something that is NOT exactly like the desktop client not wrapped in a browser, then Squirrel SQL is great.",1528346053.0
eugenekropotkin,"In linux, PgAdmin3 was fixed some time ago, now it works fine with Postgres 10Good PgAdmin3 version is 1.22.2  


P.S. Also, there is some alternatives exists, like OmniDB  
[https://www.slant.co/options/208/alternatives/\~pgadmin\-4\-alternatives](https://www.slant.co/options/208/alternatives/~pgadmin-4-alternatives)",1528353671.0
boy_named_su,"PGAdmin III works great

SQL Workbench works too. It's Java, so not ""native"". Used to be MySQL only, but is now generic",1528391066.0
thelindsay,"I came across an extension for time travel functionality in Postgres contrib extensions, I guess maybe it doesn't cover the standard but it's more than nothing: https://www.postgresql.org/docs/current/static/contrib-spi.html

The row matching looks really interesting, its like a whole new sub-language. As confusing / powerful as regex. But since the window version is not too much longer I wonder if it'll be a priority.",1528314456.0
jmswlms,"You probably also want [pg_stat_statements](https://www.postgresql.org/docs/current/static/pgstatstatements.html), for query-level stats. [pgmetrics](https://pgmetrics.io) might also be helpful if you're not looking to write queries to fetch the stats yourself.",1528274934.0
TheSqlAdmin,"Yes, This will help, But you cannot track on a particular query (eg: you cannot track a query runs in 2sec today and we can't get how much time it took yesterday). But in general you can get an aggregated results of all the queries. 

select \* from tbl where a=1;

select \* from tbl where a=2;

Both are considered as a same query in the stats. 

Also there are so many options are there like most read intensive table, write intensive table, delete intensive tables and also you can track the IO of the queries. 

pg\_stats\_statements will give results about the queries, but if you want to track table level, you can take a look on  

    pg_stat_user_tables",1528280248.0
j03,One alternative option would be to enable query logging on the server and parse the log files,1528281197.0
eugenekropotkin,"You can try PgBadger \- it can help to find bottleneck, long queries and so on  


[https://github.com/dalibo/pgbadger](https://github.com/dalibo/pgbadger)",1528355325.0
cybernd,"Switch to dbeaver. When you are used to it, there is no way to go back to pgadmin 4.",1528226154.0
myurr,"I find pgadmin 4 to still be the best way to navigate the structure of the DB on the desktop, but the query tool is dysfunctional at best.  Postico is a much better query tool so I tend to use that in preference.",1528214059.0
leamanc,"Having done a few discography databases myself, I find it worth the time to make separate tables, with primary key IDs, for artists and releases. It may seem like a lot of work at this point, but if you start by doing a SELECT DISTINCT on your artists column, you’ll have the start of your artists table.",1528057296.0
arrelgray,"I think what you’re looking for is a GIN index:

“GIN stands for Generalized Inverted Index. GIN is designed for handling cases where the items to be indexed are composite values, and the queries to be handled by the index need to search for element values that appear within the composite items.”

https://www.postgresql.org/docs/9.1/static/gin-intro.html",1528061787.0
anras,"Have you tried indexing on the expression string_to_array(artist, ',')? That may help. [Expression indexes](https://www.postgresql.org/docs/current/static/indexes-expressional.html)  
  
That said I agree with /u/leamanc that a more normalized structure might be better.",1528057615.0
cbunn81,">artist IN \('Queen'\)

That won't work because you're searching for the entire ""artist"" array in the string literal ""Queen"". You need to do it the other way around, using the ANY keyword:

`SELECT * FROM song WHERE 'Queen' = ANY (artist);`",1528057527.0
,[deleted],1528068487.0
guru42101,"Two methods I see. 

1), if you can create a materialized view or proper table of the data you can use it instead. E.g copy the data into a new SONG_2 table where Under Pressure has two rows, one with Queen and the other with David Bowie.

2) The likely more correct choice. Create a Text Search index over Artist. I've done it previously for research data where almost all of it was freely entered by users and someone may enter the researcher and some topic which was stored in separate columns in my DB. https://www.postgresql.org/docs/9.5/static/textsearch.html

If you just want the index on the artist, then you can simply do:

    CREATE INDEX song_idx ON song USING GIN (to_tsvector('english', artist));

This will allow someone to search for Queen, Bowie, David and get the song, and others. If they search on Queen Bowie or David Queen. They'd get Under Pressure. 

If you want to index all of the columns in SONG. I would recommend creating an vector table. I would also recommend adding an ID and LAST_UPDATE column to the SONG table with ID as the primary key and LAST_UPDATE indexed. After each import insert or update (I forget if merge exists in Postgres) into the vector table any records that were updated. 

(Oracle like merge syntax, because it is what I've been using lately)

    merge into song_vector
    using (
        select 
            id, 
            to_tsvector('english', coalesce(artist, ' ') || ' ' || coalesce(title, ' ') || ' ' || coalesce(release, ' ') || ' ' || coalesce(type, ' ')) as vector, 
            last_update
        ) new_data on (new_data.id = song_vector.id)
    when not matched then 
        insert  
            (id, vector, last_update) values (new_data.id, new_data.vector, new_data.last_update)
    when matched then 
        update
            id = new_data.id,
            vector = new_data.vector,
            last_update = new_data.last_update
        where last_update < new_data.last_update;

This is assuming that not all songs are updated every time. If it is a complete reimport every time, you can just do all of them.  

The song_vector can then be used to search against and joined on the other tables. So if someone searches Queen 1981 they'll get Under Pressure with Bowie. If they search the original release date they'll get the original release. If they search Queen Bowie they'll get the 1981 release, same with Queen David. If someone simply searches 1979 they'd get all songs released in 1979 as well as the Smashing Pumpkins song 1979. 
",1528090886.0
sticknman,"how about LIKE. 

'Queen,David Bowie' LIKE %Queen%
true",1528050497.0
-markusb-,"I think you missed setting hot_standby to on. So the WAL is applied, but a login is not possible. ",1528010688.0
jeffdn,"These can be awesome tools, but a word of caution — it can be tempting to use a chain of materialized views to create a data derivative pipeline, where each set of results is useful to multiple derivatives. This is awesome until you need to change one — you must then drop and recreate all dependent materialized views, and it becomes an enormous pain in the ass, even with great migration tools.",1527954953.0
thelindsay,"> The aggregate table is used by the API, which means that there are read access locks on this table. If lock, then wait for insertion… and in this case, wait for a while. Plus this lock accumulation generates some performance issues.

This paragraph doesn't seem consistent with the docs - normal DML locks don't conflict with each other.",1527966526.0
i-w-d-w,"Another useful application for materialized views not mentioned is caching dblink queries on the consuming host.  For certain classes of central configuration data that can change over time, but not between hosts, they make maintaining that go much smoother.  ",1528079727.0
Shananra,Has there been much progress on those automatically updating materialized views? It's one of the few things I miss from oracle.,1528351276.0
flyingmayo,"I would suggest you start by removing variables.

Are you able to make valid SSL connections to the master from a psql \(or some other non\-dblink\) client?",1527796711.0
DoomPirate1,"Solved it. It turns out I needed to add the full trust chain in the cert file on my ""Server"".

I used the comodo 90 day cert instead of the lets encrypt cert.

Heres now I laid out the server.crt on the ""Server""

The freshly issued comodo cert ontop.

The 2 comodo intermediate certs.

The root comodo cert

I used this tool to check my chaining. [https://tools.keycdn.com/ssl](https://tools.keycdn.com/ssl)",1527867294.0
QuantumRiff,"Honestly, 'rolling your own' monitoring never seems to work well. I setup Prometheus for my systems, along with postgres\-exporter.  Among other things, it will alert if the db is not 'up' because the exporter can't reach it.",1527794170.0
smugbug23,"The best way is probably to hook into something you already have.  There are plenty of monitoring solutions out there, many of which will have plugins for PostgreSQL, but they won't bill themselves specifically as PostgreSQL monitoring as they are general purpose monitoring tools.

I just used cron jobs which would email if they couldn't connect.  Because that is what I had on hand, already configured to send emails upon failure for other cron jobs.  It was easy to extend them to this purpose as well.  They didn't batch-up alerts, there was no snooze feature, it wouldn't use SMS or escalate to a different email.  But I didn't need those things.",1527955256.0
depesz,"Well, if it's sql text file, and it contains ""create trigger"" and other things that you don't want - you just have to edit the file to remove/comment-out what you don't need/want.",1527761296.0
skeletal88,"By identity columns you mean primary key fields created with the serial type?

If you look at the table definition in your sql server then you will see something like this 

    CREATE TABLE public.foo
    (
      id integer NOT NULL DEFAULT nextval('foo_id_seq'::regclass),
      bar integer
    );

The ""DEFAULT nextval('foo_id_seq'::regclass)"" means that when no value is present then it will generate a new id from the sequence foo_id_seq.
If your ORM lets ju set the id of your objects then postgresql will accept them and save them to the database.",1527760938.0
pilif,"The values of an identity column are only set by virtue of their default value. There's absolutely nothing stopping you from inserting your own values into columns of type `SERIAL`.

However, inserting your own values will not cause the underlying sequence to be incremented, so it might be producing IDs you've already set manually.",1527765882.0
therealgaxbo,"I'm not aware of a global setting, but as this is just for test data, you could temporarily `alter table xxx alter column yyy set generated by default` before the data generation then `alter table xxx alter column yyy set generated always` afterwards to reset it?",1527759718.0
jakkarth,"Are you sure you're passing eg -12 there, and not just - by itself? Because that error looks like you're passing - by itself, which isn't a valid number.",1527712589.0
urcadox,"Default port is 5432, are you sure your server is configured to use 5433?",1527667835.0
chock-a-block,"We need a little more information.

What installer did you use?  Did you change any of the defaults?

Can you *telnet -6 ::1 5432*  How about the same test, ipv4?

What's in the postgresql start up log? 

Maybe Windows' privileges system is preventing listening? That's not the same thing as their firewall.",1527709519.0
OneOlCrustySock,iOS keyboard has `backticks`.,1527657346.0
elmicha,"> And all I know is the left inner join, not the right inner join, outer joins, etc. Learn one well before you learn two

Where can I read more about ""left inner joins"" vs. ""right inner joins""? I know about left and right outer joins, but I didn't know that inner joins could be left or right. I guess the author is confused.",1527658219.0
zieziegabor,"There is also https://postgrest.org/en/v4.4/index.html
Which you could then beat up from JS directly as well, without a special server component, if you so wanted.",1527641664.0
boy_named_su,"You could use PostgREST for a nice restful api. It's written in Haskell though but it's mostly just configuration.  https://github.com/PostgREST/postgrest/blob/master/README.md

Otherwise you'll need a nodejs server

You can even write stored functions in JavaScript! (Plv8)

And JSONB is nice for fast json storage",1527643771.0
GFandango,"they are pretty much completely unrelated.

you have to get the data from the server into the browser then make it into charts with javascript.

it doesn't matter where the data comes from.

you can take care of chart stuff first without database with some fake JSON data.

once the data stuff is in place you can replace the fake data with an AJAX query to get real data from db.

it could be postgres or anything else.",1527645552.0
EvanCarroll,"You have to create a server side interface into PostgreSQL using something like Koa or Express and Node.JS. Then you have to query that endpoint to get the data into the browser. Then you have to render that data using something like D3.js which can be tricky if you don't know how: check out one of the D3.JS front-ends or tools built atop it, like nvd3.",1527640747.0
Tostino,"I've had plenty of bad experience with PGAgent., Much of it similar to yours. I eventually got fed up and created a replacement agent which still works with the PGAgent schema and UI tools.

I'm on mobile and can't link ATM, but it's called jpgAgent. Just Google it and you should see the GitHub page.

I've been using it in production for a few years now. It's been Rock Solid.

If you have any questions or need help if you decide to go that direction, I'll be happy to help in any way I can.",1527639347.0
SomeGuyNamedPaul,"Numeric is straight up slower than integers because if the internal operations needed for how to represent and compare it.  Use it only if your need and an int won't do it.  Floats are imprecise.  **Never** use a float to store monetary values or anything where you're not ok with the value returned being approximately correct versus the value stored.  Use it for real world sensor data and probably that's it.

Int and bigint is what you should be using to store integer data.  You can use unsigned integer to store numbers larger than 2 billion and change, but if it's even close then you should be using bigint instead.",1527535288.0
EvanCarroll,"* bigint is fixed at 8 bytes.
* decimal is an alias to numeric
* numeric is variable-width which means it *may* store to the same size, but it *may* not be as compact in memory and overflow-expansion logic is likely to be slower.
* double precision is 8 bytes too, but it's float. However, keep in mind floats do interger math up until MAX_INT. So at best, it's not any worse, but if you don't need it you're better off using bigint.

Alas, using int if you can and it's safe is always the best idea.",1527531209.0
punpunpun,"Since they are all going to the same table, here's a shortcut from the bash prompt:

cat *.csv |grep -v '^#'|sort|uniq > merged_table_name.csv

Then start psql and run the \copy command:

> \copy table_name from 'merged_table_name.csv' csv;

This is assuming that if you have header lines in the CSV files, they start with # and you don't need them because the CSV has the exact same number of columns, and in the same order, as the table.",1527519962.0
colloidalthoughts,"That script is to be run at the Mac prompt, in bash, not inside the postgres cli. It's a shell script.",1527519542.0
shobble,"that 'script' isn't intended to be run in the psql shell, it's intended to be run in your system shell (probably `/bin/bash`, the prompt you get when you first open Terminal.app before running `psql WHATEVER`.)

You'll need to replace the `<folder_name>` and <table_name>` bits of it with things appropriate to your specific needs.",1527519719.0
ypsthelove,This is completely an advertisement. We don't have any rule here? OP is using this sub. ,1527497154.0
graycube,"You can put Cayley on top of PostgreSQL:  https://github.com/cayleygraph/cayley

Most of the features I've seen itemized for PG11 are continued improvements for partitioning tables and parallelization.  A quick glance through the 11 features and open items:  https://wiki.postgresql.org/wiki/PostgreSQL_Open_Items

Doesn't show it has having been done.

On a side note:

There is a movement towards developing a common graph query language - if there is uncertainty about which query language would work best I'd think any new graph project would be following this very closely: https://gql.today/  
",1527415542.0
smugbug23,"That is the road map of one particular company which sometimes contributes features to PostgreSQL.  It is **not** the official roadmap of the project itself.  Consider it an informal wish list.  It is a wiki, afterall.",1527515799.0
raevnos,"If it's not listed in the documentation, it probably doesn't.",1527397778.0
reallyserious,"> Implement Cypher and/or Gremlin as the query language through UDFs. 

That's huge! A proper graph query language with the stability and maturity of postgres. ",1527412392.0
linuxhiker,You may want to look at Bitnine.net,1528241587.0
simcitymayor,"```
$ cat if_prompt.sql 
\prompt 'enter abc here: ' x
select ('abc' = :'x') as is_abc \gset
\if :is_abc
\echo you can read
\else
\echo you are illiterate or impertinent
\endif

$ psql test -f if_prompt.sql 
enter abc here: abc
you can read

$ psql test -f if_prompt.sql 
enter abc here: 123
you are illiterate or impertinent
```
",1527285354.0
merlinm,"hm, this really ought to be possible.",1527251708.0
dcalde,"Checkout https://www.postgresql.org/docs/9.6/static/queries-with.html#QUERIES-WITH-SELECT
and https://www.postgresql.org/docs/9.6/static/functions-json.html

You will also need custom function to build the final json document. pg 9.6 do have pretty good json manipulating functions, but it would make the task unnecessarily messy.

Also, a bit of googling will find examples of other doing the same /similar.
e.g. https://lakshminp.com/building-nested-hierarchy-json-relational-db

",1527250262.0
JavaJuke,I'd recommend doing this at the application level as SQL isn't the best for constructing trees. Substantial data transformation should be done at the application level.,1527258806.0
swenty,"My first thought was to do this using a recursive common table expression. One could start with the leaf nodes, replacing a group of child comments with its parent, rolling up the children into a JSON field in the parent, until you have only top-level comments. That plan falls apart though, as aggregate functions aren't allowed in the recursive part of a WITH RECURSIVE statement.

But we can get the desired result instead by using a recursive function call (or two):

The comment_tree() function recurses over all of the descendants of a comment node, building up JSON objects for each node as it goes:

    create or replace function comment_tree(comment_id integer) returns json as
    $$
    declare
        children json;
        comment_obj json;
    begin
        select json_agg(comment_tree(id)) into children
        from comments
        where parent_id=comment_id;
        
        select json_build_object(
            'id', id,
            'comment', comment,
            'username', username, 
            'children', children
        ) into comment_obj 
        from comments
        where id=comment_id;

        return comment_obj;
    end
    $$ language plpgsql;

Then to see a comment with all it's children, just pass in the node number:

    select comment_tree(1);

If you want to build a structure for all top-level nodes, a second function can do that by finding comments with no parent:

    create or replace function comment_forest() returns json as
    $$
    declare trees json;
    begin
        select json_agg(comment_tree(id)) into trees
        from comments
        where parent_id is null;
        return trees;
    end;
    $$ language plpgsql;

    select comment_forest();

It's possible to bump into the stack limit on recursion using this technique. The PostgreSQL superuser can increase max_stack_depth to increase the depth of comments that can be computed. I did a little testing and found that the default stack size of 2048KB is enough for a comment depth of around 3500, which would be sufficient for most use cases, I guess. That number does not appear to depend on the size of the comments or the total number of comments. Presumably the contents of returned JSON values are not stored on the stack.

Note also that these functions run many times slower if instead of returning type JSON you declare them as returning JSONB.
",1527410809.0
mage2k,"[Use the manual.](https://www.postgresql.org/docs/10/static/plpgsql-control-structures.html#PLPGSQL-RECORDS-ITERATING)

Side note:  Formatting SQL queries for readability is just as important as any other type of code.  The entire thing on one big line that wraps and wraps is pretty much unreadable.  [Here's](https://www.xaprb.com/blog/2006/04/26/sql-coding-standards/) an outline of some pretty sane standard formatting choices.",1527190831.0
simcitymayor,"A good first step would be to use CTEs (aka `WITH` clause blocks) to generate the intermediate tables.

Creating non-temporary tables, even if you name them temp_something, is going to cause problems if anybody tries to run this query simultaneously.

Ultimately, you don't need the function, or the custom type. You just need to understand the query you want to write. Being able to articulate that, like one would a math story problem, is the first step toward truly understanding what you're asking the database for.",1527193073.0
dopperpod,/r/domyhomework,1527256618.0
jk3us,">Parallelized CREATE MATERIALIZED VIEW

Does `refresh materialized view` also get parallelized?  This would be a great benefit to us.",1527172064.0
cediddi,"I'm going to ask a very stupid question here. Should I develop with this, for my next product (will be released in a month)?  I need foreign keys inside a partitioned table and current implementation doesn't support this.


I'm writing the table routing in application so I can partition from hand. I also need approx 150 partitions, a composite unique index in each partition and 5 foreign keys are defines inside the table. This is like a 5 dimensional matrix with 3 integer values. I'll be bulk updating the values by incrementing or decrementing (or creating). I use a logic like this, 

   insert datafield=1 where uniquefield=1 on conflict do update datafield +=1.  

No rows get updated by their partition key, all partitions should have unique indexes that include the partition key.",1527181253.0
raghar,"I have a question about partitions.

Let's say I do not use any right now, but I would need them at some point e.g. in 5 years. Will I be able to easily migrate all tables (that need it)?",1527241692.0
jlrobins_ssc,"Has the veneer of a good article, but reads biased, opinion-based, and actual benchmark-free. A definite opinion piece with pretty diagrams. Executive clickbait.",1527083201.0
grupwn,"PostgreSQL provides enough nice-to-haves for developers with sufficient performance for most moderate use cases, that I don't see any reason to choose MySQL except for niche use cases around high-velocity write transactions at high-scale or if the app I'm using has a hard dependency for it.

It would be nice for PostgreSQL to explore different storage engines like they did with indexing techniques, but I'm not living or dying by the lack of variety there yet.  The author did make a great case for MySQL's continuing improvement, which is nice to see.  Competition keeps things moving forward, and I'm glad to see MySQL is trying to give PG a run for its money.",1527089245.0
EvanCarroll,"MySQL can't even get ENUM's right https://dba.stackexchange.com/a/207541/2639

and still has problems with petty things like UNION and NULL: https://jira.mariadb.org/browse/MDEV-15999
",1527090747.0
flyingmayo,"That was a far better piece than I expected.  I agree that it's long on opinion and short on test data (zero) to back those opinions up, but some of the points might be valid.  Postgres certainly isn't perfect and despite the impressive progress made over the past few major releases, there are still large hurdles ahead.

My experience has reliably guided me away from mysql at any size over 100G due to a drop off in performance, replication breakdowns and just a steady stream of general weirdness that I don't get with postgres.

I'd be interested in an apples-to-apples comparison at 100G, 1T, 10T that incorporates more than simple read/write tests.  e.g. backup and failure recovery scenarios, maintenance tasks, etc

",1527092821.0
EvanCarroll,"Try PostgreSQL Up and Running. PostgreSQL also has much better docs and the official material is exception. I'd probably the Up and Running book, follow along, and then read the official docs. You should be able to move through both of these *very* quick if you come from SQL Server background.",1527028574.0
jtwyrrpirate,"Here is a lightning-fast phone-typed overview for a COMPLETE Linux/Postgres noob:

Learn how to SSH into a server & navigate the file system.

Examples:

ssh you@someserver (this connects you to a server)

pwd (“print working directory” will show you where you are in the file system)

ls (this will list the contents of the current directory)

cd /tmp (change directory to /tmp)

cd - (go back to where you were)

cd .. (go up one directory from where you currently are)

cd ~ (go to your home directory)

Enabling, starting, stopping & checking services is done typically via systemctl, like this with elevated “sudo” privileges:

sudo systemctl enable postgresql-10

sudo systemctl status postgresql-10

Etc...

With Postgres, you could also use pg_ctl, first by changing to the Postgres user account:

su - postgres 

And telling Postgres to stop a cluster:

pg_ctl -D /var/lib/pgsql/10/data -m fast stop

Or reload the config files without restarting the Postgres daemon (good for some settings, but not all):

pg_ctl -D /var/lib/pgsql/10/data reload

(Note: Postgres uses the term “cluster” where just about everybody else would use the term “instance”...so when somebody talks about their Postgres “cluster” they typically aren’t referring to a clustered computing environment, but rather a single Postgres instance. This is probably the only genuinely weird use of terminology in the Postgres lexicon so I figured you might as well hear about it now).

Some important files in a Postgres cluster:

postgresql.conf (the config file)

pg_hba.conf (access control, changes to this file require pg_ctl reload as above to take effect)

recovery.conf (used for replication)

You use the “psql” command to connect to a database. Also learn about .pgpass files & the syntax/permissions they need to work.

Also good to know out of the gate: pg_dump, pg_restore & pg_basebackup.

If you’re going to be running a high-traffic db (e.g. lots of connections), read up on pgbouncer for connection pooling.

Never ever ever use Slony for anything under any circumstances. Ever.

Always use LVM when deploying your servers. You’ll thank me later when you run out of space & need to expand without downtime.

The weirdest thing to get over going Windows-> Linux is almost everything in Linux is a file. So, rather than blobs of exe-ness, think about how you might manipulate/ utilize a file. It probably sounds weird, but can help get you in the CLI mindset.

firewalld is the firewall you will most likely encounter in Linux. Don’t just turn it off!

selinux is a thing you will encounter. It will sometimes appear to break things for no reason. However, there is ALWAYS a reason so don’t just turn it off if you can help it.

As others have/will suggest, read a good Postgres book, practice in some VMs & have fun!",1527038026.0
joeDUBstep,"I am in the same boat as you! Primarily a SQL Server DBA and being forced to learn Postgresql + Linux. I know this post is a month old, but I was wondering which resources you ended up going with.

It just all seems so difficult to me right now.",1531866427.0
r0ck0,"The dangerous issues ENUMs could cause in mysql has scared me off them for life.  But never looked into them since moving over to postgres.

What do yall think of using ENUMs in general in postgres?  

So that I still feel ""in control"", I always either:

* Create a new table with a row for each ENUM value, with a SMALLINT primary key that is the FK everywhere else.
* Or just use a SMALLINT number in the table and do the conversion to the text name in app-level code.  Usually it's just a constant in the app code with the number as the value, which gives me autocomplete in my IDE.


So I'm curious how others feel about them...

1. Am I just being paranoid avoiding them in postgres too?

1. What situations do any of you think are better suited to actually using ENUMs?  (some real world examples would be cool) 

1. And are they completely safe to use when you go adding/renaming/deleting ENUM values?  This is my biggest fear with them.",1527051147.0
Sysmonster,May be unrelated to your eventual goals but if you are thinking of using AWS RDS/Aurora don’t use enum types. It inherently modified system catalogue tables which is not ideal and not allowed by AWS for the most (all I think) part. Th awfully we only had to refactor out a few instances of it in our schema to a lookup table when we migrated .,1527069533.0
ccb621,"What exactly are you trying to figure out? Yes, there are differences. ",1526952748.0
dark-panda,"The code base is all one repository/project and can be found at https://git.postgresql.org/gitweb/ or on its GitHub mirror at https://github.com/postgres/postgres. It is written in mostly C, not C++. Platform differences are handled via abstractions, like most C code bases of comparable size, via platform-specific makefiles, C preprocessor macros, the usual host of cross-platform tricks. Much of the cross-platform code can be found in the port directory. Builds for each supported platform are facilitated by platform-specific makefiles. ",1526994820.0
leamanc,"The code is written in C, so I would think it's the same on all platforms, and the C compiler takes care of tailoring the binary for the target platform.",1526961663.0
mage2k,That is part of the [cube](https://www.postgresql.org/docs/10/static/cube.html) extension so you'll need to create that extension to use that operator.,1526948046.0
cyanydeez,"https://www.postgresql.org/docs/10/static/cube.html

theres no mention of bytea bing valid inputs or conversion...",1526959876.0
black_gis,For what is that exactly useful did not get the idea?,1526985403.0
Tostino,"One thing mentioned is ""the data is too big to back up"". I find that hard to believe. There are tools like pgbackrest which deal with huge amounts of data just fine, and do it in an efficient way.

Cool write-up none the less though.",1526930139.0
felixge,"Are you sure you need a distributed database?

Sure, you've reached a scale where things get trickier to operate and maintain, but your DB size is still very much in the sweet spot for a single instance.

My team is operating several Postgres instances that are ~1TB each, and I know many people with >10TB PostgreSQL installations. Sure, we've had to solve a few issues here and there, but generally speaking things are running very smoothly.

So I'm pretty sure the issues you're experiencing can be solved. Figure out how to optimize the slow queries. Use `statement_timeout` to reduce the impact of the queries you haven't optimized yet. Maybe use triggers to pre-aggregate counters or similar things.

But please, don't think that switching to a distributed DB will make your life easier. If anything, it will probably be more difficult to operate.",1526903901.0
ants_a,145GB is a relatively small database that is easy to fit into memory on a single machine. If you are not able to optimize your queries to run fast on a single machine a distributed solution will just use more machines to be slow.,1526905894.0
stympy,"I have to echo the other replies... your database isn't that big.  Definitely spend some time optimizing your queries, etc., to improve performance on your current setup, and/or scale up your database server.",1526906360.0
CockroachBram,"Hey!  I work at Cockroach and I think we would fit exactly what you're looking for.

> * to have a distributed database solution in place which is horizontally scalable

That's exactly what we do.

> * Easy to add nodes, remove nodes without any downtime (sure I can accomodate some write-locks for setup)

No write locks needed.  Nodes can be added and removed easily.  We even sure that all upgrades can be rolling upgrades.  More than that, we even have online schema changes.

> * Have the ability to tweat replication factor. Ideally I would love to have replication = number of nodes i.e. each node has complete database. So that when there are simple queries, it doesnt have to do distributed queries (which make system slow) and when the query is a bit complex, it does distributed since data is available in each node.

We do both local and distributed queries, but it does depend on how you setup your tables.  We offer [interleaved](https://www.cockroachlabs.com/docs/stable/interleave-in-parent.html) tables to ensure that child tables are in the same range and don't require a distributed query.  And you can set the replication factor (and replica locations) based on Database, Table, and even row based.

> * Best case: Some GIS based plugin / extension on the solution would be icing on the cake.

Sadly not yet.  But what exactly are you looking for?  We've had a few requests for geo-spactial .

> * SQL compatible, so that least of the application rewrite is required.

Cockroach speaks the postgres wire protocol.  So the SQL you know already should mostly already work.

If you have any questions, I'd be happy to answer them.
",1526917555.0
merlinm,"It sounds like you need to employ basic optimization strategies, not 'distribute'.  The most obvious one is to employ SSD if you have not already done so.",1526914796.0
MrBaseball77,"145GB, that doesn't seem too large for a Postgres DB. Maybe you just need to look at your architecture?",1526924916.0
jakdak,"Your DB can still fit on a SSD card or even entirely in memory on a reasonable sized box.  

Probably lots of options before moving to a whole new architecture.",1526943897.0
pathaks,"Hi there!

Sumedh from Citus Data here. As others here have noted, there are often optimizations you can perform at the single\-node level within PostgreSQL before you take the leap to a distributed offering. That said, we've also seen use\-cases which need parallelism at the 100\+GB level. 

Glad to hear you evaluated CitusDB. Could you elaborate a little bit on the restrictions you faced? Citus in general will be a great fit for scaling out your PostgreSQL database. What limitations did you run into while evaluating Citus?

Thanks!",1526944075.0
jlrobins_ssc,"The ""Automatic Index Creation"" portion is huge huge huge. One of the biggest complaints about table extension and so forth was having to manage indices (and other constraints) by hand on the family of related tables.

Very exciting!",1526909433.0
Darkmere,"Oh, this solves a lot of my painpoints around indexes.

I wonder if it works well on partial indexes?  This one will be fun.",1526918397.0
Amaracs,"Sounds exciting, but what i miss the most is to be able to cache execution plans for dynamic queries in postgres too.",1526926224.0
jakdak,"Very welcome stuff.   Especially row movement.

PostgresSQL is quickly closing the gap w/ Oracle on the partitioning features.  (And, IMHO, the explicit PostgreSQL modeling of partitions as sub tables is much more intuitive and flexible than Oracle's implementation)",1526953350.0
jakkarth,"> starting pgsql-8.3

PostgreSQL 8.3 hit EOL in February 2013. 8.4 hit EOL in July 2014. I don't know what you're doing, but please stop trying to install ancient versions of postgres.

[Get the current version from the official source](https://www.postgresql.org/download/windows/).",1526843171.0
ypsthelove,For ubuntu users I'd recommend to install newest version (https://www.postgresql.org/download/linux/ubuntu/ ) instead of the default package. ,1526754915.0
jktj,I'm facing some formatting issues in my 720p laptop when viewing the webpage.,1526992952.0
therealgaxbo,"If I'm reading this correctly, the problem appears to be that the list of partitions to scan is dynamically decided at runtime due to the `any(regexp_split_to_array(m1.partnerslist,',')) ` clause.  If you replace that with a hard-coded list `in ('G01', 'G06', ...)` does the plan look better?

If so...I think the solution might be ""wait for Postgres 11"".  See [this timely blog post](https://www.depesz.com/2018/05/01/waiting-for-postgresql-11-support-partition-pruning-at-execution-time/) for details.",1526658652.0
MrBaseball77,"Changing the WHERE to this seemed to speed it up somewhat, 886ms vs. 2.0sec:

    WHERE u.ic in ('01036')                            -- part to query
      AND u.id = any ('{N40,G01,G06,G21,K17,N49,V02,M16}') -- vendor(s) to query

But the plan is still huge.",1526660202.0
swenty,"Having 1400 child tables doesn't seem like a good design decision. I realize that you may be stuck with it at this point. Nonetheless, is it really the case that every vendor has a different data layout? Is there not some commonality that could be exploited to reduce the number of tables, if you were to redesign the database?",1526732915.0
MrBaseball77,"This decreases the time to under 800ms:

    WITH cte as (
        SELECT partnerslist as a FROM sup.members WHERE id = 'N40' 
    ) 
    SELECT 
    	(select m2.company from sup.members m2 where m2.id = u.id) as company,
        u.id,
    	u.item, 
    	DATE_PART('day', CURRENT_TIMESTAMP - u.datein::timestamp) AS daysinstock, 
    	u.grade as condition, 
    	u.stockno AS stocknumber, 
    	u.ic, 
    	CASE WHEN u.rprice > 0 THEN 
    		u.rprice 
    	ELSE 
    		NULL 
    	END AS price, 
    	u.qty
    FROM pub.net u 
    WHERE u.ic in ('01036') -- part to query
      AND u.id = any(regexp_split_to_array('N40,'||(select a from cte), ','))

I cannot retrieve the company from sup.members in the `cte` because I need the one from the u.id, which is different when the partner changes in the where clause.",1526924765.0
BigEnoughRock,"TL;DR Use indexes.
Seriously.
",1526645246.0
therealgaxbo,"There's some fairly dubious advice in that article:

>Tip: Put in the first place columns, which you use in filters with the biggest number of unique values.

While having the most selective columns at the start of an index will lead to a slight improvement, I think this is a distant third-place consideration behind choosing the leading columns based on how many queries it will be useful for, and most importantly, choosing leading columns that are tested for equality not ranges.  Which brings us on to...

> Tip: Date columns are usually one of the best candidates for the first column in a multicolumn index [...]

Wut?  Date columns are almost always used in range conditions - indeed that's how they are used in the example given.  That means that the second column in the index may as well not even be there.  You can even see in the explain output that it's not used!",1526648264.0
timacles,I didn't know they had clickbait articles for databases,1526652453.0
muddiedwaters45,Articles like this make me want to kill myself.  Repeatedly.,1526739628.0
alinroc,"This may border on heresy but drop a line to the North Texas SQL Server User Group and see if they know of anyone. http://northtexas.pass.org/

Even better, drop in on their SQL Saturday this weekend and ask in person!  http://www.sqlsaturday.com/734/eventhome.aspx",1526514828.0
ulfurinn,"Your `pg_hba.conf` (the postgres authentication config) is probably set up to trust local connections to run as the user matching the OS user.

https://www.postgresql.org/docs/10/static/auth-methods.html#AUTH-PEER",1526334829.0
qatanah,Try to check if you have ~/.pgpass configured.,1526347190.0
iiiinthecomputer,unix sockets,1526360480.0
CSI_Tech_Dept,"The username is taken from USER environment variable which typically is set when you log in. As for authentication it is all up to pg_hba.conf I forgot what is set by default, but basically in that file you can define that if you for example connect through Unix socket (which is the first PostgreSQL is trying) then the PostgreSQL trusts the user. 

There are other authentication mechanisms, for example besides the standard like ldap you can also configure PAM where you ask your system it could either be used to give access to selected users on the system without password, or could allow using the same password that you use to log in. If you have a trusted network and install ident service, PostgreSQL can ask the remote server, what user made the connection and based on that decide to grant access (or not).

Here is more about it: https://www.postgresql.org/docs/devel/static/auth-pg-hba-conf.html",1526368773.0
,[deleted],1526398982.0
mokadillion,"The 'default' user is postgres and the default host is localhost and the default port is 5432 and the default database is postgres so essentially entering 

`psql`

is the same as entering

`psql -d postgres -U postgres -h localhost -p 5432`

providing some / any of the above overrides them and defaults the rest. 

whether this string will gain access is down to how you have your pg\_hba.conf file configured. Considering you do gain access its likely set to the default trust method for local host and all users both login and unix domain / user. 

You can override the default values of psql environment variables by assigning them in the bash profile or whenever you want. Postgres will look for certain ones before returning default values.  These are: 

* `PGHOST behaves the same as the` [`host`](https://www.postgresql.org/docs/9.6/static/libpq-connect.html#LIBPQ-CONNECT-HOST) `connection parameter.`
* `PGHOSTADDR behaves the same as the` [`hostaddr`](https://www.postgresql.org/docs/9.6/static/libpq-connect.html#LIBPQ-CONNECT-HOSTADDR) `connection parameter. This can be set instead of or in addition to PGHOST to avoid DNS lookup overhead.`
* `PGPORT behaves the same as the` [`port`](https://www.postgresql.org/docs/9.6/static/libpq-connect.html#LIBPQ-CONNECT-PORT) `connection parameter.`
* `PGDATABASE behaves the same as the` [`dbname`](https://www.postgresql.org/docs/9.6/static/libpq-connect.html#LIBPQ-CONNECT-DBNAME) `connection parameter.`
* `PGUSER behaves the same as the` [`user`](https://www.postgresql.org/docs/9.6/static/libpq-connect.html#LIBPQ-CONNECT-USER) `connection parameter.`
* `PGPASSWORD behaves the same as the` [`password`](https://www.postgresql.org/docs/9.6/static/libpq-connect.html#LIBPQ-CONNECT-PASSWORD) `connection parameter. Use of this environment variable is not recommended for security reasons, as some operating systems allow non-root users to see process environment variables via ps; instead consider using the ~/.pgpass file (see` [`Section 32.15`](https://www.postgresql.org/docs/9.6/static/libpq-pgpass.html)`).`
* `PGPASSFILE specifies the name of the password file to use for lookups. If not set, it defaults to ~/.pgpass (see` [`Section 32.15`](https://www.postgresql.org/docs/9.6/static/libpq-pgpass.html)`).`
* `PGSERVICE behaves the same as the` [`service`](https://www.postgresql.org/docs/9.6/static/libpq-connect.html#LIBPQ-CONNECT-SERVICE) `connection parameter.`
* `PGSERVICEFILE specifies the name of the per-user connection service file. If not set, it defaults to ~/.pg_service.conf (see` [`Section 32.16`](https://www.postgresql.org/docs/9.6/static/libpq-pgservice.html)`).`
* `PGOPTIONS behaves the same as the` [`options`](https://www.postgresql.org/docs/9.6/static/libpq-connect.html#LIBPQ-CONNECT-OPTIONS) `connection parameter.`
* `PGAPPNAME behaves the same as the` [`application_name`](https://www.postgresql.org/docs/9.6/static/libpq-connect.html#LIBPQ-CONNECT-APPLICATION-NAME) `connection parameter.`
* `PGSSLMODE behaves the same as the` [`sslmode`](https://www.postgresql.org/docs/9.6/static/libpq-connect.html#LIBPQ-CONNECT-SSLMODE) `connection parameter.`
* `PGREQUIRESSL behaves the same as the` [`requiressl`](https://www.postgresql.org/docs/9.6/static/libpq-connect.html#LIBPQ-CONNECT-REQUIRESSL) `connection parameter. This environment variable is deprecated in favor of the PGSSLMODE variable; setting both variables suppresses the effect of this one.`
* `PGSSLCOMPRESSION behaves the same as the` [`sslcompression`](https://www.postgresql.org/docs/9.6/static/libpq-connect.html#LIBPQ-CONNECT-SSLCOMPRESSION) `connection parameter.`
* `PGSSLCERT behaves the same as the` [`sslcert`](https://www.postgresql.org/docs/9.6/static/libpq-connect.html#LIBPQ-CONNECT-SSLCERT) `connection parameter.`
* `PGSSLKEY behaves the same as the` [`sslkey`](https://www.postgresql.org/docs/9.6/static/libpq-connect.html#LIBPQ-CONNECT-SSLKEY) `connection parameter.`
* `PGSSLROOTCERT behaves the same as the` [`sslrootcert`](https://www.postgresql.org/docs/9.6/static/libpq-connect.html#LIBPQ-CONNECT-SSLROOTCERT) `connection parameter.`
* `PGSSLCRL behaves the same as the` [`sslcrl`](https://www.postgresql.org/docs/9.6/static/libpq-connect.html#LIBPQ-CONNECT-SSLCRL) `connection parameter.`
* `PGREQUIREPEER behaves the same as the` [`requirepeer`](https://www.postgresql.org/docs/9.6/static/libpq-connect.html#LIBPQ-CONNECT-REQUIREPEER) `connection parameter.`
* `PGKRBSRVNAME behaves the same as the` [`krbsrvname`](https://www.postgresql.org/docs/9.6/static/libpq-connect.html#LIBPQ-CONNECT-KRBSRVNAME) `connection parameter.`
* `PGGSSLIB behaves the same as the` [`gsslib`](https://www.postgresql.org/docs/9.6/static/libpq-connect.html#LIBPQ-CONNECT-GSSLIB) `connection parameter.`
* `PGCONNECT_TIMEOUT behaves the same as the` [`connect_timeout`](https://www.postgresql.org/docs/9.6/static/libpq-connect.html#LIBPQ-CONNECT-CONNECT-TIMEOUT) `connection parameter.`
* `PGCLIENTENCODING behaves the same as the` [`client_encoding`](https://www.postgresql.org/docs/9.6/static/libpq-connect.html#LIBPQ-CONNECT-CLIENT-ENCODING) `connection parameter.`

[https://www.postgresql.org/docs/9.6/static/libpq\-envars.html](https://www.postgresql.org/docs/9.6/static/libpq-envars.html)",1526557963.0
jringstad,"You can do that, but I would really recommend just writing these things in SQL (or PL/pgSQL) -- it's actually quite easy to test! You basically just start a transaction, populate a table with the state of the world your code is expecting it to have (if any), run your code and then rollback afterwards.

`psql -f` all of your test files as part of your CI and as pre-deployment step, and you're golden.",1526323201.0
kaeedo,"You could always try out [F# Type Providers](https://docs.microsoft.com/en-us/dotnet/fsharp/tutorials/type-providers/).

Here's a SQL provider. http://fsprojects.github.io/SQLProvider/

These things give you compile time checks against the database",1526333051.0
obrienmustsuffer,"SQL is _literally_ a language designed to query relational databases, and is used (more or less standards-compliant) by practically all relational databases, excluding maybe some esoteric ones. I don't think it is in any way possible to completely circumvent SQL, neither in PostgreSQL nor in any other RDBMS; all abstraction layers like ORMs or procedural languages will, in the end, use SQL anyway. I also really don't see any point in trying to do so when SQL works just fine.

Even if it were possible, it would probably mean quite a lot of work to reimplement all the required features that SQL readily provides, and perform worse than SQL when it couldn't make use of the query planner/optimizer.",1526335334.0
lykwydchykyn,"You can create stored procedures in other languages if you install the necessary support; I think it ships with support for using Tcl, Perl, or Python, but there are other languages as well.  Not sure if that actually helps your testing situation.  

See https://www.postgresql.org/docs/9.6/static/xplang.html

I don't recommend using this feature, but if it works for you, it's there.

Or you could use an ORM, I suppose.
",1526321092.0
EvanCarroll,"> I wonder if there's a way to write a query in other languages that are not SQL. Is there an extension for this kind of things?

What do you by *query* here. Other answers are presuming you mean PL. You can in fact write PL in any language you want. But PL can only, for all intents and purposes, access your data with SQL. For instance you can only either send data in.

    SELECT proc_table(tbl) FROM tbl;

Using SQL, which requiring adding your own selectivity. Or you have to write the query from inside the function and return a typed data-set. SQL is strongly typed. At some point you have to pay the piper and type-up: either when you declare function `RETURNS type` or when you use the function `fn() AS t(x int, b text)` etc.

As far as alternatives to SQL itself, there aren't many options. PostgreSQL predates SQL, and was originally written for Quel (which had a lot of advantages at the time). That code is no longer there so the **only way to query PostgreSQL, is with SQL**. So at some level, 

* You must generate SQL.
* And everything must typed.

It would be a **massive** task to change that. For short hacks though, one thing you can do, if you absolutely must, is to return JSONB (which essentially untyped). You can build a system around JSONB or HSTORE, but it'll be slow and you'll soon tire of all the shortcomings.

Consider asking these types of questions at dba.stackexchange.com",1526326130.0
syslog2000,"You could use an ORM like Hibernate. Sure, it's heavy and impacts performance, but you can write custom HQL (Hibernate's fairly clean dialect of SQL) where needed and get the best of both worlds. This is what we do and it works pretty well.",1526327188.0
r0ck0,"> I always dislike writing a huge piece of SQLs (difficult to test and etc.)

Break them up into separate SQL views.  Makes everything easier and more versatile.",1526437416.0
Hyasynth,"Pretty confusing question, are you ordering the objects in each array, or the JSON_Column as a whole?",1526326824.0
EvanCarroll,"that doesn't even make sense you can't wrap values in {} and expect them to work, and you're inserting into two columns the username isn't even in the VALUES statement.",1526310811.0
swenty,"It's not obvious to me what a change feed is. Are you talking about a way for an application to be notified when database rows change? If so, you're almost certainly looking for the SQL listen and notify commands. You can embed notify commands in a trigger, so that notifications are generated whenever certain changes are observed by the database.

If that's not what you're asking, you should perhaps say a bit more about what your needs are.",1526151597.0
felixge,"I think you want to look into [logical decoding](https://www.postgresql.org/docs/current/static/logicaldecoding-explanation.html) based approaches. This will allow you to build a solution that won't miss any events, even when upstream receivers are temporarily unavailable.

This might be a good starting point: https://github.com/eulerto/wal2json

Unfortunately I'm not familiar with any tools that support redis/kafka out of the box.",1526197200.0
jarym,I've had good success with Debezium https://github.com/debezium/debezium,1526250264.0
R0b0d0nut,There is free tier rds on amazon. ,1526091273.0
mikaelhg,The 3€ Hetzner cloud instance.,1526096148.0
unavailableFrank,There is a free tier on Amazon EC2 \(for a year\) and a free tied on Google Cloud Platform \(always free\). ,1526098335.0
swenty,"Linode 1GB for $5/month. Presumably you want backups too, for $2/month, so that's $7/month total.",1526112636.0
kozikow,Get some AWS or GCP startup credits and run hosted version within the cloud. I run Google Cloud SQL for reasonably sized workloads and it works well for me. Alternative cost of hiring DBA or risking my customer's data seems much higher.,1526133081.0
subssn21,"I have been down this road before and we started with Linode for the first year (Digital Ocean is good too.) Cash is tight in a bootstrapped startup so we took some chances, those chances luckily didn't bite us in the ass. I hate dealing with system administration things and when you are the only tech person at a startup they can easily eat up your valuable time that you could be using to make your product better. I was lucky in that I had a friend who was kind enough to setup the backup process for the postgres server for me. 

The thing is before long (assuming your customers are businesses) someone is going to ask you about availability and your infrastructure, and business customers expect that you have failovers and point in time backups of your database. Once you decide to go to that level don't try to do that yourself. You will waste a lot of time, Just go with RDS, Aurora, or the Azure in GCP versions of them.",1526388002.0
coder111,"There's also Heroku, but the free Posgres really sucks there if I remember correct;y.",1526099701.0
warmans,5$/€ digital ocean instances are decent for low traffic applications.,1526120613.0
brandit_like123,"Startup means you are actually using it for Production, right? There are tools you can use to measure how much IOPS you are using. My answer is geared towards a startup using it for production. For development, I'd still use RDS but with more automation so your CI runners can spin up an RDS database and spin it back down when done. 

See: https://stackoverflow.com/questions/27558495/do-we-need-provisioned-iops-for-rds-instance-thats-using-60-iops-according-to-m 

I don't know if you currently have a DB but here's one way to calculate IOPS. You'll need to adapt this for Linux if you have that. https://www.brentozar.com/archive/2006/12/dba-101-using-perfmon-for-sql-performance-tuning/ 

I would not recommend hosting it yourself on EC2, it will quickly take up more and more of your time. Use RDS, this is what it's for and you can scale up quite easily. 

There are other questions as well, like are you using microservices, do you need one database for each service etc? ",1526121947.0
ppafford,"You could raise the connection limit to a higher number in PostgreSQL until you figure out what code is making all the connections.

Are you using PDO, Doctrine, persistent connections?

Sounds like you are making a connection inside a loop, instead of outside the loop and having the execute command inside the loop",1526041470.0
muddiedwaters45,"There's not a whole lot to go on here.  Are you using pg_pconnect() or similar in PHP?  How are you running PHP (e.g. mod_php, FastCGI, etc.)?",1525967387.0
geggo98,I think it's a compile time parameter: [`--datarootdir`](https://www.postgresql.org/docs/9.2/static/install-procedure.html),1525946481.0
leamanc,"There may be a config file to change this, but what I’d do is just create a symlink:

    ln -s /usr/local/share/postgresql /usr/share/postgresql",1525921968.0
jk3us,"Instead of enter/exit tables, have a ""membership"" table that has:

* group id
* singer id
* enter date
* exit date

Now you don't need triggers, you can just have a check constraint that says that exit > enter. This design also allows for a singer to be in a group multiple times with gaps between them.  You can also make sure no singer/band combo has overlapping memberships with with a [gist index](https://dba.stackexchange.com/a/166310).

Another approach would be to combine enter/exit dates in a [daterange](https://www.postgresql.org/docs/current/static/rangetypes.html) field, but I probably wouldn't do that.

If you wanted to get fancier and give singers birth and death dates and bands creation and breakup dates, you could have triggers that make sure members are alive and bands are active during all the memberships.",1525899398.0
sieghart92,"The solution i found is

    IF ( SELECT MAX(date_entry)>(new.date_exit)
        FROM entry 
        WHERE (((entry.singerid)=new.singedir) AND ((entry.group)=new.group))) THEN
        RAISE NOTICE 'exit date is before joining';
        RETURN NULL;
     ELSE RETURN NEW;
    END IF;",1525905257.0
timacles,You have to be a little more clear why you can't get it to work or where you're having issues. Because otherwise it sounds like you didn't even try,1525897718.0
Citizenfishy,"EXECUTE format('SELECT username FROM %s', o_ficheiro) 
INTO o_user",1525864195.0
jk3us,Help how? We'll probably need more info.,1525864537.0
jlrobins_ssc,"Well, buddy, not to just downvote w/o comment, but you really need to 1) work on the formatting (two space indent every line to get it to format like code), and 2) explain the actual problem.",1525874879.0
TokenMenses,"Maybe I'm not getting your exact situation, but if you need to get a docker image with initial data already loaded into postgresql, I'd think you'd create a dockerfile with a COPY command that copies an external dump file into the image and then a RUN command to execute the import of that file and delete it from the image.
",1525818526.0
HotKarl_Marx,"It looks like your container is only speaking IPv6, but your machine is wanting IPv4. ",1525805604.0
chock-a-block,"I know I'm not answering directly, but what's the point of loading a VM (I know, I know it's not a VM) with a ""fat"" database server with all the bells and whistles?  If the data is static, sqlite is fantastic. Otherwise, point your apps at a database cluster with a client.  It's much easier to scale/manage.

Can you telnet to the VM's database server? Post the pg_hba.conf and postgresql.conf files.",1525816410.0
jrwren,"I recommend not putting the data in the container at all.

Use docker -v to mount a directory which is a postgres database cluster. There may be a way to do that with docker-compose.
",1525827420.0
craig081785,A lot of credit to Paul for the original which the inspiration was built on: https://medium.com/circleci/its-the-future-90d0e5361b44,1525800685.0
beatemdown,"Bahaha - this is great. Costs more and has poorer performance is an upgrade, right guys?",1525821127.0
InfoSec812,LOL,1525796489.0
axeaxeax,Hahaha this is gold :D,1525798467.0
r0ck0,"So glad I never spent any time on the nosql fad.  I guess it has it's use cases (supposedly non-relational data)... but I've never been able to think of one.  At least a primary data store.  Maybe good as some kind of caching layer or something.  But rare that any projects ever get that large, and if you do, it's a nice problem to have.",1525821202.0
coderhs,"Good, as a satire. 😂",1525850696.0
stympy,"Yes, you can have multiple PG clusters running on the same server, running either the same version or different version of PG.  You would run the second cluster on a different port and pointing to a different data directory.",1525783727.0
mage2k,"> one is 9.4, other - 9.3

Why not get them both upgraded to 10 and put them on the same server(s)?",1525817654.0
amachefe,Quite a good read. they took a very long while to migrate to SQL... Sounds like ego issues,1525737964.0
QuantumRiff,This website redirects into a constant loop asking you to disable your ad\-blocker...,1525702595.0
dark-panda,"That’s exactly what I’ve been doing, but I just wrap it into a stored function called plainto_or_tsquery with the same arguments as plainto_tsquery and use that when necessary. ",1525627610.0
depesz,"That works, but you might want to wrap it in an sql function that would simplify calls. For example:

    create function plainto_or_tsquery(text) returns tsquery as $$
        select to_tsquery('english', replace(plainto_tsquery('english', $1)::text, '&', '|'));
    $$ language sql;

and then just use this function directly:

    select plainto_or_tsquery('red green blue');",1525689368.0
colloidalthoughts,You'll probably have better luck with this question in /r/LearnPython ,1525523880.0
iiiinthecomputer,"COPY doesn't return a result IIRC

Edit: It does report an affected rows count. You should be able to get that the same way you do an update count from psycopg2. Use http://initd.org/psycopg/docs/cursor.html#cursor.rowcount",1525576665.0
chock-a-block,Need more information.,1525449592.0
jlrobins_ssc,"[The committed patch](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=b4678df184b314a2bd47d2329feca2c2534aa12b) makes it so that errors can be reported more than just once.

  > In closing, Freund asked for some documentation that would tell application developers what needs to be done in order to durably write their data to disk. Dave Chinner claimed that was ""asking too much"", to a fair amount of laughter. ",1525446434.0
SomeGuyNamedPaul,Or you could just use docker.,1525430312.0
ccb621,Both options work. I last used Homebrew (out of convenience). I mostly run in Docker now to keep multiple projects isolated. ,1525434165.0
ppafford,"I use PostgreSQLApp, running in brew I’ve had to create another user on my machine, using the app it just works with no setup, bonus using the app the cli tools are installed as well",1525439518.0
petepete,"Postgresapp is great, it makes getting up and running super easy. I used it before I started relying on Docker, everything's nice, clean and separate when containerised.",1525442155.0
richraid21,I cannot stand `brew`. Just run a `docker-compose` file with 5 lines and you can have a running postgres instance in less than 1min. ,1525564678.0
jakdak,"In general, your options are going to be the following:

1) Pulling the changes out of the transaction logs

2) Adding audit triggers to your tables

3) Modifying your application to track changes 

",1525402005.0
chock-a-block,Jakdak is right. The bare minimum would be audit triggers.,1525407518.0
789um789,"Queries having to specify tables that contain on specific date ranges sounds cumbersome; perhaps your workload is a candidate for a hypertable.
http://www.timescale.com/

Even with your current setup you may run out of locks (depending on how many tables are in a query) but that is an easy fix.
https://docs.timescale.com/v0.9/getting-started/configuring#locks",1525441264.0
dont_ban_me_please,timescale is nice. wish i had a use case for it. I don't.,1525397665.0
axeaxeax,"Fantastic article! Much appreciated.
Do you have any benchmarks with logical replication in PostgreSQL 10 ?",1525413145.0
c0l0,"We've been using pgq since at least 2012 (then on Debian 6 with pg9.0, using the python impl. of the ticker daemon; nowadays on Debian 9 with pg9.6 and the C impl. of pgqd), and we produce and consume tens of millions to hundreds of millions events each day, every day, across four queues (with single-digit consumer counts, written in Perl and C), without having any of the trouble you see.

We plan to eventually switch over to logical replication (since the busiest queue shuttles data from one postgres instance to other postgres instance, with a little bit of transformation work mixed in that we'll eventually get rid of), but the current setup works well enough for that not to be a priority.

I made a habit of deploying custom, minimally patched versions of the pgq functions than what the extension install, that actually make the thing more verbose. Maybe you should try that, too. Other than that... look into your ticker configuration. Check that your ticker daemon instance actually does meaningful work in postgres (using strace on the running daemon, for instance). Check the output of `pgq.get_consumer_info()`. Make sure that your ticker version matches the rest of the pgq machinery installed into your postgres cluster.

pgq works _very_ reliable, in my experience - there must be some operational error at work.",1525372231.0
smariot2,sed and multiline pattern matching is the road to madness.,1525316616.0
elmicha,"`\r` is carriage return, `\n` is linefeed. If you remove both, you have no line separators anymore and your file is pretty much useless.

Get your original file back and use e.g. `tr -d '\r' <oldfile >newfile`.",1525328640.0
neofreeman,Nice probably implementation of [this blog post](http://coussej.github.io/2015/09/15/Listening-to-generic-JSON-notifications-from-PostgreSQL-in-Go/). ,1525321520.0
jlrobins_ssc,"I'd advise upgrading before shipping to production. Assuming you've got a confidence-boosting test suite to catch any issues. The greatly augmented intra-query parallelizations would be well worth it if you're doing anything similar to OLAP in production.

Always harder to upgrade major versions in production. Don't start out behind already!

[ ... says the guy with 9.5 on production still. Going to skip 9.6 straight for 10.X ]",1525091952.0
CaffeinatedT,"pg_upgrade should be fine. If not just add the postgres repositories and install the usual way and dump and restore (its compatable back to 9.5 supposedly). In terms of performance the native parallelisation is kind of a big deal for me as we have a decent sized (few TBs) 'data mart' that non-technical analysts who know SQL query and want to use filter conditions and analytical/aggregation functions on all the time and speed is a big plus to save on moaning but if its just a backend for an application I'd probably test it out to see if its worth the time. I'm very happy with it though as the guy in charge of the data. 


Native partitioning etc is also very cool but I've not used it as much as I thought we would as in the end we're just using it to make it easy dump off less frequently accessed data into csvs and access it with non relational solutions (spark) if we really need it. ",1525091186.0
analfabeetti,"Add apt.postgresql.org repositories, install postgresql-10 and use pg_upgradecluster. With `-m upgrade` if you don't want dump and restore. Can't really be easier than that.",1525143618.0
iiiinthecomputer,"The versioning changed in PostgreSQL 10. ""9.6.x"" changed to ""10.x"". So you're upgrading from 9.6 to 10, not 9.6 to 10.3, really. The 10.3 is a minor version.",1525145653.0
cachedrive,Go to 10.x before you roll into production.,1525280909.0
pelmenept,Do you mean pgadmin? Try dbeaver. ,1525040302.0
KitchenDutchDyslexic,[pgadmin](https://www.pgadmin.org/download/) supports PostgreSQL 10,1525041288.0
alephylaxis,"I've gotten into DBVis lately. I like how it can generate nice referential layouts of your existing dbs. The best feature when you're just starting work on a db that you didn't design, is its ""Circular"" format.

That's in addition to all the regular stuff you can use it for. And it has native support for pretty much everything if you have multiple RDBMSs to work with.

I've got it plugged in to a hundred PG nodes, about half that many MySQL, and a handful of DB2. Haven't connected it to Mongo or Solr, but I'm sure it would be fine there too.",1525041256.0
muminoff,I use free versiion of [Postico](https://eggerapps.at/postico/).,1525073532.0
sfasu77,"I use datagrip and i love it, its not free, however. ",1525086455.0
KitchenDutchDyslexic,"A open source Native Desktop(or Wine) SQL Client:

https://www.heidisql.com/",1525098116.0
AbstractHashMap,[Adminer](https://www.adminer.org/) is pretty good.,1525101031.0
muddiedwaters45,Uhh..  Example?  Because this is not true.,1524941225.0
minaguib,"You can also `group by` and `order by` `1,2,3` (positionally)

Makes it nice when iterating your select clause naming/etc. to not have to keep the other 2 parts in sync",1524934757.0
bigfig,Common Table Expressions (CTE) should get you what you want in SQL server (and Oracle). ,1524943888.0
casscode,"Not really sure, but doesn't the `having` clause allow that?",1524950728.0
igncampa,I've only ever used Oracle and Postgres and I must say I freaking love Postgres. Just saying...,1524935035.0
jakkarth,"This is not a postgres question, it is a PHP question. How PHP chooses to mangle the data that postgres returns is up to them.",1524919465.0
merlinm,"This is a problem with the php driver.  Keep in mind that most applications get all the data back as text anyways.  libpq does have a binary protocol mode, but it's non in common use.",1524931631.0
therealgaxbo,I would suggest asking /r/phphelp for suggestions on this subject.  There are several options - and they'll mostly apply just as well to mysql as postgres.,1524919806.0
smugbug23,I'd generally recommend gin over gist for this purpose.  Did you try both and find that gist was better?,1524873666.0
schneems,"Love the numbers comparing real world data of a Postgres 9.6 db and Postgres 10.
",1524854464.0
CherenkovRadiator,Is there a way to get this presentation's audio?,1524929260.0
Engival,"No. If all your queries in a single php page just rolled themselves back automatically, that would be a broken disaster.

This isn't a postgres ""problem"", this is how php interacts with the database, and really is the expected behavior.

In php, you could ghetto-hack this, but I wouldn't recommend it. At the start of your file, execute a ""start transaction"", then register a shutdown function that will either rollback or commit. This would be an ugly hack, and cause excessive locking for no reason. You could also register an error handler to catch queries that failed, and then rollback & die at that point.

The more sane solution is to identify the critical spots in your code, begin transactions only when necessary, check the result of every query, and gracefully handle any failure conditions.",1524828711.0
GFandango,"mysql and postgres are basically the same in this regard there's no magic.

ORMs and database libraries sometimes make it less obvious.",1524842112.0
hahainternet,"Outbound firewall on either side? Dump traffic on the server and see if you're getting any actual connection request.

`tcpdump -npi any port 5432`",1524824041.0
smugbug23,"Stop postgresql, and configure apache (or any other well-known server software) to listen on 5432.  Now use your web browser (or whatever the client is for your chosen server software) on the client to connect to that port on the server.  If it cannot connect, you have a network problem, not a PostgreSQL problem.  If it can connect, maybe you have a PostgreSQL problem, but more likely it is still a network problem just a more subtle one.",1524869589.0
-markusb-,Do you have multiple interfaces - then you may check the routing of the server. ,1524891108.0
kenfar,"How does this ""acceleration"" work?  By completely replacing your redshift cluster with a bryrltry cluster?",1524838126.0
Shananra,"It's not clear from the article, but does the entire database have to fit in the GPU? If not, does it require the data in the active query to fit? 

What makes a lot of these GPU based databases non-starters for me is the fact that once I do the math on how much memory I need to be effective with my data then the performance per dollar metric plummets. It's usually by far the more expensive option to hit my performance targets. Not the first time I've looked at them.",1524870111.0
coder111,"Right, nice example- it shows how this thing can query each database individually.

What happens if you try to do a join between two tables on different databases? 

Is it able to do query pushdown? Is it able to analyze data distribution in each table on each database and make intelligent decisions on how to plan query execution?",1524803768.0
thelindsay,Grouping sets can do that. It's an option in your Group By clause. Docs: https://www.postgresql.org/docs/current/static/queries-table-expressions.html#QUERIES-GROUPING-SETS,1524761274.0
ful_vio,"Do you need the result to be exactly like that for presentation purposes? If you just want unique value on the first column and could accept a different format you could group by column 1 and array_agg column 2, something like:

```sql
select ""Column 1"", array_agg( ""Column 2"" )
from (
  /* your query here */
) as your_query
group by ""Column 1""
```

This will result in 

```
Column 1 | Column 2
-------------------
Value 1  | { Value 1, Value 2, Value 3 }
Value 2  | { Value 4, Value 5, Value 6 }
```",1524755668.0
therealgaxbo,"You could do something like:

    select case when row_number() over (partition by col1 order by col2) = 1 then col1 else null end, col2 from foo order by col2;

Where the order by clauses in the query and the window frame must match.

Buuut...this seems like you're solving the wrong problem; are you just trying to cram some presentation logic into an SQL query, or is there some other reason you'd want it formatted like this?",1524822719.0
smariot2,"In your trigger, do:

    NOTIFY channel, payload

Then write a daemon that does:

    LISTEN channel

and does something whenever it receives a notification.",1524697246.0
Engival,"Your regex split method doesn't work well. It fails on any single letter/single digit.

> SELECT add_delimiter('test1foo34');
>
> ""test_1foo_34""",1524578799.0
Trupal00p,But why?,1524608536.0
-markusb-,"How did you CREATE the tablespace?

How do you monitor your IO on disk?",1524472370.0
MonCalamaro,Is your write-ahead log directory on the HDDs?,1524473718.0
einhverfr,"What I think is happening is that you are writing a new relfilenode to the ssd, but your wal is on your main disk.  So you write the relfilenode there and the binary stream to your disk too.  That's normal.  Then at the end you unlink the one in your main disk and note that in the wal.  Note the entire table has to be written to the wal and there is some extra overhead there for read and write.",1524559202.0
merlinm,"Couple things to consider.  First, there is the 8kb toast boundary.  Above that, you have compression to consider and finally the 1gb upper bound.  

As a rule of thumb, I start to get nervous around 10mb.  ",1524484949.0
EvanCarroll,"Depends on what you're doing. If you're not doing anything, you won't find a performance degradation. If you're doing something -- you forgot to include it. 

An update to any column on a row requires rewriting the entire row, and sometimes if the columns are indexed -- rewriting the index entry. So if you have ARRAY[1,000] and you have to update any entry in that ARRAY or on the row, you'll have to write the whole thing. You start to pay a performance hit pretty quick with updates. Sometimes it doesn't matter that much though.",1524523732.0
JavaJuke,I'd assume it's dependent on your hardware and settings but I'd like to hear someone else more knowledgeable chime in. ,1524489841.0
einhverfr,"A few things I have noticed here:

1.  You have hard size limits. You also have TOAST limits which can cause strange performance hits in strange circumstances.  Note that `explain analyze select * from foo` does not fetch TOASTed attributes.

2.  GIN indexes can get very large with large arrays.  And this can cause I/O, RAM, and such bottlenecks.

As always it depends on access patterns.....",1524493537.0
unbihexium,Does Postgre have array implementation? Does it not depend on the language of the application? ,1524473231.0
amachefe,"PG main problem us that there is always an existing classification.

Large Enterprise uses Oracle or SQL server.
Startups used MySQL. 

Any people start from there before finishing out that there are other databases. 

I feel of shld stick to being what it is. Its already serving a niche and getting around the market quietly. When it's time it will just blow up.",1524330191.0
einhverfr,"Finally?  No.  That was probably in 1995 or so...  But the time has come for ZDNet to ask this question.

The thing is that before about 2005 or so, most usage is what I call ""dark.""  You meet a lot of people using it, but there is not a lot of publicity. Now it is front and center and everyone is talking about it.",1524408413.0
jakdak,"Nope, not until you can one of the leading ERP packages on it.   (And in general, until you start seeing packaged apps supporting it alongside Oracle, SqlServer, etc)",1524278793.0
zieziegabor,"see: https://wiki.postgresql.org/wiki/Replication,_Clustering,_and_Connection_Pooling

In Other words, there is no one right answer, there are a lot of choices and trade-off's you have to make.

If you don't want to invest the time, just pay Amazon, Google, etc (they all run postgres as a service for you)  Or pay Citus.",1524246722.0
amachefe,"This is very interesting, can this be easily added to the core?",1524233294.0
softwareguy74,Wow,1524708519.0
lykwydchykyn,"Previously, PGAdmin4 worked by starting up a local webservice and then launching a QTWebKit (or maybe WebEngine?) widget pointed to it.  So it was already ""running in a browser"", they just kind of hid that fact.  For whatever reason (I think maybe issues with the QT browser widgets), they decided to drop that and just launch it in whatever your native browser is.

IOW, there's no going back to ""standalone"".",1524234212.0
SemiNormal,"Ahh... POS Admin4

I found a discussion on this issue:
https://stackoverflow.com/questions/49919149/unable-to-connect-to-postgresql-with-pgadmin4-readonly-attribute-error

And

http://www.postgresql-archive.org/pgadmin4-3-login-failure-td6017606.html



If any other client works, why not just use any other client? I gave up on pgAdmin and switched to dbeaver a while ago.",1524245060.0
torkild,"Still having issues?

Can you provide the table definition for the target table, including any index and trigger definitions?

Also, how are you currently loading the data? What commands or tools are you using? Can you provide a small sample of what the data looks like (maybe a few lines from the file)?",1524191680.0
Darkmere,"First up. Post your _script_ that you use to load data.

Do not use an interactive pgsql prompt for this, but an SQL script.

Invoke it using:

`time pgsql -f myscript.sql`


This allows you to show a baseline of how it works.

also, the config you posted is incorrect, it says work_**men** not work_**mem**  in two places.

",1524209094.0
swenty,"Not that I know of. But you could do it in two steps. First copy from file into a loading table, then merge the rows from that table into your target table, using ""insert ... on conflict update"".",1524190364.0
iTroll,It might work in an INSTEAD OF INSERT TRIGGER. ,1525010641.0
ruertar,"This is really general -- to much so for me to give much advice.

Maybe one tidbit that I've found is that I always include a serial `id` column in every table if I think I need it or not.  It at some point I'll end up needing it.",1524153040.0
swenty,"Read a book on relational database design. It will tell you why arrays aren't normally used in relational databases and help you to understand how to store array type data in what is called a ""normalized"" design.",1524170601.0
kellenkyros,"I don’t think there will be a huge performance impact. Yes, it’s main agenda is to reclaim space I believe.",1524170730.0
kellenkyros,"Yes, it doesn’t have to.",1524319031.0
einhverfr,"Here are some basic points:

1.  Start with 3NF at least for your designs.  Sometimes you have to break even 1NF but that is not where you want to start.  PostgreSQL can break 1NF sanely but there are costs that you want to see if you can avoid.... (tables)

2.  Learn carefully what the implication of CTEs are on query optimization. (queries)

3.  Learn what indexes can be used on what kinds of searches *and why* (queries).

4.  Learn when SQL language functions can be inlined and what this means.  (queries)

Most of this is about learning rather than about adopting rules.  At some scales the rules break down anyway.",1524408977.0
CaffeinatedT,"V vague question. But my advice for Arrays in specific as I work with them a fair amount, avoid them unless it's really something that's really only going to be queried once in a blue moon but you definitely need to store it relationally. Unnest with normal querying is a pain in the arse. I use it at a company where the main level of granularity we want 99% of the time is an individual customer email/session then I store the array of what they actually looked at in that session for the rare occasions we really need to see it. Then I provide the query for unaggregating that myself because it's a PITA to do. If end users didnt want it instantly accessible I'd be be storing it as a log and using other methods to get at it. 


Outside of that....don't not set a primary key? Don't set ridiculous numbers of indexes on tables that you're inserting to constantly? It's a very vague question that is kind of 'how to be a good database owner' ",1524155527.0
kellenkyros,"DO: In production, if you are expecting a lot of data write and periodical delete of old records from the same table then at some point you will end up with low disk space even though you are deleting records. 
You have to do vacuum on those tables periodically to reclaim space. For more details please PG offiicial documentation (https://www.postgresql.org/docs/current/static/sql-vacuum.html)
",1524160688.0
Whiskey_and_Sarcasam,"PGAudit should do exactly what you need it to do:

https://github.com/pgaudit/pgaudit

I recently implemented it and it was super-simple.  The only downside was having to grep the appropriate pg_log for results - the data aren't natively stored in a table and available for querying.  Though I'm sure you could write a parser agent that runs continuously to parse the logs and store them in a separate DB.",1524145007.0
stlbucket,"https://medium.com/@tobyhede/event-sourcing-with-postgresql-28c5e8f211a2

perhaps some ideas for you in there",1524140333.0
thelindsay,"This kind of jumps ahead to ""how are you going to use this data"", but you could consider implenting the audit trail using a data warehouse pattern similar to ""slowly changing dimensions"".

Essentially, the audit trail (probably) isn't directly relevant to the operation of the main application. But it is relevant for reporting, such as time trends analysis, or regulatory compliance reports. So it could just exist in the report database.

What this could look like is a copy of your database schema, but with some modifications: a new primary key column, a new unique constraint of (old_pk, audit_time), and updated relationship structures to suit. That way you can work with the data more or less as normal, without needing to unpack json data or pivot EAV data to a type (either wholesale, or ad-hoc in queries).

The best way to maintain this (or any) audit trail is to use triggers, since they act at the time of data modification. But you could also modify your app to write to both places instead of one.",1524160124.0
ilovepudding,I personally go the trigger route. https://wiki.postgresql.org/wiki/Audit_trigger_91plus and https://wiki.postgresql.org/wiki/Audit_trigger are good starting points.,1524178789.0
minaguib,You did initdb as a different user,1524093783.0
jhford,But how can I fix it?,1524112527.0
francisco-reyes,"What OS and what version of Postgres? For example in the past FreeBSD did not have postgres user when you installed form port/package, but there was another superuser instead. Don't recall right now what it was, but it was something like pgsql.

Possible whatever distribution you are using creates a different super user.",1524165989.0
jhford,It's version 9.3 on Linux Ubuntu running on an instance (virtual machine) at AWS.,1524169025.0
Abstrct,"Wow, congrats team! I know a facelift has been discussed for a long time. Nice to see all the feedback over the years put into play. ",1524072439.0
sirluiss,Awesome job :),1524075172.0
denpanosekai,looks amazing!!,1524075681.0
This_Is_The_End,"It's looking good, but where is the link to the docs? The link to the docs is quite hidden. python.org is an example how to do it",1524078177.0
doublehyphen,I like it. The site needed a face lift and improved usability and it seems from a brief check that we got both.,1524086084.0
softwareguy74,Wow.  An updated web site that doesn't suck!,1524187232.0
alcalde,"I'm not feeling it. It has much bigger fonts, lots more whitespace... and less content. Reminds me of how disappointed I was with Python's web page update. Where's all the classic technobabble? If we're making changes, why not an awesome sales pitch that compares it to all the other databases out there? ",1524096765.0
therealgaxbo,"Had a large app and DB that had only ever dealt with money in GBP.  Suddenly we needed to support many different currencies in all parts of the app.

Rather than just adding a `currencycode` text column, I created a composite type that stored the amount and currency together.  After all, ""5 dollars"" is an indivisible value in this context - ""5"" on its own is meaningless.  This made it impossible for existing or new code to ignore currencies and continue to assume GBP everywhere, and creating the appropriate operators and aggregate functions makes it just as easy to work with in SQL, with the benefit that if you write a report that tries to `sum()` a bunch of mixed currency values, it will raise an exception.  Adding a type mapping in the application means that reading that one field populates an appropriate currency-aware Money object automatically.

I fully expect a reply to this telling me there's a far better solution, but it's worked really well for the past few years.",1524070522.0
EvanCarroll,[stdaddr](https://postgis.net/docs/stdaddr.html) would be a good example of one used everywhere.,1524071077.0
simcitymayor,"The biggest use I've seen for composite types in PostgreSQL is for arrays of complex types inside a row as a sort of sub\-table.

Now, you would only want such a thing in your table if you only access the sub\-table when you're already accessing the table itself, *and* when you do need one array element you also need most if not all of the other array elements.

Here's a nice talk about how to go to extremes with arrays of composite types. If it's worth doing, it's worth over\-doing:

[https://www.pgconf.us/conferences/2017/program/proposals/toasting\-an\-elephant\-building\-a\-custom\-data\-warehouse\-using\-postgresql](https://www.pgconf.us/conferences/2017/program/proposals/toasting-an-elephant-building-a-custom-data-warehouse-using-postgresql)",1524451315.0
Darkmere,"From the documentation:

> COPY is fastest when used within the same transaction as an earlier CREATE TABLE or TRUNCATE command. In such cases no WAL needs to be written, because in case of an error, the files containing the newly loaded data will be removed anyway. However, this consideration only applies when wal_level is minimal as all commands must write WAL otherwise.

https://www.postgresql.org/docs/current/static/populate.html",1524049527.0
torkild,"Are there indexes, constraints, or triggers set on the target table? If so, dropping or disabling them while running the copy could significantly improve the data load performance. Once the data is loading, you will then need to recreate them which will have some additional overhead, but in my experience it should still be faster overall.",1524050546.0
789um789,https://github.com/timescale/timescaledb-parallel-copy works well for me when using timescaledb hypertables and should help even for normal tables due to its ability to bulk inserts and do parallel inserts.,1524060249.0
xmen81,Try pgtune? ,1524062441.0
jakkarth,I'd suggest you start by researching window functions and group partitioning.,1524000596.0
mokadillion,"Working PostgreSQL DBA here. 

The documentation is amazing read it and read it again. From start to finish it’s a great guide on managing a Postgres instance. 

Absolute must know topics. ;
WAL archiving. 
Streaming / logical replication. 
Failover methods. 

Connection pooling.: 
Pgbouncer
Pgpool

Configuration:
Postgres.conf file
Pg_hba.conf file. 

On top of this. The overlap between Linux filesystem and server admin and PostgreSQL admin is pretty blurred so get some Linux in you too. 

Master this and you will be good to take on most installations. ",1523989509.0
stlbucket,"the post in my feed right before this one:

""Unsuccessful people make decisions based on their current situations. Successful people make decisions based on Where they want to be."" 

https://www.reddit.com/r/quotes/comments/8cwolo/unsuccessful_people_make_decisions_based_on_their/?st=JG3WD9OL&sh=85387b06

...seems relevant and that you are headed in the right direction with a good attitude.  

",1523983699.0
pstef,"Join #postgresql on Freenode, observe what problems people describe and learn from solutions that the channel suggests.",1523981904.0
jlrobins_ssc,"* Read through the well-written documentation from start to finish a few times, noting differences between what you know from Oracle versus PG.
* Install and start playing! Re-implement stuff you've modeled before.",1523979701.0
Hyasynth,"There are some REALLY good presentations on youtube, many of which are from the core developers and experts in the field.  They range the gamut from beginner to expert and I would highly recommend them.  Many of them touch on core postgres concepts.


Here's a couple channels:

https://www.youtube.com/channel/UCer4R0y7DrLsOXo-bI71O6A/videos?disable_polymer=1

https://www.youtube.com/user/postgresopen/videos?disable_polymer=1",1523995490.0
3bodyproblem,"Just a reminder, OpenSCG still maintains pgAdmin 3.x LTS which doesn't suck.
https://www.openscg.com/bigsql/pgadmin3/",1523896718.0
Doza13,4 sucks so bad.  I can't imagine they've overhaulled it enough.  I use pg3 LTS.,1523912211.0
,[deleted],1523895283.0
postgrescompare,"It now loads in your system's default browser and while that does feel a little strange (the icon in your toolbar is that of the browser), it appears to greatly improve the responsiveness of the UI. 

In all fairness pgAdmin 4 is not really pgAdmin 3++, it's a reboot. I expect the idea is that in time it surpasses where pgAdmin 3 was at and right now we are seeing the incremental releases, if it were labelled ""beta"" or similar one might be more forgiving before declaring it as terrible.",1523972135.0
jossser,"They made it cross-platform but they better bundle it into virtual box image with their favorite OS instead of this and it will be faster.

",1524058013.0
thelindsay,"You could add an identity (bigint serial) column to the table and use that for ltree labels. It doesn't have to be marked as a PK, but if you want to you can mark it Not Null & Unique anyway (in which case it can be referenced by FKs). The number is then guaranteed unique and probably small relative to a hash.

2 things to note. 1st, an upsert consumes sequence values if the update happens, since the aborted insert runs nextval(), so you might get gaps in that case. 2nd, if that's a problem you can modify the sequence to set allow ""cycling"" i.e. return to zero and try to start handing out new values (but your app must then handle errors from conflicting IDs, and move the maximum up every so often). Or you could implement a maintenance function that occasionally adds a new identity column, rebuilds the ltrees based on that, and drops the old identity column.

It might be possible to get even more compact than just an integer serial, by using a base 62 counting scheme (A-z0-9) instead of just 0-9, e.g. by considering A < z < 9 and 9R < 9g < 99. But I'd only consider something confusing like that if the size if the identity numbers were causing max path length issues in ltree.",1523901489.0
greenspans,"What about the underscore idea, then a dash, then the first 8 characters of the MD5.

Would this be something you could encode with json? For more complicated hierarchical data, it may be better to use neo4j instead of postgresql.",1523868728.0
ptman,"Have you thought about these instead of md5:

- [base58](https://en.wikipedia.org/wiki/Base58)
- [base36](https://en.wikipedia.org/wiki/Base36)
- using _ as escape:
  - how long are your natural keys?
  - `foobar` -> `foobar`
  - `foo_bar` -> `foo__bar`
  - `foo/bar` -> `foo_2Fbar`
  - `foo:bar` -> `foo_3Abar`",1523879591.0
fullofbones,"Pgpool only does load balancing mode while using physical streaming replication. It also must be in control of the failover process, or at least think it is. This is because it needs to be reasonably certain which node is the primary, and which are replicas. It's not compatible with any sort of logical replication, whether it's pglogical or the built-in mechanism in Postgres 10.

Even if it were, it just sends the whole query to the target node. All tables, views, functions, etc., must be present wherever it runs. ",1523799075.0
smariot2,"It sounds like you have mojibake'd your file. 0xc0 would be À, which is a pretty common character to see when your text was originally UTF-8, but then edited as ISO-8859-1.",1523806266.0
fstak,"In Linux, run this :
File <file>.txt

That should tell you what encoding the file is in, and you can try to add that encoding to your copy statement",1523813400.0
fstak,Head thing.txt > smallthing.txt,1523815198.0
,[deleted],1523737406.0
Baron_von_Retard,"This has nothing to do with Postgres, so I picked a bunch of random answers. Don’t spam. ",1523728476.0
torkild,https://www.postgresql.org/docs/current/static/sql-copy.html,1523654962.0
r0ck0,"I'm not experienced enough to really answer, but you've got no answers so far... so here's my mostly uninformed guess...

Unlikely to work on UNC paths, as this is not direct access to a block device/filesystem.  Even if it did work, I would assume performance would be horrible.

I've never dealt much with hardware in general, but for a database you likely want some type of storage that works like a normal local block device/filesystem.  i.e. Not using any network sharing protocol (which is file based, rather than block based).

More generally on the idea of ""on-prem option for our SaaS solution"" - I'm sure you're already aware, but this is a huge undertaking in terms of having to not only deal with support of servers you don't control... but you'll also be dealing with bug reports etc that aren't even caused by your software at all to begin with.  

Given that you're here asking this question... you sound like one of the main tech guys that are going to be responsible for this in part at least... your company may not have enough resources to deal with all the support issues that are going to come up.

Personally I consider supporting software on hardware you don't control to actually be more work (time/money wise) than doing everything yourself.

Even more broadly than the tech side of things, this is a very different business model that also requires more non-technical resources and management etc.  It's a very big undertaking with lots of potential downsides.  So just make sure that everyone is aware of them all at least before they get too excited about the idea.  

I would frame it generally as not too far off setting up another business entirely... almost.

Plus there's also the obvious tech issue of customers or competitors reverse engineering your code some time in the future.  Probably not likely... but a good point to mention to managers that don't understand a lot of the other potential issues (many of which could be taken as you not wanting more work to do).",1524027638.0
_ntnn,"Multiple options.

1. Provide an installer for a few supported distributions that installs and configures postgres for your needs.
2. Provide detailed instructions so a db admin can set the database up.
3. Provide consulting services to set up postgres.

Or a combination of those. 

All of these work well, but your best shot would be an automated installer/upgrader. You'd still need to provided support for the installer though.",1523630598.0
Amacvar,"Your client is not in the minority (yet).

In addition to deploying, RDS also provides backups, failover, upgrades, monitoring, security, etc.

Unless the client is willing to take responsibility for all of that, I would recommend suggesting they use EnterpriseDB, CrunchyData, Command prompt, etc

In case you’re in https://www.postgresql.org/support/professional_support/northamerica/",1523666039.0
shif,"make an installer that bundles an script that changes the needed configurations, test thoroughly. ",1523633987.0
cyanydeez,hows the speed compared to pgadmin4?,1523584128.0
Darkmere,"No.
By default, postgres only allows the unix system user `postgres` to connect to the database.

All other database users  need to be added to the system by hand.  The ""username"" field here is not the system user ""root"" but a non-existatnt database user ""root""

Postgres doesn't syncronize users with the system database, but keeps it's own list of users, with it's own list of permissions that the user is permitted to access.

Postgres by default doesn't allow _any_ connection over the network, only permits you to connect to the unix domain socket of the postgres server, and that is only permitted to be accessed by the `postgres` system user by default.

You probably want to find a ""getting started"" howto for these concepts.",1523454358.0
cachedrive,DO NOT REINSTALL. ,1523551634.0
mgonzo,"So when you start a new docker instance and restore the base backup it uses the timeline from that base backup, so checkout from that same link you have the 'recovery_target_timeline' section. In there it tells you the default behaviour is to just stick to the timeline from when the basebackup was taken for wal recovery. so the second time you are doing the recovery with the same base backup you are back on the first timeline, but you have a mix of two different timelines. You just need to add the option

    recovery_target_timeline = 'latest' 

and it will use all the timelines.

Typically what you would do is after a recovery you would take a new basebackup. But you don't have to, it all depends on your use case.

Also welcome to PG land. It's a lot of fun here.",1523386958.0
doublehyphen,"I recommend using the Sequel gem instead, which uses the pg gem for low level operations.",1523321566.0
dGhleSBoYXZlIG5vIHdv,Look at `PG::BasicTypeMapForResults`,1523297735.0
mlotfi2017,"I found a solution :
https://github.com/ged/ruby-pg#type-casts",1523300199.0
mage2k,"The pg gem is essentially an interface driver between your ruby code and the libpq client library.  When you execute a query with it it simply sends the query string to the server via libpq and reads the result set back.  The pg gem knows nothing about the tables you're querying or their columns' data types.

If you want your results to be automatically cast data types that reflect the tables' columns' types on the client side you'll need to use something like ActiveRecord, define models for the tables you're querying, and access the database via that stuff.",1523294120.0
xenoarchaeologist,"Have you tried including ""clientcert=1"" at the end of that line in your pg_hba.conf? Edit: What version of PG are you using, btw?
Edit: You should be able to see whether or not the SSL is being used or working in the logs for PG during the exchanges, I think.

Edit 2: I misunderstood the question. My apologies.",1523344173.0
-markusb-,"Activate connection logging. 

psql> alter system set log_connections to ON

Restart database. Then you will see the details.",1523373083.0
-markusb-,"Please show the log entry of the connection. Why are you sure that the connection is not encrypted?

hostssl means it works with SSL (forced - no insecure connection allowed). 

So if you have a connection it is encrypted. ",1523360458.0
therealgaxbo,"What do you mean by 'the row matching the data'?  If there's a foreign key reference, why not just create the key with `on delete cascade` and let the DB do all the work for you?

And if the 'match' condition is more complex than just a foreign key so you _have_ to use a trigger, why are you creating and removing it repeatedly - why not just leave it there?",1523276028.0
doublehyphen,"It is generally a bad idea to dynamically add and drop triggers because to drop a trigger you need to take an exclusive lock on the table (and to add a trigger you need to lock out all writes to the table).

Why can't you just leave the triggers there?",1523276019.0
jonr,"I swear, sometimes I think the people who find out these exploits and use them from crypto currency minng, would make more money as security consultants. ",1523285004.0
dark-panda,"Have you considered refactoring this into multiple rows and then using a recursive CTE to traverse the paths? I’m not sure how the performance would be on such a large tree, but it’s worth considering. To get around the size of your ltree indexing issues would probably require modifying the source and recompiling Postgres, is some refactoring is going to be necessary if you want to avoid that whole thing. ",1523193517.0
einhverfr,"The `./configure` script has an option for block size so if custom compiling is an option you can use this to get larger index leaves.  The problem is you have a value that overflows a leaf node.  The maximum block size is about 4x what the default block size is.  Note that if you uyse larger block sizes you probably also want huge pages.

This being said, I am not actually sure how well GiST will perform in this case.  I generally find that GiST indexes sometimes have funny corner cases where you get odd performance limitations due to having to scan an entire index to determine which leaves apply.

Another option is you could use an adjacency list of ltrees for cases where you have extreme nesting.  This owuld limit the number of recursions....",1523259051.0
thelindsay,"There's a lot to consider for creating indexes which can affect performance. 

For starters, does the index need to exist? If you're always selecting all values, an index would not be used, and would instead just slow down write operations and consume disk space.

Secondly, does the index need to include all values? If you're always selecting common values (even over a few percent of unique), the index might not be used anyway. For the original problem, you might have success in excluding very large paths from the index.

Thirdly, there are column expressions and column operator classes that can further tune the index to the fastest access of data, but it is hard to give general advice at this level of detail since there are many combinations and it depends on usage patterns.

There's also system-wide and session tuneable query parameters, column statistics, system resources, etc. that can affect how a query is planned. So an index isn't the only thing that can determine performance.

If you haven't already, read up on the 'explain analyze' command and what its output means. There are tools online to help visualise the output. It essentially is a way to know how a query will be (or was) executed.",1523276097.0
flyingmayo,"What are the design objectives that have lead you to decide upon a matview?

Your comments make me wonder if you're chasing performance here or if you're just using matviews a way to isolate tenant access to data? ",1523140936.0
thelindsay,"It sounds more like a tree than a matrix. If the depth of the tree is fixed / limited then it'd be best to model it with normal tables e.g. users, child_users. If the depth isn't fixed then there's ltree extension: http://patshaughnessy.net/2017/12/11/trying-to-represent-a-tree-structure-using-postgres the label paths can be very large ~65kb

If it really is a matrix there's a cube extension, but it's limited to float data and 100 dimensions: https://www.postgresql.org/docs/10/static/cube.html

There are non-extension options using adjacency lists, nested lists, etc. They can work fine too but require some care how they are queried / maintained to make sure they are valid (it sounds like this is a problem you've already experienced), but it's a well trodden path so example stored procedures are out there. This article is a little old but introduces these options decently: https://explainextended.com/2009/09/24/adjacency-list-vs-nested-sets-postgresql/

Note that Postgres does have recursive CTEs so you won't need to go to MySQL for that https://www.postgresql.org/docs/current/static/sql-select.html

Finally it might be worth considering an XML column and using xpath etc for traversal, or a specialised graph database such as neo4j.",1523065360.0
EvanCarroll,"Could you provide sample data, and perhaps take the question to a more appropriate place (since it seems detailed) like dba.se? I mean, you're welcome to ask it here but this format is shit for q/a.",1523078527.0
einhverfr,"I will be speaking there, and presenting ""PostgreSQL at 20TB and Beyond"" about our distributed analytics infrastructure at Adjust GmbH, as well as delivering training on advanced data modelling techniques (non-1NF, semi-structured data, functions and indexes, etc).",1522999965.0
jakkarth,... you failed to create any relations?,1522959829.0
cachedrive,"Turns out I need to also do:

    GRANT USAGE ON SCHEMA wtf TO mike;

This has to be done along side the GRANT ALL PRIVS ON ALL TABLES IN SCHEMA cmd.

Thanks for down voting :)",1523287845.0
einhverfr,"you didn't set the search path when logging in as mike or such?

Or you didn't log into the right database.  Or mike doesn't have USAGE permission on the schema.
",1523021562.0
felixge,"I'd say it's extremely unlikely that this kind of select query will block any other queries. Readers should never block readers/writers in MVCC.

> That query sometimes blocks our other queries

How have you determined this?",1522953548.0
dcalde,"I used to experience this problem as well. It is especially annoying that if it blocks and you kill the pgadmin process, when re-opening pgadmin it will go back to the same table it was on last that was causing it to block.

When it blocks, connect using psql and terminate the pgadmin query.",1522983751.0
whisperedzen,"It reads like an advert to entice management. Yeah, you can transform stuff in the database, SQL lets you do that since forever. And yes, now that storage is cheap you don't have to be so picky. Feels like it is trying way to hard to sell ""the next big thing"".     
(also, the article doesn't mention postgres at all).",1522943442.0
flyingmayo,"2010 called....

This ""evolution"" happened years ago.

The customers I worked with were having interesting discussions about ETL vs ELT in 2008 to 2010 as the processing and storage capabilities of their DWs were scaling out (cheaply) horizontally.  

By 2012-2014 these were no longer novel considerations.  Most companies did some very rough transforms on initial load but most of the interesting transforms happened within the data store itself.",1522967240.0
shanestillwell,"Look into [heroku postges](https://www.heroku.com/postgres)

It offers some reasonable PG plans for your app. ",1522922322.0
v_95,"Hey man, I’m just curious. Is there a reason you don’t use ElephantSQL? I’m in the same situation as you, looking for an online hosting solution for a PostreSQL database.",1532831910.0
therealgaxbo,"You only posted the `explain` rather than the `explain analyze`, which would usually be far more useful, for future reference.

But not needed in this case.  Here the problem is a mismatch between `notificationtodesc_fk1_idx` and the clause `ORDER BY 1 DESC NULLS LAST`.  The direction of the index  and the position of nulls don't match.

Either recreate that index with `desc nulls last`, or if you want to leave the existing structure alone (and clearly the vendor have a penchant for duplicate indices...) just run:

    create index notificationtodesc_notificationid_nullslast_idx on notificationtodesc (notificationid desc nulls last);

Your query should drop to sub-millisecond times.",1522851629.0
CC_DKP,"It looks like you got it fixed, but a couple tools to keep in your pocket for when you run into stuff like this:

As a general tool, I like installing [pghero](https://github.com/ankane/pghero) every once in a while to see how things are going, then take it back out (since it does take some ram/cpu to run, and I find I don't need it 100% of the time). Especially if you are new to postgres, it's good about identifying missing indexes, slow queries, and finding unused indexes. It's no replacement for experience, and yes many things can be optimized better than it suggests (such as adjusting index orders), but it's a good starting spot.

There is also a really nice [Explain Analyze visualizer](http://tatiyants.com/pev/). I find the table layout it uses makes a lot more sense for me and helped me understand the query plan optimizer and various bottlenecks better than the wall of text output could.",1522860340.0
nkydeerguy,To be completely honest I have no clue where it gets that reputation from. I much prefer Postgres over MySQL MsSQL or NoSQL. The syntax and organization just makes sense. It’s also a lot more feature rich which could be a learning curve but I see it as helpful. ,1522842488.0
atani,"Maybe leftover from when most MySQL installs used MyIsam backends.  You would get a lot fewer errors when interacting with MySQL with MyIsam than with postgres so it felt like you “got up to speed” quickly.  Your data, as a newby in particular, tended to end up in terrible shape though.",1522852028.0
GFandango,"it doesn't have a steep learning curve, it's mostly standard sql anyway.

that reputation could come from a few bumps in the road like managing database access permissions (pg_hba, roles, grant).

often beginners get stuck behind some errors like ""could not connect to database as user blah"" or ""permission denied"" and things like that.",1522854989.0
cachedrive,I've used MySQL and PostgreSQL a bit in deployments from work and have to say PostgreSQL has been much easier and enjoyable to work with. I don't find the learning curve steep at all.,1522848101.0
scttmthsn,See what you think after reading the [PostgreSQL Tutorial](http://www.postgresqltutorial.com/).,1522852704.0
platohut,"You don't have a rich GUI environment for postgres. But if you overlook that, it is easier than other RDBMSs in my opinion",1522855454.0
jlrobins_ssc,"A sore point in the past was that you might well have needed root access in order to reconfigure arcane SYSV shared memory limits, then reboot. This has been fixed for the past few major releases.

That plus the lack of a Windows port allowing easy experimentation. That was solved many releases ago however.",1522856719.0
dsn0wman,"If you have experience with Linux or Unix and any databases that run on those platforms PostgreSQL does not have a steep learning curve.  
  
",1522859917.0
ies7,"> given that my data is small and database is not a big part of my job, I've stuck with sqlite  

> I don't have hundreds of hours to invest in learning something.   

You've done nothing wrong with choosing sqlite.    
My team spent weeks before deciding whether we should go with postgres or any other db/nosql alternative.  
But it because it is a big part of our job and the data is in terrabytes (rising about 70-80gb/days).  


Now back to this steep learning curve,  
the most steep learning curve I had when installing postgres for the first time is when I want to access postgres from other computers.  

It takes around 1-2 hours (because I kept on reading through several sites instead of applying the 1st sugggestion).    

The 2nd hardest is when using python(pyscopg2) and realised that I can't supply password in the connection string.  

Is it steep? Yes...for those who can't be bothered to google/stackoverflow.   
Is it steeper if compared to MySql/Mariadb/MSSQL/Oracle? Debatable.   
Is it steeper if compared to Sqlite? YES.  


",1522905235.0
mrtinkyholloway,"I find sqlite frustrating. I have mysql and postgres in production. I much prefer postgres. If you can google, you’ll be fine.",1522840984.0
greenspans,"I would think mostly from the setup. sqlite you can get up and running with a tiny library. Mysql is usually a quick install then ""mysql -u root"". Postgres... install it, ok init the db, ok now login to postgres user and setup your users and databases or modify your conf to work with an identity provider... no no not like that you're fucking it up. Ok you got it kind of but we're going to have to redo the permissions later because you allowed too much, ok now just make a schema and add some permissions for good practice. A schema. Don't worry about what a schema is just make one and then you can make different access levels later. Oh you're going to sqlite? oh... ok....",1522844828.0
mokadillion,"Set up CAN be more complicated if you go for a production set up , but if you’re beginning the basic install on a local machine is super easy. 

The documentation is some of the best and easiest to read I have ever seen for any tech product. ",1522884568.0
mozumder,"I thought the PostgreSQL ""/"" commands were kinda weird when I came from MySQL.",1522895015.0
einhverfr,"The reputation is legacy.  When I started with PostgreSQL in 1999 there were certain important features that were missing and later added (like `alter table drop column`).  Ok now I feel old....

As PostgreSQL has continued to develop most of the steep parts of the learning curve have gone away.  And now you have a relatively standard learning curve which starts off relatively ok and goes on forever.  Basically you will be up and running very fast and the hundreds of hours will be spent gradually.

For example, I just spent an hour going through how allocation sets and memory contexts work, but the number of PostgreSQL folks who need to know of these things is very very small (this is internal memory management and mostly becomes important if you are doing exotic things with C-language extensions).",1522930553.0
einhverfr,"Oh and you can easily invest hundreds of hours mastering SQLite, if you want to.  Custom functions, custom aggregates, auto-serialize/deserialize, etc.  It's really powerful as an embedded db.  But most people don't even scratch the surface.

Same with PostgreSQL.",1522931829.0
HotKarl_Marx,With great power comes great responsibility.,1522844434.0
neko4,"Posrgresql has more functionalities of SQL, which are based on Standard SQL. So, you can make use of them  in Oracle, MS SQL Server.",1522842725.0
lampshadish2,There’s a ton of config options.,1522884238.0
exhuma,"This is the first time I hear that that PG has a ""steep learning curve"".

If anything I found it *easier* than MySQL. I've switched to PG after several years of experience with MySQL and what struck me (and the reason why I stuck with PG) is that there are far less strange surprises in PG. Things ""just work"" and you have so many more useful data types which do ""the right thing"".

The only thing that irritated me is the user management. It still annoys me to this day to have to define users twice. Once in `pg_hba.conf` and once in the DB itself.",1522911829.0
urcadox,"I guess it's a bit more complicated to setup than MySQL / MariaDB / ... and even more so than SQLite obviously.

Using it is not more complicated overall: some stuff is easier, some stuff is harder (if you're using auto increment in mysql, you will find it harder with PG for example).",1522842484.0
therealgaxbo,"I've never run PG in AWS so this is just some more general advice:

* Are you running all the writes in their own transactions, or within a single transaction?  The latter should give much better performance, especially if the IO system is relatively high latency.
* If you _can_ run them in one transaction then you can also use prepared statements so that each query only has to be parsed/planned once which may help a small amount.
* If you _can't_ run them in one transaction (because of how your data is coming in), try setting `synchronous_commit=off` to get similar performance characteristics.  But see [the docs](https://www.postgresql.org/docs/current/static/runtime-config-wal.html#GUC-SYNCHRONOUS-COMMIT) to understand what that means.
* As it's being stored as json, Postgres will have to parse the entire 500kb each time.  If you know the json is well formatted and don't need the json query operators, you could try storing it as `text` instead to avoid the parsing cost - with the risk that you're losing data integrity guarantees.",1522834972.0
platohut,Try moving RDS instances. Sometimes the instance that is assigned can be faulty/buggy.,1522855540.0
dopperpod,https://www.postgresql.org/docs/9.6/static/sql-copy.html,1522785139.0
therealgaxbo,"That view is the cumulative values over the lifetime of the database, so you shouldn't expect the values to go down unless you call `pg_stat_reset()`.  As long as the value isn't increasing, you're OK.

Yes, work_mem being too low would cause temp files to be used, so it's entirely possible you have fixed the issue already.

If you enable `log_temp_files` in postgresql.conf, your logs will tell you which queries are creating temp files.",1522771032.0
wtf_apostrophe,"Awesome. Having come from Oracle, the lack of MERGE surprised me.",1522784828.0
doublehyphen,A word of warning: this patch might get reverted. Not everyone thinks the underlying design is good enough to ship and given how close we are to the feature freeze MERGE might not make it.,1522899172.0
Erudition303,I really hope this makes the cut for 11. Would make my ETL process *so* much easier to manage.,1523291205.0
platohut,"Docker is not ideal for any database, not just Postgres. This is mainly because of the additional volume you'd need to pass around containing the actual data. ( https://docs.docker.com/storage/ ) at which point unless it is the db configuration that you really want to dockerize, running Postgres in docker provides no additional benefit, and even slows down common processes compared to running them outside of docker. ",1522767320.0
DataChomp,"generally speaking, i think the rule of thumb is still use DBaaS offerings if you are going all in on containers.
",1522767057.0
TiCL,">hot button issue

Only if you are mentally ill.",1522720940.0
jlrobins_ssc,"Ugh. I bet this bites MySQL as well at least.

     ""There's nothing we can do about that in userspace
        (except perhaps abandon OS-buffered IO, a big project).""

Oracle's traditional skipping of kernel-managed buffered IO finally sees an upside.",1522690149.0
felixge,"It’s 2018, yet: https://imgflip.com/i/27lmgg :(

Related: https://danluu.com/file-consistency/",1522690892.0
doublehyphen,"This should affect almost any application which uses fynsc.

The alternatives are:

1. Do not use fsync and use direct IO instead
2. Exit on failed fsync
3. Keep dirty buffers in the application and redo the writes on fsync failure (meaning all writes need to be buffered twice). PostgreSQL should in theory be able to rebuild the writes from WAL, but that sounds like a mess.",1522695741.0
ilovepudding,Did I read this properly that on FreeBSD this is handled safely?,1522710364.0
jiphex,I believe you can do “INSERT INTO .... RETURNING id“ then use the returned value in your follow-up queries (see [the docs](https://www.postgresql.org/docs/9.5/static/sql-insert.html) ). As long as you do all the queries in a transaction it’ll be consistent.,1522564076.0
anonymous_subroutine,"> though then the sequence generator will be out of sync.

See *setval*

https://www.postgresql.org/docs/9.1/static/functions-sequence.html",1522556971.0
jk3us,Something like this is one approach: https://stackoverflow.com/q/42941006,1522559155.0
minaguib,"I think you'd be well served by hiring a DBA who can take a look at your data and workload.

There are too many variants to have a reasonable answer on a reddit post.  How often is new data coming in ? Is it mutated ? Is it mostly read than written ? Are you doing any aggregation or cubing ? Is the existing DB optimized ? Is a columnar DB a better fit ? etc.....

Also... ""a PC with one CPU"" seems underpowered for any serious querying.",1522430507.0
kenfar,"DB2 and postgres are pretty compatible databases.  As long as you're not using very many extensions to the sql standard or doing too much with stored procedures a migration can be very simple.

On the other hand, a 4TB dataset on a 1 CPU (1 core?) server already experiencing 30 minute queries on db2 is unlikely to be faster on postgres.  Especially with a lot of configuration tuning, maybe re-modeling the schema, definitely tuning the queries, and perhaps more partitioning (monthly? daily?).",1522434756.0
SignificantPaper,"My suggestion would be to give [db2topg](https://github.com/dalibo/db2topg) a try. It is a set of scripts that can convert your DB2 database into PostgreSQL. I have used it before and it worked well. 
Perhaps dump a subset of your data first, so that you won't have to spend as much time.",1522500897.0
AQuietMan,">  believe performance of DB2 should handle my case but I failed to optimize it.

Do you really think switching to PostgreSQL will fix that problem?",1522512903.0
Hajajdk,Any reasons?,1522430467.0
Hajajdk,"The budget  constraint makes me choose a PC, I know it is lack of capacity to meet heavy and complicated query.But for personal use, perhaps it is my best choice now.The best way is to hire a DBA, and I hope I can do so in the near future.New data comes every month and written once a month.I keep the date in their original shape.The existing DB is a little optimized.As the table is huge, I create the tables with partition by quarters.A columnar DB maybe a better choice,but perhaps it will consume a lot time to learn new skills to convert existing database to columnar DB.I hope I can spend more time on analysis data.
Thank you for your immediately reply.",1522432288.0
pavlik_enemy,Don't. Just don't.,1522430381.0
jimminy,You really should consider upgrading your OS.,1522398890.0
fullofbones,This seems like a weird reinvention of data cubes. And data cubes can be addressed with straight SQL. ,1522283304.0
ful_vio,"How this compare to using lateral joins? I've got the book ""Mastering PostgreSQL in Application Development"" By Dimitri Fontaine and he argues that lateral join is the most efficient way to compute TopN queries in bare postgres. What does this extension better?",1522314937.0
Tostino,Hopefully this starts them (Amazon) actually contributing back to the community...,1522194996.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/aws] [Major PostgreSQL Sponsor OpenSCG acquired by AWS](https://www.reddit.com/r/aws/comments/87mozd/major_postgresql_sponsor_openscg_acquired_by_aws/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1522189073.0
jringstad,"would be nice how this differentiates itself from apgdiff et al. Also, can it handle triggers and such?",1522185851.0
thelindsay,"If the columns are just bits can you recode them to a single fact? E.g. treat the n columns as collectively specifying a binary representation of a number 010011? Or if the identities matter then something like numbering the different possibilities 1=000, 2=001, 3=010, etc. This might simplify things by avoiding the n-columns part, or it could be a (somewhat meaningful) surrogate key for a column with a jsonb document containing the full data.

It's hard to suggest since it depends on a) how the data is being used, and b) why the data has the structure you described.",1522159185.0
einhverfr,"so basically it sounds like you need n booleans and 2^n combinations because this is a set of all possible combination.  I love PostgreSQL as much as the next guy but this seems like the wrong use case for the relational database as an inference engine.  So I assume you have another need for this.

To be honest I would store as an int (32-bit) if n is between 16 and 32 bits and bigint (64-bit) if n is between 33 and 64 bits).  You can then convert to a bitstring using something like:

select 32456::bit(32);  And you can convert back in the same way.  Then you can perform bitwise operations on this.  The major advantage is that this is very compact so you can cram a lot of these in a page, making disk access very inexpensive on the searches.    You can then index the bit type conversion if you like and use that for exact lookups or you can convert the bits you want to look up into an int and search on that.",1522763438.0
macdice,"New(ish) PostgreSQL hacker here (~3 years).  Some assorted ideas:

Talk to people on #postgresql (freenode)... no question is stupid, re getting a development setup and understanding the code.  Look at commitfest.postgresql.org.  Start following some threads on the pgsql-hackers mailing list.  The final commitfest for v11 is about to come to a close, and a bunch of patches will be moved from there to the next commitfest (for v12), which is several months off.  Pick something that interests you and start testing it out!  Proof-read the documentation, run the tests (or complain if there are no tests or documentation), think about whether the feature makes sense...  Subscribe to pgsql-hackers and post your findings or impressions. It'll take a while to understand details like how we manage memory, locks, errors, yada yada but it comes with experience.

See if you can get to one of the PostgreSQL conferences where the developers congregate.  Otherwise look for talks on youtube about getting involved as a developer (some things that could be starting points: Tom Lane's talk on hacking the query planner, anything by Robert Haas or Andres Freund, ... I could go on.)

It's a great time to get involved with PostgreSQL: new ground is being cleared in the areas of parallelism, replication, JIT compilation, distributed queries, storage layer, ... well, you name it.

Check out the wonderful CMU Advanced Database course:

http://15721.courses.cs.cmu.edu/spring2018/schedule.html

Watch the classes on youtube, read the papers.  Many of those topics are directly relevant to what we're working on (or they will be!).",1522100783.0
Isvara,The usual advice for any open source project is to pick an open issue and use fixing that as an entry point. It's a lot easier to learn when you have a concrete problem.,1522101852.0
ccb621,"First hit on Google after searching for ""Postgres contributing"": https://wiki.postgresql.org/wiki/So,_you_want_to_be_a_developer%3F",1522091322.0
johnfrazer783,"Seriously I don't get what you guys are griping about. For one thing the accepted answer by Erwin Brandstetter is as thorough, informed, in-depth and carefully written as any of his millions of answers on SE. 

I'm not sure whether I think that snappy remark, presented with no justification or any other extra effort put into it, should receive a downvote or not. It is certainly technically feasible to use several integer field to store a hash. But beyond being *possible* that solution has very little to commend it. Sure it's just bits, right, so why not store a hash in 32 or 64 individual boolean fields? The answer to this question is as obvious as is the answer to why you don't want to store a single bit pattern in multiple numeric fields.

And what is ""annoying that postgres doesn't have a fixed with binary type, but WTF"" meant to even mean? PG has `bytea` and `bit(n)`, and `uuid` is exactly that, a fixed width binary type. ",1522057597.0
HildartheDorf,">Using a UUID for a hash.

I mean, it's annoying that postgres doesn't have a fixed with binary type, but WTF.
",1522025065.0
tektektektektek,"And this is why I don't even bother with StackOverflow anymore:

> Another option is to use 4 INTEGER or 2 BIGINT columns.

A *perfectly valid* answer that has been downvoted negative. Perfectly valid.

Sure - just because *you* don't want to do it this way doesn't mean somebody else will read the thread and think that is for them.

Computers allow us to approach problems in different ways. There is no sole solution to most problems.

Why can't the site encourage a diversity of answers instead of show the worst of people attempting to drown out any answer that isn't their idea of perfection?",1522053783.0
bisoldi,"::mind blown::

Aurora was designed specifically for higher throughput and lower latency.  

Tell me it ain’t so!",1521893255.0
cvboucher,I'd love to see Azure added to this comparison.,1521903709.0
jakdak,Already Amazon's beta tester for Redshift- going to wait a year or two before I even consider Aurora,1521917297.0
qatanah,U might need to prewarm the EBS blocks on AWS.,1521953210.0
inertially,"Usain Bolt is faster than a Ferrari for a split second.
Aurora isn't designed for microbenchmarks.",1521916664.0
Rawrry21,"I'm assuming Metabase doesn't have the required timezone information, but not sure what timezone input Metabase requires
",1521849021.0
shobble,"issue is the addition in ` creation_date + 14` I think.

Try something like `creation_date + INTERVAL '14 days'` instead?",1521849423.0
samuraisam,Hosted postgres from Amazon (RDS) has a free tier and currently hosts some massive databases. Same goes with Google Cloud hosted postgres.,1521767892.0
PetuniaFlowers,"Just get intro credits from linode, digitalocean, vultr, etc and spin up a small VPS.  even when you run out of your couple months of free use, it will be $5 or less per month.

postgres db's are extremely portable should you decide to move it elsewhere later.",1521821696.0
r0ck0,How will you be hosting the application code?,1521790956.0
titanofold,"Heroku is pretty nice. It's limited, but convenient. There's pretty much no administration that needs to be done.

Otherwise, I recommend a VPS like those offered by Linode, but then you have to administer everything yourself.",1521986608.0
riksi,Or check out https://github.com/timescale/timescaledb ?,1521719900.0
ants_a,"If you do a composite index, put date last because you are doing a range scan on it and it's not possible to use any indexes past a range scan.

If you are looking at a significant fraction of sensors at once (>5-10%), then an index on the sensor id is not going to be helpful anyway. Just use a brin index on datetime in that case.",1521728337.0
riksi,"Why do you have an `id` ? You don't need it, right ? If yes, remove it. And make `date+sensor` primary key, which is index and also unique.

Then, you already have the index on `date+sensor` which is the `unique` one. (you can remove this if you do thing above)

Do you usually query newer data more than older ? Then partitioning will help with that. (by month for example)

Looks like your rows are very thin, meaning only 8+8+8=24 byte of data, with each row overhead of ~22 bytes. Can you group several rows into 1 row ? Like having 1 row for each sensor for each day, and inside having a jsonb{datetime:value}. You need to test this to find the right amount.",1521717684.0
dopperpod,/r/domyhomework,1521699737.0
smariot2,Ideally you'd have recorded all the schema changes you made in numbered SQL files that you can just execute in order on your 6 month old dump to bring it up-to-date.,1521664011.0
djrobstep,"I ran into these problems all the time and so I wrote a [schema comparison tool](https://migra.djrobstep.com/) to compare schemas.

You should be able to run a comparison and autogenerate a schema change script. You might have to modify this to move data around as necessary but this should get you most of the way there.",1521695606.0
mage2k,"More important that the schema versions in those dumps is the data in them and only you can really know what your data retention needs and/or requirements are.  In general, though, having access to old backups can be really helpful if it turns out you've got some bad data in your database and you need to figure out when that started happening to help track down the bug in your code.",1521758358.0
r0ck0,"What are you using to perform backups?

Aren't you backing up the schema with it?

Aside from high volume hot backups (which I don't know much about) ... your regular dump type backups should always have the schema with them.  

Old backups like this aren't going to be restored directly back to your primary production DB anyway. 

If you need something from the old backup, you'd restore to a temporary database (including its schema that it had at the time), then get the data you need from that temp database.",1521894316.0
ccb621,"Ideally, you should check your scheme changes into your source control repository. That way you can simply run the scripts in order to rebuild the schema. ",1521692526.0
kenfar,Looks like the severe impact is probably to any database that is heavily read-oriented and mostly in memory (whether large or small).,1521682408.0
,[deleted],1521682147.0
thelindsay,"Matviews recalculate and rewrite the whole table every time, which is a pretty significant price to pay for read speed.

I think it's worth the effort to first spend time on query optimisation, using explain. Then trying more precise indexes (such as date trunc on month), or adjusting / creating statistics, or tweaking memory / query gucs. 

If those don't work then set up something incrementally updating, like a query / procedure that upserts & deletes new results against the summary table, or triggers on the source table to do the same.",1521805428.0
gsusgur,"Just did it a couple of weeks ago and followed this guide:

https://www.gab.lc/articles/migration_postgresql_10_with_pg_upgrade

The only steps I added was to first remove/delete all replications on slave/master as well as delete pglogical, and then recreate the replication on the new cluster. That should work in most cases unless you have a huge db.",1521708635.0
,[deleted],1521634686.0
exhuma,I have *just* finished manually comparing a very small database. Already for this small DB it took about two hours of work. If only I had known earlier this existed ;),1521618020.0
exhuma,"Just had a look at the demo video. This looks really cool already. A few questions:

* Pricing?
* Can it be used from the CLI? I don't see myself using a GUI just for a quick comparison, especially for small DBs
* Is there a scriptable output available? I could see something like that useful for migration tools like alembic",1521618222.0
Darkmere,"Remember that your schema MAY contain sensitive information like USERNAMES and PASSWORDS to your database if you are using Foreign Data Wrappers.

Anyhow, other than that, sent!",1521632083.0
dwhite21787,have you hit https://wiki.postgresql.org/wiki/Sample_Databases,1521637750.0
postgrescompare,"So far so good, with your help I've found issues with internal triggers and querying pg_authid instead of pg_roles! 

Keep em coming please :)",1521688958.0
z0rb1n0,"The tree traversal part is very interesting, however in general user-maintained lists are small enough not to require index scans: you can just get away by storing the intended order of IDs somewhere (an array/string  with separators) and just use

`ORDER BY array_position(ARRAY[9, 1 6], id)`

or

`ORDER BY strpos(':9:1:6:', CONCAT(':', id, ':'))`

Depending on how you like to store your sorting list",1521606575.0
thelindsay,"I wonder how well an ltree with a single edge would do for this:

- doc: https://www.postgresql.org/docs/current/static/ltree.html
- how-to: http://patshaughnessy.net/2017/12/14/manipulating-trees-using-sql-and-the-postgres-ltree-extension",1521628717.0
ants_a,"I'm pretty sure that both rationals and floats will run out of bits pretty fast in worst case situations. For floats the proof is trivial, but it seems to me that you can construct a proof that arbitrary fixed size storage solution for a total order without renumbering has a worst case of log2(bits) insertions. The proof would go along the lines that there is a finite amount of bit patterns that are between two values. If you insert a new value between the two you have to divide those bit patterns up between smaller than new and bigger than the new value, if you divide them equally each insertion in between will halve the number of bit patterns available in each half. If you don't divide them equally then the worst case will more than halve the amount of bit patterns.

I think the simplest thing to do would be to just use floats and trigger a renumbering if trying to insert between values one epsilon apart.",1521637641.0
A_Mindless_Zergling,"What's it called when the primary key is also a ""foreign"" (but not really) key into the same table? Anyway just do that. Each Item has a ""PrecededByItemId"" value. To insert X between A & B, just make X preceded by A and B preceded by X. Or probably better, go the other way (FollowedByItemId). It's the SQL version of a linked list. Efficient? Yes they're integers and an insert takes only two changes. Robust? Yes, infinite inserts possible. Elegant? Beats any of that complicated stuff in the article, unless I'm missing something. I suppose you could run out of integers but if your to-do list is that long, well...
",1521647580.0
ejrh,"Basically, because the statistics aren't copied from the old cluster.  If you don't analyze, the new PostgreSQL won't be able to choose good plans for queries.

When you run pg_upgrade it'll create a script called analyze_new_cluster.sh (or .bat), which will look something like this:

	echo 'This script will generate minimal optimizer statistics rapidly'
	echo 'so your system is usable, and then gather statistics twice more'
	echo 'with increasing accuracy.  When it is done, your system will'
	echo 'have the default level of optimizer statistics.'
	echo
	echo 'If you have used ALTER TABLE to modify the statistics target for'
	echo 'any tables, you might want to remove them and restore them after'
	echo 'running this script because they will delay fast statistics generation.'
	echo
	echo 'If you would like default statistics as quickly as possible, cancel'
	echo 'this script and run:'
	echo '    ""$BINDIR/vacuumdb"" --all --analyze-only'
	echo
	""$BINDIR/vacuumdb"" --all --analyze-in-stages
	echo
	echo ""Done""

The comments in the script explain things a bit more.",1521535051.0
Darkmere,"Most of them are various tasks that the database is always doing.

Here's a similar dump from a Linux system that shows you in more detail what's happening:

    /usr/pgsql-10/bin/postmaster -D /var/lib/pgsql/10/data/
    -- postgres: logger process   
    -- postgres: wal writer process   
    -- postgres: checkpointer process   
    -- postgres: writer process   
    -- postgres: autovacuum launcher process   
    -- postgres: stats collector process   
    -- postgres: bgworker: logical replication launcher   


Now then, ""logger"" is the process writing logfiles. 

WAL writer is the Write Ahead Log writer, that's what's making Postgres crash-resilient and ensures that your changes hit the disk, and what makes transactions function.

The Checkpointer takes WAL segments and turns them into Checkpoints,  basically ""folding them into"" the main database, and then writing a signature that ""at this point, everything is a-okay"".

The Writer process writes data to disk.   

Autovacuum launcher is responsible for starting the autovacuum tasks when it thinks it's needed.

Stats collector gathers data about the system and databases, for internal and external use.

BGWorker is for logical replication, ie. sending data from one postgres DB to another.

Several of these will only start if their actual setting is enabled in the database, Statistics collector, Autovacuum, and replication launcher are among them.  However, unless you want to fit Postgres into a really small space, I wouldn't recommend disabling them. 



",1521472989.0
dbtgbp,"I realise this doesn't relate directly to your question, but don't judge PostgreSQL by PGAdmin 4.  Other clients exist.",1521623937.0
-markusb-,Patroni or Pacemaker with PAF,1521456018.0
titanofold,"You might find the talks from TripAdvisor interesting.

http://www.pgconfsv.com/sessions/heart-giant-postgres-tripadvisor

",1521458725.0
binarycleric,"Heroku employee here (my views don't represent my employer, blah blah blah). Not trying to sell you anything, just offering professional advice.

We use native replication between leaders/followers (master/slave but we avoid that terminology) with wal-e running to push backups off-server for continuous protection. We push these backups to S3 and take regular base-backups. The advantage of this is that bringing up new followers is a bit faster due to only needing to reload the base-backup and any diffs since then. 

Failovers to a follower are fairly fast but you'll need scripts to update other followers to point to the new leader. This has been a pain point for us in the past but we recently shipped an update that does this for our servers automatically.

I posted a write-up about our use of AWS a few days ago on lobste.rs. It could be useful for your decision making. https://lobste.rs/s/mqadwp/small_startup_using_aws_i_have_some#c_zckfq0

I used DRBD with MySQL a few years ago and I was less than impressed. We'd run into weird issues with split brains or one server refusing to sync blocks. Cross-datacenter is a bit of a problem. DRBD-proxy was a product they used to offer to solve this but there can be some rather substantial lag. Looking at this from a ""shared nothing"" (really ""share as little as possible"") viewpoint can be useful for building truly robust systems. Persistent data containers with redundancy, especially with systems like Postgres where just syncing files won't cut it, is an extremely difficult problem to solve without a bunch of hacks and human interaction. 

Honestly, it sounds like you may be in a bit over your head. To go from next to no database knowledge to trying to be a 5 (or 6!!!!) 9s setup is hard enough with fairly advanced knowledge of these sorts of setups. Keeping a database highly available is a lot different than keeping file servers, app servers, etc. highly available. I hope your organization understands the cost of a project like this.

I am *absolutely* not criticizing you here. I've just been in your position at other points in my career and have been burned. You can try and investigate some high-level architectures but at the end of the day, building a system to your specs is incredibly expensive and will require a good bit of planning, operational knowledge, and automation to get right.

Is on-site required or can you use third-party offerings? AWS RDS and Aurora have their faults but are fairly well understood, very configurable, and come with decent support if you are willing to pay. AWS Direct Connect is pretty slick for integrating AWS services onto on-site infrastructures. This can be a tough sell at some organizations but stressing ""total cost of ownership"" can work out in your favor when you factor in human time.

Happy to try and answer any other questions, assuming I have time.",1521482495.0
seb2020,"I use in production the EntrepriseDB EFM (https://www.enterprisedb.com/products/edb-postgres-platform/edb-postgres-failover-manager) 

Take a look on it",1521453730.0
TheSqlAdmin,Recently many experts used Patroni (https://github.com/zalando/patroni) for Postgresql HA. Try it out. ,1521531496.0
wSupabib,"First of all, thanks a lot to all :)

Special thanks to binarycleric : no offense taken. I do agree with you.

After posting here I started to read docs about repmgr. I think that the current vision of the architect is way too ambitious... I think that I could start with an simplified architecture and propose something robust with automatic failover, only 1 master and 2 slaves. I'll also have a look to patroni
",1521552675.0
antlife,What connection driver are you using?,1521429368.0
Mittalmailbox,Have you tried postgres on WSL?,1521433830.0
fullofbones,"Hey, glad you found my book helpful. ;)

Depending on how your application is designed, BDR could work out. There are certain limitations, as unlike Oracle, BDR doesn't have a global lock manager. Each node essentially operates independently and sends a logical replication stream to the other nodes after commit. It's very loosely coupled.

I can answer more specific questions if you have any. ",1521254929.0
ebm1,sudo syntax looks off.  try this instead:  sudo -u postgres psql,1521227604.0
joaodlf,"As long as you can install and run PostgreSQL on it, it doesn't really matter. Does the org already run stuff on Linux? Do you have colleagues that manage Linux boxes? Might be a good idea to use the same distro, or at very least, ask them about it.",1521220314.0
SemiNormal,"FWIW, we run PostgreSQL on CentOS. ",1521223938.0
UnlikelyExplanations,"I have used PostgeSQL on Ubuntu Server (which based on Debian), Debian itself, as well as the Redhat RHEL, CentOS and Fedora family of distros and there was no difference in performance because they are all running pretty much the same Linux kernel.

The biggest difference is the content of the repositories. Ubuntu and Fedora have more recent versions of everything (which may or may not be a good thing) while the packages in Debian and CentOS are always behind by a version or two for reasons of stability. RHEL also prioritises stability and by paying the subscription you get a rock-solid distro with excellent documentation and support.  

The package managers are different between the two families: RHEL and CentOS use yum (dnf in Fedora) and the packages are RPM format versus apt-get for the Debian and Ubuntu packages in DEB format.

It's more convenient to stick to one family (RPM or DEB) for easier maintenance because they differ slightly where they place the installables.",1521258047.0
thrakkerzog,We've had better luck with FreeBSD than Linux for PostgreSQL.. but YMMV.,1521226828.0
melt_Doc,"If you want upsert, use Debian. I think CentOS build in package is still 9.1 or 9.3",1521225537.0
eCommerce_2015,"I've been running postgres on ubuntu for a few years without problems.  I'm not sure the OS is all that impactful - unless the os itself is unstable. 
",1521226374.0
dtechnology,"Any server oriented distro should be fine.

I personally like Debian, but if you want support or enterprisey features CentOS or RHEL is a good bet.",1521228945.0
lalligood,"FWIW, if sticking to just core PostgreSQL elements, CentOS is fine. However, if you have a need for things such as Foreign Data Wrappers & some other lesser attractions, the Debian repo is more complete. (So you will have to compile many FDWs on your own from source.)

tl;dr: The Debian repo is more package complete than the RH/CentOS repo.",1521229138.0
CSI_Tech_Dept,"Linux distro doesn't really matter, what Postgres interacts with is the kernel, and kernel is pretty much the same in all linuxes (that's what Docker exploits and that's how you can have containers with different distros).

If your question would be whether you should for example use BSD vs Linux, even though it is painful for me to say it, I must say that it would be Linux since Postgres is better supported and tuned to work there, if I remember correctly Postgres on FreeBSD was not handling SHM as well as on Linux, it's possible that it might have been fixed since then.

Going back to Linux, use what you like and what you're most comfortable with, from Linux distros I personally like OpenSUSE, it looks like very well done especially around packaging, but everyone has their own favorite.
",1521234010.0
MyOtherUserIsAThrow,"I agree with most that the distro doesn't matter much as long as you're comfortable with it.  I'm curious about what filesystem most people use for postgres?

Ext4 is ubiquitous, but still likes to do fsck on boot every so often, which can lead to longer downtimes at painfully inconvenient times.

Xfs is my current goto, nice and stable.

I'm considering zfs for its near zero cost snapshots, but I'm wary of unforseen performance problems.
Can anyone way in with their filesystem experiences?",1521236010.0
ibishvintilli,Use docker for multiple postgres servers.,1521236643.0
jyrandrianiaina,"Thank you all for your answer. So all Linux use the same kernel. The question should be Linux vs Unix. 

You said debian, centos, rhel. This postgres is for a company. 

I will learn linux command and will use centos as its rhel roots that have enterprise support.  

Thank you all. ",1521266025.0
sisyphus,"The Postgres project packages Postgres for Debian, Ubuntu, CentOS, etc. you don’t even really need to care about system packages much if you just add their repo (I am assuming you will only be running Postgres on this server).",1521334509.0
jlrobins_ssc,One step closer to actual content [here](https://www.imperva.com/blog/2018/03/deep-dive-database-attacks-scarlett-johanssons-picture-used-for-crypto-mining-on-postgre-database/),1521220378.0
,[deleted],1521169892.0
bn-7bc,"Hold on a secon, postgresql 9.0 which was EOLed in  septrember 2015 —— it is now  march 2018, what went wrong, why did they not pick a version that still has at least a year left?I’m nor critisicing them just curous",1521236800.0
therealgaxbo,"I'm not 100% clear on what you're asking, but it _sounds_ like you think that the `concurrently` option makes the command run asynchronously - i.e. in the background.  This is not the case.  The command will run in the foreground and block the current session until it is finished.

What `concurrently` actually means is that _other sessions_ will not be blocked while it is running; they will just continue to use the old data transparently while the refresh is in progress.",1521130110.0
disclosure5,"I'm confused by what they actually did here.

>Run the PostgreSQL upgrade on the Master server, using a script to automate as many steps as possible

So.. pg_upgrade? 3-5 minutes for a 50TB database?

I've never run it on anything at that scale but that's significantly faster than I'd expect.",1521087706.0
hahainternet,I'm confused as to why they used filesystem snapshotting rather than the built in facility in Postgres.,1521070028.0
fullofbones,"Not sure if they didn't know about the rsync trick to quickly upgrade the replicas, or just decided it was too sketchy. It probably would have been a lot simpler than the snapshot shenanigans. ",1521114609.0
dopperpod,You can uninstall pgAdmin4 and install DBeaver.,1521038184.0
Vardy,You could check your process in the pg_stat_activity table for a transaction ID.,1521045728.0
boy_named_su,Be careful not to copy any private data (like cc numbers or SSNs) to staging...,1520971525.0
tucsonflyer,Why not write seeds to fake the db? Do you need the actual live data in staging?,1521003046.0
fr0z3nph03n1x,"Is it normal to copy production to staging? I have never done something like that, seems like a lot of work to make sure you don't copy something over that could expose users on the production system.",1521007943.0
zeha,"there are a number of tools out there. some setups just do a storage-level snapshot.

maybe have a look at https://github.com/ankane/pgsync",1521150849.0
therealgaxbo,"Not familiar with `cross apply`, but I assume what you're trying to do is pivot the table so that you get a separate row for each of InfoOn, InfoTwo and InfoThree which are then aggregated as usual.  In which case it would look very similar:

`...from online cross join lateral (values (InfoOne),(InfoTwo), (InfoThree)) c(value)`

i.e. literally replace the word `apply` with `join lateral`.",1520967274.0
spinur1848,Have a look at postgrest.com,1520994028.0
jenkstom,Everytime I have to use MSSQL you can hear me cursing from 1.4 miles away. And if I have to do anything with stored procedures I'm gonna punch somebody in the face.,1520970865.0
ants_a,Use postgres permission system to grant only select and insert permission to the target user. ,1520957160.0
eric_ja,"How about:

    create rule rotable_update as on update to rotable do instead nothing;
    create rule rotable_delete as on delete to rotable do instead nothing;
",1520952979.0
dcalde,Thanks. Very informative article,1520927769.0
pstef,"Since Postgres 10 there are identity columns which I think you should be at least aware of.


https://www.depesz.com/2017/04/10/waiting-for-postgresql-10-identity-columns/


https://blog.2ndquadrant.com/postgresql-10-identity-columns/",1520919572.0
jakkarth,"They are separate objects. A sequence can be shared across multiple tables, some of which you may not want the user being able to influence. Or no tables at all.",1520898366.0
f0rgot,"Welp, just realized I was looking at teh docs for the SQL command `COPY` instead of the PSQL meta command `COPY`.",1520875556.0
spinur1848,We've had some success wrapping Oracle with the postgres oracle_fdw. This allows us to provide a modern and stable postgres interface to new applications until we can completely transition over to postgres. ,1520892049.0
thelindsay,"What did you have in mind to use this for? I can see it'd be a nifty bit of glue for lots of things, but I'm curious what you had in mind.",1520668161.0
teilo,This is very useful for those cases where you need to integrate with a vendor-developed app that does not have its own integration points (event hooks for scripts) but you have admin access to the DB. ,1520698710.0
chrj,"I'm not familiar with Windows, but do you have access to the postgresql logfile? It might hint at what's happening.

A bit of googling show that it should be in `%PROGRAMFILES%\PostgreSQL\<version>\data\pg_log`",1520636533.0
Kayco2002,Yeah.... logs are super-helpful here.  It'll usually explicitly tell you why it didn't start.,1520703963.0
therealgaxbo,"While I agree with the article, a few points about the 'Multiple values per column' section:

Multiple values in a column doesn't have to mean an ad-hoc delimited text field, it could also be an array, hstore, jsonb....the latter even being previously suggested as an alternative to EAV.  And with e.g. an array, it _does_ collect statistics, can support more complex queries (matches multiple tags, matches at least one tag - probably more simply than the normalised model, even), and can do so using indices.

And even if you do have the abomination of the semicolon-delimited version, because Postgres is awesome you can get all that functionality (query power, statistics and index) back with a functional index:

`create index tag_array on tweet using gin (regexp_split_to_array(tags, ';'))`
`select * from tweet where regexp_split_to_array(tags, ';') @> ARRAY['magnets', 'explanation']`
`select * from tweet where regexp_split_to_array(tags, ';') && ARRAY['colour', 'color']`

Not that I'm saying it's a good idea to go against 1NF, just that there are ways of making it work, and times when it can be useful.  Ironically, tags might be one of the few times when it really might be.

Also, my usual rant:

> You might have to retry producing a [UUID], though very rarely.

No.  If you ever get a UUID collision then you've found a **bug** that should be reported.  The probability is so small that you should treat it as zero.  It just won't happen.  Not 'rarely', not 'a couple of times a year on my unusually busy server', never.

",1520609143.0
sisyphus,You might look at iconv to encode it in utf8.  It is fast even for quite large files.,1520476410.0
Dysl3xicDog,It's so hard to remember commands when you only pop in the database to check things once in a while.  This is going on all my postgres deployments.,1520442576.0
,"You can use pgcli with pspg for enhanced paging.

When you pip install pgcli, the config file is located in ~/.config/pgcli/config

- The pager config line, which by default is commented out. You can define the pager to use pspg here, or leave it commented out and simply add something like this to your shell rc file: export PAGER=""pspg -s 2""

Additionally:
- Check out syntax_style (I like monokai)

from within a pgcli terminal, F4 lets you switch into VIM mode

Also, play around with the table format in the pgcli config. I have yet to find a solid line grid but haven't stopped looking for it yet.",1520509181.0
scttmthsn,"The best postgres client period. Thanks so much for making this, it truly has made my world better!",1520439746.0
lykwydchykyn,"Just started using this recently, and I can't praise it enough.  Much speedier than loading up some bulky GUI client, and much friendlier than `psql` alone.",1520444770.0
zieziegabor,"I use this instead: https://github.com/okbob/pspg

it's a pager replacement for postgres.",1520473269.0
vittore29,Using it every day for last 8 months. Love it.,1520472403.0
___GNUSlashLinux___,I went to an SFPGUG meetup where this was demoed. Its a really cool tool. just reminded me I needed to update it.,1520474258.0
hjkl_ornah,Compatible with Redshift?,1520475080.0
scottocom,I think pgcli and pspg should be in a standard postgres client installation along side the original pg command for backwards compatibility. ,1520542691.0
ptman,I like to visualize postgresql schemas using [PostgreSQL Autodoc](https://github.com/cbbrowne/autodoc),1520429864.0
,[deleted],1520404694.0
joaodlf,"Hi, I needed a quick tool to create the same index across multiple partitions in PG10, and since this isn't supported out of the box, I decided to build a solution for it. Hopefully this helps others too.",1520350689.0
mage2k,This is looking great.  One question I'd like to see numbers on:  With the DO/UNDO log rollbacks will no longer be O(1).  What kind of performance/time are we looking at there?  I ask this after having suffered through some fairly nightmarish rollback scenarios with MySQL where someone would do something like running an unindexed update on a massive table and then try to cancel it after it had been running for hours.  Definitely no fun.,1520373647.0
postgrescompare,"Hi, I responded to your post on StackOverflow. I'll copy here:

Hi,

Something has happened to the pg_catalog.attrdef table on your server such that the unique index that should be present on the oid column of attrdef is missing. 

You can see the warning being raised here https://github.com/postgres/postgres/blob/master/src/backend/catalog/catalog.c#L312

I read it as that the statements you are issuing are attempting to set a default so they are writing to the attrdef table. Postgres has a built in assumption that catalog tables will have a unique index on the oid column and outputs a warning if one is missing. I couldn't recreate on my v10 box so I think someone has dropped the unique index on your specific instance.

Perhaps you can recreate it?

    CREATE UNIQUE INDEX ON pg_catalog.pg_attrdef (oid);

There are probably no ramifications other than the warning you are seeing.

Neil",1520312040.0
Darkmere,"This looks pretty awesome.

Maybe I should update [PgZabbix](https://github.com/Spindel/PgZabbix) to use your code rather than doing it all by myself.",1520250870.0
thelindsay,"Maybe these will help:

- https://github.com/begriffs/postgrest
- https://github.com/informatics-isi-edu/ermrest

These are projects that allow REST-based interaction with the database and delegate the enforcement of access controls and row level security to Postgres. It's not necessary to use them but they present a detailed example of this architectural model.

The problem is that while column level permissions are part of the privileges system (using GRANT/REVOKE), row level security policies require careful implementation of access methods (views, functions) that respect some kind of context (current role, session variable, etc.) passed to the database, which makes the whole thing hard to generalise.

But say you do set up ACLs/RLS, and interact with the DB similar to or using the above projects, then your application would present a UI for managing permissions in terms of which users (or user groups) had what context data - rather than them editing the underlying ACLs/RLS machinery you put in place to support it.",1520220794.0
to_wit_to_who,"I use [pgcli](https://github.com/dbcli/pgcli) during development for schema-related stuff for PostGraphile.
  
If you're talking about production/operations management of roles/permissions, then I would suggest integrating your own AuthN/AuthZ provider (e.g. Passport) and passing the resulting token into PostGraphile.  You can then use jwt.claims.* within RLS context queries from there.",1520400194.0
thepiper_,You can write a user defined function that will calculate the difference and then use the function in your query.  If you write it in pl/pgsql you can use EXTRACT() on your datetime to find the day of week and time of day to do your arithmatic.,1520189922.0
grupwn,"I think creating a Calendar table would be a good start.  You can join your table to that, and filter out weekends and holidays that way.  Then all you have to do is specify the hours in your WHERE clause.

https://www.sqlshack.com/designing-a-calendar-table/",1520196867.0
MacCH2,"Been there, done that.

Basic idea:

* Have a table with all the considered days, including the ""Business Hours"" for the day - for instance 08:00-20:00 on weekdays, and nothing on week-ends. The trick is to store them as the geometric type [box](https://www.postgresql.org/docs/9.4/static/datatype-geometric.html). In my DB it's the field ""business_time_box"".

* When you have an event with a start and a stop time, do the intersect of the box defined by the event's boundaries and all the date boxes... this will give you a box with the time actually elapsed during business hours. This way, you can benefit from all the optimisations done on geometric types and simplify your SQL a lot...

In code:


    select * from dim_date where date = now()::date;
       id   |    date    | day | month | year | year_week | year_month | year_quarter | month_name | business_time_start | business_time_stop | day_of_week |       business_time_box
    --------+------------+-----+-------+------+-----------+------------+--------------+------------+---------------------+--------------------+-------------+-------------------------------
     105280 | 2018-03-05 |   5 |     3 | 2018 | 2018W10   | 2018-03    | 2018Q1       | Mar        | 08:00:00            | 20:00:00           |           1 | (1,1520276400),(0,1520233200)
    (1 row)

Then, to know how much ""Business time"" there is between 2 timestamps:

    select interval '1 second' * (SUM(AREA(BOX(POINT(0, EXTRACT(EPOCH FROM '2018-01-12 12:00:00'::timestamp)), POINT(1, EXTRACT(EPOCH FROM '2018-01-19 18:35:00'::timestamp))) # business_time_box))::integer) from dim_date;

     ?column?
    ----------
     66:35:00
    (1 row)

This allows to:

* Finely tune the business hours per day (the Saturday hours may be a bit shorter... or there may be specific days when there is no service). The limitation with this approach is that you can have only 1 box per day.

* Take into account changes of opening hours - typically if you decide to extend your opening hours after a certain date - you can still keep a full history of the past.

* Be super-fast in the computation of the ""Business time""... typically a few milliseconds here, with 11k days in the DB and no index.",1520266504.0
fullofbones,"Be wary if you run Postgres in a VM. Most virtual machines don't support PCID right now, so you'd feel the higher 10-20% performance impact. ",1520177027.0
shaz00m,Luckily for me Ubuntu 16.04 got a kernel with it out very quickly. Been using it on my guest for well over a month. Meltdown had negligible impact on my work; spectre that's another story,1520209303.0
,How do I use PCID?  What is the cost of using it?  Will PCID be used by default installation?,1520214600.0
Darkmere,"select 1;  is the traditional round-trip measure of a database.

In pseudocode:

    start = gettimemillis()
    result = exec('select 1;')
    elapsed = gettimemillis() - start
    print(elapsed)

",1520089665.0
swenty,"You can simply ping the IP address of the database. That will give you a pretty reliable measure of the network latency between you and the DB server. Of course it may vary over time, but that's the nature of network latency.

You should also pay attention to packet loss. If some ping packets are disappearing that tells you that there is congestion or other other network problems causing intermittent packet loss. For TCP connections such as Postgres, the network retry mechanism will make packet loss appear as a slow connection also.

If you are executing pg_dump over the network, you will also want to measure bandwidth. That will tell you whether the speed of the dump is being limited by the  network capacity between the client and the DB server. That's perhaps the most likely scenario. The easiest way to measure bandwidth is to transfer a medium size file (say a few 10s of MB) and divide the file size by time taken to get a measure of actual transfer speed in bytes per second. Then you can compare the speed that you're getting for raw file transfer with the speed you get for the dump process. More than likely the raw file transfer speed will be similar to the DB dump speed, which would indicate that overall network performance is the primary limiting factor.",1520103853.0
dopperpod,"That’s definitively not why it is slow, but couldn’t you just use an IP geolocation tool?",1520083579.0
stympy,"I'm a huge fan of Patroni... we use it at honeybadger.io on EC2.  Instead of a VIP, we use consul-template to reconfigure and reload pgbouncer when the leader IP address changes.  We had an unplanned primary failure happen (the instance just stopped responding), and our app barely noticed.",1519950360.0
chock-a-block,"Pacemaker with a libvirt fence works great. You can also do it low-tech and build active/passive with vrrp via keepalived.

VRRP + replication is a *brutally* simple and reliable way to get good failover on cheap hardware. Not many seem to embrace the benefits of running simple failover.",1520054805.0
thelindsay,"Tldr the CVE affects all supported versions but is only an issue if you have untrusted users that can run DDL in the same database that other users can access, and you haven't properly segmented user access to schemas.

The problem is that if multiple functions in different schemas have the same name and argument types, search_path will be used to pick a winner.

So Alice can make a malicious public.lower(text) that Bob will end up using, unless Bob has no privileges on public, or Bob's search_path excludes public, or Bob always calls built-in functions explicitly e.g pg_catalog.lower(text).",1519948966.0
linuxhiker,"1. a pg_dump does not reflect the size of indexes
2. a pg_dump does not reflect the amount of bloat you have in the database

To get an accurate representation you will want to load the 800kB file into a test database and then use pg_database_size. That will give you an idea of how much bloat you have vs on disk index size etc...

Also, you are going to want to look at this blog if you are using pg_dump for backups:

https://www.commandprompt.com/blog/a_better_backup_with_postgresql_using_pg_dump/",1519848976.0
TheCrazyGnat,"Also, if the custom file option is being used in the dump, it does compress the file.  You can specify the level of collision from 1-9, buy it uses level 6 if nothing is explicitly stated.",1519865190.0
jlawler,Oh  and don't forget to backup your globals!,1519872695.0
jlawler,"In addition to what everyone else said, pg_database_size is based on number of pages, but that includes tuples that are out of visibility, sparse indexes, etc.  It seems reasonable.  Just make sure the user you are running it as has all the perms it needs, and I recommend always doing the custom format.",1519872635.0
annazinger,"Your tables may have lot of free tuples because of updates & deletes. Also your indexes may be very bloated. You may consider reindexing & Vacuum some of the large tables and then check the size of the DB and compare with your PostgreSQL [dump](https://n2ws.com/blog/how-to-backup-your-aws-cloud-based-postgresql-database.html) size.

REINDEX reduces the space consumption of the index by writing a new version of the index without the dead pages. 
 Syntax:`REINDEX { INDEX | TABLE | DATABASE | SYSTEM } name [ FORCE ]`

VACUUM reclaims storage occupied by dead tuples. In normal PostgreSQL operation, tuples that are deleted or obsoleted by an update are not physically removed from their table; they remain present until a VACUUM is done. 
 Syntax:`VACUUM [ FULL ] [ FREEZE ] [ VERBOSE ] [ table ]`",1522916992.0
skulgnome,Appears to ignore MVCC wrt row-level locking. Is this actually about PostgreSQL at all?,1519679442.0
jonr,"I forgot to wear socks.
",1519721905.0
Darkmere,"One thing that recently bit me was that locks of _children_ in a partitioned table also lock the parent.

So a table foo with some children, and you create a new index on one of the children, you lock the parent unconditionally.",1519682521.0
-markusb-,You have to install the database-software on the server not the „content“. Then you can work with any PostgreSQL-compatible tool. ,1519644884.0
iiiinthecomputer,"Install the PostgreSQL *server* on the place with the storage, and access the PostgreSQL server over PostgreSQL's native TCP/IP networking support.

Set `listen_addresses` to `*` (assuming trusted-ish network), `pg_hba.conf` to allow connections from the host(s) you want, and create one or more user roles to connect with.",1519649320.0
who_died_brah,"Basic recommendation:. Create a separate server for postgresql installation.  Define the data directories on a different drive.  Use tools/scripts/jobs to import the Excel data and query it as you need.  

Rule of thumb in terms of performance:
Local instance > local network > external network/cloud

However, if the network has enough throughput I believe network latency should not be your concern.",1519649609.0
NewDimension,Through SSH you can bind a local port to the Postgres default port on the server. You then run Pgadmin locally and point it to the local port.,1519603201.0
grupwn,VPN if you really need to connect to PGAdmin4 over public wires.,1519598982.0
cyanydeez,"socat 

https://stackoverflow.com/questions/31193485/combining-socat-with-ssh-to-have-dynamic-port-forwarding-through-socks-proxy",1519612021.0
asdfhomerow,"I personally think it's bonkers to run a DBMS in a container for a variety of reasons. However, depending on your use cases and the design you employ, it is possible and in fact totally feasible to meet uptime, durability, fault tolerant levels of traditional methods.

Josh Berkus - former core contributor to Postgres, employee at RedHat working on Kubernetes has been working on this a lot. We chatted about it at DevOps days and he opened my mind to it.

Looks like he has a talk about it from December, check it out: https://www.youtube.com/watch?v=Zn1vd7sQ_bc


All that said, I still think it's best to go traditional long lived pet server approach, or outsource that shit to Amazon and RDS. Data tier uptime, Ha, FT, DR is serious business and unless you are really willing to do the hard work to understand your risks and mitigate those risks, kick it old school.",1519526491.0
sisyphus,"Sorry this doesn’t answer your question but if anyone does do this I’d like to know what you gain by it because I don’t understand why anyone would Dockerize Postgres.  In production we always, always give Postgres its own server, its own ssd partitioned specially for a database server, upgrades are relatively infrequent and from Debian packages created by the Postgres project itself.  What exactly does Docker buy for a production database server?",1519523632.0
cbunn81,"I'm not familiar with Flask or SQLalchemy, but feel free to post the code that you used to solve the problem in that.

First, since you have two tables with a common column, you should look into [joins](https://www.postgresql.org/docs/current/static/tutorial-join.html).

Then, since you need a maximum value for a column, you should use an [aggregate function](https://www.postgresql.org/docs/current/static/tutorial-agg.html). It should be obvious which one.

Then, if you want to do these two things together within the same query, you should employ a [subquery](http://www.postgresqltutorial.com/postgresql-subquery/).",1519467914.0
behavedave,"I was feeling generous and as a student I took many shortcuts so I thought I'd add something back:
    
    --create clients table
    create table clients
    (
    	code int primary key,
    	address varchar(512)
    );
    
    --add the values to the clients table
    insert into clients
    values(1010, '120 ABC Barcelona'),
	      (2468, 'S/N ZZZ Jaen');
    
    --create table consumptions
    create table consumptions
    (
	    code INT,
	    date DATE,
	    consumption INT
    );
    
    --add the values to the consumptions table
    insert into consumptions
    values(1010, '2015-10-21', 25),
	      (1010, '2015-10-22', 60),
	      (2468, '2015-10-21', 75),
	      (2468, '2015-10-22', 40);
    
    --figure out the results using row_number in a common table expression
    --the cte is used here so that the result from the row number can be used as a clause later
    with combo as
    (
	    --the row number is as simple as partitioning and ordering
	    --giving a number to represent each entry grouped by the code
	    select clients.code,
		       clients.address,
		       consumptions.date,
		       row_number() over (partition by clients.code 
		   					      order by consumption desc) as top
	    from clients
	    join consumptions on clients.code = consumptions.code
    )
    --the where clause means we only show the highest consumption
    select code,
	       address,
	       date
    from combo
    where top = 1;
    
    --here is an alternative where we use an aggregate in a sub-query
    select clients.code,
	       clients.address,
	       consumptions.date,
	       consumptions.consumption
    from clients
    join consumptions on clients.code = consumptions.code
    join
    (
	    --the aggregate works by the max consumption grouped by its code
	    --this is then joined back to limit the results to only get the max
	    select code,
		       max(consumption) top_consumption
	    from consumptions
	    group by code
    ) agg on clients.code = agg.code
	     and consumptions.consumption = agg.top_consumption;
    ",1519480333.0
Darkmere,"If you wrote it in SQL ALchemy as a single query, then you can ""cheat"".

SQL Alchemy supports printing the native query, so if you use the Object Relational Model to build your query, you can simply `print` the query (Rather than execute it) and you'll have your functional query.

It'll look a bit odd since all the columns will be named explicitly, but that's fine.",1519472201.0
Foodei,This article will help you: https://www.xaprb.com/blog/2006/12/07/how-to-select-the-firstleastmax-row-per-group-in-sql/,1519479891.0
dustyshackleford,"You need to use PostgreSQL's [UPDATE \[...\] FROM](https://www.postgresql.org/docs/9.5/static/sql-update.html) abilities. Something like this, which I typed up without having access to your database so please test in a copy:

    UPDATE itemsite a
       SET a.itemsite_posupply = fttemp.posupply,
           a.itemsite_sold = fttemp.sold
      FROM fttemp
      JOIN item i ON fttemp.fttemp_venditem_numb = item.item_number
      JOIN itemsite b ON i.item_id = b.itemsite_item_id
     WHERE a.itemsite_id = b.itemsite_id;

You can test the select portion of the query by just inserting a SELECT * before the FROM:

    SELECT *
      FROM fttemp
      JOIN item i ON fttemp.fttemp_venditem_numb = item.item_number
      JOIN itemsite b ON i.item_id = b.itemsite_item_id;

I have not tested needing to use DISTINCT in UPDATE FROM but I don't see any reason why it wouldn't work.

Have fun working with xTuple, it is nice to have a sane schema to work against. I'd recognize those columns/tables anywhere :)",1519396049.0
philrhurst,"There are several ways to perform an UPDATE in Postgres. But this is a method that helped me when I was learning SQL and allowed me to iterate quickly with my database work. Adapt it as you feel necessary. Also, remember that just about every SQL statement you execute in Postgres returns a table (see #1 [here](https://blog.jooq.org/2016/04/25/10-sql-tricks-that-you-didnt-think-were-possible/)); which means that SELECT queries can be encapsulated as tables and used in a variety of other cases (i.e. JOINs, UPDATEs, etc).

Here's what I would do.
1. Write a SELECT query where each row contains the primary key of the record to be updated and the columns are the new values you want stored.
2. Embed that SELECT query in an UPDATE statement.


For example,

    UPDATE myTable as u
    SET column1 = RS.column1, column2 = RS.column2, ...
    FROM (
        SELECT ... as id,
        ... as column2,
        ... as column3,
        ...
        FROM ...
        WHERE ...
    ) as RS
    WHERE u.id = RS.id;

The inner SELECT clause is written in such a way that is clearly defines (1) what rows are to be updated and (2) what values you want set. There are more elegant solutions, but I believe this is a good method for beginners to use when learning how SQL works.",1519397014.0
thelindsay,"Looks pretty good, from the PUM readme I ended up going down a rabbit hole of articles and projects about database migrations and came across Pyrseas - which seems to be very similar to PUM but uses YAML for metadata and seems to cover more object types in depth plus fancy things like dependency sorting: https://github.com/perseas/Pyrseas . Maybe they are different in other ways I don't appreciate, but any case, I hadn't seen either tool before today (despite previous browsing on this topic) so thanks for posting!",1519407782.0
ypsthelove,You forget the Logical Replication part.,1519403261.0
pypt,I’d go with the cheapest ($5/month) Digital Ocean VPS and install + manage PostgreSQL myself: https://www.digitalocean.com/community/tutorials/how-to-install-and-use-postgresql-on-ubuntu-16-04,1519371767.0
cybersurfer2,"You can try a nano ec2 instance on AWS (either install postgres yourself or use a preconfigured image), which should be only a few bucks a month. 50MB storage is rather small so should fit easily. ",1519371881.0
petercooper,"Another question is which cheap VPS provider is closest to your preferred host (say, AWS or Heroku) network-wise? You want that latency to be as low as possible :)",1519821341.0
ppafford,You could always use the free tier for a limited time https://aws.amazon.com/rds/free/,1520384209.0
qsnoodles,"I'm on mobile and can't test this right now, but generally in Postgres I believe that ""not equals"" is represented as ""<>"" instead of ""!=""

Perhaps that might help.",1519281063.0
kodifaer,"EXECUTE scripts uses standard SQL language, despite being called from a PL/pgsql function. As such you don't have access to the IF statement in there.

Options you could use to get around the limitation are:
* Wrap the code in a function and call the function with your desired values

* Use a do block in your execute, however that is in a different scope, so doesn't have access to the $1 and $2 variables. You can still concatenate them directly but that's riskier

* Use select case and combine that with a CTE to do the INSERT

Personally I'd go for the first option of calling a proper function.",1519292184.0
kodifaer,"I'm not somewhere I can test the below, so please excuse any typos. sending in the column name is totally optional though. Standard concatenation as you were using before will still work. The idea should work though. In both cases you prevent the need for the IF statement being in the execute statement.

Option 1:
    
    create or replace function log_crime_change(_id text, _column text, _old_value text, _new_value text) returns void as $$
    begin
    	if _old_value is distinct from _new_value then
    		insert into chicago_crime_change_log(_id, _column, _old_value, _new_value, now());
    	end if;
    end;
    $$ language plpgsql;
    
    EXECUTE 'SELECT log_crime_change($2.ID, $3, $1.$3, $2.$3)'
    			USING OLD, NEW, quote_identifier(col);
 
Alternatively just:
    
    EXECUTE $code$
        insert into chicago_crime_change_log
	    select $2.ID, '$3', $1.$3::TEXT, $2.$3::TEXT, now() where $1.$3 is distinct from $2.$3;
    $code$
    USING OLD, NEW, quote_identifier(col);",1519323872.0
jlrobins_ssc,Anyone tried / found examples of RLS using something other than `... USING (username = CURRENT_USER)` ?,1519244235.0
shobble,"can't you just do something like:

     IF OLD.arrest <> NEW.arrest THEN 
     -- log arrest changed
     END IF
     IF OLD.location <> NEW.description THEN
     -- log desc changed
     END IF

If you know it's going ot be one of those fields. ISTR (but I'm by no means proficient at plpgsql-magic) that dynamic field-name resolution is tricky, alhtough there might be some hacky way (casting the whole OLD or NEW record to jsonb or hstore?) to get a structure you can fiddle with.",1519212075.0
pinkcodetiger,"I have to record changes to either column “arrest” or “location_description”, but I won’t know which. 

How can I record the old and new value and e: the column name? Aka. How do I get that dang column name and how do I use it? 

I tried what’s in the picture but psql yells at me for “unnamed” in the trigger. ",1519182395.0
Valachio,I figured it out.  The problem was the file owner for `postgresql.conf`.  I had changed it to my own user.  Changing it back to `postgres` fixed it.,1519163344.0
Vardy,"Try connecting to psql using port 5432:

sudo psql -p 5432


Might also be worth checking the pgstartup.log file. Usually find it in your log directory.",1519162103.0
NihilCredo,"Make your boolean logic order explicit:

`case when isnumeric(total) then (total < 1000000) else false end`",1519153407.0
kodifaer,"I'd normally try to avoid mixed data such as this, but when forced to I'd look into either a case statement or a cte to force two step filtering. Using a subquery instead would still allow the optimizer to potentially push your outer filter down and trigger the exact same issue.

In this particular instance I'd probably go for the case version since your function is immutable. If you have a big enough table to justify it, an index and is numeric field (optionally populated by before insert /update trigger) might begin to be worth considering.",1519153667.0
dcalde,"Why don't you create a tonumeric function that will return numeric or null otherwise. Null will return false for the comparison, making the query quite simple. Select total from metrics where tonumeric(total) > 1000000",1519181290.0
davesharpe13,"There are a couple of ways, common table expressions (CTE) or CASE expressions, depending on size of tables, implementation of ISNUMERIC() etc, I don't know which would be better:

    WITH numerics(total) AS -- with cte can not be 'combined'
     (SELECT total FROM metrics WHERE ISNUMERIC(total))
    SELECT * FROM numerics WHERE total < 1000000;

    -- case expression in the WHERE clause
    WHERE CASE WHEN ISNUMERIC(total) THEN total ELSE NULL END
                < 1000000;",1519153950.0
,[deleted],1519094095.0
doomvox,"Actually no, it's not a performance related issue, it's being pedantic about the meaning of a NULL and the way it behaves.  You can't say that NULL is not equal to any definite value like 123, because NULL means ""I don't know what the value is"" it isn't any particular thing, including ""not 123"".  If you want NULLs you typically need to actually look for them using a test like IS NULL.  

Another good gotcha is doing something like ""GROUP BY type"" and expecting to get a grouping for the cases where type is NULL-- actually, the NULL records don't get included in any of the sets, so that GROUP BY acts as a filter for anything where type is NULL. 

Myself, I tend to agree that NULL is a misfeature-- something like perl's undef (which just acts like a funny value) is a lot less counter-intuitive-- but the people who designed SQL had a mathematical frame of mind, they cared a lot about being consistent with their fundamental principles.  
",1519094106.0
iiiinthecomputer,"This is true for all ANSI SQL databases not just PostgreSQL.

`(NULL == NULL) IS NULL`.

",1519099874.0
dcalde,This behaviour can be overridden using the transform_null_equals parameter. See https://www.postgresql.org/docs/current/static/runtime-config-compatible.html,1519094027.0
cybersurfer2,"Can you provide an example of the data, table schema, and query output?",1519073524.0
mgonzo,"Nothing in that query is adding to the field  path, so that means your data has that extra whatever you are seeing. You will need to find some way to either normalize the path field data or clean you data in whatever way makes sense for it.",1519074718.0
elraineyday,Use one of the built-in string functions to remove the junk characters and then find out why your data has junk to begin with,1519179273.0
faggatron0,"see https://stackoverflow.com/questions/4477301/creating-jobs-and-schedules-programmatically-with-pgagent/25345447

You need to add a schedule to pgagent.pga_schedule",1519070279.0
iiiinthecomputer,"Gotos are fine for patterns like ""do X; if error goto bailout. Do y; ..."" . If there's common cleanup code and your language doesn't have exceptions or they're too expensive, a goto is fine.

No time to read over sorry.",1519044170.0
swenty,"In case, like me, you're wondering why:

[Thoughts on Bump Charts and When to Use Them](http://www.kenflerlage.com/2017/01/my-thoughts-on-bump-charts-and-when-to.html)",1518996571.0
thelindsay,"Thanks for posting this, I had not heard of bump charts but they seem like a useful vis type to know.

I have a few suggestions for your SQL. I realise the post is about bump charts so this is bike shedding but in your other post you mentioned you're working on your SQL skills so maybe it'll help a fellow analyst.

- All 3 CTEs will materialise in memory but you only need/use the third one so they could be collapsed into one CTE. 
- The CTE computes rows for all dates and all groups but many of these will be discarded by the row limit and series limits, so it may be better to move these filters up to the CTEs and do inner joins / equals filters in the main statement instead.
- Sometimes an IN filter can be sped up by changing it to EXISTS with a filter against the outer query inside the filter expression.
- Subject to the query logic and the 'from_collapse_limit' GUC (default 8), the nested subqueries will be flattened out by the query planner, so if the flat version is equivalent and easier to read then you might as well write it that way: that is to say the subqueries aren't necessarily having a performance impact. CTEs and the ""OFFSET 0"" trick are the main/(only?) hinting tools in postgres.",1518999587.0
Darkmere,"Frankly, it depends on the structure of your data.  The reason it takes so much time to index is that it needs to page through all the data in order to build an index.

If your data can be sorted in significant manner,  Partitions and BRIN indexes can help you.

Brin indexes take less space on disk than btree, but has some limitations (are lossy, can only do certain kinds of queries)  but in larger datasets they can be faster than btree, simply because the time to page in the entire btree index from disk can be more IO than the extra overhead of the brin index.

This is _very_ dependent on the dataset and queries though, and _also_ depends a lot on how you sort your data in the tables.

Partitioning  isn't required for parallelism, you can have parallelism on a single table too, but it can give the query optimizer a good chance on picking out data that it should NOT look at.  This can improve performance a lot.


OVerall, there's no way to improve index creation, moving data into partitions and indexing there _still_ requires as much data to be read from disk, totally.",1518869328.0
anras,"If you're never going to modify the table, only read from it, why is index creation time a problem? You would run it once, then it never needs to happen again.",1518879503.0
merlinm,"yep...you are going to have to scale. partitioning is the first step; you can thread out various operations like index creation until you hit the limits of your machine.  you definitely should be using fast drives (solid state) for data this size. 

if that's still not fast enough, you have to scale horizontally. you can do this ad hoc (using FDW wrappers, or possibly asynchronous dblink) or go with a commercial solution such as citus (I don't think redshift is a good fit for your use case).",1518887418.0
NathanClayton,"This is similar to the SQL standard, which is to normalize all names to uppercase unless quoted.",1518799166.0
CowMakesGlue,Is it possible to read without registering?,1518791160.0
hunleyd,"You should not worry about it as it is part of the normal operations of a PostgreSQL db. I'm not sure about RDS, but normally that particular logging can be disabled. I always turn them on ands use pgBadger to graph out the checkpoints to help me understand if they're configured properly or not for my workload.",1518806135.0
Dijit,"I don't think they could have used a worse icon for the table.

for Chrome on Windows 10 it looks like checks and crosses (no colour), for firefox on FreeBSD it looks like little square ""unicode unavailable"" boxes.",1518769767.0
JavaJuke,"Is your data in Postgres? If it is, what are your tables (columns) and what are you trying to achieve?",1518562772.0
faggatron0,"use a tool like liquibase or flyway to manage schema changes for you

I do my db design in pure SQL but that's not for most people",1518560779.0
justAnotherCodeGuy,"I use a text editor, psql, and subversion.  

I write all DDL by hand, and run it on my test server until it works right, then run it on the live database.  

I check everything into subversion.",1518633276.0
lampshadish2,"I used [Navicat Data Modeler](https://www.navicat.com/en/products/navicat-data-modeler) for the initial schema and then [Goose](https://bitbucket.org/liamstask/goose) to handle updates to that schema.  I just edit the SQL in vscode, and tend to use either postico or pgcli when connecting.

DataGrip was really nice, bu pricey for me.

I haven't found a tool that would output DDL diffs at the level of control I wanted.",1518992218.0
zieziegabor,"We don't use PG this way. We use Liquibase(http://www.liquibase.org/) to handle all of the DDL and applying it to various databases. As for designing the DDL, we just do that by hand.

Our Liquibase files live in their own version control repo, and on every push, it's deployed to our dev DB. Every day we(read Jenkins our CI/CD server) run it against our nightly DB dump of production, and if that passes, then it is auto-applied to production.

There are other options besides Liquibase, some are even PG specific (which Liquibase is not).  I'd definitely recommend investing in managing your DDL and schema with *something* and putting it in a revision control system (hg, git, fossil, etc)

As for actual DDL/Schema design tools, that's outside of what we do, since it's all by hand for us, so I'll leave that for others. ",1518566900.0
flyingmayo,"If you could post some specifics (table and trigger definitions?) it will be easier to help.

Some general thoughts in the mean time:

     ""If multiple triggers of the same kind are defined for the same event, they will be fired in alphabetical order by name.""

ref: https://www.postgresql.org/docs/current/static/sql-createtrigger.html

Bearing this in mind, it may be possible to solve your order-of-operations problem with a simple name change (I must admit that I am not crazy about this solution)

It is also possible to have your trigger function include some internal logic that could be used to control what you do and when you do it:

    CREATE OR REPLACE FUNCTION some_trigger_function() RETURNS TRIGGER AS
    $$
         ....
        IF (TG_OP = 'INSERT') THEN
            -- DO THING A
        ELSIF (TG_OP = 'UPDATE') THEN
            -- DO THING A
            -- AND
            -- DO THING B
         .....


Or something along those lines....",1518493132.0
shif,why would you ever want to allow the postgres port to the open web?,1518481995.0
bioszombie,Is the Postgres password really stored as plaintext?,1518531890.0
jroller,Keep in mind that Let's Encrypt certs normally have a 90 day expire time.   You may want to schedule a config reload to reread updated certs.,1518546844.0
mage2k,"> My Rant – Over the last few days, I’ve spent many hours over storage benchmarks from various vendors, but honestly, what’s up with benchmarks that do not use production-grade conditions to demonstrate performance numbers? Some papers appear to purposely hide details to avoid vendors replicating their benchmark, while others game their numbers to make them look good. As an industry, we need to be better than that!

Nope, you're correct.  Vendors will often pull shady crap to post better numbers than are realistic.",1518477078.0
chock-a-block,"Welcome to the IT industry! Management makes bad decisions and sticks you with the consequences.

>I decided to skip the read-only benchmark because it’s useless for production environments.

Maybe for *your* production environment it's useless, but, using read only replicants can be useful.


Can you commit unique spindles to different pgSQL operations?  The common example is to move the transaction log to its own disk/block/channel.  The server logs should be on their own spindles as well.
 ",1518536524.0
trinitri3,ERD is not supposed to be a substitute for the DDL.  Why do you want to model such details in the first place?,1518470453.0
dark-panda,"Get rid of the curly braces around your phone numbers, that isn’t valid JSON. ",1518153546.0
,[deleted],1518159920.0
skeletal88,"Where did you see that conventional wisdom about caching? It doesn't seem correct.

Which configuration parameter are you talking about, that can't go higher than 512MB? Did you mean shared_buffers and the sentence ""The useful range for shared_buffers on Windows systems is generally from 64MB to 512MB."" ?

It doesn't mean that it won't use more than 512MB. You can also read "" ... but because PostgreSQL also relies on the operating system cache, it is unlikely that an allocation of more than 40% of RAM to shared_buffers will work better than a smaller amount"" - meaning that postgresql relies on the operating system file cache. I don't know about windows but if you look at top in Linux then you can see a line similar to ""9179388 buff/cache"" - if Postgresql requests a file from the disk from the OS then Linux will cache it if enough memory is available, so the more memory you have the more will be cached and your queries may not need to hit the disk.

Not a DBA, but went on a course from https://www.cybertec-postgresql.com/en/ They would be happy to help configure your database and answer questions about Postgresql.",1518078500.0
riksi,".. the filesystem will cache the data in memory..
See configs for big boxes . Or post on the mailing list (or search `performance` mailing list )",1518081135.0
Cmshnrblu,One idea before you toss out the windows is to play around with a linux virtual machine on virtualbox (free from oracle). The scope of the rest of the question is beyond me. Good luck!,1518066085.0
,"Disclaimer: I'm no pgsql expert, but you might be able to leverage pgfincore: http://www.villemain.org/projects/pgfincore.

This project purports to be able to - among other things - allow you advise pgsql that you will need table_x often in your workload, and thereby have table_x loaded into OS cache memory (if I read the doc correctly).  It does not say that it changes size of cache, so the cache limits will probably be the same.

Tangentially related, I worked on a project where the fact tables were too big to load into RAM, but loading all dimension tables into RAM improved query performace significantly. 

Food for thought, maybe?  HTH",1518090980.0
gengengis,"If you are open to paying for it, DataGrip from Jetbrains is becoming extremely good.",1518069918.0
scttmthsn,https://www.pgcli.com FTW,1518074929.0
APIglue,"DataGrip is nice, but not FOSS.",1518070682.0
,"Navicat is pretty nice also. Takes some getting used to. 

pgAdmin is SHIT. ",1518064969.0
shanestillwell,Postico on a Mac. Nice UI and toolset for every day db work. Might have to use the CLI psql from time to time. ,1518065566.0
riksi,Assuming you mean 4. Old pgadmin3 is nice too.,1518080975.0
SomeGuyNamedPaul,"Huh, I have DBeaver for dealing with Spark atop of Datastax Cassandra but never considered looking at it for PG.  Will have to give it a try.",1518081824.0
joaodlf,"DataGrip for me. Would love to see Sequel Pro available for Postgres, though.",1518077175.0
Crashthatch,"HeidiSQL works ok... certainly better than pgadmin 4. I get the impression Postgres isn't Heidi's #1 preferred database, but it gets the job done at least as well as pgadmin 3.

Windows only, but supposedly works on Mac and Linux through Wine.",1518091772.0
faggatron0,pgAdmin III or 4?,1518115767.0
cybernd,"DBeaver is okish. Just as warning: don't blindly trust the DDL views. For example primary keys are missing their names there.

My personal hope: it seems like toad for postgresql is in the pipeline. They only released mysql support so far.

--

Edit : DBeaver 5.0.0 has fixed the DDL issue i was reffering to.",1518990206.0
dsn0wman,psql with vi key bindings is pretty much all you need. Unless your some kind of weirdo who doesn't like vi.,1518124302.0
,[deleted],1518086775.0
kellenkyros,"Psequel is a great option, which is lightweight and very handy.
http://www.psequel.com/",1518084307.0
mage2k,For high traffic databases it is absolutely a valid concern.  If you have a light traffic db then you'll probably be fine.  If you do want to set it to all I'd suggest doing it during the busiest part of the day/week for your app and watch things closely so you can quickly turn it off if things do look like they're going bad.,1518048867.0
dtechnology,"Interesting presentation.

Can someone explain why SQL:2003 contains a language feature (filter) which apparently none of the large RDMSs implement or planned to implement? (except for Postgres 12 years later)",1517732487.0
scottocom,Thank you. I learnt a few things one of which is how much more I love postgres and I am glad we moved from MySql. But Most of these I did not know,1517701628.0
thelindsay,"Nice one Markus, your explanations are always thorough and clear.",1517722729.0
antlife,Wait... MSSQL supports XMLTABLE like commands...,1517724306.0
neomis,A good list but you should add all the JSON functions as well.  My company wants my team to migrate from PostgresSQL to Oracle so we conform with corporate IT and the JSON capability in Oracle is really lacking.  ,1518024837.0
mikaelhg,"Thanks for submitting this, shit has changed since 1984!",1517695393.0
wbsgrepit,"It seems the documented use case is much better served having a separate data warehouse/reporting instance (off of the platform and designed for this workload).   If you were to separate the usage from prod data does that not reduce the issues you describe as blockers? 

I agree that AWS has made some pretty substantial architectural tweaks to postgres to drive the goals of the platform -- but I fail to see how (as long as your prod needs align with the goals of that platform) this makes that platform unsuitable for the workload.  

Is your goal to have all of the advantages of that platform with none of the constraints that really must be in place to make it possible?  ",1517769082.0
chock-a-block,"Nice.  

And to think I just got turned down for a job migrating reporting to Amazon's pgsql.  Lucky me!",1517979961.0
,[deleted],1517544306.0
Shananra,Does the community version not support rebalancing at all? Doesn't that make that version a ticking time bomb?,1517548085.0
merlinm,"    SELECT json_build_object( 
        'type', 'Feature', 
        'properties', json_build_object(
          'Foo', 'bar', 
        )
    )

Etc
",1517534009.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/sql] [\[PostgreSQL\] How do I make an object within json\_build\_object()?](https://www.reddit.com/r/SQL/comments/7unm2u/postgresql_how_do_i_make_an_object_within_json/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1517533190.0
ants_a,"RDS is a bit of a ""Hotel California"" in the sense that people find it very hard to migrate off of it when their needs outgrow it.

If you need to implement replication I strongly recommend Patroni. It pretty much automates everything related to failovers and replication. It also comes with scripts for WAL-E so you can store your backups and transaction logs in any S3 work-alike, so you can easily configure backup retention and will have PITR capability. I think DigitalOcean has an object storage solution that might work with S3. Patroni works fine both on a VM and as a containerized solution. There is a prebuilt docker image called Spilo that integrates Patroni and backups via WAL-E. It is meant for Kubernetes deployment, but you can use it without Kubernetes too.

As an alternative to spending your engineering hours on figuring everything out and setting this up or buying a database as a service package you can consider hiring a consulting company to set up your cluster on your infrastructure of choice.",1517574398.0
stympy,"Option 1 is definitely a good option if you don't mind spending a bit more to host everything at AWS/Azure/Google.  It's definitely the option that gets you where you want to be (a stable, highly-available database cluster) without having to spend a lot of time setting up all the moving parts yourself.

Option 2 is the approach I've taken, partly because I like saving money, and partly because I enjoy the process of setting up all the moving parts. :) My setup looks like this: Each postgres instance runs on a VM, leader election and failover is managed by patroni, consul is used by patroni to store state, continuous backups are done via wal-e, and replication is done via postgres' built-in streaming replication.

I'm actually considering bundling all this setup info (including ansible scripts to automate all the setup, and pgbouncer config to make it painless for clients to move to a new leader during failover) into a product (e-book bundle with code, or something similar).  If you'd be interested in such a thing, feel free to send me a message to let me know... I'd be happy to give you free access to what I put together in exchange for your testing it. :)",1517587058.0
chock-a-block,"1. RDS isn't the silver bullet you want it to be.  It doesn't solve your problem of ""not worrying about the database."" You get a new set of problems, easily more vexing than running your own.  Did I mention cost?

2. Maybe you guys are still in hobby mode, but, you still need backups.

3. You can run a SQL server in a VM just fine.  It's probably a pretty good idea for a while.

4. Why complicate replication? Use the log shipping in the server.  It works.  It's well documented.

5. Pacemaker cluster?  It is very good.  If you are really broke, set up two identical servers.  There are a couple of different ways to set up a poor man's cluster.

Your DB resources need monitoring.  I like zabbix.  Plenty of Postgresql resources for zabbix.  ",1517635678.0
francisco-reyes,"For a small setup, using a service may be ok. Amazon has 2 different setups RDS and Aurora.

Amazon claims Aurora performs better than running your own database on a EC2 machine. If I recall Aurora is over $100/month so if you wanted replication, will be over $200. 

Aurora may be a good starting point and then later, once the company is making money, you could re-evaluate your options.

Ultimately you want your DB close to your front end machines. if you plan to have your front end machines in AWS then postgres in AWS (on a VM, RDS or Aurora) is your best bet. About the worst, in my opinion, you could do is to have front end / web server in one provider and the DB in another. Specially if those are in different geographical areas.",1517537418.0
jrwren,systemd caches its services files. You need to refresh it.,1517448943.0
BloodGutsAndBeer,"I had to google that and came up with 
systemctl daemon-reexec and systemctl daemon-reload. Neither of those worked so I rebooted.
Now the DATA_DIRECTORY is the original one again.
Obviously I am missing something here.  I guess my question at this point is: How do I permanently change the DATA_DIRECTORY location for postgresql_9.4? ",1517450566.0
to_wit_to_who,"I use [COMMENT](https://www.postgresql.org/docs/10/static/sql-comment.html) on all database objects.  I also use [PostGraphQL](https://github.com/postgraphql/postgraphql) to generate the base API.  PostGraphQL uses the COMMENTs to generate documentation for my APIs, which can be seen when using tools like [GraphiQL](https://github.com/graphql/graphiql) which comes built-in with PostGraphQL.  
  
In addition to that, I use Liquibase as well and keep comments there, but those are higher-level and not meant to be detailed documentation.  
  
My primary source of documentation is using [mkdocs-material](https://github.com/squidfunk/mkdocs-material) in dedicated repo for the project.  I use CI/CD to automatically build the static site and deploy it for reference.  
  
Hope that helps.",1517355314.0
vitingo,I use [liquibase](http://www.liquibase.org/) + git,1517400728.0
MonCalamaro,"We use [flyway](https://flywaydb.org/) for our database migrations.  I really like the simplicity of the command line interface and being able to just write plain SQL for my migrations.  I have heard good things about liquibase as well, but I haven't used it personally.

",1517410723.0
DatAss-embler,"We use pgmodeler. If you haven’t checked it out it’s totally worth doing. Inexpensive (free if you compile yourself). You design the database in pgmodeler, export that to a Postgres instance. The file that is saved is XML so you can use git to source control the changes. ",1517369622.0
anupsaund,We use https://gitsql.net,1517692079.0
jgardner,NO.  This is how Oracle used to do it and it was the cause of so much downtime. What happens when the rollback segment is huge and the transaction aborts? What happens when it fills up? MVCC is the best way. ,1517343226.0
linuxhiker,"This is great news, sounds a bit like rollback segments.",1517341370.0
therealgaxbo,Isn't there also a tacit implication here that infrastructure is starting to be put in place to allow pluggable storage backends?  I'm pretty sure I've read Robert previously saying that's a direction he'd like to go in.,1517391594.0
h2m85,"You may want to use json_build_object function which you would call like 
SELECT json_build_object(
        'tractce', tractce,
        'countyfp', countyfp,
        'statefp', countyfp,
        'geom_geojson', ST_AsGeoJSON(geom)
       ) 
FROM  theme_mapbasetracts 
WHERE ST_MakeEnvelope( -74.084246673584,40.67638769936645, -73.7572317504883,40.800601763990066) && ST_Transform(theme_mapbasetracts.geom,4326)

This outputs in key-value pair.",1517262434.0
dopperpod,Hosting it yourself. I’m not aware of any others that go that high. ,1517233405.0
mikaelhg,You can't afford 3€ a month for https://www.hetzner.com/cloud or competitors?,1517357642.0
SomeGuyNamedPaul,Google Compute Engine has a free tier VM option that would be more than sufficient.,1517235934.0
dopperpod,I just meant hosting it locally on their dev machine or a Raspberry Pi. ,1517239800.0
Darkmere,"Others mentioned Digital Ocean, so here's a [Referral Link](https://m.do.co/c/223d87857923) to DO.

Use that for a signup and you'll get $10 credited, which nets you their cheapest VM for 2 months, which should be enough for 30k records.

",1517266559.0
joelparkerhenderson,Good work and thank you for sharing!,1517168298.0
wviana,Really liked the \s and \p. Will set vim as editor and try it. ,1517189959.0
w1539521,"Just a small note, I advise you to declare your get_last_2_messages function as STABLE. This allows the planner to cache the result so a subsequent call (in the same execution) can profit from this. See here for more information: https://www.postgresql.org/docs/current/static/sql-createfunction.html and https://www.postgresql.org/docs/current/static/xfunc-volatility.html",1517229793.0
alephylaxis,"If your concern is the node having shell access, you could just set up ssh like [this](https://unix.stackexchange.com/questions/235040/how-do-i-create-a-ssh-user-that-can-only-access-mysql). Make a user account specifically for tunneling and keep your file permissions tight.",1517131750.0
jk3us,">Why would someone want to create a commented out DROP DATABASE statement immediately? 

I don't use pgadmin and don't know where this comes from, but if you're developing, you're going to want a script that you can run repeatedly, so you'll likely be dropping things you've created before creating them again. So scripts often begin with a bunch of drops or drop if exists statements.",1517329231.0
funny_falcon,Don't you try pg_pathman? It should provide better performance when partitions number increased. But probably it is not integrates as seamlessly as pg_partman.,1519017655.0
merlinm,Can you give an example? Afaict to_json gives well formed json in all cases... ,1516943777.0
riot-punch,"How are you performing your database access? EF, ADO.NET using DataReaders, Dapper? Are you using Npgsql?",1517023040.0
stympy,"We use [patroni](https://github.com/zalando/patroni) at honeybadger.io for our HA PG cluster, and we love it.  The leader takes all writes and replicates to N followers.  The clients use [pgbouncer](https://pgbouncer.github.io) as a proxy, and pgbouncer is reloaded with a new leader IP address during failover by [consul-template](https://github.com/hashicorp/consul-template).",1516882621.0
chock-a-block,"Active-active isn't actually HA without some kind of pool/load balancing.   And then connection pooling/load balancing introduces a bunch of complexity that require baby sitting until you find ""good enough.""  Some are okay with this.

I've used pacemaker cluster in active/passive and it's great.  Plenty of storage options to try.

I've also used streaming replication as reliable failover with keepalived.  It's not going to give you *instant* promotion to master, but it's close, ridiculously simple, and allows you to use ordinary hardware.  It is ""good enough"" for many situations. 


",1517007287.0
dtechnology,"I'm struggling to understand your difficulty here, can't you just `OR` the three keys existing check (for filtering) or use `COALESCE` on the three keys?",1516821929.0
Citizenfishy,DROP view view1 CASCADE will drop the dependents but you will need to recreate the dependent views,1516743212.0
therealgaxbo,"Depends what you're wanting to change about the view.  If you want to drop columns then you'll have to drop and recreate all dependent views.  If you only want to add new columns (and/or change the calculation behind existing columns) then you can do so with `create or replace view1 as select ....`

See https://www.postgresql.org/docs/current/static/sql-createview.html",1516743661.0
djrobstep,"You can use migra to generate the necessary changes:

https://github.com/djrobstep/migra

If will compare your before vs after state and spit out something close to the minimum changes required.

(disclaimer, i'm the author)",1516777904.0
mage2k,That covers the old-school hacky method.  It should be noted that Postgres does now have built-in [declarative partitioning facilities](https://www.postgresql.org/docs/10/static/ddl-partitioning.html).,1516566194.0
skeletal88,"Corrupted, how? What kind of error messages are you getting?

If just indexes were corrupted then you could recreate them.

If your backup was made by copying the datadir then you need to set up the same version that you made a copy of, that is 9.2 for you.",1516353331.0
mage2k,"> Restore the Full backup from 9.2(Is this equal to restore the basebackup)

No.  A base backup is not enough.  You need the base backup plus all of the WAL data written from the time the base backup started and the time it finished.

> Restore the incremental backup (Is this possible to restore the WAL logs from 9.2 to 9.6).

No.  You can not use 9.2 base backup + WAL to directly set up a 9.6 server.

In order to upgrade to 9.6 you'll need to either:

1. Get pg_upgrade working.
2. Dump the data from a 9.2 server and restore it to a 9.6 server.

All that being said, I'm also interested in hearing about this index corruption after your pg_upgrade run.",1516395618.0
mage2k,"> Restore the Full backup from 9.2(Is this equal to restore the basebackup)

No.  A base backup is not enough.  You need the base backup plus all of the WAL data written from the time the base backup started and the time it finished.

> Restore the incremental backup (Is this possible to restore the WAL logs from 9.2 to 9.6).

No.  You can not use 9.2 base backup + WAL to directly set up a 9.6 server.

In order to upgrade to 9.6 you'll need to either:

1. Get pg_upgrade working.
2. Dump the data from a 9.2 server and restore it to a 9.6 server.

All that being said, I'm also interested in hearing about this index corruption after your pg_upgrade run.",1516395632.0
mage2k,"> Restore the Full backup from 9.2(Is this equal to restore the basebackup)

No.  A base backup is not enough.  You need the base backup plus all of the WAL data written from the time the base backup started and the time it finished.

> Restore the incremental backup (Is this possible to restore the WAL logs from 9.2 to 9.6).

No.  You can not use 9.2 base backup + WAL to directly set up a 9.6 server.

In order to upgrade to 9.6 you'll need to either:

1. Get pg_upgrade working.
2. Dump the data from a 9.2 server and restore it to a 9.6 server.

All that being said, I'm also interested in hearing about this index corruption after your pg_upgrade run.",1516395644.0
Darkmere,"Test if you can in-place upgrade your existing first, and then migrate. That may be a faster way of migrating for you.

The other thing to ponder is, do you need _all_ that data to be back from the downtime, or can you migrate _parts_ of it, get service up and running, and then move in the larger bulk of data?

",1516270852.0
z0rb1n0,"If you need blazing fast DML performance, the client library efficiency is the last thing you should worry about.

You need to eliminate overhead, and inserting/updating rows one by one is the most common mistake when it comes to that.

Use multi-line inserts:

    INSERT INTO things (thing_id, updated, payload) VALUES
    ('2188df06-0375-4194-b56b-4b65b9474616', '2010-01-01', '[""foo""]'),
    ('6c318dd5-2e55-4895-85c4-7e8648d90171', '2013-05-11', '[""bar""]')
    // ...
    ;
(Avoid inserting too many rows per batch this way as that may take more memory than you have)

Unfortunately parametrised queries (the way most client libraries go about it nowadays) are not good when the set of parameter is arbitrarily long, so here's the recipe I give to our developers when they want to wrap everything in a single argument and conveniently feed the whole data set to the query as a JSON string:
    
    -- assuming the following table...
    BEGIN;
    
    CREATE TABLE things(
    	thing_id UUID NOT NULL,
    	updated TIMESTAMPTZ NOT NULL DEFAULT transaction_timestamp(),
    	payload JSONB NOT NULL,
    	CONSTRAINT pk_things PRIMARY KEY (thing_id) -- note the name. This is internally implemented as a perfectly usable index
    );
    
    
    INSERT INTO things(thing_id, updated, payload)
    	SELECT
    		upsertables.thing_id,
    		upsertables.updated,
    		upsertables.payload
    	FROM
    		/*
    		 * The following guarantees cast to the target tables's types, it
    		 * however also emits the remaining table columns, but without the defaults
    		 * you might want there hence why in the list above you want to
    		 * only select the columns that matter
    		 *
    		 */
    		jsonb_populate_recordset(
    			null::things,
    			'
    				[
    					{""thing_id"":""1277c75e-3d53-44c3-abe2-709514732aab"",""updated"":""2017-10-25 17:23:41.081271 UTC"",""payload"":""{\""the_value\"": 718, \""the_string\"": \""hello_world\""}""},
    					{""thing_id"":""2188df06-0375-4194-b56b-4b65b9474616"",""updated"":""2017-10-25 11:38:15.924766 UTC"",""payload"":""{\""the_value\"": 421, \""the_string\"": \""hello_world\""}""},
    					{""thing_id"":""21eb94a6-2cf1-48c9-aa50-58bdc5dd743d"",""updated"":""2017-10-25 23:47:44.600941 UTC"",""payload"":""{\""the_value\"": 233, \""the_string\"": \""hello_world\""}""},
    					{""thing_id"":""34ea52ff-00a7-49e2-8b71-5baf519e4b8f"",""updated"":""2017-10-25 22:25:11.972041 UTC"",""payload"":""{\""the_value\"": 81, \""the_string\"": \""hello_world\""}""},
    					{""thing_id"":""6af0df92-278b-47cb-98f5-ca39b94425c3"",""updated"":""2017-10-25 09:51:53.420499 UTC"",""payload"":""{\""the_value\"": 320, \""the_string\"": \""hello_world\""}""},
    					{""thing_id"":""6c318dd5-2e55-4895-85c4-7e8648d90171"",""updated"":""2017-10-25 13:21:57.788946 UTC"",""payload"":""{\""the_value\"": 699, \""the_string\"": \""hello_world\""}""},
    					{""thing_id"":""845d23cc-df43-43a8-8be3-7265608eba04"",""updated"":""2017-10-25 18:53:34.954931 UTC"",""payload"":""{\""the_value\"": 782, \""the_string\"": \""hello_world\""}""},
    					{""thing_id"":""afc9b142-5488-496f-b780-9e78286b16a5"",""updated"":""2017-10-25 13:13:53.643393 UTC"",""payload"":""{\""the_value\"": 301, \""the_string\"": \""hello_world\""}""},
    					{""thing_id"":""cab67da5-7030-410d-99d6-11e7cc5d0289"",""updated"":""2017-10-25 06:18:58.256359 UTC"",""payload"":""{\""the_value\"": 878, \""the_string\"": \""hello_world\""}""},
    					{""thing_id"":""e2a18567-57dc-4370-b349-30ad7206e153"",""updated"":""2017-10-25 14:15:30.831024 UTC"",""payload"":""{\""the_value\"": 190, \""the_string\"": \""hello_world\""}""}
    				]
    			'
    		) AS upsertables
    	ORDER BY
    		-- this avoid confusing deadlocks
    		upsertables.thing_id ASC
    ON CONFLICT ON CONSTRAINT pk_things
    DO
    	UPDATE SET
    		(updated, payload) = (EXCLUDED.updated, EXCLUDED.payload)
    	WHERE
    		(things.thing_id = EXCLUDED.thing_id)
    RETURNING
    	thing_id,
    	updated,
    	(xmax = 0) AS is_new_record
    ;
    
    
    ROLLBACK;

This is obviously an UPSERT.

The JSON approach is less efficient than more packed input record representations but should still run circles around row-by-row inserts.

Also, I modified the above example for the post and I have no postgres instances within reach, so it's untested.
",1516217460.0
frankwiles,"“Felt slow” isn’t the best complaint. MANY high performance systems use psycopg2, so many some extra info on what you’re doing. COPY is always going to be faster than INSERT no matter the client library you use. ",1516242032.0
3bodyproblem,I’d be curious if/when AWS RDS gets around to supporting it. ,1516241243.0
stympy,"We use [patroni](https://github.com/zalando/patroni) at honeybadger.io for our HA PG cluster, and it is fantastic.  Primary failure and failover is almost a non-event.",1516203978.0
bradtech,So I got the out of the box Master-->Hot slave stand by working. Created a table & do some inserts. It replicated over fine. I guess the part that is missing for me is what happens when you failover to the slave? You have to reconfigure the app to point towards the new read/write master?  ,1516306292.0
bradtech,"Had any of you guys looked into using pacemaker/corosync that has a vipmanager? How does that solution look compared to patroni? 

https://icicimov.github.io/blog/database/PostgreSQL-High-Availibility-with-Pacemaker/",1516371850.0
linuxhiker,An easier route would be to use a table space.,1516146091.0
swenty,What's the current status?,1516125773.0
eCommerce_2015,"It looks like a useful tool.  Though do you really think that a gif was the best medium to show off the features?  

Also, it would be great if the snapshots and comparisons had semi-meaningful names.  Maybe variations on the database names?  Otherwise unless your users use this process only once, it's going to get confusing.",1516082216.0
,"Try Open Modelsphere.  You would need to define the missing types yourself (possible in OM). You can reverse/forward engineer via ODBC. Printing of models is possible. 

There’s quite a learning curve, but you’ll get most of what you want for free.  

Caveat: cannot speak for specific pg functionality because I was using other dbs. ",1515927993.0
jrwren,I think pgmodeler does all of these things https://www.pgmodeler.com.br ,1515964131.0
,[removed],1516359291.0
sylgeist,I believe that is by design. Pg_hba is a first match wins config so you have to arrange accordingly.,1515887337.0
qsun7,"No, the IP is unique for each entries we entered. So fir match wins does not apply here it seems.",1515893380.0
qsun7,"mage2k, if that is the case, why am I getting no pg_hba.conf found error?",1515976626.0
linuxhiker,"The biggest are:

1. Our data types are strict see other comments on things like dates.
2. We will not truncate data to fit in a column (MySQL safe mode off does this)
3. Vacuum is important don't forget it.
4. Oh crap, you mean this works? Yes you will experience that.
5. Our gui management is not as polished or useful as MySQL. Expect some frustration unless you purchase something commercial.
6. We rock.",1515792599.0
jgardner,"Not so much ""gotchas"" but ""Wow, this actually works!""

I would learn more about the many features of PostgreSQL. Use it for several smaller projects.

If I were moving from MySQL to PostgreSQL, I'd have to do a ton of planning and experimentation and modifications to the code to get it all to work. It would be a major undertaking that would take months, not days. (Assuming your codebase is sufficiently large.) I would try to do it one piece at a time, maybe move one table or a group of tables, rather than everything.

Hopefully you're using a data abstraction layer like SQLAlchemy, or you're going to have a really bad time.",1515791674.0
sisyphus,"Usually that things that used to get accepted now will not because postgres has no 'loose modes' of operation, like

I notice colleagues who came from a lot of mysql can get lazy with grouping, like ""select a,b,c from table group by a"" - pg will reject that, you have to write out all the columns you want to group by.


If you are admin for this server as well, getting used to postgres's permission system with pg_hba.conf over just having to do 'grant all on db.* to user@wherever identified by password' I've seen lead to some frustration.


Dates have to be real dates--your imports will start failing instead of inserting 0000-00-00 into a date field on some invalid input.


If you rely on sorting via mysql collations you want to check carefully that pg is doing the same thing.",1515791732.0
Flyen,"Logical replication is pretty new to the Postgres core, so be careful if you're trying to apply MySQL replication strategies to Postgres.",1515798134.0
geosoco,"A super tiny gotcha I've encountered. You can't insert text/json with NULL characters in it (it'll often be encoded, \0, \x00, \u0000). 

This probably isn't an issue unless you're ingesting datasets from elsewhere on the web where bad apps have inserted it. ",1515813735.0
EvanCarroll,"> I want to hear about the bad parts

... crickets... MySQL's logical replication is a little bit more advanced I think which may make a difference for big installs. Namely, PostgreSQL uses one backend and writes in-line, and MySQL can use multiple backends and write out-of-line. Also I consider myself a fan on MySQL's [INSERT ... SET](https://dev.mysql.com/doc/refman/5.6/en/insert.html) syntax.

Shy of that I don't think there is *anything* that makes a difference that would benefit MySQL. Sane transaction model, more indexes, extension support, procedural language support, GIS, foreign data servers, and FDW. Super long list of improvements.",1515874663.0
Foodei,Are you converting code from MySql to PostgreSQL ?,1515797309.0
So_average,"Be careful when installing packages (for extensions for example). I needed a -devel package and mistakenly upgraded the database binaries. Postgresql.conf got overwritten and the db wouldn't start (pgdata was a non default location).

Pg_hba.conf management is (IMHO) à passion in the arse.
",1515829013.0
tektektektektek,Your table joins are about to get a magnitude faster.,1515842289.0
r0ck0,"I'm also a recent convert (about a year ago).  I love postgres, and will never be starting a new project in mysql.  But here's some things I miss from mysql / initial hurdles:

* You can't reorder columns.  I know this is a preference/superficial one, but it was very convenient being able to put the most important columns at the start of my tables when navigating/debugging mysql databases in a GUI, without needing to add views n stuff.
* ""updated_at"" columns that automatically set the current timestamp every time you update a record need a trigger.  Mysql's ""ON UPDATE CURRENT_TIMESTAMP"" was simpler/cleaner
* As others mentioned, postgres is much stricter in most ways.  Sometimes it was a little frustrating to have to figure this stuff out with project deadlines etc - but in the long run it is way way better.  Once you figure these things out they will no longer bother you, so just be patient with that stuff.  It's really just forcing you to do things properly and avoid future problems, especially silent failures which mysql is terrible for.  One example is that you cannot DROP VIEWs that other VIEWs are referencing.  You need to add `CASCADE`, and this will also delete any depenent VIEWs at the same time.  So you need to add them all back with a script or something.
* I reckon both pgadmin 3 and 4 are pretty shit.  Heidisql's support for postgres is pretty limited, not much good for changing tables etc.  Note that if you use a jetbrains IDE, it comes with the functionality of datagrip.  So I just do most stuff in phpstorm now.

Even having only used postgres a decent amount for 1 year, the number of things I'd miss if I went back to mysql is already huge.",1515846193.0
Darkmere,"Diskspace:  
Postgres is a bit more disk-hungry, especially for indexes, than MySQL is.

An index can easily end up taking a _lot_ of space, and careless use of indexes ( too liberal)  can sometimes kill your disk-cache simply because it's paging in a large index.",1515876363.0
merlinm,"* Biggest headache from people coming from other systems is automatic case folding of identifiers.  There are simple techniques to work around it (for example, you might want to consider naming_identifiers_like_this) but it can be obnoxious at times.

* The query optimizer is a capricious beast on any system, and your little bag of tricks you know to make queries go quickly may not translate very well.

* Generally speaking, major release updates on mysql are easier than in postgres.

* There is no analog to 'index organized tables' in postgres.  This is the way that both sql server and mysql do things.  I think we're on the right side of that tradeoff but you have to be aware of the differences.

* vacuum and hint bits can cause I/o headaches.  learn about 'hot' and use it when designing tables (for example, try to avoid indexing frequently updated columns if you can).

* replication works *very* differently in postgres.  Better generally, but definitely different.

* psql is an excellent shell. learn to use it.",1516316347.0
ants_a,"    CREATE FUNCTION array_pick(arr anyarray, indices int[]) RETURNS anyarray AS $$
            SELECT array_agg(arr[i]) FROM unnest(indices) i
        $$ LANGUAGE SQL;
    SELECT array_pick('{""a"",""b"",""c"",""d"",""e"",""f"",""g"",""h"",""i"",""j""}'::text[], array[1,5]);",1515769041.0
craig081785,"The numbers in this post feel off and counter to what we've seen in a number of cases. No not every database we run and manage for customers has observed a 30% hit, but we're very familiar with cases where that type of hit is very realistic. If you're running on AWS and on PV instances instead of HVM then expect a more significant hit. 

The point that it is not something Postgres can directly fix absolutely makes sense, but to say it doesn't have much impact to Postgres... well the reality of that really depends on your infra and on your workload cause it can absolutely have a significant impact.",1515684051.0
,[deleted],1515645974.0
shobble,"I have 

    \pset null '(null)'
    \pset columns 180
    \pset format aligned
    \pset border 2
    \pset linestyle unicode
    \set _lessargs `echo $LESS`
    \set _pagercmd less ' ' -S :_lessargs
    \setenv PAGER :_pagercmd

I forget exactly why the `$LESS` contortions are needed, but it doesn't seem to always use them as expected otherwise. the `-S` is particularly useful with `\pset format aligned' to not otherwise screw stuff up.

my default shell `$LESS` args are `-i -g -M -R -x4 -X -f -F -z-1`

There's also https://github.com/okbob/pspg that looks interesting that I still haven't gotten around to messing with properly.",1515608776.0
ants_a,"I would store all ancestors on a single row in a `int[]` column to at least somewhat reduce the per row overhead. You can still easily do the query you had there

    select c.* from maps c
        inner join treepaths t on c.id = any t.ancestors
        where t.descendant = 4;

You can also create a GIN index on ancestors to speed the query to find all descendants:

    select c.* from maps c
        inner join treepaths t on c.id = t.descendant
        where t.ancestors @> array[4];",1515613953.0
faggatron0,you don't need to use closure tables in PG because it supports Recursive CTEs. Use an adjacency list,1515704219.0
mardukaz1,"Nice, can't wait for PSQL 11.",1515587107.0
z0rb1n0,"Speculation time:

pg_dump uses a `REPEATABLE READ` transaction and issues `LOCK TABLE %s IN ACCESS SHARE MODE` on each table to ensure that the DB snapshot is perfectly consistent.


If someone's workload is holding a lock that conflicts with table-level `ACCESS SHARE`, pg_dump's `LOCK` command will block until they release said lock


You can easily verify this by selecting from `pg_stat_activity`: if I got it right, the pg_dump session will be in a `waiting` state while running a `LOCK` command over the contended table",1515498333.0
depesz,"You seem to have either lots of tables, or lots of churn in tables (create/drop).",1515496828.0
merlinm,Your system catalogs could be bloated.   Try vacuum (with full if you can handle the lock) on various system catalogs.   Start with pg attribute and go from there. ,1515511981.0
Lithic_Declina,"""We have been attempting to investigate the issue and coordinating with Mailgun, a third-party vendor we've recently been using to send some of our account email messages including password reset email messages, "" it continued",1515530873.0
dustyshackleford,"I would do this using plpgsql. You cannot be quite so dynamic using a straight SQL query, but a plpgsql function has you covered. I whipped this up for text columns since thats what you appear to be working with, you could adapt to other types of columns too. 

One thing to note, you are returning t.* in your sample up there, you can return SETOF records from a function but I did not implement that here. Also, as I have no idea how you intend to consume the input data, one row at a time, passed in as an array, from a json object, or whatever, I wrote this in expecting that you can call it for each new table/column/value pair you have.

     CREATE OR REPLACE FUNCTION updatetextcolumn(
        pkey integer,
        pkeycolumn text,
        ptable text,
        pcolumn text,
        pnewvalue text)
      RETURNS boolean AS
    $BODY$
    DECLARE
      _query TEXT;
    BEGIN
    
      IF pcolumn IS NULL THEN
        RAISE EXCEPTION 'Column cannot be null.'; -- check others if you wish
      END IF;
    
      _query := format('UPDATE %I SET %I = %L WHERE %I = %L;',
                        ptable, pcolumn, pnewvalue, pkeycolumn, pkey);
    
      EXECUTE _query;
      
      RETURN TRUE;
    END;
    $BODY$
      LANGUAGE plpgsql VOLATILE;

Use it like so:

    select updatetextcolumn(0, 'colID', 'mytable', 'colA', 'newvalue');
",1515521042.0
johnfrazer783,"What I've come to do (with datasets many magnitudes smaller than yours TBS) is to read that data into JSONb and then copy out the common and most relevant pieces into ordinary relational tables. 

JSON is a great format to source data from that is not 100% consistently typed but it's also a bit of a nuisance to do all your DB things with it (read 'SQL quickly loses its elegance'). When faced with product data I'd always assume there to be lots of commonalities across most products (shipping size, price per unit, manufacturer, EAN or similar product code) and also lots of specifics that require more thinking and also more experience with the data and what queries will look like. 

I recently solved that second part in an application by storing JSONb objects containing the 'post-normalization informational residue' alongside with the respective key record; that way, none of the data is lost, the vital parts are all clean and rectangular, and nothing keeps you from tapping that residual data and build more normalized views or tables later on. It certainly helps that in this design, newly added data will just flow into the existing structures and become visible without extra effort. Where updates might  slow down things, you can always insert materialized views, but then you'll have to organize your `refresh materialized view` statements in some way.",1516043584.0
ants_a,"These are some pretty large JSON documents, reading them in from Toast and decompressing them is going to be expensive. Work that is going to be wasted if you then extract a small fraction of it. If the access patterns are known I would consider splitting the documents into separate columns based on what is filtered on, modified and/or accessed together. ",1515431590.0
Darkmere,"Well,  http://www.pgbarman.org/ or maybe https://github.com/wal-e/wal-e",1515407670.0
jakkarth,There's nothing in that error/stack trace indicating the problem is with postgres. What led you to that conclusion?,1515334534.0
Darkmere,"So, do you still need to store all passwords for all users in cleartext on the bouncer configuration?",1515284857.0
jcriddle4,"This has pgbench v10.0 test:  
https://www.phoronix.com/scan.php?page=article&item=linux-415-x86pti&num=2",1515016495.0
CSI_Tech_Dept,Not sure how to feel about it when elastic search is a runner up and mongo two times winner.,1514910035.0
ruertar,"As much as I like that PostgreSQL is getting the positive attention it deserves, I always considered it a secret weapon.  I loved to hear that other people used MySQL or some other RDBMS.   Popularity doesn't make a database good or interesting.   In fact, if they were compared merely on technical merit, PostgreSQL should have come up winner every year.",1514914446.0
jakdak,"Headline: ""PostgreSQL is the DBMS of the Year 2017""

Look at the data:  PostgreSQL is #4 after Oracle, MySQL, and MSSQL

Reality:  Making a popularity list like this with a mix of enterprise, NoSQL, search engines, etc is completely meaningless.",1514945316.0
therealgaxbo,"Very well reasoned article, thanks.

> [artificial keys] also improve lookup and join speed by avoiding string (or multi-column) key comparisons

One flip-side of this is that in some cases using natural keys means that you can avoid a join altogether, as the key may already contain the information you require.  I wonder which effect dominates in a typical application (as if there is such a thing)?

I expect the large majority of queries would still need their joins and so would benefit from surrogate keys, but the minority of cases where joins can be eliminated would show a much larger benefit for that query...",1514890430.0
billrobertson42,Find the directory where EnterpriseDB dropped the psql program and add that to your path.,1514659598.0
,[removed],1514527098.0
MacCH2,"While this is not specific to PostgreSQL, you may want to check some background information about Cubes on [Wikipedia](https://en.wikipedia.org/wiki/OLAP_cube) for instance.

This is all linked to Datawarehousing techniques, typically the [snowflakes schemas](https://en.wikipedia.org/wiki/Snowflake_schema) - they can speed up access to data in case of large tables and provide a more ""business-oriented"" view of data. It's a good practice to use those cubes for reporting / analysis reasons.

Typically Business Intelligence databases are built by taking the data in your [OLTP](https://en.wikipedia.org/wiki/Online_transaction_processing) database and use an [ETL](https://en.wikipedia.org/wiki/Extract,_transform,_load) tool to populating an [OLAP](https://en.wikipedia.org/wiki/Online_analytical_processing) database where your cubes live.

The data is then access by business analysts using a reporting tool - for instance Jasper Reports, Pentaho (both partially Open-Source) or Tableau, QlikView (both commercial) to name a few.",1514544715.0
billrobertson42,Thanks to all for the info.,1514593320.0
EvanCarroll,"I wrote an answer one time on DBA.StackOverflow that used it. Check it out,

https://dba.stackexchange.com/q/163207/2639",1514566290.0
macdice,"Very interesting, thanks.  I also recently did some PostgreSQL on FreeBSD vs Ubuntu benchmarking, on AWS 64 vCPU instances, and saw the same difference: FreeBSD was almost twice as slow on read-only tests.  I didn't know if I was seeing virtualisation effects, but your results are similar and  from bare hardware.  I'm very interested to figure out what's going on there on the FreeBSD side.  I was planning on investigating further with flame graphs of user and kernel stacks, when I have more time...",1514455281.0
francisco-reyes,"During the FreeBSD test was the zfs recordsize changed to  8k to  match the PostgreSQL block size? I have read that helps.

On this URL http://evol-monkey.blogspot.com/2017/08/postgresql-on-zfs.html, also read that this additional setting may also help: logbias=throughput",1514472321.0
BasementTrix,"Only ZFS was configured for compression.  Why?  Although beneficial for space, it does incur overhead.",1514475372.0
jrwren,Ubuntu supports ZFS. It would be nice to see test results using that.,1514483547.0
AQuietMan,"> While unique constraints are great for maintaining application level consistency . . .

Unique constraints are for maintaining database-level consistency. ",1514427048.0
z0rb1n0,"Or, you know, you could just use the oldest trick in the book: have all your transactions modify data (including creating unique index keys) in the same order.


That way deadlocks are made mathematically impossible.
",1514462972.0
bisoldi,"I ran into this (specifically the unique constraint violation) while building a twitter pipeline using Hibernate, that broke up the various components of the tweet (the tweet, user profile info, @mentions, as well as various deduped tokens.  

I ran into the unique constraint violation all the time.  I was trying to allow the violation to simply refuse to insert a unique value already in the table and return the Id of the existing value.  Seemed perfect until the violation simply kills the hibernate session.  ",1514425494.0
,[deleted],1514498114.0
joeblou,I use it. Love it. Not sure I am missing anything just gaining. Gaining hints and colours. ,1514345575.0
qatanah,I tried to use it on a db with lots of tables.  I just had to killed it since it was preloading stuff.,1514371426.0
lilith2k3,"Yes. I am using it for quite some time. And I never looked back - or actually I had only a short intermezzo with psql ane the very next day I stumbled upon pgcli.

Regard of what I am missing, I can no say anything worth mentioning. OTOH I can't imagine switching to psql, since pgcli does everything I need. ",1514322311.0
preggo_worrier,Been using it ever since I found it. I use it as a 'last resort' and 'doing it raw' of some sort if both of my web-based clients doesn't like my SQL query.,1514365482.0
francisco-reyes,"See self contained sample code below of my understanding of what you are looking for:


    drop table if exists entries;
    drop table if exists users;

    create table users
    (
    user_id serial primary key,
    name    text
    );

    create table entries
    (
    entry_id serial,
    user_id  int references users,
    distance int
    );


    insert into users (name) values ('first'), ('second'), ('third');
    insert into entries values (1,1,1);

    select * from users;
    select * from entries;
    \echo outer join
    select u.name, sum(e.distance) from users u left outer join entries e on u.user_id = e.user_id group by u.name having sum(e.distance) is NULL;

And the output:

    DROP TABLE
    DROP TABLE
    CREATE TABLE
    CREATE TABLE
    INSERT 0 3
    INSERT 0 1
     user_id |  name  
    ---------+--------
           1 | first
           2 | second
           3 | third
    (3 rows)
    
     entry_id | user_id | distance 
    ----------+---------+----------
            1 |       1 |        1
    (1 row)
    
    outer join
      name  | sum 
    --------+-----
     third  |    
     second |    
    (2 rows)
",1514313990.0
dstrait,"Given my understanding of your problem, I would try something like this:

    SELECT *
    FROM USERS U
    WHERE  NOT EXISTS (SELECT * from ENTRIES E on U.ID = E.User_id)

That should give you a user that has not signed up for a run of any type. 

I'm not familiar with Postgresql, so I'm ignorant of any performance best-practices that I might be violating.",1514307663.0
torkild,"Maybe this:
select *
from users u
    left join entries e on u.id = e.user_id
where e.user_id is null",1514311994.0
therealgaxbo,"    Select *
    From USERS U
    Left Joins ENTRIES E on U.ID = E.User_id
    Where E.Entry_type not in (0,1,2,3) and U.Runs_completed = 0

So you're doing a left join from users to entries - this is so it will also include users with no entries.  In which case, what is the value of e.entry_type?  It's not 0,1,2 or 3 for sure, but it's also not NOT 0,1,2,3 or 4.  This is initially counterintuitive but does actually make sense once you get your head around it. Google ""three valued logic"" for more info.

I apologise for not just straight-up giving you the answer, but at the risk of sounding patronising, I REALLY think it'll be helpful for you to figure out why your query doesn't work, rather than just be told how to fix it.  This is a common source of confusion with SQL that once you've had the ""a-ha!"" moment will not bother you again.

If you have any follow up questions I'll be happy to help though!",1514328885.0
ammoprofit,"The easiest solution if left join right null (exists in user table, but not in entries).

SELECT
  U.ID
  ,U.whatever 
FROM USERS U
LEFT JOIN ENTRIES E on U.ID = E.User_ID
WHERE
  E.some_column IS NULL;",1514406312.0
,ELT’s been around for years.  Sunopsis was an early product that adopted the model - that morphed into ODI (Oracle Data Integrator). ,1514378648.0
fullofbones,"This is very similar to how Patroni works. Though I'd personally trust Patroni more since it has pg_rewind, raft consensus for the rw node, and other failover management built in.

This certainly works, but there are already more mature stacks out there. ",1514298620.0
flyingmayo,"You can issue SQL statements to find the type of info you're used to seeing returned by show status (and a lot of other things also)

There are also a wide range of tools and scripts you can use to get at this info.

see: https://wiki.postgresql.org/wiki/Monitoring",1514308835.0
zieziegabor,"Used to do pg_dumps, now we just use pgbarman. (http://www.pgbarman.org/)

It's painless, and you can restore to any point in time, which is pretty nice.  We restore every single hour into a prod_hourly DB (on a diff. machine) so developers can test against basically production.
",1513996074.0
disclosure5,"I really like [wal-e](https://github.com/wal-e/wal-e).

I perform a [complete test restore every day](https://lolware.net/2017/02/02/continuous-backup-tests-with-docker.html) just to be really sure we're on track. ",1514115483.0
SomeGuyNamedPaul,"pg_basebackup plus some rsync scripts to copy the resulting file plus WAL files elsewhere just in case.  Remember, the whole server can get wiped with a click.",1513965291.0
xmen81,We use pg_dump via crontab on a daily basis. ,1514009715.0
Tostino,We use pgbackrest. Provides good Point In Time Recovery.,1514046887.0
zieziegabor,"Almost!

Skip all that unicode output stuff, and set your pager to this:

https://github.com/okbob/pspg

and then you have an awesome PSQL experience.

Also, use Postgres services file for easy connection: https://www.postgresql.org/docs/9.1/static/libpq-pgservice.html
i.e.```psql service=prod``` and bam, it will get you there.",1513995788.0
bersace03,"I just released ldap2pg 4.0 with well-known ACLs built-in.

How do you manage ACL in your Postgres cluster ? How do you validate them ?",1513955257.0
ants_a,"Look at your queries, and think what is the fastest way to narrow down the amount of index entries and rows visited. And make sure that the index can actually be used for those conditions, e.g. range comparisons are on the final column in the index.

So for example if you have orthogonal conditions, order_date and product_id on a table of 1B entries, and both conditions will filter data down 1000x, then having index on either column will have to find 1M mostly sequential index entries and 1M rows scattered all over the table, having both indexes will find 2x1M index entries, intersect them and find 1k rows from the table, and having a composite index will scan 1k index entries and go directly to the 1k table entries.",1513945239.0
francisco-reyes,"When trying to decide if a column, or columns, should be indexed there are a few things worth looking into:

* What is the cardinality? Low cardinality = lots of repeated same values and likely not a good column to be indexed. High cardinality = usually fewer rows per each value and a potential good use case for an index.

* How many rows are expected to be returned? If the number of rows expected to be returned is high then the planner may just do a sequential scan.

&nbsp;

A few examples may help clarify.

Example of very low cardinality. A column with individual's sex. You will have only 2 values and either one will return a big percentage of the table.

Example of high cardinality which would be useful. If you have a table with  phone number. That will return one row.

Another example of high cardinality. Zip code, as long as any given zip code will not be overly represented.

&nbsp;

If you are on a version of postgres that can use BRIN indexes then you could potentially have use cases where even if a large number of records are returned you may still be able to have some improvements by using an index. https://www.postgresql.org/docs/current/static/brin-intro.html",1514072356.0
ants_a,"Due to how PostgreSQL MVCC works, when you update a row and can't do a HOT update (indexed column updated or new version of row doesn't fit on same page), you will have two index pointers, one to the old version and one to the new one, until vacuum cleans up the old row and deletes the pointer to it from the index. If you delete something from the index it leaves empty space behind, and as the index is never repacked, you now have a lower density index. If autovacuum is being regularly run the amount of space wasted is usually a small fraction. But with pathological workloads you can end up with quite a lot of very sparsely used index pages.",1513934474.0
0x2a,"Well it will save some space, but it's negligible in most cases. On the other hand, it's premature optimisation that also violates the spirit of good database design (attributes of entity x should be in table x and not somewhere else) and makes querying the data cumbersome. 

Not recommended unless you have a very specific usecase.



",1513882658.0
MacCH2,"It's likely that your XML / JSON data are [TOASTed](https://www.postgresql.org/docs/9.5/static/storage-toast.html) - you can check this with [\d+](https://stackoverflow.com/questions/23120072/how-to-check-if-toast-is-working-on-a-particular-table-in-postgres).

In the general case, TOASTed data will not impact your performance in any significant way.

",1513888485.0
0x2a,"You can get any format you want with the `to_char` function and [the desired format string](https://www.postgresql.org/docs/10/static/functions-formatting.html). 

For your example probably something like:

    select x, to_char(x, 'YYYY-MM-DD""T""HH24:MI:SSOF"":00""') from foo;

http://sqlfiddle.com/#!17/215ac/14",1513883945.0
neomis,Does the fix have to be on the postgresql side?  I found it was easier just to include moment.js in my web apps. https://momentjs.com/timezone/,1513874387.0
koalillo,"Docker is ideal for development; as you have discovered, you can have a local development instance in your laptop in a container, not installing Postgres to your system directly. This means that you can have multiple independent containers, with different Postgres versions, in a way which is simpler than most approaches, and many other neat stuff (shameless plug: https://github.com/alexpdp7/pypgdev is some experiments I was making using Docker for PostgreSQL development).

Production environments are quite different from development environments. Docker seems to offer some advantages here, but really to take advantage of them, I think that you need to take a very Docker-oriented approach which is very different from the most usual traditional non-container-based systems administration practices.

For production you need to take into consideration backups, patching, availability, monitoring, etc. that kind of stuff. IMHO, it is quite well documented how to manage production-ready PostgreSQL databases in the traditional non-container-based systems administration practice, but I'm not so sure about on Docker.

What you comment about separate instances/databases lies outside the Docker question, IMHO. Postgres has some pretty granular access control features, so in theory, barring other specific requirements, you can do all you need in a single Postgres instance- so separating into many instances is something you should evaluate carefully, weighing the pros and cons. Having multiple instances means more administrative overhead in most situations, so that's a con- but there are pros to that.
",1513769836.0
chock-a-block,"1. If the data isn't particularly valuable or time intensive to replace, a Docker image seems okay.

2. You can use a file system outside the container, but, IMO, that is complexity that really isn't necessary.   You may as well install postgresql on the host.

3. That gets complicated in a hurry and complexity isn't security.  Check out domains or even start a new schema.  Maybe you want to use Kerberos or LDAP (or..) and externalize authentication?",1513920530.0
DataChomp,This is a fun one! It is a great stocking stuffer too!,1513727398.0
francisco-reyes,"Brytlyt sounds like an interesting DB, but given the fact they don't list pricing anywhere makes me wonder if it has **enterprise pricing**. Would be interesting if they had it as a service like AWS RDS.",1513611641.0
Sterny,"Anyone know what's so special using IBM's POWER chips over Intel and the same GPUs?

The magic is happening at the GPU, no?",1513650629.0
fagnerbrack,"TL;DR: try tuning random_page_cost & 
seq_page_cost on SSD drives to improve Postgres performance",1513508170.0
Resquid,Database administrators hate him!,1513533959.0
fullofbones,"Even when using SSDs, it's generally inadvisable to use a random page cost of 1. I'd recommend starting at 2 and possibly moving down from there. Sequential reads are still faster on SSDs, it's just not as much of a discrepancy. ",1513523288.0
AQuietMan,"aka ""I finally read the [PostgreSQL server configuration documentation](https://www.postgresql.org/docs/current/static/runtime-config-query.html).""

> Random access to mechanical disk storage is normally much more expensive than four times sequential access. . . . The default value can be thought of as modeling random access as 40 times slower than sequential, while expecting 90% of random reads to be cached.
>
> . . . Storage that has a low random read cost relative to sequential, e.g. solid-state drives, might also be better modeled with a lower value for random_page_cost.

The article *I* want to read would explain how amplitude.com's infrastructure team failed to read and understand the server configuration documentation, and why they had to wait for a customer complaint to bring their misconfiguration to light.",1513527242.0
jlrobins_ssc,"Excellent overview of the internals balance between PostGIS and PostgreSQL core. The `ST_Subdivide()` trick is interesting!

The slide deck is wrong on one point though -- parallel sequential scan didn't make the 9.5 cutoff. [It landed in 9.6, not 9.5](http://rhaas.blogspot.com/2015/11/parallel-sequential-scan-is-committed.html).",1513612560.0
therealgaxbo,"But what sort of web app are you running where each request initiates a new connection to the DB, with or without a separate pooler?",1513377149.0
faggatron0,"Pentaho Data Integration (Kettle) is good, and FOSS

It's written in Java",1513363156.0
grupwn,"Talend, all the way on this one",1513367917.0
myurr,"Postgres actually can return structured data like you are after.  Rather than write the query for you I'll point you in the right direction so you get the benefit of working it out for yourself, but feel free to ask follow up questions if you get stuck!

PG has a range of [aggregate functions](https://www.postgresql.org/docs/10/static/functions-aggregate.html) and [json functions](https://www.postgresql.org/docs/10/static/functions-json.html) that you can use to construct your data structure within your query.  In your specific case I would use either array_agg or jsonb_agg to build the list of versions, with the list of changes then using jsonb_agg and either jsonb_build_object or row_to_json to collate the list of changes.

Hopefully that's enough to help you work it out.

Edit:  Complete solution, using sub-queries to make it easier to understand how it works, is as follows (although I still think you'd be better off working it out yourself):

    SELECT apps.id, apps.name, apps.slug, apps.description,
    	(
    		SELECT jsonb_agg(jsonb_build_object(
    				'id', v.id,
    				'version', v.version,
    				'changes', (
    					SELECT jsonb_agg(jsonb_build_object(
    							'id', c.id,
    							'description', c.description
    						) ORDER BY c.id ASC)
    					FROM changes c
    					WHERE c.version_id = v.id
    				)
    			) ORDER BY v.id ASC)
    		FROM versions v
    		WHERE v.app_id = apps.id
    	) AS versions
    FROM apps",1513320284.0
Crashthatch,"Your DB structure looks correct.

Traditional databases would typically return many rows (as Arffman suggests) which would then be turned into a data structure by iterating over them on the server side.

Postgres does have the aggregate functions which are capable of doing the aggregation and converting to json on the DB side. 

I don't have a database handy to test, but I think you'll need to use array_agg (or other aggregation function) **twice**: first in the inner query to aggregate the multiple ""changes"" into a single ""row"" (one per version), then again to aggregate the multiple ""versions"" into a single row for each app.

I would start with the inner query to return the versions + changes for a single (hard coded) app, then once it's working as expected, wrap that query in an outer query to aggregate all those ""version"" rows together, and finally add the rest of the fields from the app table.",1513324462.0
Arffman,"Caveat: I'm new to Postgres too, but not to SQL.

There's nothing weird about your tables, but if you query with SQL you're going to get denormalized tabular data back as I imagine your query already does. To get the hierarchical structure you wanted, you'd need a program to re-normalize it. 

For example, if this were a web service response then your web service would run the query, then analyse the tabular response to turn it into objects (e.g. step through each row, picking out the three entities from each, and adding the child objects to the appropriate parent objects).",1513317983.0
dtseiler,You might want to log into the Citus public slack to get some answers. They're usually pretty active in there.,1513287434.0
lampshadish2,"Does the open source version of citus support having multiple coordinator nodes?  I thought only the MX version (still proprietary) supported that.

You might need to get creative to support faster writes.",1513284037.0
ptman,pg_dump,1513241216.0
ants_a,"pg_upgrade with --link is the fastest process, but read the documentation carefully, practice on a backup and test your application first. There is somewhat more opportunity to do something wrong and screw up your database so your only option is to restore from backup.",1513248552.0
GFandango,pg_dump then read back in.,1513259576.0
jlrobins_ssc,"pg_dump, plus many trial runs. Use 9.4's pg_dump to produce a custom format dump (-Fc), then you can restore it with many workers in parallel using pg_restore's '-j' option, reducing your overall downtime. But, again, practice and trial runs.

But why upgrade to 9.4? That's near the chopping block for becoming unsupported (couple of years). Why not do some additional testing and target PG 10?

",1513262470.0
pstef,"It makes me sound like a broken record, but every time someone mentions migration from PG 8.4 to 9.X, I'm reminded of one of the biggest database-related failures I've experienced: provided that there is no ""name"" column in the table, `SELECT (t).name FROM table_t AS t` is not a syntax error in PG 8.4 but will be a syntax error once you upgrade. It's because ""name"" was a function that got removed in 9.0 (if I recall correctly) and because (obj).func is an alternative spelling of func(obj).",1513284370.0
HaniHani36,"You need to get *creation_date* into the outer select somehow, and then use a 

    date_trunc ('week', creation_date)::date AS week

to return the week breakout.",1513204322.0
therealgaxbo,"Does `column` contain any values >= 10,000,000?  If so, it is out of the defined range and so will end up in an overflow bucket that it one greater than the maximum specified.  Similarly, if you have any values less than your lower bound (0) then they will end up in and underflow bucket 0.  So if you ask for 10 buckets, you could end up with 12 distinct buckets unless lowerbound <= every value < upper bound.",1513186069.0
stympy,"Store one history entry per user per hour rather than updating the same history entry repeatedly.  Do this per hour rather than per day to make it easier on yourself in the future... you may get a user who wants the ""day"" of data to start at a different time than midnight of your database's timezone because that user isn't in the same timezone.  Of course, even that won't solve for users who are in a timezone that has a half-hour offset, but it'll handle most of your users. :)

If you are concerned about ending up with too much old data, you can always delete the old data on some schedule.  If you think you might have hundreds of millions of rows of this data, then use table partitions to partition by week or month so you can drop old tables, which is much more efficient than deleting large amounts of data.",1513189734.0
wjv,Umm… /r/oracle is probably a better place to ask that than /r/postgres!  :),1513066581.0
z0rb1n0,">How would you isolate the config files from the data directory ?

Conceptually, the config files are part of the database cluster and I find it desirable to leave them there (eg: postgresql.conf provides the cluster an identity, plus you get every file you need to run the cluster in the physical backup, making it self-contained upon restore)


If you _really_ want to separate them, then this is the drawback; historically only regular files and directories are considered by pg_basebackup (actually by the BASE_BACKUP command [in the replication protocol](https://www.postgresql.org/docs/current/static/protocol-replication.html) ). This is because symlinks are reserved for tablespaces, and in fact are accepted in pg_tblspc and decoded accordingly if you use TABLESPACE_MAP.

Part of the reason the dev community was/is reluctant to allow symlinks in there is that it opens up the possibility of colossal screwups such as non-acyclic directory tree, for which they'd have to code detection and complicate matters.",1513019147.0
petereisentraut,"Apparently, you have created symlinks in your data directory to the actual files. Why? If you want to move them elsewhere, just move them and set the configuration settings such as hba_file to the new location. Alternatively, if you like the symlinks, that's OK too. Just ignore the warnings from pg_basebackup then.",1513020437.0
ants_a,"Why do you want to separate the config files from db files if you actually would like the system to behave exactly like they are in the data directory? Sounds like you are just making your life more complicated for no good reason. If you still want to do that, take a look at how things are handled in Debian - their standard layout uses configs outside the data dir.",1513071195.0
obscurant,">How would you isolate the config files from the data directory ?

Debian does this, placing config files in /etc/postgresql/{#.#} for example. It's not a desirable setup, imo. But you can take a similar approach - use postmaster.opts file to pull in conf from another directory. Since the confs are in another directory, they will not get backed up by pgbackrest/pg_basebackup.",1513089747.0
Brytlyt,"Mark, Thanks for doing the benchmark. We are really excited to become the fastest GPU database. Onwards and upwards. ",1512991621.0
weaseldotro,Export the db with pg_dump arguments --no-owner --no-acl,1512988019.0
Fosnez,Best practice for using Azure: don't use Azure.,1512880415.0
jlrobins_ssc,"Article seems to be a paid Citus advert, for better or worse. It's stuff like JSONB, logical replication, and the parallelization improvements which are good for us all.

Edit: Ah, taking a gander at /u/craig081785 confirms.",1512681804.0
ulfurinn,"> As opposed to functions, procedures are not required to return a value. With this addition, you can now invoke a procedure by simply using the new CALL statement rather than using SELECT.

So… what exactly is the big deal about this distinction?",1512693831.0
eCommerce_2015,"I feel like this article is either terribly worded or I'm taking crazy pills.

>  we now have the ability to write Stored Procedures in PostgreSQL

Really?  What?  When were we not able to write stored procedures in pgsql?

>  With this addition, you can now invoke a procedure by simply using the new CALL statement rather than using SELECT

I'm not sure what the relevance is?  Also I've always used either SELECT (when I want a result) or PERFORM when I just want the usp to run.  What is the benefit/distinction of CALL?  I just figured CALL was how MySQL invoked stored procedures.  

.

I feel like this is either so esoteric and abstract that I don't have the high level understanding to get it *or* this is gibberish.  Can someone please educate me here?  ",1512746810.0
Tostino,Super excited for this! Is one of the only reasons I need to write wrapper code in anything other than plpgsql for a lot of the logic. Can't wait for both subtransactions and multiple result sets.,1512681114.0
oliness,"Good article. I've been working on adding full support for window frame clauses to Postgres. My patch [here](https://commitfest.postgresql.org/16/1398/) adds these. It's still in review, but hopefully will be committed by PG11 and then we'll have full frame support.",1513953105.0
Tostino,"I don't know of any pre-made tools to do that for you, but those types of queries should be very possible to write yourself.  Use system catalog to dynamically generate queries for you which do each analysis over all of your tables which match the criteria, and combine the results.",1512649817.0
mgonzo,"What pg version are you running? You prob want to look at checkpoint_timeout. And also this page https://www.postgresql.org/docs/9.5/static/wal-configuration.html
The pg docs are pretty good. Good luck.

",1512659327.0
kknizhnik,"Right now the status is the following: 

* My pthread version is passing all regression tests.
* It is working only at Linux.
* It provides 50% speed improvement at read-only queries (pgbench -S) at desktop for large number of connections and about 30% improvement for simple update queries (pgbench -N) for 1000 connections at SMP system with 72 cores.
* Backend startup speed slightly increased: 1000 vs. 800 for pgbench -C with 10 clients.
* Replacing sync. primitives with pthread sync. primitives doesn't give any measurable performance advantage.

The main problem is that Postgres backend is not lightweight. It has its own catalogue cache,  relation cache, prepared statements cache and requires large enough stack (>=0.5 Mb). So smaller memory footprint of thread comparing with process is negligible. It seems to me that road to efficient connection pooling leads to shared plan and catalog caches. Without it multithreaded model doesn't provide significant advantages.",1513790015.0
Tostino,"I really hope some work gets done in this area.  It seems like there is so much extra being done now because there is no good way to share memory currently, and the technical overhead for trying to keep the single process model is going to slow progress.


",1512615292.0
petereisentraut,"It looks like you are trying to install from a repository that was not meant for your OS version. Recent Ubuntu versions should have a newer version of ICU (e.g., libicu57), so the fact that your installation wants libicu52, and with a funny version number, means something is fishy. Check with apt-cache policy where it thinks your packages are coming from.",1512576410.0
jakkarth,"How did you come to the conclusion that your database size is 17mb? Have you tried other formats?

My first guess would be network bandwidth, since you're dumping it from an internet service.",1512560431.0
eras,Might there be something locking the operation? `SELECT * FROM pg_stat_activity`?,1512564241.0
petereisentraut,Also try stracing the pg_dump process to see what it's doing.,1512568485.0
enlil_reddit,"not an answer, but I wonder if ltree may help:

https://www.postgresql.org/docs/current/static/ltree.html
",1512504035.0
petereisentraut,"Sure, that sounds like a great feature.

I don't think it's easy to do this using existing facilities. Rules can only replace queries, not individual column references.

It would be a fair amount of work to do this right. You couldn't just replace values by their masked values when you first encounter them, because then you don't have the original value for evaluating expressions. You'd need to carry the masking declaration around in the system until the value is presented somewhere. There's also questions about leaking unmasked values in error messages and so on. It's not easy.",1512505408.0
Tostino,"I'd suggest posting this to the mailing list rather than here.  You have some hackers read here, but a vast majority only pay attention to the mailing list.",1512493863.0
simcitymayor,Looks like Row Level Security to me. Pg already has that.,1513037872.0
z0rb1n0,"Wrong sub, as this is the PostgreSQL one. [Please proceed this way](/r/oracle/)",1512444372.0
mage2k,"> metadata for database

That's pretty much exactly what the catalogs are.
Postgres's [system catalogs](https://www.postgresql.org/docs/10/static/catalogs.html) are Postgres-specific tables and views where information about what relations, columns, types, functions, and more are kept.   They are what are called a data dictionary in other RDBMS's.  Similar to the SQL specified information schema, which Postgres has and implements via queries against its own catalogs.  You wouldn't add any, they are added, modified, and maintained by the Postgres developers.",1512411888.0
z0rb1n0,"Other people in this thread already answered on the basis of naming conventions.

I'm going to raise a different point to instead: unlike what your framework teaches you, indexes are not meant to be based on columns or sets thereof; they're built around _search conditions/purpose_ instead. 

Thinks about what you're filtering _by_. Here's a simple example of what targeted indexing can do (note that this example uses domains too, but that's unrelated):

    CREATE DOMAIN email_address AS VARCHAR(256)
    	-- this was off the top of my head, so bear with me
    	CONSTRAINT cc_rfc822_compliance CHECK (VALUE ~ '^[0-9A-Za-z.,_-]{1,255}@[0-9A-Za-z.-]{1,63}(\.[0-9A-Za-z.-]{1,63})*\.?$')
    ;
    
    CREATE TABLE customers (
    	customer_email email_address NOT NULL,
    	name VARCHAR(200) NOT NULL,
    	country_code SMALLINT NOT NULL,
    	current_timezone VARCHAR(32) NULL,
    	dob DATE NULL,
    	CONSTRAINT pk_customers PRIMARY KEY (customer_email)
    );
    
    -- birthdays are more likely to have an higher cardinality and more even distribution, so go first in the index
    CREATE INDEX ix_birthdays_by_country ON customers USING BTREE (extract('doy' FROM dob), country_code);
    
    
    -- send ""happy birthday"" to all US citiziens in the system if they're in the right time zone at the moment
    SELECT
    	c.customer_email
    FROM
    	customers AS c
    WHERE
    -- note that the order does not matter as the planner generally knows better
    	(c.country_code = 1)
    AND
    	(extract('doy' FROM dob) = extract('doy' FROM transaction_timestamp() AT TIME ZONE current_timezone))
    ;
    


Any ORM and/or similar frameworks that bases indexing on mere columns - therefore forcing you to materialise search terms into fields and create far more indexes than it's healthy - was written by people who understood _precisely jack all_ about how efficient indexing works (either that or they based their capabilities on mysql, to which the above point still applies).

Obviously, in a lot of cases search terms still match columns.

If I had a penny for each time I saw dev teams cursing the limitations imposed by their ""auto-magic"" toolkit's naivety, I'd probably have north of a dollar: I recommend that you don't paint yourself into a corner and always grant yourself a way out with raw SQL.
",1512400122.0
pypt,Maybe rethink your table naming strategy? Could you give us an example of a table name and the derived index names that you’re using?,1512364141.0
stympy,"If I were working on your project, I'd much rather see you specify a name for your indexes than see meaningless UUID index names:

    add_index(:accounts, [:branch_id, :party_id], unique: true, name: 'by_branch_party')",1512395277.0
fazzah,"I have no experience with Ruby ORM, but can't you manually define table names?",1512389953.0
alinroc,"This is why I have trust issues with ORMs.

>Is there any reason why we shouldn't simply use a UUID in place of a meaningful value for an index name?

""Because it's meaningless"" is reason enough for me. You don't have to go to the extreme lengths that Rails seemingly does with your naming convention, but use names that mean *something* without having to resort to a magic decoder ring.",1512389957.0
mage2k,If you ever want to be able to read query plans generated by EXPLAIN with any amount of ease don't use UUIDs for index names.,1512415223.0
tsqd,"Clarify, is each row in a task table a listing of who performed each sub task contained in that task, plus a single timestamp?",1512334661.0
mgonzo,"Ok so the basic problem that you have here is a many to many relationship. And that's why you are having such fun with unnest. You need another table to express those as rows (you are making this table JIT with unnest and it's costing you).  

Now I saw you say you can't combine the task tables which i'm skeptical of, so I just used your idea you mentioned below, I think you will like why in a moment. OK so let's make this many-to-many thing work for you instead of against you. 

Keep in mind your taskA and taskB tables still exist they just point to the new task table. Full sql in the fiddle link below.

    CREATE TABLE task (
    task_id bigint NOT NULL,
    name text NOT NULL,
    task_type integer NOT NULL
);

    CREATE TABLE task_work (
    task_work_id bigint NOT NULL,
    task_id bigint,
    sub_id integer,
    completed timestamp with time zone,
    work_user_id bigint NOT NULL
);

The main thing is to separate the user from the task definition and instead make a table to describe the sub work the user actually did.

And the query:

    SELECT
      work_user_id,
      completed,
      task.name,
      sub_id
    FROM task_work
      JOIN task USING (task_id)
    ORDER BY work_user_id, completed DESC;

I'll let you do the top stuff, I think you can get pretty easy at this point. And if you really really can't combine the task table you will need to make two sub task tables and then join the results, but I'd advise against that.

Now just as an aside, in the future it much easier to help if we can just see the actual schema (obscure names whatever secret stuff you need but ya) and the postgres version. It just makes it easier for us to help.

The whole schema and some sample data I used is here:

http://sqlfiddle.com/#!17/3fc85/1/0

Anyway I hope this helps. enjoy",1512407024.0
mgonzo,Can you change the structure of the tables at all? or are you stuck with it?,1512357566.0
francisco-reyes,"A few thoughts.

&nbsp;

Couldn't you do a id, max(date) as part of a CTE? So the select unnest only works on the most recent date, reducing the data to process.

&nbsp;

Does the report need the most recent data every time? Could you use materialized views in any way?

&nbsp;

Do you have a large set of data and only a relatively small set gets updated in between? For example if only 1% of your data changed daily/weekly. You could do a general pass, save the results and then save the date the report was last run. Next run you would only process IDs with records with dates newer than your last run. If the amount of data that changes between report run dates, this may running a new report run much fast. However, that approach is not viable if a large part of the set gets updated in between reports.",1512358815.0
EvanCarroll,"Commitfest 16

https://commitfest.postgresql.org/16/",1512192233.0
petereisentraut,"Some companies and individuals post roadmaps here: https://wiki.postgresql.org/wiki/PostgreSQL11_Roadmap

However, in my experience, not all of that actually gets done. So making specific promises about PostgreSQL 11 (due in 2018) will be tricky if you're not well in tune with the hacker community.",1512500736.0
MonCalamaro,"Are you looking to give access to a subset of tables or a subset of data?  If you are looking for a subset of tables, you could use logical replication (if your db is very new, but it sounds like it isn't) or slony to replicate to a different database.

If you are looking to give access to a subset of the data in the tables, you could replicate to a read-only hot standby and use row security (available in 9.5) or security barrier views (available in 9.2).",1512225941.0
shobble,"Not sure I completely understand the problem/scenario you're describing, but would [Foreign Data Wrappers](https://www.postgresql.org/docs/current/static/postgres-fdw.html) be of any use?",1512171642.0
Benoit_T,Even if the final database is Redshift. This can be replicated with PostgreSQL database. Also I love this subreddit and I would love to get feedbacks. Thanks,1512114686.0
OHotDawnThisIsMyJawn,"Your example for the second run doesn't set a value for entries.run_id but I assume it has the id of the second run?

Anyway, you need some way to sort the runs so that you can figure out which one is first.  Assume there's a column like runs.date_time.

    SELECT * FROM 
    users
    JOIN entries ON users.id = entries.user_id
    JOIN run ON entries.run_id = run.id
    WHERE xxxxxxxxx
    ORDER BY  run.date_time ASC
    LIMIT 1

Or you could just SELECT min(run.id) FROM entries WHERE user_id = :user_id and then a second query to select your instructor from the run table by run.id.  Depends on how much data you're dealing with, the order by could cause performance problems.

Fill in your where clause as appropriate and pick whichever columns you want.  

The quickest way I can think of to get to your final goal is to take your first query to get ""first instructor by user"" then create a second query which gives you ""runs by user by instructor"" (SELECT user.id, instructor.id, count(*) ..... GROUP BY 1, 2) then join the output of the two queries on user.id and instructor.id.  If the count from the second query is 1 then you know the user didn't go on a second run, greater than one and they did.  So from there you can group the whole thing by instructor.id, count the number of rows for the instructor, count the number of rows where count > 1, and divide to get your percentage.

There's probably a way to do it with a single query just using partitioning/window functions",1512100719.0
Rawrry21,"so using this query:

    SELECT *
    FROM
    (
     select row_number () over (partition by username order by r.start_date) as row, username, u.creation_date, r.start_date, r.firebase_instructor_id 
    FROM entries e
    left join users u on e.user_id = u.id
    left join runs r on e.run_id = r.id
    where ((entry_type = 0 and distance > 1) or (entry_type = 1 and seconds_running > 600))
    order by r.start_date asc
    ) x
    where row = 1

im able to pull all of the very first entries of the user, along with the trainers that ran the class.

the part im stuck on, is how can i get a total count of users that have completed more than 1 entry (doesn't matter with what trainer) after their initial trainer?

ideally, I get a column with three rows: one for each trainer, one for how many runners had their first experience with each trainer, and then a third row with a percentage of users who had completed more than one run after their experience with that trainer.

",1512942724.0
jakkarth,"This really isn't good design. You need to handle that in your application, not at the database level.",1512090068.0
minaguib,Step away from the database. HTML escaping is a presentation-layer concern. ,1512102112.0
EvanCarroll,Ask this on dba.stackexchange.com and I'll answer it.,1512092167.0
