author,comment,timestamp
Toprelemons,Find one with a penguin.,1543602654.0
ISvengali,"Oh good!  I get to reread then share [The Night Watch](https://www.usenix.org/system/files/1311_05-08_mickens.pdf) about system programming.  ;)  

",1543591430.0
The7DeadlySyns,"[https://www.stilldrinking.org/programming-sucks](https://www.stilldrinking.org/programming-sucks)

One of my favorites",1543598709.0
ISvengali,[RFC 1149](https://tools.ietf.org/html/rfc1149) is a great read if you havent already read it.  ,1543592327.0
flexibeast,i never tire of Steve Yegge's [Execution in the Kingdom of Nouns](https://steve-yegge.blogspot.com/2006/03/execution-in-kingdom-of-nouns.html).,1543587506.0
HaplessOverestimate,There's always [How it feels to learn JavaScript in 2018](https://hackernoon.com/how-it-feels-to-learn-javascript-in-2016-d3a717dd577f) if you want to poke some fun at front end,1543594904.0
aalapshah12297,"Someone made a joke language called [Rockstar](https://github.com/dylanbeattie/rockstar/blob/master/README.md) earlier this year (so that everyone who can program in it is literally a *Certified Rockstar Developer*). Basically, it allows you to write code in a way that it would appear as the lyrics of a song. The examples in the readme are hilarious. For instance, the following code runs a counter from 16 to 0:

    Tommy was a dancer
    While Tommy ain't nothing,
    Knock Tommy down",1543591001.0
philipwhiuk,"* [UML: The Positive Spin](https://archive.eiffel.com/doc/manuals/technology/bmarticles/uml/page.html)
* [500 Mile Email](http://web.mit.edu/jemorris/humor/500-miles)
",1543595586.0
NeonSpaceCandy,[Programming Manifesto](http://programming-motherfucker.com/) ,1543586643.0
JamesRustleford,"One of my favorites: [A Brief, Incomplete, and Mostly Wrong History of Programming Languages](http://james-iry.blogspot.com/2009/05/brief-incomplete-and-mostly-wrong.html). I remember and subsequently re-read it about once a year. Very good article!",1543623613.0
coggro,[The Codeless Code](http://www.thecodelesscode.com/contents) ,1543612529.0
crippledspider,"James Mikens has some quality content

[https://www.usenix.org/system/files/login-logout\_1305\_mickens.pdf](https://www.usenix.org/system/files/login-logout_1305_mickens.pdf)

[https://mickens.seas.harvard.edu/](https://mickens.seas.harvard.edu/)",1543598501.0
hexaga,[And then there's Haskell](http://www.xent.com/pipermail/fork/Week-of-Mon-20070219/044101.html),1543587433.0
heyzo,This Stroustrup interview about the real reason he created C++: [https://www-users.cs.york.ac.uk/susan/joke/cpp.htm](https://www-users.cs.york.ac.uk/susan/joke/cpp.htm),1543588476.0
qqwy,https://www.willamette.edu/~fruehr/haskell/evolution.html is absolutely golden! ,1543617197.0
theRealJuicyJay,"Dijkstra has some pretty good essays. When you read them, they aren't necessarily humorous, but great insights into the industry of that time and applicable to today. ",1543591201.0
Wynro,"Not exactly humoristing, but I like: https://web.mit.edu/kerberos/dialogue.html",1543610292.0
singham,https://github.com/Shokodemon/hilarious-scripts,1543612043.0
ssegota,"Did no one link the epic ""How to write unmantainable code""?

Enjoy: https://github.com/Droogans/unmaintainable-code/blob/master/README.md",1543619536.0
tilde_tilde_tilde,"This paper [Pessimal Algorithms
and Simplexity Analysis
](https://www.mipmip.org/tidbits/pasa.pdf) is very entertaining. ",1543631435.0
ibcooley,"real programmer goes to disco to watch light show

Hahaha, i do this.",1543642039.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/programmingandtech] [Humorous Essays](https://www.reddit.com/r/ProgrammingAndTech/comments/a21by7/humorous_essays/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1543652260.0
VVVDoer,"Is it not considered to be egregious to accuse a community of ""gatekeeping"" based on personal, anecdotal evidence that doesn't even seem to relate to the point being made?",1543584690.0
steelorca,"Honestly some people can and some people can’t code. It has nothing to do with gender gatekeeping. There are men who can’t code and there are women who can. It comes down to interest in the subject. 

Most people learn coding on their own and not dependent on an institution. This article implies that the only way you can learn is through an institution and if a person cannot code it based on a gender biased system. 
",1543583983.0
pesmmmmm,"Not discussed is also the problem that most programming classes, projects and workgroups are not friendly or accommodating to women.  They are for the most part dominated by men, often socially awkward men, who make a very unwelcoming environment for women.  Some women are still drawn to the field despite this, but many are so discouraged they turn elsewhere.  If they were smart enough to be good coders, they are smart enough to succeed in other fields where they don't have to put up with the stereotypical jerks they are likely to encounter in programming, or at least in programming classes.",1543900969.0
pissedadmin,"This question is woefully underspecified.

>html js react node sql

If that's what you want to do, isn't that your answer right there?

",1543597777.0
AtlantisCodFishing,React Native will let you build an app for Android and iOS,1543846280.0
lwnst4r,Id like to be completely cross platform but android and/or ios is fine.,1543571027.0
green_meklar,"Let's step back for a moment and think about what this looks like conceptually. Consider that it can be stated as a geometry problem. The space of possible colors is a cube 256 units on each side. And the set of colors you want to select from is the region inside that cube that lies in a diagonal plane oriented with its normal pointed along the axis from (0,0,0) to (255,255,255). You want to select colors uniformly from the area of that region. If the plane is close enough to (0,0,0) or to (255,255,255), then the region may be a simple triangle. Specifically, if N is at most 1/3 then you have a triangle close to the (0,0,0) corner, and if N is at least 2/3 then you have a triangle close to the (255,255,255) corner. If N is between 1/3 and 2/3, you have a 6-sided polygon inside the cube that you need to select from.

Before we proceed, let me point out a simple method of selecting a point uniformly inside a triangle: You imagine a parallelogram corresponding to the triangle and its 'flipped' version along one edge; you randomize a point inside a rectangle; you flip the point from the 'high' triangle-half of the rectangle to the 'low' half if it is in the 'high' half; and then you map the rectangle onto that parallelogram and the point follows to end up inside the triangle. This method is described more rigorously [here.](http://mathworld.wolfram.com/TrianglePointPicking.html)

So, if N is at most 1/3 or at least 2/3, you have a triangle to work with and you can just pick your random point, round the floating-point coordinates to integers, and you're done. The interesting case is where N is between 1/3 and 2/3 and you need to pick a point inside that 6-gon. Notice that the 6-gon can be considered made up of 6 triangles, with their points at the center of the 6-gon and their bases on the faces of the cube. You can find the coordinates of all 6 points around the edge of 6-gon by solving for the intersection of the diagonal plane and the lines representing the corresponding edges of the cube (there's a straightforward formula for line/plane intersection that you can look up), then just arrange them in order and match them up with the center point to get your 6 triangles. Find the area of each triangle, weight them by area, select a random triangle using those weights, and then select a random point inside that triangle and round the coordinates to get your final output.

I'm sure there are many ways to further improve this, and maybe some math geek will come by to give you a more refined version of this approach that saves on floating-point operations. But for the time being this should give you a feasible algorithm that selects uniformly (assuming floating-point inaccuracies can be ignored) from all possible colors with the given average and works in constant time regardless of the color component precision.",1543564700.0
WeirdEidolon,"Pick all three at random and then normalize them. There's probably some fiddly bits with rounding to work out, but here's some python code to start with:

r, g, b = rand(), rand(), rand()

\# prevent divide by zero - choose one color to max

if r+g+b == 0: r g, b = sample((0, 0, 1), 3)

\# normalize by the total of all colors

x = r+g+b

r, g, b = int(n\*r\/x), int(n\*g/x), int(n\*b/x)




Edits for formatting",1543544415.0
clownshoesrock,"Well, there is the WeirdEidolon solution, but I'd suggest something more interesting, if you aren't worried about performance.
rgb gives you 16.7 million options, so check all 16.7 million options and place any values that equal N in an array, then pick randomly from that group.

The array will be different sizes depending on your N value.
I think this will have a different distribution than normalizing the RGB after picking each color.

Assume 255,0,0 is done in each method.

You have a 1/16.7 million chance of getting this with normalization. or with rounding  perhaps 7/16.7 million

But in ranged values you have 32896 options to choose from (and no, I didn't dyslexify 32768), one of which is 255,0,0

This should give you a less gray option than normalization, but either way you go you should be able to defend your choice.",1543556010.0
cheald,"There's nothing hard here. The other answers are sufficient to answer the question as asked.

If what you actually want is to generate colors that appear to be perceptually in a common band, you should generate an HSL color, varying H randomly, then convert to RGB. A constant saturation and luminosity with varying hue will produce a pleasant rainbow of random colors.",1543560018.0
szienze,How about a [Dirichlet distribution](https://en.wikipedia.org/wiki/Dirichlet_distribution)?,1543567956.0
Ravek,"I don't want to be the X/Y problem guy, but I feel it should be pointed out that the average of sRGB values is perceptually a pretty meaningless concept. Is there a specific visual property you'd like your set of colors to have?",1543572181.0
greysvarle,"Calculate the possible number of combinations.

Random a number from 1 to that number, lets call it k.

Calculate the k-th combination.",1543577877.0
davecrist,This isn’t hard at all. Your only requirement was for random samples that each average to be N. Distribution of the result wasn’t specified.  ,1543557311.0
lrflew,"First off, if you problem has to deal with RGB, it's often the case that you're doing something incorrectly. Why are you looking for a solution to this problem? If you're trying to produce colors of the same luminosity (brightness), this isn't going to be entirely accurate, and you'd need a more complicated color model of [lightness](https://en.wikipedia.org/wiki/HSL_and_HSV#Lightness) to handle this correctly. If you're trying to randomly generate a hue/saturation, then you might want to use random HSL/HSV values. It may not be exactly the same distribution of what you're talking about, but you could uses a non-linear random saturation to make it *look* more uniform (the random RGB method may produce more grays than you want, so HSV would allow you to fine-tune the distribution).

That being said, a solution to the problem as written would be to extend a single randomly generated value into a color. There are `(N+1)*(N+2)/2` possible RGB values where the sum is `N`. Generate a random integer `k` in the range `[0, (N+1)*(N+2)/2 - 1]`, and compute R, G, and B as such:

    int R = 0, G = N, B;
    while (k > G) {
        k -= G;
        ++R;
        --G;
    }
    B = G - k;
    G = k;

I'm sure there's a more efficient way of doing this, but this is what I came up with when quickly prototyping. This should always produce a sum of `N`. Note that this method would need to be updated if `N > 255`, as this doesn't account for clipping.",1543561035.0
singlelinelabyrinth,"Your other solutions all skew your distribution or otherwise don't work; this one does and should distribute your r, g and b values as randomly as possible over possible values for the constraints.

This turns out to be a pretty interesting and hairy problem; I'm curious if anyone can come up with a better solution than mine or identify a way that mine skews the distribution.

    #!/usr/bin/python3
    
    import random
    
    def rgbavg(n):
        '''Gets a number, N, between 0 and 255, and returns random RGB values with mean N
        or something close to it if n is a noninteger. '''
        # Generate list of three integers less than the total RGB value desired
        i = sorted((random.randrange(round(3*n) + 1), random.randrange(round(3*n) + 1), random.randrange(round(3*n) + 1)))
        # Assign presumptive RGB values based on the differences between these three numbers.
        i = [i[0] + (3*round(n) - i[2]), i[1] - i[0], i[2] - i[1]]
        # If any of these values happens to be over 255, subtract 255 from it,
        # then split 255 into two numbers at random and add them to the other two.
        # Repeat until all numbers are <= 255
        # Will glitch if n is greater than 255.
        while max(i) > 255:
            x = i.index(max(i))
            i[x] = i[x] - 255
            y = random.randrange(256)
            z = 255 - y
            i[(x + 1) % 3] += y
            i[(x + 2) % 3] += z
        return i[0], i[1], i[2]

&#x200B;

edit: I see an alternative to my solution above where I just loop through generating new rgb values until they are all under 255 which doesn't run the risk of skewing the distribution. Five minutes I'll have that one posted, more elegant solution anyway.",1543563090.0
Tagedieb,"Maybe you can make the optical trick work, if you compress the greyscale range you use. So for example instead of using 0-255, you could try to use 96-159. Basically reduce the contrast of the image so that the colors become more pronounced compared to the brightness contrast.",1543574228.0
absorbantobserver,"Pick the first two numbers at random and that determines the third number 

Thinking about this further... there will be cases where the first two numbers don't allow you to find a third that makes the average correct. Since there's only 3 values we're looking for I'd just have it try again until the conditions work. ",1543542754.0
takitus,"Do a random max 3, which will choose the color to start with. Random between 0-255 for that value. Then subtract that from 255 and random again from 0-that value.  That’s the second value. The remainder is the third value. ",1543547640.0
MonsieurCellophane,"This is what I come up with (returns n values in \[0,1\]). Does not appear to be biased, but I did not test that.

         def waver(n,ave):
            """""" Computes n random values with a given average in [0,1] """"""
            if n < 0 : raise ValueError(""n must be positive"")
            if type(n) != int : raise TypeError(""n must be an integer %r, %r""(n,type(n)))
            if ave <0 or ave > 1: raise ValueError(""ave must be between 0 and 1 included (%r)""%ave)
            # compute random values <= ave
            res=[random.random()*ave for i in range(0,n)]
            # equidistribute rest between members
            S=sum(res)
            R = ave- S/n
            return [ l+l*R/S for l in res]
    ",1543575919.0
maheshhegde,"Generate two random nums between some arbitrary limits, mod both by 255. If sum of both remainders exeeds your target, try again. Then automatically find 3rd int to leverage sum.

Is it going to work?
 
Relatively a newbie here. I think I didn't consider worst case scenario that it always exeeds, but I hope it won't ever happen in practice.
 
Does some difference / mod operation work if sum of two nums exeeds?",1543586666.0
E46_Nerd,This sounds like an easy beginner program. In C# just create a uniform distro method and use it for the Val’s. If you want help or needed the uniform distribution method just pm me. ,1543591311.0
theJakester42,"Gaussian distribution. Just use a random number generator with gaussian distribution. I think python has it built in. You can turn a regular random number generator into one with a little math. Or, you can just make a number generator that gives a random number in the range (-k,k) and then add N to it. Not as gracefully as gaussian because -k+N is just as likely as N, where as with gaussian N is the mostly likely and any movement from from there is less likely. ",1543594805.0
Vallvaka,"I think a simple solution to this would be to take your given value of N and multiply it by 3 to get the sum of the color values. Then you can select 2 ""cuts"" of this sum to randomly split the total sum into 3 numbers that sum to 3N, which is equivalent to the average of them each being N.

One problem with this approach is that the partitions may make one of the numbers larger than 255, but this can be avoided by rejecting that sample and keep resampling until a valid partition is found. This is done with any random number generation with an upper bound that isn't a power of 2 anyway, and in practice the performance hit from repeated rejections won't be noticeable.

Another approach to avoid this problem is to take the number larger than 255, and mod it by 255 to get the excess. Then select a single cut to partition this excess into 2 pieces, then add each piece to the other 2 numbers, and repeat until no cuts are above 255. I'm not sure if this would bias the distribution or not but my intuition says it wouldn't.",1543611104.0
davecrist,"You could brute force it. Considering ( x, y, z) use this pseudo code to generate a list of 3 number sets that satisfy your equation ( x+y+z) / 3 = N. 
( tying code on a phone is a pita...)
numList = []
For x from 0-255
  For y from 0-255
   For z from 0-255
    If x+y+z == 3*N then numList.append([x,y,z])

After that runs ( and it’ll take a few secs, maybe, but it’s fewer than 17 million iterations) you have a complete list of all the permutations of numbers that satisfy your need.  You can then generate n random sets of colors simply with:
ColorList = []
ListLength = numList.length()
For I in n {
    X,y,z = numlist[ math.randomInt(ListLength)]
    ColorList.append( Color.rgb(x, y, z) )
   }


Hope you get a good grade on your homework 
",1543545131.0
fefy96,"Hey! I'm a computer science student from some part of Europe, I want to try something new. A little background of mine, finished the first year of college with an average of 9.76 / 10, now I'm in my second year of college. Had a few projects with Arduino mkr/uno and Node MCU, learned much more stuff than I needed to for school. I love programming/ hacking/ practical projects. I am now going to some competition n.x.p.cup (without dots), worked for a small company on web-development as an intern, read plenty of C++ books. The problem is that I lost most of my motivation for studying and this is why I'm here. I want to find some other student / students doesn't matter m/f who want to make a ""team"" for studying, by that, I mean talking on skype/ whatsapp/ anywhere -except facebook-, about what we've learned and maybe we have some common courses and we can really help each other with some ideas/ notes. I would like someone who talks English pretty good ( better if you are a native English-speaker). I just want to try, if it's not working well or it makes things harder to understand  we can stop it any moment.",1544007801.0
ImaginationGeek,"I’m super confused about what you’re asking for.  What do you mean by “digital aspect”?  Surely you’re not taking a course that’s teaching you about analog computers (although such things did exist, historically)...

And I assume you don’t mean digital logic, like logic gates, flip-flops, clocks, finite state machines, etc., since you said “not hardware” and that’s all hardware...",1543523018.0
seanfonz,"Computer Architecture is taught this way because an Assembly Language depends entirely on the CPU.  At a very high (basic) level, an assembly command is a mnemonic that represents applying current to certain pins in a predefined way.

&#x200B;

I'm sure you could find an institution that will teach assembly uncoupled with hardware, but that would be like learning how to code without also learning any of the core CS principles.

&#x200B;

C on the other hand is a high-level language, so there's no reason not to learn that on it's own.",1543521959.0
firmretention,"What do you mean by hardware? The intro computer architecture class I took went through the design of a basic 8-bit computer. I guess you could call that hardware, but everything was built from logic gates, so that was still digital.",1543517113.0
Quexth,"I guess you can look for a Computer Organization course. It is still mainly about hardware but it provides some behind the scenes of operating systems as well. It teaches topics such as caches, memory, ISAs, processor architecture etc.",1543519473.0
anydalch,"If you're interested in how computers work on the inside, you should take an Operating Systems course. If you want something to do in your own time, I recommend [this series on bare-metal programming in Rust](https://github.com/rust-embedded/rust-raspi3-tutorial), but I'm sure it wouldn't be too hard to find a class you can attend --- Operating Systems is a required course in most undergraduate Computer Science sequences.",1543542213.0
Euphorya,"Not sure if it's exactly what you're looking for, but take a look at Nand2Tetris. Really fun course where you go through all the steps of building a computer, starting with just NAND logic gates all the way to programming an OS in a custom High level language. ",1543596275.0
reality_boy,We had a digital logic class in college. This sounds like what you want. ,1543512558.0
Vallvaka,Semantic errors should have already been caught before any IR code generation. That's the responsibility of the module that builds the AST and asserts that the AST conforms to the attribute grammar for the the language.,1543513634.0
unlocal,"Are you asking how to do this with existing IR generators, or how to design an IR to do this?

In the first case, you're at the mercy of whatever annotations the current IR generators give you. Typically this is going to be aimed at generating debug symbol output, so you will get function names and line ranges over a span of the IR at least.

If you're designing your own, you get to have more fun.  Without too much thought, I would want to annotate the AST with the byte range(s) of the input file consumed to generate each node, and then annotate each instruction in the IR with the AST node(s) that it was issued in response to.

Optimization at the IR level then becomes an interesting exercise in backtracking (since one IR sequence may map back to several arcs in the AST...)

&#x200B;",1543515741.0
combinatorylogic,"> The problem I see is that this representation might be to memory-consuming if the translation created multiple IR terms per original term.

If for some unknown reason you care about memory consumption, and you do not want any backtracking, just drop the previous IRs after the passes that used them.

Just keep the source location information throughout all your IR rewrites, it's easy. You can even automate it, make keeping the metadata in rewrites implicit, if you're using your own tree rewriting DSL.

The nanopass-style compilers can use dozens of different IRs, and there is never a problem with memory consumption.

",1543575840.0
julesjacobs,"I doubt that an extra pointer in each node will overflow memory. The memory representation of objects in some languages wastes more than one pointer per object, and that's for every single object not just the AST. On top of that GC wastes half your memory because it needs lots of empty space to be efficient.",1544003039.0
ineffective_topos,"Try to catch errors very early in the IRs. I know for at least one compiler, step 1 is typecheck the program. So the very first IR has no errors from the original program. Remaining IRs (should) preserve types and correctness. In the meantime, when working with an IR that's very close to the original code, you can simply keep line numbers (or other positions) with the expressions.",1543513423.0
,"Nice very interesting, I was wondering if you had any resources for Data Science such as articles, courses, or presentations ",1543513480.0
sidkha_,"Hi
It seems that 15625 is 5^6.
So the problem might be that e should be 6 or you might have to do ""5 to the power of (e+1)"".
Hope it helped, sorry for my english.",1543507412.0
entropyfails,They didn't.. 5\^5\^5,1543507044.0
disposable_silence,"Looks like the textbook is wrong. `5^6 = 15625` , so probably a typo in calculating the answer.

**But, what should it be?**

I'm not sure whether `6` is even a valid exponent (given the whole coprime-with-`(p-1)(q-1)`\-business).

For `n=91`, `p` and `q` are `7` and `13`, leading to `e=5`. Also, `d=29`. This is a very common paper-calculable example for RSA.

So, solving your problem,

`m=5`

`m'=m ^ e mod n =` [`5 ^ 5 mod 91`](https://www.wolframalpha.com/input/?i=5+%5E+5+mod+91) `= 31`

and `31` in binary is `11111`. Still an interesting encrypted value.

**Everything is worth taking too far**

For a little fun, let's decrypt!

`m'=31`

`m=m' ^ d mod n =` [`31 ^ 29 mod 91`](https://www.wolframalpha.com/input/?i=31+%5E+29+mod+91) `= 5`",1543507841.0
d_kajla,"I've been awake for 48 hours once, guess my brain has more RAM...... ",1543492305.0
HeeRowShee,Interesting... Why this sub though?,1543492904.0
teteban79,"Your vocabulary is all over the place, and/or uses terms in such a confusing way that it strikes me as odd that you claim you spent 10+ years on this. You seem well out of your water and there are issues a freshly minted CS grad would spot. The ""halting decidability decider"" name strikes me as nonsense. More on this below

Your definition of a TM as either ""halting"" or ""looping"" is weird, especially on the use of the ""loop"" word. As has been pointed before, ""looping"" can have many connotations (as in, looping the exact same instructions in the same order, or looping through the instructions of a program in a non-periodic fashion -- which says nothing really--), which make this confusing. I would stick to halt / not halt.

On the nonsense of the ""Halting Decidability Decider"" (HDD) You say:

>**A Halting Decidability Decider H could be defined as:**  
>  
>A David Hilbert formalist proof

wait what? A Hilbert proof is a (***finite***) sequence of Hilbert axioms. How is this a ""decider"" for anything?

I can think of two alternatives (which again, speak to a poor command of the language)

1. I assume you have a proof that does not fail if the program halts. (You could have another two proofs, one for non-halting and one for ""undecidable instances"", if that even makes sense, but let's say). You could try applying the proof to a program. Thing is, the proof is finite; so the proof will definitely fail for (some) large enough programs and (some) large enough executions that surpass the finite abilities of the ""proof"", even though they definitely halt (or not)
2. Alternatively, what you mean is you propose a process for constructing said proofs. If so, you are basically just restating the halting problem in terms of a Hilbert proof system, which does not help you in any way. It is, if anything, an interesting mental exercise, but just as undecidable as the Halting problem itself.

There might be something else you are trying to say, but you don't seem to be able to convey it in a cogent way, or at least one in which people familiar with the topic would recognize anything reasonable.",1543483475.0
DamnableNook,"Uh, just to save people some time, I should point out that this poster is a full-blown crank. Read his website he links to, or his other reddit posts, if you don’t believe.

Also, if you Google his name (which I hope isn’t considered doxing, since he uses his real name as his reddit username, and freely puts it all over his website, which he linked), you’ll find that he was charged with 71 counts of possession of child pornography. His defense, according to the news article, was that he thought it was legal because “he was god”. 

The mug shot the local news has is exactly what you’d imagine from his writing.",1543464648.0
which_spartacus,"What about a system that doesn't halt or loop?  There are several examples, one easy one is doing the ""palindrome"" problem:

&#x200B;

Take a value, reverse the digits, add it to itself.  If it's a palindrome, stop.  If it isn't, repeat.

&#x200B;

Now, the number 196 doesn't terminate.  Or does it?  But it certainly isn't a loop.",1543460608.0
eliot_and_charles,"You've had multiple posts where you use the phrase ""halting undecidability"" as though it refers to a decision problem instead of a theorem, but you never actually present a decision problem and say, ""that's what 'halting undecidability' means."" Why keep beating around the bush?",1543462689.0
ecco256,"Yeah, each char will contain a number in the range [0..255] so in total you have 256x256=65536 numbers in the range [0..65535]. I think what you’re missing is you cannot actually store the number 256 in one byte , it overflows",1543440871.0
quentech,"> This'll be 256^2 + 256

No, that will be 256 * 256",1543441255.0
spinwizard69,Why are you storing integers in character arrays?  Seriously.   ,1543441248.0
sjh919,If you really want a detailed answer just read a computer architecture textbook.,1543422510.0
darkwingfuck,"I remember being so frustrated when I was growing up, trying to understand ""how computers worked"".  I knew HTML ran on browsers written in C++ which ran on operating systems written in C and assembly which ran on a processor that had an instruction set and... what did that run on?

The true final answer was physics.  We are exploiting simple characteristics of silicon doped with impurities, lasered into shapes, with layers of other materials caked on.  We exploited the properties of semiconductors that mirrored math formalisms which decomposed tricky problems into easier ""computable"" problems.

To learn how to do math with logic gates, look up half-adders, flip-flops, full-adders, that will get you started.

To learn how to decompose interesting problems into math check out a language like C, lisp, forth, assembly, or really any language.

In the end, familiarity with these concepts on the low and high levels lets experienced teams break up problems.  Systems-based thinking is needed to make technical decisions, and complicated parts must be abstracted into simpler inputs and outputs.",1543423956.0
jmite,You might be interested in Nand2Tetris,1543472018.0
khedoros,"Computers started large and simple and got to where we are over the past 80 years or so by building on past work, while miniaturizing over the same time. From electromechanical relays to vacuum tubes to individual transistors to integrated circuits to single-chip CPUs to single-chip computers. You can get some crazy stuff by pouring hundreds of billions of dollars and a huge number of people into something.

Recursion is easy; that just takes hardware support for a call stack in memory. Multitasking is mostly implemented in software, although there are various ways that hardware can be used to help it work.

> I hope someone can go super deep into this rather than just tell me it’s all 1s and 0s.

A book would do a better job of describing the history of computing and how modern machines work. Reddit's good for a few paragraphs, but not even for something the size of a large blog post.

> Anyone else have this anxiety? It just seems like black magic.

They seemed like black magic until I spent a bit over 4 years studying them, and then about 10 years working with them, using what I learned.

>  Does anyone in the world know all of the circuit schematic to the iPhone or Android?

No. Modern CPUs are designed by using a piece of software to specify what blocks of components it should have, and how they should be connected together, and essentially having a computer try all the possibilities for how to wire it up to produce a working design that gets manufactured. Go back to the '70s and '80s, and computers were sometimes designed by individuals. Some of them even included full circuit diagrams of the machine in their documentation.",1543424308.0
thedabbe,"No one will deny it is crazy how it can all work so consistently and at those clock frequencies. It is indeed very complex, but it is approached by breaking it down to tremendous amounts of smaller problems. 

What is important to remember is that extremely few people would be able to explain everything in great detail, if anyone, and that is important to remember. We wouldn't be able to move forward if not for the interfaces and abstractions made. We are indeed standing on the shoulders of giants. 

Thousands and thousands of engineers have spent countless amounts of hours developing and learning from other people's mistakes.

As others have said in this thread, reading a book of computer architecture may clear it up. I took that class back in uni and it made me understand the fundamental concepts. That being said, I'm still impressed every year improvements being made. ",1543424390.0
orksliver,https://youtu.be/EKWGGDXe5MA Feynman does a great job explaining in layman's terms.,1543428366.0
combinatorylogic,"1) Read C. Petzold, ""Code"".

2) Go through the NAND2Tetris course

3) Explore in detail ""Project Oberon"".

After these perfectly manageable steps you'll realise that computers are in fact very simple.",1543426819.0
nukl,"Basically, we've figured out how to make the switches that do the 1 and 0 stuff so small that we can do over 4 billion calculations a second with them. As well, all the things that most people use on computers has been slowly developed by building on other people's work, and by taking advantage of the constantly increasing computer speeds since we made the first microprocessors. It is a lot of systems interacting with each other, leading to what we have now. What we think of as a simple process like installing a program is a lot of little complex things happening, and that's taken the last 50 years to make into what we have now.   


As sjh919 said, look into computer architecture to understand a bit about the physical side, and how a computer works under the surface. But, afaik, it really is just a lot (a lot a lot) of small switches that we've figured out how to make do some crazy things over the last 50 years.",1543424462.0
SweetOnionTea,"I think a good start would be understanding the theory of computability. Check out this computerphile video on Turing Machines and how they work.

https://www.youtube.com/watch?v=dNRDvLACg5Q&t=

Alan Turing proved that with this model of a machine could do all kinds of tasks as long as you could explain how to do them step by step. Computers now a day are made to be much more efficient, but a Universal Turing Machine can still do everything a modern computer can. 

Complexity can be done with abstraction. Maybe you're an expert on some small part of a computer that relies on a bunch of other computer stuff that you don't know as much about. That's okay because you only need to know what information your part takes in and what it is supposed to output. 

An analogy might be that your job is to separate green apples from the red apples. Every day you get a bunch of apples to sort. You don't need to know how they were harvested or what they are going to do with the apples when you're done sorting. 

On the other end, if you are using green apples to make pies you don't need to know how they are seperated from red apples or even that they were seperated in the first place. You just need to know that you received a bunch of green apples and that you know how to make pies with them. You send those finished pies along the way to someone who packages them. And so on..

You might know an overview of what the whole apple to pie making process is, but each person has a specific job that they know very well and doesn't need to know the details of anything else.

From the outside the apple to pie factory looks incredibly complex from start to finish. No one person in the world knows how to do every job from picking apples to shipping apple pies, but if you get the right people in pie making order you can make pies from an apple tree. ",1543424502.0
cogman10,"Nobody can give you a full answer because no one person has all the knowledge needed.  The fact of the matter is, it is incredibly complex.  There is a reason why companies like intel, AMD, ARM, micron, etc employ 1,000s of individuals all working on chip design.

A broad abstract, though, isn't too hard to paint.

At the lowest level, it is a high and low voltage (not a 1 and a 0).  Think of a simple AA battery, it has a positive and negative side.  We arbitrarily decide that the negative side is a 0 and the positive side is a 1.  Even though the voltage difference between the two is 1.5V.

From there, you have to get into the concept of resistors, capacitors and inductors.  All play a strong roll in computing, but would take too long to discuss in full detail.

After you grasp that, it is time to talk about diodes.  These are little devices that only allow electricity to flow one direction.  They do this through a process called ""doping"", a chemical process that gives special characteristics to silicon or other materials really.  Gallium was first used.  Even before that, vacuum tubes did it by having a point and foil and exploiting the fact that concentrated electricity would be forced to jump the gap but the larger surface area of the foil prevented it from going the opposite direction.  But that is another topic all together.

From there, we have the transistor.  BJTs were the earliest form because they are easy to manufacture.  Later, Mosfets came on the scene.  Now, state of the art is 3d transistors like finfets.  These take the idea of a diode and modify it so that electricity will flow proportional to the voltage applied on the gate.  How that works, is again, a length discussion (read up about P and N wells).

After the transistor was built we started putting them in interesting formations.  First we created NMOS and PMOS systems.  They were cheap to make, but had a nasty energy consumption problem.  We later developed the currently used CMOS system, doubling the amount of transistors needed, but cutting power requirements down significantly.  (Previously, a system at rest would consume power, CMOS doesn't consume power at rest... even though modern computers never rest.)

From those formations we started making gates.  This is where ideas like AND, NOT, OR, NAND, NOR, and XOR gates come into play.

Those gates are then combine into more complex digital circuits.  Half adders, full adders, latches, flip flops.

Those complex circuits are then incorporated into more complex systems. ALUs, FPUs, etc.

From there, things are coordinated.  You start talking about instruction pointer counters, registers, pipelining, instruction decoding, out of order operations, super scalers, branch prediction, etc.

And throw that all together and you get a basic understanding of how things work from the ground up.

Each of these topics can be very in depth (I spent an BS going over all of it in depth with my CompE degree, I barely scratched the surface).  They all have experts that bridge the knowledge gaps between the various layers.  Literally PhDs that do thesis's on the performance characteristics of a new full adder layout, transistor gap layout, or the lithography mask creation and optimization.  That is where you have to understand that it is basically impossible to know it all.

But if you want to learn in more in-depth details, pick any one of the terms I listed here (NMOS, for example). Google it, look up the wikipedia page and start reading.

If you really want to get a understanding of it, pick up an FPGA, learn an HDL (like Verilog or VHDL) and try to program your own CPU.  It isn't terribly hard, but it will take a while to do.",1543452326.0
diseasealert,Check out Ben Eater on YouTube. He has a collection of videos where he builds an 8-bit computer from scratch. It's great for learning about the fundamentals.,1543495751.0
Semi_Chenga,compyootas... how da fuck dey work,1543422882.0
CultistHeadpiece,I don't know.,1543422322.0
titanicx,"Miniaturization. 

As we are able to manufacture smaller and smaller things, we can pack much more power into space that was previously occupied by larger transistors, resistors, chips, etc. So a phone now that contains so much power was not possible 10-20 years ago. As we are able to move smaller thanks to the computing power now available you will see more growt as well. ",1543422485.0
nightfx91,"No one here is going to be able to hand hold you through understanding everything you want to know in one reddit post. There are people who go to school to learn this stuff and struggle to understand it.

No need to have anxiety about it, there isn't anything magical about computers. ",1543507195.0
which_spartacus,"First, a simple heuristic -- is someone's name in the title?  (""Bob:Sue 1:1"" or ""Meet with John about"")

&#x200B;

Next, scour the calendar of all people, and then cluster.  So, if Bob is having a meeting with Sue, what is the most common repeating set of people invited to that meeting?  Start with the smallest meetings that have Bob and Sue in them, and start building your clusters.  As the user adds more people, you are  able to strengthen the cluster being used.",1543423863.0
istarian,"I feel like you've left out some data on why anyone would be less or more valuable to invite. And what sort of meeting is probably also relevant.
  
Otherwise you could just screen by availability and invite those who are least knowable busy. I.e. don't invite the guy with 15 minutes to spare just because he's free when it starts.",1543424623.0
worstpossiblechoice,"Some sort of graph traversal with the people being nodes and their affiliation/utility/etc being the edge weights (or conversely use attributes that show them to be a poor for for the meeting as the edge weights), then simply use a max-cost path (or min-cost if the edges are used to model a poor fit) traversal algo.",1543423992.0
remy_porter,"Stupid solution: send the invites to everyone in the organization. The invite includes a button, ""I don't care about meetings like this"", in addition to accept/decline. Train a Bayesian filter for each user using those responses as your fitness function. Over time, everyone will get invited to the meetings they care about.

",1543430911.0
itsnevereasy,"You could also approach this as a Bayesian inference problem, where you want to calculate the posterior probability that someone will end up being invited to a meeting given features such as their availability, the meeting time, the location, the organizer, the other attendees, and perhaps a topic extracted from the title (or set from a pool of topic tags). The system could then suggest the top 5 hits, or people with greater than 50% probability of being invited, and so on. The nice thing about this approach is that, beyond the tags, it doesn't rely on data that you wouldn't normally create in a calendar event and you probably have some historical data that you can use to build priors. There is no need to model a social graph, though you could build one from the prior data as well.",1543431525.0
WetSound,"I wouldn’t solve this algorithmically. I would gather as much meeting data as possible in a database and treat it as a BI problem. That is; create some heuristic queries/rules: which people does this person meet with the most, which people does this person meet with the most *in this room*. Maybe weigh the meeting organizer’s suggestions higher.

You could even back propagate a score for the most used suggestions, so the rules that put them there gets higher weights, so suggestions improves over time.",1543437881.0
markgraydk,"What about using inspiration from recommender systems? It's a well researched field and you should be able to find workable examples online. Of course, translating your problem to something that recommender system algorithms can solve might be a bit of work. ",1543425055.0
seansleftnostril,"Bouncing off the graph traversal idea, why don't you use a euler or hamiltonian cycle with extra details, such as group, time, meeting purpose etc, and then traverse the graph with these specifications in mind and send invites to the people that fit all the criteria. Other than that all I can think of is using graph coloring algorithms, and make the ""colors"" the criteria such as meeting purpose, time, group etc.

I am still a student so any feedback on my comment is welcome!",1543428534.0
lacks_imagination,I'm still working on the P = NP algorithm.  I'll send it to you once I finish it.,1543442922.0
metaphorm,"keep a database of users who have made bookings in the past, and with whom. the people they are most likely to meet with in the future are those they have met with previously. when a user in the system makes a new booking, can populate suggestions based on past bookings. ",1543445629.0
Sparklingcobweb,If you find out everyone's mass and measure the size of the meeting room you can solve it as a knapsack problem.,1543448413.0
CaptKrag,Checkout out the a-priori algorithm or more generally shopping cart algorithms. ,1543455025.0
MysteriousHo-Oh,While I would recommend you to use Microsoft Cognitive Toolkit to make an API that automatically makes recommendations to you based on certain information(like what IBM Watson was supposed to do) I think the same thing could be achieved with a switch case or if-else statements.,1543422045.0
pulsar512b,"Well, one potential way would be to:     
a) Get a list of people that could be there (have a reason to be there, and/or are free at that point      
b) Invite all those people.    
The first part is just some if-else stuff.",1543423839.0
NonHausdorff,This may be one of the worst researched and written pieces on ai I have seen.,1543408772.0
DigitalCare,Yikes ,1543402850.0
NaughtiusMaximus_,You work around the clock only to get your ass kicked by some intelligent machine without appreciation. Also T-800 is the best model ,1543407928.0
richardathome,"Computerphile have done an excellent series on AI and AI safety: https://www.youtube.com/watch?v=IB1OvoCNnWY

TLDR; We currently don't know how dangerous they could be. Only that it's *very* dangerous.",1543408646.0
abrazilianinreddit,[Code Monkey by Jonathan Coulton](https://www.youtube.com/watch?v=5W_wd9Qf0IE) ,1543402182.0
dwhite21787,"Computer World - Kraftwerk

“Computer Sind Doof”  - Spliff

“Strict Machine” - Goldfrapp

Kilroy Was Here - Styx

“Temples of Syrinx”, 2112 - Rush",1543403044.0
Gnockhia,"I'm a computer - DHMIS

Pocket Calculator - Kraftwerk

Don't Copy That Floppy - 80/90s piracy ad",1543409197.0
Awia00,[Longest path](https://www.youtube.com/watch?v=a3ww0gwEszo) NP related :) But not really rock/rap,1543420626.0
TimtheBo,[Eternal Flame by Julia Ecklar ](https://open.spotify.com/track/1l2JbTwIt6C14R77NvcCyL?si=RCn8JmL4QYC_lvoHo7akcA),1543408930.0
forsakenplace,"just check the band The Algorithm. No lyrics though, just some nice and heavy instrumentals",1543419385.0
cmcraes,Pretty fun song: https://youtu.be/X0WNQ3UYbJE,1543420748.0
LifeManualError404,Any song by GLaDOS...,1543421649.0
chamcham123,"Have you heard of Monzy? 
He went to Carnegie Mellon and Stanford.
He made some nerdy CS rap songs.

Pimping Lemma
https://youtu.be/Z3G70ico6_0

So Much Drama in the PhD
https://youtu.be/JimUMZTiAjo

Kill Dash Nine
https://youtu.be/IuGjtlsKo4s

This last one isn’t computer science related.
Instead it is a physics rap about the Large Hadron Collider (LHC) particle accelerator.   
https://youtu.be/j50ZssEojtM",1543422349.0
gausby,Perhaps this genre of hip hop is of interest: https://en.wikipedia.org/wiki/Nerdcore,1543418918.0
spleenandpie,https://youtu.be/IstdV0EwsqU,1543419319.0
VapeOnYourNape,"Big Data + The Internet Ain’t Safe 

https://m.soundcloud.com/jpegmafia/big-data-produced-by-jpegmafia-the-internet-aint-safe-produced-by-freaky",1543419662.0
TheVarianty,Might not be what ure looking for but its cool nonetheless. Emperor & Mefjus - Void Main Void,1543420900.0
bum0010,love.bi [g.code](https://soundcloud.com/lovebittie/gcode) ,1543421400.0
oantolin,[Developers](https://youtu.be/yjdlB_rGyxg) by Smixx (feat. Steve Ballmer).,1543421739.0
Dumpin,"Lupe Fiasco has some ""compsci"" references on ""King Nas"":

> I wanna see it all, never half asleep some niggas can't see the walls

> 'Cause of the masterpiece my data breach like hacker leaks

> Delete the McAfees and slam the backdoors in back of me

> Social engineer peers stare at it, give a chat to me happily

> Actually activated off accolades of my apogee

> And act yakkity then deeply speak it back to me

> Then move it further, we can have a junta or a jerga

> I smurf it and spoof the server, reboot and produce a cursor",1543422183.0
ballofcheese,OpenBSD songs: https://www.openbsd.org/lyrics.html,1543425810.0
Catbred,"King Nas - Lupe Fiasco

End of the second verse he goes off with a lot of comp. related terms. it’s not 100% related to programming, just uses some related terms but you should appreciate it. ",1543452419.0
NovaX,"Uptime Funk is a music video from SUSECon 2015 in Amsterdam.

>I'm all green (hot patch)

>Called a Penguin and Chameleon

>I'm all green (hot patch)

>Call Torvalds and Kroah-Hartman

>It’s too hot (hot patch)

>Yo, say my name you know who I am

>It’s too hot (hot patch)

>I ain't no simple code monkey

>Nuthin's down

https://www.youtube.com/watch?v=SYRlTISvjww",1543461363.0
combinatorylogic,"Any of the iterative / counting nursery rhymes is relevant (like ""roll over"", ""this old man"", and so on).",1543476603.0
Byamarro,"Also, loosely relevelant:  
Jim Noir - Computer song (Tower of love)

&#x200B;",1543503290.0
time2program,"ytcracker is amazing.  He is like the only rapper I listen to and his lyrics are so brilliant and clever.  The more I listen to him, the more I enjoy his music. Some favorites of mine of his are...

[bitcoin baron](https://soundcloud.com/ytcracker/ytcracker-bitcoin-baron-v1-ssl)

[crack cocaine](https://soundcloud.com/ytcracker/ytcracker-crack-cocaine)

[terminal](https://soundcloud.com/ytcracker/ytcracker-terminal)

[packets](https://soundcloud.com/ytcracker/ytcracker-packets)

[cryptoilluminati](https://soundcloud.com/ytcracker/ytcracker-cryptoilluminati)

[whip it](https://soundcloud.com/ytcracker/ytcracker-whip-it)



",1543517282.0
Byamarro,"""Nerd vs Geek"" by Rhett & Link, and ""Bill Gates vs Steve Jobs"" by Epic Rap Battles of History are closest to what I can think of.",1543402115.0
vorpalsnickersnack,Get a Life by OMG?,1543408379.0
thelastapostle,"This is not gonna help but some of Martin Garrix Tracks are named like: Proxy, Virus, Byte...",1543411334.0
rfyuz,"Udemy constantly have that deal, though. Wait a week and they'll be hosting another one. ",1543389667.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1543368965.0
AboutHelpTools3,"Just sharing what I think is a memorable quote. Followed by what he said next: 

>In general, I think people who really believe that you design everything before you implement it basically are people who haven't designed vary many things. 

&#x200B;",1543369051.0
ISvengali,"While I would like that too, Im ok with any that doesnt have an active voice chat.  

",1543375119.0
interactionjackson,search for the programmer’s hangout. ,1543417463.0
XirAurelius,This would also interest me.,1543414841.0
CaptainSweno,"computer science problems are too intricate to be solved in real time. Thus, a chat is useless compared to a thread, where answers are expected to be more well written and thought out than a chat. Also spam is annoying, especially when dealing with script-kiddies.  Voice chat is useful, but generally only with a few people with the same, specific problem to solve and therefore there shouldn't be a general CS thread. Do you disagree with any of these statements?",1543382410.0
Jmo199,!remindme 1 day,1543375764.0
Carpetfizz,couldn’t you generate it yourself? create a bunch of random strings and generate several substrings by indexing them at a random start and end index within the bounds of the string.,1543360777.0
Dajoky,"Very useful data sets for tests can be found by browsing golang source code and add _test to see how they're tested, for example on string matching :
https://golang.org/src/strings/strings_test.go
It should work for you.",1543368891.0
jourmungandr,Try the Bible. There are concordances that list every major phrase in the Bible and where they can be found already compiled.,1543369858.0
TheCodeSamurai,"Also, for testing substring matching algorithms, you may want to look into pathological cases or those that really work the different algorithms differently.

This could be something like finding ""aaaaaaaaaaaaaaaaaaaaaab"" in a string of half a million ""a""s, or that same exercise with a ""b"" sprinkled in the half-million ""a""s. You might test really long match strings vs. really long target strings in varying combinations, etc.",1543389990.0
Xziof,Why not just input a book or an essay?,1543361236.0
pali6,"https://github.com/almondtools/stringbench

This might be close to what you are looking for. (found by query ""substring search benchmark""). You might also want to look into some DNA data, I know several people in bioinformatics and they say that quite a lot of research goes into improving the algorithms for efficient search of substrings in the DNA.",1543394056.0
HaruAnt,You could make them yourself as well using quick theories,1543383169.0
mrexodia,"Rewrite the code as follows:

    public int mystery(int n)
    {
    	for(int k = numbers.length - 1; k >= 0; k--)
    	{
    		if(numbers[k] >= n)
    		{
    			continue;
    		}
    		else
    		{
        		return k;
    		}
    	}
    	return -1;
    }

Observe:

- The loop goes through the numbers from back to front
- As long as the number is bigger or equal to n, keep going
- Otherwise return the current index in the loop (this is m)
- Edge case: all numbers are bigger or equal than n. Then m+1 = (-1)+1 = 0.

Since we know that all numbers after index m in the numbers are bigger or equal to n answer C is correct.",1543318245.0
Nijajjuiy88,"So the loop starts from numbers. length  and decrements. I will tell you why other options aren't correct.

A)  if length is 8 and m for it is 5 then numbers\[7\] ,\[6\],\[5\]  gets checked, returns 5 a so we can say \[5\] is smaller than n, but since we didn't check for \[4,3,2,1,0\] we can't be sure about it.

B) Let's say if the statement is true, so when i execute the function, it will return the number.length-1 as m, now m+1 goes out of the bonds of array. hence not true

D & E)  same as A, we don't know since we haven't gone that far.",1543321877.0
ExCorvan,PACKT is bad. Their books are like copy pasted from blogs and not-edited properly. ,1543316990.0
skeeto,"Here's the real URL:

https://www.humblebundle.com/books/cybersecurity-packt-books

OP is a spammer that uses multiple layers of redirection (Twitter->Twitter->Google->HB) to hide referral links from spam detection. They do this using [multiple accounts](https://www.reddit.com/user/whiskeyman53).",1543332876.0
haoPT,"Thinking of getting this one but i heard PACKT was bad, is this still the case?",1543316673.0
everyonelovespenis,"Why not? As ""user interaction events"" or something like that -> events passed to the game engine -> produces mutations within the world state.

That said, I'd be surprised if any game dev studios do this kind of thing - this stuff and ""continuous crunch"" aren't really compatible.",1543264360.0
flexibeast,"Full abstract:

> Since the early days of LISP, functional programming (FP) has evolved into a solid paradigm for producing software. How did this happen? A look into the past shows how FP has been inspired by category theory. The use of monads in main-stream FP surely is one of the “turning points”, admitted even by category-theory non aficionados. This tutorial talk addresses recent interest in adjunctions as a generic device for structuring and reasoning about FP code.",1543240087.0
gamed7,That's my professor!,1543243045.0
with_the_choir,I wish I understood this better.,1543289702.0
Xef,"As someone that works with remote coworkers, normally geographically distant(e.g. Ukraine), I'll fill out this survey when I get a chance. I'm also interested in the results, what will be done with them?",1543180669.0
inephable,“Hi - volunteer to work for me!”,1543167398.0
IdealImperialism,Project for who,1543166629.0
sdunc,Very cool stuff! I will definitely check this out. ,1543099593.0
NonphotosyntheticMao,Very cool! How can a newbie get up to speed on using a tool like this or learn a little bit about the lambda calculus?,1543121196.0
justinba1010,This is dope! Amazing work!,1543115559.0
Ask-Alice,"Now, I'm sure there will be a few people who will argue that HTTPS does this, and it does... sort of, but the feature discussed here/in this paper is different. I'm talking about encrypted data served by the server, for the end-user (in this case, the browser) to decrypt. This way you don't have to trust the server hosting your encrypted content to not backdoor in a way to copy decrypted text or private keys.

also here's some sauce from this conference http://www.ieee-security.org/Cipher/ReadersGuide/archive/WWW4-paperlist.html smart tokens... in 1995? lol",1543087106.0
ImaginationGeek,"I assume the idea is that the server and service provider who own the server cannot get access to your cleartext.

I use a number of services that do this...  LastPass, ProtonMail, Sync, Carbonite with a custom key (formerly CrashPlan with a custom key).  But if the point is that each of these services have had to roll their own solution to this problem because no open standard for it exists, then yeah, that’s a good point.",1543094254.0
clelwell,"In our browser extension, we hold the user’s private key and sign every auth’d request (payload + signed payload) for verification with public key server-side. Works pleasantly well so far. (https://clearcoin.co/chrome-extension/ code: https://github.com/clearcoin/extension)",1543094654.0
vsync,"because there is an insistence on a dumb user agent

fancy  
but dumb",1543118459.0
bizarre_coincidence,"Why would you use an ANN for a hand classifier?  The rules for classifying a hand are decently simple.  For any of the small number of types of hands, you can easily say what cards would be required for you to achieve that type.",1543008619.0
dynamiteboy8,"I'm actually working on something similar, but just wrote my own functions for the hand classifiers so you shouldnt need an ann for that... how do you intend to use a decision tree for the fold, call, raise actions though?",1543439739.0
WhackAMoleE,Just keep it away from the free drinks and the scantily clad waitpersons and I'm sure it will do just fine. Also tell it not to draw to an inside straight. And if it can't tell who's the sucker at the table ... it's the sucker. Good luck.,1543005109.0
Carpetfizz,"PintOS

lol",1542999717.0
derleth,"[Unikernels](https://en.wikipedia.org/wiki/Unikernel):

> A unikernel is a specialised, single address space machine image constructed by using library operating systems.[1][2] A developer selects, from a modular stack, the minimal set of libraries which correspond to the OS constructs required for their application to run. These libraries are then compiled with the application and configuration code to build sealed, fixed-purpose images (unikernels) which run directly on a hypervisor or hardware without an intervening OS such as Linux or Windows. ",1543000321.0
krum,FreeRTOS has an x86 port. ,1542995879.0
uh_no_,windows?,1542995716.0
new-washing-machine,TempleOS (serious). It’s pretty amazing for a niched area of OSes. ,1542996282.0
ellenkult,[Minix](https://minix3.org/) maybe?,1542998136.0
Exaltred,"[Plan 9](https://9p.io/plan9/about.html), specifically the public fork [9front](http://fqa.9front.org/)?

It's more fully featured than some systems out of the box, but is fairly easy to port as the code base [is reasonably compact](http://aiju.de/misc/sloc).
",1543003313.0
JustinXin,Perhaps you're looking for [TinyCore Linux](https://distro.ibiblio.org/tinycorelinux/downloads.html). ,1542995865.0
rmrfbenis,"TempleOS, ToAruOS.  
Maybe Haiku, Inferno, Plan 9 and ReactOS for generally rather independent operating systems.",1542999484.0
khedoros,"Some sort of embedded and real-time OS would probably be what gets closest to the scale of GEOS, although they aren't really meant for regular users (more like running IoT devices, or acting as the OS for computers embedded in other devices).",1542997285.0
deftware,TempleOS,1542998959.0
paralysedforce,"Not sure if this is exactly what you're looking for, but you should check out embedded operating systems like TinyOS or TockOS if you want to see OSes that function with extremely tight hardware constraints",1543001849.0
bart2019,"While we're all delving into the pet projects we ever heard of, I give you [Oberon](http://www.ethoberon.ethz.ch), both a language and an OS, by the designer of Pascal: Niklaus Wirth.",1543012763.0
zesterer,Take a look at [https://github.com/klange/hobby-os-landscape](https://github.com/klange/hobby-os-landscape),1543020698.0
cp5184,Netbsd tends to try to support anything and everything,1542997017.0
supercheetah,Look into unikernels.,1542997773.0
FloridsMan,"Assuming you're not limited to open source:

Vxworks, qnx, freertos.

They run everywhere on everything from barely a microcontroller to a full x86 system. ",1543026124.0
Troopar,Aros,1543051167.0
NicolasGuacamole,Pretty much any modern Linux distribution can be very lightweight depending on what you use by way of desktop environment.,1542997724.0
Unzile,Alpine is pretty small,1543006433.0
TheLinguaFranca,I believe that's called a microkernel? There are probably some Linux builds you can download online,1543010139.0
ak681443,XINU?,1543038405.0
mach_i_nist,seL4 microkernel and Xenomai RTOS. Firefox OS  still intrigues me. But Yocto Linux is likely what you want. Can take a long time to setup a custom bitbake though. ,1543042171.0
intronink,That computer that apple will never update has a really shitty OS,1543050118.0
justacasualgamer97,Tiny os?,1543060159.0
swxxii,Windows Phone,1543061205.0
Sun_Kami,"Not intentionally low functionality, but here's an awesome project: https://github.com/froggey/Mezzano",1543101786.0
maheshhegde,"Plan9, Kollibri, freertos & other embedded ones, freedos and of course, old school unix code is available.",1543587235.0
hexidon,slackware,1543002939.0
BassyNico777,Puppy linux,1542996732.0
DatTurban,"Idk what your application is, but I’ll leave this here:

https://www.yoctoproject.org

Embedded Linux is about as simple as it gets I think",1543002005.0
BananaHockey,TempleOS. Praise Terry,1543003885.0
spacegirlmcmillan,"Xubuntu?

Edit: or Ubuntu server",1542996118.0
Putnam3145,"It takes 10,000-100,000 times as much time to access your hard drive as your RAM. This is not ideal, so it's rarely done. However, as mentioned otherwise, your OS does do this.",1542994547.0
Doommius,It's called paging and your os does it already. ,1542994085.0
theblacklounge,Your OS already does that on its own. It's called swapping pages in [virtual memory](https://en.wikipedia.org/wiki/Virtual_memory).,1542994318.0
aranciokov,For a second I believed OP thought [this](https://downloadmoreram.com/) was *not* a joke.,1542994538.0
PineappleBoots,"Paging is a thing as others have stated, but the real issue here: RAM is specialized hardware, and very very fast memory. 

It’s the same as asking: why can’t I download a CPU? 

A standard HDD is roughly a million times slower than RAM. 
RAM is also closer to the CPU physically, and operates on a different bus. ",1542994606.0
gabriel-et-al,"Your OS already uses the hard drive when your computer runs out of RAM. The problem is that the hard drive is muuuuch slower than RAM, so it's only used when there's no other choice.

[This Wikpedia entry](https://en.wikipedia.org/wiki/Paging) is very informative.",1542994662.0
philkav,"Ram is physical hardware. Just like your disk. They operate at very different speeds. Depending on your workload, reading from RAM can be tens of  thousands of times faster than reading from a hard disk.

If the speed of a single ram IO was the equivalent of you walking to the other side of your living room, then IO to disk would be like travelling to the other side of the country. 

The limit here is the physical hardware. You can perform ram functions on the hard disk, but your computer will run incredibly slow, because it will constantly be waiting for the disk to return, and disks are very very slow.",1542995317.0
gooooooddoggy,"Ah I see, thanks for all your help!",1542997698.0
_georgesim_,Because disk is not RAM. RAM is RAM is RAM. You cannot download hardware.,1542994574.0
rehevkor5,"Maybe take a look at Flink's event time watermarking, windowing? Your question is a bit broad.",1542980368.0
Lendari,I think youre looking for a stream? ,1542986204.0
BumbuuFanboy,"I've studied Kleene Algebras and Brzozowski derivatives a decent amount, so hopefully I can shed some more light on the topic for you.

&#x200B;

First, I would just like to comment on something you may already be aware of but did not mention and that it is essential for getting only a finite number of states. When deciding if an expression is the same as one that you've seen before, you need to collapse expressions by ACI (associativity, commutativity and idempotence). Associativity of multiplication means that a(bc) = (ab)c. Commutativity of addition means that x+ y = y+ x. Idempotence of addition means that x + x = x.  There is actually a variation on Brzozowski derivatives called Antimirov derivatives that handles this by representing products as lists of terms and sums as sets of terms.

&#x200B;

So moving on to some of your specific questions. It is definitely true that creating states from Brzozowski derivatives leads to some of them being indistinguishable.  Here is a simple example. Consider the expression 1+xx^(\*) in an alphabet only containing x.  It should be clear that this is equivalent to x^(\*) which of course can be represented by a single state machine. However, consider the derivative construction for 1+xx^(\*). D\_x(1+xx^(\*)) =D\_x(1)+D\_x(xx^(\*))=0+x^(\*)=x^(\*).

&#x200B;

Any further derivatives wrt x with loop back to the x^(\*) so we are done. So this construction contains two states, a start state which is accepting and has an x transition to another state, also accepting with an x transition to itself. This is obviously not minimal.

&#x200B;

You are correct that this construction does not introduce any non-reachable states. Suppose you construct a state s in the machine for expression e with some sequence of derivatives D\_x1x2...xn(e), then starting at the start state and taking transitions according to the string x1x2...xn will get you to s. This is not a huge win though, the computationally difficult part of DFA minimization is identifying indistinguishable states (also known as bisimilar states in more general coalgebraic lingo). Identifying non-reachable states can be done simply and efficiently with a graph search starting at the start state.

&#x200B;

I'm not exactly sure what you are talking about when you say that the reverse of an automaton is nondeterministic, but I think I have a good idea. It is important to note that given a regular language L, the set of strings s such that the reverse of s is in L is also a regular language. There are a few ways to see this. For one, if you reverse a regular expression, you can prove the resulting regular expression matches the reverse language. Another way that you can see this is by assuming you have a DFA for your language. Now consider a machine where every edge is reversed, every accepting state in the old machine is a start state, and the original start state is the only accepting state. This machine is obviously non-deterministic because it has multiple start states as well as the fact that any state with multiple x transitions going into it in the original machine now has multiple x transitions coming out of it. This is probably what they meant, there is a nice and concise construction that says the reverse of a regular language is regular using an NFA constructed from a DFA.

&#x200B;

I also could help a bit with the idea of coinduction. In general, a coalgebra is a set with two functions defined on it, an observation and a continuation. Intuitively, you should consider this set a set of states. The observation is just some sort of info you can deduce from being in that state. The continuation is some function that will produce a new state from an old state and perhaps some new information. Hopefully, this sounds familiar. A DFA is a coalgebra where the state set is the state set, the observation map is the function that maps states to whether or not they are accepting, and the continuation is the transition function.

&#x200B;

When dealing with coalgebras, you usually want to reason about reachability of some sort. Usually, in a coinduction proof, you want to prove that if you started in a ""good"" state then you will necessarily remain in a ""good"" state. I don't exactly know what you are trying to prove so I don't think I give any more helpful information.",1542945828.0
elfsternberg,"I don't know how helpful this will be as I'm not all that familiar with Brzozowski's algorithm for the reversal of the DFA, but I am very (uncomfortably) familiar with Brzozowski's algorithm for the derivatives of a regular expression.  As far as I can tell, non-accepting states can only be introduced in the original DFA, not in the derivative— And the improvements introduced by Adams (""On the Complexity and Performance of Parsing With Derivatives,"" 2015) use advanced constructors to eliminate any derivative states that result from those non-accepting states. (For example, the rule for the derivative of the union operator would normally be a union— except Might & Adams both realized that a null set as one side of the union means that the derivative is instead the derivative of the other set, without a union. X ∪ Ø == X.  Concatenation is similar: X ◦ Ø == Ø. And so on...)

I don't... quite... understand what the issue is with the Delta function.  Delta is an operator introduced into derivatives of the initial language to store the results of successful recognitions; you shouldn't have one in your starting language.  (For that matter, if all you're doing is *recognition* and not *parsing*, and I recommend starting with the recognition of regular languages before moving on to *either* recognition of CFGs or parsing of regular languages before you try to combine the two, you shouldn't have Delta functions at all!)

Might's Plum variants are most instructive: https://github.com/plum-umd/parsing-with-derivatives/blob/master/racket-code/racket/rerp-core.rkt

The ""rerp-core"" is a regular expression recognizer; the ""herp-core"" is a CFG recognizer, and ""derp-core"" is the full-blown CFG parser. Optimizations (automatic branch elimination, mostly) and sugar (Kleene-plus operators and the like) are provided in other stages.  

For something more advanced, Adams's Derp-3 is the crazy example: https://bitbucket.org/ucombinator/derp-3 Rather than use the recursive fixed-point operation for creating the minimal return sets, it uses flow-control mechanisms to only revisit the nullability of nodes that haven't yet been analyzed; it's optimizers are wildly more complex than Might's, for example the concatenation operator rebalances left-heavy concatenative trees into right-heavy trees, then introduces a reduction at the end to rebalance the resulting tree *back* to the AST's expected structure; this preserves both the operation and shape of the original regular language while at the same time eliminating the deep left recursion that could happen under those circumstances.

I've been deep into this for a couple of weeks now, trying to implement it in Rust. The going has been... slow.",1543116753.0
maruahm,"Are there any good examples where an algorithm has a significantly better complexity, but is rarely used over poorer complexity algorithms due to the massive size of the hidden constant?",1542935786.0
vanderZwan,"Hey, something trivial: are you peeps familiar with [Mario Klingemann's Stackblur algorithm](http://underdestruction.com/2004/02/25/stackblur-2004/)? It uses a very simple trick to do a diamond-shaped blur in O(n), where *n* is total pixels, no matter what the radius of your blur is.

Imagine a box blur. Done naively, it would be O(n * 4r²), where r is the blur radius. We can improve this by doing the blur in two passes: one horizontal and on vertical. That makes it O(n * 4r). Great! But then notice that given two adjacent pixels, the only difference in summed pixels are the two pixels at the start and end. So after setting up an initial sum for the leftmost (topmost) pixel, we can scan across a row (column) and find the next pixel sum by adding and subtracting only two pixels. With that, creating and then averaging a moving sum becomes an O(n+kr) algorithm (k = width+height), which is pretty neat because usually n is much bigger than kr, so it no longer matters (significantly) if your blur radius is big.

Anyway, Klingemann's stackblur is taking that idea of a moving average and applying it to itself one more time. The result is diamond shaped kernel with pyramid-shaped weights. [I made an Observables notebook explaining it semi-interactively](https://beta.observablehq.com/@jobleonard/mario-klingemans-stackblur), it should make things more clear.

So I realized that this idea could be applied to itself one more time, and that it should make the kernel more normal distribution-like in shape, while still technically being O(n+kr). I have yet to fully implement this ""quadratic"" stack blur in the linked notebook, but in principle it should work and be really fast for large radii. I think it's pretty amazing that we can get [what seems to be a pretty decent visual approximation of Gaussian blurring](https://beta.observablehq.com/@jobleonard/mario-klingemans-stackblur#QuadraticStackBlur) in O(n+kr), regardless of radius size! [I asked Mario Klingeman](https://twitter.com/JobvdZwan/status/1063477054371479554) if he ever tried applying the moving sum trick one more time, but he hasn't, so maybe I discovered a new fast approximate bell curve blurring algorithm? Sounds a bit too trivial but I've never seen it in the wild.

Edit: had skipped one intermediate step of reducing O(n * 4r²) to O(n * 4r), hopefully my explanation makes a bit more sense now",1542943379.0
CheezeyCheeze,What is new in the world of Computer Science this week? Any news these past few months?,1542943966.0
abdyzor,I never had to use heaps to solve a problem. Can someone reply with a problem he solved with heaps? How to identify a 'heap' problem?,1542960891.0
Feminintendo,"1. What is the deal with ""gradual typing""? Why is it such a big area of research? Doesn't it just make type inference easier? 

2. Hindley-Milner type inference: Is there a better algorithm? If so, why does everyone seem to only do HM? (Isn't it exponential in some cases?)

3. And why do subtypes make type inference hard? (Or do they?)

4. Is graalvm as amazing as the website claims it is, or is that just marketing? The research papers are really cool, but I can't tell if it's actually effective in practice. If it is effective, why has their not been mass adoption?

5. If caching is becoming the bottleneck, could we not just have hardware partition the memory space at some cache level (possibly ram level) among each core in a way that can be managed by the OS so that each core has its own locality advantage? Or am I misunderstanding the problem at some fundamental level?

6. Why don't we have code editors that give us a transparent version of the type signature for the function we are typing a usage for while we are typing it? It'd be so perfect for so many situations. Start typing that Pandas function and see all the named arguments and their defaults, with code completion for the next token in the signature as you type. (Wouldn't have to be just function usage/signatures but could help with normal syntax.)

7. Why isn't Pratt parsing used more often? Why have I never seen Pratt parsers or precedence climbing in any compiler theory or engineering textbook when versions of it are used in some major production compilers (if only for expressions)? 

8. Why have I never seen a Pratt parser generator, which seems like the easiest thing to make ever? (ANTLR4 actually does do this, but only one operator per precedence level, and only implicitly.) For simple expression languages you could practically just use a CSV file as the ""grammar,"" or just generic usages with pre-defined/declared variable names. ",1543036640.0
Jab5684,"As a high school senior who is going into computer science, what should I look for in a laptop or what would be things that are good to know about relating to technological needs in college?",1542993950.0
__-_-_-_-_-_-_-_-_-,"I have been out of college for a few months now. As a result, not quite following the current advancements in CS. Can someone provide a tl;dr?",1542983434.0
Quexth,"Your post is too vague. You need to give more information about the requirements, constraints and specifics if you are to get help.

Now, you will probably need a wave generator object. Then, if you need to  produce a sound you just calculate it through formulas. Else, if you need to visualize it your generator needs to produce wave objects, which then act on their own. However, this all depends on your needs and these are just some ideas. ",1542915515.0
ethanfinni,"Formal verification is not for the faint of heart and certainly not something frequently seen in most industries. It is something that is important in safety-critical systems (e.g. a pacemaker) but not a web-app or other typical office application.  Formal verification is expensive in the context of the software development lifecycle.  It takes time and resources to do it, very specialized talent (read: expensive) and not your ""learned programming online"" type. Also, living in the world of Agile these days, too much requirements engineering and system specification seems passe...The motto no is: ""prototype the product, show it to the client, fix/add what they want, ...rinse and repeat..."" -- it just does not lend itself to formal verification.

Keep studying it, topics like these are what really make the difference between an ephemeral coder and a real computer scientist/software engineer.",1542908814.0
TruePlatypus,"Mostly because formal proofs of correctness for any non-trivial code virtually explode in terms of computational complexity. 
Thus we use these methods to prove only the mission critical parts of code, hoping for the best in the rest. ",1542907158.0
richardathome,"What do you want the database to do? Pick a word from a list to test you? You probably don't need a database for this - just a long list of words (like this one: https://github.com/dwyl/english-words)
",1542901328.0
UseyMcUser,"How about a table for letters that maps to a string representing the dots and dashes.  A table for words - maybe let the user add their own.  You could record user sessions - when did they start, how long did they play. 

Focus on making it fun and an idea you like. You likely have some long nights ahead (that’s how we all learned) and building something you want to make will help you get through. 

Good luck!",1542906928.0
reality_boy,"You just need a string table that stores phrases in it. You can sort the phrases by complexity or category (funny, emergency, etc). There is no reason that could not be stored in a database.

From there you write an ascii to Morse converter that sends a message out and either ask the user to translate it back or repeat it back as Morse (or the other way around)

As for detecting Morse you want to convert the incoming pulse train into millisecond durations for both the pulses and spaces and then convert those times into ratios by comparing them to a running average so you are not locked to one fixed rate of transmission.  Then from there you can threshold everything down into dot/dash/break.",1542908849.0
sarahloyd,"Ok, remember that you need to include numbers as well as letters and punctuation. In practice punctuation is rarely sent in morse but should be included for completeness. The timing between words, and characters is as important as the timing of dots and dashes. Also don’t forget procedure characters where one character is run into the other. For example the procedure code DE is sent with no gap between the letters and means “from”. You will find lists of around 70 procedure codes in common use. Procedure codes are important in real world use of morse because they allow radio operators who do not understand each other’s languages to communicate. For example if I was in communication with my husband I might send a message like, M0XYX M0XYX M0XYX DE G7TXR G7TXR UR RST 577 QRM + QRN G7TXR K

Apart from our callsigns the only part of the message that would be sent with normal morse spacing between characters is 577 :-) 

The point I am trying to make is that a program that just does a lookup of the next character in an array and   Sounds the requisite dots or dashes in sequence until the end of the string may be sounding the correct beeps but without the correct timing between characters, the correct timing between words and the different timings between characters in procedure codes it would not actually sound like real morse. 

You can find all of the procedure codes and timings online. It would be a fun project to create a program to do it all properly but maybe a bit ambitious for a short assignment. 

Good luck with your program either way. There are plenty of basic morse programs out on the internet as inspiration:-)

Best regards Sarah G7TXR ",1542922359.0
90c87,"I think you're asking about process management and that's the entire discipline of studying operating systems. 

Queueing typically refers to data travelling on a network and being queued for processing in the order it arrives, like a checkout at a store. 

If I'm understanding your question correctly, you're wondering how to manage processes on a networked application to service multiple clients from a central server location? If that's the case, you're entering the territory of cloud computing. If that's the case, you just need to write a multi-threaded program that has error handling for client devices that have timed out. 

I'll need you to expand on your question in order to help you further.",1542934285.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1542891197.0
kc3w,For your use case I would suggest using a heap.,1542895579.0
WhackAMoleE,"The top four Chinese miners control well over 60% of the bitcoin network's hashing power, and this has been true all of 2018. In other words, *bitcoin's cryptographic security has already failed*. The basic value proposition for bitcoin is nonexistent; and the true value of bitcoin is zero. Investors are starting to react to this basic fact. 

Satoshi envisioned tens of millions of individuals mining bitcoin on their spare PC. He never imagined the hardware arms race that has actually come to pass.

In short, bitcoin has already failed. All that remains is for the last few hardcore believers to figure this out.

Next stop: Zero.",1542917563.0
takitus,Nano is the way to go,1542896112.0
,[deleted],1542865703.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1542851187.0
LearnedGuy,"The ""apply"" and ""lapply"" functions in the R language do this. In general data structures are not viewed as having built-in operators. They are nouns, and it sounds like you are looking for an operator, effectively, a verb.",1542823472.0
Crysis456,"    Binary Heap:

    Finding min value: O(1)
    Removing min value: O(logn)
    adding new value: O(logn) worst case, O(1) average case

Total complexity of what you want is O(logn)

    Balanced Binary Tree:

    Finding min value: O(logn)
    Removing min value: O(logn)
    Adding new value: O(logn)

Total complexity of what you want is O(logn)

So either a binary heap or a balanced binary tree would be suitable for this

If you instead only wanted to find the min value instead of remove it, then the heap would be able to do the entire thing in O(1) on average

Edit: also even if you are set on removing the min item, the heap would still be better in practice, coefficients are much lower, allocates much less often than the tree, and is a bit better for cache.",1542820033.0
Xziof,What about a minimum priority queue?,1542825199.0
verimithix,Don't heaps do this? Extract min is very easy and always the root node. ,1542818633.0
everything-narrative,"The principal structure you want is one that maintains at least a weak sorting of it's elements. On most modern computers, a binary heap (often referred to as a priority queue) will do the trick. It maintains an implicit tree where each node in the tree points to nodes with lesser keys (or greater, depending on configuration.) Runtime performance for removing the minimum element and re-balancing is O(log n), same as a balanced binary tree, but it is much more compact in the memory representation (good cache performance) and uses no pointers (low memory constant.)",1542819401.0
coriolinus,Most languages will have a BTreeSet in their standard library or easily found in external libraries which will allow that use case. ,1542818504.0
nealeyoung,In Python: heapq module or queue.PriorityQueue (built on top of heapq).,1542841473.0
acroback,Use the classic heap.,1542826011.0
NytronX,"Didn't read other comments, but the first thing that comes to mind is a min-heap. See CLRS.",1542861533.0
mavavilj,Currently thinking of using heapdict [https://github.com/DanielStutzbach/heapdict](https://github.com/DanielStutzbach/heapdict),1542882915.0
TKAAZ,"Depends on the application. If you don't care about worst case performance, maybe a fibonacci heap or a binomial heap.",1542825434.0
in_the_comatorium,I think a list in Python could do this?,1542819003.0
happySmiles96,Why do you want to use deep learning for search engine? Wouldn't you be able to make a good search engine with help of documents relevance score and author's reliability score?,1542812123.0
Grimaldi2,"Check out this review paper: https://arxiv.org/abs/1705.01509 „Neural Models for Information Retrieval“. Everything you need to know is referenced. I think it will be challenging to do everything with DNNs, especially given your stated skill level.",1542829353.0
remy_porter,"I skimmed a little bit, but if you're going to posit an information theoretic model of color perception you have to explain why this is superior to the opponent-model.

Human beings do perceive color in a well understood fashion. Nobody who cares about color perception uses RGB color spaces, and the fact that none of this is contextualized within a perceptual color space, nor is the biological basis for color perception discussed limits it to being a mild curiosity.",1542819844.0
NewComputerNewUser,"If you want a fast and optimal answer use concorde ([http://www.math.uwaterloo.ca/tsp/concorde.html](http://www.math.uwaterloo.ca/tsp/concorde.html)).

If you want even faster use  Lin-Kernighan heuristic of Helsgaun ([http://akira.ruc.dk/\~keld/research/LKH/](http://akira.ruc.dk/~keld/research/LKH/))  or a genetic algorithm with  edge assembly crossover.  


edit: I think with  1577  nodes concord can find an optimal solution in less than 3 min but you need to try to find out. The running time doesn't only depend on the amount of nodes but also on the structure of the graph.",1542799696.0
jourmungandr,You can use [Christofides\_algorithm](https://en.wikipedia.org/wiki/Christofides_algorithm) if you want a fast approximate solution. It's guaranteed to return a tour that is at most 1.5 times the length of the actual optimum tour. ,1542815603.0
bizarre_coincidence,"How many cities are there? Is there any extra structure (e.g., are you working with a restricted case like the planar TSP, can you assume that the cost from A to B is the same as the cost from B to A, etc.)?  What algorithms are you considering?",1542797856.0
pali6,Do you have to find the optimal path or is the goal to just get a path that's as good as you can find?,1542815465.0
Curtis2point0,The transition will be electrifying. ,1542770768.0
BasedGodot,"I'll be direct, I've seen better alternatives ",1542772789.0
KyleOckerlund,This hertz me so much it's shocking,1542791160.0
khedoros,"More of a /r/askcomputerscience thing. You might add some info on what ""emu8086"" is. It sounds like an emulator for an 8086 CPU, but the name is so generic that it would probably be helpful if you could give a link to the exact program you're talking about.",1542776216.0
PlasmaBolt,Check out Google's Mobile Vision API,1542762322.0
cskc97,Do you know anyone who used it?,1544376722.0
jet_heller,The same? Maybe you need an example. ,1542736906.0
pmpforever,"The only way to pass a string in C is by pointer, because a C string is just an array of characters with a null terminator at the end.

To pass a string to a function, you must send a pointer to the first character in the string.

&#x200B;

e.g.

// This function takes a pointer to a character, which is the first character in a string

myfunction (char \*stringparam) { ... }; 

// This creates a new C string, which is actually syntactic sugar for declaring an array of characters

char \*mystring = ""string""; \\\\ this is implicitly char mystring\[\] =  \['s', 't', 'r', 'i', 'n', 'g', '\\0'\]

// Here, we pass the pointer to our function. Notice that mystring is already a pointer to a character (char \*), so no reference is needed.

myfunction(mystring);",1542767399.0
khedoros,"Ummm...you'd call the function, providing a pointer to a string as an argument. I guess I don't understand what you're actually asking.",1542737270.0
theantigod,"void func ( char* str )
{

  printf ( ""%s\n"", str );

};

int main ( int argc, char* argv[] )
{

  char* string = ""my string"";

  func ( string );

  return 0;

};
",1542737730.0
shookees,"A similar thing is being done during audio analysis - just that it's not the letters, but specific sounds called phonemes. With them words can be built and further analysis applied",1542737140.0
wsqmkgecb,"Udemy courses are ALWAYS 9.99. I don’t mean to shit on udemy because I am happy with a couple of courses I followed on their platform but their pricing model is a scam. There is a 70-90% sale every day, and  the price changes for new users as well. Pro tip: use incognito mode and press purchase before logging in...",1542732838.0
WhackAMoleE,Math courses are $9.9999...,1542738672.0
barsoap,"Instruction sets have only a very tiny impact on performance: In modern architectures they get translated into a different, architecture-specific language before execution anyways.

The reason ARM gets better performance/watt is that ARM has tons of experience optimising for that metric due to their largely mobile origins. Intel, OTOH, has spent decades maximising peak single-thread performance for CPUs that get plugged into a wall outlet: They only started really caring about performance/watt once cooling became a serious issue and their server customers were finally realising that they're paying for electricity costs. Going a bit further IBM's POWER chips are even faster, but even worse at performance/watt.

Where you actually can see instruction set impact on performance is in the area of memory models and synchronisation primitives. Also things like VLIW not working well anywhere but on DSPs, SIMD being worse than vector-style, but that's not things that would differ between x86 and ARM.",1542721876.0
spinwizard69,"Well a few things to understand here:

1.  ARM since day one has been focused on low power.  
2.  ARM never really focused on Application processors, that is chips that are the equivalent of Intels chips until very recently.  
3.   ARM and Apple go back decades.  
4.   Apples ARM based chip is their own design and frankly nothing ARM designed comes close performance wise.  Basically Apple design high performance cores to execute ARMs 64 bit instruction set.  
5.    Never fully trust benchmarks.  For example Apple gets really good single core performance when running on the performance cores.  However running a desktop OS, with more processes and apps running means that some of those processes will be running on the low performance cores.  So how well the processor runs a more demanding workload isn’t knowable at the moment.  In any event it would vary with user workload.  
6.   Intels processors contain a lot of junk electronics to support modes hardly even used these days. There is a lot of legacy hardware in an Intel processor.  
7.    Contrast this with Apples ARM processor cores that are highly optimized for ARMs 64 bit processor archetecture.  I’m fully expecting Apple to delete all support for ARM’s legacy 32 bit instruction set in the future.   This means just about every transistor in each CPU core is doing real work.  

As for why we haven’t seen ARM based laptops, it is mostly due to performance and the perception of a small software library.   Apples chip is the first to give the alternatives a run for the money.  So you need to ask yourself (well maybe Apple) when will we see ARM based hardware running Linux or MacOS.  They obviously have the cores todo it now though in my opinion they would need a souped up SoC to better implement features laptops and desktops need.  

The reality the world of ultra mobile devices is significantly different than that of the desktop or even a laptop.  You need support for things like external GPUs, multiple I/O ports, very fast SSDs, and other features on a desktop, that don’t make sense on a mobile device.  In Apples case they could easily add whatever a desktop / laptop would need.   A Mac Book need a minimal of upgrades to the processor, a Mac Book Proon the other hand would need a major new redesign.  ",1542724927.0
PolarTimeSD,[Here's the actual link to the course](https://developers.google.com/machine-learning/crash-course/),1542728870.0
ultimatt42,"""Free Machine"" learning course... has the uprising begun already?",1542730432.0
greysvarle,"What is the math and algorithms background for machine learning? I already know some basic algo and data structures, is that enough?",1542782073.0
WebNChill,This is an awesome resource. Thank you for posting. <3,1542757181.0
WhackAMoleE,I shall enroll my laptop.,1542934692.0
maheshhegde,"If somebody talks `ethical hacking`, I know they won't deserve to be on the earth.
 
Hacking means a different thing than portrayed in films, and security industry is of course garbage.",1543587447.0
ep1032,Recursion is just a loop poonted at a subroutine.  What do you need proven?,1542680543.0
Rebuta,If the data are already partially sorted,1542679260.0
iconoklast,"Dumb old insertion sort will probably be faster for small amounts of data. For instance, Python and Java by default use an [adaptive sorting algorithm](https://en.wikipedia.org/wiki/Timsort) that degenerates to insertion sort on small arrays.",1542682600.0
nealeyoung,"In spite of its worse worst-case behavior,
Quicksort is twice as fast in practice (on ""typical"" inputs, or random permutations), because it does the partition step in place in a single pass,
whereas in merge sort the merge requires an extra buffer and takes two passes.
This is why quicksort was the standard Unix sort instead of merge sort.

",1542684186.0
gct,"There's faster ""real world"" sprts, and merge sort requires O(n) extra storage to do the merge in the worst case.  The fastest sorts I'm aware of do a hybrid approach like [timsort](https://en.wikipedia.org/wiki/Timsort).",1542684342.0
minno,"1. Merge sort requires additional memory for intermediate steps. Other algorithms like quicksort and insertion sort only need a small, constant amount of additional space.

2. There are other sorting algorithms that have better time complexity than nlogn on certain kinds of data, like counting sort for small integers.

3. There are other sorting algorithms that have worse time complexity, but on small inputs are faster due to performing less work on each step. Insertion sort is extremely fast due to how efficient the ""shift this block of memory over by one space"" operation is on modern processors.

4. Other sorting algorithms like Timsort are designed to be particularly good on datasets that exhibit the patterns that many real-world datasets do.",1542686332.0
feedayeen,"Rolling window functions are often used in signal processing and image processing. The cost to sort 20*20 pixels to construct a histogram is tiny until you need to do it on 3*5000*5000 pixels... Then O(nlogn) really bites you.

There's several different ways to deal with those types of datasets allowing for you to reduce it and some features that normally require a sorted list like median can be computed very cheapy. ",1542701499.0
green_meklar,"Mergesort requires a lot of extra memory allocation and doesn't adapt to partially sorted data. If you want to keep memory usage low or expect to be sorting partially sorted data, you might pick a different algorithm.",1542702966.0
updice,"If you are sorting N item of a fixed length, M (10 digit numbers, for example), then they can be sorted in O(N x M) with radix sort.",1542706227.0
spooderboop,If you’re trying to shovel your driveway.,1542720745.0
tori_k,Homework due soon?,1542688225.0
which_spartacus,"With that modification, what does a \* b + c look like, and what does a + b \* c look like?",1542710908.0
disastercomet,"tl;dr we didn't need other CPUs on a board historically. It wasn't worth developing a new chip unless it was dramatically better than just waiting for a new CPU, and even then only when it made sense financially.

EDIT: please read the other comments; I wrote this in a hurry and am inaccurate in some areas.

CPUs are capable of *general computation*, which means they can compute anything that is possible to computer (with varying levels of performance).

What this meant is that, if we wanted to make NLP (or any AI program) faster, historically all we had to do was wait for Intel / AMD to develop a new CPU, which historically ~~were 2x faster~~ had 2x the performance every 2 years just because they could shrink the transistors on the chip (Moore's law). We didn't need specialized processors back then, because CPUs were advancing so quickly.

That said, even though can *perform* anything, CPUs are not always *optimized* for everything. They're usually designed to minimize single-threaded performance, which means shortening the time a *single task* needs to complete (minimizing latency).

Historically, computer graphics and gaming were the first domains where this fell woefully short, because instead of handling one task they need to handle thousands (or millions, by now): calculating the color of each pixel on a 640 x 480 screen needs *bandwidth*, not *latency* optimization.

Recent trends have subverted the world order of CPUs, in my opinion due to: a) the breakdown of Moore's law, and b) the rise of neural networks in AI, driven by vast quantities of data.

If you've taken a look at CPU specs recently, they've taken a strange turn: the clock frequency (a rough indicator of operations per second) is largely stagnant around 4GHz, and the number of cores has multiplied. We're hitting quantum mechanical limits on how small we can make transistors, so we can't push clock frequencies any faster.

In addition, recent AI (\*ahem deep neural network buzzwords here\*) now heavily depends on matrix multiplication on large matrices (potentially hundreds or thousands of entries wide), which the CPU wasn't designed for. That said, GPUs conveniently handle matrix multiplication well, since computer graphics already uses matrices, so they're becoming the ""other CPUs"" you're looking for: specialized units on the motherboard.

Clarification, GPUs aren't good for everything, but they're used for sufficiently valuable tasks (like powering Google's ad business) that hardware manufacturers have followed the money.

There are other, niche chips: audio chips for audiophiles, Google's TPUs made specifically for certain AI tasks (mentioned already), etc. But the big two at the moment are CPUs and GPUs, because of history (as always).",1542673101.0
MrPopinjay,"We kind of already do- for example the GPU is a specialised processor that is used specific tasks (primarily grpahics rendering, but AI is another suitable task).",1542670897.0
GayMakeAndModel,"I just want to chime in and say that this is one of the most excellent questions I’ve seen asked on Reddit.  This question highlights where all the change is occurring in the hardware market.  You’ve gotten some great answers too.

But I will try to add something;  anything you can do in software can be done more quickly in hardware.  You can think of software as a simulation running atop the hardware.  Specialized hardware is expensive, so economy of scale is crucial.",1542676067.0
NicolasGuacamole,They do they’re called application specific accelerators and in servers and HPC etc you’ll see them all over the place. Furthermore your desktop computer has things like this for things like DSP. ,1542672769.0
NytronX,"There *are* boards with more than one CPU. It's most common in server applications. This isn't new technology either, it's been around for a long time.

So your question would have to then be: Why isn't there more than one CPU on consumer oriented motherboards?

Answer: Cost. Devices also get way bigger. Hardly any of the computer scientists will need a specialized CPU for AI, let alone the entire market. This may change over time. You can offload calculations like that to your GPU.",1542703519.0
kyuubi42,"Companies are starting to. Google has their TPU, Apple has started selling phones with “neural engines”.

This hasn’t taken off sooner as until recently Machine learning has been fairly niche, and much of it is simply matrix math, something GPU already excel at.",1542671751.0
zerrosh,"It doesn’t really benefit the everyday work on your laptop or office pc to have extra hardware for specialized tasks, when your cpu could do the same but a little slower. The increase in price for development and production just doesn’t make it viable in the competition. 

For specialized workloads, especially in scientific or design related workloads the gpu (graphics card) is often used for speeding up workloads with dedicated hardware. Gpus have different types of cores for different tasks. 

In SoCs (System on Chip) found in devices like Smartphones we just experience, what you are describing. They add more and different types of processing cores to the same chip to enhance different  types of tasks on your phone. Such as units for picture processing or for neural networks. The difference to a pc is, that the whole processing unit is integrated in one slice of silicon making it cheap to mass produce and easier to design and integrate compared to additional chips, like you proposed. ",1542672151.0
mrwesleycrusher,"Because its harder than its worth.

&#x200B;

I am a Computer Engineering Student getting my undergrad, so decide however much ethos you want to let me have here. But, basically, there are multi-socket boards for servers like [https://www.newegg.com/Product/Product.aspx?Item=9SIA5EM2RK4564&ignorebbr=1&nm\_mc=KNC-GoogleMKP-PC&cm\_mmc=KNC-GoogleMKP-PC-\_-pla-CA+Server+Pro-\_-Motherboards+-+Server-\_-9SIA5EM2RK4564&gclid=Cj0KCQiA28nfBRCDARIsANc5BFDJbPfeSTOP81hSHahYoqBN0n0l2p4hNAuu1802Pd7\_1ujkYOLlFjwaAljkEALw\_wcB&gclsrc=aw.ds](https://www.newegg.com/Product/Product.aspx?Item=9SIA5EM2RK4564&ignorebbr=1&nm_mc=KNC-GoogleMKP-PC&cm_mmc=KNC-GoogleMKP-PC-_-pla-CA+Server+Pro-_-Motherboards+-+Server-_-9SIA5EM2RK4564&gclid=Cj0KCQiA28nfBRCDARIsANc5BFDJbPfeSTOP81hSHahYoqBN0n0l2p4hNAuu1802Pd7_1ujkYOLlFjwaAljkEALw_wcB&gclsrc=aw.ds)

&#x200B;

Unfortunately I don't know too much about how motherboards make all the CPUs play nice with each other.

&#x200B;

And, motherboards do have specialized Integrated Circuits besides the CPU in-house to ensure that everything goes smoothly. (Example, any device you plug in via USB is going to talk to the CPU via a USB controller, because the CPU does not speak USB). 

&#x200B;

But as far as laying it out like a brain, if you think about it, it kind of is, except computer brains are laid out really weird. inside of a CPU theres parts that decode instructions, store data that will be used soon, and do math for the most part. So it kinda is like a brain, except our brains do things like ""process language"" really well but we have no clue exactly what happens under the hood. Meanwhile a computer only really knows how to do math and move numbers from one spot to another. So when you try to say ""this CPU does only the AI bits"" its really a munch better idea to say ""lets design something the CPU can leverage so that it can do the computations needed for AI faster""

&#x200B;

I don't know if what I am saying is making sense to you, feel free to reply and ask questions or DM me",1542675931.0
S1eeper,"Apple is already doing this with their [new A12X chip](https://arstechnica.com/gadgets/2018/11/apple-walks-ars-through-the-ipad-pros-a12x-system-on-a-chip/), adding cores designed for AI instead of general purpose compute. I expect this trend will only grow.",1542699405.0
Enlightenment777,"Multiple processor & Multiple FPGA boards have existed for decades.  If you have a mountain of money, then you can get amazing amounts computational power.  Most ""brokes ass"" college students and home computer owners aren't aware of high-end expensive specialty boards.

https://www.dinigroup.com/web/index.php

",1542706880.0
krum,"There are GPUs, and other devices you can plug in to a USB port that do this.  There are also GPUs that don't even have video outs that are mainly used for heavily computational tasks such as AI or cryptocurrency mining.",1542671494.0
jhaluska,"In theory it could happen, but that kind of integrated circuits are very expensive to make.  So currently it's more cost effective to utilize the CPU or GPU for the task instead.",1542673453.0
steventhedev,"The short answer is there already are. The practical answer is because it's expensive to add more that are available to the user for general computation. That having been said, there are special purpose cores all over the place, even inside your CPU (e.g. Intel ME).

[Gwern wrote about this relatively recently.][0]

[0]: https://www.gwern.net/Turing-complete#how-many-computers-are-in-your-computer",1542693294.0
spinwizard69,"First as has already been pointed out, your header line statement is false, there are many processors on a mother board beyond the CPU.  These specialized processor accelerate many functions but don’t directly execute user code.  In modern SoC, like Apples A series, the specialized processors are on board the SoC.     Again many never directly run user code.  

Now if you consider Apples latest SoC it in fact does have a specialized processor that is designed to run user ML code and likely other codes if programmers can figure out a way to leverage the unit.    Being Apple they call it their Neural Engine, in reality it is amatrix professor operating in a heterogeneous environment.  

In a way what you are asking for has skipped board integration and moved directly to SoC integration.  Apple isn’t the only ARM vendor doing this.   As for Intel they do a number of custom processor that never get talked about so it is possible such tech might be running on Intel servers someplace in this world.  At last count Intel had like 30 custom processors in various stages of design so you can’t rule out one or more of them being extended to better support ML.  

I haven’t even touched upon GPUs because the stAte of hardware support for ML applications is in flux.   However both AMD and NVidia are going after ML support in their external GPUs.  So you already have a chip external to the CPU that accelerates ML applications and those chips are now being optimized for far better ML performance. 

What it comes down to is this, you haven’t seen specialized hardware until now because there wasn’t a reason to support such hardware until now.   AI techniques have only recently matured to the point that it makes sense to add ML hardware.   Further in the case of SoC designers no have surplus area on the die to implement matrix processors.   So we have a convergence of technologies moving us forward.  ",1542694714.0
funk_monk,"Computers used to do just that.

In the 80's and early 90's most computer processors couldn't natively do floating point maths (it was assumed that regular users didn't have enough need for it to justify the expense) so they had to emulate it through software using lots of integer operations which made the code run slowly. However, lots of motherboards featured a socket for an add on FPU which would significantly speed up floating point maths so you could buy a second coprocessor if you saw the need.",1542707201.0
combinatorylogic,"If you're thinking about *symbolic* NLP - no specialised CPU architecture can be better than just a generic good old RISC anyway.

If you think about all that ANN stuff - sure, a GPU is a much better fit (and you're likely to have one), and some specialised inference cores starting to appear now, from google TPU to some [weird external devices](https://techcrunch.com/2017/07/20/movidius-launches-a-79-deep-learning-usb-stick/).

Also, you'll probably find this talk interesting: https://www.youtube.com/watch?v=ctwj53r07yI",1542707450.0
LearnerPermit,"Various computers over history have had multiple processors as in the CPU. A decade or two back, you could get Windows systems with dual cpu for desktops, today most desktop processors are multi-core, which are basically multiple cpus inside of ceramic case, and or they might be on the same physical silicon chip. 

Plenty of intel servers are multi-processor in addition to multi-core. For example: https://www.dell.com/en-us/work/shop/productdetailstxn/poweredge-r930

Almost all of the top500 super computers are multi-processor and often multi-motherboard systems https://www.top500.org/lists/top500/


Oracle/Sun and Cray pretty much specialize in multi-processor big-iron or super computers. 


At the desktop world, as others have noted there is often one general processor, aka the CPU. But that's not to say there aren't other processors for specialized components. Higher end audio might have their own processors and digital-audio converter chips. Video cards have their own GPU which are kind of specialized general processors.
",1542687969.0
shawnwork,"I believe the new MacBook Pro’s have an intel and an Arm cpu in the same motherboard. Read more about the northbridge initiative and issues.

Complexity arises when both CPUs need to communicate together. It was attempted before in the early 2000 with the second gen pentiums but lost traction after that.

Hope it helps.",1542695827.0
bumblebritches57,"There isn't.

There are multi socket motherboards available where you can have two entirely separate CPUs each with their own heatsink, etc.

Also, most ""CPUs"" these days have not only multiple cores, but entire ""core complexes"" aka multiple CPUs within a single package.",1542707538.0
LongUsername,"Most other people seem to cover it well:

There are server class boards (have been since the 90's) that can socket multiple General Purpose CPUs. These have faded a bit from popularity now that CPUs are heavily multicore.

There are several ""Special Purpose"" processors in most computers.

The first well known one would be the x87 FPU Coprocessors for early Intel chips. These were later put on the same die in the 486DX.

The common one is the Video Card which has a massively parallel compute array aimed at vector calculations (usually 3D graphics)

For a while there were dedicated Physics processor cards available. These have generally been replaced by using some of the Video cards processors to run the physics tasks.

Higher end network cards also have fairly complex specialized processors on them.

Most desktop level computers don't have NLP or AI processors on them because it's a fairly specialized task that they can dedicate the resources of a core to when it's needed.  Phones are seeing more dedicated voice processing chips, especially with the need for low power wake on voice (""Ok Google"", ""Hey Siri"", etc)

If you had a very dedicated use case for an AI system you can create a dedicated processor using a Field Programmable Grid Array (FPGA) chip. This allows you to make a custom hardware processor much cheaper than you could with a non-programmable part (ASIC). These are used a LOT in industrial applications, but generally aren't something you'll see in consumer gear.",1542713809.0
Lucretia9,"There have been dual socket motherboards in the past, not just for x86. You just have to look.",1542690642.0
chadwickofwv,Price.,1542732742.0
in_the_comatorium,"I'm not sure if someone already mentioned the term ASIC to you - it's short for Application-Specific Integrated Circuit - but that's basically what you are talking about. These are definitely a thing that exists. As far as I know, they would plug into the motherboard via one of the expansion slots, like a GPU does.

Also, I know this isn't the question you were asking, but there are actually high-end motherboards out there with slots for more than one CPU.",1542684854.0
hexaga,"> I just need to think of a situation (no matter how rare) where a single thread would do the job

is this not like, most situations",1542674071.0
ElecProg,"As others have stated, a single thread will almost always get the job done. However then the question remains, will it ever be better than a multi threading approach?

If by multi threading you mean using OS threads there is one clear case where a single OS thread would be better: when you have only one core. The best solution here is to have green threads and thus manage multiple threads of execution at the application level. This allows for a greater level of control and has less overhead. However once there *are* multiple cores this overhead is negligible next to the added benefit of parallelism.

Do note that we still have multiple threads Ove execution. For most applications this is preferable as networking is slow (comparatively speaking) and while one thread waits another can use the processor and your throughput will be higher.

The only case where not even having multiple threads of execution is the best is when the only thing you're doing is monitoring incoming packets and maybe sending a packet back, without any retries or anything else that would mean you'd be waiting for the network. In this case any sort of threading would be probably only induce overhead (you can only receive as many packages as the speed of your connection which is typically way slower than your program).

So yeah only if your CPU runs at about 100Hz would there be any reason to not use any sort of multi threading.

Hope this helps. :)",1542738547.0
Grimaldi2,"Have you had a look at non-blocking synchronization? That means implementations of simple data structures like linked lists, that work with-out blocking (contention) on a multi-core machine and are thus more scalable than their lock-based counter parts. This is a kind of black art, check https://queue.acm.org/detail.cfm?id=2492433 „Nonblocking Algorithms and Scalable Multicore Programming“.  If you can do it single-threaded, the world is so much more predictable...",1542831880.0
aeveltstra,"Modern CPUs and GPUs already try to handle multiple threads at once, either by having multiple cores, or by employing time share techniques, or both. This allows them to run multiple applications simulaneously (if only seemingly, in the case of time sharing). 
  
Making our applications multi-threaded on top of the 1 CPU core they get assigned means that either the language or the application now has to perform the techniques that the CPU already does by itself, whilst trying not to confuse the CPU. 
  
Both layers of multithreading have proven rather difficult, and a cause for data corruption and security breaches. 
  
Some languages make multi-threading easier (rust, julia) than others (c, java, javascript). Therefore I would say that anything we build in a language not particularly designed for multithreading should not even attempt it. Just run multiple instances next to each other, or on multiple virtual servers.",1542709698.0
sjh919,"If it's a server it pretty much has to be multi threaded to connect to multiple users concurrently, yea? Not sure where people are getting it's okay to have a server with a single thread.",1542764311.0
khedoros,"They contribute equally. CISC and RISC are just as powerful in the operations that they can represent. As far as I'm aware, no one has been genius enough to come up with a formally-defined language that's more powerful than a Turing machine.",1542671644.0
kc3w,A RISC processor could run a program to simulate a CISC processor. This is also possible for a CISC processor in regards to RISC therefore there shouldn't be a semantic gap.,1542768940.0
kc3w,"Basically you can just write the loops as two sums `∑ i = 0 to n (∑ j = i+1 to n (1)) `

and then just do a bit of sum magic.

```
Length of List n
∑ i = 0 to n (∑ j = i+1 to n (1)) 
= ∑ i = 0 to n (n-i) 
= ∑ i = 0 to n (i) 
= (1 + 2 + .. + n) 
= n(n+1)/2 
= (n² + n)/2 ∈ O(n²)
```


Edit: 
To make it more clear think of how the algorithm works. Find lowest element means you have to iterate over the whole array to check which one is the lowest. Now next time you have one element in the right place meaning selecting the next lower one takes one less step. Intuitively this means you have to do the find lowest element n times.",1542656486.0
Yoghurt42,"Best case also iterates n^2 :

    for i=1 to n:
        for j=i+1 to n:
             ...

So n (outer loop) times O(n) (inner loop) = O(n^2 )",1542655130.0
ared38,"\> For best case: j=2, i=1 \[Array is sorted\]

&#x200B;

What are you substituting in for tj in that case? And what's your rationale for doing so?",1542656017.0
reality_boy,"The one you can afford. I went to NAU and my friend went to Stanford and we both basically make the same money and have similar career paths.

His only advantage is he has more Silicon Valley contacts (although I managed to intern there).

When I’m doing interviews I’m much more interested in work experience than education. Show me a resume with 5 jobs in 5 years and I’m suspicious. On the other hand one job in 5 years is a great sign.

The other biggie is acing the technical exam. If you can’t work out how to implement an uppercase routine (in ascii) then your not a good fit.",1542605187.0
CorrSurfer,"Cambridge University has a couple of good research groups, as does ETH Zurich. TU Munich may be an institution that you want to have on your radar as well.",1542632409.0
PastyPilgrim,"I really like bazel/blaze. This weekend I was working on a project I wanted to open source, which meant I wanted a makefile for ease of access, and I had forgotten just how cumbersome it is to make makefiles. If only bazel could export makefiles for portability.",1542570072.0
feihcsim,is svelte the future?,1542580968.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1542558779.0
Bromskloss,"I, enthusiastically, expected something about category theory being built on a solid foundation…",1542487471.0
pm_me_ur_photo,Mysteriously breathtaking!!!,1542477630.0
foreheadteeth,"I've directed a couple of MSc theses on algorithms for fast arithmetic. It turns out you can do almost all operations in O(n log^2 n) time, see [here](https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations).",1542474798.0
SirClueless,"Man this is hard to read. The notation doesn't keep straight what is multiplication, what is exponentiation, what is function application, and what is subscripting. You must use a lot of context clues.

This is made harder by overloading variable names to mean various things at various points. For example, in the Pseudocode section it says ""Here t=X\^k"" meaning to break the input into k-digit numbers in base X. But X was previously the name of one of the input multiplicands, k was the number of splits used, and this base X\^k was previously called B not t (and also, redundantly, b). This is made more confusing by the lines *immediately following* asking the reader to assign the values (0,1,2,-1,inf) to t -- not the t of ""t = X\^k"", the t which is the parameter of w(t) as it was used earlier.

EDIT: Oh, after checking out [the Wikipedia page on Toom-Cook multiplication](https://en.wikipedia.org/wiki/Toom%E2%80%93Cook_multiplication) it's pretty clear why this ""article"" reads so poorly: it's basically a plagiarized copy of the Wikipedia entry with the mathematical notation mangled and some sections paraphrased.",1542534338.0
r8night,Could you explain why you think so ?,1542447392.0
pmpforever,"This is the essential difference between imperative and declarative languages. In an imperative language, you describe how to reach a solution; in a declarative language, you describe what a solution looks like.

In an imperative language, you typically have no choice but to procedurally mutate data. This inevitably leads to a lot of state which must be maintained.

In contrast, declarative languages provide higher order functions and alternative evaluation methods to reduce the amount of explicit state necessary to perform a computation. If you find this interesting, I would recommend looking into Haskell.",1542768323.0
r8night,QUIC looks interesting although it isnt really new. https://en.m.wikipedia.org/wiki/QUIC,1542443637.0
pycckuu2000,What is RL?,1542444303.0
zetsubou-tan,let me just put some stuff in my knapsack ,1542508848.0
NytronX,Is it DPBUA or DPTDA?,1542491639.0
IdealImperialism,Most of those are terrible,1542382073.0
rizwakhan001,"Computer science is a very exciting thing to learn and moreover, there are many options for majoring in meaning you can hardly run out of options when you are simply looking for the best certification courses that you can major in when it comes to computer science. There are small computer packages that you can learn for beginners including use of Adobe, Ms. Word, all which tend to add up in the end when it comes to software development. 

On the other hand, we do have major fields which you can major in and the good thing with them is the fact that they have career opportunities as well in that you can get real employment unlike the ones above that I have mentioned. These fields include; programming, hardware experts, database experts, networking and telecommunications as well. 

The good news is that you can be able to get all these as a full package simply by doing our full stack software development package at our good programming school called [Holberton School](https://www.holbertonschool.com/education) where we teach front-end and back-end web development and software development in just 2 years. In the end, you will get a certification that will help you do most if not all of the above fields in computer science that I have just mentioned. Good luck. ",1542508665.0
DevFRus,"I came across this article due to [Arjun Raj's tweet](https://twitter.com/arjunrajlab/status/1062306765042339845), and I thought his comment was fitting:

>Interesting article about the poor state of electronic medical records. However, article pits humans vs. computers as some grand battle between rebels and industrialization, when the problem seems mostly to be just straightforward crappy user interface design.

How should we approach better HCI and UX for these sort of systems?",1542373443.0
GayMakeAndModel,It’s because of requirements like meaningful use and MIPS imposed by CMS that seem to change constantly.  The haste with which EMR vendors implement these initiatives leads to a bad user experience.,1542378082.0
wh0dareswins,"Medical student here. The existing EMRs (at least what I've used) have been optimized to serve the hospital and billing/insurance bureacracy rather than time efficiency for doctors. Makes sense too since it's these larger entities, not the health professional end users, are bankrolling the software. ",1542409819.0
berf,"It is worse than bad UX.

The main point of computerization is to ""replace"" people, where ""replace"" is in scare quotes because what it actually does is replace clerks and data entry people with professionals.  Many years ago doctors had a lot of clerical help, now the have to use (admittedly bad) UX to do *themselves* what those helpers did.

This is an aspect of the ""automation crisis"" that no one much talks about.  Yes computers eliminate many jobs.  But they don't replace all the work that those people used to do.  They often just move that work to other people -- not necessarily to other employees of the same companies, sometimes to customers, sometimes even to innocent bystanders.",1542407488.0
EdHerzriesig,"Speaking from what I’ve heard about the Scandinavian/Northern EU hospitals; the main problem seems to lie in inflexible, old data architectures. Hospitals need systems that can securely pipe data amongst each other from which everything else can be built upon. 

Making better and more efficient data structures for the health sector is huge but extremely important issue ",1542394910.0
RPGProgrammer,"This is a faster horses problem.  Someone smarter than us needs to come up with a solution they like first so they can pivot in a direction they want.  This problem also seems to solutionize in the direction of a device rather than a UX.  I mean, UX lives matter but starting at the device level with something that can be used by every care facilitator at a given hospital that can withstand vomit, blood, being throw by a psychopath/Hospital Administrator then moving into the views that support nurses/PA's/Docs in different clinical categories seems like the best way to go.",1542377265.0
jomofo,"This isn't unique to HIT, but endemic to enterprise software in many domains. Every organization evolves its own nuances so a budding enterprise software provider has to build high levels of customization into the product to even sell it. This greatly increases the complexity of the software and makes it nearly impossible to create a UX that's efficient, pleasant and works for everyone. It's a very different environment than say working for a Twitter or Facebook where the UX can be locked in and optimized wholly across the entire userbase. 

The product managers in such companies tend to wield very little power compared to salespeople who are working on potentially huge contracts. How do you turn down building a custom feature that helps you sell into a large organization even when it means adding complexity to your codebase and ultimately making it difficult to make sweeping changes to your UX? 

There also tends to be a huge business opportunity for consultants (often a profit center for the enterprise software provider itself) that the organization has to pay to configure and customize the system for their unique business processes. Standardized business processes would greatly help, but the provider would lose consulting money nevermind that the organization might view their custom workflow as a competitive advantage and thus be unwilling to change.

I could go on, but suffice it to say that developing enterprise software sucks. ",1542437237.0
NytronX,"Someone sick a summary bot on that article, it's TLDR. I can't see how anyone in the medical industry would want to go from computerized databases back to paper.",1542428225.0
ENORD,"The doctors are not the customers, and often even a minority of the end users.

The actual, paying customer is the hostpital administration. In addition, the de-facto customer is in large part the local government/Health service (insurer). 

Doctors want easy/fast data entry, administrators want easy/fast data extraction and refinement (Reporting), 'cus that's how you bill insurers, government and Direct customers. These are at odds. Guess which one wins.",1542706342.0
diamened,Doctors are among the first professionals to be replaced by AIs. AIs are already diagnosing better than human doctors,1542418689.0
abottomful,"So I am a linguist, and I lean towards computational linguistics and conversational AI as my interests. Through just casual discussion with some peers and bored thoughts, I was wondering why computers don’t have dedicated processors for things like NLP/NLU versus computer processes? So to make it a bit more understandable, why doesn’t a company incorporate another CPU on a motherboard strictly for the sake of artifical intellegence, similar to regions in the brain?",1542574886.0
FearlessObject,Raspberrypis are so overated if i see another rpi project im gonna puke,1542359610.0
orangejake,"We need more information to say anything intelligent about this. This is because ""Not all TSP's are created equally"". As an example, for the *most general* case of the TSP, there's an easy result:

> For any poly-time computable function alpha(n), the TSP does not have an alpha(n) approximation unless P = NP.

This is a tremendously bad result. For reference, a k-approximation means that, while your algorithm might not output an ""optimal"" answer, it is at most within k multiplicative distance from optimal. As an example, if we want to maximize some quantity, a 2-approximation will always output something that's *at least* 1/2 the true maximum (and in practice can be much closer to the maximum).

So approximating the general TSP seems quite difficult. Let the ""metric TSP"" be the TSP where all edge costs have to be ""real distances"", meaning they satisfy the triangle inequality:

d(x,y) <= d(x, z) + d(z, y) for all z.

There's a rather easy 2-approximation for this, and a slightly more advanced 1.5-approximation.

___
Essentially, to answer this question we need to know more about your graph G. If it has certain ""nice"" properties you might hope to do well, but in the general case TSP is extremely difficult, even to approximately solve.",1542345039.0
SOberhoff,"> Is there any known problem that looks like that?

What about TSP (duh)? There are plenty of approximation algorithms and heuristics for TSP in the literature. I'd try adapting one of them for this problem.",1542331133.0
gnupluswindows,"This is clearly a generalization of TSP, which is this problem with every set being a singleton. So unless you assume the triangle inequality, it's inapproximable. If you do, the problem gets a little more interesting. You'll probably get good results by applying Christofides's algorithm as if it were normal TSP and then greedily shortcutting the most costly redundant vertices. ",1542346429.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1542321717.0
ajcave,"I’m not convinced by the other answers. Perhaps I’m missing something, but lookahead looks like an intersection operator to me, and IIRC regular languages *are* closed under intersection...

Backreferences, on the other hand, definitely take you out of regular languages",1542312004.0
GiantRobotTRex,"Yes, many ""regex"" tools use languages that aren't truly regular. These are often referred to as extended regular expressions.",1542304870.0
aunva,"Yes. It's why you can have regexes that can match strings of the form a+b=c, where a,b,c are numbers. That's definitely not possible in a regular language, but it is when you can use lookahead/lookbehind",1542304423.0
Jaxan0,"I remember reading a note of Dijkstra. He was claiming that we shouldn’t say “programming language”. As it then seems that a programming language is good only if it is similar to a natural language. He mentions COBOL, designed to be close to a natural language, but bad in his regard. Instead, he proposed to call it a “programming formalism”.",1542299836.0
gnullify,Perl creator Larry Wall studied linguistics. IIRC he earned a degree by convincing his college department that programming classes should count towards his linguistics requirements. Either that or he convinced them to let him do his own integrated studies program which combined the two. Something like this.,1542326453.0
josephs_1st_version,"A lot of the theories of linguistic are the same theories of computation. 

Chomsky’s “Syntactic Structures” and his “Three models” (published in IEEE?) laid the foundation of modern information theory and parsing techniques. Knuth took a copy of the book on his honeymoon. (!)

The Chomsky Hierarchy maps to regular languages, context-sensitive, context-free languages, and Turing machines exactly. 

Both linguistics and theory of computation use the Chomsky Normal Form to describe things. 

(On mobile, and it was yeeears ago at Uni, so all this might be wildly inaccurate) 
",1542340064.0
vaderfader,"abstraction, categories/classes, domains, hierarchies.

( object, action, attribute) :: ( noun, verb, adjective ).

being both a reflection of our human consciousness they're bound to have some inert same meta-structures, cause and effect (if -> else) etc. hell even variable passing into a function could be viewed essentially as cause and effect as well.

they differ fundamentally in their nature of communication; how vs description of state, mainly because in computer languages we are not talking to an agent, but a computer that will evaluate our set of logical propositions.

(an interesting question is how we talk to a computer, when the computer has some form of agency: for instance when we are training a statistical model, we can transform our data into some coordinates our computer can read, and let it evaluate what is what, underneath a general guide).

these are probably trivial comparison's but it was the best i could come with on the spot

&#x200B;",1542343849.0
dontcallmelate4dinnr,"Programming language is explicitly created, natural language is emergent. ",1542309365.0
sinrin,Syntax and Semantics.,1542313329.0
nemesit,Precision,1542360457.0
rizwakhan001,"There are many theories and debate as to what are the similarities and differences between natural language and programming language. As for what is similar between programming languages and natural languages, they are as follows;

\- The first similarity between the natural language and programming language is that they both get to express various programming and software development algorithms in a definite number of steps.

\- The second similarity between programming language and natural language is that the programming logic that is used to solve various software development problems tend to be the same or similar in many ways. 

\- Lastly, both natural and programming language are meant for communication. Natural language is often used for communication between humans. Programming languages too are used between humans and also computers as well. 

To know more about these programming techniques, you can go to our good programming school called [Holberton School](https://www.holbertonschool.com/education) where we teach full stack software development in 2 years. We have highly trained faculty members who have worked at Google, Instagram, Facebook, Slack, Dropbox among others and therefore, you will get more guidance and assistance in learning more about the difference between natural language and programming language. All the best. ",1542508622.0
i-var,"Programming languages are all basically products of kontext-free grammars according to Chomsky hierarchy. Google it.

Natural languages are (at least I think) more than that. Thats also the reason your computer cant “understand” you algorithmically (through a programm that always halts) as it can understand programming languages (through a compiler, yay!)

And im no geek, im just in the middle of theory of CS course lol. ",1542350246.0
p3s3us,"Of all languages, is Scheme so worse than Python? It has a few parentheses more, but other than that the structure is mostly the same. ",1542353770.0
sccrstud92,"Missing parens

> `f >>> g x` is equal to `g(f(x))`",1542339258.0
rizwakhan001,"Recursion schemes are very important in helping you as a programmer to be able to create sophisticated software applications and the good thing is that it is not limited to only one programming language and therefore, this means that it can be used in various programming languages. 

By definition, recursion schemes are simply be defined as a set of simple, composable combinators that get to automate the process of traversing and recursing through nested data structures. As I earlier indicated, recursion schemes are not a product of any programming language or environment meaning that you can get to express recursion schemes in any language with first-class functions. Clojure for example, gets to use recursion schemes to power it’s Clojure walk API for generically traversing and maps. There are many articles and research that is being done towards getting the most out of recursion schemes so do not hesitate to check them out. 

To learn recursion schemes from introduction to advanced levels, you can go to our good programming school called [Holberton School](https://www.holbertonschool.com/corporate-partners) where we teach various programming languages in a period of 2 years to help you to get to use recursion schemes as much as you want when creating application programs for any platform you are intending to start working with. ",1542639341.0
cjeris,"Because no one else has said this yet:

**Learn your math.**

If your math classes are hard, work on them until they are easy. Not so much because the topics in them are directly relevant - many of them aren't. But until you can reliably think in a clean, disciplined, precise way - until you can look at a calculation or an argument or a program and _know_ whether it is correct - you will always be struggling.",1542247269.0
bpatterson500,"[https://teachyourselfcs.com/](https://teachyourselfcs.com/)

&#x200B;

This website has a bunch of resources that describe kinda the standard topics that are covered in an undergraduate computer science degree, and why it's important to learn about them. It also has links to lectures and book recommendations for each of the topics on the site. 

&#x200B;

In your position, I would recommend learning programming. HTML and CSS are used primarily in front end web design, so try learning another language such as C++, JAVA, or Python. Do some personal projects with these. Once you master one of these languages, learn about data structures and algorithms. I've heard that these are the most important for getting a job in industry, but I'm not entirely sure how true that is. 

&#x200B;

But the actual field of CS encompasses way more than just programming, because you get into Operating Systems ( ideas such as Virtual Memory, thread scheduling algorithms, etc.), Computer Architecture (improving hardware performance with things such as pipelining, caching, speculative execution), Compiler theory (most efficient way of translating high level code into assembly language), programming languages (how programming languages are actually designed, as well as the various programming paradigms such as Object Oriented and Functional), Security/Cryptography, and more. These are all things you'll learn in college, but if any of those things interests you, then after you learn programming and data structures/algorithms, you could look into diving deeper into any of these areas. Another good piece of advice (IMO) is to learn to use Linux. A lot of software development, with the major exception being .NET applications, is done on Linux, and is something that will look good on your resume when it comes time to applying to internships.",1542245459.0
acommentator,"FWIW there is a big gap between computer science (theory of computation systems) and software engineering (building the right thing in the right way), with the overlap area being programming (getting a computer to do what you what).

* A good way to learn programming is to find things you are intrinsically motivated to build, and to figure out how to build them. I used to suggest Python as a first language, but for a variety of short term and long term reasons I'd now recommend that you start with JavaScript and then transition to TypeScript.
* A good way to learn CS is to learn math (particularly discrete math) and major in CS at university. If you do machine learning you'll probably make a lot of money.
* A good way to learn software engineering is to build things for other people that you have to keep working for a long time. It is basically impossible to teach software engineering in a university setting where you don't have users, you don't have long term projects, and you don't have real teams. There is less overlap between software engineering and CS+math than many folks realize. If you want to build software products, then a background in design would be a lot more useful than a background in math.

&#x200B;",1542250758.0
Sirmrwhale67,Not advice but that’s really cool how you’re getting into computers and cs now. I wish I started earlier. ,1542252784.0
nicoladawnli,"Just to add some balance: if it turns out math and you don't get along there's still a ton of IT work. Policy advising, hardware repair are the main two I'm in. I can't solve a differential equation, but ohms law makes sense to me! 😁",1542251319.0
tinbuddychrist,"Strongly second the earlier comment about learning your math. Algebra, trigonometry, and calculus are all good to know. When you get to college, up through Calc III (or whatever class covers ""cross products"" which are useful for 3d graphics programming) as well as linear algebra (good for both 3d graphics and machine learning a.k.a. AI).

Other than that, learn a general-purpose progeamming language. Good first choices are Python, C#, Java, or C/C++, in approximate order from easy to hard. (Any of those has a ton of jobs.)

Because you're starting young, if you can devote 30-60 minutes a day to this, you'll be way ahead when you enter the job market.

If possible, try to find a mentor with professional experience who will review your code. I'm happy to volunteer - feel encouraged to PM me.",1542251182.0
cp5184,"Pursue what you like.

When in doubt, test the waters first.  

Do you like web design?  Do web design.

Do you think you would like something else?  Say, databases.  Well, try databases, see how you like it.  Do you think you'd like circuits/circuit boards or chips/silicon?  Try designing circuit boards, try designing chips/silicon.",1542253102.0
dkfgo,"You're going to get a lot of advice because I bet everyone here wishes they did this when they were 15!

Anyway, here's my two cents: if you are liking HTML/CSS and want to go deeper on building websites, start doing courses on Javascript, look into front end development, and prepare yourself to be overwhelmed. If you start now and you're a capable learner, by next year you could already be earning a lot of money.

If you actually want to know even more, pick up a backend language, and start building some project from scratch. A personal blog, a news coverage website, something like that.

There is a lot to learn, everyone feels intimidated at first, dont worry too much about it, just go learning things as you need them on your project.",1542257116.0
YuhFRthoYORKonhisass,I think the most interesting field of computer science right now is machine learning. Look up Two Minute Papers on YouTube.,1542246223.0
sd_glokta,"If you like web design and you have the guts/brains, you can't do better than the [React.js](https://reactjs.org/) framework. 

It's not easy to learn, but once you get the hang of it, you can accomplish great things.",1542244826.0
daedalus4210,"Explore some coursera classes.  They’re open and free and you can do them self paced.  Or tune into a YouTube CS channel and explore different specific areas you may be interested in.  Off the top of my head a few specialized topics could be

Web design
Quantum computing 
Machine learning (AI)
Biotech (computers + biology)
IT and communication systems

I wouldn’t go too in depth at this point, just get a high level view of different areas and see which area grabs you most, then concentrate more efforts into that


",1542245094.0
uniq,"I started programming games in an academy when I was 12. When I was 15 I started reading some ""hacker magazines"" and I joined an IRC channel that was full of people who were using Linux, programming in Perl, bash, etc. They taught me a lot and we did several little projects together. Actually, I still enter in that channel every day, but we do not talk much nowadays.

When I was 18 I started a Computer Science degree and started working in a software company. I think every good computer scientist/engineer needs a solid background, and university gives you that. For me it is a bit boring to learn by myself theoretical things without any direct practical purpose, but since in the university it is mandatory to study that, it was a nice way of solidifying my understanding of how a computer works, from logic gates to abstract software architecture patterns.

Now I'm 29, I have been working in the same software company for 11 years, starting as a junior developer, becoming VP engineering, and now performing other roles such as product manager and CPO, but always programming. I also started (but did not finish) a master in artificial intelligence, because it is interesting.

Regarding to your question, I think the key is to **work all the time, mostly in personal projects**, collaborating with people who know more than you, and not being afraid of maths and logical thinking. I think this last point makes the difference between a good software engineer and an average one.
",1542247601.0
gusmeowmeow,"I would get into some basic hardware (arduino, raspberry pi), AI/machine learning, IT security - that's a mean combo that will take you far. Someone mentioned React JS - a good base in JS  & frameworks will go a long way ",1542247756.0
Furious00,"Learn cybersecurity and you can write your own ticket these days. Pentesting, forensics, threat hunting...all lucrative jobs at the moment. Bonus points if you learn operational technology with it.",1542249483.0
henzosabiq,teachyourselfcs.com,1542250442.0
red_linop,"Just wanted to add an engineer's perspective. If you are into portable devices then look into microcontrollers. You can start with something simple like arduino which is a development board. Then go into microcontrollers. You can accomplish many real life applications with low cost. 

Later to get a real good understanding start with digital logic design. Learn combinational and sequential logic. See some basic construction of processors using sequential logic. See assembly language. Study embedded systems design. 
This route will give you deep insight into hardware part of programming and how computers and mobile phones and other electronic portable devices  work.",1542260086.0
ollee,"Advice that I don't see anywhere here and wasn't given to me at the time that would have helped my drive a lot: look into your school's programs for taking college courses while still in high school. Find out the requirements so that you can take a college course or 2 your senior year and take a programming and a math course. I didn't know my school had these at the time but I wish I would have done that and I think it would have helped a LOT for my drive to do well in my courses. Also, you're getting both HS credit and university credit in one sweep. It's an often overlooked opportunity and while you might be a couple years away from that, if you know it is an option now, you might feel more inclined to steer your focus towards that.",1542270617.0
my-best-guess,Here is a nice video that lays out [a map of the field of Computer Science](https://www.youtube.com/watch?v=SzJ46YA_RaA). Take your pick!,1542283489.0
berryer,"* self-written calculator programs are free game on standardized tests.  Learn TI-Basic and automate anything relevant to maximize your scores.  TI-Basic sucks, but it will be a decent lesson in working with system constraints.
* what games do you play?  Getting involved in the modding community is a great way to give your project relevance to yourself (which will help make sure you see it through)",1542284242.0
rippingbongs,Man I wish I was posting this at 15. I was high as hell. Keep it up,1542311060.0
gnullify,Great job. The CompTIA A+ would be a good way to get your feet wet if your parents are willing to support it. It's $200-300 iirc. It focuses on computer repair and some basic networking. If you can tackle that then you can try to get some more technical certifications. ,1542247298.0
kellenbrown12,"Take your certification courses! Anything you can find. A+ cert, networking +, CTS. Once you take one and pass one it’s like an addiction. There is an app for the CompTIA A+ cert that gives practice questions that will help for the exam. 

With that being said, certs aren’t necessary but are sometimes viewed as more important than a degree ( I know you’re a ways away from thinking about a college degree choice). 

Just learn as much as possible whether it’s on your own or through sophisticated courses. HOW YOU ANSWER QUESTIONS IN AN INTERVIEW WILL ALMOST ALWAYS BE THE MOST IMPORTANT PART OF ANY IT RELATED JOB APPLICATION PROCESS. Get familiar with networks of all kinds and explore your own computer. This field has no limits  ",1542245217.0
acroback,"Computer science != Programming.

Don't mistake one for another. If you dislike math, start to work hard.

Just because you like programming doesn't mean you understand computer science.

Computer science is math and how to solve a problem while programming is like application of a tool to solve a problem.",1542258680.0
theblindness,"Math is easier to learn in highschool than college. Try to get as far as you can im highschool so that you have as few classes remaining for college as possible. If you want an easy next stepping stone in your coding journey, the next logical step is client-side scripting (JavaScript/ECMAScript). You will soon find that there are limits to what you can do client-side and need something server-side. A lot of folks started with Perl or PHP, but ruby and python are more fashionable today. Ruby seems to have a more web dev focus, and python is more generic. So, a strange idea...computer science isn't actually about programming. Programming is a tool that allows discussion of a lot of topics that are really just math under the hood. You have to really love math, and I mean love it. If you like coding, but not math, maybe computer science is not the right degree. Also, when you finish a computer science degree, you will be about 4 classes short of a math degree, but everyone will think you have a degree in fixing computers. Fixing computers is actually a 3-credit class that goes over info that computer science majors don't know after 120 credits, because computer science is really about math problem and algorithms, more than it is about computers. If you do talk about a specific computer, it will likely be about a theoretical virtual machine for the purpose of your systems design class, which won't teach you how to remove coffee from a laptop. (IT already has the answer to that one: order a new laptop.)",1542262350.0
Humble_Transition,Well i am 14 years old turning 15 next year wanna be friends ?,1542274773.0
IdealImperialism,"This is about as good as you'll get

https://www.prowesscorp.com/computer-latency-at-a-human-scale/
",1542231475.0
wtallis,"Hard drive caches are DRAM, not SRAM like CPU caches. You can look at the circuit board on bottom of a hard drive and read off the DRAM part number. For example, one of my Seagate 3TB hard drives has a 64MB (512Mb) Winbond DRAM part that is rated for DDR2-800 CL5. Newer hard drives probably use DDR3, and some SSDs are now using DDR4 or LPDDR4. ",1542233534.0
LearnerPermit,"You're not going to find a universal answer. It's highly dependent on the product or family of products.

A processor's L1 cache will typically be faster than the L2 cache and in same cases L3 cache. 

https://ark.intel.com/products/123613/Intel-Core-i9-7900X-X-series-Processor-13-75M-Cache-up-to-4-30-GHz-

On intel processors of late, from memory, they've been publishing speed as number of clock cycles, not refresh/read time. 

On a hard drive with a spinning platter, there's no need for an absurdly fast cache, even a few DRAM chips is good enough. Someone else already mentioned the cache on their SSD. 


Plenty of other things have caches or buffers, your cdr/dvdr drive might have a cache and/or buffer. Again that's most likely to be device specific. And again any cheap DRAM will likely work. Your video processor might have a cache on or near the gpu. 


If manufacturers aren't publishing it, it's probably because the market is saying they don't care about that data. ",1542248835.0
future_security,"According to [Agner Fog's ""The microarchitecture of Intel, AMD and VIA CPUs""](https://www.agner.org/optimize/) the L1 cache latency is about 4 cycles on every CPU described. The L2 (and L3 cache, if it exists) have double digit latencies, the fastest take roughly 15 cycle. Convert between cycles and nanoseconds using the clock speed.

You probably care more about latency than bandwidth. (Bandwidth is the maximum amount of data you can transfer under ideal conditions.) They are related but you shouldn't assume that you can calculate one from the other.

I believe latency and seek time is easy to find for RAM and hard drives respectively. I'm not aware of where to find information about hard drive caches, but there may be benchmarks that include that data.",1542257262.0
Spearovisi,"I don’t know.  The tools are definitely becoming automated, but the human-centric insights required to leverage that information into meaningful business solutions... probably not so much.",1542218127.0
alpacalisp_now,"""Analysis"" implies understanding. We do not have computers that ""understand"" yet. Computer vision and language processing are still mathematical processes. Google appears to understand language, but it only understands the mathematical patterns embedded within language. That is insufficient to represent a concept, which is the fundamental unit of understanding. They don't even recognize the existence of a ""concept."" Analysis requires translation of abstract concepts into tangible concepts, then into something executable. There are too many layers of idea processing for machines that don't even know what an idea is. Until we have a fundamental structure for a thought, we can't have thinking machines.",1542222087.0
90lg,"Well first of all Business Analyst could include every possible position in informatics so your colleague's statement is rather vague. We don't have a working AGI (far from it). Yet there are some people who would say that there are indications of working AGI on the horizon, with vague statement that if future discoveries turn out to yield results on the same scale of what we have learned from recent progress (currently being discussed on this /r/) we COULD be near. So ""analysis"" will eventually change to **some** **degree** but I highly doubt that anyone within the industry should be worried.",1542304829.0
_georgesim_,No.,1542439687.0
martinky24,"No, it will not.",1542217880.0
noam_compsci,"What is a business analyst anyway? Its an undefined, generic position that can cover ANY number of things. As such, unless you have an automation for every possible job that comes up under the scope of a business analyst, its automation will be incomplete. 

Also, a lot of the data-scentric areas of business analysis are highly subjective, with little past data sets to rely on. 

I think business analyst roles are one that will be augmented by automation. It might get automated, but after our work-life times",1542218295.0
wolfpack_charlie,"Their claim that deep learning is the ticket to AGI is based on ""rapid progress made in the past 6 years,"" but the examples they gave are... not convincing. 

* better score on ImageNet
* better score in autotranslation
* GANs exist 
* RNNs can play video games (sort of)

These are all very exciting results in their own rights, but they are also all highly specialized tasks. None of these show potential for generalized intelligence, especially in the near term. 

I think hype like this is detrimental to progress in Machine Learning research, and part of the reason why there have been 'AI winters' in the past. ",1542216154.0
tobascodagama,OpenAI founder seeks venture capital.,1542217970.0
JTyoP,I'm not convinced ,1542212305.0
Blue_Q,"I work  with both classical AI and ""Neo AI"" (Referring to multilinear algebra based AI developed in the past years). I know why he is saying it and I would probably say the same on that stage. I do however don't personally think it AGI is a thing in the next 20 years. ",1542224607.0
orig_ardera,nah it's not,1542214118.0
Spearovisi,Keep that hype train churning!! Winter is coming lol,1542217464.0
tjscollins,AI researchers have been over promising for decades. ,1542215376.0
drakinosh,">medium.com

Pass.",1542216263.0
pilibitti,[well...](https://www.youtube.com/watch?v=Z0YIJQ1jgEI),1542228482.0
null000,"> AGI has the potential to meet basic human needs globally, end poverty, cure diseases, extend life, and even mitigate climate change. In short, AGI is the tech that could not only save the world, but build a utopia.

\[citation needed\]

Seriously though, way more likely that we use it to further concentrate the wealth with whatever company develops one first, assuming its anything other than some distant pipe dream (which I'm not) or that it's as meaningful a technology as optimists assume (which I'm also not, considering the problems we have with an over abundance non-artificial general intelligence and the massive pile of resources needed to make and run an individual instance of any hypothetical agi)

Edit: for those who have been questioning skeptics, the main reason *I'm* not convinced is that there's still a gaping chasm in ai that hasn't been solved yet. Nobody's got a good answer for generalizing ai - all of the things mentioned in the article are different uses of classifiers and generators - each require ridiculous amounts of training data, as well as major up front and ongoing computational power. 

There's been some progress on making classifiers learn faster or in real time, but none on getting something that generalizes learned knowledge or can do anything beyond the narrow task it was trained to do. IOW, we've made zero progress on the ""general"" part of agi, even if we've got a lot of intelligence to throw around",1542240762.0
powerofshower,Snake oil.,1542232890.0
thosakwe,"Obviously a guy who founded an AI group and is seeking funding is gonna say something like this.

I honestly don’t think deep learning + stats is going to lead to much more progress in AI. It’s not scalable, and doesn’t generalize to other problems well, if at all. 

",1542222224.0
onyxleopard,"> Sutskever told the audience one of the main reasons short-term AGI is possibile is Deep Learning, the powerful AI technique that has “repeatedly and rapidly broken through ‘insurmountable’ barriers.”

Then they list a bunch of specific tasks that have been improved upon...

How is improving on specific tasks in any way progress towards AGI?  I'm seriously not making the connection here.  Doing a bunch of specific things well independently doesn't necessarily bring you any closer to a generalized solution.  I can put a bunch of different tools in my toolbox, but unless I know which tool to use on which problem, those tools are not very useful.

How do we know when a system has achieved AGI?  Before we formally define AGI and objective measures of its performance, I think it's a wee bit too soon to start saying that it is coming soon.",1542229275.0
Tpm248167,">It’s believed AGI has the potential to meet basic human needs globally, end poverty, cure diseases, extend life, and even mitigate climate change.",1542218178.0
Capitalist_P-I-G,"Better build a temporary body for them, then, too.",1542219192.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1542209717.0
GodHatesJavascript,This is not the place to post job listings. ,1542166475.0
TaXxER,"Well, personally I would not be happy with websites being able to use 100% of my CPU.",1542191490.0
Xenoprimate,"I imagine the future for almost all languages will be multithreaded eventually. If Moore's Law really is coming to an end and we're forced to turn to increased core counts for improved performance, anyway.",1542174789.0
olliej,"See also https://webkit.org/blog/7846/concurrent-javascript-it-can-work/ from quite a while ago.

The real problem is how you keep all existing content working given the general assumption of single-execution-thread that pervades everything. Historically we've had issues with ordering of events because of questionable assumptions about ordering (mostly these have now been made super well defined)",1542175803.0
ILikeTypes,"There already exists a polyfil implementation of threads in JS: https://arxiv.org/abs/1802.02974

Tech demo: https://www.stopify.org/",1542180033.0
jesseschalken,"The JavaScript approach to using multiple cores has always been to have multiple threads/processes (Node.js processes, Web Workers etc) and communicate by message passing. This isn't changing, and it is fundamentally different to the shared state+locks model you find in C/C++/C#/Java.",1542203678.0
parski,Hopefully it's unthreaded.,1542232622.0
maheshhegde,"I wish JavaScript on client side comes to an end, but sure it won't.",1543587559.0
combinatorylogic,Expecting webshits to write thread-safe code? Impossible.,1542188575.0
sailorcire,"What have you tried?

Generally we won't give homework answers, but tell us what you've tried first, what you expect, and what you got ",1542148024.0
_-Notorious-_,"[https://pastebin.com/G4acDkz3](https://pastebin.com/G4acDkz3)

Added comments to your code.

I think you can just ignore variable d, your professor probably just meant that p,n,z need to be initialized to some value (zero in this case).",1542595879.0
scotty_p76,"Is d being initialized in the scanf function? I have limited experience with C, but that’s what it looks like to me. The user sets d by input and it propagates down the line (or up the line). Then other variables can be mutated. 

Although, I think the code is very incomplete; syntactically and fundamentally.  As I guess it should be. 

In fact looking through it again, I see errors all through it. It’s a mine field of syntactical errors. 
 Good luck my friend. ",1542156692.0
YannZed,"Well if you want mathematical exactness, with the information you provided, the runtime would be `r = n * t / m`, which is quite similar to what you found by running simulations, just with a different constant factor.

However this won't generalise when the input gets large, because there isn't enough information in that toy problem, and you are assuming you are dealing with a perfect computer and perfect input, which is ready instantly when a thread finishes. 

This won't be the case and if you want to generalise for large input, you will have to take into account utilisation and interval time, as your different threads will not all always be running at 100%, and also not always processing input as soon as it's done. The thread might have to wait some time for the input to even be ready. Once you have that statistical information you can use the formulas provided in those lecture slides and get a more accurate model. ",1542145497.0
jourmungandr,"The decision version of this problem is NP-complete. That is: is the a schedule that complete the job in time < k. It's called simply multiprocessor scheduling. There is a simple greedy approximation algorithm that'll give you a schedule that is at most 2x the optimal makespan. 

That is unless I missed something in the problem specification since I'm kinda tired. ",1542162620.0
sinrin,Are you asking for more information about adversarial search in a stochastic environment or backgammon?,1542137742.0
freddycheeba,The course material is a good place to start,1542120946.0
flexibeast,"Full abstract:

> Our main models of computation (the Turing Machine and the RAM) make fundamental assumptions about which primitive operations are realizable. The consensus is that these include logical operations like conjunction, disjunction and negation, as well as reading and writing to memory locations. This perspective conforms to a macro-level view of physics and indeed these operations are realizable using macro-level devices involving thousands of electrons. This point of view is however incompatible with quantum mechanics, or even elementary thermodynamics, as both imply that information is a conserved quantity of physical processes, and hence of primitive computational operations.
> Our aim is to re-develop foundational computational models that embraces the principle of conservation of information. We first define what conservation of information means in a computational setting. We emphasize that computations must be reversible transformations on data. One can think of data as modeled using topological spaces and programs as modeled by reversible deformations. We illustrate this idea using three notions of data. The first assumes unstructured finite data, i.e., discrete topological spaces. The corresponding notion of reversible computation is that of permutations. We then consider a structured notion of data based on the Curry-Howard correspondence; here reversible deformations, as a programming language for witnessing type isomorphisms, comes from proof terms for commutative semirings. We then ""move up a level"" to treat programs as data. The corresponding notion of reversible programs equivalences comes from the ""higher dimensional"" analog to commutative semirings: symmetric rig groupoids. The coherence laws for these are exactly the program equivalences we seek.
> We conclude with some generalizations inspired by homotopy type theory and survey directions for further research.",1542108323.0
carette,"I wonder how many of the people who upvoted this read parts of the paper?

&#x200B;

My favourite part of writing this still remains the observation that Curry-Howard is 'naive', in that 'A and A' is logically equivalent to 'A', but AxA is not equivalent to A (as types).  So logic isn't really the best place to look for physically-sound computation principles.",1542160034.0
cholz,I know some of these words!,1542151720.0
turning_tesseract,"Clicked on your given link, but the page just shows a 'loading' icon that keeps spinning, but the actual survey never loads. Please check.",1542201688.0
PrivilegedVoyager,"I think your problem is not resources/time to learn, but to get off your habits created over time and getting back to work.

I would recommend to wake up everyday at the same time (by sleeping at the same time), and then devoting the first hour trying to code from whichever place/ resource/ book you find suitable.

&#x200B;

Also read ""The power of habit"" by charles duhigg to get better understanding of where the problem lies.

&#x200B;

I am assuming you are getting anxious as the internship/placement season arrives before you graduate...

Well, in my opinion 2 months of dedicated efforts on [https://www.geeksforgeeks.org/](https://www.geeksforgeeks.org/) can get you an entry level job.",1542119448.0
turning_tesseract,"Since you are into video games, maybe you can try to get back your passion for programming through games. 

Start a course or book on game programming, and design and then program your own game. That should help you get back into the groove of programming.

Here are a couple of free courses on MIT OCW on building games. They include video lectures too. Check them out.

a.  [6.073 - Creating Video Games](https://ocw.mit.edu/courses/comparative-media-studies-writing/cms-611j-creating-video-games-fall-2014/)

b.  [Learn to Build Your Own Videogame with the Unity Game Engine and Microsoft Kinect](https://ocw.mit.edu/resources/res-3-003-learn-to-build-your-own-videogame-with-the-unity-game-engine-and-microsoft-kinect-january-iap-2017/)

Good luck!",1542202451.0
furbyhater,"Explore a new subject that interests you through programming? For example, try to procedurally generate a maze with entrance, exit and one treasure. Then program an AI that only sees 4 squares around itself (""fog of war"") to get the treasure and exit the maze in as few moves as possible. Or come up with some other goal of your own. The path is more important than the goal, but without a goal it's hard to advance on the path.",1542099928.0
Void_Polar,"Hey there. I think it’s great that you want to get back into programming. I’m not an expert but there are a ton of good books and online resources. I’d recommend sites like ‘Learn X in Y Minutes’ for a quick refresher on syntax. I also really liked Learn C the Hard Way by Zed Shaw, and it may help you relearn the basics. There are some really good YouTube channels that are about coding, and computers in general. I guess you should really just start somewhere.
All the best.",1542099546.0
Deskbot,"Hi lost, I'm Dad.",1542104020.0
WhoSherlock,GabeN is cruel everyone knows that. Sorry. Start with Ansi C book by Balaguruswamy. Then search for mycodeschool on YT. Its great for DS and algo. And read geeksforgeeks and code interviewbit \[If your end goal is to be placed. (Which I assume must be)\]. Everything is EZ PZ if you give tiem and effort. Delete steam and use K9 web blocker to block steam domain. Good luck,1542101633.0
goxul,"Sipser's book on Theory of Computation is great, although it does leave a bit lacking when you read it for the first time.

If you don't mind an Indian accent, I augmented it with these series of [lectures](https://www.youtube.com/playlist?list=PL3-wYxbt4yCgBHUpwXDTLos3JStccGIax) and found them really helpful. They're meant for undergrads who are taking the course for the first time.",1542085559.0
maruahm,"Always liked [*Introduction to Automata Theory, Languages, and Computation*](https://www.amazon.com/HOPCROFT-INTRO-AUTOM-THRY-LANG/dp/0321455363) by Hopcroft and Ullman as an intro text. Undergraduate-level but good treatment of TCS.

If that's too basic, I recommend [*Theory of Computation*](https://www.amazon.com/Theory-Computation-Texts-Computer-Science/dp/1846282977) by Kozen. It's roughly 1st-year graduate level, intended for those already with some background.

If *that's* too basic, for a research-level survey of TCS, take a look at Wigderson's [*Mathematics and Computation*](https://www.math.ias.edu/files/mathandcomp.pdf).",1542076496.0
erjcan,"geeksforgeeks.com  is a place where I study data structures. it has all stuff: os basics,compilers, big O, graphics,threads, lang specific stuff",1542084054.0
daxiongmao69,This channel helped me learn about different languages/grammars and automata: https://www.youtube.com/playlist?list=PLBlnK6fEyqRgp46KUv4ZY69yXmpwKOIev,1542085562.0
ottoottootto,"[Computer Science: A very short Introduction](https://global.oup.com/academic/product/computer-science-a-very-short-introduction-9780198733461)   
I really enjoyed that book. I think it's especially beneficial if you already know some concepts and want go grasp the bigger picture and what else there is to know.  
",1542093950.0
swardson,"Nothing better than the MIT courses.
https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/",1542092606.0
trolock33,"* Theory of Computation (Automata Theory) - Shai Simonson Lectures on Youtube + Peter Linz's Book.
* Compiler Design - Stanford Course on their website or IIT Kanpur Lectures on Youtube.  
",1542092000.0
rizwakhan001,"Learning computer science fundamentals and programming techniques is an exciting thing to do and what is good is the fact that there do exist many ways in which you can be able to learn computer science theory and fundamentals. 

One of the ways to learn that is simply by going to coding boot camp where you can learn the fundamentals and basics of computer science. On the other hand, you can opt to buy various computer science books to help you in the process of learning computer science. However, all these means, conjoined with online courses prove to not be so efficient in helping you to be an accomplished computer scientist. There are other better ways in which you can learn not just theory but the practical aspects of it as well. 

I believe that you can be able to learn computer science theory and practical’s the best way and that is by going to our good programming school called [Holberton School](https://www.holbertonschool.com/education#project). Our school has a good software engineering and software development program that will help you to not only learn the theory but the practical aspects of computer science like networking, programming, database administration and much more in a short period of just 2 years. Good luck. ",1542707855.0
MjrK,">For some commentators *The Structure of Scientific Revolutions* introduced a realistic humanism into the core of science, while for others the nobility of science was tarnished by Kuhn's introduction of an irrational element into the heart of its greatest achievements. 

...

>AI has undergone similar transformations since its creation in the 1950's. The emergence of data-driven approaches over the past few decades has allowed for more detailed, refined approaches for computers to complete machine learning tasks.

...

>Emphasis moved from knowledge, logic and reasoning to data, learning and statistics. Information seemed to triumph over wisdom and gratification of immediate results over epistemic truths. Success stories like the ones mentioned in the “Unreasonable Effectiveness” paper started being passed on, from teachers to students, more often than stories involving rules, theorems and logical deductions. Data became central in the new narrative, intelligent behavior was acquired by the system through automatic analysis of vast amounts of data.

...

>The stories of current AI do not involve sentient robots or Turing tests, but rather efficient processing of digital content, or the modeling of its consumers and producers. They don't even dig into details about investigation of general cognitive capabilities. Instead, they're more focused on the reproduction of very specific behaviors. This is precisely the kind of shift that Kuhn talks about and it has far reaching consequences. Success stories tend to act as templates for future research, to define for new researchers which new puzzles should be solved and how.

...

>Through each shift of a paradigm, we soak in new sources of light and adjust our vision accordingly. Only then can we understand the importance of our current paradigm in artificial intelligence. Through that, we can inch closer to the elusive objective truth as Kuhn envisioned. 

...

>Though I stare into the abyss with wonder and fascination at how AI research has evolved since it's inception, I don't look back in anger. Only the solemn hope that we're making the right decisions in our current paradigm. ",1542114340.0
RailsIsAGhetto,"""AI will kill us all.""

-Elon Musky",1542092565.0
magdalenaday,"I'm not sure we could call AI as a paradigm. Kuhn was trained as a physicist, and he used physics as a model of scientific community, normal science, etc. So, today, there are lots of new and exciting fields changing traditional disciplines.  Paradigm in Kuhn's terms was much more than a theory, or in this case a technology/field. It was about commitments, agreements between scientists, and what he lastly called ""disciplinary matrix"". 

Therefore, the first question you should ask yourself, is AI a paradigm? What was the previous one? What are the new questions and problems that it is helping to solve that the previous one didn't? Is there a concrete/distinguishable scientific community struggling to solve anomalies with new answers? ",1542110432.0
WhackAMoleE,Weak sauce. Doesn't say much at all.,1542171868.0
martinky24,"Knowing how graphics are generated, rendered, and displayed, and how that is achieved through both software and hardware has a lot of benefits. My job involves 3D graphics in the web, and while I’m not writing the code that generates them, knowing how they work is very useful. ",1542066511.0
krum,Someday you might be the only programmer in a small shop and the owner of the company comes up to you and shouts there’s a huge opportunity that’s come up and asks you if you know anything about computer graphics and all you can say is “nope!”  ,1542070101.0
Porrick,"I found it incredibly useful. It's the only specialized computer science course I took that has been useful after graduating - my first job after school was at a game physics middleware company in Dublin, and almost all the questions in the interview were intersection tests I'd only just covered in the computer graphics course. Also, I found even just learning Phong shading to be intensely interesting, never mind writing a ray tracer.

Now I work at a games developer, and aside from a few buzzwords I learned in an AI class (A-star, in particular), everything I didn't learn on the job, I learned in that one computer graphics class. ",1542068087.0
firecopy,"Computer Graphics is a complex field and is math intensive.

So learn it:

1. If you are curious about how Computer Graphics work
1. If you are up to the challenge

Checkout the [Wikipedia page on Computer Graphics](https://en.wikipedia.org/wiki/Computer_graphics_\(computer_science\)) for a brief overview.",1542465897.0
pretty_meta,"Having something to point to when you apply to a position as a ""computer graphics engineer"" is pretty useful.",1542067760.0
pala4833,"/r/humblebrag 

",1542064026.0
vomitHatSteve,"My CS program had about a 67% drop rate in the first two semesters.

That said, I think it's because most folks didn't like or want to understand SQL. If that sort of thing clicks with you, it's not gonna be as hard.",1542061414.0
zarrel40,"Depends on how much natural computer skill and problem solving you have already.

&#x200B;

Personally, I switched to CS in my second year of University (away from Mechanical Engineering because I didn't enjoy how tough it was and didn't find the theory all that interesting), and I was able to pass all of the introductory CS courses with flying colors as well as I found programming fun. So for me the first two years were extremely easy, and then got a little tougher in the 3rd when I was learning how OS's worked and getting into the more complicated algorithm math, followed by my senior project most of my 4th year of studying CS (so easy again).

&#x200B;

Now I'm making $100k and spend half the day on reddit. My job isn't too difficult day-to-day for me besides actually spending the time and effort to stare at the code base for a few hours a day and get my tasks done. But YMMV.",1542062164.0
jet_heller,"Yea. Some people are just smart and get it. Good. Now, challenge yourself. Find external projects to work on. Start helping others. Find out if any professors need any extra work done. ",1542062876.0
fnlq20,"Some things to consider:

> I've been a B+ / A student all my life and I've always had a passion for programming 

The first year is supposed to be somewhat introductory with the programming subjects, so there's likely nothing new taught to you yet. If you were a B+/A student because you diligently did your homework and prepared for class daily, you likely have the work ethic required for university. Many students fall victim of one or several of the following:

* they get good grades without studying, or only studying before tests, usually optimizing for getting good results and not a good and lasting understanding of the subject at hand. this doesn't work in university.
* they are told that they're so good with computer stuff by grandma, mom, the neighbor, their dog, but in truth they don't really have the sort of text-to-abstraction thinking that is essential to both math and compsci.
* they know the first few weeks or months of the material and get comfortable, play some video games, enjoy their new social life, and suddenly they are behind.

From what I can tell, you are 'immune' to the first two, and the third one might still be lasting. Don't fall for that one, keep challenging yourself and fully use whatever the university can provide to grow.",1542065908.0
RedditTekUser,Are they not teaching you excel and word? That is what you ll be using most of the time. ,1542061100.0
IdealImperialism,"I know what you mean OP...I'm just amazing as well, maybe even too cool for school...",1542065456.0
xyzen420,Is Freshman year easy? Yes.,1542065832.0
jmr324,You’re taking intro to programming and calculus lol just wait until you get into upper division courses. ,1542065296.0
XirAurelius,"An undergrad degree may not challenge you. Then again, you may get six weeks into your junior year when the professors expect you to start doing more than regurgitating what they tell you and hit a wall. Who can say?",1542065796.0
IdealImperialism,"I treat my computer as a magical black box, so one magical black box",1542055356.0
SanityInAnarchy,"I thought I had become jaded to this when I had to install a firmware update for my keyboard. Not one of those fancy RGB/LCD keyboards, this was a plain old Apple aluminum USB keyboard.

And then, I thought it couldn't get much more absurd when I had to reboot my monitor, because *of course* there's a computer in that.

But when I learned about the ARM chips *inside your SSD,* that still blew my mind a bit:

> the controllers for tape drives, hard drives, flash drives, or SSD drives typically all have ARM processors to run the on-disk firmware for tasks like hiding bad sectors from the operating system; these can be hacked.

On SSDs, they do a lot more than that -- they implement something that looks like a linear array of sectors on top of flash memory, which is so fundamentally different that back when the raw flash was exposed to the OS, you had flash-specific filesystems! It's a combination of wear-leveling, probably a sort of log-structured abstraction and some corresponding garbage collection to deal with...

I mean, to give you an idea: Flash cells are *written* to in 4K pages, but must first be *erased* in much larger ""erase blocks"", anywhere from 128K to 4MB. Think about what kind of crazy remapping you have to do to get decent random-write performance on hardware like that. And the OS doesn't see *any* of it -- the only thing OSes have done to support this new breed of SSDs is implement TRIM to help with the garbage collection.

I knew this was happening, but still, the idea that there's a little Turing-complete ARM chip in there that's capable of doing this fast enough for modern SSDs is amazing, and of course a huge opportunity for shenanigans if you can convince it to run your firmware instead of the manufacturer's.",1542071925.0
Enlightenment777,"**Title** should be ""How many Processors are in your Computing Device?"", instead of a silly ""click bait"" title.

In 2018, the term ""computer"" is too generic and isn't specific enough to clarify the point in a title.

EDIT: Just because an article says it one way, doesn't mean you can't reword to clarify when you post on Reddit. 
 I'm aware that some subs require titles to be the same, but this sub doesn't.",1542070066.0
,[deleted],1542060840.0
MemesEngineer,"As a computer engineering student, this is my time to shine.",1542065673.0
The_Sloth_Racer,Infinite. ,1542109756.0
maheshhegde,"there actually on an average 2.5 kernels running, according to some conference paper.",1543587667.0
istarian,"One, most likely. It does depend on how you define *computer* though.
  
After all the other devices are, by and large special (rather than general) purpose and do a predefined task on the input data. That sort of holds true even for *firmware* which is fixed and only alterable through the use of some other device and a specific mechanism.  
  
Microcontrollers can't run code from RAM.",1542083519.0
theblacklounge,"No. There is very little empirical evidence in programming language design. There's barely anything about errors/exceptions/assertions. It's all anecdotes. If you want to read those, look for ""best practices"". 

What we do know can be summarized in a small article: https://quorumlanguage.com/evidence.html  It's an ongoing effort to collect evidence for things like this. ",1542042168.0
gmfawcett,"[Hoare logic](https://en.wikipedia.org/wiki/Hoare_logic) is a method for establishing pre-and post-conditions for every code statement in a program. It is used extensively in software verification / formal methods. It is reasonable to say that Hoare logic is used because it is effective, and therefore it is a ""concrete thing"" that CS has to say about the value of making assertions.

However, Hoare logic itself doesn't make distinctions between debug vs. release: it doesn't really say anything about the deployment of software, and is only concerned with its design. Furthermore, it doesn't require that conditions are ensured by `assert` statements: they could be ensured by type-level expressions, or by  proof statements that are not embedded in the runtime representation of the code. In the extreme, one might prove an algorithm correct with Hoare logic, and then implement it in an untyped language with no assertions. Whether this is a good idea or not is a personal choice. :)

If you're writing software and want to ensure that it is correct, and the language doesn't provide any better mechanisms than `assert` for this assurance, then using `assert` throughout your code is a (partial, local, and fuzzy) application of Hoare logic in my humble opinion.",1542044629.0
1wd,"* [Evaluation of Assertion Support for the Java Programming Language, 2002](http://www.jot.fm/issues/issue_2002_08/article1/) claims ""There is some evidence, that assertion techniques, i.e., preconditions, postconditions and invariants have a positive effect on the overall software quality."" but at a quick glance I didn't find a reference for this.
* [Belief & Evidence in Empirical Software Engineering, 2016](http://cabird.com/pubs/devanbu2016evidence.pdf) states ""using asserts improves code quality (reduces defect occurrence)"" citing:

  * [What do the asserts in a unit test tell us about code quality? a study on open
source and industrial projects, 2013](https://www.researchgate.net/publication/261355925_What_Do_the_Asserts_in_a_Unit_Test_Tell_Us_about_Code_Quality_A_Study_on_Open_Source_and_Industrial_Projects)
  * [Robustness and diagnosability of oo systems designed by contracts, 2001](https://www.researchgate.net/publication/220902945_Robustness_and_Diagnosability_of_OO_Systems_Designed_by_Contracts)

* [A historical perspective on runtime assertion checking in software development, 2006](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.456.8976&rep=rep1&type=pdf) has a section ""Empirical Evaluation of Assertion  Capabilities"" citing:

  * [The Use of Self Checks and Voting in Software Error Detection: An Empirical Study , 1990](http://sunnyday.mit.edu/papers/assertions.pdf)
  * [Practical approach to programming with assertions, 1995](https://www.researchgate.net/publication/3187681_Practical_approach_to_programming_with_assertions)
  * [Two controlled experiments concerning the usefulness of assertions as a means for programming, 2002](https://ieeexplore.ieee.org/document/1167755)

(I don't know if any of these are any good. Reading your question I got curious and [searched Google Scholar for ""contracts assert quality programming language evidence""](https://scholar.google.ch/scholar?hl=en&as_sdt=0%2C5&q=contracts+assert+quality+programming+language+evidence&btnG=).)",1542047892.0
bkalle,"This entirely depends on your runtime stability and reliability requirements. In general at our company (very safety critical stuff) we do not even remove asserts and checks at release since we do not want any runtime change and are willing to sacrifice runtime performance. 
I'd say thats the most common deployment case.
But, an argument can be made to remove asserts everywhere when the only thing that counts is speed. E.g. in academic work where speed is your sales pitch of your algorithm.
Similarly to asserts, this argument also applies to using exception-wrapped accessor, e.g. .at() over [] for maps.",1542037829.0
ExcellentFunction0,"It depends, but the principle is called ""fail-fast"". By hiding an error and returning wrong results, you will propagate the incorrect values throughout the system. Later, when something goes wrong another place in the system because of this, it can be very difficult to understand what happened. That is why it is often more desirable to fail as soon as possible instead.",1542046051.0
inephable,"“CS Research” does not think about assertions. You’re looking for “Software Good Practice”

edit: /u/theblacklounge said my point but better",1542040247.0
amp180,"Design by contract, formal methods and Hoare logic are used for software where failure is perilous to human life. Eg. Plane autopilots, radiotherapy machines and insulin pumps. 

Design by contract is normally enforced by defensive programming and using assertions for debug and testing builds.

No idea if it's been empirically researched though.",1542052322.0
KidDaedalus,Check out this paper from Microsoft Research: [Assessing the Relationship between Software Assertions and Code Quality: An Empirical Investigation](https://www.microsoft.com/en-us/research/publication/assessing-the-relationship-between-software-assertions-and-code-qualityan-empirical-investigation/),1542054295.0
deong,"It's going to be a case-by-case argument really. But most of the time, if a program is worth running, it should be producing something that is going to be used. And you'd very often prefer a loud obnoxious signal that something failed over a silent error that causes to you to make potentially catastrophic decisions based on incorrect data.

Here's an example of the kind things we're working on in my job this week. We're building some reporting and analytics around a particular type of bad behavior by our employees in the field. We're going to start firing people based on these reports. If the software blows up one day, we'll go fix it. Worst case scenario, we're down a few days and send out a field communication announcing that the reports will be back online in a few days. If we fail silently, people lose their livelihoods. 

I don't need research to guide my decision there. Common sense suffices. And I say this as someone with a CS PhD who spent 15 or so years doing academic research. I understand the value of research. This isn't an area where it's needed. That's not to say there isn't room for good research on how these practices affect defect rates in the field or whatever. I'm just pointing out why it may not be a high priority for the research community to try and answer your specific question. The specific question is very often just a no-brainer.",1542055337.0
metaphorm,this is a Software Engineering concern and Computer Science is unlikely to have any research on this topic. ,1542055786.0
gabriel-et-al,"Yes, your language's type system. Languages with rich type systems can prove (assert) a lot of things in compile time.

Look at F#'s units of measure. The language asserts that you never pass 10cm as parameter to a function expecting 10hz, while other languages (like C for example) will happilly accept it.

People often say that if a Haskell program compiles then it's correct. Of course it's not really true, but the thing is that Haskell's nice type system provides lots of informations about your code in compile time and thus it notices many bugs that would only be catch at runtime in languages like Ruby or JavaScript.

Do not underestimate ~~my power~~ the benefits of a good type system.
",1542049300.0
shponglespore,"This depends a lot on what you mean by ""assertions"". If you mean runtime checks in development/test code that are compiled out in production, I don't think you need any research to demonstrate their value, because everyone who has done any nontrivial software development knows from personal experience that checks like that are a great way to catch logic errors and speed up development.

The real question is how much error checking should remain in production code. Checks like that are sometimes called ""assertions"", but IMHO for the sake of clarity it's better to use terms like ""error checking"" or ""validation"" when you're talking about logic that's not meant to be compiled out.

I tend to agree with the view (already expressed in this thread) that the question of how much validation should happen is entirely a matter of the requirements for a particular program.

If you're writing the back-end for a web service that's going to be actively monitored and has multiple load-balanced instances, you'll usually want the process to crash at the first sign of trouble because the impact of a single request failing is minimal, whereas allowing an errant process to commit corrupted data to persistent storage can create big problems. Plus crashing the process with a core dump brings the underlying problem to the attention of the developers and gives them a lot of information they can use to diagnose it.

If, OTOH, you're writing a game, you probably want the program to keep running no matter what. Crashing is *almost* the worst thing a game can do, so it's usually better to keep running even if a problem is detected, because it may not be something the user will notice or care about, and if it is, the user can decide on their own to restart the program. Writing corrupted data to persistent storage is still potentially an issue, but it tends to be a very small part of what games do, and as long as an old saved game isn't overwritten by a corrupted one, it's pretty easy to for the user to recover from a good state.",1542060728.0
ReedOei,"You may want to post this to r/ProgrammingLanguages as well, it’s sort of their thing.",1542041940.0
TheLoneDonut,"This is pretty awesome, great work",1542036750.0
Gavcradd,"Hey - this looks great. I teach Computer Science in the UK and am always looking for ways to introduce younger students to textual programming. We currently use Python, but there are too many things that I have to say ""err, Python does this differently to other languages...."" to make it really useful as a teaching tool.

This looks good though. A couple of things that I can't see on initial use - for loops? Not included? passing parameters into procedures/functions, or is it just all passed by global variables?

If you're capable of doing this though and want to make a REAL name for yourself in the UK, we desperately need an easy to use high level language for teaching. I can tell you more if you're interested. ",1542053606.0
chuk155,"I love how much this feels like TI-Basic, which is what I started programming with. And it has functions! not goto/lables, so any new programmer won't have to figure that out.",1542063144.0
mrexodia,"For most of your terms the answer is: it depends.

Rotating a phone might cause an interrupt, but it depends on the sensor. You could also poll the sensor data in a loop.

Pressing ctrl+c might cause a trap, but it depends on the situation (for example in-game nothing will happen, but in a terminal the os makes sure it causes a trap.

A system call is generally issued by executing an instruction that generates a trap (software interrupt).

Plugging in a charger might cause an interrupt (again depends on the hardware).

Terminating a program is unclear. The program usually signals to the os it wants to be stopped (for example through a syscall), but the termination itself isn’t a trap or interrupt.

—

Memory reads to privileged memory regions is a privileged operation.

Causing an interrupt (hardware) is not really privileged but you cannot cause a hardware interrupt from software so it doesn’t matter if it’s privileged. Causing a software interrupt (trap) could be unprivileged, but there are privileged instructions that cause a trap.

Reading from disk is unclear. You can write user mode software that reads a file, but you cannot write unprivileged software to talk to the disk controller. Same with a cd rom/network connection.

Clearing memory has no meaning. Memory is only cleared by taking away the power.

Termination of a process is unclear. You can usually terminate your own process, but you cannot terminate system processes.

My recommendation is to read the material provided to see how the terms are defined and just answer what your professor wants to hear. In reality you need more context to determine the answers to these questions.",1542015369.0
soluko,">  an action sends a trap or an interrupt, 

Traps are caused by software, interrupts are caused by hardware. 

>- Are these trap or interrupt; Rotating mobile phone, 

Your phone's motion sensor is hardware

> pressing ctrl+C

Your keyboard is hardware

> issuing a system call

Your application is software

>  Which of these are privileged; Memory access, causing an interrupt, reading from the disk, writing data to CDROM, opening a network connection, clearing memory, terminating a process 

Which of these can your program do by itself (or via library functions) without invoking the kernel via a system call?  (It's not always obvious whether a library function is a system call or not so you might have to read some man pages.)",1542045758.0
affinitive2,Using state-of-art YOLOv3 and the darknet architecture!,1542000915.0
SkeFranc,Fantastic,1542028496.0
Menturi07,"Great tutorial! Before I begin, does this work on macOS?",1542038967.0
hextree,"What did the author say? If he didn't write a solutions manual, then you have your answer.

My guess is that there isn't one, as this book is intended as a course textbook. What's the reason that you need it?",1541985439.0
nerdshark,"They might be used for controlling CNC hardware and interfacing with DIY electronics, among other things.",1541988562.0
DementedPeople,Pretty much everything on the motherboard is parallel bus. Parallel communication busses just have a lot to deal with in terms of cross talk and attenuation that a matter of millimeters of distance for a component means the difference between good and bad motherboard design. This is also why there were some limitations with SCSI and IDE and they attempted crazy braiding patterns in an attempt to overcome those limitations. ,1541977892.0
GeoCSBI,"If you're gonna work for a company it doesn't really matter, do something that proves you're a good software developer. ",1541974689.0
CyAScott,"I’ve always enjoyed thinking about the [clique problem](https://en.m.wikipedia.org/wiki/Clique_problem), although I’m not talented enough to come up with better solutions. I’ve came across this problem in the real world 3 times. IE: we had a large set of photos with one or more people tagged in each photo. We wanted to generate a photo slide show video of the photos for each person that took a photo. The requirement was the person had to be tagged in at least 5 photos to get a video of photos they were tagged in. Since a lot of the photos are the same group of friends in each photo, it is more efficient to generate one video for multiple people. The clique problem comes into play when trying to find the smallest set of sets of photos that covers the most amount of people.",1541991536.0
hippomancy,"Hi! I’m currently teaching a class like yours, I’ll tell you what I’ve learned. Think of learning to program like learning a musical instrument, not like learning a language. If you’re memorizing things, you’re doing it wrong.

The heart of learning to program is practice, you should be writing a little code every day, solving practice problems. If you don’t remember certain expressions, that’s ok! Look them up on the Internet. The important part is the “muscle memory” for different sorts of problems.

Memorizing code is like memorizing music when learning an instrument. It’ll come in handy if you don’t have the sheet music, but it won’t make you a better player.",1541946610.0
hextree,"> writing down code that I want to memorize

Do you mean pseudo-code? You definitely should not be memorising code. Learn the paradigms and patterns instead.",1541946113.0
mojocookie,"Join a study group, and if there is no study group, find two or three friends and start one. Tutor each other on your strengths and work together on the hard bits. ",1541948448.0
AkranarKun,"Everywhere you go, you'll have people telling you that memorizing things is unnecessary and you shouldn't be doing it, but this is due to a lack of understanding of what memorization actually is. They read or hear the word ""memorize"" and immediately think of rote and shallow memorization, which is bad memorization, but by thinking about memorization like this, they're also limiting their mastery of the subject. See this [link](https://www.bestvalueschools.org/benefits-memorization-school-beyond/) for reasons why memorization is important, but the most important reason I can give for memorization is that creative problem solving requires you to draw from knowledge from multiple contexts and combine them to form a solution. If you don't already have the information stored, you won't be able to do this efficiently and you definitely won't be able to take advantage of your brain's ability to do this process unconsciously while you're doing other things during the day.

For your case, using flash cards isn't the problem. In fact, it's actually much better than just reviewing your notes. See active recall versus passive recall strategies [here](https://getatomi.com/staffroom/what-is-active-recall-and-how-effective-is-it/).

The problem is that you're not prioritizing the right information to memorize, so you're doing shallow memorization. You're probably focusing too much on the words and text. But words and text by themselves are only sign posts pointing you to meaning and a particular understanding of that meaning. Memorizing words and text also means the data you have is uncompressed, which further exacerbates your problem. Memorize the meaning, ditch the sign posts. Remember that [scene](https://www.youtube.com/watch?annotation_id=annotation_207297&feature=iv&src_vid=sDW6vkuqGLg&v=LH1GFaw09hk) in ""Enter the Dragon"" where Bruce Lee was telling you that if you're so focused on the finger pointing at the moon, you'll miss the actual moon and all it's heavenly glory.

If you're creating the flashcards by hand, that's also a reason you're taking so long. Writing things out by hand will help you memorize the information while making the cards, due to the levels of processing cognitive bias, but it takes too long and there are other ways to improve the initial memorization. In fact, I would just cut out hand writing in general.

You're also probably reviewing them more often than you need to. Look up the spacing effect and use software that can schedule your reviews for you.

As a quick example, I have the binary search algorithm memorized. Not because I'll ever need to write it out, but because having it gives me an understanding of how half-open intervals can be used for divide and conquer array problems. I also don't memorize the actual code, and the only pieces of information I manage are:

1. Use the half-open interval \[begin, end) to track which range of the array I haven't eliminated yet.
2. Compare the target with the element at mid = (begin + end)/2.
3. When I update ""end"", set it to ""mid"".
4. When I update ""begin"", set it to ""mid + 1"" to avoid infinite loops.

Everything else I can reconstruct from those 4 elements and even (2) and (3) I don't really memorize, I only wrote those out so that (4) makes sense to you. So really I only hold two pieces of information. Also, it seems like a lot when I type it out like that, but like I mentioned before, words are only sign posts and I don't memorize the words. In my head, they're stored as 4 (or 2) elements of information, rather than however many characters or letters it takes to type them out.",1541966082.0
DementedPeople,Learning is about engagement in the curriculum. So play with the problems at the end of each chapter and try to tweak the objective. Coming up with your own problems and failing also helps you to later know what isn't possible so you also don't use wishful thinking as the foundation of your algorithms. You'll also come across a lot of design patterns and catch on to common ways people approach software engineering problems. ,1541947391.0
washeduplol,"A lot of folks have shared some awesome tips, but one thing I haven’t seen someone bring up yet is office hours. Both the TA’s and Professors are incredibly helpful during office hours if you are struggling on a particular concept. I’ve saved myself lots of time by visiting office hours rather than just scratching my head.",1541954126.0
Nike_Zoldyck,"1. Don't study one topic at a stretch. You will saturate your mental capacity for it and forget things.

2 The way memory works, is that you remember stuff from the last time you tried to remember the information. Not the first time you ever read it. So reviewing it immediately to see i you understood the content or trying it the next day will allow you to retain maximum information and remember the concept rather than the words

3) Split your day into hours for multiple courses and do that daily while having time for other activities to help you cool off and follow other passions. Try studying fro 25 minutes and take a 5 minute break or 50 minutes and then a 10 minute break. This way when you resume studying the same topic again at the same time next day, you'll review what you did the previous day.

4) While making notes don't just write stuff down in paragraphs. Try encoding the information and storing it in a different way, like a mind map or an image. Use different color pens. People are receptive to different kinds of information. Some learn better through visual info( watching videos), listening(audio books), reading(books) or writing stuff(taking notes). Try combining multiple modes and see what works. Can't go wrong with videos or in class lectures because you'll do all 4.

5) Try using github to upload your material and codes so you can access them anywhere. Don't write code down, type it and execute it. More often than not, you'll run into error and trying to fix these will help you gain a better understanding of what the code is supposed to do. It will also help you type faster and become a better coder . You can save time by using copy paste for redundant stuff instead of writing .

6) Review everything at the end of the week, again. Then once a month

7) Try writing blogs for popular topics you've come across in an attempt to explain it to laymen or a five year old. That will help you a lot. Flashcards are a waste of time. You won't gain anything by short term memorization. instead form a long term association with it.

8) Get into a better study location without distractions. Don't study in the same place where you eat, sleep or play games. Go to a library or a separate room where you always study.

9) Make a to-do list each day and strike out the things you've finished.(Can't tell you how good this feels)

10) Don't go all out for every subject. Not everything is useful in life and not everything is a priority. Figure out what is necessary and spend effort on those that will help your career. Rest is just noise.

11) Practice coding 2-3 problems a day for an hour and look for better ways to solve it and store all that in a github repo

12) purchase few online courses and do them. You can get most of them for 10$ on udemy. They have much better content and streamlined process with code to explain everything better. You can afford it.

13) Try forming study groups with helpful people and discuss ideas or explain stuff to each other.

&#x200B;

Come up with more of these based on your personal experience and adapt. Understand that everyone regrets not spending enough time on few things and wasting it doing other stuff. This means, that there was always time to fix things , so if you don't have time right now, you must be managing it wrong. Change the way you approach things and you'll get a difference in results.",1541962572.0
LutchDand,"As others have stated, don't memorize code but learn how things work. [project euler](https://projecteuler.net) may help you to figure out how to write code. Project Euler starts out with the fizzbuzz challenge. Often used to test if someone knows how to code because you need some of the very basics of coding. For the fizzbuzz problem you need (If statements, comparing and loops)
Fizzbuzz = print out all numbers from 1 to x and if a number is dividable by 3 print fizz, dividable by five print buzz end dividable by both print fizzbuzz.

For datastructures and algorithms you only need to learn how they work. You don't need to remember all the code. Because if you know how to code and you know what steps to take to solve the for example sorting problem you can code it yourself.",1541973263.0
firecopy,"You should take written notes, so that you can better understand the fundamental concepts and having something to look back to when you forget a particular concept.

Would recommend against using note cards, especially if you are only using them to memorize syntax. The time would be better used (and is more effective) taking more notebook notes or writing actually programs

Here is what people mean when they say learn the algorithms / paradigms and patterns over memorizing code. **IMPORTANT** Many beginners misunderstand this advice, so if you take anything from this comment, this would be the topic to learn.

Instead of memorizing something like this:

    // Code taken from: https://en.wikipedia.org/wiki/Selection_sort
    /* a[0] to a[n-1] is the array to sort */
    int i,j;
    int n; // initialise to a's length
    
    /* advance the position through the entire array */
    /*   (could do j < n-1 because single element is also min element) */
    for (j = 0; j < n-1; j++)
    {
        /* find the min element in the unsorted a[j .. n-1] */
    
        /* assume the min is the first element */
        int iMin = j;
        /* test against elements after j to find the smallest */
        for (i = j+1; i < n; i++)
        {
            /* if this element is less, then it is the new minimum */
            if (a[i] < a[iMin])
            {
                /* found new minimum; remember its index */
                iMin = i;
            }
        }
    
        if (iMin != j) 
        {
            swap(a[j], a[iMin]);
        }
    }

You should understand the following:

Selection Sort:

* Input: An unsorted Array 
* Output: A sorted array (Note: In this example, I will be calling integers elements)

Algorithm:

1. Loop through the array starting at the **first** element and ending at the last element. Let the index of the **first** element be defined as ""the smallest index"".
1. When iterating, if the element at ""the smallest index"" is bigger than the current element, replace ""the smallest index"" with the index of the current element.
1. After looking at the last element, swap the **first** element with the element at ""the smallest index"".
1. If the **first** element is the last element, you are done.  Else, go back to step 1, but replace every bolded element with the element after it (Ex: First becomes second. Second becomes third .... etc .... second to last becomes last).

Edit: Didn't realize I was in the /r/compsci, so no flair, but I have a bachelors degree in Computer Science and work as as a Software Developer for a large company.",1541961056.0
davenobody,"You just asked a very important question.  I wish there was a class right up front of any degree that explains how to be successful.  As others have said, comp sci is about doing.  I think I do so well in this field because I do well with generalities and worry about the details later.  And I find this is the best way to attack writing software.

Software is similar to writing a term paper.  You start out with an outline how you are going to attack the problem.  By outline I mean I lay out the basic data structures and mechanisms that will solve the hard problems.  I leave a lot of stubbed out methods and classes along the way.  At the start I just concentrate on the stuff that are not knowns.

Once I'm comfortable I have solved the parts that I wasn't sure how to accomplish I go back and implement the easier stuff I just stubbed out.  The point t of stubbing out is I can compile the code as I go and roll through the code in a debugger to be sure it really does what I want.

When I have it all compiled I go over all of it as many time as needed I until I am happy with formatting, drive out magic numbers, eliminate any awkward logic etc.

Also, you can use unit testing frameworks to stand up your solution in pieces and verify functionality individually.

This is all stuff I do in my every day job.",1541961703.0
versaceblues,"1.) The act of making the flash cards and writing notes helps to solidify the knowledge in your head. So just by making these things you are studying. 

2.) Very rarely in CompSci do we need to memorize things. You learn concepts, patterns, and ideas. Then you practice applying them. 

3.) Create dedicated times for study, dedicated times for relaxation, and dedicated times for sleep. Even when you have finished your study session, your subconscious is working to assimilate the things you just tried to learn. 

4.) The best way to learn anything is to develop a genuine sense of curiosity. Try things, fail, try again, investigate details because it excites you in that moment. Not just because the professor assigenes it",1541976697.0
Santamierdadelamierd,You need allocate Chuncks of 3 to 4 hours where there is no or little possibility for disruption. That time should end maybe at least two hours before you go to bed. That's just how I do it and it took me a long time to find out this is what works best for me. ,1541986867.0
dankmeems,If you haven't already go do a bunch of project euler problems. If you can find one that matches up with the content you learned that week even better.,1541988122.0
TKAAZ,">The only problem is, at the end of every week, even though I'm putting in the effort, I don't actually get a chance to study because I spent all week making the flashcards, doing assignments, and writing notes.

I am not sure that I understand the problem. How about dropping the flashcards in favor of actually studying?

&#x200B;",1541946093.0
got_succulents,There are much more ideal subreddits to help with your question... 👌,1541951983.0
undefined_fox,"(applies to CUDA only) If you're using shared memory, allocating more memory (or using more registers) per block reduces the number of concurrent blocks that can run. Might also have to do with memory access patterns. Try profiling your application with NVIDIAs visual profiler - it shows the causes of bottlenecks in your code. 

Though in general the speedup curve for parallel code should not be expected to be smooth unless you have infinite parallelism - for example with less than 1000 rows it might be possible to assign one thread per row, all executing in parallel. However as the number of rows gets larger some of the rows will need to be queued and executed serially after other blocks, as practically there are a finite number of CUDA cores / multiprocessors - so time will basically scale linearly once you've reached a certain number of rows.

(I am definitely not an expert, feel free to correct me on anything)",1541951174.0
sjh919,Ok? What does that have to do with CS lol,1541899719.0
Suspicious_Code,"Design is just another facet of product planning.  So, the answer to your question is simply ""do it whenever you would normally do your other product planning"".  Do your user stories and task descriptions get fleshed out in the same sprint as development?  Hopefully not!  Nor should the design work be.  A sprint or two ahead of time is fine, but I also don't really see any problem with doing it way ahead of time either, just as I don't see similar issues with fleshing out task descriptions and user stories way ahead of time.  If they end up needing to change down the road due to changing business requirements, that's fine.  It's no more wasteful than writing code today that ends up being removed or changed a month from now (and this happens quite a lot - that's just the nature of evolving software).",1541888444.0
mcdowellag,"I think the practical success of Agile relies on the use of pre-existing frameworks, such as Spring and webservers/webservices in general, to reduce the need for design, especially architectural design. If you have a project which fits neatly into one of these frameworks, and especially if you have a lot of previous experience with these frameworks, a lot of the questions which would be high stakes long timescale design decisions if you were starting from scratch are answered by ""this is how the framework does it"" or ""this is the way we always do it.""",1541925749.0
firecopy,"Here is my analysis of each of your points.

> Doing design as part of the development sprint

Design is already done within a ""Sprint Iteration"" (I am taking a guess that is your intention when you say development sprint). Remember, doing design is not the same as ""Sprint Planning"", which is about assigning points/hours to tasks and putting them in the upcoming sprint. Edit: Also designers should not be the only one working on user stories. The entire development team works on user stories in Scrum.

> Designers have separate sprints parallel to the development sprints

Same advice as above, but I can start seeing where your misunderstanding comes from. Both designers and developers fall under the development team, in the Scrum framework.

Another role that would fall under this team are:

* Database Administrators

> Designers have separate sprints ahead of the development sprint

Scrum allows for this suggestion already. Note: Trying to separate designers from the development team (or making another team just for designers) is similar to how projects were made in Waterfall (Where individual ""knowledge"" silos (developers, designers, testers, etc.) would pass information to each other).

> All design is done up front before development

That doesn't make sense, if you are following the Scrum framework. At the end of every Sprint iteration you want to have a ""shippable"" MVP product. For software projects, a design is not a shippable product. This “All design is done up front” suggestion is Waterfall.

To summarize, Scrum dictates the overall process on how to manage a project with multiple team members (It doesn't tell developers how to program, or designers how to design, etc.), and is why you aren't finding research on those topics. If you have any more questions, definitely leave a reply to this comment.",1541963587.0
feherfarkas,"Do you have Time Machine enabled? It makes local snapshots of files as they change. Log files and stuff may change even if you don’t do anything. Local snapshots show up as used disk space but they get deleted if you need disk space. 

Look at this and see if it helps: https://discussions.apple.com/thread/8343991",1541884396.0
dewise,"Not authenticating user would be most secure option. Very inconvenient for user, though. ",1541873379.0
tejoka,"You probably want to read through something like this: https://pages.nist.gov/800-63-3/sp800-63b.html

But in any case, the state-of-the-art is username/password combined with a hardware second factor: a Yubikey or other U2F / FIDO2 device.",1541878267.0
Suspicious_Code,"If we consider authentication on a confidence scale, from 0% (meaning we know for absolute certain that the user is not who they claim to be) to 100% (we know for absolute certain that the user _is_ who they claim to be), then several things are true:

1. Neither 0% nor 100% are actually attainable, even theoretically speaking
2. Adding more layers to your process to raise your confidence level can be done _infinitely_, at the cost of usability
3. The point at which you are ""confident enough"" (90%? 95%? 99.999...%?) in the authentication is entirely subjective

Given these things, the only literal answer to your question of ""what's the most secure authentication process, ignoring usability"" is to use every conceivable layer, factor, proof, test, etc. in an infinite authentication process that can never actually result in a successful authentication, because infinity.

If you want a more helpful/useful answer, you simply can't ignore usability.  It's critical to the process.  Modern authentication is premised around finding an appropriate balance between confidence and usability for your specific context, so you can't just ignore one piece of the two-piece puzzle or you'll never arrive at anything that can actually be of any use to you.",1541875605.0
Maristic,"This is great, but we should also consider n-y twees, pokable threes, fourth-order search trays, 3-4 treats, skipable frees, B-beas, and back-ordered blue–green trims, and nice tries.  Don’t forget streets, or entrees, and tees. 

  

^(I made these up; but someday I bet one of them will be real.)",1541866709.0
thecrazypriest95,What the what?,1541867794.0
morth,"A nice advantage of trees is that no hash function is needed. I've honestly never been a fan of hash tables, well except generated perfect hashes at least, so it's nice to see good alternatives. ",1541879036.0
sjh919,"Haven't looked at the algorithm, but maybe base them all originally off of their Rating, then if 2 shows/movies/whatever are within some threshold difference (like 0.2 or something) *then* base it off of viewer count?",1541838382.0
RichardMau5,"Does chaging Q not help you?
Maybe first draw out a list of ordered pairs what you consider correct ordering and try and play a bit.
Ask yourself, how many more votes does a lower ranked review need to appear higher?",1541838467.0
OkinawanSnorkel,"Do you think that a supervised machine learning approach could be useful for this problem? Specifically you can try something like a linear regression on the ratings, number of votes, and number of views to generate an aggregate weighted score that would more closely align with what you'd call ""important"" or ""interesting"". ",1541838206.0
maweki,"Another approach would be to fill up the smaller ratings with random ratings. You know what the average movie score is. As long as very few people have rated it, there is no reason to believe that the movie is anything other than average.

Or if you know what the standard deviation for ratings (for similar movies?) is, you could cast other phantom votes with the same average and the standard deviation from somewhere else. Especially for title that have a rating near the maximum this would give you a better idea if you believe the movie is actually an undiscovered gem.",1541862431.0
spectrogramaniac,Try /r/techsupport,1541831692.0
zer0_g_,"Try booting from a Linux USB and run photorec (actually recovers a whole lot more than just photos!)
If you performed a quick format there’s a good chance most of the files are there",1541853455.0
Techno_Jargon,Just sneak in and pretend you belong.,1541815707.0
kcdragon,"How many places have you applied to? When I was looking for an internship in college, I applied to about 20 places, had 5 to 10 interviews and got a couple offers. Sometimes you just need to cast a wide net since there isn't a lot to differentiate you from others who are applying. 

Another tip is to apply to places that have a lot of openings rather than a place with just a couple. The place I interned at hired at least 20 interns every semester. ",1541814846.0
OkinawanSnorkel,"It could be that it's just competetive.  Sure there are a lot of positions for software engineering interns, but among all the applicants how do you stand out?  These companies are probably recruiting at tens or even hundreds of other universities!  The top students all know C, C++, Java, Python etc. and have fundamentals in Data Structures, Algorithms, Math, etc.  Unless you're doing something extra, it may be hard to shine through.  Sorry if that sounds harsh, but that's sorta the reality of it.  Get some projects and unique experiences on your resume if you haven't already.   Best of luck!",1541815945.0
Jack2423,Do u have anything useful you have written? Give them something to show you will be useful . Do you prep for the interviews? Know what they do and they they need? Do you have a lab at home where you can again display that your more than book smart but can apply your knowledge. ,1541809773.0
gott_modus,"Do things that you can put your name on - things that other people can vouch for. Maybe it's contract work, or an open source contribution that actually makes a difference to a project people care about.

Above all, the big take away is that you show what you are capable of providing for other people. If anyone is going to take a chance on you, they have to have some kind of inclination that the chance itself is a good bet - the only way to verify that is through work that you've done that has some significance for other people.",1541836595.0
deltrak,"My sophomore year I applied to 25 places and went to career fairs. Got 3 replies. Junior year I applied to 30 places and I got 5 interviews. All applications were tailored to the specific position.

Basically what I’m saying is there are few positions and a lot of applicants. It’s a numbers game and it takes a bit of luck. It’s also somewhat early to be getting worried. I would start to worry if you have no responses by March.

Search for yourself on google. Find what a recruiter will find from the most basic effort. What impression does your LinkedIn have on your experience? Does your linked  github repo have sloppy/empty code? Make yourself attractive 

Goodluck ",1541859327.0
Bayleafqween,"Have you gotten your resume reviewed anywhere? If you can, I would recommend posting it here with personal info blurred out",1541866370.0
RailsIsAGhetto,"The biggest reason is probably because there are simply so many computer science students and software developers in the country now. For various reasons, mostly financial, it all became very popular in the past five years. Even if you are good, you have to apply to a lot of places to get seen through the crowd.

",1541877534.0
Carpetfizz,you are <3,1541797818.0
someredditaccount2,"Not exactly what you're look for, but I think the cutest Turing machine is this one: [https://www.youtube.com/watch?v=E3keLeMwfHY](https://www.youtube.com/watch?v=E3keLeMwfHY)",1541808746.0
Josuah,Something that comes close and satisfies the cute requirement: [Human Resource Machine](https://tomorrowcorporation.com/humanresourcemachine). A sequel with parallel computing is [7 Billion Humans](https://tomorrowcorporation.com/7billionhumans).,1541806731.0
chazzeromus,there's this game thing google did for Alan Turing's birthday https://www.google.com/doodles/alan-turings-100th-birthday,1541806581.0
taqueria_on_the_moon,It's name is CHRISTOPHER! -Alan Turing,1541801700.0
pgboz,"I made Turing-complete machine that's also a game called Turing Tumble. It's intended to be tangible and easy to understand. It's not a traditional Turing machine with tape, but it also doesn't require any electronics to run. You can see it here: [www.turingtumble.com](https://www.turingtumble.com).",1541828173.0
Jason-Hu,"The one simulated within Game of Life, the most famous cellular automaton.

The rule is simple: imagine an infinite plane of grids, each grid is called a cell and is either dead or alife. For the next round, the situation of each grid is determined by the following rules:

(I'm citing Wikipedia here: [https://en.wikipedia.org/wiki/Conway%27s\_Game\_of\_Life](https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life))

1. Any live cell with fewer than two live neighbors dies, as if by underpopulation.
2. Any live cell with two or three live neighbors lives on to the next generation.
3. Any live cell with more than three live neighbors dies, as if by overpopulation.
4. Any dead cell with exactly three live neighbors becomes a live cell, as if by reproduction

Some interesting patterns could be drawn:

like this: [https://upload.wikimedia.org/wikipedia/commons/f/f2/Game\_of\_life\_animated\_glider.gif](https://upload.wikimedia.org/wikipedia/commons/f/f2/Game_of_life_animated_glider.gif)

even this: [https://en.wikipedia.org/wiki/File:Gospers\_glider\_gun.gif](https://en.wikipedia.org/wiki/File:Gospers_glider_gun.gif)

&#x200B;

So, back to the topic. Someone has used this concept to build a functioning computer with von Neumann structure: 

[https://www.ics.uci.edu/\~welling/teaching/271fall09/Turing-Machine-Life.pdf](https://www.ics.uci.edu/~welling/teaching/271fall09/Turing-Machine-Life.pdf)

it looks like this: 

[https://imgur.com/vNgoY0l](https://imgur.com/vNgoY0l)

&#x200B;",1541820333.0
zoba,Don’t forget /r/turingmachines ! :),1541836763.0
HalfAkbps,JFLAP,1541810553.0
heimlichmaneuverer,https://www.turingmachinesimulator.com/ is pretty neat,1541887161.0
hsxp,[http://hgentry.github.io/turing/](http://hgentry.github.io/turing/),1541809800.0
LordeLordeYaYaYa,Simsimi,1541801340.0
drakinosh,Fuck off. All of you are so goddamn fake.,1541853587.0
DC-3,"I thought that most of the stats about women's role in early computing were somewhat misleading as at that time, 'programmers' were those who prepared punch cards or operated switchboards, and it tended to be mathematicians (predominantly men) who were performing the role we would today refer to as 'programming'.",1541769683.0
chadwickofwv,A large portion of this article is feminist clap-trap.  It starts out in reality then nose-dives into feminist drivel around the middle.,1541775364.0
spinwizard69,"Well back in the day most of the females in my comp-sci one course never continued on into the rest of the program.   It wasn't a matter of intelligence from what I could tell.   Sadly I didn't ask (working full time ruled out a social life).

The real problem with woman these days is their rather stupid behavior in social settings.   The linked article has comments about woman being preyed upon but lets be honest if that wasn't happening those very same woman would be claiming that every one there was gay, boring or whatever insult of the moment fit.   After 58 years of life I've seen too much of this sort of hot/cold nonsense.   Frankly keeping woman out of tech has probably help the industry more that it has hurt it.   ",1541771792.0
TheHowe,"When I was first starting out in software engineering, I believed I should wrap all dependencies in wrapper classes implementing interfaces I designed based on my needs, so that when inevitably that dependency had an API change, I could swap it out easily, hidden by the interface API.

Then when I had dependencies change their API, I realized my interfaces were subtly tied to patterns in the old API, and I had to change all my code anyways.

&#x200B;

As a more mature software developer, I typically don't bother wrapping an external dependency in a custom interface (unless I would truly have to touch thousands upon thousands of lines of code, and I believe an API change is actually feasible). Then, when I inevitably get bit by an unexpected API change, I figure out the proper interface that is not tied to either the old API or the new API - which is significantly easier when you can see both the old and new APIs.

&#x200B;

Perhaps someday I will be wise enough to anticipate what the wrapper interface should look like with any degree of accuracy before the wrapped API changes. But hey, I've only been at this 10 years or so, I've still got lots to learn.",1541787012.0
maruahm,"Copying my comment from /r/math, I found this after reading Terence Tao's comment (Remark 16 in [this blog post](https://terrytao.wordpress.com/2009/11/05/the-no-self-defeating-object-argument/)):

> One can use the halting theorem to exclude overly general theories for certain types of mathematical objects. For instance, one cannot hope to find an algorithm to determine the existence of smooth solutions to arbitrary nonlinear partial differential equations, because it is possible to simulate a Turing machine using the laws of classical physics, which in turn can be modeled using (a moderately complicated system of) nonlinear PDE. Instead, progress in nonlinear PDE has instead proceeded by focusing on much more specific classes of such PDE (e.g. elliptic PDE, parabolic PDE, hyperbolic PDE, gauge theories, etc.).

After following a few MathOverflow pages discussing this ""disproof"" of a general theory of PDEs, I found the paper I linked.",1541735264.0
wellnhoferia,"The site says it’s unavailable (Europe), is there a mirror?",1541761446.0
univalence,"Even more interesting, there's a reasonable notion of ""polynomial length"" ODE, and polynomial time Turing machines are exactly those which can be simulated by a polynomial length ODEs of a certain form. [Arxiv link](https://arxiv.org/abs/1609.08059)",1541783396.0
pulsar512b,What can I do with a Computer Science degree? I'm especially looking for out-of-the-box ideas.,1541732951.0
dirtyLittleMonkee,"Current CS student enrolled in Data Structures, Algorithms, and Discrete Mathematics. Wondering if anyone can make any recommendations for resources on strong induction and big-o notation. Half the class is struggling with these topics, and the teacher & text aren't particularly helpful. Thanks.",1541734863.0
bubbles_of_love,"I have no idea what I want to do for a career, or what job titles to look for when I graduate.

So far I seem to have a huge interest in classes that are math or research related, like learning about algorithms, computer vision, operating systems, and security.

Any suggestions?",1541736211.0
jacobglines,"Cyber security is going to become a new major at ISU and I'm thinking of taking it over Software engineering, any recommendations?",1541736447.0
djthewomba,I have a computer science degree and manage a team that does Lean Six Sigma Process Improvement and Robotic Process Automation / AI. Both of those would use skills from CS. I also know Project Managers that studied CS. ,1541752715.0
SCElectrons2025,"Is there anything computer architecture or OS related that a self-taught programmer without a CS degree should teach himself for interview purposes? I plan on going through and learning the subjects throughly on my own when the time comes, but I want to make sure I'm OK for interviews in the interim.",1542249386.0
pulsar512b,"What was your worst prof?      
Mine... well.... here's some copy-pasted rant.     
*long unrelated rant*     
so, what i'm getting at      
is that this new assignment     
has a shorter time frame       
is solos         
and comprises of at least the same amount of work, and potentially more     
than that group project     
so, he's expecting way more     
with less time      
(oh, and some of his instructions are faulty)      
(oh, and he didn't mention this in class)       
(oh, and today was the ""due date"" and nobody has posted anything)      
uuuugh     
this isn't even my only complaint          
1. half the time he's teaching, he's reviewing very basic shit      
giving the exact same lecture he gave last week     
and the week b4 that     
and the week b4 the week b4 last week     
and so on     
uugh       
and he is supposed to keep to the syllabus        
but never does     
and his quizzes      
HIS QUIZZES       
ONLINE QUIZZES       
FOR CHRIST SAKE     
THOSE QUIZZES      
breaks down       
first off         
80% of the questions are from random quizlets      
across the internet      
(and thats the only way to find the answers)        
and half of the shit isn't taught       
and some are flat out wrong       
uuuugh       
For context, this is a Database Intro class. The basic stuff is ERD, RDM, etc. We are closing out the semester.       The new assignment requires us to: create a design for a relatively simple project, figure out the code, which is terrible esp since the design revolves around a many-to-many relationship, oh, and get queries, which are HARD. And on top of this, implement it in Access or Microsoft SQL server. Oh, and the setup for SQL server is hell, and the instructions for access are out of date....... uugh",1541740070.0
dappersilence,Take a look at [graphviz](https://graphviz.org). I think it could probably generate something similar. ,1541720905.0
beeskness420,tikz or Ipe should do it. ,1541727271.0
nii3lsh,In python I use  networkx . I made an example here in the past : [https://github.com/nii3lsh/Barabasi-Graph-Generation/](https://github.com/nii3lsh/Barabasi-Graph-Generation/),1541960848.0
LgDog,"His answer is wrong, are you sure isn't a communication problem between you two?",1541712183.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1541706978.0
cirosantilli,And better fighter jets? :-),1541748999.0
digitotal,"Depends on the schools curriculum. Most CS majors take Calc 1, calc 2, stats for engineers, discrete math. Those are usually required. For Calculus, much algebra will be needed. Stats will be after the calculus and usually just uses this. Discrete math is another whole topic but you are usually taught with number theory, set theory and usually encryption. No needed math other than basic arithmetic in that one, but is much more difficult for some than algebra per se. If I could sum my answer to one word -> algebra. This is the thing your teachers expect you to know in any class. They will not go back and teach you simple maths. After all, computer scientists are mathematicians. ",1541701424.0
BananaHockey,"I'm currently a computer science student who is awful at math. I had to start with College Precalculus, and have to take Calculus as a co-req for any computer science courses. After that it will turn into discreet math.

Like previously mentioned, I am awful at math. If you dedicate yourself to the course and put in all the effort you can, you'll make it.",1541710402.0
markchristian33,"You know computer science came from math right? Back then there were no computer scientists, just mathematicians..  then they created computer science. So any cs degree usually takes a lot of math",1541711407.0
profhelios,"You can never learn too much math. Take as much as you can, but honestly, unless you are planning to go onto super theoretical computer science, you can probably stop after second level calculus and a course or two of statistics. I wouldn’t honestly recommend taking any less than that though, especially if you want an advanced degree.  You will of course take discrete math.

If you just want to do generic app dev, you could possibly get away with just college algebra/pre-calc and stats, but you’d be missing out actually on calculus where I found math got fun. Legitimately. I was ‘terrible’ at math as a kid because I got bored with it (and the US primary school math teachers are pretty bad) and Calculus showed me ways to quickly solve problems that took forever in previous iterations of math.",1541955510.0
pheonix940,"This depends highly on the specific models involved. Some i7s are 6 cores, some are 4 with hyperthreading. Some are 2 with hyperthreading. Some i5s are 4 cores with hyperthreading, some have no hyperthreading, some are 2 cores with hyperthreading. 

Really, you need to look at the specific model that your laptop comes with. Look it up on intel ark, or just put it into google.",1541691934.0
IdealImperialism,"It depends on the device, Intel I'm not so sure, but definitely for their Xeon/atom range.

The microarchitecture may be tuned for a mobile application and in some cases will add extensions to the ISA. This usually requires an additional compiler however.",1541696739.0
arkrish,"I believe the mobile processors change processing power to optimize battery life while this is not the case in desktop processors. (This is in commercially available SKUs, you could always custom-build a mobile machine with a desktop processor). ",1541703376.0
aebkop,Main difference is power tuning ,1541772055.0
lobomos,"That is pretty interesting, definitely makes me realize I do not miss programming in C.",1541698752.0
RailsIsAGhetto,"> Quicksort combined with Insertion sort - even faster

Even faster still, in practice, was combining insertion sort with mergesort. This tends to be called ""timsort"" after Tim Peters. Java went this this approach to simple array sorting starting with Java 7. ",1541742268.0
chkas,Just for info: the original post is [here]( https://www.reddit.com/r/programming/comments/9v8ouc/an_overview_of_sorting_algorithms_and_a_pretty/),1541748661.0
beeskness420,"A language is regular if it has all or any of those properties as they are all equivalent. 

A non-regular language is any language that is not a regular language. So they can't be recognized by a regular expression otherwise it would be recognized by a finite state machine, and then would have to be regular. 

",1541642369.0
jmite,No,1541641374.0
GiantRobotTRex,Just wait until you see how engineers write imaginary numbers,1541641013.0
beeskness420,"I always write lg() for base two, ln() for natural, log() if it's in asymptotic notation and I don't care about the base, or log_b() for a specific base. Seems like that's pretty common from almost everything I've dealt with. ",1541654675.0
future_security,"    log(x);    // base 2
    ln(x);     // base e
    log_10(x); // base 2 (binary)

But, seriously, there is very little use for log base ten compared to base 2 in computer science.",1541642476.0
csp256,"The killer feature for programming languages remains a meta one: a large, established community.",1541637090.0
beeskness420,"Pattern matching, higher order functions, lambda functions, optionally typed, Unicode support, clean iteration abstraction.... wait I think I'm mostly just describing Julia. ",1541632469.0
AnyhowStep,"Before I discovered algebraic data types: move semantics in C++.

After I discovered algebraic data types: algebraic data types.

It's allowed me to be so much more specific about the expected inputs and outputs of my functions/methods.

The closer you make your data type declaration match the actual data type you handle, the more bugs you avoid due to variables accidentally being outside the scope of your algorithms.

I have yet to find a static type system that's expressive enough for my personal needs but TypeScript comes pretty close at the moment.",1541631867.0
c3534l,"I would like it if I could write a data structure in a fairly low-level way with proper memory management, and just stick that file next to the one where I write high level code. Just throw on the top ""import LowLevelStuff"" and have syntax-level support for high-performance code without having to mess with foreign language interface crap or even having to learn a second language. Just have the one language that supports C and Rust like systems programming, and then maybe have a statically typed compiled version, and finally a built-in interpreter as the cherry on top. And if they could figure out a way so that you could write some big fancy high-performance code and then let people write an easy-to-learn scripting language to control your DAW or Game Engine or whatever big complicated compiled thing you wrote, that would be awesome.",1541644842.0
jmite,"Haskell, but total pattern matching, with proper stack traces, easy debug printing, and extensible record types.",1541641315.0
Best_Pseudonym,Pointers in c/c++ are nice but overkill for most purposes ,1541640857.0
dreymatic,Comprehensions in python are fun,1541635785.0
seanfonz,Default values for arguments.  I love Python for this but apparently C++ has this now as well?,1541641359.0
InfiniteStateMachine,I'm a big fan of discriminated unions. Unfortunately I don't tend to work in languages that have good support for them.,1541651308.0
pwab,"[homoiconicity](https://en.m.wikipedia.org/wiki/Homoiconicity) in short, the code is expressed as literal data structures. ",1541656300.0
WetSound,"LINQ in C#. It’s like being able to SQL query not only databases, but *all* data sources. And not with SQL, but something better.",1541666575.0
PM_ME_UR_OBSIDIAN,"Strong gradual typing is underrated for prototyping.

Otherwise I just want a mix of Rust and Coq.",1541644107.0
calaedo,Well I would choose constexpr (C++). One of the C++11 extensions wich saved my ass quite often from debugging.,1541641448.0
defunkydrummer,"Metaprogramming together with s-expressions and homoiconiciry. 

And it must be able to operate at compilie time, runtime, and read time (as chosen by the programmer)
",1541733757.0
eugf_,"- Underscores in numerical literals ([Java](https://docs.oracle.com/javase/8/docs/technotes/guides/language/underscores-literals.html))

- Asterisk operator ([Python](https://medium.com/understand-the-python/understanding-the-asterisk-of-python-8b9daaa4a558))

- Macros ([C](https://gcc.gnu.org/onlinedocs/cpp/Macros.html))

- Goroutine ([Go](https://gobyexample.com/goroutines))

- Monads ([Haskell](https://wiki.haskell.org/Monad))

EDIT: Formatting",1541635562.0
,"Records, algebraic data types, lambdas, higher order functions, pattern matching, immutable by default, optional parameters with default values, reflection, static typing, and.... OO.

C# and D are _almost_ there for me, and Kotlin seems like it could be similar, but I still have yet to find my perfect language.

And yes I have heard of Rust; we don't get along.",1541661770.0
themissinglint,minimal magic.,1541671562.0
Groundbreaking_Lion,Pointers in C ,1541651029.0
eleete,<?php include ('file.html'); ?>,1541632495.0
eikenberry,"My favorite feature is a lack of features. To many languages think more features == more good, when it usually just means more complexity and bloat.",1541645970.0
BushDidN0thingWr0ng,"I think if I had to make a summary, I would want a higher level Rust, but with the more familiar and common standards of classes and inheritance, a heavy influence from python, and SML style datatypes.

I also think programming languages take themselves too seriously, so I would want a large standard library that also includes some silly and ridiculous modules, such as one for *every* sort that can actually work (no quantum sort) and one that allows the use of `goto`, but also prints to the console ""this is a bad idea"".",1541651880.0
guareber,"Honestly? Top of the line, standard library debugging. So much easier working with something where debugging is a first class citizen. ",1541664337.0
acconrad,"Error and debugging messages that were written for humans, especially beginners. Elm seems to do this well.",1541686753.0
Madsy9,"For my everyday programming tasks, I would definitely say [extended static checking (ESC)](https://en.wikipedia.org/wiki/Extended\_static\_checking) with compile-time contracts.

Basically, you write contracts in form of annotations like ""requires"" and ""ensures"", where ""requires"" lets you specify invariants which must hold for inputs of a function, and ""ensures"" specifies invariants for any outputs. These contracts then get fed to a more advanced SMT solver at compile time, which again provides a low-level model sent to a SAT solver. Here is an example of these annotations in OpenJML, the successor to Java/ESC:

    public class Main {
    
        public static void main(String[] args) {
	        System.out.println(Main.add(1,2));
        }
        
        //@ requires a > 0;
        //@ requires b > 0;
        //@ ensures \result == a+b;
        public static int add(int a, int b){
            return a+b;
        }
    }

The annotations should be self-explanatory. But this class contains a bug. What does the OpenJML checker have to say about it?

    Main.java:16: warning: The prover cannot establish an assertion (ArithmeticOperationRange) in method add:  overflow in int sum
            return a+b;
                    ^
 
Aha! So a+b can still overflow provided that the values are big enough, which would break our ""ensures"" contract. We fix that with a modification of our input contracts:

    //@ requires a > 0 && a < (Integer.MAX_VALUE/2);
    //@ requires b > 0 && b < (Integer.MAX_VALUE/2);

And now it passes.

Contracts like this have been around for a long time, in languages such as Smalltalk and Eiffel, but then the contracts were turned into runtime assertions. The big deal with Extended type checking is to be able to prove that the contracts hold at build/compile time, and there is a lot of current research into this as well as SMT solvers in general. You have completely new ESC-based languages such as [Whiley](https://en.wikipedia.org/wiki/Whiley_\(programming_language\)), as well as ESC tools such as Spec# and OpenJML that extend existing languages (C#, Java).

I mention this as my ideal, but it already exist so what's up with that? Well, getting these contracts to work in practice is not that easy. First of all, anything stupid like

    //@ensures pow(a,n)+pow(b,n)==pow(c,n);

would just grind the proof solver to a halt. Did you really think it could prove fermat's last theorem automatically for you? In other words, we have to be realistic of what we can prove in a single contract, just like we can't just throw anything willy-nilly at Coq either.

Second, ESC tools and languages that support ESC out of the box can be really difficult to use as soon as you break one of your contracts, because all the function contracts depend on each other in a hierarchy and it isn't necessarily obvious where the mistake is. Sometimes it is the code, sometimes it is the specification itself! Just like an SMT solver or SAT solver, these ESC validation checkers like to give you a counter-example when it finds an error, but it's a challenge to make those counter-examples human-readable. You think C++ template errors are bad? Then you haven't seen the counter-examples the Whiley compiler gives you when one of your contracts try to prove something about an array.

Anyway, even with the technical challenges, computers are currently so fast now that we can throw SAT problems at them which were unthinkable say, 15 years ago. With increasingly better models for the type systems (collections, real numbers, concurrency-support) ESC-languages can only improve. I wait for the day where I can prove my programs correct against a specification like this, generate unit tests from the same contracts and get highly readable counter-examples.",1541719056.0
maheshhegde,"Simplicity, minimalism. Being efficient, even at the cost of abstraction is nice.",1543587932.0
bumblebritches57,"Getting the fuck outta my way and letting me do what I want.

#\#CMasterRace",1541645925.0
jkuhl_prog,"Typescript's typing system.

It's strong enough to let me know what shape my objects are supposed to be and what types my functions are using, but it's not as verbose and annoying as Java and not as loose and bug-prone as its parent JavaScript.",1541685047.0
theemptyqueue,"C/C++: I manage my own pointers, I can iterate through an array using only conditional if statements.  Structs allow me to create a variable and use it multiple times without breaking anything.

Assembly Language: great for learning how hardware processes instructions, but can be difficult to read.

Java: good libraries for making basic GUI templates, not being able to manage pointers kind of sucks.

BASIC: Great for recursive function practice, easy to read and easy to learn how languages read 

LISP: Everything is in a list, and don’t forget your parentheses.

Python: can do anything, just really badly.
",1541678349.0
gott_modus,"C++ meta programming. I can write 

###whatever the fuck I want

#how I want

and still have time for drinks at lunch",1541701511.0
MemesEngineer,CS50 Harvard online lectures.,1541614512.0
keanedawg,"I think it depends, if you're not in school and just want to learn coding, I really think codecademy is the best option. That was my first dive into coding.

If you're still in school, I would look for a basic programming course (many high schools and all colleges offer them)

If you're seriously considering a career, I would think about taking an introductory computer science course in your university (Or take CS50 from Harvard which you can complete from edX).",1541614760.0
SilasX,Question for https://www.reddit.com/r/learnprogramming,1541621107.0
KevinKelbie,"I just downloaded python and started tinkering with it in the IDE. Try looking up a tutorial on the basics either on YouTube and use [tutorialspoint](https://www.tutorialspoint.com/python3/) as a reference to look up things.

> What would be the easiest way to make an algorithm that does something if a part of the screen is a specific color?

If you are a beginner with no experience whatsoever then you might be getting ahead of yourself. Try writing a program to produce the Fibonacci sequence or something. :)",1541619523.0
tempus-temporis,"First off, computer science has to do with much more than just coding. Check out /r/learnprogramming or swing by a used book store for an introductory text. Notepad++ is as good of text editor as any. The best way to learn is through doing.",1541615348.0
JAVAOneTrick,Countless online sources you can find through google. ,1541619178.0
kwuz,"CS50 on harvard online is a good place to start.
Learning compsci is a lot more than just programming -- you'll definitely be better off if you understand the basics of how the underlying reasons computers work.
Other than that, just make sure you practice and be forgiving of yourself. It's hard to get the hang of at first. ",1541620072.0
MysteriousHo-Oh,"If you want to code, I'd recommend you to start off with a simple language(Java, or Python). To learn coding, hit the r/learnprogramming subreddit to get free online books or used books. I'd recommend any of the Head First : A Brain-Friendly Guide books as they explain the concept very well for people who haven't got much experience in coding. Any online lecture on edX would also help(such as CS50 if you want to do C++) as they are often quite good in explaination and have a rigorous testing method, but just remember that they can be challenging and frustrating at times.

&#x200B;

For your code, I'd probably recommend you to start off with something easier(maybe a program to print tables of number or it's square?) but if you do want to perform your code(again, I do not recommend this because you need to have a pretty decent knowledge on languages), I suggest you use Eclipse if you go Java(easy to use as it has several functions, excellent and robust error handling).

&#x200B;

tl;dr: Read the Head First: Brain Friendly Guide, watch edX lectures, test yourself weekly and learn to enjoy coding. Don't do programming everyday if it gets to be too much and take a more lax approach to it as programming is only as difficult as you make it out to be(I know I did C everyday and eventually I burnt out and didn't want to program for a month).

&#x200B;

I hope to see you program!

&#x200B;

Edit: Please don't use Notepad++, it's IDE is quite odd and there are tonnes of free and well made IDEs you can get for the language of your choice.",1541621958.0
pseudorandomcoder,"For starting coding, python (or another language) on a web environment would get you started right away. Like this:

[https://youtu.be/zZHSKtPVbp8](https://youtu.be/zZHSKtPVbp8)

&#x200B;

For accessing colors on the screen, then taking action. I recommend autoit scripting as an easy way to get started in that:

[https://www.autoitscript.com/site/](https://www.autoitscript.com/site/)

&#x200B;",1541615475.0
Ceberzz,The coding train has a great playlist on YouTube. He teaches in JavaScript on p5,1541622166.0
coshjollins,"As others have said CS50 lecture videos. Derek Banas is also amazing at explaining everything you need to know about a language in one video, he is very quick to get to the point. Bucky Roberts tutorials are really easy to follow and enjoyable to listen to. Because there are many different types of programmers, I would suggest trying to figure out your end goal, and looking up small example programs that might help you achieve expertise in that area. Also find a library for whatever task you're trying to accomplish, like OpenGL for graphics in many different languages.",1541622300.0
atlasffqq,"Since you mentioned a tool, I would also recommend VSCode as an editor. It's available cross-platform for free and has great support for most languages.",1541622900.0
horrofan,"&nbsp;

to answer you question the easiest way to start coding is to have a IDE which builds compiles and runs your code for you, it will help you be able to just focus on ""building sets of instrutions""   (programming)

&nbsp;

but i wouldn't just go diving into something without knowing anything about it , maybe you should develop some intution about the science (maybe what a program really is, maybe how the programming language is interpreted by the specific computer your using) and obviously you should have some prior basic mathematical study before. as it will help you with the ""easyness"" you seek.

&nbsp;

if youre just interested in graphics and visual things you should just trying editing css of webpages your something.",1541708835.0
gabriel-et-al,"Looks like you're a young person who wants to use your spare time for learning something cool and useful. You'll want to start with a language that offers a small learning curve and fast feedback. Python, Ruby and JavaScript come to my mind, and I strongly suggest either Python or Ruby.

There are many online learning resources you can choose. There is no way to tell for sure what's the best for you because each person adapts differently to each resource. I for instance prefer texts, some people prefer videos, etc. Explore and find out what works best for you. Also don't lose motivation if you don't adapt well to your first learning resource, just move ahead to another one, you`ll eventually find a good one.

Often programming is an endurance challenge -- expressing your ideas with a programming language requires much training to master. Be prepared.

Good luck!
",1541616295.0
ninjastarkid,My friend don’t torture yourself. Don’t use notepad++.  I would recommend any other poor windows developing technology over notepad++. ,1541616219.0
Fcuco,"Play a little with JavaScript first. You don’t need to download anything, simply create a few programs using your editor and use your browser to run them.

If you find yourself immersed and enjoying the experience jump to python right away.",1541621421.0
green_meklar,">Easiest way to start coding?

Starting with Javascript is probably the most straightforward. There's no memory management, no compiler issues, no extra software you need to download, it runs in every Web browser and surprisingly fast.

>What would be the easiest way to make an algorithm that does something if a part of the screen is a specific color?

That's actually a lot harder than it sounds. Usually you'd get some sort of library that handles the interface with the graphics output for you. I wouldn't worry about that particular functionality for now.",1541622667.0
alesi_97,Notepad++ and coding (compiling) shouldn't be in the same sentence ,1541624154.0
lildickgirl111,Myspace. ,1541673112.0
Captain_Flashheart,"Disclaimer: I'm a neural networks guy. The kind that sees some problem and instantly thinks.. ""There's a NN for that!"". Often it is obvious the problem doesn't need any NN-level things. 

For your problem I would highly recommend a Yolo-based approach. If having labelled images is a problem you might want to look at U-nets. ",1541609979.0
smart_and_witty_name,"I'm not 100% sure if I understand your question correctly, but if I do, you may want to consider trying a generalized Hough Transform based approach, once you have isolated the shape you are looking for.",1541613121.0
lobomos,I don’t know how to help you but this seems really cool.,1541609912.0
einezahl,"The way I understand your approach is that you transform the information contained in the original space into a feature space, and given this feature space object that lie close to each other, in this space, look alike. In this case dolphins would be clustered in some way in this feature space. Given that you have a few images of dolphins I would approach this by inspecting the resulting cluster in the feature space and take the median or mean vector as the golden standard.

&#x200B;

Sidenote: I think this approach of alikeness in a feature space is used for some neural networks for the generation of larger datasets necessary for training.  
",1541613980.0
wasabichicken,"You're looking for /r/learnprogramming and/or /r/programming, this subreddit is more about the theoretical math stuff.

Best of luck!",1541599014.0
WArslett,"your friend is right. Most of the expertise in programming is around learning the concepts of algorithms, design patterns, data structures, problem solving etc. Learning the syntax of a language is a very small part of it. It's like the difference between knowing how to lay bricks and knowing how to build a house. A lot of bad programming comes from people learning syntax and then thinking they know everything they need to build software. But yeh try /r/learnprogramming for useful resources.",1541601097.0
FeezusChrist,"Honestly, programming a  shortest path algorithm and mixing it with a GUI sounds pretty brutal for a first year CSE class, especially if it's just community college. If you can train yourself to keep up the worth ethic I don't think you'll have any problems with future classes.",1541582468.0
christianbailey,"> how complex can it get( I know the assignment was just the tip of the iceberg, so could someone site me an example of something to show how complex it gets)

The problems and their solutions can get much more complicated, **but**: with practice and training, the complexity that you have to keep in your head will decrease. You'll learn to split problems into smaller subproblems, write modular code, and abstract away details behind a function call.

> what is it that you spend yourself doing on a daily basis(as a person who has just coded for about 16 hours straight I have heard the term code monkey passed around and am thinking that no matter how much I enjoy the other aspects this would not be the field for me if I have to spend my time like that for a significant portion of my life)

I find myself spending less time coding and much more time thinking about how to define and break down problems. I'm in academia though, so it might be a bit different in industry. In a well functioning team environment, you can generally expect a manager or team leader to do the ""problem splitting"" and present you with a (somewhat) well defined and small problem. In a less well-functioning environment... it can be bad.

> what keeps you going when you feel like giving up on a project or algorithm or w/e it may be (fairly self explanitory on this one)

Tbh I usually do give up, at least for a few hours. Spending some time to clear my mind sometimes helps.

> what tips would you give someone who is just starting out(I dont do much personal projects and what I currently know is OS theory, minor GUI things, minor python, minor java, minor c++)

Don't spend all your time learning ""coding"". Sure, learn a few different programming languages (ideally different paradigms), but the small details can be looked up whenever you need them. I think that learning about how other non-programming systems work can help when thinking about how to solve a problem.

> what did you find the hardest to learn( so I can get a headstart and have it not make me fail a class or struggle in a job or whatnot)

Time management. My strategy of ""giving up"" in 3 doesn't work so well if I have a deadline in a few hours.",1541577931.0
Kristler,"1. How complex things can get? [Extremely](https://www.fastcompany.com/3021256/infographic-how-many-lines-of-code-is-your-favorite-app). Many of the everyday programs you interact with are absolutely massive in both complexity and size. Rest assured though, because things don't get to this scale without the combined effort of hundreds of engineers. The reality is, it's common that nobody ever fully understands every facet of a flagship software product. Instead, many people each own their own piece of the puzzle that all fits together.

2. A very well regarded book on good software engineering, ""The Mythical Man Month"", [makes the claim that the average developer writes 10 lines of code per day](https://stackoverflow.com/questions/966800/mythical-man-month-10-lines-per-developer-day-how-close-on-large-projects). While your mileage may vary, there's definitely a lot of truth behind the claim. As a software engineer, much of your job is focused _around_ the code - in drafting requirements, communicating expectations with management and clients, and so on. The idea that someone could drop a book full of requirements and specifications and then expect you to put your head down and churn out feature after feature without any sort of iterative process is basically unheard of. Instead, we're often asking why we're building something, and if we _should_ be building it. This is especially true if you join an established company, since you'll mostly be building out improvements and changes to an existing code base than whipping something up from scratch. We resemble gardeners more than we do bridge builders ;)

3. What keeps me going is a hard one question. I would describe myself as the type of person who's incredibly stubborn about not giving up, so when things get challenging I just get more motivated to find answers. I would say that a fair amount of determination is necessary, because those moments of pure struggle are when you grow the most as a developer. To shy away from that or to give up prematurely is just robbing yourself of one of the best learning experiences you could have.

4. Tips, keep an open mind and focus on breadth of knowledge before you dive deep into something. Don't be prejudiced against different fields and industries of software. You never know when you'll find that you actually really enjoy a particular type of software. Also, don't forget that the infrastructure around your software is just as important as the software itself - so learn you some version control, your system administration, and some essential hardware topics!

5. In terms of pure difficulty, I would say parallel computing and distributed systems are some of the hardest topics to grasp, mostly because thinking in an asynchronous fashion doesn't come naturally to many people. Low-level graphics and image processing is hard because of all the linear algebra involved. Those are all topics that are optional, though - they're not of concern to you unless it's a field you want to get involved in.",1541586931.0
Eeyore_90,"1) It depends. Different jobs work in different domains and the complexity can vary widely. You could be building operating systems, GPU firmware, compilers, or writing CRUD web apps. Everything runs on software these days so the options are pretty much endless.

2) Again, it depends. I'm a web app developer so I spend lots of my time figuring out how to make web apps do what people want within my business domain. Often times programmers have multiple responsibilities depending on their job title which often include non-programming tasks. 

3) Honestly, needing to be paid and not wanting to be a flake. I want to be good at what I do, be respected for it, and get a paycheck. 

4) The real world is not like school. At all. When I was in college, everything seemed too hard and like too much work and I hated it. Every semester of college I was on the edge of changing my major but I had just enough determination to stick with it. You'd be surprised how quickly yet subtly you improve and you'll look back at school projects and wonder why you had so much trouble with them. It's really all about practice, and frankly college is not long enough to get enough practice unless you program 24\7.

The other thing is, in college, they teach you the raw fundamentals. In the real world, ain't nobody got time for that. Those wheels have already been invented by someone else because everyone realized a long time ago how much of a waste of time it is to code your data structures and whatnot from scratch every time. Your focus is on solving the business problem using tools. College is about building and understanding the tools. 

It's not an exciting career, unless you're one of *those* people, but it pays the bills pretty well and it's not going out of style any time soon. If you decide to stick with it, just don't let yourself become obsolete. Keep learning and trying to get better. Keep up to date on what skills and languages are in demand and make sure you can work with them.",1541607037.0
gunhox,I am also 2nd year community college student as well!,1541657147.0
DSrcl,Find professors working in areas that seem interesting to you can talk to them.,1541533922.0
askaboutmyquery,"Hey current MSc ComSci student who also did a Undergrad Thesis and presented at a conference.  I got into my research by thinking of some problems or ideas I might be interested in and chatting with a professor I liked about it.  From there, she had a colleague who was working on it at a different Uni and now for the last year or so I have been doing some research on the side for my topic during the end of my Undergrad/beginning of Masters.  I am personally looking into epidemic modeling/simulating.  Concept wise it isn't overly complex which has been fun to figure out what research is and get a better understanding of my options.  I would be happy to answer any further questions.  Feel free to reply ot message me :)",1541534086.0
aetherman,"Asking a professor for a research project would be ideal. However, depending on your luck, this may be unavailable.

If asking them for a project is unrealistic, then you should ask them to recommend some papers to read. If in reading those papers anything doesn't seem logical, it may not be! Investigate!

If you're really audacious, read some of that professors papers and bring up questions to them.",1541567689.0
zcleghern,"Professors with active research labs are always looking for undergraduates to help out if you can bite off a manageable amount of work (I was blown away by just how much effort it takes to do any amount of research). You should contact professors in your department whose research is interesting to you, no need to have the exact skillset they need (though you wouldn't ask to do databases research if you don't even know SQL).

Good luck!",1541537680.0
Techno_Jargon,I always like programming procedural terrain/environment. And if its a large project you can take it a step further and do solar systems galaxies and even galactic clusters.,1541542892.0
RichardMau5,"What do you like to do in your spare time? I listen often to techno music, so I went from there to make something interesting. I am just finishing up the same type of research you will do",1541540394.0
cp5184,"Maybe just do some simple basically benchmarking?  Maybe just do some stuff with, say, machine learning, try various methods of doing simple tasks, say, using packed math on an intel processor, amd processor, amd GPU, and nvidia GPU.  Try to find someone in your college or your general area, maybe even a neighboring college who's doing work and ask them if you can just do some simple performance analysis for them on building block type stuff.  Maybe just do some simple networking benchmarking.  What kind of performance can you really get out of, say, a 10Gb network card with small packets, large packets, and jumbo packets, and what the performance implications are.  Maybe 25Gb or 40 GB, or bonded 10Gb, or 100Gb.  Maybe just do some ssd filesystem benchmarking.  What's the write multiplication of various filesystems?  ",1541536304.0
singham,"Personally, I like the idea of automatic generation of complex solvable puzzle instances for some puzzle games. 

Conside a game like the following.

Orangle : https://play.google.com/store/apps/details?id=hi.brett.orangle&hl=en 

Youtube Trailer : https://www.youtube.com/watch?v=dTsSh5fTItw

Data-Driven Sokoban Puzzle Generation with Monte Carlo Tree Search : http://giga16.ru.is/giga16-paper6.pdf",1541599985.0
Crazypete3,"There's a good chance some of the professors who teach your courses already do it, so just schedule a meeting in their office and discuss it with them. ",1541534998.0
RedDeathofOsiris,"Assuming you'd be running this research yourself, you need to take more classes and gain more experience. As a sophomore unless you've been recruited to join a research group that already has ongoing projects (this is what I did at your age, nothing from your post indicates this), you're most likely not ready for any kind of 'real' research. This is particularly true in math heavy topics, such as the latest numerical analysis, computer vision and even AI/ML/NLP research. 

&#x200B;

I did/do research in systems and networking. There is *less* barrier to entry here, but there is a ton of material and many papers you must read to really work in this field. I can think of 10 off of the top of my head to even scratch the surface. You also really need to be a through and through expert on a lot of stuff pertaining to architecture (x86, RISCV etc depending on what it is) and operating systems (usually Linux, file systems, networking, I/O, multi-threading). 

&#x200B;

Systems and networking, like 'computation and mathematics' (or any CS subfield) is a HUGE field. If you haven't narrowed it down at all, no one will be able to tell you much. From your post, you haven't narrowed down your focus aside from 'computation/mathematics'. Do this first. Gain the experience necessary to find an interesting problem that no one has looked at your way, and do it when it comes. Not all PhD's have extensive undergraduate research experience, if this is the path you're considering.",1541538074.0
spinwizard69,"I really have no idea.   The factory I work in has the machines connected In one very long cell.  Actually a number of cells.  

In any event a word of warning theory often fails when faced with the reality of the factory floor.   That and some of the best advice on factory layouts can come from the people on the floor.  Often addressing human factors will have a big payoff.  ",1541524190.0
Ihaveexpectations,"Let there be program that takes a program and an input for the program, and determines how many steps it will take to compute the result.

Let there be another program that takes a program, and uses the other program to determine how many steps it will take to compute the result of that program, with the input being the program as well. If the other program's result is an infinite amount of steps, it returns 0, otherwise it goes into an infinite loop.

What happens when you compute the second program with the input of itself?",1541585802.0
ParaplegicOctopus,"I need a supper computer to understand women.

And i don't think even in 50 years the quantum computer will calculate what they want. ",1541497526.0
somethingdangerzone,The one with the shittiest prof. Everything can be learned but not everyone is taught equally.,1541524302.0
TaXxER,"I'm in doubt between 1) cryptography, 2) compiler construction, and 3) theory of computation.",1541512917.0
benpva16,"For me, Data Structures and Algorithms was both the most fun and the most challenging course I took. Fun because I enjoyed learning how to actually implement theory in C++. Challenging because the topics were diverse, and keeping track of all the little fiddly details about this or that algorithm’s amortization or Omega() characteristics or whatever just slid out of my brain so easily. Calculating Big O performance for recursion algorithms was a tricky topic to get as well. 

The graph theory stuff proved really cool and useful. In the class, we used it to build a program that could spit out all the words in a Boggle board. I later used it to solve a chess-based puzzle game. And I’m about to use it to take a crack at designing belt balancers for the game Factorio. (Can I take 3 uneven belts of items in and output 5 balanced belts, for example)",1541522059.0
usair903,"I had big trouble with mathematical optimization. Other courses might have been hard, but I could always visualize or attribute the problem in one way or another. However the mathematical notation and the algorithms we used in optimization were completely beyond me. Still no idea how I passed :D",1541498444.0
veridicus,"Way way back when I got my degree the hardest course was assembler. Learning the language wasn’t difficult but applying it to real problems stumped a lot of students. My school had the most CS dropouts from that one class. 

I think a lot of students didn’t really appreciate the complexities of programming until they took that course. And so many then switched to work on an Information Systems degree. ",1541512065.0
magnificentbop,Algorithms and the tail end of theory of computation.  Most of our grades were based on our ability to generate proofs quickly on exams.,1541504107.0
XirAurelius,For me (twenty years ago) it was assembly language. Even with the hardware class I had just completed I had issues with it. Of course it made my OS class easier later.,1541517297.0
jmite,Operating Systems was killer for me,1541515277.0
nightfx91,Theory of Computation was quite a bit more difficult for me than algorithms.,1541529957.0
,"For me, the hardest class I had so far was mathematical statistics. Most challenging CS-related course was probably cryptography.  


Generally people find theoretical CS classes more challenging than e.g. software engineering and areas like OS more difficult than e.g. databases, but ultimately it comes down to personal preference.",1541530602.0
chidoOne707,“Discrete Mathematics for Computer Science” for sure.,1541539022.0
hextree,"The more mathematics and proof-based theory involved, the harder, generally. And I say this as a former mathematician who went on to do theoretical computer science for PhD, I'd still be kidding myself if I said theory was any easier than pretty much everything else.",1541602854.0
Mavoor,"Theory of computation remains the only class I’ve taken where I genuinely felt I simply wasn’t smart enough to fully grasp the material. Decidability and reducibility were concepts I really struggled with.
Granted this was a 4th semester class and I wasn’t really interested in math / complex logical reasoning at the time (nor am I now tbh) - I’d love to revisit the material at some point and see if I’d become better at it, but yeah parts of that class really fucked with my brain.",1541528835.0
panderingPenguin,"It depends entirely on the student in particular (what they are and are not good at), the curriculum at a particular school (for example some schools have a brutal compilers class, others have a much easier one, and some don't even make compilers a required class at all), and the professor teaching it (how hard their exams are, how harshly they grade, how good their lectures and office hours are, etc). This question has far too many unknowns to give anything resembling a meaningful answer.",1541531028.0
BerniesMyDog,"Depends.

* Not all profs will be equally good teachers
* For me personally I found theory of computation and computer graphics to be easy but I know a lot of people struggled with the math requirements for these
* For me personally I found compilers to be difficult",1541566447.0
ash663,Start with getting to know the basics of networking. Highly recommend the textbook Computer Networks: A Top Down Approach,1541481146.0
Carpetfizz,"cs168.io

check out the slides, they are publicly available! ",1541485364.0
alfiedotwtf2,"You're not going to get a better intro than ""TCP/IP Illustrated, Vol. 1"" by W. Richard Stevens. Ignore the other volumes.

[https://www.amazon.com/TCP-Illustrated-Vol-Addison-Wesley-Professional/dp/0201633469](https://www.amazon.com/TCP-Illustrated-Vol-Addison-Wesley-Professional/dp/0201633469)",1541499315.0
evantreb0rn,„Computer Networks“ by A. S. Tannenbaum and D. J. Wetherall. ,1541487994.0
goldenliar,Introduction to Networking - Stanford course. It’s free on their website. ,1541488329.0
throw_cs_far_away,https://beej.us/guide/bgnet/html/multi/index.html,1541489534.0
LexanderSky,[https://www.bau.edu.jo/UserPortal/UserProfile/PostsAttach/10617\_1870\_1.pdf](https://www.bau.edu.jo/UserPortal/UserProfile/PostsAttach/10617_1870_1.pdf),1541495225.0
sharjeelsayed,"Computer Networking: A Top-Down Approach (7th Edition)
https://www.amazon.com/Computer-Networking-Top-Down-Approach-7th/dp/0133594149

CS144 Introduction to Computer Networking Stanford University
https://suclass.stanford.edu/courses/course-v1:Engineering+CS144+Fall2016/courseware/ac9d1eef5aaa4bb5bcfe4d42f51f0f5b/c5c384e648cf404c837d05497c6e36b0

High Performance Browser Networking
https://hpbn.co

Beej's Guide to Network Programming
http://beej.us/guide/bgnet

Unix Network Programming, Volume 1: The Sockets Networking API (3rd Edition)
https://www.amazon.com/Unix-Network-Programming-Sockets-Networking/dp/0131411551

CCNA Routing and Switching 200-125 Official Cert Guide Library
https://www.amazon.com/Routing-Switching-200-125-Official-Library/dp/1587205815

CBT Nuggets by Jeremey Cioara
https://www.cbtnuggets.com/it-training/cisco-ccna-icnd1-100-105
https://www.cbtnuggets.com/it-training/icnd2-200-105

Eli The Computer Guy
https://www.youtube.com/playlist?list=PLF360ED1082F6F2A5

More at http://Learn.SharjeelSayed.com",1541621273.0
-LOLOCAUST-,"Someone linked this recently and it was well upvoted. I can not attest to it’s quality.

http://intronetworks.cs.luc.edu/current/html/",1541507670.0
usemranenotavailable,Eli the computer guy has a ton of YouTube videos that I found to be helpful. ,1541513706.0
bartturner,"Depends on the time you want to invest.   But me personally wrote a TCP/IP stack from RFCs over 30 years ago is how I really learned it.

Now you have the Comer books that make it a lot easier.  But you write it and you really, really learn it.",1541514926.0
WhackAMoleE,"Internetworking with TCP/IP by Comer, volume I.",1541525152.0
istarian,A good book would be helpful. Depending on what you want to know the internet and wikipedia might be sufficient. You could also get the code for a TCP/IP stack for older systems or a micro controller so you have a simpler version to parse.,1541527182.0
landgar86,CCNA,1541532675.0
Guido125,"Seen a lot of suggestions that focus on the theory side. I would take the opposite approach.

Install wireshark and start messing around with it. You will learn a lot. Experience is something books can't teach.

After that I would read up on the theory to balance out your understanding.",1541527243.0
Lanko,"So basically you're looking at Cisco CCNA courses.  Either from your local colleges and universities,  Online courses, or through Cisco Directly.    I'm sure there are youtube or lynda.com courses you can take as well.  But I know I learn best in a classroom.

I'm taking it part time right now and it's been incredibly useful. 

Edit:  Downvotes?  for this?  Am I wrong?  is there something else we should be looking at?  Don't just downvote and ghost.  Contribute so that others can learn too. 
",1541522539.0
Skyyacht,"ICND-1 (CCENT), ICND-2 (CCNA), CompTIA Net+, and Sec+ for certifications.

For protocols, read the respective IETF Request For Comments documentation.",1541480527.0
arkrish,"Consul by HashiCorp has a good read on scalability: https://www.consul.io/docs/enterprise/read-scale/index.html
It’s built on top of raft. 

Zookeeper has similar issues and there have been recent jiras about having non-participating readers (observers) which increases the number of participants. ",1541470480.0
tchoup23,"Though rambling, the following 'faq' on sharding project for Ethereum is endlessly informative and likely a good place to start. Good references.

https://github.com/ethereum/wiki/wiki/Sharding-FAQs",1541463297.0
jacksonmills,"I tried taking this but honestly this survey is kind of a mess.

Many of the questions are very vague, and a few make zero sense whatsoever. ",1541463634.0
BeepBoopSpace,"Start looking up python image comparisons through google searches and go from there. OpenCV works with both python and c++ iirc, but there’s more support for python if there are problems",1541462488.0
simondvt,I would suggest Andrew NG course on coursera to get started. ,1541436772.0
DashAnimal,"Dude wtf why is everybody telling this person to read Cormen when they have ZERO background CS knowledge? For reference, I didn't touch Cornen until my second or third year. 


Here's my advice.


Don't make your knowledge redundant. You have five free months which will be really hard to ever get again in your life. Learn some other skills that university won't teach you. Do you know how to cook? Do you have any hobbies that you find fufilling, or would this be a good time to explore that? There's another benefit to this in that, the first semester of University you learn HOW to learn at University. It's completely different from high school. If you decide to try and learn the material before university starts, you're really skipping the process of learning how to learn in a university environment. And then you're left to learn that in later in semesters with harder classes, when you haven't had time to become ""better prepared"". So you're doing yourself a disservice.



I'm not saying don't read CS or economics stuff. But you absolutely don't need to, and you shouldn't turn your five months into semester 0 in these courses, which you'll repeat for another few months in semester 1. If you want some light reading, I'd grab ""Code"" by Charles Petzold.




But whatever you decide to do don't grab Cormen. That is not the first book I would recommend for someone just getting into CS. Not by a long shot.",1541431322.0
dave2118,"Figure out what languages you’ll be using in upcoming classes, start watching free videos and start practicing.  

I’m assuming you can find a syllabus for the courses you’ll be taking?

Also, for those starting college, make sure your professors know you.  Stop by and ask questions.  If they know you, it’s more likely you’ll get better grades.  That’s my experience, but I’m old.",1541421998.0
testicular-toupee,"You can take The CS50 course on edx.org
It’s totally free and it’s a great introduction to computer science and the basics to code in the most popular programming languages. You can even pay $90 and get a certificate of completion. ",1541435005.0
teawreckshero,"I think everyone should take CS on the side. It's the science of problem solving, and everyone has problems they need to solve.

The key takeaways that I hope you'll incorporate into you actual major(s) are: 

* boolean logic/first order logic/logic in general: understand truth tables and understand fallacies.
* programming: know when you're doing something automatable, and consider using a computer to do it instead.
* combinatorics/counting theory: if someone asks you how many possible license plates there are given a certain letter/number pattern, the answer should be trivial. If someone asks how many different ways a set of (not necessarily independent) discrete events could transpire, you should not be overwhelmed.
* complexity theory/i.e. ""Big-O"": know how algorithms scale in time and space based on their inputs.

Again, these are the topics I hope you can make use of after graduation. But it won't hurt giving yourself a primer on each before going into the major.",1541441927.0
__-_-_-_-_-_-_-_-_-,"Learn C. Seriously, you can brag about knowing all the hot technologies out there, but if you haven't played around with pointers, you will never be a true Computer Scientist. Since you have no programming experience, I would advise not to get lured by so-called 'simple' languages (you will hear Python a lot). As they say, do it the hard way. It would certainly be tough, but don't get intimidated. Remember, everyone starts at zero. Now if you really want to jump into the ocean, get yourself a copy of K&R and a Linux machine!",1541444713.0
noam_compsci,"If you want to prepare for the academic side of things, I would recommend looking at some of the books I spoke about in [this](https://www.reddit.com/r/compsci/comments/9txahc/im_doing_a_complete_180_i_hate_my_current_career/e8zwcaa/) post. Especially the algorithmics one. I'd especially recommend going through the maths textbook as it is really good. 

An alternate is that in my opinion, degrees do not necessarily teach you the type of algoirthmics and coding needed to pass coding interviews (they seem to be totally different skillsets imo) and so if you want to land a great tech job and you know that you will be applying for internships etc, I would take the time to start on that front. Check out hackerank, leetcode etc. + the book 'cracking the coding interview'. ",1541422791.0
SubtleGenocide,"Learn about Boolean logic (and, or, xor, not, gates, expressions, circuits, TT)  it's easy to learn and this way when you start it you don't have to worry about it and have time to do other things during that that unit. Also data encoring might be good to read up on. I'm sure others have said it but find out what code you will be using and what version (python 2 or 3 type shit) and get familiar with errors. Hope this helps.",1541438636.0
AaronKClark,Check out https://www.reddit.com/r/csMajors/,1541440803.0
TrashConvo,"Advice from my professor in my junior year, unless you’re doing something more specialized, do it in python. If you learn a little python you won’t regret it! Also recommend a micropython board. ESP8266 nodemcu boards are dirt cheap and people make a lot of projects with them in python. Alternatively, Arduino is fun to mess with and a decent intro to C-like languages.",1541492314.0
DTH97,"Learn how to write some code. I reccomend learning in Python using the free book **Think Python**, but there are some good tutorials online that would work well as a substitute to this book.

Also, get familiar with discrete mathematics. It will help you understand the thinking behind the fundamentals, and you will likely be required to take a course in it. I reccomend the MIT OCW course for it [found here](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-fall-2010/video-lectures/).",1541429765.0
eatingtheuniverse,"I would do 1 of 2 things:

1) Take a comp-sci class you find interesting. A big part of comp-sci and getting a job in the field is having a portfolio of work. If you have 5 free months, what better time to figure out if you truly want a double major?

2)Study math. To me, this is more boring, but in both economics and computer science you will have to take math classes that overlap, so by doing this you will lose no time if it is not for you. You will for sure see Calculus and Statistics in economics, and (I think) Linear Algebra. These are common to both degrees.

You'll know what you feel like doing and I hope it works out for you!",1541431817.0
Audiblade,"Play [Human Resource Machine](https://tomorrowcorporation.com/humanresourcemachine). It's a video game where you program an office worker using a simple assembly language. I played it after getting my master's in CS and was astonished by how much it taught me about low-level programming. And it's very accessible: My ex-roommate, who's a music educator with no STEM background, enjoyed the game as well. It should only take you about a dozen hours to beat, and it will get you in a good mindset for your first CS class the following semester.

I saw another commenter say not to worry about it too much and enjoy some electives while you still have some extra free time. I agree with this. If you're going to do anything, I really think Human Resource Machine will give you the biggest benefit for the least time invested. I am really and truly impressed by that game and truly believe it will help you be prepared for your first class. It's not a replacement for actually taking a class - it is just a video game. For being a game, though, I think it can give you a bit of a leg up without taking up much of your time and energy at all.",1541470038.0
gmoharram,I would figure out what programming language the intro course teaches and take an online beginners course on that.,1541452560.0
trolock33,Take a discrete math and graph theory course. It helps :),1541429451.0
NewComputerNewUser,"I suggest that you start with programming. 

Not to make something fancy but to implement some basic (and advanced) algorithms. 

Read  ***Introduction to Algorithms***  by [Thomas H. Cormen](https://en.wikipedia.org/wiki/Thomas_H._Cormen) and simultaneously implement some stuff in a programming language (python, C# or Java).",1541422079.0
MemesEngineer,"Ignore all these comments... Learn basic Python and thats it. No math, no anything. ",1541450407.0
Caddy05,mycodeschool on YouTube will be your friend,1541424389.0
SearchAtlantis,"Not knowing anything about the field - it's everything you're doing standard practice? It strikes me as very odd to be using dynamic time warping to shift after feature extraction - does MFCC even yield a time series or is it a vector of coefficients?

My other thought is that cosine distance might be there wrong similarity measure to use.",1541424466.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1541359450.0
naval_person,Yes.,1541358221.0
billccn,"To me ""memory topologies"" means which NUMA nodes are connected to which RAM sticks. 

If you meant to ask about ""memory hierarchy"", I don't think there is one that is set in stone in computer theory. The one we have now is the result of engineering limitations and is under constant flux as memory technology develops.",1541357981.0
orlock,Not sure if this is what you mean but have a look at the [Data Diffusion Machine](https://en.wikipedia.org/wiki/Data_diffusion_machine),1541369536.0
combinatorylogic,See https://www.cs.virginia.edu/~skadron/Papers/wang_APoverview_CODES16.pdf,1541364995.0
toxicdevil,[https://www.youtube.com/watch?v=XDpttL5\_JIQ&feature=youtu.be](https://www.youtube.com/watch?v=XDpttL5_JIQ&feature=youtu.be),1541383097.0
Zulban,"Hard to give advice when you haven't explained your career goals, interests, and finances, at all.

Frankly I would ignore the advice anyone gives you in response to such a vague post.",1541280672.0
llN3M3515ll,"A bit of general advice, you need to choose your major/minor at your university first, then work with consulars at BOTH the community college and the university to ensure your credits will transfer to your major.  Even if they transfer in - they may not apply toward your major, and if that is the case you will be wasting time / money.",1541350139.0
noam_compsci,"To be very very blunt, I really think formal qualifications are only useful if you are getting a brand name. 

On a more supportive note, I think it depends on what you want to do.

There are just so many less useful courses bundled into this like...english and general education electives.

Of the useful stuff, a lot may be easily self taught (bootcamp, books, MOOCs). In fact you can self teach it better because you can make sure it is current (contrast doing a MOOC on current devices vs your Architecture and Hardware lectures which can be based on an 1980s microprocessor that no one uses any more). 





",1541278696.0
Solax636,"[https://www.reddit.com/r/cscareerquestions/wiki/index](https://www.reddit.com/r/cscareerquestions/wiki/index)

&#x200B;

They also might have better info and opinions",1541280784.0
treller,"Here are some issues I have had over the years that I hope will help you out.


First off, if you already have a bachelor's degree, check to see if you can even get your second one. I originally have a B.S. in English with Math and Computer Science minors and not able to get a second bachelor's from the same university. Their reason was that I can't get a second bachelors degree in the Arts and Sciences college. This university is also where I work full-time as a developer.


Second, if you are dead-set on going to the university after your associates, make sure the classes will transfer. Check with the university first. I went by the guidance of my high-school counselor and the only classes that worked toward the degree from the community college were the non-computer related classes. Basically went to the university, had to start over, and ended up switching my major.


Third, are you sure Computer Science is the degree for you? Just because you are into computers does not mean a computer science degree is for you. Maybe look at a few other major options to find what you would like. At the time, English was a better fit for me but now I am working towards a M.S. in Computer Science and Data Analysis.


Fourth, in my experience, the classes at the community college were staggeringly different than the university. The community college courses tended to focus on specific languages and some group projects while the university focused heavily on the math and science of the computer. At the time, I did not realize the difference going into the major but completely appreciate this now.


Side note: I switched to English because I have a strong creative side but also these were the only classes I was enjoying at the time. I was going through some personal problems (drinking too much, dad had cancer, and not successfully balancing family-social-school-personal lives). Thankfully now the university I work at and taking classes is offering classes in the Digital Humanities which is definitely in my interest area.

Sorry about the rambling last part but I hope this was overall helpful.",1541282136.0
PineapplePanda_,"I'm finishing up my second BS in comp sci.

It's a special program offered by Oregon State University for people who already have degrees.

You can get your BS from 1 to 4 years, depending on your dedication. Average is 2.

Check it out /r/osuonlinecs
",1541553866.0
fistyit,Building a data visualization app with Vulkan. For mobile ofc. ,1541268528.0
supercargo,"Do either/both of your source and target languages have a formal semantics available (probably in some academic paper somewhere)?

https://en.m.wikipedia.org/wiki/Semantics_(computer_science)",1541265736.0
Peter-Campora,"Look for papers on compiler correctness/certified compilers. We don't really use the term transpilation in research. Maybe later I can link you some papers, but unless you're well versed in pl semantics, they might be a bit rough to read.",1541270295.0
combinatorylogic,"Please just avoid using the word ""transpiler"". It's meaningless and misleading. By all possible definitions, any source-to-source compiler is just that, a *compiler*.

Having said that, I'd recommend you to have a look at [K-framework](http://www.kframework.org/index.php/Main_Page) for some inspiration. Also, have a look at CompCert.

And if you want to keep your rewrites understandable and easy to prove - you'd better employ the Nanopass approach.",1541270005.0
Eugenethemachine,"You may want to look into string transduction/transformations and the various formalisms that can describe them. Similarly to how grammars, state machines, etc. Can precisely define languages there are state machines with output and algebraic expressions that can precisely capture translation between languages. These are pretty abstract objects though, so idk that's exactly what you're looking for. ",1541261093.0
m4dc4p,Look into small step & big step operational semantics. There is a nice bracket notation available for describing how one expression translates to another.,1541297945.0
stefantalpalaru,"[""Some essentials of graph transformation"" (2006)](http://www.informatik.uni-bremen.de/~kuske/Papers/IntroGraTra.pdf) should get you started. Basically, you transform the source and destination grammars to graph grammars and then you specify graph transformation rules.",1541338654.0
ReedOei,"I don’t have much experience, but perhaps something like the [K Framework](http://www.kframework.org/index.php/Main_Page) would help you.",1541258263.0
ranok,"If they are LLVM targetted, you could emit the IR and compare them with Seahorn. Abstract interpretation of programs is the more general method to compare semantics in a more computationally feasible manner.",1541287683.0
esbenab,"I have no knowledge of how to do it but wouldn’t something like a common BNF description be a way?

Just an idea. ",1541259969.0
esbenab,"I have no knowledge of how to do it but wouldn’t something like a common BNF description be a way?

Just an idea. ",1541259974.0
Kopaka99559,"Linear algebra is one is if not the most important mathematical topic you can learn for many fields, including CS. It’s the basis for anything that involves vectors and matrices, including graphics and simulations. 

The second big one is Combinatorics, especially relating to graph theory. There is a huge portion of algorithms that are based in graph theory and lots of cool current research in the field. Knowing how numbers work and how to aggregate them and manipulate them is invaluable. ",1541223350.0
Howdy_to_you,"I recommend taking a cryptography class, I'm in my last semester, same major, and I'm just now taking it, I really wish I would have taken it earlier and gotten a chance to take the second class. It would be good for if you might be interested in cyber security, or even just learning the basics of how any security systems work out mathematically.

You can also look into probability classes (same situation as before I'm taking one now, wish someone had told me to take it earlier) or even statistics classes past what your school requires if your interested in being a data analyst. 

Also I recommend learning extra languages on the side when you have time, beef up your resume, you'll be at a disadvantage finding a programing job when you graduate if you only know the few languages they will teach you in you're time there, as compared to the pure computer science majors. Don't think you need to know all of them though, You will have learned enough by the time you graduate that you can pick up any language pretty quick if a company wants you to use a language you don't know yet. 

Also maybe think about going to grad school or switching to computer science with a minor in math if thats available. I recently realized with the major we are both in currently, I'm underqualified in my computer science side just based on the fact that I'm essentially only getting a minor in CSCE not a full degree, so I have some catching up to do or more education in my future for that. On the math side, again underqualified because the thing I can apply it in I'm underqualified in, can't get any of the ubundant finance or accounting jobs that would use my mathematical  skill set, and most anything else is research that I’d need a PHD in math for.

Trust me there are jobs, I have found them yes, and it’s a great degree to have, but honestly my life would have been so much easier if I had gotten a degree in computer science with a minor in math. There are so many math classes I have taken that I doubt I’ll ever use again, and I wish I could have taken more computer science instead. By the time I realized all this it was too late for me unless I decided to take more than a few extra semesters (my school got rid of pure computer science for some reason, so I would have had to go into computer engineering which I'm not interested in)

Sorry don't mean to scare you from your plan, you'll be perfectly fine with that major if you don't want to change from it, but since it's still early, and I doubt anyone will tell you otherwise, I personally think getting a computer science degree is better for your job prospects, and you'll still have plenty of math to take to keep you going . If you don't like the idea of changing your major, just ignore the last three paragraphs 👍🏽",1541247932.0
net_nomad,"Read about this: https://en.wikipedia.org/wiki/Therac-25

A chilling tale of negligence.",1541217407.0
tchoup23,"Volkswagen (and others) emission scandal comes to mind.
 https://en.m.wikipedia.org/wiki/Volkswagen_emissions_scandal

I recall an early coming any response was that it was a team of rogue software engineers. Yeah right!
",1541251957.0
SOberhoff,"I once discovered a table of user data that was encrypted for data security reasons. However, the encryption was completely transparent, so the data wasn't really encrypted at all. When I brought this up the response was ""Yes, but this data is useless anyway. So it doesn't matter if it's not really encrypted."" Ok, I guess. ¯\\\_(ツ)_/¯",1541207619.0
existentialwalri,Why fix today for what you can bill for tomorrow!,1541219120.0
combinatorylogic,"Toyota engineers faced a dilemma - deliver shit quickly, skipping all the regulatory requirements, ignoring the industry standard MISRA-C guidance and kill a lot of people, or do the right thing, slow and steady, but probably get yelled at by the impatient managers. Guess what did they do?

",1541278559.0
jokasx,"I would bet most people haven't heard about those ieee ethics. I hadn't. So good work you are doing trying to teach some of these. In the end we all work in what we need to survive, if not would be unemployed.

Some cases: you are working in a cool product and technologically viable but the whole business model is undefined and shady. Eventually you discover it almost touches the scammy level. Gotta make money fast, preferably while users are not looking at their credit accounts.
Another, you are also messing with interesting and new tech, probably touching the limits of what can be achieved with current tech, but you know that what you are creating could eventually be one of the most powerful weapons in the wars that are still to come and will eventually be used for evil besides all the promisses it is just being used currently for some important cases by law enforcement. ",1541278080.0
Rant_CK,"[Deep Fakes](https://github.com/iperov/DeepFaceLab) - Deep Fakes is an artificial intelligence-based human image synthesis technique. It is used to combine and superimpose existing images and videos onto source images or videos. Deepfakes may be used to create fake celebrity pornographic videos or revenge porn.

&nbsp;

[Cambridge Analytica](https://cambridgeanalytica.org/) - Cambridge Analytica uses Facebook data to change audience behavior for political and commercial causes [[Guardian article](https://www.theguardian.com/news/2018/mar/26/the-cambridge-analytica-files-the-story-so-far)]

&nbsp;

Fake News Bots - Automated accounts are being programmed to spread fake news. In recent times, fake news has been used to manipulate stock markets, make people choose dangerous health-care options, and manipulate elections, including last year’s presidential election in the U.S. [[summary](https://www.wired.com/story/internet-freedom-2017/)][[role of bots](https://www.technologyreview.com/s/608561/first-evidence-that-social-bots-play-a-major-role-in-spreading-fake-news/)]

&nbsp;


[SenseTime](https://www.sensetime.com/intelligentVideo/84) & [Megvii](https://megvii.com/)- Based on Face Recognition technology powered by deep learning algorithm, SenseFace and Megvii provides integrated solutions of intelligent video analysis, which functions in target surveillance, trajectory analysis, population management [[summary](https://www.reuters.com/article/us-china-facialrecognition-analysis/backing-big-brother-chinese-facial-recognition-firms-appeal-to-funds-idUSKBN1DD00A)][[forbes](https://www.forbes.com/sites/shuchingjeanchen/2018/03/07/the-faces-behind-chinas-omniscient-video-surveillance-technology/#4f2b9a264afc)]

",1541378162.0
Madsy9,"If you use fixedpoint with a power-of-two base, then you have a good idea of how many bits you need for the result after an operation. For example, for addition ""a+b"" requires log2(a+b) bits. When a and b are close to the maximum value you can simplify it to log2(2a), or just 1+log2(a). For multiplication you get log2(a*b) = log2(a)+log2(b). You can get a rough approximation of log2(x) by finding the position of the most significant bit of x. It's equal to floor(log2(x)).

The precision you need for a mandelbrot renderer is directly related to your zoom and to some extent the iteration limit. What we usually do is to define the complex coordinates of our viewport corners and scale them with 2^-zoom. If zoom is a real number as opposed to an integer, then the base precision you need just to represent the initial complex numbers for Z and C is log2(ceil(zoom)) plus however many bits you need for the integer part. I usually have my extreme corner points be between [-2,-2] and [2,2] because nothing interesting is outside that radius.

Besides being conscious regarding the required precision, also be on the lookout for loss of significance. Avoid subtracting numbers of equal magnitude. Rewrite your algorithm implementation by moving terms around to handle such cases.

https://en.wikipedia.org/wiki/Loss_of_significance

There's no real way to automate these kind of finite precision issues which works for all cases. Good sense, some knowledge of real analysis and careful looking over our implementations is the current solution.

Edit: Regarding Mandelbrot and figuring out which digits in an intermediate result are *significant*, I have no clue. My post here assumes that all bits in all the intermediate calculations are significant, which is probably not always the case. Due to the nature of Mandelbrot, values you get in the complex numbers are all over the place.",1541207707.0
foreheadteeth,"With the Mandelbrot iteration you have to use the highest precision you intend to use from the very first iteration or else you lose all the accuracy and never recover it. (It may be possible to do the first 3 or 4 iterations in small-width fixed point but the number of fixed point digits you have to use doubles at each iteration so this doesn't scale.)

With other iterations (e.g. a Newton iteration to find an x such that f(x)=0) you can indeed vary the precision, this is often done in arbitrary precision libraries like https://gmplib.org/. I'm not sure what algorithm is used in GMP but the [best algorithm for bignum division](https://en.wikipedia.org/wiki/Division_algorithm#Newton%E2%80%93Raphson_division) is based on the Newton iteration and uses more and more bits of accuracy as you converge.

If you want to do this automatically, you might want to start with interval arithmetic, although this is not the only way. https://en.wikipedia.org/wiki/Interval_arithmetic",1541208319.0
logicbloke_,"You might want to look at 

	
""A Verified Certificate Checker for Finite-Precision Error Bounds in Coq and HOL4"" by Heiko Becker, Nikita Zyuzin, Raphaël Monat, Eva Darulova, Magnus O. Myreen and Anthony Fox @ FMCAD'18

They have their code on GitHub.",1541210473.0
cp5184,https://www.google.com/search?source=hp&q=ieee+floating+point+error+estimation+754,1541194859.0
maruahm,"If *f*, *g*, and *h* are strictly positive, then if there is some *c*>0 such that *f+g*<*ch* pointwise, then it's pretty clear that *f*<*ch* and *g*<*ch*, giving us that *f*=*O*(*h*) and *g*=*O*(*h*). So yeah, mathematically true.

However, this is not true for functions in general. If for some reason *f* and *g* were allowed to be negative, then consider *h*=*x*^2 and -*g*=*f*=e^x. Immediately you get that *f+g*=0=*O*(*h*) but clearly *f*≠*O*(*h*). This is generally outside the consideration of computer scientists, but should give you a sense why the statement is not usually given as a bidirectional.",1541195767.0
green_meklar,"First, rather than using '=' we should really be using 'ϵ', the 'element of' symbol. That is to say, O(H) is the set of all functions which are bounded above by C\*H for some finite positive constant C across all inputs larger than some input, and F ϵ O(H) means F is a function in that set.

Second, F ϵ O(H) does *not* imply that ~(F ϵ O(L)) for some L such that ~(H ϵ O(L)). That is to say, F could be bounded above by some function 'strictly smaller' than H. As a few practical examples, we have 1 ϵ O(1) but also 1 ϵ O(N), 1 ϵ O(N\*log(N)), log(N) ϵ O(N), and so on. There is another notation Θ(H) for which F ϵ Θ(H) *does* imply that ~(F ϵ Θ(L)) for some L such that ~(H ϵ O(L)). The confusing part is that in colloquial usage we often use the O(H) notation as a substitute for this Θ(H) notation, that is to say, we assume (incorrectly, according to the strict definition) that F ϵ O(H) says something about the *exact* behavior of F rather than merely a (possibly very bad) upper bound on it.

For the purposes of your problem, the underlying assumption here is probably that O(H) is being used equivalently to Θ(H). In that case, we have F ϵ Θ(H) and G ϵ Θ(H) therefore F+G ϵ Θ(H), which still holds. But merely given F+G ϵ Θ(H), it is possible that F ϵ Θ(H) but G is much smaller (still in O(H) though), or that G ϵ Θ(H) but F is much smaller. For instance, given F = N and G = √N, definitely N+√N ϵ Θ(N) and yet ~(√N ϵ Θ(N)).

If on the other hand O(H) is being used literally and not as a substitute for Θ(H), then you're right: F+G ϵ O(H) would imply that both F ϵ O(H) and G ϵ O(H).",1541208078.0
khanh93,"As far as I can tell, all of the things you said are mathematically correct.

Generally, if it's obvious that A implies B but not obvious that B implies A, people are more likely to go around saying ""B implies A"" than they are to go around saying ""B is equivalent to A"" or ""A implies B"".

Indeed, often when speaking about a theorem of the form ""A is equivalent to B"", people will naturally speak of an ""easy direction"" and a ""hard direction"". ",1541190782.0
SOberhoff,"There's no reason for this statement to only go in one direction provided that, as you say, the functions involved are non-negative.",1541192127.0
bdtddt,O(1) + O(n) = O(n),1541189577.0
PM_DIY_PI_OR_NUDES,"<a href=""url""><img src=""imagehere.jpg"" /></a>",1541154831.0
khedoros,"CRC is only a hash function intended to detect accidental changes to data. It isn't a protocol, on its own.",1541143683.0
pissedadmin,"Google's book on Site Reliability Engineering (free to read online):

[https://landing.google.com/sre/books/](https://landing.google.com/sre/books/)",1541175759.0
rotharius,"* Just Enough Architecture
* Deploy it! 
* Enterprise Integration Patterns.
",1541192415.0
maytash,Thank you!,1542130005.0
Brown_Mars,"Hahah I tried to make it guess stickers, but it didn’t guess it. But great job with everything! It’s fun and visually appealing! ",1541192066.0
TheOneRavenous,"Every question lined up to be a car. Does it have wheels? Yes. Does it have doors? Yes. Is it in America and Europe? Yes. Etc.etc. It guessed floors. 

What did you do to train the network? I ask because the face ID techniques are supposed to create a network that finds differences and figures out how to  ""ID"" individuals. Thinking of recent  computerphile video. 

Also fucking Awesome background, the gyroscope activation for mobile was a nice touch. [Comouterphile Face ID](https://youtu.be/mwTaISbA87A)",1541194458.0
Neil1398,"Nice man I was thinking a of a hat and it guessed it!

I’ve been wanting to get into machine learning for a while. I think now is the time!",1541345053.0
noam_compsci,"Thats really interesting. It did not guess it right (i thought of a pen) but I got a great sense of how this was going through a neural net and centering in on an answer. 

I also liked how it made the wrong guess of a fan and then almost unwinded itself. 

My only suggestion is to better update for human error and ask multiple questions on the same feature and store the answer and refine later. For example, is a pen made of wood? I said unlikely, but I kind of winged it. What is the difference between unlikely and maybe? Maybe if the user was asked the question in a different way, they would give a different answer... So ask the same questions multiple times but slightly different, and compare how consistent the player/user is. This is used in personality tests where people may say they are 'out going' but when phrased in a different way (e.g. 'do you speak out when something wrong occurs') give a different answer. 


I loved the animation, very catchy!",1541423396.0
sociobiology,I legitimately wonder how long it will take until we get to CSI-memery tier with this stuff.,1541129182.0
Alexbalix,Enhance! ,1541133518.0
Hexorg,"Tl;dr to catch suspects, we need to store 4 weeks of 8k video, and ask the suspects ahead of time to stand 4ft away from the victim while victim stands 1ft away from the camera.",1541153527.0
yesnahno,[There's a reflection!](https://www.youtube.com/watch?v=LhF_56SxrGk),1541122759.0
lara_antipova,This is fucking mind blowing ,1541127930.0
YoungGuyDude,We have finally caught up to Wild Wild West. ,1541128107.0
celerym,Did anyone who thinks this is impressive actually even read the paper in the link? Because there's nothing mind-blowing or novel about it. I'm surprised this was even published anywhere.,1541138851.0
jacksonmills,"I'm pretty sure this was done a few years ago, maybe even a decade ago. I can't remember where it was but I don't think this is entirely new science.

EDIT: Oh nevermind, I was right, this is from 2013.",1541134684.0
IndiraLuna,This is insane and awesome.,1541179098.0
Youvegotmethere,I wonder if they tested it with different eye colors and how the cameras handled that.  It’s much easier to catch reflections in dark irises.  ,1541146558.0
findhumor,"NCIS, is that you? ",1541179368.0
IkonikK,*enhance*,1541186177.0
yesnahno,"Of course it's feasible :-). Although I don't think this needs to be tied to number plates, automated driving works off probability and average, there's no way to know whether a specific person is driving a specific car at any particular time.",1541122948.0
feedayeen,"Really, the state of the tech is really poor right now. It solves some of the human problems with inattention, which cause the vast majority of accidents, but recognition of their environments and recognition that a subset of legal activities are safe at a time are still major problems. ",1541126311.0
xeyve,I would think it'd be more efficient to anticipate a statistical average of everyone and reacte to edge case in real time. ,1541123520.0
AtticusRoberts,Sounds like a massive invasion of privacy to me,1541124649.0
ConfuNineTwo,"I know very little about self-driving car systems but a couple of thoughts;

How would that work when different people drive the same vehicle?
How does anticipation fit into a self-driving car model?

What can the self-driving car do to avoid the problem before it happens without potentially having negative flow on affects for traffic behind? 
",1541123326.0
jhillatwork,"In theory, *yes* given sufficient time around individual drivers, computing power, and storage capability.

Is it practical? *No*, because the value of the resources needed to handle the worst case (inter-country commercial drivers) far exceeds the value its utility.

Is it ethical? Also *no*, not just because of breach of privacy & right-to-be-forgotten laws, but because of a bigger issue. Suppose the ""worst driver in the world"", someone who is not only inexperienced, but outright aggressive; suppose this driver is also in a remote part of the world where law-enforcement may be poorly resourced. Such a planner system would allow this driver to continue to be horrible due to lack of negative feedback (i.e., the accident such a driver may cause). Yes, it's horrible to say that we should permit bad things to happen to others, but remember that we all learn that fire is hot by getting burnt.",1541185025.0
WhackAMoleE,"Yes Comrade. Then the autonomous cars can report speeders and other miscreants to the proper authorities. You yourself have already been reported for subversive thoughts. The authorities are on their way.

Ye Gods.",1541178314.0
CuriousSlide,"Im a student who just graduated from university. At the time of graduation I was sure that I would study Theory of Computation for masters however that did not work out and now i am working. I plan to work for another 2 years before is start applying for universities again. During these two years I want to study as much as possible so that I can prepare myself for university.

&#x200B;

I want to study Theoretical Computer Science because somehow reading about it makes me realy happy. I do see myself getting a Pd.D in this field soemday. 

&#x200B;

During my undergraduate I taught myself theory of computation by reading the book on the same topic my michael sipser.

&#x200B;

I am really impressed by the complexity involved in the whole of computer science in general; but i find myself being drawn more towards the theoretical computer science.

&#x200B;

In theoretical computer science I am interested in Programming Languages and logic.

I am not very good in mathematics but somehow for reasons that I do not know, i love reading about things like Lambda calulus, first order logic etc etc. 

&#x200B;

I also love how functional programming. Currently I am going through the SICP book.

&#x200B;

Parallely I am taking a series of courses on coursera about programming languages (university of Washington)

&#x200B;

The reason why i am writing this is that I have no-body to talk to who can kind-of guide/help me with all. College professors are no help at all.

&#x200B;

Recently I am also finding myself being drawn towards languages (human languages) (turns out that there is considerable overlap between the study of programming languages and natural languages)

&#x200B;

Moreover i am really conflicted because recently someone told me that learning something just because it makes me happy is selfish and instead i should try to learn something so that i can help society.

&#x200B;

Am i overthiking this. Am i being stupid ?

&#x200B;",1541339501.0
firmretention,"Hard to know without seeing your code but a few examples

* undescriptive variable names
* not following language conventions (e.g. camelCase for Java)
* line width too long (a common rule of thumb is max 80-100 chars per line)
* long functions that do multiple things rather than calling multiple smaller functions dedicated to one task

Nice code is pleasing to read, consistent in style, follows the conventions for whatever language you're writing, and allows one to easily and logically follow what is going on (i.e. not spaghetti code). ",1541118266.0
ryclorak,"Ok so I'm just learning so maybe don't listen to me at all, but:

Spacing helps with readability (separating variable declarations by a line, then having other code similarly chunked will help)
Uhh if you're doing functions and just longer programs then maybe there are simpler/""more elegant"" ways to work through something.
Maybe it has to do with your variable names? If you use random words/letters that don't relate to the days, that can be hard to follow.
Aannnd I don't really know what else. Comment stuff to clarify what it's meant to do? 

Ideally your professor or someone else with experience will look at your code and tell you what's up with it, but maybe these suggestions are useful!",1541118067.0
CyAScott,"Some lint tools can help enforce coding styles, like Resharper. Everything from simple stuff like white spacing and casing styles, to complex stuff like imperative vs declarative can be done.",1541122709.0
future_security,"I hope you post the example if they happen to be right about the code. I have an interest in programming languages and so I'd like to see a messy coder's idea of clean code. :)

What are dev notes? Note-to-self type comments? That might be enough, depending on the readers taste. That and leaving lines of incorrect code commented out. 

If it's not a first year class then you're likely expected to plan ahead larger parts of the project before you even start typing. In computer science education understanding how to implement something is the important part, not learning how to program. A kludge isn't considered acceptable even if it compiles with no warnings and produces completely correct output. If you're basically working in a type-compile-fix-compile-test loop, then it's a lot more likely your code will be less coherent, buggier, and harder to maintain. It's good to minimize those problems once you start working in groups because those problems compound when you add more humans. Plus you'll use your time more productively if your code doesn't need constant testing and tweaking.",1541222440.0
thecrazypriest95,#begin again,1541101885.0
thecrazypriest95,"#Proffessor Oak
Men
#Meet
Cristian odan a
A+ is faster
#python#42##F
Ound out.....
@#$_&
Finale CLUE.....
197",1541090480.0
tulip_bro,"This doesn't belong here, IMO.",1541080660.0
Stop_Sign,"As a lazy interviewer, I'll probably be using this list to interview candidates.",1541081448.0
readet,"Let me just save this to never look at again.

Thanks!",1541080119.0
Ruko117,"I'm in a similar place to you so definitely wait for a more knowledgeable person to comment before you call your question answered. As far as I know, in category theory we don't talk directly about sums, however using category theory we can implement sums with coproducts. So, a coproduct is more general than just a sum, but in most cases and especially in the case of programming language type systems, the terms are functionally interchangeable. 

&#x200B;

I found a [Stack Overflow](https://stackoverflow.com/questions/43572970/is-coproduct-the-same-as-sum-types) thread about this question.

&#x200B;

Also, let me know if you've found any good resources or methods for learning this kinda stuff! I've mostly just been jumping around Wikipedia, Stack Overflow, Reddit, and random people's blogs. ",1541076148.0
univalence,"Indeed, no, there is no difference.

Interestingly, the *product* type can be different from a categorical product in certain situations (e.g., if you have a type theory for an effectful language... [Paul Levy's notes](http://www.cs.bham.ac.uk/~pbl/mgs2014lam.html) are quite good here), but the sum type cannot.

Why no duality here? The ""two types"" (from Levy's notes) of products correspond to the additive and multiplicative rules for sequents. In order to separate these two rules for products, we need to have multiple assumptions/input-types, which we can do in type theory, but in order to split additive vs multiplicative sum types, we would need multiple conclusions, which we usually can't do.

Or in other words, the sort of sum types we have access to are restricted because type theory is intuitionistic.",1541108877.0
felis-parenthesis,"In type theory? I don't know.

In Group theory? It looks like the category of groups doesn't have a coproduct. How would you make the disjoint sum of groups A and B into a group? It works fine for multiplying an element of a A with another element of A, just use the operation in A. But how do you multiply an element of A with an element of B; these could be very different groups. Worse still, the morphisms from A or B to the coproduct have to be homomorphisms of groups. So you have f:A->AxB and f(A) has to be some kind of variation on A so that f(xy)=f(x)f(y). Meanwhile g:B->AxB must obey g(xy)=g(x)g(y), so g(B) has to be some kind of version of B. WTF? 

But there is a clever construction, the free product, that interleaves an element from A and an element from B and another from A and another from B and yet another from A ... as much as needed. It actually works. There is a coproduct in the category of groups. And it causes [puzzlement](https://math.stackexchange.com/questions/1121243/coproduct-of-groups) because it is ""bigger"" that the product.

So that is an example of a coproduct that isn't essentially a sum.",1541105686.0
flexibeast,"Don't know of a book, but you might find [the ""Computational Complexity of Games and Puzzles"" page](https://www.ics.uci.edu/~eppstein/cgt/hard.html) interesting.",1541074956.0
GoAwayStupidAI,Try cross posting in r/GraphicsProgramming there are a few OpenCL developers there. ,1541052844.0
jellysci,Your link doesn’t work for me. Is it cut off?,1541054803.0
,[deleted],1541091259.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/graphicsprogramming] [Parallelizing recursive octree dual algorithm with OpenCL](https://www.reddit.com/r/GraphicsProgramming/comments/9t9d9p/parallelizing_recursive_octree_dual_algorithm/)

- [/r/opencl] [Parallelizing recursive octree dual algorithm with OpenCL](https://www.reddit.com/r/OpenCL/comments/9t9aiu/parallelizing_recursive_octree_dual_algorithm/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1541078153.0
BuckDollar,Have you tried rebooting? Or posting to /r/iamverysmart?,1541051050.0
IdealImperialism,"Umm, that's a pretty convoluted question which and you can argue for each case depending on what 'memory' you mean.

The program reads/writes to virtual memory, the driver might buffer IO and write as blocks and the OS does the actual writing. ",1541034839.0
rich-zachary,"B. Database driver
The operating system is in charge of taking care of how memory per program is stored in general. The database driver is what is responsible for reading from the hard disk into memory in such a way that is useful to the programmer. The program can then use a database driver to query data from a database. ",1541038565.0
woojoo666,"I guess this is more math than CS, but the fact that there will always be assertions that are impossible to prove. I always thought given enough time we could prove anything. Godels incompletely theorem proved me wrong",1541033093.0
SOberhoff,"It's hard to pick a winner here because one tends to get used to ideas over time. For instance, I think we've all become blasé about the universality of Turing machines at this point.  
Let me just name something that's recently intrigued me. [Zero-knowledge proofs](https://en.wikipedia.org/wiki/Zero-knowledge_proof) allow one to establish a proposition via an interactive protocol such that the interrogator doesn't learn anything other than that the claim is (almost certainly) true. So if somebody else wanted to later hear the proof from the interrogator, the interrogator would have to answer: ""Sorry, I just know that it's true. The prover was very convincing."" Furthermore, it's quite straightforward to show that zero-knowledge proofs exist for every problem in NP. ",1541034448.0
erethismdesign,"Cleverbot has learned the prequel quotes, so I’d say it’s a friend to r/prequelmemes.",1540990768.0
grautry,"Well...

https://en.wikipedia.org/wiki/ELIZA

Your test was arguably passed in the 60s. ELIZA simulated a psychotherapist and many apparently found the AI quite a helpful and positive influence on their lives.

I'm not sure whether that'd qualify as a ""friend"" in your definition of the word, but many users were convinced that ELIZA *understood* and helped them.",1540990443.0
theLucasEffect,"This lil robot, [ElliQ](https://www.youtube.com/watch?v=W-j8htpoyx4)  is being tested on real elderly patients around SF now. Patients have been known to confess things like loneliness and depression to the robot when nobody else is around.",1540993656.0
Lanko,"I...

Talk to Google home on days when I'm feeling especially lonely.  

I do wish she had a chatbot option.  
",1541005470.0
WhackAMoleE,Are you thinking of a cat?,1541004533.0
mithra97,Now were getting to the real questions of being a computer scientist.,1540994430.0
ibuysleep,r/me_irl,1541006299.0
zitterbewegung,One would think that to be a friend to someone you would be able to pass the Turing test .,1540992105.0
omgitsjo,"I really like this idea.  I think the test, as defined, has already been passed, but it's still a nice metric.",1540994416.0
jassisinghbrar,Can you please lend me a grand?,1541001723.0
ImaginationGeek,"Getting away from the “can a bit be a friend question” and back to the “What would be a test for it”...

Off the top of my head, I’d say it would be different from the Turing Test because for a bot friend to be indistinguishable from a human friend, it would first have to be indistinguishable from a human.  So passing the Turing Test would be a prerequisite to that approach, which may not be very helpful...

One approach would be to define “friend” more precisely, and then evaluate if the bot meets the more specific criteria.

Another approach would be to have humans interact with it (probably knowing full well that it’s a bot) and then give them a survey with questions about how they felt.  As a control, you may also have some humans interact with other humans and then take the same survey.
",1541014191.0
amazondrone,"As others have mentioned, it depends a lot how you define friendship; it's a VERY broad term.

I submit that one defining characteristic of a friendship, like any relationship, is that it builds and changes over time based on (amongst other things) shared experiences. (Without that characteristic it's probably more of an acquaintance than a friendship.) I'd say that'd make the programming of a friendship AI significantly more challenging than the technology we have today (even if it doesn't need to pass a Turing test).

Practically speaking, the Turing test is a standalone test; the AI isn't required to remember anything about its interactions with a participant between tests. Given the above, a friendship test couldn't be standalone in this way, making it a lot harder to define the test.

Perhaps it'd be simpler to remove the friendship part of your idea and concentrate on the alleviating loneliness part. That removes a lot of the ambiguity and although ""alleviates loneliness"" is still subjective, I think it'd be a lot easier to gauge.",1541016627.0
makethat2largepizzas,https://replika.ai/,1541021367.0
girlBAIII,Isnt that just the Turing test?,1541028080.0
nom-de-reddit,"Yeah, I got a friend test... she doesn’t get drunk and hook up with you then tell you it means nothing then give mixed signals and just keep badgering you for alcohol every time you get together then tell you how much she isn’t into you but do you want to buy her some more drinks then ghost you leaving you feeling like shit...

If your bot can pass that test then ironically it will be a better friend than expected.",1541006923.0
picnicnapkin,"You always see the gamers in first year with their razer laptops or other oversized gaming machines. In my experience, a lot of them flunk out because they spend too much time gaming. I wouldn't worry -- there is always a fairly broad spectrum of people who are into lots of different things. ",1540934549.0
mikolv2,"Despite what stereotypes you might have seen, not all CS student are these hardcore nerds. Yea CS probably attracts higher proportion of nerds but it will be, for the most part, just like any other group of people. Some nerds, some into sports, some into music, some into all of these things, some into none, unless you have really niche interests there will be someone else that's into the same things. Besides, you all have shared interest in computers/computer science, that's enough.

Remember, making friends on your course isn't the be all and end all.",1540934852.0
StannisBaratheon_AMA,"I did bodybuilding and played in a punk band while I was going through my CS courses, and I dont play much video games. While you might not get that initial spark of a shared common interest, by the time you get to your upper levels, you've all been through the same hell and, as the old saying goes, misery loves company.  

EDIT: PROTIP. Force yourself to be a little out going if youre not already. Ask people how their assignments are going. Every one loves to bitch about their own struggles and its a great way to meet and get to know other people. ",1540935944.0
Relmnight,"From my experience that shouldn't be a big problem. There are all different kinds of people and even quite a few who don't enjoy gaming, like you. 

A friend of mine does not play anything either, and messes with cars in his free time. Another likes to go snorkeling when she has the time and money. 

As long as you have something that interests you, you should be fine.",1540934618.0
The_Real_Tupac,"From my experience you have less gamers with their massive, colorful laptops and more of engineering type guys. Pretty normal overall just good at math. The one stereotype that held true for me was the lack of girls in cs courses. The ratio was like 30:2.

If you want to make friends do your homework in the school lab and talk to your classmates about the assignments. There will be plenty to talk about.

Also I made my best friends outside of cs through...drinking I guess lol.",1540935977.0
xkqd,"On the flip side, some of the most talented engineers I know are highly active and take part in a ton of outdoor activities.",1540937012.0
nclos,I’m a student in CS at a large southwestern research university. I am very similar because while I used to play a lot of video games I don’t anymore. I have not had trouble making friends in my classes. Besides your usual ‘basement dwelling’ people in CS at my uni are very approachable and don’t care if you game or not.,1540934421.0
ejrh,"Nope.  I was into gaming when I did my degree; I made no friends at all!

Basically you'll have (possibly) trouble making friends for a plethora of reasons, but gaming is pretty far down the list.
",1540936772.0
aviddd,"If you don't click with people in your CS program, make friends in other departments. Do that anyway. You'll be a more interesting person when you graduate. ",1540941257.0
khedoros,"No, you won't have trouble; a fair number of even CS students aren't interested in gaming, and those who are will also (usually) have other interests as well. Among my college friends are a good number of CS majors, but also agriculture, business, computer information systems, various kinds of engineering, finance/accounting, and so on.

The people who are really in the degree *because* of games usually won't last long, unless they branch out.",1540937419.0
eka5245,"In my experience? No. You’ll have trouble making friends because most people will be hard to talk to in general. Most of the CS kids where I went weren’t that into gaming, just computers and programming. 

There was a special program for gaming, and the computer graphics course was graduate (and special exceptions) only. 

This experience might have also been skewed because I’m female. ",1540935380.0
zsaleeba,"No, that won't be a problem. All sorts of people do CS.",1540937312.0
NumbersWithFriends,"Of the few friends I remember having in undergrad CS...

* Two were hardcore gamers
* One was super into running and weightlifting
* One was a musician (double majored in Music Production or something like that)
* One was into DIY electronics/hacking together little projects
* One played frisbee golf almost every day
* A couple loved camping and hiking ( I went with them a few times too)

Like others said, there's always a bunch of wannabe game designers going into CS freshman year, but they usually drop out or change majors by sophomore year when they realize CS is more about math and theory than creating programs.",1540948207.0
LOLayto,"You’ll be fine, I might only be doing A-Levels but I’m not into gaming much more either. It doesn’t come up that much and if it does, you can make a joke about it ",1540936788.0
apstls,If you’re actually posting this seriously then I really wouldn’t worry about it ,1540960938.0
afiefh,"The real question is who the hell has time for gaming while doing a CS degree? Most people I studied with preferred sleeping or actually talking to other human beings about something not involving multivariable calculus.

Also, is this really a thing these days? People call themselves ""gamers"" and build their identity around that? Back in my day (and I'm only early 30s) everybody played video games to I've degree or another, but nobody let it define who they are or who they'd hang out with.",1540983698.0
Ramin_HAL9001,"Software engineer checking in:

I lost interest in gaming back when I was a kid. I used to like puzzle games and RPGs. I tried to make my own games, but found that the process of figuring out how to fit algorithms together into a functioning program was in itself the ultimate puzzle game, and that your persona in the world of open source software is sort of the ultimate RPG, with achievements measured in how much useful code you write, and skills being, you know, actual skills.

Basically, building software is the most fun ""game"" there is, and you are rewarded with much more than meaningless points or virtual achievements. I don't belittile people who like gaming, but I prefer not to partake.

Also, I personally believe your friendships should be based on more than common interest. Some of my friends are software engineers, but some of my friends are not. Whether or not I fit in has more to do with me just being friendly and willing to hang out and listen to people and be engaging in conversation, regardless of what the topic of discussion is.",1540991593.0
BuffWalnut,"I've started my study this year, and am not at all into gaming except for the occasional nba2k with friends. I haven't at all found it hard to make new friends while not into gaming. There is always a group with the same hobby's as you. Don't worry about it!",1540934924.0
locotxwork,"Nope.  Graduated with my CSE degree and didn't play games . . I did my work . . .partied . . . met girls . . . did my work . . .worked  . . and kept my eye on the price.  Getting my degree and getting the hell outta there.   There's a tribe there that is CSE and not into gaming, find your tribe.   Join some social organizations too.  Can't do much gaming if you are in the Nature Walk Society . . or . . something like that.",1540935008.0
CookieLust,"I work with all types. Former gamers, never gamers, current gamers, etc. You'll probably fit in somewhere.",1540935534.0
GloriousLaserChicken,"I got into computer science because I wanted to make video games. I'm still a student currently, but my interests have expanded so much in recent years, that not only can I see so many other ways to apply my skills in useful ways, but my interests in other areas of my life expanded as well. So while gaming may be a key interest for many, it's not the only one.

Besides gaming and programming as hobbies, I'm also an avid paintball player, like to occasionally doodle or make some random bit of art, I love to cook (especially barbecue) and make drinks, as well as hang out with my friends, watch movies, listen to music, etc. Gaming has also become less and less a part of my life as well, probably for the better.

Honestly you should just be yourself. Don't worry about conforming to other's interests simply to gain social standing (though that's not to say you shouldn't take an interest in what your friends find interesting), because you'll probably find lots of other people who share your interests as well. ",1540936848.0
spicy45,"No problems. Believe it or not, it's not all nerds. Lol",1540935834.0
0xAAA,"I’m a CS major and pretty much everyone is pretty nerdy and into games. Doesn’t mean you can’t find people that you vibe with though. 

For me I like to party and drink with girls so I joined a frat. If your into that stuff I highly recommend. I don’t understand how CS majors play video games. We already have to sit in front a screen for so long. ",1540942530.0
ThreeHourRiverMan,"Nope, not at all. I didn't game at all when I was in school, and I went back for my CS degree at age 28. I wasn't exactly a shoo-in to fit in, but I made plenty of lifelong friends. Some of whom I workout and play basketball with. This isn't exactly a niche field any more, there's a lot of us with a wide range of interests and abilities. You'll do fine. ",1540943300.0
DeluxeMarbles,"I don't think you'll have trouble making friends, but in the case that you do, there are always non-cs students there as well!",1540946879.0
Hellbasedgod,"As someone who is an avid gamer and a CS student, don't worry about it. There's all types of people in CS. People who enjoy hunting, working on cars, bodybuilding, sports, people who are parents, etc. Those gamer Freshman won't be there for long, trust me. ",1540946912.0
JK635,"As CS grows you find that you'll get a diverse group (like any field). I wouldn't worry about, I don't have that many friends in CS, but a bunch in a whole lot of other faculties. Join any club and you won't have a problem",1540948628.0
thecuseisloose,I was a big gamer before college but pretty much stopped when I got to college because there was so much other shit to do. You’ll be fine. You befriend the people you want to. ,1540948839.0
sjh919,"I used to play vidya a lot my first year but dropped off because it (along with a couple of other factors) were getting in the way of my academics. I still do it sometimes casually when I'm on breaks but not nearly as much. I know many others in my CS that don't touch gaming, and they tend to be the smarter ones imo. You just have to look around",1540948891.0
pythonicus,Join clubs and what not. Your social circle doesn't have to (and shouldn't be) just CS people. I didn't hang out socially with anyone in the CS department.,1540948921.0
EScar21,"Current CS student, na man you'll be fine everyone does their own thing if you game you game if you don't you don't.  You'll make friends there's always an interest like shows ect, i'm a heavy gamer but I'm about to graduate with my degree and I have friends that game and friends that don't so I mean you'll be fine.  The only issue is most of the time we have very poor communication skills ECT but just chat around you'll find someone that has the same interests as you ",1540949404.0
pastermil,"Computer science students are students; and students are people

Just think of them like regular human being

Some are into gaming, some are into other things (as well); simple as that

source: comsci graduate",1540949797.0
simplethingsoflife,I'm an accomplished architect and don't game. Computer science is far more than games. I use it to try and save real lives... not fake ones.,1540950658.0
trippleequal,I mean you say you'll begin studying CounterStrike next year so I think you're good.,1540953535.0
PC__LOAD__LETTER,"I’m not at all a gamer (besides shitty mainstream stuff like FIFA and Halo) and I had a great time in college. Granted, not a ton of friends in CS, but I was friendly enough with people and not once did I feel like an outsider because I wasn’t a “gamer”.     
      
Honestly I think it’s a little strange that you’re expecting CS majors to be into stuff like manga and anime and fantasy. Maybe my experience was out of the ordinary (I doubt it), but I didn’t know many people at all - that stuck with CS - that were openly/loudly into that type of thing.",1540953790.0
portableoskker,"When I was in CS in school, I was really in to cycling, sports, and working my butt off to get through my major.  I did game occasionally with folks in my major but it was always social, like ""dude, you gotta try this game"" or something like that.

&#x200B;

CS is even broader now than when I went in to it -- lots of people give it a go.",1540954540.0
curt94,nope.  only about 10% of my co-workers are interested in games.  ,1540954792.0
RatherPleasent,"You usually get along with everyone. I don't know about long term friendships, I commuted to college, studied a lot when I was on campus, and really made a small handful of strong acquaintances. 

The only time I ever avoided super nerd shit is when a group of dudes were playing their 3DS's and talking about MLP before a class. 

That and when some guy brought up hentai casually to make a comparison to some girl in class the first time I met him.

Other than that, I steered clear of l33t haxxors on campus. ",1540957655.0
deong,"There will probably be CS students who are into it because they have an interest in mathematics or AI or computer graphics or just general programming. In my experience, people who choose CS because they're into gaming don't last that long. That's not to say that gaming is a problem; just that if that's the main reason for studying CS, you won't find the motivation to wade through the math and theory classes. Which more or less leaves CS not too different from any other discipline in terms of the variety of interests of students in the field.",1540959271.0
cubravk,"4587079776
8112861614
6644905234
1498241000
0586242477
3788329464
6514945582
0946793357
8136588035
4738177761
",1540960790.0
J3T6B,I never gamed once in college. You should be extremely thankful your not going to school gaming. It does zero for you. You will make so many friends don't worry bud. Worst comes to worst an hour or two of gaming a day wouldn't hurt anyone. Obviously make sure you get your shit done first. ,1540961297.0
Actually_Ted_Cruz,Almost none of my co-workers play video games. Where are you on alcohol? That's much more important in tech.,1540964511.0
dankmeems,"That's just the ""IM GONNA WERK 4 VIDEO GAMES"" crowd.

You'll have a hard time making friends with fellow CS students because we're all anti-social shut-ins, not because you don't play games :D

And FWIW uni friends are nice but I didn't stay in contact with any of mine after uni. Some people say the connections they made in tertiary education are important, but I haven't found that to be the case.",1540964848.0
6lick-6lug-6lucker,"Depends on where you go. Yeah there are some people into gaming that are into CS and people that aren't into it.  Just avoid the ""I'm way smarter than you and everyone else"" types because they are toxic as fuck. ",1540966788.0
bubbles_of_love,"I've never really been into video games,  and I'm also very bad at working with computers xD

Having to properly install programming languages and set up environment variables for them is something that eluded me, and Im in my 4th year.

Pretty much everyone is super helpful and nice and gets you through it xD and most of the profs are too",1540967324.0
ryanstephendavis,"So what are you into?  Hang with those people and work hard in your degree....I was into drinking and playing music, found plenty of cool people to hang with:D",1540967842.0
VBassmeister,I'm into gaming and anime and I still don't have any friends! :D,1540969662.0
trenmost,Not really. I have many friends from university who are not into playing games.,1540969805.0
mghoffmann,"You might actually get married in college.

But seriously, computer science degrees are rigorous engineering degrees at most universities. You'll bond with your classmates over the struggles if nothing else, and a lot of your gaming-obsessed classmates will drop out or flunk out quick if they don't keep things in check. Being well-rounded and interested in things outside of your degree aren't bad things.

I like rock climbing, playing music, building things, gardening, and video games. Everyone's different and that's fine.",1540971665.0
Blue_Q,"No. In studied in germany and at my university, only a small percentage of my friends were into video games. To be honest, we were far to busy with learning and starting businesses to worry about games. Some of the people in our CS courses intended to study CS because they wanted to make video games but eventually dropped out because CS wasn't what they expected. Sure, there are gamers who are able to finish a CS degree, but it's rarely hardcore gamers.",1540973481.0
pjrocknlock,"CS major here. I’m a MMA fan, train Muay Thai and BJJ; also take music lessons. Here you go. :)",1540974833.0
PrivilegedVoyager,"You would have difficulty in fitting in social circles if you are anxious. It shows. People at some level like diversity..
Just be nice and confident and polite.

It is possible that there are people who game only to get into the circle but are completely different people...maybe you can be the one who makes them feel comfortable by being you.

Also, we take ourselves too seriously. Chill out.",1540980977.0
AndreasTPC,"You'll be fine. There will be a wide variety of people with all kinds of interests and hobbies. In the CS section where I studied there were ""everyone is invited"" parties and dinners organized on pretty much a weekly basis, a group of people who were into athletic activities and organized group exercise events anyone could attend, a board games club, etc. You'll find something.",1540984167.0
jake_schurch,"I think that you may be putting the majority of CS students into a one-size-fits-all box.

Of course the true answer may largely depend on a combination of different variables, such as the size of the college as well as the program. But the majority of students (CS majors/minors) that you will meet will most likely have a diverse set of backgrounds, personalities, and interests -- among many other differences.

I know from personal experience that it can be anxious to start college, and I have two things that may be of help to you or someone else going through a similar experience:

1. Things take time, you usually can't force them. The best mindset to take, may be to be somewhat passive (I.e. not aggressive) when starting off and kind of go with the flow, gain insight into your experience and adapt. Meaning that if you go to college and see that all of them are gamers, adapt. 

2. It may be helpful (I know it was for me) to find friends outside of your CS studies. Now I don't know if you're program allows/makes you take a common core of classes (mine did) which allowed me to bond with students over interests that were largely non-technical or related to my area of study. It was also really glad to have that group of people so I wouldn't have to talk about CS all the time.

I hope I was able to provide some insight! I wish you the best of luck in your new collegiate role.",1540985058.0
TKAAZ,"From my personal experience, my program was filled with quite a lot of /r/iamverysmart people with superiority complexes over getting into CS, the occasional bronies and gamers. Most flunked out or became more chill as time went by. Besides that you have all kinds of people, with highly varying interests, and I reckon you will have no problems making friends.",1540985473.0
SlightlyCyborg,"I gamed a lot in HS, but I rarely gamed in college. I didn't have time for that and I wanted to be the best programmer I could become. I didn't really have friends though.",1540986644.0
EnjoiRelyks,"Nah you’ll be fine. I don’t game all that much and made a few friends in CS. 

**Bit of advice:**  
You’ll likely have to do group projects. If you want to find a good partner, go for the non-traditional students.  

They’ll expect you to carry your weight but they’ll often do their part too. 

I was non-traditional and partnered with non-traditional after learning the hard way. We came back to college for a reason. We can’t afford to be flippant. ",1540987746.0
Alwaysafk,"Lots of CS guys do things outside of PCs hobbies. Wood shops, lifting, hiking, sports whatever. You'll see more gamers than other fields but there are a lot of people with other interests.",1540988043.0
nondescriptshadow,None of my CS friends were gamers. We got along just fine over pizza and soda though,1540988202.0
frody1111,"I majored in game design and played absolutely no video games outside of classes. You will make plenty of friends. You will be in classes with the same people over 4 years so I think you will just get used to seeing each other often and that, over time, will translate into friendship.",1540990011.0
LeftBrainCo,Meetup (app/site)is a cool way to meet others with similar interest.  I made some really cool connections/friends that way.  Also makerfairs!  Just ask people about their projects.  They will have plenty to talk about.,1540990406.0
CrackenZap,"Yes, but if you game A LITTLE it helps. Just don't call yourself a ""Gamer."" Play something YOU like.",1541004537.0
TheCodeClown,CS is super casual now compared to 30 years ago. A lot of the hardcore nerds without social lives that use video games as a social crutch are no longer participating in CS alone. I'd imagine most courses where you only find weird anti-social individuals would be the more advanced mathematics and chemistry courses. Could be wrong.,1540942265.0
secrets_meow,Fuck those dorks ,1540948835.0
ctags,"Probably. 

I've found that not caring for video games has cost me a lot in my social life. I'm not sure how much bearing CS has on it though, I assumed it was just a general societal thing.",1540934523.0
fantom530_1997,"Hi, I’m a CS major and I’m also not a big gamer so I understand EXACTLY what you mean, there’s a fair amount of CS undergrads that are obsessed with gaming. My advice is to find a group that you’re comfortable with, like with clubs. There’s a Linux Users group and CS tutoring club at my university and I fit in much more with them than with my housemates playing LoL.  Once you find a group with like minded people that want to succeed it’s amazing what you’ll accomplish :) 
P.S. my fav high school teacher told me that my income will be the average of the 3 ppl I spend my time w the most. If there’s any truth in that, then I recommend spending time with like minded ppl :) Good luck!",1540943328.0
danhakimi,"Maybe. It won't be easy. They aren't outgoing, for the most part, so you'll have to be. Some of them are gamers, some of them are loners, some of them are friendly but shy... Some of them are loud but strange... some of them are sketchy enough that you'll want to stay away... There are a lot of different reasons why CS students tend to be the ways they tend to be, so good luck navigating that.

Or, you could worry less about the CS classes, and more about student clubs. What activities do you enjoy? If the list isn't very long -- go to the club fair or whatever, and make it a little longer!",1540935205.0
BenjiSponge,"Maybe ""high level"" means something else outside of the states, but do you just mean like a high ranking university, or do you mean going for a doctorate or master's or something? Again, not very familiar with the education system outside of the US.

I suspect you're way overthinking this. If it's just a high-ranking university, there's a great chance they don't expect you to know anything about CS at all, and I might even recommend you pick a different major if you really want to go there because what you're describing is a huge part of a CS degree, and you'd only need a couple of courses to round out the degree. Given what you say you know, you could probably get an entry-level position in C++ right now if your real goal is to get a job in the industry.

In all cases, the best thing to do is probably to reach out to the university. If you're a good fit, they will actively work to get you to apply and attend. If you're not, they'll tell you that. If you want a less formal process than a full application, you might just reach out to a professor or student there and chat with them. In my experience, most people in the world like talking to intelligent people regardless of intentions (as long as they're not malicious). So don't be afraid to send out a few cold-call emails to staff or students (but don't stalk them!).

Best of luck. =)",1540907186.0
atk93, Your algorithm knowledge looks pretty good from a use case stand point but there are also course on solving NP hard problems. And designing algorithms for quantum computers. You didn't mention anything about system architecture which is an important foundation of all forms of computing. There is courses in AI and machine learning ( not already the same thing). You can also study compiler and programming language design. There are courses in Non-procedural programming. And many more. Most universities expert you to have minimal or no knowledge of anything before you get there so everything I mentioned is second third or fourth year material in the University I went to. The first year is covering what your said you know. At the University I went to we have an honors intro course that is what I would recommend to someone with your background.,1540909890.0
pissedadmin,"> I am concerned that this lack of experience in other parts of CS would make me struggle really hard

Just apply. If you get in, you're qualified.",1540916287.0
iwantashinyunicorn,"Scottish universities don't have specific CS (or even programming) requirements for undergraduate entry, because they like accepting English students and the quality of secondary computing education in England is rather, er, variable. A good chunk of your first year at Edinburgh will be spent learning Haskell, partly because it's sufficiently different that even students who can program already will have to work hard at it. Instead, entry is based primarily upon your maths and any science exam results.",1540919540.0
clownshoesrock,"Apply, if you get in, then don't fret how qualified you are.

Even at the prestigious universities, you can get ahead with medium smarts, and very good study habits/discipline.

And even if you did lack the brainpower, which I doubt, it's still better to give it your all, and fail, than to never know if it was possible.

Many of the classes need to be passable by non-majors, who aren't steeped in the same background.  I think you'll find that some of the classes will be kinda dull.

Anyway, don't build it up in your head too much. The real benefit of a prestigious university is the contacts you will make.  Those contacts will open doors in your future,  either in academia or industry.  When you have dozens of people who think well of your work ethic, and think you're reasonably bright, you will always have gainful employment.

  ",1540956672.0
Mic5RS,"Thank god for the comments, I also probably want to study CS at Edinburgh and don't understand half of what you just mentioned.",1540920632.0
fluffypiranha20,"Try applying to the schools that have good options. If you like the engineering prospect of things, focus on a school that excels in that but not too hard to get into. I initially chose CS but two years into I realized i didn’t like the topics anymore. Good for you to love programming but it changes when you’re forced to use ide’s you don’t like and are taking background Cs, like data structures and data assembly. Find a school that feels right being there, where you can see yourself living there, has good academics in the general area of what you want, and is affordable. You don’t want to be set so hard on a major because you’ll be miserable for a while contemplating on changing it. CS is competitive. You have to put everything into it to be on top. I’m sure you’ll figure it out, good luck.

And it’s a mix of gpa and experience. You don’t want to have a shitty gpa but you don’t need a 4.0. For cs, projects are crucial in showing your ability. But there are going to be plenty of them at a large school like Michigan state. As long as you choose a school that isn’t known for awful cs you’ll be fine. You don’t need Stanford to succeed and have a solid career. ",1540899760.0
Mieko14,"Yeah, IMO it’s not worth it to go to a really expensive school for comp sci unless you get a decent scholarship. You’d be much better off going to a less rigorous school and using the extra time for a job/internship/research opportunity. 

I go to University of Hawaii for CS and most of the people I’ve met from out of state chose the school for the same reason I did, which was “No one is gonna really care where I went to school, so I’m going to Hawaii.” 

I’ve met quite a few high-ranking, successful tech people who have degrees like “Associates Degree from [community college] in Performing Arts.” As long as you can pick up some work skills and experience in college while doing some networking, you’ll be good to go. ",1540901480.0
MTGplayer1254,"I would first make sure you actually want to get into CS. My good friend way in your shoes a few years ago.  Loved AP cs in highschool and hates it in college. 

I would also check up on the job opportunities in CS and see where you wanna go, as alot of people confuse coding and CS , forgetting computer science is a science and is much much deeper and complex then that


Just food for thought.  

Sorry i didnt answer your main question, good luck though ",1540888042.0
mashleys,"It’s better that you go to a school that works for you and where you will be able to learn the material vs going to a school where you’re in over your head and you’re struggling to get decent grades and learn the information. The benefit that a school like Stanford, MIT, etc. has is networking and it sounds fancy, but you can easily get networking at other schools too.  I have found that once you get your first internship companies are really more interested in that than your GPA. You’ll be fine at a state school.",1540894731.0
tending,ELI5? I know what an SMT solver is and still don't understand what they're looking for in the bug reports.,1540909581.0
psyc0de,"Very cool, an interesting approach to managing false positives. ",1540917061.0
thecrazypriest95,"#NoOnePlays
#Truth is found in both...........

Xxxxx!'$ aaaaaaand oooooooO?'$

ABC,
123
OODA Loop.
Times a loop.
No robots just signatures of voices.....

#BohemianRhapsody
#DontStopBelieving
#Don'tPanic.....

Life insurance company......

Sickness..... Religion.... Science..... USA=PBS

Ooga, booga, book, picture, meme.....   
Pepe......",1540869347.0
thecrazypriest95,#DontPanic!,1540848667.0
Wurstinator,"Just anecdotal: I applied end of summer, had my interviews in September or October and could start the internship October the year after.

MS is faster to react and to get all the paperwork done than Google in my experience. No idea about Amazon.",1540891095.0
lrem,"There are no deadlines because there are no set internship dates. As the companies hire globally, they have to accommodate for academic schedules that are wildly different. Add to that any particular situations students find themselves in and we have interns coming and going all year round.

What you need to know is that most internships concentrate in the summer break. I wholeheartedly recommend going for that time slot, as there will be other interns to hang out with and companies tend to have an event or five celebrating internships at some point in the summer. This means that you have to have the plan settled mid-spring, needing interviews early spring/late winter, so best have application in mid-winter.

Of course there are internal deadlines for going with the year's main intern-project matching. Thus, if you apply late, the most attractive sounding projects will have been assigned to the previous wave of applicants. But as long as the position is open, there should be projects that some teams thought are a good use of your time.

Source: I just waved goodbye to my intern in Google, obviously out of the main cycle. Our project was probably picked late because being a very advanced SRE topic, we probably failed to explain the potential impact to the reviewers. We may also have applied past some internal deadline on our side ;) The internship started after the big summer events have passed. Still, had a good time and she delivered great results.",1540891817.0
AnyhowStep,"Pathfinding can be thought of as an algorithm on a graph.

The nodes of the graphs can represent states.
The edges of the graphs can represent transitions.

Pathfinding finds the transitions that bring you from one state to another.

Some pathfinding algorithms are solutions to ""get me to this desired state"", others are ""get me to this desired state, using the least amount of energy"", and others are ""get me to this desired state, but also go through these other states"".

-----

For something like Google Maps, each ""state"" is just the ""I am at location X state"".

And each ""transition"" is just ""Take a bus/car/plane/boat to get from X to Y"".

-----

But you can do more with this idea of ""states"" and ""transitions"".

If you've played escape rooms, you can think of trying to escape as starting from the ""trapped"" state and finding a path to the ""escaped"" state. And this ""path"" isn't as straightforward as Google Maps' where you move from one location to another.

This ""path"" requires figuring out puzzles, collecting keys, backtracking, etc.


-----

You can even use pathfinding in one-player games. Like solitaire. Or sliding puzzle games. Or the Rubik's cube.

The start state is the ""unsolved"" state. And the end state is the ""solved"" state. And you want to find moves (or paths) that move you from the ""unsolved"" to the ""solved"" state.

It's still pathfinding but not in the sense of ""Move from X to Y location on the map""

-----

[EDIT]

You can even have pathfinding for games like tic-tac-toe where you start from the ""board is blank"" state and want to end in any of the ""I win"" states.

Usually, when you think of pathfinding, you think of Google Maps, or RTS games, or auto-navigation in MMOs, or enemy players in shooting games. But there's more to pathfinding than just that.

And there are many ways to find paths. Depth-first, breadth-first, A\*, Dijkstra, iterative A\*, Jump Point Search, hierarchical path finding, nav-meshes, nav-points, are some of the more obvious ones that come to mind.",1540839859.0
mikey__w,Have you done any research into the “Convex Hull problem” that is generally used when connecting dots and finding the longest or shortest route between dots,1540838389.0
beeskness420,"Last I heard Google uses a variant of the landmarks algorithm for quick routing you could check that out. Lots of path finding is different flavours if shortest path, A* search, and the travelling salesman problem. ",1540844981.0
onelordkepthorse,Grid based games use pathfinding algorithms as well,1541019930.0
ShiftedClock,"In a sense, procedural generation is the inverse of machine learning. For examples, you can browse /r/proceduralgeneration.

With ML you're given data and try to deduce a set of functions that reproduce that data (with error). With procedural generation, you design the functions that generate the data.

ML: given data -> learn functions

PG: given functions -> generate data

Procedural generation is usually done with computer code, and often generates data of the image, video, or sound varieties. However machine learning has traditionally learned a representation of the data that is constrained to a specific type of function. They're still functions, or ""computer code"", just not the kind a human would write.

There are people working to make neural networks generate readable code, which makes the PG/ML connection more apparent.",1540840168.0
NicolasGuacamole,"I think you’re just describing another part of Machine learning really.

Generative modelling is a part of Machine learning focusing on probabilistic models of the data. From these models we can easily generate new samples from the (approximation that we have of) the distribution the data originated from. For example given class labels we can generate plausible feature values for new examples from that class.",1540848194.0
lmericle,"For a random forest, you'll have to use some stochastic or else derivative-free optimization technique, e.g. genetic algorithms.

If you're using something whose optimization procedure is based in calculus, you can use the gradients ""backwards"" to find an input for which you get maximal/minimal activation at the output of your choosing. For neural networks specifically, you can extend this to find the input that maximally activates any neuron. Take a look here for examples with CNNs: https://cs231n.github.io/understanding-cnn/.",1540842035.0
ML_astrophysicist,"YES! This is exactly what we are doing here: 

[https://www.reddit.com/r/MachineLearning/comments/97jbbf/r\_analyzing\_inverse\_problems\_with\_invertible/](https://www.reddit.com/r/MachineLearning/comments/97jbbf/r_analyzing_inverse_problems_with_invertible/)",1540843190.0
TurdFurgis0n,"Now this is a thesis project! Design a machine learning system to favor picking a bad/the worst choice. It makes me think of the Wheatley core from Portal 2. Then what happens if you stack two-such systems on top of each other. Make the worst choice for the system designed to make the worst choice. Could you come back around to the best choice, like a double negative?",1540838464.0
CorrSurfer,"Formal verification!

There are a couple of papers in which models derived by machine learning algorithms are analyzed automatically, which includes answering questions such as the one you posted.

This involves mapping the problem to be solved to a problem instance for Mixed-Integer linear programming or other types of solver input and using such a tool for performing the actual analysis.

For the case of neural networks with ReLU nodes, there is a recent paper that provides a pretty good overview of the results out there: https://arxiv.org/abs/1711.00455",1540843133.0
uncleXjemima,Machine forgetting,1540882944.0
ImaginationGeek,"Google’s Deep Dream is kind of “reverse machine learning”...  but it’s also different from the exact thing you’re asking about, so your idea may still make a good research project...",1540841244.0
Tr0user_Snake,"If your model is differentiable, you can optimize inputs with respect to outputs using your trained model and a gradient descent algorithm.

DeepDreams, and similar ideas use this method to make trippy pictures.",1540882438.0
daermonn,"I read some article by an author I forget who asserted that, in some sense, cryptography is the dual of machine learning, as ML is extracting structure from chaotic data, where cryptography is taking some structured information and converting it into noise. I don't know if this is properly a dual though.",1540898507.0
HRK_er,Cant u train it to untrain data?,1540908597.0
hippomancy,"For some kinds of models, this problem is called “max a posteriori” (or in your case, min). The goal is to find the evidence (inputs) which maximize the resulting label.

In a more general sense, this problem is either generative modeling, where you try to generate data under certain constraints, or a search problem, where you are searching for the optimal inputs for a given output. Local search algorithms (like gradient descent) won’t work very well, though, because classifiers often are very convex functions on their input space and have local minima.",1540850151.0
HiggsBozo3-14,Algorithms to Live By,1540828973.0
WWFredRogersDo,"I think almost anything that gets into actual code is going to be very difficult to get into without visuals, so you should expect these to be more like casual discussion about programming concepts rather than real lectures in most cases. I would definitely try to find podcasts first because there are a lot of techy people out there who make pods, but most who want to write books about code do it with visuals and code snippets.

**I highly recommend you just download a bunch of podcasts and try some snippets and see what you like, because your taste will vary so much based on the skill level and stack they are trying to appeal to, and also just which hosts you might enjoy or find annoying.** That said, here are some recommendations I see a lot:

◙ Syntax.fm   
◙ Coding Blocks   
◙ Programming Throwdown  
◙ Hanselminutes   
◙ The Changelog",1540834385.0
alanbdee,"Any programming book with code examples will be difficult to understand with audio only. But a few books that would be good are: 

* The Pragmatic Programmer by Hunt, Thomas. 
* Design Patterns; Gang of four (or Head first Design Patterns)
* Effective Java (if you're a Java developer)",1540827902.0
kernalphage,"I've been getting a lot of mileage out of cppcon, pacific++ and defcon talks while doing chores and playing mindless games. You could download the slides if you wanna read along without downloading the whole video.",1540854002.0
-------hi-------,"Prediction Machines: The Simple Economics of Artificial Intelligence 

Not theory heavy and mainly aimed towards the general population (and not just comp sci people) but tries to answer some questions related to the impact of AI on society and the economy. ",1540867707.0
AngryFace4,"Cyber Security Sauna by F-Secure,  Podcast.",1540857495.0
pseudorandomcoder,"Podcasts have been much better for me than audiobooks. There are good podcast options for python, AI, and machine learning (just what I've looked up). Any audiobook that starts reading code is not very helpful since I listen while driving. Might be an option if I was sitting at a desk listening...but then why would I need the audio version. Not a huge John Sonmez fan, but I do enjoy his books on audio, worth checking out.",1540867398.0
Rob_Royce,"The Innovators by Walter Issacson (details the history of computing from Charles Babbage and Ada Lovelace all the way up to modern times, highly recommend you start here)
The Master Algorithm by Pedro Domingo
Superintelligence by Nick Bostrom 
The Information: A History, a Theory, a Revolution by James Gleick
The Inevitable by Kevin Kelly
Why Information Grows by Cesar Hidalgo
The Attention Merchants by Tim Wu

Biographies:
Steve Jobs by Walter Isaacson
The Everything Store (Jeff Bezos) by Brad Stone
Elon Musk by Ashlee Vance


Books about coding are hard to come by and are probably terrible anyway. If you’re in to podcasts, try

Software Engineering Daily
Coding Blocks
Software Engineering Radio
",1540880163.0
Santamierdadelamierd,"Not books exactly but there are some really fun and informative podcasts. There is programming throwdown, talk python to me and my favorite is lambda cast. ",1540888687.0
prawnandcocktail,"I know an amazing podcast on computer science, if you are interested?",1540823099.0
GetOnMyLevelL,Only skipped through them ( will watch fully later) in two of your videos there is a lot of background noise. Maybe you could record new videos somewhere else or change the audio.,1540816943.0
Putnam3145,"does ""a machine learning algorithm overfit to household objects"" really ""extrapolate in unsettling ways to autonomous driving""?",1540802989.0
stathibus,This shouldn't be news to anyone who actually works on ML,1540824774.0
crwcomposer,"This could possibly be fixed with better training data, but it would be a huge effort. I'm assuming that, at the moment, training images for elephants nearly always have the elephant in the context of nature or a zoo. And couches are nearly always in the context of a house. But if you removed the background of the images (which would be a monumental task for the whole training set) it might learn to consider the objects in arbitrary contexts. ",1540819268.0
CyAScott,Humans are just as bad at this [too](https://www.npr.org/sections/health-shots/2013/02/11/171409656/why-even-radiologists-can-miss-a-gorilla-hiding-in-plain-sight).,1540871344.0
Ravek,"A human would be as least as confused by the appearance of a tiny floating elephant in the room while they were trying to play video games, so I'm not that impressed by such an outlandish situation. It would be more interesting if they actually confused vision systems in realistic situations, or by merely adding subtle visual glitches. And isn't that what adversarial machine learning does anyway?",1540826572.0
,[deleted],1540798474.0
IMurderPeopleAndShit,"One of the greatest tricks computer scientists have discovered is that you can slap a ""computational"" in front of anything and get a new field.",1540774831.0
Nerdlinger,Reversible computing is an area of overlap.,1540769906.0
carmichael561,"There's this book on statistical mechanics, theoretical computer science, and information theory: 

https://web.stanford.edu/~montanar/RESEARCH/book.html

",1540771026.0
bdd4,Concepts of entropy,1540774497.0
aelsilmaredh,"Computational Chemistry is a very active field of research.  Statistical Mechanics is the study of how the quantum energy states of atoms sum together, and extrapolates the mass behavior of the quintillions or more of atoms in a gas or liquid.  This involves calculating large infinite sums to take into account the contribution of each atom.  It allows Chemists and Physicists to make predictions about materials that may be difficult to create in large quantities for direct observation.  Everything from the properties of superfluid helium at less than 2 K to the complex geometric behavior of a 6,000-atom protein can be modeled.  The software is complex and the required computing power is very large.",1540770566.0
HimDaemon,">I ask here because I've overheard that the concept of **entropy** crops up in CS and information theory?

[Relevant xkcd.](https://xkcd.com/1862/)",1540776206.0
marcblank,I’ve been in software development for 40 years and I’ve seen a lot of randomness.,1540770661.0
heyzo,"It comes into play when thinking about the physical limits of computation:

[Berkenstein bound](https://en.wikipedia.org/wiki/Bekenstein_bound): A limit on the amount of information that can be contained in a region of space

[Bremermann's limit](https://en.wikipedia.org/wiki/Bremermann%27s_limit): The maximum computational speed of a system


[Landauer limit](https://en.wikipedia.org/wiki/Landauer%27s_principle): A lower bound on the energy required for computation",1540818845.0
Augur137,"Feynman gave a few lectures about computation. He talked about things like reversible computation and thermodynamics, quantum computing (before it was a thing), and information theory. They were pretty interesting. [https://www.amazon.com/Feynman-Lectures-Computation-Frontiers-Physics/dp/0738202967](https://www.amazon.com/Feynman-Lectures-Computation-Frontiers-Physics/dp/0738202967)",1540843111.0
maruahm,"This is on the frontiers right now, but industrial research into quantum computation and superconductors heavily uses [Bose-Einstein condensates](https://en.wikipedia.org/wiki/Bose%E2%80%93Einstein_condensate). Probably academic too, but my academic work is not in this research area.

Finding resources is left as an exercise to the reader. All that I have is (unfortunately) protected by NDAs.",1540769996.0
Estanho,"I did a small university work in Molecular Information Theory and it had a CLEAR connection between thermodynamics second law and information theory's entropy.

I think common information theory entropy has little to do with thermodynamics entropy, and I say that as someone with a BSc in computer engineering and an ongoing MSc in mechanical engineering. If I'm not mistaken, the name was chosen as some sort of joke. Not sure if this is a myth, though. But it was very impressive how molecular information theory managed to connect both concepts. ",1540781425.0
wenoc,You can use thermodynamics to show bruteforcing AES256 is a terrible idea. ,1540793795.0
RedditorSinceTomorro,"Yes, for some good techniques in computational thermodynamics check out Numerical Recipes http://numerical.recipes/",1540803629.0
Viehzeug,"So definitely Information Theory is a large area of overlap. I've heared  it called ""thermodynamics on bits"". 
The standard text there is [Cover & Thomas](http://www.cs-114.org/wp-content/uploads/2015/01/Elements_of_Information_Theory_Elements.pdf).

iirc the intro of the book also talks about the relation to other fields of science (and especially thermodynamics).

e: Parts of Machine Learning are build on information theory. So again a lots of physics crops up in the idea of [maximum-entropy-inference](https://en.wikipedia.org/wiki/Principle_of_maximum_entropy).",1540806000.0
AndreasTPC,"The wikipedia article on it is probably a good place to start:

https://en.wikipedia.org/wiki/Entropy_in_thermodynamics_and_information_theory",1540810731.0
SilasX,"For a less formal thread, here's an xkcd discussion about how Joules per Kelvin and bits are measures of the same thing: http://forums.xkcd.com/viewtopic.php?t=113079",1540931597.0
bartturner,Love how the poster changed the title to try to get people to click.,1540727032.0
Kplow19,"Some schools only offer a BA, I have a BA in CS from Dartmouth because they don't offer a BS",1540733462.0
hextree,"These are just letters, different universities define them differently. For example, Cambridge does BA degrees in Computer Science, Maths or Natural Sciences, and these are considered to be amongst the top such programs worldwide. Also these degrees are not mixed with other subjects.",1540729267.0
m00fster,"I did BA, and I don’t mention it on my resume. Just says Computer Science from X University. Also had pretty average grades and don’t mention it. I’ve never had an interviewer ask me about my degree or what I studied.

BA and BS in my university were pretty much the same thing except for a few senior classes.

I did however work my ass off with side projects and reading/learning stuff outside of school. My second job out of University payed over 100k USD.

",1540728034.0
janzima,"By nature of the BA, you’ll probably have more required courses in the humanities and arts, and maybe a few less science classes or at least easier sciences. Humanities classes are also important to your education though, they make you a more well-rounded student.",1540732877.0
Carpetfizz,"UC Berkeley offers a BA in CS. Considering the heavy Silicon Valley recruiting that’s done here, and my own personal experience with job searching, the letter after the B makes absolutely no difference. It’s all personal effort and some luck.

I appreciate my “liberal arts” education because it has allowed to meet people outside of my major department, and develop a more holistic world view. Who would’ve thought that there are people who work on non-engineering problems (/s)!

EDIT: Physics, Math, Biology all earn a BA too. It’s just a matter of college history within a particular university. ",1540748917.0
cuberandgamer,"I would go with a BS because you will probably take more math classes, but that's just me enjoying math. Go with whatever you want",1540742056.0
santropy,Provide list of subjects in each stream to get useful comments.,1540725927.0
Zackeezy116,Whether the degree is BA or BS at my university is determined by your minor,1540735271.0
weezeface,A stupidly high number of engineers with BAs and BScs work at every “top” company. Just go to school and do your work and don’t worry about it. ,1540744606.0
ianwold,"I went to UIowa, and the distinction there between a BA and BS in CS was that the BS took more coursework in the CS department plus some extra science classes required for all BSes in the college. I opted instead for the BA, which was two classes/semester for five semesters (I tested out of CS101) in CS, and I was able to get a second BA in Philosophy, plus some cool ECs I might not have been able to get around to otherwise. I'm very happy I chose that path. When you go to find a job, they probably won't care if you have a BA or a BS. If you're going for a Masters or a Doctorate after your Bachelors, the Universities you're applying to *might* care, but I'm not sure. ",1540745940.0
Zohren,"Nobody will care when you’re applying, and once you’ve got some experience, nobody will care if you even have one in the first place.",1540748474.0
NotEzekielLeAderen,"My university offers BA and BS in CS, but there is no difference in curriculum between the two. The BA is offered through the College of Arts and Sciences, and the BS is offered through the College of Engineering. The only difference is in distribution requirements for the specific college.

If you want to have more humanities in your time at college, go for BA. If you want more engineering or science, go with BS. In my experience employers don't really care about which one you choose. ",1540749491.0
teacamelpyramid,"I had a choice between a BS and BA in CS and chose the BA so I could get an additional major and a minor. I still managed to get into CMU's School of Computer Science for grad school and earned enough fellowships to graduate without debt.  

BA vs. BS has not once come up in my career. I've managed departments, have patents with my name on them, and have hired my own team. For hiring, I care a lot more about code samples and capabilities than exact CS degree titles. After a certain point in career longevity college degrees fade into the background and your work history will be far more important.

I'd say focus on some extra projects if you choose the BA. It will show your dedication and creativity if anyone brings it up as an issue.

My one regret is that I missed taking the class on compilers.

&#x200B;",1540751611.0
Feynmanfan85,"I think ultimately it is the substance of the course work that matters most, rather than the title of the degree. I have a BA and I don't think there was any substantive difference (to my knowledge) in the coursework when compared to a BS program. My personal recommendation is to take at least an introductory course in computer theory. Though you're unlikely to ever apply the concepts in a career outside of academia, the coursework itself will expose you to the history of the computer, as well as its connections to human languages, which I found particularly fascinating.",1540753343.0
mclarenf3,"Nobody will care.

I still have trouble remembering if I have a BaSc (Bachelor of Applied Science) or BS (Bachelor of Science) in Computing Science from my university. 

All that matters is what you did in your degree and that you completed it.",1540753605.0
MUDrummer,"What do you want to do with your degree?


If you have any plans to do big data work or analytic, the advanced math that usually comes with a BS in comp sic is absolutely invaluable. 

Want to just build cool websites and some crud APIs to make them work?  Either one would be just fine. 

A BS isn’t a requirement for much in CompSci, but the math makes some thing easier (and is useless for a lot of other situations). 

Personally I would recommend anyone getting a degree in comp sci should at least get to discreet mathematics. Set theory, graph theory, complexity, etc are all parts of discreet math and are hugely useful for solving real world problems (the stuff a programmer has to solve all the time, not the theoretical stuff that you learn but never actually use). ",1540753621.0
shadeofmyheart,"Im enrolled in a BS in a school that also offers a BA. 

The BS has more requirements in math and science.

The reason I went with the BS is because its ABET accredited and the distinction is important to some companies as to whether they consider you an engineer and pay you an engineer’s salary. Defense contractors in particular.",1540756802.0
brett_riverboat,Whatever you decide I highly recommend getting real work experience as soon as possible. Make sure you graduate with at least one if not two internships.,1540758876.0
GNULinuxProgrammer,"Nobody cares as long as you go to a good university. UC Berkeley gives BA in Computer Science and graduates on average go to top CS schools for PhD or average $105k jobs in the industry. Nobody cares if it's a BA or BS or whatever. Just focus on learning. Focus on being a good software engineer, writing programs, solving problems and understanding the theory, then you'll find a job.",1540761306.0
TheChisler,"As long as you know how to code, you'll get hired. If you don't know how to code, it doesn't matter if you have a phD. I knew a girl with a masters from Stanford in CS, but she was shit at actually coding, couldn't get hired. Eventually she gave up and went for her phD in civilE. 

I also knew a salesman who taught himself programming in his off-hours, hired at Uber as a mid-level SWE just last week. 

Go to the school with the cooler classes,  more awesome professors, and smart students. College is a FANTASTIC time to develop whatever the fuck you want in new technologies. If I were to start over I would love to be 18, in a technically-adept college, and play around with some cool new technologies like VR/AR or AI/Machine Learning development with a group of 3 other classmates. I let my college years for CS go to waste and although I got my degree, I didn't get to experience TRUE free, unhinged development until afterwards.",1540762829.0
the_brizzler,Although the BA and BS are usually pretty similar (minus a few differences in non-major courses)....I have seen a lot of job postings in the US specifically ask for a BS. I am assuming they would be okay with a BA in computer science. Has anyone else noticed that in job postings.,1540763342.0
vorlik,literally zero difference,1540768143.0
scoobydoobiedoodoo,"It seems odd to offer a Bachelors of Arts in Computer Science. 

Makes more sense for it to be a Bachelors of Science in Computer Science. 

Either way they are treated the same on a resume. 

You get a Bachelors degree in Computer Science. 

The discrepancy becomes the courses offered towards that degree. 

Employers make the choice at that point to hire you. ",1540773412.0
Dodobirdlord,"If your university offers both a BA and a BS, you should probably take the BS because it likely involves more depth work in focus areas of computer science theory. People who know your university will know this and will care, and will likely prefer you to have taken the BS. If your university only offers a BS or only offers a BA then it doesn't matter. It can be a little unfortunate if your university only offers a BA because sometimes people who went to universities that offered both will get confused.

To make things abundantly clear, some of the best CS programs in the world are BA programs because the universities that offer them never adopted the modern convention of calling liberal arts degrees in the sciences ""science degrees"". It's impossible to make a judgement about the significance of BA/BS unless you happen to know the particulars of the awarding university.",1540776973.0
LordeLordeYaYaYa,"Thank you all for your contributions, people! Your thoughts have truly shaped my perspective in making the right academic choice for myself.  This is why I love Reddit! :D ",1540813572.0
easyMoose,"From a CS perspective, a BS will likely always look better. However, it depends on what job/domain you want to go into after college. If you want to have a more technical focus, a BS would be a better route. I've seen a lot of jobs list BS/MS as a preferred qualification. However, if you are more interested in product design or user experience than the technical challenges, it might be better to have the flexibility to take a wider range of classes that would provide you exposure to those skills as well.",1540733093.0
TrammelBranch,"As a hiring manager, I would definitely look sideways at a BA in CS, but it wouldn't be disqualifying.

The good news is that CS is pretty meritocratic, because it's not hard to assess a candidate's skills in an interview. That being a BA rather than a BS may make it a little harder to get an interview, but after your first job it won't matter much.

That said, if you have a choice of doing a BS instead of a BA (not entirely clear from your post), I recommend you do that.",1540725914.0
uh_no_,">What effect will this have if I were to graduate with a BA, e.g. future job opportunities?

None. nobody cares.

*nobody cares that it's a BA or a BS....not that you won't have any job prospects. You'll have just as many prospects as a CS BA as a CS BS....which is plenty.",1540748134.0
pulsar512b,"BS will give you more computer science knowledge.   
In general, BS's in a specific field are better than BA's.",1540726012.0
noam_compsci,"I'd say forget about the optics and what it looks like, and think more what you want your career to be like and what you want your uni life to be like.

Are you happy with it being potentially more UX and softer skills related? Do you feel comfortable doing a lot of hard maths? 

What are the exact reasons the two degrees are different? What % of the course will be replaced and with what will it be replaced? For example, the BA could mean a lot of cognitive design modules...or it could mean long essays on the history of comp sci. 

Instead of looking at this top down, I suggest you take a bottom up perspective. There is no right answer. Want to become a product manager or UX designer? Potentially a BA could be better. Want to be an SW engineer? Clearly BS is the way to go. ",1540737340.0
dnabre,"The college, which I did my undergrad work at, had a BA in addition to the normal BS. There were too different BS tracks (applied, and applied with a little theory). During orientation, the department head while explaining the differences between tracks mention the BA existed. 

He described as the degree they kept on the books for students so that could give some sort of degree to students that barely managed passed the general liberal arts requirements and had the university minimal amount of CS credits for a degree, and who they felt sorry for. It was some time ago, so that's definitely a very rough rewording of it, I'm sure he said in a way that made it sound more pitiful than that (the department head was a total ass). 

**edit** To be clear, BA vs BS varies a lot based on school. My program using it as a sort of pity-degree and being clear that it was like considered that doesn't mean anything about other school's BA. ",1540727420.0
metaphorm,get the BS,1540734176.0
railtrails,"I'll just leave [this here](https://www.seriouseats.com/2013/10/how-to-carve-any-photograph-into-a-pumpkin.html)

^^^All ^^^hail ^^^Kenji ^^^and ^^^Serious ^^^Eats.",1540739925.0
Roachmeister,Not really what this sub is for. May I suggest r/matlab?,1540735836.0
paladin314159,"Sounds like you'd enjoy doing machine learning or data science work. That's the most mathematical work you can find in the industry today, plus it's in incredibly high demand right now. You might also want to consider doing a PhD if the theoretical and research aspects appeal to you.",1540710094.0
cp5184,"Maybe get a minor in math or switch to a math major. 

Math is always in demand.

You could go towards crypto, towards stuff like compression, machine learning's a possibility, algorithmic trading, but this is all just a small slice.  Electrical engineering/radio/wireless is another tack

If you want to follow CS, hardware/computer architecture courses  and assembly courses are just a small part, once you've finished those classes you won't be going back over that stuff again unless you focus on it.",1540709929.0
Mas0n8or,"You should dip your feet into some AI/ML. Based on affinity for math, data structures/algorithms, and higher level languages (especially python) I think you'll enjoy that branch of logic quite a bit ",1540710922.0
Center60,Are you me? Wow.,1540718092.0
lostera,"First of all, relax.  Computer scientists who are strong in math are in high demand and will continue to be for the foreseeable future.  You've got a lot of options in industry that play to your strengths, including: machine learning, robotics, computer animation, computational genomics, and medical imaging.  And of course, almost any academic route is going to require a fair bit of math.

There are too many interesting options that fit your criteria to name them all!  Tell us what your favorite class or project was, and I'll try to provide more specific suggestions.

As a side note, if low-level programming isn't your jam, I would advise avoiding algorithmic trading, compression, and cryptography (outside of an academic setting).",1540712041.0
tymbender1000,Join Air Force get into DRONES you will never be unemployed!,1540712763.0
iambeingserious,Have you thought about going into Quantitative Finance? You could get a job at a hedge fund doing research type of work,1540720513.0
arnoldferns97,"Tbh I didn’t read the whole thing, I just saw that he had a problem and so I  wanted him to live a little.",1540726032.0
arnoldferns97,"Be a humanitarian, smoke some weed, explore the world, smoke some more weed. ",1540720439.0
TheCodeClown,[https://www.cisco.com/c/en/us/solutions/collateral/data-center-virtualization/storage-networking-solution/net\_implementation\_white\_paper0900aecd800f592f.html](https://www.cisco.com/c/en/us/solutions/collateral/data-center-virtualization/storage-networking-solution/net_implementation_white_paper0900aecd800f592f.html),1540692985.0
Gavcradd,"""the"" syntax? What language are you using? Python, Java, Ruby,C,C++, C#, JavaScript, PHP, etc..",1540661738.0
Taleuntum,"You could try competing on [Codeforces](http://codeforces.com/). They have around two competitions every week, and after the competitions you can also read the editorial for the solutions.

It is also useful to research common topics, such as sorting, **DFS**(Depth First Search) and other graph algorithms, **DP**(Dynamic programming), complexity of algorithms, data structures (heap, segment tree, **fenwick tree**). You can of course find these in CLRS too, but I also like [GeeksforGeeks](https://www.geeksforgeeks.org/fundamentals-of-algorithms/). 

Without an error message, I can't say what the error is. It might be:

* You forgot to include something that is needed.

* You don't have your includes in the path.

* You have some other syntactic or semantic error in your code.",1540663253.0
LearnerPermit,"It's been a very long time since I did an acm programming contest. In those days the challenge at the first round was typically figuring out the question, and the algorithms to solve it. Coding was typically the easy part.

Typically it's not an algorithm like make and search a binary tree. It's more like this question is a math problem that's being asked in an obfuscated manner. From memory one of the questions that stumped my team was just Pascal's triangle.

Here are the final questions

https://icpc.baylor.edu/worldfinals/problems",1540664768.0
zzopp,"The pseudo-code in CLRS is not very implementation-friendly. I recommend Steven Halim's Competitive Programming book instead.

To become better at programming competitions, try to practice a lot. The following websites have both competitions and an archive where it's possible to practice past problems:

[TopCoder](http://www.topcoder.com) (has editorials for most problems, and all solutions submitted by contestants during the competition are available afterwards. However, the website it a mess.)

[IPSC](http://ipsc.ksp.sk) (Has a very well-written solution booklet for most of the competitions.)

Many ACM ICPC regionals have online competitions that run in parallel to the onsite competition. [NWERC](http://www.nwerc.eu/) should happen in a month or so. Solution slides and code are often made available after the contest.
",1540670068.0
FireyFly,"I've done a couple programming competitions in the past. Choice of language generally doesn't matter *that* much for many problems in my experience (they're often designed to work well regardless of language), but sometimes it *does* matter, in which case it's great to have a decent knowledge of C or C++.

Personally, I ended up using Python for most of my competitive-programming solutions (even though Python isn't really my ""main"" language/environment) since it has an extensive standard library and whatnot. C++ is also good in this regard vs plain C, in that you have the STL (standard template library--basically, the C++ standard library) at your disposal, which has a lot of handy data structures and algorithms. I found it worthwhile to learn C++ to the extent of being able to implement graph algorithms and such in it relatively easily.

As a practical tip for that kind of competition: there's often a lot of problems to work on, at different difficulty levels. Try with what seems easiest/most approachable/where you seem to have a mental idea of how to solve it. If the approach/idea doesn't get you anywhere, don't be afraid of putting that problem aside and looking at a different problem. I generally started by looking through the problem set to get a rough idea of each problem before selecting a first problem to work on.

There's a ton of websites around with competitive programming problems, of course. I like [Kattis][1] a lot, personally. (Disclaimer: it's what we also used for courses at uni, and competitive programming events at uni--it's also developed by alumni from the uni I went to.)
In particular, it might be an idea to look at the [list of past contests][2] (scroll down the page a bit) and check out some problems from their problem sets, to get a feel for what kind of problems one might see in a programming competition.

Good luck, and happy hacking! And make sure to have fun more than anything else. :)

[1]: https://open.kattis.com/
[2]: https://open.kattis.com/contests",1540673274.0
sturdyplum,"I do collegiate level competitions. The best book for people that are starting out is ,""competitive programming 3"" it's like 20$ but you can get 1 and 2 for free.  The best site is definitely codeforces.com",1540688287.0
firmretention,I stopped using Linux because why would I use an OS that forces me to read MAN pages? ,1540583238.0
khedoros,"I remember seeing the stories about that when they came out. It seems like there was something similar more recently, but I don't remember specifics. I see where it comes from, but I'm so used to technical terms inspired by non-technical ones that I have difficulty actually being bothered by them.

""Leader"" and ""follower"" seem like they'd be natural replacements, without abandoning the clarity of the current terms.",1540578773.0
WhackAMoleE,I remember that. Hard drives were configured as master or slave with a jumper. I remember when it became un-PC to call them that. I was looking for a USB female-to-female adapter the other day and I wondered if it's still ok to call connectors male and female. Just a matter of time before it's banned.,1540620179.0
Lemmings4Friends,"Plantation style slavery was still going on umtil the 1960s in the US, so to say im under a century that these words dont sting for a lot of people(as they should) is wrong.",1540591574.0
Ebanflo,Why this sub tho?,1540572537.0
cupOfJoe_shallWe,"First came in contact with Julia about 2 years ago when I was studying linear algebra. It was a nice experience, but I've never continued exploring it.

I'm glad Julia is experiencing such a proliferation of use cases. Hope to see more of it.",1540566738.0
4Cheese_Ropanouie,"Was introduced to Julia programming this semester for my numerical linear algebra class, spent days looking at juliabox, glad to see it's picking up traction.

Is there a benefit to using Julia over similar stuff like Matlab? ",1540568245.0
Crysis456,"The meaning of the code is internally represented by a parse tree, the tree is created using a stack, code is generated from the tree using a stack to traverse it too. Hashtables are used to map variable names and more to their corresponding address/meaning. I'm sure each implementation uses a bunch more but these are the major components.",1540541423.0
Jaxan0,"Graphs (and the usual algorithms on them) are used for analysing dependencies, calls and optimisers.

Graph colouring for register allocation is also a nice thing. ",1540543157.0
leftofzen,"Just collapse all the identical nodes in the tree into one node and preserve all the edges. Then you'll get a graph.

Edit:
Should look like [this](https://www.draw.io/?lightbox=1&highlight=0000ff&edit=_blank&layers=1&nav=1&title=Untitled%20Diagram.xml#R5Vpbk5owFP41vnYgIVweu1u3fbCdzjiddh%2BzEoEWCROjYn99gwQQoqyzriFjfTE5Obl953znJNEJfFwVnxnO4680JOkEWGExgZ8mANiObYmvUrKvJK4PK0HEklAqtYJ58pdIoewXbZKQrDuKnNKUJ3lXuKBZRha8I8OM0V1XbUnT7qw5jogimC9wqkp%2FJiGPK6kPvFb%2BhSRRXM9su0HV8oIXfyJGN5mcbwLg8vCpmle4HktudB3jkO6ORHA6gY%2BMUl6VVsUjSUtsa9iqfk9nWpt1M5LxizpIu6z5vt47CQUUskoZj2lEM5xOW%2BnDYX%2BkHMEStZivUlG0RZEUCf9Vij8AJKvPdVPG2b5qQ3X1WY7wm3C%2Bl06AN5wKUTvxjNJcjlEttVzf2d1K0Zpu2EJqAek%2FmEVEasEGaeHBhK6IWIxQYSTFPNl2R8fSlaJGr4VTFCSiZ9BFt0HXMxpdRxO6cuotTjdy0LkC9y5OOJnn%2BLDcnQhWXUjxOq%2FCxzIpSsglCFvCOCmGYVA3KDtAV3Jbxj5HVndtIKlF8VEMqWVX%2BZt392yGqr95utjs62czGB9epAleqNDZMYHOwO%2FS2QYa%2BRzo57On2eOc8TyuOYTdb3o%2BgW6gCV1H4TMwgc%2BOMx6fga7T9qVONcj7t3kcUj3O18VnNWHcG59PoOtqQhcpfFZvNwYct4Glkc83Om%2Bb42%2Bu6m%2F2GXtc7HCHrh8Zw%2FsjhZwmGV8fjfy9FByF7Z6Za7M%2FndFH1qC%2BKFQraO3cbOUi07sKG1wT2NDPbs3pVQcbbAWBu7sfeeNlNzjCU50B8NrWlfi%2BKdw0L9H1I44%2FHG6gM6h%2FdbjxlHCjpp4xLsfWiMn3Ro%2BrBj12%2BcbQIei5t%2FMKHdCg%2FtV0OJFs7y7VnDL%2BKEcv2I%2BF1rDxnb6zWO9rfF%2BJhULipgKjhxcmShFvMDfr6VDrYew%2FeDoMTGEIgr3LRv9X176%2BPah%2FNUPgCCdxE4w%2FSm5EvVyHXjF%2B%2F8ECvbPxAyU8qheHEYJhvc0xjoo13Y4w%2BfZjNjMBFtgLHUjnz8W2AsvUBEwc%2F%2FTp8QaYiGr7Z5uKbO0%2FmuD0Hw%3D%3D). Notice there are not any duplicate nodes, but notice that some nodes now have more than one arrow going into them. This makes it a graph. Trees only have one edge leading into them (but may have 0...* edges leading out).",1540521289.0
Meesterwaffles,Impact of blockchain technology,1540527275.0
aklabarow,"Hey everyone. I am currently working on a project for uni, where I need to analyse the ways people are interviewed for internships/jobs. If you have the time, could you please answer my survey: https://goo.gl/forms/rela3Wp3RMmY3Ng33",1540563556.0
jnwatson,"Congratulations! This is a great research area. You can get lost for years.

Sending and receiving packets is the easy part. Figuring out when and how often to send the packets is the hard part (at least if you care if the other side receives them).

Anyway, when I had to do this, I first wrote a simulation. It is far easier to test and tweak your logic when you can change network bandwidth, loss, traffic patterns with a click. I didn't see any good network simulation frameworks that operated at the low level that I needed, so I used a discrete event simulation framework called SimPy.

It turns out that lots of interesting behavior emerges once you have multiple agents on the same network, all competing for bandwidth.

I learned that the folks that wrote TCP (and Reno, and CUBIC) aren't dummies after all.

&#x200B;",1540522320.0
crazysim,"I didn't see these keywords in your post: ""[raw socket](https://en.wikipedia.org/wiki/Network_socket#Raw_socket)"". Are you missing that terminology in your search? ",1540512118.0
bluehavana,"UDP is normally used as a protocol to use in experiments if one is sticking with an IP network layer. Instead of thinking of UDP specifically as a connectionless protocol, I try to think of it as the lightest weight transport protocol that can be multiplexed to different processes on a single IP, with ""connectionless"" just being an inherent attribute that comes with the simplicity of such a small protocol.

Google used it with QUIC specifically to be able to experiment with different control flow methods and error correction methods within user space instead of having to rely on OS support for a new transport layer. You might also want to check out the control flow and error correction methods explored by QUIC for edge case research/comments/inspiration.",1540515306.0
chipperclocker,"Plenty of good advice here on design already, so I’ll mention a tool that might be useful - Scapy is a great python toolkit for most things network-protocol related, including custom extensions of UDP, or even building your own custom Ethernet frames.

https://scapy.net/
",1540523736.0
onlyNexusHere,"Python with the Scapy library - you can build and  dissect packets. You can define your own layers. 

Scapy can be a pain, but very useful.",1540523477.0
rabdas,Network protocol researchers use a program called “network simulator.” We all just call it “ns” followed by the version. I used ns2 but that was over 10 years ago. I think it’s now on ns3 or ns4. The simulator allows you to explore what happens to a packet from pretty much every network layer from physical topology to comm protocol. It does wired and wireless networks and you also adjust the time of the simulation. It’s a beast. ,1540562302.0
26julio,"Read about the SCTP protocol and TCP congestion algorithms like Tahoe, Vegas, Cubic, etc. A lot of interesting work has been done in this area and you might get some ideas for your own work.",1540522565.0
FuzzNugs,"You may already know this but you could get the source for tcp, build it, and play with it on Linux for example. Fun to do and will give you a starting point to build from.",1540523970.0
pm_me_great_stuff,"Extending the thought of building on top of UDP: you might wanna have a look at [JGroups](http://www.jgroups.org). It does quite a lot of different stuff. But it can also do stuff like acknowledging multiple packets with just one reply packet AFAIK. Also negative acknowledgement is an interesting thing, it can do for 1-N communication.",1540542661.0
Ravek,"You'll probably always want the UDP features (length, ports, checksum) so I'd build on top of that rather than sending raw IP packets.",1540534500.0
deeharok,"> I understand that it’s used to “guess” the formula that a sequence can take via solving for polynomial coefficients

I'm not sure what problem you are asking about, but I'll try to explain induction and provide an example.

The main driving force of induction is implication (if ... then). What I mean by implication is, if we know something is true then this implies that something else is true. For example, if we go outside to see that the sidewalk, grass, etc. are wet then we could state that it has rained (well most likely at least). Maybe an example in math would be, if we know x is a prime number then x must also be odd (which is true, and pretty trivial to prove). Note that this doesn't necessarily go the other way around, e.g. if we know x is odd then x might not necessarily be prime.

Now let's talk about induction. Induction is a tool that can be used to prove the statement f(n) for all valid n. To do so, we build on (i.e. construct) the result of previous step(s). Specifically, induction aims to prove f(n) is true by using the fact that we know that f(n-1) is true. In other words, induction aims to prove if f(n-1) is true then f(n) is true. Alternatively, we can prove f(n) is true using the fact that we know f(x) is true for some x < n (one or more).

Once we prove this implication, that is f(n-1) implies f(n), then we're golden. The only caveat is, we need to prove some initial base case, e.g. take n = 0 (lowest valid value), i.e. if we know f(0) is true and f(n-1) implies f(n) then we know f(x) is true for all x >= 0. If the base case is not proven, then the initial statement may or may not be true; this is due to the fact that all we've done is proven an implication, e.g. holding an unpinned grenade in your hand implies that your hand will be blown off; but knowing this doesn't mean you have had your hand blown off (I mean, this might be the case, but probably not).

To summarise, there are two steps, let's prove f(n):

1. Proof of some base case(s), e.g. f(0)
2. Assume that there exists some n where f(n) is true. Let's prove the implication f(n) implies f(n+1)

If there exists some case where 1 is true (the base case holds), and step 2 is shown to be true then we know f(n) is true for n >= the base case.

---

I will cover an example of proving that 3^n is odd (i.e. f(n) = 3^n is odd) where $n >= 0$ (and where n is an integer).

**Step 1: Base Case**

Let's take n = 0

3^0 = 1 which is odd

Therefore, the base case is true for n = 0

**Step 2: Inductive Step**

Let's assume there exists some n>=1 where f(n-1) is true.

f(n) = 3^n
       = 3*3^(n-1)

Due to the assumption, we know f(n-1)=3^(n-1) is odd. Therefore we are multiplying two odd numbers, which will equate to another odd number (odd*odd = odd). 

Therefore f(n) is true and f(n-1) implicates f(n). i.e. 3^n is odd. QED


Now, notice that if you don't have the first step in the above proof, you could 'prove' the implication (step 2) that 3^n is even for all n (given that 3^(n-1) is even, since odd*even = even), but this does not imply that 3^n is even.

---
Note that proving the implication (called inductive step) can (will) use other methods of proof, e.g. direct reasoning, contradiction, counter-example, etc. and will entirely depend on the statement/problem.",1541045344.0
TrammelBranch,"Proving the optimality of Prim's algorithm is generally done in terms of cuts. See https://web.stanford.edu/class/archive/cs/cs161/cs161.1138/lectures/14/Small14.pdf, starting at slide 21.",1540482472.0
tapdncingchemist,"Your question is basically asking why a greedy algorithm produces a globally optimal result. 

The other poster said it is thought of in terms of cuts, which is correct. Basically, you will need to connect the component you have already with the rest of the graph. Something on that cut will be necessary.  You can think if it as if you had a black box that will connect the other part of the graph for you later. You just need to connect to one of the vertices and your decision will not affect the decisions made in connecting the rest of the black box parts, so you can be greedy. ",1540482820.0
Workaphobia,"Prim's algorithm says at any given step, take an edge with the lowest cost you can, so long as that edge doesn't create a cycle and is connected to a part of the tree you've already formed. The result is supposed to be a (not necessarily unique) tree with minimum weight.

Suppose that there's a tree that is better than any of the ones you can get with Prim's algorithm. If you can't get this better tree via Prim's, it follows that at some step, none of the possible edges with smallest weight were in that better tree. We can form another tree by adding to this one an edge that Prim's would've chosen, and taking out one of the ones Prim's didn't choose. Since Prim's picks the smallest possible edge, this forms an even better-er tree, proving that the ""better"" tree wasn't actually the best solution after all.",1540502947.0
nathreed,"I think a lot of those tools have it in their terms that by using them, you submit the paper to a database, so that they can check against all papers previously checked with the service (in addition to performing internet searches most likely). 


EDIT: yeah, just read the terms for TurnItIn, a popular service. They archive student papers for checking all other papers against (student retains copyright), unless the institution explicitly opts out. They also search the internet and academic sources, so they probably are tying the text search in with a full-text database of academic journals, etc. 

EDIT2: to expand further, this use of student work apparently constitutes fair use under copyright law because courts have found the comparison service sufficiently transformative. A.V. vs iParadigms LLC (4th Cir, 2009) is the relevant case if you’re interested in reading more about it. ",1540516280.0
psychedelicmilk,"My best guess is that it searches for similar sequences of words through Google or whatever search engine. I have submitted assignments with quotes I found online, and the “originality report” shows what percent of my paper wasn’t copy and pasted basically",1540466006.0
spiler50,"I have used the same plagiarism that Itslearning use. A software many of the schools in norway use. And there they compare you text with other peoples text from a database where they have gathered other studnets text and text from the internet. If you use your text from a small webpage there is a big change the plagiarism is not going to find it as it needs the datbase to check. It does have manye text and website it also checks, but if there is a small webpage that is hard to find it just would not find it.

&#x200B;

Now what the plagiarism do is see if your sentences is the same as other paper and it gives you a percentage of how similar it is. If you copyed one sentence it would probably give you a smal percent. Therefore it is easy to cheat the plagiarism by changing the setences or use multiple sources. But for the example you provided it takes the rest of the sentence in considuration and see the entire structure of sentence and not only the words. Google work very much with keywords. So changing meaningfull words in your google search it makes sense that the search would be different. But in a plagiarism software they see the entire sentence.",1540545230.0
might_be_a_troll,"My best guess is that it searches for similar sequences of words through Google or whatever search engine. I have submitted assignments with quotes I found online, and the “originality report” shows what percent of my paper wasn’t copy and pasted basically",1540582883.0
mytuny,"I'm not a tech guy, but from my experience with the free academic plagiarism checker called [Scholar Plagiarism](http://scholarplagiarism.com), I've noticed that they search for duplicate content using famous search engines like Google Scholar and ScienceDirect!

Anyway, it serves me very well and doesn't need to know how all the magic happens when it happen for your benefit \^\_\^'",1540755829.0
phlummox,"Can you clarify a bit what ""like Adobe Illustrator"" entails? It's a pretty complex bit of software.

Personally, I'd suggest building things with the Eclipse Framework might be a good stepping stone. It's opinionated and sometimes a bit clunky, but should give you a good idea of what goes into a desktop app and how you might structure it.",1540460005.0
Doriphor,"Since you’re already familiar with JS, wouldn’t it be a good idea to work out ideas and algorithms in a browser first? Pixlr is a pretty good free alternative to Photoshop, so it’s definitely possible. You can then always figure out how to reimplement it in C++ or Java without having to wonder what you want to do. I don’t know :)",1540535571.0
clownshoesrock,"If I were doing this as an undergrad cosc project:

1.  Have a synonym list.. each line would have an alphebetized list of synonyms.

2. convert pairs of sentences to their alphabetically lowest  terms.

Example: I have to accomplish a few goals in my life.  (Note 'a few' is a single word in this system, which complicated the issue)

If the two converts are the same, they match.

This is academic, and not good for much in RL.
",1540494887.0
penguin_named_tux,"If you're looking for a python library, I would look into gensim  ([https://radimrehurek.com/gensim/index.html](https://radimrehurek.com/gensim/index.html))   


Here's a stack question on sentence similarity prediction: [https://datascience.stackexchange.com/questions/23969/sentence-similarity-prediction](https://datascience.stackexchange.com/questions/23969/sentence-similarity-prediction)  


Further, you should probably read into lexing and tokenization if you want to roll your own solution. ",1540482613.0
geon,"Compare the sentences word by word. If not equal, check if they share a synonym list?",1540461071.0
ggPeti,"I certainly don't understand it at first glance. Let me try to explain why:

Reduce is an operation that takes a list and reduces it into a single object of whatever kind.

Map is an operation that applies the same function to all elements in a list, returning another list.

So just going by the expected type of these operations I would say your diagram is incorrect. Could you explain further your thinking process?",1540549957.0
nerdshark,"That's like asking if more victims of the Black Plague could have survived if modern medicine was a thing back then. The answer is obviously ""yes"".",1540415254.0
kevinlamonte,"Sure, it could have been better.  Early 80's was largely asm and BASIC, so the rise of structured programming (Pascal and C) was a huge improvement, and then OO (Borland Pascal and C++) went even further.

If one had a time machine and could take a modern laptop with a compilable-to-bare-metal language (perhaps Rust) to 1980, it could have changed a lot.  Many more applications coming out earlier, able to do far more on the limited CPU and memory of the time due to the advanced code optimizers on modern compilers.

Its hard to say if the optimizers would have beaten the hand-rolled asm of the day, but it would have kicked the pants out of the Microsoft and Borland compilers.  On the extreme end, we might have been able to get Windows 3.x or GEOWorks running acceptably fast on a 8086, or been able to do DSP / play mp3s with a 286.
",1540415759.0
mcdowellag,Early use of C++ was very much a learning experience for all concerned. Providing a small but well targeted collection class library would be a step forward.,1540490027.0
keksper,Multivariable Calculus and Linear Algebra,1540445043.0
anonrektard,bayesian statistics,1540408500.0
DrKriegerPhD,"I would second Bayesian statistics but would go broadly to say a strong statistical background will be really helpful. It also depends on where on the Theory-Application scale you want to be.

As someone who didn’t have a strong statistical background taking ML classes in a good ML program, I had to do a lot of outside reading to catch up to why certain models work the way they do, but once you do have a strong statistical background, the intuition comes a lot easier. Assignments and projects were a lot easier since I wasn’t designing new models, just picking and fine tuning existing models to fit my domain. 

Good luck :) ",1541037802.0
ImaginationGeek,lol  :D,1540406670.0
CxSomebody,And now I find a [website](https://academic.odysci.com/) that may contain these info and I have sent a email. ,1540394462.0
trill_winds,"I haven't read your paper fully yet, but I have some thoughts inspired by your notes and my memories of your previous work that I wanted to share in the interim.

&#x200B;

I notice you alluding to a lot of ideas from quantum mechanics, but you never phrase them in those terms, or from a quantum mechanical perspective, e.g. when you speak of the quantization of a photon. I think quantum mechanics would be a more fruitful perspective for you to consider your alternative interpretation of the laws of physics as a computational process.

&#x200B;

I think the primary difference in the idealogical approach would be to, rather than think about the universe as ontologically executing a computational process, think about the amount of information available to each open system interacting with each other in the universe. So, for instance, by Turing's paper on the Entscheidungsproblem, computers (and for our purposes, we can generalize the concept of a computer to anything that processes information - in your interpretation of physics, this would be any open system) can only compute functions from N -> N, that is, a function from and to sets of the cardinality of natural numbers (i.e. discrete). That means that whatever the ontological reality of the universe is, an observer could only measure and process discrete values.

&#x200B;

So, rather than thinking of the information in the universe as fundamentally discrete, think about the act of observation and interaction between systems as discrete. I think this would help you reconcile your theory with quantum mechanics, which, by bell's theorem, implies that it is impossible to predict an object's physical properties deterministically until it is interacted with.",1540399027.0
thebin666,.,1540394228.0
niklausbooga,"I like your intuition about the laws of physics but not your understanding. If I saw the something moving close to the speed of light, I would see this computational lag, but they would also see the same thing with us. ",1540835383.0
simm4582,This is the first P vs. NP video I’ve been able to actually follow to the point of wanting to learn more about it. Thank you! I thought it was very well done.,1540397928.0
FIREWALL005,"This was the frirst  time I was actually able to understand this problem. It was very interesting.
Thank you!



A high school mathematician",1540402782.0
PineappleBoots,"Great material! 
Sincerely, thanks for taking the time to make this. 

The PvNP is a mystifyingly awesome thing. I’ve given a guest presentation on it, seen all the videos, and wrote a paper on the topic. 

It’s not too much to say that at the heart of this solution is the thread that holds our modern world together. 

Keep up the great work! ",1540413109.0
vomitHatSteve,"Very clear explanation. Nice work.

Are you Canadian? I heard some ""oot""s in there.",1540404463.0
Hcif,Very clear explanation of the problem. Thanks for this!,1540399144.0
turtle_13,"It was a great video, I loved it.

Can you make a video about incompleteness theorem?

I really want to see your take.

Nice work, I've subscribed.",1540419111.0
ollee,The understanding and comprehension of what P vs NP is is really well conveyed in this video. Much faster and clearer than my algorithms instructor.,1540407226.0
HamuraiSnack,Question! So I’m currently an ECE major at UT and I’m wondering if this is the same concept as big O analysis? We are currently doing this in my software design class.,1540420603.0
paralysedforce,This was really well made. Great job!,1540436254.0
mainhaxor,A small thing: the class you call EXP in this video is typically called E. EXP is the class of problems solvable in O(2^n^k ) for some k (i.e. 2^n^O(1) ). The difference is that E is exponential in a linear function while EXP is exponential in a polynomial. These two classes are different by the time hierarchy theorem.,1540456289.0
varno2,"Great video really clear explanation to the lay person, only one minor nit, the class L generally refers to log-space computation not linear computation. It is not known if L=P or not. ",1540468108.0
willworth,"This is great. Thanks! Is this hypothetical nondeterministic machine related to ""quantum computers? Sorry if off topic. Loved the video. ",1540416047.0
Maristic,"This video represents a good attempt to explain what most people understand to be true about P vs NP; unfortunately, the vast majority of CS folks (including many CS professors) actually misunderstand (or sometimes just conveniently ignore) what the problem is and isn't.

A good article to read is [this one](https://rjlipton.wordpress.com/2009/07/03/is-pnp-an-ill-posed-problem/).  Read the article (really!) but, in short
 
* If someone did show that, P = NP it wouldn't mean what you think it does — it wouldn't necessarily mean suddenly hard problems could be solved *any* quicker.
* If someone did show that, P ≠ NP it wouldn't mean what you think it does — it wouldn't mean that hard problems will stay beyond the reach of fast solutions.
* Claiming that polynomial algorithms are the fast ones and exponential ones are the slow ones represents a fundamental misunderstanding of what these all these terms mean — for every polynomial runtime you have, I have an exponential runtime that I would prefer.",1540443904.0
Plazmatic,very good video!,1540393805.0
Mukhasim,The music gets distracting after a while.,1540399499.0
BubblegumTitanium,Really good!,1540401158.0
all-your-bases-are,Great video. My attention span is short but it piqued my interest due to a recent issue that we solved that we determined to be an np problem. Once you convince yourself (or others) the problem is solve-able then it sometimes is.,1540405937.0
TKAAZ,I think you the part on reducibility is glanced over way too quickly. I think you need to introduce the concept and stress that we are talking about reductions that have polynomial size in the input as well. Maybe mention the Cook-Levin theorem?,1540476711.0
KrombopulosRik,nice vid man.  very nice an easy as you can make that stuff I think lol. to make it understandable anyway. I'm math dumb an still got it.  =) ,1540477424.0
KillerN108,I know you! Your video explaining RSA encryption algorithm is the BEST on YouTube. Nice work and Thank you!,1540482190.0
fracta1,"Thank you so much! I'm in a theory of computation course right now, and I feel like this helped me fill in some gaps in my knowledge. Do you think you could do a quick one on performing polynomial time reductions? It's really confusing to me, for NP hard problems specifically. ",1541770667.0
ggPeti,[Sauerbraten](http://sauerbraten.org/README.html) is a cool open source hackable game. Here's a tutorial for the map format: [https://incoherency.co.uk/interest/sauer\_map.html](https://incoherency.co.uk/interest/sauer_map.html),1540550416.0
CorrSurfer,"The question is where in the bay area. In Fremont, it should not be bad. In San Francisco, not so much.

Also, please add a current unit the next time (in the title). In Rupees, for instance, it's not a great salary (""bay area"" could also mean another bay area outside the US)",1540372822.0
Tr0user_Snake,Check stackoverflow's salary calculator.,1540372773.0
TwoSoonOrNah,So 135 total year one? Read reviews and pending that I'd take it. Can double it in 2-3 years pending how much ass you kick.,1540373182.0
TrammelBranch,"Adjusted for inflation, that's almost exactly what I started at in the valley in 1995, and I had a PhD. Take that for whatever it's worth.

I think this is a good offer, as long as you don't plan to live in an expensive area. Which, of course, depends on where exactly the company is located and/or your tolerance for commuting. Traffic there is terrible and mass transit nearly nonexistent.",1540383581.0
yung_AI,"wow lot of great feedback here, going to mention this is AFTER negotiation, really the best I can do for negotiations. I agree stock is the most important here but they would not budge on it. I think something I need to remember is that its not permanent, its a start and if I find something better in a year then so be it. Love the experience in this sub reddit, and the clarify by the bay I mean right in san francisco. ",1540406544.0
hackingdreams,"It's in your best interest to negotiate, even if it's just a little. Worst they say is no - if they want you bad enough to give you an offer, they probably won't rescind it just for asking for a few percent more. Existing salary is the best predictor of future salary - every percent counts for raises. Also, you left off the bonus and the vesting schedule for your RSUs. What about a 401k match? HSA contributions? Commute credit? Personal hardware budget? Gotta come up with numbers for the total package in order to really see where you're at.

I'd probably immediately go back and ask for more stock and slightly more salary. The stock is a big deal, as it'll most likely grow much quicker than your salary will, and if you want any hope for buying a home or condo in this area, the stock drop + maturation is probably your best bet for a down payment (outside of long-shot investments/gambling/getting very lucky on Ethereum trading and altcoin arbitrage and getting out early enough to pay more in taxes than a few years of your salary)... but even so, you have to get over the psychological hump of getting a jumbo loan and dumping literal millions into a completely average townhouse in Sunnyvale that may not mature much more in value that you could have bought with an all cash offer in the midwest for $140K without even liquidating any investments... err, maybe I've gotten a little off topic...

I think my first Silicon Valley tech job (after being laid off from the company that acquired us ultimately to kill the competition, despite paradoxically insisting upon relocating the group of us out here to work on the product for six more months...), I got around $100K in RSUs vesting 12.5% every six months over 4 years I think? Forgive me, it's been almost a decade and it's 3AM. (And I even remembered thinking back then it was a bit of a lowball for my level of experience, but they had me over a barrel since I needed the job urgently after being blindsided... I wasn't much older then than you are now, and far more poorly formally educated.) Wound up being worth around $350K by the time it all had vested - that's why you want every single one of those RSUs you can get and stay to vest those. Try ignore the golden handcuffs after that initial block vests though; usually you're primed for a new job and can get a better income bump by switching companies after a few years rather than sitting at one for a decade just to vest - nobody in the Valley really rewards loyalty like they should.",1540377559.0
anon4357,"LOL ""New"", ""Powerful"", ""Fast""",1540362575.0
grizwako,[https://simonmar.github.io/pages/pcph.html](https://simonmar.github.io/pages/pcph.html),1540377009.0
jmnel,"Not sure what languages you are using but I found *C++ Concurrency in Action: Practical Multithreading* by Anthony A. Williams to be insightful.

I also suggest learning a library like Intel Threading Building Blocks. It has great documentation and I believe there is a free ebook available.

Writing safe and efficient concurrent code requires a dramatic shift in thinking.",1541135582.0
paralysedforce,"I'm not really sure if this is what you're looking for, but if you'd like to see some interesting applications of parallel GPU programming, you should check out [NVidia's GPU Gems](https://developer.nvidia.com/gpugems).

If you want a general textbook on concurrent programming, I would recommend The Art of Multiprocessor Programming by Herlihy and Shavit.",1540436578.0
IdealImperialism,"Com Sci is the best tech related degree, the maths involved and lectures on computability are very interesting.

Plus if you slog it out at university doing com sci, you'll be much further ahead that most of your peers in the industry.",1540360667.0
beeskness420,If you have more questions about the degree you could check out r/csmajors,1540362904.0
maheshhegde,"Algorithms are intertesting. Some of free books I can name are `Open Data Structures`, yale university lecture notes on `data structures and programming techniques`, and MIT OCW courses appear nice.",1540566835.0
Gh0st96,"Nah man age is never a problem for getting better education. It's good that you have covered most of CLRS and and have some idea of what interests you in the field . Studying for a degree gives you exposure to more fields that you previously did not know existed , and makes a better and well rounded education. I'd say go for it !",1540358729.0
blatantforgery,"My hunch, state funding/support. ",1540349797.0
jake_schurch,New Pied Piper,1540353057.0
quarkie,"Old World societies are very much offline and are not interested in web and tech as much as in the US and emerging economies. Startup in the Netherlands is a waste of money, - people just don't care",1540353657.0
TrammelBranch,"I currently do machine learning and data analytics stuff. After 2+ decades the work is still pretty cool, though I’m really over sitting at a desk indoors all day. ",1540348099.0
,[deleted],1540353100.0
tomridesbikes,"My personal experience: I graduated from No-Name State with a CS degree and a GPA I'd rather forget about. My first job sucked, only upside was international travel. After a year I quit and got a job with my current company. Worked really hard to prove myself over the last 2 years and now I am transitioning into a position that has more actual CS and I am working with PhDs in research. If I could do it all over again I would find a field that I find interesting (Speech rec, NLU, AI, quantum) and go all in and get a masters. I am extremely lucky to be where I am today and it was not easy. ",1540346865.0
khedoros,"Are you thinking of computer science jobs, like in academia, or something more like software engineering? My software engineering career has treated me really well for the past 10 years, but it's kind of a job on a treadmill. You've got to keep up with the state of technology and keep your knowledge relevant.

I graduated from a Cal State with a decent CS program, but a GPA that I don't like to talk about. I really clicked with the people at a company where I interviewed right out of college (2nd interview I had), and started work there a couple months later. That was an awesome job for about 5 years, then an OK one for another two. I switched teams within the company and stayed another 3 years, then got hit by a group layoff.

So I'm hunting again for the first time since I graduated. It sucks, but even though my current city doesn't make it onto the map of tech hubs, and even though my specialty is a little niche, I'm finding plenty of prospects in my area.",1540352200.0
xkqd,"I graduated from meh university with a meh gpa and spent 2 years at at a bank that rhymes with JPMorgan&Chase where I stunted my career for 2 years. Following that job security program, I moved over to a competitor bank literally across the street to make it to 6 figures by 23. I work pretty hard when I do but average \~45 hour weeks with plenty of ping-pong and redditting time sprinkled in there.

Most days, I write software. The other days are my weekends where I sometimes write software.

&#x200B;

Edit:

I failed to read your post before writing the above, but I considered long and hard going to school to be a nurse. Some days, I still wish I did. Every once in a drunken bender, I feel bad about ratfucking society by being an engineer in banking.",1540347526.0
kyleschaeffer,"I have spent most of my career building web applications. Twelve years ago I was building pure HTML/CSS/JavaScript apps and I felt like I had mastered my trade. The iPhone came out in 2008 and suddenly I knew nothing. The mobile web exploded and I had to learn everything again. Time goes by. I became a responsive design master, and again felt like I had mastered it all. XHTTP requests really took hold and single-page applications and frameworks exploded into the scene. Again, I knew nothing. This happened several times over with the introduction of technology like tablets, reactive interface frameworks, new versions of HTML and CSS, the introduction of ES6, so on and so forth. Today, I’ve set my sights on machine learning, data science, and artificial intelligence.

The acronyms/technologies don’t matter in this case. The point is that there is always, and always will be, something new. I think the most important question for you is: do you like to learn? Perhaps more importantly: do you like to self-educate, and do you see your self-education as something you could sustain for decades to come?

I love this field, and I love to learn. I’ve seen other developers burn out, or move into the business side of things because one day they just decide, “That’s the last technology I’m going to learn. I think I’m done.” When I’m hiring people for my business, I value an “ability to learn” over “X years of experience” because I truly think that’s the most important skill to have in this industry.

Best of luck! I hope this helps.",1540353019.0
thebishopgame,"Well, I wouldn't have written it this way, but I'm pretty happy with how it ended up. I did Comp Sci at a decent/good tech school that isn't a household name with a manga cum laude GPA, did a short stint in game development (the reason I did comp sci in the first place) and then promptly ditched all of it to go to music school and start working in recording studios, intending to never do programming again.

Fast forward a few years and I get a job as a product tester for a company that makes digital guitar gear to make ends meet. Through sheer serendipity, they simultaneously have trouble filling an engineering position and find out that I have a dev background, and long story short I'm now somehow the lead firmware engineer on their flagship product - something that I am the target demographic for and use daily as a major element of my music making. It's challenging and engaging,  I care deeply about what I work on, and I get to learn from some incredibly talented engineers. Also, because they are music company, they are extremely cool about me needing to work remote and do music stuff and I've coded pretty significant parts of our products from tour busses, so there's that.",1540357822.0
,[deleted],1540353520.0
simplethingsoflife,13 years healthcare IT Architect here. I love my job and work daily with clinicians. Feel free to ask me questions.,1540393284.0
gabriel-et-al,"At work I maintain large legacy entreprise-level systems. I make US$7000/year (yeah you read right) because 3rd world country.

Computer Science is beautiful and I'd love to leave my job to apply 100% of my time in academic research, however it's not possible currently. Someday, maybe...",1540353807.0
ianwold,"I write regular .NET enterprise apps, and I really enjoy my job. It's on the easy side of CS careers, decently laid back, and I can keep up my coding skills and stay in touch with things in the field/industry, which helps me when I go to work on side projects doing some more ""CS""-ey things. All in all very positive for me.",1540356889.0
uint64,"I am at a university in the UK doing things with computer vision and deep learning. Overall I think it's great and I can't see myself wanting to do something else any time soon. You need to make sure you have some hobbies away from the computer, preferably something where you actually move.",1540363708.0
iamrob15,"This is coming from an industry standpoint. I have no idea about academia / research institutes. 


Most CS programs that are considered Bachelors of Science ‘include’ a math minor. By that I mean you may only need one more class to complete your math minor. 


Some classes may be too difficult for you based on your school. I went to a state school that was not top tier in CS, but they had a well established CS program and my degree is accredited from a top 10 engineering school. I did not find any of the courses too difficult whatsoever, but I did have to put in a moderate amount of effort. I probably averaged about 3-4 hours of studying and homework for all my courses per week (yup I was pretty lazy, but I still had a solid GPA for computer science). 


If you can stand looking at a computer most of the day (let’s be honest most white collar jobs are like this now). 


If you enjoy solving problems and thinking in an abstract way.


If you have a ‘can do’ attitude - for the most part. Majority of developers I have met think along the lines of ‘I can solve any problem’. Obviously not curing cancer overnight etc, but anything that would be feasible with current technologies. 


Deadlines. This is not to scare you out of CS, because everyone has deadlines. However, business individuals think in terms of money. How can I monetize this software solution. Alternatively how can I make this process more efficient (automation, etc). You may end up working on weekends, because the business expects results and have typically have little to no clue as to how the development process works.


Now for a few more positives: I work from home nearly everyday. This requires me to be more self motivated than most, I personally thrive in this setup as I do not have to get ready for work and drive to work which could range from 30 minutes out of your day to 2+ hours out of your day. 


The current market is hot for full stack developers so the pay correlates fairly accurately with this. If you do well in school & have internships & you negotiate your salary it will be LIKELY you will end up in the top 10% salary range compared to your peers about your age. I had almost no issues finding a job when I looked for a new job with about 2 years of experience. I was surprised to receive nearly a 20% salary increase and I was not underpaid with to begin with. 


Let’s be honest a CS degree (with a solid resume) can get you in the door at pretty much any place to work. If you want to work your way up the corporate ladder you can always go back to school for your MBA or gradually move into less technical and more business oriented roles. It is much easier to go technical > business than business > technical. If you want to be management you will have the technical advantage over those who went to business school. 


These are a few factors I considered / learned during and upon completion of my BS in CS. BS in CS - that one cracks me up every time. 
",1540391550.0
RogueNinja64,Most likely you're going to end up in software. If you're into finding new things to solve there's plenty of it out there. I moved from a backend integrations engineer to more UI work and love the UX challenges. There's a lot more to it than just making things line up. Solving some of the user frustrations and creating interfaces is super enjoyable. ,1540398181.0
datlanta,"I went to a no name and work at a university as a researcher. It's dope and I love it. It's mostly the learning and uplifting environment that keeps me happy as the work can vary from soul sucking to life defining and it's constantly changing.

Before this I worked at a super small industrial controls shop for shit pay... But I still kinda liked it. The variety and robots were the draw. ",1540354265.0
dwkeith,"I am a high school dropout and work as a software engineering manager in Silicon Valley. I don’t recommend taking my exact route, but have had to opportunity to work on many great products, from Genius Bar software, to Xcode & iCloud at Apple, followed by a ton of different projects at Nest over the years and now Emerald Cloud Lab, where we are using software to accelerate scientific discovery. (including drug discovery) In all three companies remote work was few and far between, so we are completing with everyone else in the Valley for top talent.

Personally I have loved my career: making computers more accessible (Apple), saving energy (Nest Thermostat), saving lives (Nest Protect), to accelerating scientific discovery at Emerald. Working on world changing products is fun.

So, in short, software engineering is what you make of it, but opportunities are not evenly distributed throughout the world. Depending on why you chose nursing you may be more interested in a job in MedTech as it combines the two fields and lets you code for the greater good. (Mostly, see capitalism)",1540354300.0
ztherion,"I'm an engineer at a Fortune 500. I'm a colllege dropout who makes a very large salary (plus benefits and equity), has very flexible scheduling (outside of on-call work) and works on essentially the same kind of stuff I did as a hobby as a teenager. Fuck yeah I enjoy my job",1540353702.0
persnickIT,"I was Comp Engineering. Currently work for a major conglomerate with a lot of software/engineering jobs. 

Industrial controls + advanced software and analytics is a hot area. 

Traditional controls engineers are usually blue collar workers who learn to program. Many places recognize that an actual programmer can do the job faster and better. The only caveat is you also need controls experience/knowledge. 

Look for PLC/DCS programming opportunities. If you can find jobs that also reference “data analytics” or “machine learning” you’ll see the pay go up and the jobs are usually more interesting. 

I really enjoyed the coding part of my education but I thought it might be boring full time. Controls programming is nice because there is usually a physical recognition of your work. 

Oddly enough, I actually did a ton of international travel when I started. It was amazing but it gets old fast. Look for jobs with startup/commissioning accountabilities if you are seriously interested. Obviously the end product needs to be international but many opportunities seem to be these days. 

(I don’t recommend working for an EPC....)
",1540353215.0
cas8,"Yes.

I was with one of the Big Four for a while doing primarily front-end work building user experiences for what could be considered clunky and complex sets of features. I really enjoyed my time there and learned a lot (especially about software engineering in a larger team dynamic and other things you don't get in a typical CS degree path).

After that I was fortunate enough to get an offer from the company I've dreamed of working for since I was in high school, and it's definitely living up to the dream. I am excited each day about going to work and doing what I do. Unfortunately, it's a bit of a one-of-a-kind company, so take my enthusiasm with a grain of salt, as I consider myself extremely lucky.

Overall though, I think if you find yourself enjoying software development in the scope of persona projects and/or CS classes, it's not too farfetched to imagine yourself also enjoying a job doing it.",1540353607.0
CookieLust,"I'm an old nerd. I love all things tech. Grew up with the first personal computers. Gamer. Electronics. Robotics. 3d printing. I do internet stuff right now. I enjoy where I am. I work with other nerds like me. I think it's important to do what you enjoy, thus finding others like you to share in the experience. ",1540355098.0
p5y,No.,1540370353.0
Jezon,"Does **everyone**...

Easy answer: no.

It's so weird when picking a career because you pick it based on certain things you like but those things often have nothing to do with the job you're going to do. Just one thing to remember about a computer science career is you're probably going to be staring at a computer for 6 to 8+ hours a day and dealing with a lot of emails and meetings Etc. Sounds fun  but  just keep in mind that that's going to be your reality for decades. And there's a good chance he'll be working on a project that isn't very interesting at least some of the times.  There's also stress about deadlines and stuff, you can probably expect staying late Some Nights depending on the industry you're working in. For example the game industry can have crunch time lasting many months when a deadline approaches.

There are of course many benefits but I just thought I would mention some of the downsides.",1540373571.0
mjgood91,"I do it all at a small business - system administration, application development, web design, API integration, etc. The office is less than ten people and I've got a good bit of stuff automated, so overall it isn't too bad.


There's something new every day. Sometimes it's stressful, but I have a very understanding boss which helps a lot. I'm happy doing what I'm doing",1540382230.0
Chris2112,"Most people with CS degrees go into IT or Software Development or something. Few actually go into ""computer science"" jobs which are pretty much purely a academic (similar to a mathematician). I can't speak about the latter but as far as Software Development goes I enjoy it but it's the kind of job that if you don't have a passion for it you may go insane because it can definitely be very frustrating at times ",1540385543.0
MiddleEarthChick13,"If you want to meet in the middle you could look into Clinical Informatics. This is a huge field right now. I graduated from a smallish University with a degree in music but ended up working my way up through a hospital. I'm now doing clinical Informatics (programming an EMR) and I love it. Every day I learn something new and there are constantly new innovations in the software. Plus the work itself is very flexible. I do have a desk but really as long as I have a computer and phone, I can work anywhere. ",1540390722.0
specimen_m,"I actually don't have a CS degree (I did mathematics for my undergrad and master's), but I am a pretty decent programmer and worked as one for 3 years. My experience is that whether or not it's fulfilling depends on who you're working for and what projects you get a hold of.

Anyway I got sick of industry pretty quickly (the lifestyle just doesn't suit me) and now I'm doing a CS PhD in a field I'm really excited about. I think I'm going to enjoy research very much :)",1540356306.0
NullClicker,"Well, I've been in your shoes.

I've been a male nurse for close to ten years. I enjoyed the job for most of those years. Helping people is very gratifying, it puts a smile to your face and soul when you know that you contributed to the process of someone return to a healthy state, if you forget the fact that some people attribute their wellbeing exclusively to the doctor (depending on the culture and specific area you are in off course).

It is mostly a stressful job. Not only because of the crazy shifts that go entirely against your purpose(in a sense that you are helping people while degrading your health. Ex: A 10/12 hour nightshift), but you also have to cope with unorganized teams, irresponsible (non)Team workers, team workers with very bad attitudes( be it because of low pay, hard shifts or sometimes regret that they couldn't make it to be a doctor...yes, very often), a lot of responsibility with less of a gratitude, and off course the bad to medium pay/work conditions.

Its not always like that. There are very pleasant work environments, with medium pay/conditions and very organized and responsible teams, but it is hard to keep it stable because of the lack of personal (at least in the countries where I worked). It depends a lot of the country, area and structure/organization of the place you work.

But...since computer science has always been something that was stuck in the back of my mind, I decided to try out the shift two years ago and go  for a Bachelor. 

Do have in mind I hadn't studied anything even slightly related to logic, math or programming in quite some years, and on top of that, I'm in a foreign country where the main language is not English ( yes I know, I took the hardest path I could get...still, I was looking for challenge in life and I found it in mountain size lol), so I have to say I'm working on some first semester courses. Also doesn't help that I'm somewhat shy and have some trouble getting help or making networks.

It is being hard for me, and I sometimes fantasise the nightmare of not making it, but even so I don't want to give up that easily. If I fail, I will try another way.

I find the amazing diverse possibilities of career very thrilling. There's something for everyone and the prospect of being able to help society not just for the present but also devising solutions for the future really touches me somewhere deep.

So in my biased opinion, I would say, try to take some time to have a peek at what's the bachelor and the classes like. In most universities you can probably check on their website the composition of the courses and the time and place so you can sneak in and hear a lecture. Search online for exercises and course materials from universities and try some. You'll then have a better idea if you want to do it or not and be ready for what's coming.

Have fun and explore the world!",1540356263.0
lavahot,It kind of depends. Do you enjoy your life now? Do you enjoy the company of women? Do you mind dealing with socially awkward people for the rest of your life?,1540355931.0
AaronKClark,"No. Not everyone enjoys computer science. If you had even tried to do a search in this subreddit you'd find many people unhappy in this field.

PROTIP: Learning how to search is step one in a computer science career.",1540393644.0
naval_person,"Do a little research and find the capacitance of one memory cell in an SSD.  Then do a little more research and find the lifetime that the manufacturer guarantees a cell will hold its stored data.  Then do a little more research and find out the supply voltage for this SSD.

Call these three numbers ""C"" and ""dt"" and ""dv"".

Plug them into the equation

* I = C \* dv/dt

and solve for I.  That's the current in amperes (coulombs per second).  Divide by ""q"" the charge on the electron and you'll get the leakage rate in electrons per second.  I think you may be impressed at how very VERY few electrons leak away in one second.   Do some quick arithmetic to find out how many electrons leak away in one year.  It's amazing!",1540347964.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1540329561.0
destiny_functional,">(4) A Unified Model of the Gravitational, Electrostatic, and Magnetic Forces

>In this note I present a unified model of the gravitational, electrostatic, and magnetic forces that is consistent with the model of physics presented in [1], without space-time, that allows for time-dilation due to gravity, and the gravitational acceleration of photons.

>With these additional notes, I've presented what I believe to be the outlines of a simple, complete, and physically intuitive model of physics, rooted in information theory, that allows for physics beyond the Standard Model.

This isn't computer science and as for physics you have shown repeatedly in your comments (as well as your ""work"") that you have a very lacking (to non-existent) understanding of basic physics, let alone are able to provide alternative explanations/models. 

Stop flooding the subreddit with crackpot stuff.",1540630945.0
pamplemouse,"going by the OCW course: 

* consensus: raft, etcd, zookeeper, blockchain 
* MSP: networking
* clocks, deadlock, mutex, : distributed databases

",1540319359.0
DrSweetscent,"The important question you have to answer is this: is your approach better than running the k-dimensional Weisfeiler--Leman? Because that's pretty much the gold standard.


This paper might be a good starting point:

https://arxiv.org/abs/1704.01023",1540298081.0
jarekduda,"[Strongly regular graphs](https://en.wikipedia.org/wiki/Strongly_regular_graph) (SRG) are the toughest cases of [graph isomorphism problem](https://en.wikipedia.org/wiki/Graph_isomorphism_problem) – it seems extremely difficult to distinguish their vertices with local descriptors, or to get different invariants for such two non-isomorphic graphs with the same parameters.

They seem also extremely difficult to distinguish from algebraic perspective: testing Tr(A^k ) = Tr(B^k ) for k=1..n, where A, B are their n x n adjacency matrices, gives us certainty that they are similar: there exists orthogonal matrix O such that O^T A O = B. 

However, we are interested in existence of such permutation matrix, which is a very special type of orthogonal matrix: having only 0 and 1 coefficients.

The problem is that the space of possible matrices O is huge if A has large eigenspaces, and for strongly regular graphs they are the largest possible – we have large matrices having only 3 different eigenvalues.

I recently got **simple invariants** (of isomorphism) which are able to distinguish tested SRGs – define:

t(A)_ab = sum_ij A_ai A_aj A_ij A_ib A_jb

then beside testing Tr(A^k ) = Tr(B^k ), test also Tr(t(A)^k ) = Tr(t(B)^k ) for k=1..n.

This 't' function uses kind of generalization of standard [matrix functions](https://en.wikipedia.org/wiki/Matrix_function) – we would obtain them using linear graphs (degree 2 for standard product of matrices) for matrix operations, this one is for a more complex graph (any degree, getting e.g. sum_a M_ab M_ac M_ad ). While standard matrix functions would not distinguish SRGs, this one includes some strange tricks – can even reduce degeneracy of eigenspectum, what is impossible for standard matrix functions.

For some graphs (no nontrival automorphisms) it can completely reduce degeneracy – getting unique eigenvectors, each one determining a nearly unique vertex order: presented diagrams are adjacency matrices for the same 29 vertex SRG, but using 28 different nearly unique vertex orders defined by 28 eigenvectors (first is trivial). [Here are analogous for 9 different SRGs](https://i.imgur.com/4dGEmeS.png). Here is for a [35 vertex SRG](https://i.imgur.com/D8diBal.png). Here are [all the files](https://www.dropbox.com/sh/e4vyf5njr48cq71/AABjOckfR-d6sdbr83dPMKOFa). More details are on page ~15 of https://arxiv.org/pdf/1703.04456

This test is a polynomial algorithm (n^4 ), which nicely works experimentally (sometimes requires also t(A+I)), but there is no proof: that no two non-isomorphic graphs will give the same all invariants.

Any suggestions how to attempt such proof?",1540289733.0
SacredHenry,"Yes, smart word, smart saying, smart agreeing word. ",1540294671.0
slaphead99,Could some generous person explain the practical uses of these puppies? Assuming there are any. ,1540308392.0
NiceGuyMike,I see a sailboat,1540305848.0
combinatorylogic,">  Don't test your candidates on programming riddles, challenge them with real-world problems in a realistic environment. 

Not this disgusting bullshit again...

Whiteboard riddles have a huge advantage - they're *simple* enough to fit into 10-15 minutes.

Hardly any real world problem can be solved in 10 minutes.",1540292999.0
ChoccyMilkshake,A rubber duck. Debugging is something programmers do a lot.,1540265730.0
JLoosifer,"A Rubik's cube, because I know what the end result needs to be but most of the time I'm just fucking twisting and poking shit until stuff happens and I look smart.",1540264834.0
Gh0st96,Your laptop/PC/workstation ? Why hasn't anybody suggested the obvious ?,1540292048.0
WhackAMoleE,Coffee cup.,1540313448.0
nalta,"Don't bring in anything and say it's all in the cloud?


Bring in a cloud?",1540267763.0
ghedipunk,"An object to represent computer science?

An inverted telegraph repeater.  Literally a NOT gate in its first form. From it, you can build a fully functional general purpose computer... (it would be far slower than even the first computers were, since those computers used vacuum tubes... but you're presenting an abstract representation of a profession which has been founded on abstract representations ever since even before Grace Hopper created the very concept of compilers...)",1540339446.0
asdfman123,"You could bring out a posterboard showing a programmer ""object"" with properties like name, age, gender, etc. and methods like WriteCode() and explain the basics of OOP to people. ",1540268296.0
feedayeen,"Man, I kinda hate this phrasing, 'your major and who you are' and I think that you might be misunderstanding the project. The 'your major' is probably just for the students who don't know anything else interesting about themselves. 

I'm paid to be a programmer, but not even half of my waking hours involves programming. If I where picking an object to represent me, it wouldn't be the thing that I only do because people pay me. If anything it's the things I want to do so much that I pay for it! For me, that'd be something I built and am proud of!

If you want stereotypical CS/programmer stuff. There's not much physical stuff. Use a sharpie (they're erasable) on a portable whiteboard and make some diagram of an algorithm or system like an interview is probably the most accurate physical thing. Printouts of doc pages and StackOverflow posts. None of these are really good TBH. ",1540279460.0
datlanta,"TBH I don't understand your problem enough to be specific in answering your problems. But.

FPGAs should be capable of helping ya out. Memory might be an issue, but generally what you would do is create some block of logic. Then you can instantiate that logic as many times as you want or do some kind of temporal folding or whatever provided you have the resources (DSP slices/LUTs/etc) and you can execute them in any order or all at the same time. And FPGAs are fast.... Fast as shit. So state changes shouldn't be a problem for you and I'd be surprised if they couldnt do analysis faster than 12 seconds. 

The only downside is price and the skill wall. Fortunately, high level synthesis is a thing and you can write for FPGAs using C. ",1540357727.0
marcblank,A perfect world can’t be improved so the question is meaningless. ,1540273620.0
MasterBob,https://teachyourselfcs.com,1540291008.0
PensivePhilosopher,What is your background? What is your math level?,1540257347.0
ConsolesAreOverrated,I always recommend : https://github.com/ossu/computer-science,1540316989.0
Gh0st96,I guess start from discrete maths basics. Then move up . You'll get an automatic feel of what to study next I suppose.,1540292156.0
Politicalmudpit,N/a,1542341786.0
iwantashinyunicorn,"Quantum is a cool topic with lots of funding behind it, and ""science journalists"" have heard of it.",1540252875.0
AnonWales,Erhm...best bet is to ask your local library.,1540224201.0
spinwizard69,"Look up the author Tracey Kidder (I think that is right) who wrote “The Soul of a New Machine”.   This is a great read about the early years of the computer industry that revolves around companies that don’t exist anymore.   It isn’t computer science per say but I found it to be fascinating to read when I was young.  

As far as programming goes well you will get endless answers as to what to do here and why that path is right and others are wrong.   I prefer the ground up approach before graduating my to the easy.   To that end I would suggest learning to build programs from the ground up with a simple editor and command line tools.  The first language should be C++.  Frankly the goal here is to learn how the tools work at the lowest level so that you can better leverage advance editors and IDE’s.    After you develop a grasp of the tooling basics you can upgrade to an IDE and concentrate on building your programming skills.  For C++ programming I’m not sure what I would reccomend book wise these days.  Sorry!   What I can suggest though is to install Love mud and as much ch of the free documentation as you can find.    In any event one book I always liked is Accelerated C++ by Barbara Moo and one other.  It is a bit dated so I can’t say it is a good reccomendation to learn modern C++ but combined with other resources could get you started.  Like I said I’m reluctant to reccomend Accelerated C++, just really liked the books style and focus on getting people started.   In any event the free C++ sites are very valuable to the learning process.  

The next step in the learning process in my mind is todosome embedded programming on really simple micro controllers.   Ideally a PIC or maybe an Arduino.   You will want to learn and program your first projects in assembly.  This effectively is the niche of embedded programming and is an underserved niche if you ask me.  In any event the goal here is to learn a bit it assembly programming but the bigger goal is to learn about computer architectures (really simple ones in this case), the low level details and I/O.   Again no books to suggest but plenty of stuff on line.  Almost all the required documentation and software is free online.  The required hardware is related felt inexpensive.   One thing to consider is that Arduinos and their IDE puts you right into a C++ like environment    This isn’t bad but you mis the learning process of starting up a processor with an assembly program.   Some will try to dismis embedded programming for a 14 year old but I see it as an ideal place to learn a hell of a lot of useful information.  Beyond that these days Arduinos are widely used by students in robotics and industrial arts related classes.  

At some point after starting to learn C++ you will want to pick a scripting language or two to learn.  I highly suggest learning Pythons n at this time. It is easy to get started in Python but the language has Eno their depth to be a serious tool.  It would s a fantastic complement to C++ when it comes to programming.  

Sometime after getting C++ and Python down you might want to consider a modern language.  This could be 3 or more years from now or next month.  Languages that come to mind are Swift, Rust and Julia.   All three of these are under heavy development so again your best bet for current info is the internet.  

By the way, there are people that will argue against C++ due to its supposed complexity.  It is true that it can be complex but you do not need to used by the complex parts to start learning programming. I prefer to see people start out on C++ because it can take them wherever they want to go in the comp-sci world.  Arduinos use a variant of C++, the big 3 all support C++ compilers, it is used on super computers, airplanes and backhoes.  As such there is an almost endless supply of information both in books and online.  Some of the online stuff is bad so be careful there.  

",1540252455.0
sammyIsLife,"I don’t know how experienced you are already, but I’d recommend to you some basic books such as Al Sweigart’s “Automate the Boring stuff with python” and my personal favorite “cracking codes with python”. Now if what you meant is REAL computer science and not just computer coding, such as computer architecture and organization, database manipulation, etc, then I’d recommend looking into some college textbooks. Amazon has a rather huge selection so give them a try. ",1540225430.0
BlakeJustBlake,What sorts of things interest you about computer science? How strong are you with math? What do you use a computer for the most?,1540232698.0
Gh0st96,"Nice to see someone so young be so enthusiastic. If you want to learn about the mathematics behind compsci then I guess you should start with learning basic discrete mathematics - probability and statistics , combinatorics , some calculus.

And then move into basic Complexity theory. Read Intro to Algorithms by CLRS , one of the gospels of computer science really. If it turns out to be too advanced , then learning whatever is needed to understand the content of the book should be a good aim for you.",1540292583.0
NerdAtTheTerminal,"I suppose 14 is enough age to start learning programming. It need not be some kiddie language such as `scratch` but try learn C if you like systems level programming, or `python` is an easier choice

If not enter the GNU/Linux world and go on exploring open source masterpieces.",1540567404.0
rasulnrasul,"Start with programming C, and try to understand how programs works.( look at briefly how compliers, memory, micro processor, operating system works). Once you get some overview just dig into what interests you.

Also remember that computer science and software engineering are different.",1540749078.0
MajesticBalance,"Forget books. 

Just find coding examples and start copying. Start with ""javascript tutorial"" and ""python tutorial"". After copying and make it run, delete the code and try to type it out exactly, peek when you get stuck. Do one per day, repeat previous day's code when necessary, after 100 days you are a programmer.

Then start with MOOCS if you are still interested.",1541693359.0
greysvarle,"Concrete mathematics a foundation of cs is a good book.

If you are into coding then I recommend solving easy problems on programming websites.",1540299390.0
html_is_not_coding,"https://discord.gg/nC4KhVG ask here, this discord is mentioned for hacker but I'm sure they would be happy to help you out with general cs.",1540224059.0
gnullify,"The Code Book, This Machine Kills Secrets",1540211926.0
The_MPC,The Feynman Lectures on Computation are an absolute joy to read ,1540212231.0
Tier1Gear,"AIQ

The fourth age

The art of invisibility

Future crimes

Industries of the future

The war on normal people

The efficiency paradox

Enlightenment now

Bold: how to go big

Wtf? What’s the future

Everybody lies

Stealing fire

Zero to one

Cyber security and cyber war

Countdown to zero day

Messing with the enemy

The dark net


American kingpin

Dataclysm 

Irresistible 

The internet of us

Ask Gary vee 

Dark territory

The inevitable

Weapons of math destruction

The formula

Ghost in the Wires


Not all pure comp sci but most cover close topics
",1540214828.0
chntny,Algorithms to Live By,1540214299.0
neusbal,Gödel Escher Bach by Douglas Hofstadter ,1540215120.0
0PointE,Absolutely and unequivocally [The New Turing Omnibus](https://www.amazon.com/New-Turing-Omnibus-Sixty-Six-Excursions/dp/0805071660/ref=sr_1_1?ie=UTF8&qid=1540225199&sr=8-1&keywords=the+new+turing+omnibus).  There are all sorts of scientific questions and solutions along with proofs and use cases over the years usually surrounding computer science.  I loved everything I'd learned in this book and still remember learning what the Hamming Distance is and how it helped us develop a reliable way to communicate at such long distances with satellites and other such things.  I have a copy sitting on my shelf next to me atm so I got a photo for you to check out the [TOC](https://imgur.com/pyI5rfc) if you wish.  Truly a light favorite of mine.  ,1540226160.0
,[deleted],1540236628.0
sderyke2002,"Soul of a new Machine

Hackers

Cukoos Egg",1540224301.0
karl_das_llama,"It treads the line and maybe leans a little on the side of taking notes (I managed, but if I put it down for too long I had to go back and review stuff) but I'd recommend Charles Petzhold's ""The Annotated Turing"". It's built around reviewing Turing's seminal paper on computability where he defines the Turing machine, but pads the crunchy paper review with biographical stories and thorough explanations. If you managed GEB then I suspect you'd enjoy it. 

...man I really need to dust off and finally read through GEB...",1540213713.0
bdf369,Sorry if this is too CS101'ish but I really enjoyed the SICP book and it's online for free.,1540224684.0
got_succulents,Programming Pearls is a great read.,1540226007.0
space-creature,"I really enjoyed But How Do It Know, enlightening but not too demanding.",1540212936.0
quantum_dog,Quantum Computing Since Democritus by Scott Aaronson,1540213431.0
Santamierdadelamierd,"The annotated Turing, code the hidden language of computers by the same authors. They are fun anf useful without being textbooky.",1540217322.0
ReverseEngineered,"* The Pragmatic Programmer
* Lauren Ipsum
* The Design of Everyday Things",1540220656.0
,[deleted],1540242180.0
Be_True,"Computational fairy tales

https://www.goodreads.com/book/show/15891129-computational-fairy-tales

Computer science concepts set in a Fairy Tale world

Also by the same author

The CS Detective: An Algorithmic Tale of Crime, Conspiracy, and Computation

https://www.goodreads.com/book/show/28396776-the-cs-detective",1540220164.0
dasfsi,"I really enjoyed reading ""Expert C Programming: Deep C Secrets""
by Peter van der Linden, even though I don't really program in C nor ever wanted to.
It's quite easy to follow, although you probably do need to know a bit of C, and an interest in the *guts* of how things work (or, sometimes, used to work as it's an old-ish book)",1540220287.0
0xAAA,The Annotated Turing ,1540225853.0
xpressrazor,The Phoenix Project. It is related to devops.,1540227184.0
Cocomorph,"All of the following books are old; all are still interesting, and some are timeless. I do not necessarily endorse all the viewpoints in some of them. The quality is a bit uneven (from seminal to merely thought-provoking and worthwhile) and I wish I had more time to pick more, but I have to almost literally run and catch a bus. If I had to pick two from the following list, the top choice would be Vehicles and the second choice would be Metaphors We Live By.
  
---

[Vehicles](https://mitpress.mit.edu/books/vehicles)  
[The Society of Mind](http://www.simonandschuster.com/books/Society-Of-Mind/Marvin-Minsky/9780671657130)  
[Cambrian Intelligence](https://mitpress.mit.edu/books/cambrian-intelligence)  
[Affective Computing](https://mitpress.mit.edu/books/affective-computing)  
[Metaphors We Live By](https://www.press.uchicago.edu/ucp/books/book/chicago/M/bo3637992.html)
  

Mathematics, but mathematics is, needless to say, relevant to CS:  
  
[The Mathematical Experience](https://books.google.com/books/about/The_mathematical_experience.html?id=XdXuAAAAMAAJ)  
[A Mathematician's Apology](https://books.google.com/books/about/A_Mathematician_s_Apology.html?id=beImvXUGD-MC) -- get the version with the foreword by C. P. Snow  
[Proofs and Refutations](https://www.cambridge.org/core/books/proofs-and-refutations/575FC8A6B4FAB79E649EDF5FBB9C6E10)  
  
Video games and learning:  
  
[What Video Games Have to Teach Us About Learning and Literacy](https://us.macmillan.com/books/9781403984531)  
[A Theory of Fun for Game Design](https://www.theoryoffun.com/)",1540229832.0
cyrillk,The Phoenix Project by Gene Kim,1540238540.0
BirdBlind,"The Clean Coder - Awesome book for the professional coder. It's not about the code - it's about the mindset of the writing the code. I highly recommend it.

&#x200B;

The Soul Of A New Machine - The story of a rising company and the unsung heroes behind the advancement of computers in the 80's. A good read, but it can admittedly be a bit slow at times.

&#x200B;

Algorithms to Live By: The Computer Science Of Human Decisions - We all apply algorithms in the real world to some extent, even if we don't know that's what we're doing, but this book takes that to the next level. It explains how you can use computer science concepts to enrich your daily life.

&#x200B;

The Mythical Man-Month: Essays on Software Engineering, Anniversary Edition (2nd Edition) - Even if you've already been working in the software engineering field for a while, this book is a great read of one man's experiences that many of us can relate to.",1540257900.0
theemptyqueue,"I would personally recommend the C++ standard library.  It contains of theory and practice, plus it has a lot of contextual information as well and teaches the basics of coding.",1540218683.0
SkepticalSagan,"You can always chill with some Ludwig Wittgenstein IMO.

/s",1540216065.0
donowhy11,The Singularity is Near - Ray Kurzweil ,1540218233.0
SOberhoff,"The Nature of Computation (the greatest textbook in existence, fight me)

",1540227080.0
henzosabiq,"1. *Jeff Hawkins with Sandra Blakeslee - On Intelligence.*   
Its thought-provoking, although I admit its a bit annoying when the author kind of fanboying too much over certain brain part (I don't know other words to portrait it; thats just how it feels) but its worth a shot. It made me realize deep learning is not *that* awesome (to the point it'll become a base for artificial general intelligence) and that there are so many room left to explore.   

2. *Nils John Nilsson - The Quest for Artificial Intelligence: A History of Ideas and Achievements.*   
I never thought I would enjoy history this much. I'm only at page 50 but its been a great read so far. ",1540232838.0
dat_cosmo_cat,"I read [Complexity: A Guided Tour](https://www.amazon.com/Complexity-Guided-Tour-Melanie-Mitchell/dp/0199798109) on a flight a few years back. It's a thoughtful and well written non-technical CS book, uses concrete real world examples with interesting historical tangents weaved in (I enjoy that  ""here's what people believed at the time/here's how this person figured out XYZ"" sort of stuff). It kind of reminded me of Hawking's [A Brief History of Time](https://www.amazon.com/Brief-History-Time-Stephen-Hawking/dp/0553380168/ref=pd_lpo_sbs_14_t_0?_encoding=UTF8&psc=1&refRID=8XAW8B3370YZ2M4YGZ1W).",1540233572.0
singham,"[Surfaces and Essences: Analogy as the Fuel and Fire of Thinking](https://www.amazon.com/Surfaces-Essences-Analogy-Fuel-Thinking/dp/0465018475) by Douglas Hofstadter 


https://www.youtube.com/watch?v=36OscZs3cCQ",1540239392.0
jmhummel,"[""Predictive Analytics: The Power to Predict Who Will Click, Buy, Lie, or Die"" –  Eric Siegel](https://www.amazon.com/Predictive-Analytics-Power-Predict-Click/dp/1118356853)

If you like books like ""Freakonomics"", you may enjoy this one. Similar style and depth, each chapter discusses interesting applications of PA, without getting really technical. ",1540240005.0
Druid-of-Luhn,"[The Pattern on the Stone](https://www.goodreads.com/book/show/496281) by W. Daniel Hillis. I was recommended this by an Oxford professor of Computer Science and I love it. It’s short, easy and abstract enough, but it is a very good read about the field of Computer Science.

[Intercept: The Secret History of Computers and Spies](https://www.goodreads.com/book/show/33828946) by Gordon Corera, is not quite as good as Simon Singh’s code book (already recommended elsewhere), but it is a very interesting book on more modern attacks and the different methods that are available, based on actual attacks.

[The Thrilling Adventures of Lovelace and Babbage](https://www.goodreads.com/book/show/29952886) by Sydney Padua, is a collection of comics about Babbage’s Analytical Engine set in an alternate universe where he built it and it can do anything. It’s fun and interesting, and is full of researched tidbits and the accompanying references, as well as many commentaries on and extracts from primary sources. The author’s website contains a lot of the material.",1540242773.0
blockcha1nboi,The Innovators by Walter Issacson. Good overview of the history of computing. ,1540245220.0
ErwanLikesRacketLang,The annotated Turing,1540257407.0
incompetentrobot,Probably Approximately Correct: Natures Algorithms for Learning and Prospering in a Complex World,1540257457.0
p_ryaaa,Algorithms to Live By ,1540264447.0
7itemsorFEWER,My software engineering professor wrote a book called the killer robot (by Dr. Richard Epstein),1540272363.0
lxpnh98_2,The Mythical Man-Month.,1540214961.0
N6ixty4rtnite,read scott aaronson's blog lol : shtetl-optimized,1540326160.0
NaiveSkeptic,"Once Upon an Algorithm: How Stories Explain Computing, by Martin Erwig ",1540392030.0
NerdAtTheTerminal,Thanks everyone commenting out here. It is worth a page in this subreddit wiki. Would love to see a curated list there..,1540567095.0
poptart808,[Operating Systems: Three Easy Pieces](http://pages.cs.wisc.edu/~remzi/OSTEP/)  It’s a pretty easy read and I found the footnotes to be funny sometimes. This is the most interesting and easy to understand textbook I’ve read so far and everything is available online for free. I liked the book so much that I purchased a hard cover version. ,1540288024.0
gunnerman2,Complexity,1540217305.0
James_ericsson,.,1540238862.0
BubblegumTitanium,Try bad choices! It’s a fun book about Algorithms and how they can be used in real life. ,1540293781.0
not-just-yeti,"Joshua Bloch's ""Java Puzzlers"" -- less ""puzzles"" than surprising-corner-cases-of-languages (some Java-specific, some much more general).  Each puzzle is on one page, and the solution on the reverse.",1540294812.0
ghedipunk,Surprised that I haven't seen Code by Charles Petzold here yet.,1540233001.0
Slideboy,why you rereading? read once and remmember. I suggest for you to read comicbooks,1540277929.0
DrSweetscent,"There was a lot of research into image segmentation using graph cuts, back before convolutional neural networks came and ate everyone's homework. That's probably a good starting point.",1540191489.0
The_Serious_Account,"My feeling is that people generally assume that there won't be any change. That, of course, doesn't prevent people from thinking about possibilities. 

General relativity allows for something called closed timelike curves(CTCs), which is a form of time travel. Time travel comes with the grandfather paradox, but this was [resolved by David Deutsch](http://thelifeofpsi.com/wp-content/uploads/2014/09/Deutsch-1991.pdf) by inventing a more restricted form of time travel. Using these *Deutschian* CTCs, Scott Aaronson and John Watrous [were able to show we could solve all of PSPACE in polynomial time.](https://www.scottaaronson.com/papers/ctc.pdf) Since PSPACE is almost certainly bigger than BQP, these would be much, much more powerful computers. But do remember this assumes time travel turns out to  be possible, which it almost certainly won't. ",1540190421.0
fluffynukeit,"Ability to DEBUG.  Not just your own software, but others', and your toolchain, and whatever else.  I'm in embedded systems, so this also includes how your software interacts with hardware.  I've recently seen a lot of ""stabbing in the dark"" when trying to get something working, let alone understanding what the problem is.",1540167011.0
bargle0,Humility ,1540170122.0
,[deleted],1540160673.0
Gyppi,"Humans are one of the hardest and most complex components of engineering great software.  Nearly all software is created in teams.  You need to master soft skills and communication.  Learn to explain your system designs in picture form.  Master the whiteboard!  Selling your ideas and design builds consensus and leadership skills.  Learn to automate everything, including your software tests.  Be an evangelist for high quality software.  Never compromise the high bar you set for yourself.  Pick 2 or 3 words you would want people to use when they describe you.  I picked trustworthy and dependable early in my career and it has served me well.  Notice I haven’t even mentioned technology.  I found the tech skills to be the easy part.  Enjoy the journey.",1540174189.0
codeandchords,"Be able to admit when you don’t know something and concede when you’re wrong. 

Engineers who pretend they’ve grasped something rather than asking or refuse to hear opinions and grow are always the weakest to me. ",1540174732.0
rockinghigh,You’ve listed skills around software development but you’re missing the skills related to project management: setting the technical roadmap and priorities.,1540166271.0
yawkat,Isn't this more /r/programming than /r/compsci,1540186325.0
combuchan,"Your ability to deal with people.  I have worked with people that were intelligent, but also arrogant, lazy, unfriendly, ""helpful asshole"" types, or even had personal issues like terrible odor.  You deal with it and move on.  Drama can consume a workplace and they pay you the same otherwise.  You should be in a spot where you can leave if the culture is too toxic, this field is in demand.

I switched specialties from datacenter infrastructure to security infrastructure and a friend who knew my manager told me to focus on humility and hunger.  

In devops, I strongly value people who understand the customer service aspect of it--in traditional software engineering the customer is an end user or QA engineer, mediated by product and management.  In devops, my customers are other developers and system administrators.  

I am massively annoyed by ""no,"" statism with regards to upgrades, etc, people who are slow to respond on slack, or fail to consider what the customer is asking...like neteng set us up a load balancer but their firewall rules are breaking things.  I don't have access to these systems and sometimes it's like pulling teeth to get them to use their stuff right.  They might have some technical competence in theory, but it disappears with their lack of soft skills and poor customer service (the other CS).  ",1540168057.0
Enlightenment777,"For each ""flavor"" of software type, there are other virtues that are important.

Great embedded software engineers tend to have a good hardware background.
",1540168519.0
bzBetty,"Figure out how you learn, then do some every month in an area you currently lack.

Different topics might have different learning styles. For example I prefer blogs over videos for technical concepts as I find a lot of videos are paced too slow.  Whereas on the business skills side of things I really enjoy audio books, the pace isn't as much of an issue as I pickup the concepts slower. 

I take advantage of my different down time differently - eg business audio books while driving, technical blogs while watching TV, hands on practice while at work for both.

Then i purposely share my learnings with my company and ask others what they're learning so I can find new things to explore.",1540172674.0
mr_ryh,"Documentation & [literate programming](https://www-cs-faculty.stanford.edu/~knuth/lp.html).  These are treated as an afterthought, but they rightfully belong to forethought and should be a critical part of execution, as Knuth understood.  Great software engineers understand that code is meant to be human-readable first, and machine-readable second; they should therefore strive to write documentation (and code) like Kernighan and Stevens and Bentley: concise, clear, and chock full of pearls.",1540181068.0
thedomham,"> Work well within or as a leader of a team

Communication is key. Being able to explain your ideas and solutions to your coworkers and to convince them that your concept is actually the best choice are very important skills. But that also spans specification, documentation and writing in a readable concise manner, both in code and prose. 

Asking for help when you need to and giving your colleagues feedback without becoming mortal enemies are also important acquired skilled. 

You really undersold that by putting it at the end of your list.",1540186787.0
Neker,"Seasoned software engineer here.

I see nothing wrong with your bucket list. That's a list every (software) engineer sould have in the back of their head at any time.

However, IRL, each bullet point would be the job of one individual, or most often, even a whole team. The whole list would be a sizable part of the mission statement of a whole IT department.

To your list, I would add some (un)savory bits :

* support users *(eeeek)*. Beware of the bug located between the keyboard and the chair.

* ergonomics, which you touched with *customer needs*, but this goes deeper

* deal with accounting : all of this costs money, the cost/benefit ratio of what you do will be questionned at all time. Most software projets hit a budget wall at some point.

* salesmanship. What is obviously a good idea for an engineer is often met with disdain or disbelief by the rest of the organization. Plus, you'll need to justify the cost.

* planning, scheduling. Software projects span days, weeks, months or even years and sometimes *decades*. They involve many people, several teams, deparments or companies.

* and yes, compsi. A good, deep, understanding of how computers and software really work will save your skin now and then.",1540197623.0
gltovar,"Understanding (at least at a high level) workflows and tools other people on your team use. 

So can ask your peers what kind off tools and plugins they like. You might learn about tools that make frustrating tasks a non issue, things like code snippet generation, code linting, dealing with refactoring, multiline simultaneous editing, and more.

Also talk to your non programmer collegues. A designer using Sketch to create a UI mockup or an artist delivering assets in a Photoshop file, you can get familiar or even comfortable with these tools to make quick revisions and to even implement general workflow improvements to make yours and their lives easier.  

",1540178882.0
mrexodia,"Write software, write software, write software. All in a team.",1540187650.0
mapcar-carmap,"The ability to read and comprehend code quickly, and ideally, to explain it to others. Much of your career will be spent working with code you didn’t write — you may spend more time reading than writing code.",1540193002.0
mc8675309,"Identify your particular skill set. You should be able to do lots of things but you should know your strengths and weaknesses. Use others on your team to fill in your weaknesses and do the same for them with your strengths. 

I manage a small group of software engineers and each one has very specific sets of strengths and they’ve developed those strengths and rely on each other to shore upntheur weaknesses. 

Examples: one guy is a master at taking code he’s never seen before and unwinding the original intent and figuring out where the flaws are. Another guy architects great generic libraries. Another is a big picture design person and yet another is strong on thinking through new code to identify all the corner cases and potential pitfalls. As a team we get a lot more don’t and at a higher quality than five people all working in silos. 

And that illustrates the key skill, teamwork. ",1540217301.0
__-_-_-_-_-_-_-_-_-,Looks something straight out of a software engineering textbook. Chill mate. Just know what you are doing and you'll be fine.,1540169189.0
feralwhippet,"the answer is, its highly situational. bootstrapped startup vs venture backed startup vs consulting/contracting vs big software house vs it in big non-software company vs ... are all very very different. so I think first you have to have an idea of what kind of organization you are targeting. a lot of the other answers here are from more of the big company perspective, just be aware that these are only addressing one facet of software engineering.",1540180163.0
Shafi,Communication. ,1540182880.0
YakumoYoukai,"You've listed a bunch of considerations that go into making a system work, and work well. But equally important is to consider what contributes to making a system fail.  Lots of people can design something that works under expected and ideal conditions.  That's the job.  But when everything is going to shit, can your system still function, or at least fail in a predictable way that recovers on it's own when the triggering fault is removed?  And does your system avoid reacting to failure in ways that make the problem worse?

This is typically the kind of thing that one learns over a career of experiencing failures of your own, or observing the failures of others.  To accelerate the process, I'd advise seeking out and studying root cause analyses of software system failures.  What's the core flaw in the approach that allowed the failure to happen, and how might you have replicated the same flaw in some system you've built?",1540188955.0
bartturner,"Curiosity and fortitude

",1540207160.0
stochastaclysm,Being able to say no to requirements.,1540208258.0
ShenanigansPara,"I would like to add, know your audience. In any project based performance measure Time, Money, and Deliverables (Product) seem to be the most important issues (in Business). If you know the audience you can manipulate the 3 to your advantage. So your team delivers a great product, on time and within budget.",1540212126.0
qoou,"SW Engineer here. 25 years experience. 
I read your list. What you have on the list is par for the course on most points.

Let me add to you list the qualities that are much more important. 

1.	works well with others. The so-called alpha geek is often a passive aggressive prick or total cunt. Don't be a prick (or a cunt). I realize you (and I use the plural generic you here) might not know if you're a cunt, so this is a difficult challenge). Contrary to popular wisdom from Game of Thrones, there *is a cure* for being a cunt. It's called **listening**. Listening is not just a pause in your own speech to give other the opportunity to bow to your wisdom. I mean really listen to others. Especially in situations when you just *know* you're right. I have never seen a design that did not get better with input from others. Period. This is number one item on the fucking list for a reason! 
2.	communicate. Learn to do it well. Communicate early if you're stuck. Seek out the senior members in the team and pick their brains. So this early. Seek out management and get a full understanding of the task. Offer help when you are aware of someone on the team struggling. And be humble about it giving and receiving. Give credit where it is due. Give frequent status update to your management. If you got help, tell them. 'Joe really helped me with this design' kudos to joe. Learn to get accurate with your status. 
3.	automate any mundane task that you find yourself doing more than a handful times - if you can manage it. In other words done spend 2 man-months automating something that won't pay off.  
4.	document your work so others can understand your intentions and design. Document your fucking technical debt. You-know, that quick and dirty hack you put in because you needed to make the code do something it wasn't originally designed for and you left a todo comment in the code saying you'd fix it if you had time. You don't and you won't - so document that shit and make management aware of it. 
5.	readable code is better than slick/elegant/fast code that **only you** understand. (See #4). 
6.	automated unit testing. (See #3)
7.	4-6 seem like common industry practices but you would be surprised.... ",1540218372.0
gibizer,"In many hard situations you have to take your bullet points and make well informed compromises between the goals in them, as you cannot fulfill all of them at the same time with given constraints (time, money, knowledge, people).",1540220974.0
ChadRStewart,"Thank you very much for the feedback everyone. They've been quite informative and have given me a lot of insight into being a good software engineer. I plan on rewriting my list taking into consideration the comments I've received (which overwhelmingly has been have strong soft skills or ""don't be a dick""). I might have forgotten but I'm not sure if I saw anything being able to learn new tech which I think could also be a virtue.

Again I really appreciate the feedback. One of the things I noted is a lot of the suggestions allows for a much more flexible engineer where my list is more rigid in scope. Anyhow, interesting notes.

Please continue with your feedback if you're still interested in sharing and again, thank you.",1540228107.0
JavaScriptPenguin,You might want to read Soft Skills by John Sonmez. ,1540162150.0
86LeperMessiah,"From a computer science perspective perhaps it would be to choose the right algorithm for the data set being handled. Even if we might not perceive a bubble sort from a quick sort because of current processing speeds, these differences can build up and give us sluggish experiences on our computers, for example imagine if all programs on our computers executed bubble sort instead of whatever is the optimal algorithm for their case, we would definitely notice that difference.",1540178680.0
iier,Boobs,1540164001.0
BuxOrbiter,"The minimum solution is traveling salesman problem. Represent road segments as nodes, and connected road segments as edges.

Edit: I’m probably wrong ",1540157650.0
gct,"You're on the right track with a graph approach.  Since they you don't specify it has to be in any way optimal, you can find an [Eulerian Path](https://en.wikipedia.org/wiki/Eulerian_path) on the graph, which like many graph algorithms (I find this surprising), can be done in linear time on the number of edges.  Your graph might not be traversable on its own, but you can easily fudge it I think.",1540161344.0
sderyke2002,Depends on how precise the answer has to be.  You could get the area of the city calculate the density of the roads assume typical separation of parallel road both east west and north south and then calculate total distance and divide by 30 mph as an average speed,1540160088.0
cheifminton,Drive down every road with a stopwatch.,1540161911.0
birningmongoose,"Since the question doesn't specify that you have to find out how long it would take to drive each road in a single trip, you could just represent the data as an array of lengths and average speed limit for each road. Divide each pair and sum the results. ",1540167167.0
very_sneaky,"Perhaps it would be satisfactory to answer in terms of time complexity? Best case theres 2 nodes for every road, worst case theres n-1 roads for every node (best case linear, worst case factorial). Multiply the relation by an average travel time to get an estimate
Edit: wont get you an exact answer but it should get you in the ballpark",1540162317.0
ImageWagons,About 5 minutes ,1540173417.0
richardathome,"The wording of your question leads me to this simple solution:  


Total length of time to walk down each road in city at least one = sum of the length of all roads in city / walking speed  


We don't need no steekin' graphs! (just a list of the lengths of all streets)",1540210739.0
combinatorylogic,"> e.g., the velocity of a neutrino is c, despite it having mass

Meh. No, it is not. It's very close to c.

Also, you seem to miss the whole point of where this mc^2 is coming from.",1540130925.0
LukeB42,Very interesting paper. Have you gotten any feedback from physicists yet?,1540131139.0
LiquidInsight,"On page 32 of your document, you predict a relativistic doppler shift that deviates from the prediction of special relativity. You predict a frequency deviation of f\_s v\^2 / c\^2 ((1+v/c) / (1-v/c))\^(1/2). You say that this deviation is small, which is it is, and wouldn't be observed using a Mossbauer detector. However, deviations of this level would be observed at even low speeds when using the incredible accuracy of an atomic clock. Your theory predicts a relative frequency shift of \~10\^(-15) from the source frequency for an object moving at 10 m/s. [Chou et al.](http://science.sciencemag.org/content/329/5999/1630) tested at such speeds, to a precision of 10\^(-17) (see section below equation 3), and found no deviations from special relativity. ",1540169191.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/math] [A novel derivation of E = mc\^2 using information theory](https://www.reddit.com/r/math/comments/9q7mlo/a_novel_derivation_of_e_mc2_using_information/)

- [/r/quantumcomputing] [A novel derivation of E = mc\^2 using information theory](https://www.reddit.com/r/QuantumComputing/comments/9q3lsf/a_novel_derivation_of_e_mc2_using_information/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1540159606.0
Studying_All_Day,"No you haven't.

&#x200B;

Einsteins equations are not properly said to be routed in information theory,at least at the level of access participants in this universe have to the source code.

&#x200B;

Either you have misunderstood all of science,or everyone else has.

&#x200B;",1540307117.0
cheeseless,"I know this is the weirdest nitpick, but I can't bear the word ""choco"". The mouthfeel of it, even when just reading it, is so absolutely awful that it wrecks my concentration.",1540140889.0
qwazwak,"Nahhh just quantumSort

template <class That> void quantumSort(Tvar* arr, size_t size){
     while(isSorted(arr, size)!){
         wait(1000);
      }
}",1540145267.0
c3cris,"So there are 5 possible routes each loop.

B-C-F

B-C-D-F

B-D-F

B-E-F

B-E-D-F

If the loop is ran 18 times the possibilities are exponential c^(n)

So we have 5^(18) =  3.814697265625×10^(12)  = 3,814,697,265,625 ",1540116660.0
vosper1,"What do you mean by ""security engineer""? 

If you mean cryptographer, or an implementer of cryptographic protocols... yeah, big math, great low-level programming skills, and very few (albeit probably high paying) jobs. If you suck at math you probably won't do well here.

If you mean penetration tester, or someone working on securing corporate systems, or doing audits... I'd guess very little to no formal math is required. But highly logical, and a good understanding and memory for processes, both technically and socially. Maybe take a look at this? https://cryptopals.com/",1540111515.0
TrammelBranch,"I sucked at math, or thought I did. I failed Algebra I the first time I took it. I studied CS anyway, and discovered that I hadn't sucked at math; I had just been taught poorly, and lacked interest/motivation because I couldn't see its applications. In college I caught up and then some, now motivated by what I could see that I could do with that math on a computer. Ultimately I got a PhD in CS in a very mathy CS discipline (graph algorithms) and have had a very successful career in CS, some of it very math-heavy.

Also, as some others have pointed out, CS doesn't use much of the math you learn in high school past algebra. You'll find much more value in discrete math (counting and permutations, set theory, combinatorics) and in statistics.",1540119098.0
disastercomet,"Do you understand algebra? If so, great! This will get you through most of the math of a security position.

Calculus is less useful for most software jobs; most of the fields you’ll be working with, in particular encryption, text manipulation, and computer architecture work with discrete (whole number) math, not smooth curves from calculus.

If I would recommend two classes to take, try to go for a discrete math class and an algorithms class. Discrete math will give you the basics, and algorithms will help you design programs, and think like a computer.

If you really want to go the extra mile, linear algebra is the math of matrixes (oversimplified, but not wrong). Much of modern data analytics depends heavily on it (in particular, neural networks that are all the rage are fundamentally matrix transformations with a twist.)

Khan Academy, as /u/au_tom_atic mentioned, has been pushing hard to build a CS curriculum. It’s pretty good, and worth your time: https://khanacademy.com

If you want to see what university-level discrete math is like, UC Berkeley’s CS70 class materials are all online: http://www.eecs70.org

Don’t worry about sucking at math. We all did, too. Remember to spend some time practicing, spend some time taking a break, and being patient with math. With time and experience you’ll become better.",1540111783.0
gabriel-et-al,"There is a lot of math in Computer Science. This is a good thing and you must get used to it.

---

edit:

I didn't see your full question because the formatting was broke.

> Do you guys know where I should start ?

Any pre-calculus and pre-algebra content will be useful: Trigonometric functions, logarithm functions, exponential functions etc. You have to be OK with these things in order to learn things like: limits, derivatives, integrals, curve fitting, linear regression etc. Learning a bit of statistics is also good.",1540110471.0
youstolemyname,"If you can add, subtract, multiply, divide, modulo, bitshift, AND, OR, and XOR you're pretty much good to go as far as implementing goes.

If you want to work in the field of cryptography, developing cryptographic algorithms or taking shots at current shards, you'll need a more firm grasp.",1540127572.0
DTreatz,"Professor Leonard, PatrickJMT are your friends.

&#x200B;

Khan Academy, or any MOOC (Massive open online course) for that matter.

&#x200B;

Get some math books: pre-algebra, algebra (beginning, intermediate, advanced) Pre-Calculus, Trigonometry, Calculus, Linear Algebra, Discrete Mathematics.

&#x200B;

I'd say practice multiplication tables, fractions, and radicals frequently on their own.

&#x200B;

Last but not least, do the work. There's no getting around it in math.",1540111000.0
WArslett,"Now this might be a controversial comment, and maybe my degree was a little different to other redditors on this sub (maybe it's an American university vs UK university thing) but I've always felt the focus on maths in computer science at bachelors level is a little overstated.

In the UK your entry into university is normally based on A-levels (courses you do between 16 and 18 equivalent to freshman year in US). I had an A-level in Maths but I went to a University that didn't require it which was a good thing because I failed my final two A-level maths exams and got a D overall. We had the option in the first year to do an introductory maths for computer science module or an advanced maths module. I picked the advanced module and it was *significantly* easier than the maths I did at A-level and it was one of the highest grades of my whole course.

Then for the rest of the course we had modules that involved maths but honestly most modules didn't directly. They involved a lot of problem solving which overlaps some of the same skills but the sort of heavy calculous, algebra, trigonometry and statistics I had to do at A-level very rarely came into play in my course.  There were people on my course who had dropped maths at 16 and were still able to complete their degree with a decent grade.",1540113306.0
Alaskan_Thunder,"More of a note to you for the future than an answer to what you are asking, but I've always felt that discrete mathematics are a somewhat separate learning experience than algebraic ones, and that someone always had trouble with algebra might do better than they would think with discrete(You still need algebra though). So even if you struggle through algebra, don't quit. You might pick up some of the other topics a lot faster. ",1540114761.0
shaestel,"I also suck at math, and I am in a major at computer science. It is hard and I have to study 3 times a normal student of computer science has in order to not fail math, but I love programming and it is worth it. If you want to be a programmer you can be, you just have to put the effort.",1540133009.0
au_tom_atic,Khan academy ,1540110257.0
stochastaclysm,"You’ll be surprised how little maths you can get away with. Get a book on discrete mathematics for computing, and you should be fine. ",1540110467.0
mithra97,You do to have to be good at math. You just have to be able to count to 1.,1540128331.0
BrightLord_Wyn,"Here's the thing. Do you like math? Because if you hate math then find another field. If you genuinely struggle with math, then the course work will be challenging. However, you can still do it.",1540128916.0
sabboo,"Welcome to the wonderful world of free libraries! How can I help?

Maths be did fur yew.",1540132032.0
jkuhl_prog,"Khan Academy.

  
Salmon Khan is great at explaining math concepts and he goes through everything from basic addition (which you can skip lol) to calculus, so anything you need is available.",1540133505.0
ThePerfectAlias,"Khan Academy and PatrickJMT taught me more than the actual college courses. They are comprehensive. WolframAlpha step by step solutions for when you get stuck. 

Go way back to the beginning, spend a few hours each nights not just watching the videos, but practicing practice problems. Once you master algebra, the rest falls into place. ",1540134762.0
sheikhy_jake,"How bad do you suck at math? Lol. When was the last time you took a math class and at what level was it?

Eirher way, Icertainly wouldn't despair. You can learn what you need to know. It might just take some grafting. ",1540135420.0
rizwakhan001,"Having a thorough knowledge in mathematics is a huge requirement for anyone who wishes to dive into the world of computer science and software application programming. Since you suck at mathematics, I do not really think that you have any choice but to pull your socks up and find a way of loving mathematics for you to be a good software developer. 

The relief to your situation however is the fact that there do exists some fields in computer science that do not require you to be very good at mathematics. Such fields do include Database Administration, Hardware maintenance and networking among others. So theoretically speaking, you can achieve your goals and dreams of doing computer science without needless having any good skills in mathematics. 

Furthermore, our good programming school called [Holberton School](https://www.holbertonschool.com/education#learn) does not have any need for new students to have a good background in mathematics or software application methodologies in any way. We admit students from all types of backgrounds meaning that you too can enroll and become proficient in software development in just 2 years. So do not really worry about your mathematical skills, unless if you simply want to end up being a professional software developer. All the best. ",1540853048.0
bertbuffet,"Think of mathematics as a field which is build on solid foundations and there is no need to bullshit. 

Start by picking up a year 12 book and build your foundations by practicing some basic questions and switch into practicing past tests. 

It will build your knowledge for 1101 classes while compounding your knowledge for math related subjects. 


I found this website to be formative of my math knowledge, a pure milestone in one's math education.

http://tutorial.math.lamar.edu
",1540111596.0
hextree,"If you aren't good at maths, CS isn't the right degree for you. CS is very maths-dependent, despite people here claiming otherwise.",1540117632.0
zroomkar,"You may just have not had the right prof or method yet. 

Math is very important in CS, and you should love to problem solve. There are three main fields of math at my school's program: Calc, Linear, and Discrete. All are different from one another, and some people excel at one more than the other. My school demands that you do the basics of all three math subjects, and study advanced math in the field of your choice. Calc is boring to me, but beautiful. Linear is easy to me and boring, which makes it the most challenging as its hard to stay motivated. Discrete is exciting to me, and beautiful - making it easier as I'm more motivated to learn it.

A CS degree is math heavy so you may find similar degrees more appealing, like degrees which are more application based rather than theoretical. If it's a employer footing the bill, they maybe more interested in application based programs as well. The related program at my school is call Bach. of Applied Computer Science. It's a CS degree without the math, more management, and more group work.

Edit: Where can you start?

For Calc, study pre-cal online. There are tons of resources available for this topic as everyone loves to teach calc.

For Linear, here's a intro book( [https://www.mathstat.dal.ca/\~selinger/linear-algebra/downloads/LinearAlgebra.pdf](https://www.mathstat.dal.ca/~selinger/linear-algebra/downloads/LinearAlgebra.pdf) ): Try and do the first 3 chapters.

For Discrete, here's a intro book( [http://www.people.vcu.edu/\~rhammack/BookOfProof/](http://www.people.vcu.edu/~rhammack/BookOfProof/) )

If you want some sample exams in a few months inbox me and I'll be happy to send some over.",1540111716.0
Blue_Q,"This auto translates to ""I suck at math but I want a foundations of mathematics degree"". Computer Science evolved from questions fundamental to mathematics, around the time (late 19th and early 20th century) mathematics was in a foundational crisis due to Russell's Paradoxon. All the work beginning with George Boole and Frege on strictly formalized mathematical reasoning and culminating in the work of Gödel, von Neumann and Turing were laying the foundations for modern Computer Science. You will come across formal logic, maybe Hilbert style systems, formal language theory, automata theory, data structures and algorithms (DSA) and complexity theory. If you don't love mathematics - if you don't have a natural affinity to doing it, then CS isn't for you as it's nothing but math. It was basically my whole studies: Given a theorem, find its proof and then sometimes implement it programmatically. If you want to start with CS begin with first order logic, then set theory, DSA,...",1540124498.0
tomdon88,"You go be that person, no need to walk in anyone’s shadow.",1540137080.0
PM_ME_IRL,lol jerking at questions,1540081022.0
SteeleDynamics,"The ""frontend"" of the compiler is considered the Lexer, the Parser, and the Type Checker. After these three ""filters"" are applied, the output is a valid program  Abstract Syntax Tree (AST).

The ""backend"" is what converts an Intermediate Representation (IR) to any form of code that is machine specific (assembly code). This is usually referred to as code generation.

Forms of IR can be the AST, three-address code/SSA. The machine independent optimizations can before the code generation phase.

The assembler, linker, and loader are separate programs outside of the compiler. Those diagrams indicate as much.",1540131013.0
timlee126,appreciate any clarification!,1540126218.0
maheshhegde,"If you mean if a compiler needs/invokes an assembler, yes and no. GCC invokes `as` which is part of binutils. While many like `tcc` do not.",1540567608.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1540062858.0
Findlaech,"I started at 16, when a friend of mine that I met on IRC gave me a VM on his hypervisor.
The year I turned 18, he told me he was using Erlang at work so I was intrigued. Next thing he told me about Elixir, and I fell in love with that.  
Around 20 I started haskell, on my own. 

My first job was this year, after I dropped out of business school (don't ask why) with Elixir, and last month I got a new, well-paying job doing Haskell and maintaining a node legacy app since last month.

I'm 23 now, and basically: 

*  lots of Stack Overflow searches,
*  side projects that were never finished
*  going to a bunch of meetups, to socialize and that's what got me my current job.",1540063725.0
245rocks,"I started in 1985 being 13 y/o. I got a Sinclair TS1000 with 2KB of ram and since my tape recorder never worked in a trustworthy way I got the use to re-code the programs from zero, I've made using my human memory. Then played with a Commodore 64 and with it started programming in assembler language, I felt so powerful, LOL. Played coding with music, images, quirkier games... At 16 i got my Programmer Analyst title and make some systems for stock, accounting, etc in CoBOL and Pascal languages in PC. At 19, in the University I learnt C++ and I felt in love... Years passed and I moved into Web Design and Programming... It's being 21 yrs so far on it and counting... I had some projects made in Flash and Director environments. Later Jscript, jQuery library... I hope what I wore here was inside your subject. 🙂",1540062725.0
yggdrasilly,I imagine facebook cooperates with these agencies? The chat messages are only encrypted until they get to facebooks servers.,1540060905.0
philipwhiuk,Facebook chat is not normally end to end encrypted so yes it is taken (willingly or otherwise) from Facebook’s servers / internal network.,1540074738.0
NerdAtTheTerminal,"Unless e2e encrypted, your communication can be read by provider and thus NSA and like.

Even if E2E encrypted, you can't tell communication is secure if encryption process isn't transparent. For example, whatsapp ain't open source, so you can't tell it won't secretly transmit your private key to their servers.

Third and most complicated layer of surveillance relates to microprocessor and device firmware level eg: Intel Management Engine. While there are lots of conspiracy theories, suspicion on silicon companies is far from being false.",1540569386.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1540058725.0
QRUXInc,Play Star Citizen In Pre-Alpha Stage Now: STAR-YJPJ-BGDD,1540058797.0
Barrucadu,What do you think?,1540046241.0
Kibouo,"Why would it? 

I'd say it's exactly the opposite. 

> ""How can this small box contain so much black magic?""

> ""I want to make my phone do stuff.""",1540047037.0
jmite,No,1540047773.0
aCrazyTheorist,Does anybody have access to this? How do you prevent tunneling?,1540040200.0
naval_person,"What's the finesse that lets them claim a device having a separate drain, gate, and source ... consists of just one atom?  I would have imagined you'd need, at minimum, three atoms: one for each terminal of the device.",1540055384.0
MCPtz,"https://www.nanowerk.com/nanotechnology-news2/newsid=50895.php

Found [here](https://www.reddit.com/r/science/comments/98bj81/worlds_smallest_transistor_switches_current_with/) from 2 months ago",1540067822.0
LithiumEnergy,"[Aussies did this over 6 years ago](https://www.abc.net.au/news/2012-02-20/team-designs-world27s-smallest-transistor/3839524)

Edit: Not sure why I'm downvoted when I was just saying who actually invented the first one. OP's article is clearly specific to a particular branch of this topic.

> Thus, a first demonstration of the single‐atom transistor operating in *the quasi‐solid‐state* is given.

You'll likely learn more from reading [the original article](https://www.nature.com/articles/nnano.2012.21)",1540093802.0
jessi-t-s,"If you’re already published, that’s a really good start. The GPA might be a problem, but I know some schools are a bit lenient with their GPA requirements if you have a good relationship with your potential supervisor and they are willing to work with you anyway. But it’s not a guarantee. I would say your best bet is to figure out who you would want to supervise you and reach out to them, get that rapport going and hope that they want you!

Edit: Also, yes. You most likely need a PhD to have a career in CS research. There may be some places that would hire you with a MSc but I’m not sure.",1539984943.0
nablachez,"Make sure you know your math very well. Of the papers ive read, a lot of them seem more (applied) math than just simply programming.",1539998548.0
CorrSurfer,"Would you consider doing a PhD outside of the US? GPAs are really a US thing, so abroad, that shouldn't be a big deal, especially since you come from a good school.",1540044444.0
irabonus,"I’m a first year PhD student at an Ivy Leaque university and I had a 2.8 GPA and no publications.

I never cared much about grades, numbers matter much less than you’d think!",1539991342.0
kc3w,And why post this here?,1539975651.0
ImaginationGeek,"I would suggest *not* looking for “hot” or “up and coming” specialties.  You don’t want to fall into the trap of specializing in a fad.  What would be better is to specialize in something that’s going to be a mainstay in computing going forward.

That said, a couple things you mentioned are going to be such “mainstays”...

Security
Using data to solve problems (data science, machine learning, etc.)
Large scale systems (including cloud)
Software engineering (not new at all, but definitely a mainstay)

but if something is interesting to you, then by all means learn it!",1539976360.0
a3cite,"Virtual/augmented reality, and related technologies. Also, machine learning and artificial intelligence.",1540000959.0
shmageggy,"ITT:

> The one thing I know about",1540003533.0
MinuteTradition,Functional programming,1539997757.0
crossCounter23,Machine learning ,1539989499.0
jnwatson,"Keep in mind that what is hot now ain't necessarily going to be hot tomorrow. When I was in school, VR was super hot. It languished for 15 years, before that, in the late 80's, AI was super hot. It languished for 20 years.

The counterargument is that both technologies needed hardware to catch up. On the other hand, I bet you haven't heard of reversible computing or the transputer. ",1540001588.0
mach_i_nist,"SAT and SMT Solvers, Concolic Testing",1540008802.0
DSrcl,Computer Architecture,1539997921.0
jmite,Homotopy Type Theory. In 20 years it's going to have revolutionized math!,1539999149.0
jaykedge,"Es6 JavaScript, AWS,kubernetes,docker,python, Machine learning",1539985075.0
feedayeen,"There are other factors too. Companies want someone who can do X. X is in demand!

Is X actually useful or is this a fad? Maybe AI models are useful to some companies, many more hired a guy to just show off investors that they know the future and ignore the enterprise application in the background made in Excel (I'm not kidding).

Why is X in demand? Well demand implies that the ratio of jobs / capable workers is high. Is a fresh grad a capable worker here, why aren't the guys with 5 or 10 years of experience looking at it? Is the number of jobs actually good? ",1540019376.0
Santamierdadelamierd,NLP,1540020066.0
nud3l_,"As someone who is doing a PhD specialising in blockchain/smart contracts: focus on the basics. The useful things you will learn and apply are the building blocks of CS. Most other technologies or hyped are more or less applications build on top of that. What do I mean by basics:

- languages and compilers
- algorithms and data structures
- cryptography
- ....

I know blockchain is quite hyped at the moment, but the relevant top papers appear in security, systems, and distributed systems conferences. You will have it much easier to be part of the next hype (if you wish to do so), if you have a strong understanding of their underlying concepts.

That being said, I think blockchain and smart contracts are very interesting because the time from research to deployment is very short. That makes them prone to errors and failures but I think it's a great learning experience.",1540025640.0
BubblingMonkey,"Open source software is one thing I was told to make sure I get familiarized with. I've got a little list of recommendations from guys in the field, but this is more of a focus on software if that's what you're leaning towards.",1540070343.0
theepiphones,Data science is sexi,1539984219.0
Feynmanfan85,"In the event researchgate doesn't work for some users, here's a link to the pdf on my personal blog:

https://derivativedribble.files.wordpress.com/2018/10/on-the-value-of-gamma-6.pdf",1540056972.0
IIIBlackhartIII,"I'm not sure this is the best place to ask this, but to answer your question- no, you would expect no change. The tracker server in a torrent simply acts to manage all of the peers and where the data is being held at any time. Think of it like the chaperone on a school field trip. Just because you borrow a chaperone from another group, does that mean you have more kids to work with? No. The primary limiting factor for a torrent download is the bandwidth of the peers- how limited your download speeds are, and how limited the upload speeds of the seeders are. More seeds with better connections would be the best solution, but obviously you cannot always guarantee that. Distributed downloads have that uncertainty, which is why dedicated server hosting is usually the better option. ",1539945118.0
Fluffy_ribbit,"There are lots of classes that attempt to walk you through this, such as NAND2Tetris, but essentially,

Your code is parsed into tokens

The tokens are interpreted into instructions

The instructions are probably somewhat cleaned up by a lower level language and then by your operating system, but eventually, you get assembly instructions

On one level, you can think of assembly instructions as functions that take arguments. They have the neat quality of being numbers on the lowest level of the operating system. So an instruction like

AND 1 9

might turn into the hex numbers

A1 01 09

and then into binary numbers.

These instructions are hardwired into your computer as logic gates made of physical circuits. 

And these circuits are in turn made of simpler circuits, and you can make every circuit out of NAND gates.",1539946645.0
richardathome,"Ultimately, the 'code' is translated down to instructions to move electricity around a logical circuit. It's atoms don't 'know' what the code means in the same way a book doesn't 'know' what the words inside it mean.  


The code we write is a high level (easy for us, hard for computers to understand) abstraction of setting up those circuits. It's translated into a lower level language (hard for us, easy for a computer) that the computer uses to set up those circuits.  


When we 'run' a program we push electricity down the circuits and return what happens. That result is translated from the low level, back to the high level so we can understand it.",1539950554.0
Microscopian,"This is an extremely complex topic which cannot be explained in a few paragraphs. A computer is built from many smaller, simpler subsystems, each of which is built from even more smaller, simpler components. Eventually it is built from components like bits and electrons, which you don't have to build.

The best explaination I know is in the book Code by Charles Petzold: https://en.m.wikipedia.org/wiki/Code:_The_Hidden_Language_of_Computer_Hardware_and_Software

Avoid ""explainations"" that describe subsystems as complex as the original systems; such as ""a computer is like a city ...""",1539945912.0
heyzo,"Not a direct answer, but this talk by Richard Feynman might interest you: [https://youtu.be/EKWGGDXe5MA](https://youtu.be/EKWGGDXe5MA)",1539952925.0
combinatorylogic,"If you want to understand this in depth, I'd recommend to follow either NAND2Tetris course, or read through the Project Oberon materials. Ideally, do both.

Also, read C. Petzold, ""Code"".
 
",1539951030.0
add7,"If you want to get your hands dirty, you can implement a programming language interpreter yourself, by following this guide: http://craftinginterpreters.com (written by u/munificent)

It's a fun book to read, and it introduces some hard concepts in an approachable way.",1539947280.0
SevroTheGoblin,"I'm no expert, Ive only been coding for a year in high school, but I like to think about it in levels of abstractions. First there is the idea of binary data, electricity and light being sent through wires and transistors to represent the movement of 1's and 0's (1's represent 'on' and 0s represent off). Once there is data, there is the concept of analog logic and storage. While there are multiple ways to store data by using different hardware, all data goes through logic gates at a mechanical but extremely small level that are organized in such a way to basically be really complex adding machines. Now each light signal or lack there of is (Known as a 'bit' of data) being processed in a machine, and is paired with other signals, often in data structures like bytes: 8 bits (11011110).
    Each signal can be now assigned meaning by machine language. Bits actually represent a counting system, imagine our counting system. The 10s, the 100s, and the 1000s place can be described mathematically by the digit multiplied by 10 to the square root of the numbers location/place. 128 is actually 1*10^2 + 2*10^1 + 8*10^0, 100 + 20 + 8. The same can be said for binary, but instead of 10 digits as our storage for counting (humans have 10 fingers and 10 is a nice number), we use 2 digits, 1 and 0. So, counting in what can be called base 2, there is a 2s place, 4s place, 8s place and so on. 128 is represented as 10000000 because the 8th place is actually 1*2^7 = 128. A byte can store 256 numbers using this system of counting, 0 - 255, because 2 digits, to the power of the number of bits (binary digits) -1 is 255.
    Now we know how numbers are conceptualized in a computer. A central processor can then begin interpreting information using these rules to do math. Its useful to realize all math is just different forms of addition. The bytes are now being interpreted by machine language, which is basically at its core using logic gates based around asking ""If this then do this"". This machine language of digits can then be refrenced to other charts like Unicode which mandates numerical definitions of letters and values. 65, 1000001, represents 'A' and the computer can use multiple programs to display information in human tounges.
    We dont speak binary though, and logic gates are not exactly friendly, so to make programs that tell the processor what to do we assign words to mean different processes. In Java, words typed into a program actually are read by the program by taking it as a logical action. If (true) then do { whatever is in this area} . 
    I have got really tired typing this and I can't really find a way to finish. If someone could help.",1539948373.0
sayubuntu,"If you really want an in depth understanding of this rather then a high level ELI5 you may want to start with high level concepts, mainly an abstract syntax tree, move into things like parsers, and wind up taking on arguably the most challenging undergrad CS concept: compilers. 

Writing a compiler was the most illuminating experience I’ve had on this question. Alternatively it can be relatively straight forward to write a “domain specific programming language” in python. ",1539964115.0
Pella86,"https://youtu.be/9PPrrSyubG0

The playlist linked to the video (~40h) describe how to build a very simple computer from electronic components.

It has also a video about the language and how it works. Is very well done and very clear.

This might help you to understand",1539971615.0
net_nomad,https://www-users.cs.york.ac.uk/~mjf/simple_cpu/index.html will keep you busy for a while.,1539946381.0
mazesc_,"As others have already pointed out this is a highly complex topic, but I'll anyhow try to give you a simplified view from the CS side only. I'll also provide Wikipedia links so you can continue research on your own using these pointers:

1) Assuming you mean a high-level, human-readable, programming language: Such a language is [compiled](https://en.wikipedia.org/wiki/Compiler)^1 , and in the process translated to so-called [assembly code](https://en.wikipedia.org/wiki/Assembly_language) and stored in the computer's memory.^2

2) Assembly code is then executed by the computer's CPU in a [fetch-decode-execute cycle](https://en.wikipedia.org/wiki/Instruction_cycle). This is the jump from software into hardware that you are probably interested in. The [CPU](https://en.wikipedia.org/wiki/Central_processing_unit) consists of a multitude of circuits which take care of the various stages of the instruction cycle.^3 These circuits, responsible for the CPU executing the instructions corresponding to the computer program, are implemented in hardware using [logic gates](https://en.wikipedia.org/wiki/Logic_gate), which in turn are implemented by transistors. Unfortunately this is where my understanding ends, as my knowledge of electrical engineering is non-existent.

Footnote 1: or [interpreted](https://en.wikipedia.org/wiki/Interpreter_\(computing\)), but I'll ignore that here for simplicity.

Footnote 2:  You might ask how the first compiler was written, this is the issue of [bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(compilers\)).

Footnote 3: Modern CPUs are incredibly complex (and I'm no expert). They use features such as pipelining to get more work done, for example.",1539949889.0
philthechill,Remember when you could read a book like From Chips To Systems and basically everything was explained?,1540007526.0
commutativemonoid,"There are a couple different types of languages. Interpreted languages are read line by line by another program (the interpreter) and the interpreter knows what each command should do. This means that that as long as you have the interpreter installed you can run the code straight from the source. 

The other type is compiled languages. These are languages where a program (a compiler) translates your code into assembly (very low level code that's specific to a type processor) and then into byte code (1s and 0s that the processor itself can understand). Most software that you download from the internet is the bytecode that's already been compiled. 

This bytecode is usually a very limited instruction set (ie move one value from a register to another, add two values in two registers and put it into a third register, store a value from a register to memory etc). A processor has circuits inside them to execute each of these instructions.",1539946253.0
noam_compsci,"This is minimum a 5/10 (depending on intensity) week course to understand properly IMO.

I am not an expert but here is how I understand it (note I have heavily abstracted up, to the point where large generalisation and assumptions are made. ITalics means i have made a generalisation/simplification):

There are at least * 3 different layers* to understand. Basically you have (1) Application code (say python), which is then *compiled/interpreted/assembled* into code that is more closer to binary (2), which is then* shoved into little transistors* (3) which need to be on/off for a specific binary instruction and binary data to apply the instruction to. 

For example, lets say your computer will perform an addition on the next two numbers it gets after it gets the* add  instruction which is  '1000'*

Lets say you want to do 1 (0001) add 2 (0010)
At the most primitive form you would have something like:

Layer 1: Your python code
return 1 + 2

[The transformation between L1 and L2 is predetermined based on your computer architecture and O/S, compilers etc] 

Layer 2: *Transformed into assembly or machine language*. 
1000 0001 0010

Layer 3: 
[Lets pretend the 1000 add instruction will tell the CPU to do an **OR** to the next two things it receives (we are pretending doing an OR will result in an add). Note that there are two types of digital logic gates, an AND gate will only pass electricity through it when it gets two electric pulses. An OR gate will pass electricity through it if it gets 1 or 2 pulses. 

Think of group 1, 2 and 3 as running a single circuit  with group 1, item n and group 2 item n connecting to a specific type of gate, determined by Group 0. For example, for the 4th digit, we have Group 1 = 1 and group 2 = 0. Based on group 0 being add, we do an OR gate and so group 3's 4th digit will be 1 OR 0 = 1.]

You will then have a number of groups of bits:
Group 0 will hold the add: 1000

Group 1 will hold the number 1: 0001  

Group 2 will hold the number 2: 0010

Group 3 holds the OR of the above: 0011 = 3. This is 1 OR 2",1539953509.0
Plazmatic,"First tutorials-point actually has a good overview of this here: https://www.tutorialspoint.com/basics_of_computer_science/basics_of_computer_science_fundamental_concept.htm

First parse into tokens (lexical analysis), which are the parts of expression

so `a + b` turns into 

 `ARITHMETIC_VALUE BINARY_OPERATOR ARITHMETIC_VALUE`

and along with the token, the actual string representation is usually kept. 

Tokens are typically recognized individually by [regular expressions](https://en.wikipedia.org/wiki/Regular_expression), a language in itself but can also be parsed by just looking at the values of the characters, though you will find it is much harder to generalize what a ""identifier"" token is with out some sort of parsing language.   Regular expressions cannot be used however, to recognize all [context free languages](https://www.cs.rochester.edu/~nelson/courses/csc_173/grammars/cfg.html), so they are used typically only for this step.  Regular expressions cannot recognize things that need nested ""objects"" with ""closing values"" like `(abc(abc(abc)))` where you need multiple parenthesis to sorround. They can however recognize each individual part ie `PAREN IDENTIFIER LPAREN IDENTIFIER LPAREN IDENTIFIER RPAREN RPAREN RPAREN`. If you don't know what a context free language is, it basically means a language that doesn't need multiple values on the right side of an expression (EXPR_A = X Y Z) in order to figure out the left side, ie it doesn't need ""context"".
 
this would not be a context free grammar for example 
`EXPR_A = Q`
`EXPR_B = R`
`EXPR_A EXPR_B = X Y Z`

Computers can parse context sensitive languages, but it takes a lot more time and is harder to do.   A lot of human languages are context sensitive, but programming languages usually don't need context sensitivity to function properly.   To describe context free languages, we use a different form to describe it called Backus-Naur Form, or the extended version, [Extended Backus Naur-Form](https://en.wikipedia.org/wiki/Extended_Backus%E2%80%93Naur_form), BNF and EBNF respectively. 

If you've stopped here, its because you didn't input a valid token  (like @^%)  next is expression parsing (where we use EBNF/BNF forms).  Here you try to see if your tokens form valid expressions, like our original binary operator expression value.  The simplest way to do this is recursive parsing of tokens using [top down left right parsing](https://en.wikipedia.org/wiki/Top-down_parsing).  Basically here you start with a definition of your whole expression (in this case, your language) defined by EBNF form.  EBNF form is made of terminal symbols (single tokens) and other expressions (other EBNF definitions), it might look something like:

MAIN_EXPR := MAIN_HEADER LBRACKET FUNCTION_BODY RBRACKET 
LBRACKET := {
RBRACKET := }
FUNCTION_BODY := [FUNCTION_CALL | VARIABLE_DECLARATION | ARITHMETIC_EXPRESSION]* RETURN_STATEMENT
RETURN_STATEMENT := RETURN_KEYWORD VALUE
FUNCTION_CALL := IDENTIFIER LPAREN [VARIABLE | VALUE | FUNCTION_CALL]* RPAREN

this is not a complete grammar.   * = repeat 0 or more times.  This parsing into expression phase is called syntax analysis. 

Now the issue here is that when you do top down parsing, you actually cannot parse all valid context free expressions, on some you'll have the same expression  defined in the expression itself (called left recursion) and [there are ways to transform](https://en.wikipedia.org/wiki/Left_recursion#Accommodating_left_recursion_in_top-down_parsing) grammars into a top down LR version which can be properly parsed.  Most parser however, do not use top down, but instead bottom up parsers with different look ahead rules that use tables instead.  These end up being faster and no preprocessing on the grammar is typically needed (but these parsers still may not be able to parse the whole of context free grammars).  [Flex and Bison](http://aquamentus.com/flex_bison.html) (common tools for parsing and generating languages) use a modified LALR (look a head left right) parser https://en.wikipedia.org/wiki/LALR_parser.  That is an advanced topic however, it is not trivial to implement. 

Once you parse these expressions, you may use similar systems to express proper type matching (in statically typed languages, IE languages where you know the types at compile time, like C#, C, Rust, C++ and Java) and other types of static analysis (analysis at compile time).   This is semantic analysis.  You go through the parse tree you created and make sure these static rules are upheld in the tree.  You basically go from the bottom up, attach types to each of the values described in your expression (remember how we kept the actual text associated with tokens?  where here is where you use that) and make sure valid types are created. 

ie INT_EXPR := INT_TYPE_TOKEN IDENTIFIER EQUALS ARITHMETIC_EXPRESSION

then you check

INT_EXPR.inttype := INT_TYPE_TOKEN INDENTIFIER EQUALS ARITHMETIC_EXPRESSION.inttype

and if they types don't match you fail semantic anaylsis. 

After this is all done, we create [symbol tables](https://www.tutorialspoint.com/compiler_design/compiler_design_symbol_table.htm) which map variables to locations to be used later to make sure you've defined a variable before use, and then you can directly translate your language into either your source (binary 0s and 1s that correspond to op codes) or another intermediary version (typically this is also in binary, just not your actual assembly on your computer) that allows you to perform optimizations easier, and compile multiple languages with the same compiler easier. 



",1539954209.0
lanemik,"A lot of great answers in here. One thing that I didn't see (when I skimmed) was something that helped me a lot when I learned it. Back in ""the day"" before any kind of programming languages, there were computers that weren't all that different than what we have now. Sure, they were far less powerful and had a single CPU and very little memory, but the basic idea of how the CPU did its job was the same. Fetch the next instruction, decode it, and execute it, then fetch the next instruction, decode, and execute it. And so on. Fetch, decode, execute.

There is a chip that informs the CPU where the next instruction should be found called the Program Counter, or PC. The computer will address that specific memory location to get the next instruction loaded into the Instruction Register. The instruction is then sent to the decoder which basically has a lookup table of instruction numbers to actions. So increment has some number associated with it and add has some other number and XOR has some other number and so on. The control unit is then told to execute whatever action the decoder found.

So back in the day, you, the programmer, had to know the instruction set architecture. You had to know what binary values corresponded to what operations in the architecture that you were dealing with. Maybe 0011 was increment, maybe 0101 was add and so forth. So your computer programs might have looked like this:

    0011 0001
    0101 0010
    0110 0100
    etc

As you can imagine, this was extremely prone to bugs. So at some point, someone got the idea to replace the actual lookup numbers with the desired function names. So instead of having to use 0011 for increment, you could use INC and instead of 0101 for addition, you could use ADD and so on. So now a program could look like this:

    INC A
    ADD A 1
    etc

And hence, assembly language was born. There was an extra benefit to this. The basic instructions were generally the same across architecture, but the numbers that corresponded to them changed. So by abstracting the numbers to the human-readable-ish instructions of assembly, you could easily write a program on one machine and make it work on some other machine. All you needed was an intermediate program on the target machine that would compile the assembly code down to the corresponding machine instructions. And so we had compilers. 

Of course, we humans are never satisfied and human-readable-ish asssembly is better than binary code, but not by much, and it gets complicated and unintuitive really fast. So we developed languages that were even more understandable to humans. Like C. Now a program could look like this:

    a = 0;
    a++;
    a = a + 1;
    etc

And this is where the idea of lexers and parsers and such ended up coming into play. You see, it's extremely difficult to translate natural language into computer code, but if you have a special kind of language that follows a set of rules called a Context Free Grammar, you can translate the language down to basic sets of instructions with great precision. And that process is covered by other responses in here.",1539959857.0
pfannkuchen_gesicht,I think Ben Eater's 8bit CPU build video series does an excellent job explaining this.,1539962883.0
agumonkey,"They don't, it's an encoding isomorphism. We attach meaning to bits, and we make ridicully small operations to map from group of bits (bytes, or arrays of bytes) from group of bits. Hoping that we don't create a senseless mapping (aka a bug).",1539999688.0
green_meklar,"On the machine code level, the proper responses to each instruction are built right into the chip. The circuit is designed so that it will react in the appropriate way. The machine code consists of numbers, which are represented in binary bits, which are represented by high or low voltages in the circuit components. The circuit components are made of materials with particular properties so that high or low voltages will combine together in particular ways to create other high or low voltages at the other end of the component. By hooking up many such components into a circuit, we can have the circuit perform the arithmetic we want when given certain numbers.

All other program code gets translated into machine code in one way or another. For a compiled language, like C++, the compiler takes the code written by the programmer and creates a complete translated version of the same logic in machine code. For a scripting language, like Python, the interpreter reads the code written by the programmer and uses its own machine code procedures to behave in the appropriate way. (Some interpreters compile the script code into a more efficient secondary form during execution, but that's getting pretty technical.)",1540002287.0
JNCressey,"The various logical circuits in a computer are compelled to do things when they receive a signal on their control input wire or wires.

The computer 'knows what to do' when it receives a certain coded number because a part of the CPU called the instruction decoder takes the code as input then outputs to the needed control wires in the sequence determined its memory. This code is decided by the CPU manufacturer and is called machine code.

Then from this point up, higher level programming languages are interpreted or compiled by a program written in a lower level language, cascading all the way down to turn what a programmer writes into machine code.",1540002536.0
dankmeems,"As a massive over-simplification, you're translating from your programming language to logical gates in the CPU, https://whatis.techtarget.com/definition/logic-gate-AND-OR-XOR-NOT-NAND-NOR-and-XNOR

Every function you write and use can be decomposed into these basic logical operations. The CPU has a bunch of these logic gates built out of transistors.",1540003970.0
key-blades,"I really don’t know much whereas I’m just starting out. From my own limited understanding, here’s a basic explanation:

High-level programming languages are compiled into machine code of 1s and 0s. Programs are stored onto a computers storage device, waiting to send input to the processor. Transistors on the processor control the flow of electricity, where “1s and 0s” translates into “on/off.” It gets pretty complex from this. Modern microprocessors have billions of transistors, organized into logic gates, which companies like Intel are able to fit onto a chip by means of photolithography onto silicon wafers. Of course, a microprocessor is much more complex. There are books that explain CPUs in-depth that I have yet to read myself.

Looking through this might help wrap your head around this: https://www.intel.com/content/dam/www/public/us/en/documents/corporate-information/museum-transistors-to-transformations-brochure.pdf

However, pay more attention to what others here tell you. They clearly know more than I do, but I hope I could help you nonetheless. 

Edit: If I just relayed incorrect or misleading information, then someone please correct me.",1540006823.0
jeezfrk,Fueled by coffee ... And bugs in and out.,1540007820.0
Pondering_Moose,"what I find really cool is much like how everything in our universe (for the most part) is made up of just over 100 elements, a lot of complex code boils down to combinations of just around 1000-2000 basic assembly commands, and just like those elements are made of up just protons neutrons or electrons those 1-2k commands are eventually just made up of even more combinations of 0s and 1s whether your typing into a calculator or asking siri what traffic is like downtown right now, your just manipulating groups of 0s and 1s",1540015639.0
BirdBlind,"A computer doesn't understand code by itself, which is why we have compilers and interpreters to help it out. It can only understand 0's and 1's, so we have to translate it.

&#x200B;

A language is first translated using its BNF (Backus-Naur Form). This is a set of rules that every language has. It defines everything from where a line ends (i.e. semicolons, new lines, spacing, etc) to the order of operations (usually PEMDAS like in math).

&#x200B;

For compilers, to translate the BNF, a lexer and a parser are usually used. A lexer creates tokens, based on the BNF, with token names such as ""expression"" with rules such as <expression> = <expression> + <term> . This is what allows us to type things like a = b = c = d = e = f = 0; , since the rules used allow us to assign each of these values from right to left, one assignment at a time, without caring about how many assignments we're actually doing.

&#x200B;

A parser can come in many different forms, such as LR parsers that are called bottom up parsers because they start at the lowest level of a parse tree (basically a tree of the BNF) and work their way up. Other parsers, such as the LL parser, are called top-down parsers because they start at the highest level of the tree.

&#x200B;

These lexers and parsers can be created using turing complete languages, such as C and C++, to create other languages such as Python (also turing complete). Turing complete basically means that a language can theoretically solve any computational problem.

&#x200B;

There's a lot more to learn, and I'm just giving an overview of the basics (anyone is welcome to correct me if I'm wrong), but that's one way that code is translated into human-readable form. It's easier to translate languages such as Assembly to human-readable form because they are already close to the 0's and 1's that humans can understand. A computer can fairly easily do operations such as addition and subtraction at a bit level by moving numbers into registers and such, but other operations such as division can require more work.",1539946374.0
wenoc,"Lots of answers here on assembly and so on but I think there’s a crucial bit of understanding that has been left on the table and that is how we build complex things from simple things. 

The simplest code, which is the hardest to write is assembly language (ASM). ASM calls instructions directly in the processor, like read bit  at address x or subtract one from address y and so on. Every processor family has its own ASM. 

These instructions are hardwired (literally wired) in the processor. 

An x86 processor has a vast number of instructions, many of which aren’t even documented but if you want to get some understanding of this black magic, look into the MIPS processor which has a simpler instruction set. The MIPS can easily be emulated if you want to try it out. 

Now, as the processor already can add and substract, we should be able to write a program that does multiplication by looping these instruction with a counter. 

A compiler is a program that turns code into assembly instructions for the processor. We can now write a compiler (again, in assembly) that takes code as text and translates it into ASM. 

Now we have our first compiled programming language. In it it should be simple to write programs that do logarithms and exponents using the multiplication algorithm we created. 

You expand the compiler to include more and more instructions until finally it can compile itself! Now you can extend your compiler with your own language!

We build on top of that and on top of that etcetera. Essentially pulling yourself up by your own bootstraps. By not implementing the wheel again, soon enough you’ll have a complex chain of languages translating all the way down to machine code, growing easier to use with every iteration. 

There are currently only two programming languages in use that create compiled machine code for consumer PC:s. C (and variants) and Go and there are no languages built on top of Go to my knowledge (it’s rather new and modern). That’s it. There are no others. PHP, Java, Ruby, Python etc all translate down through C into machine code somewhere along the way. 

We’re standing on the shoulders of giants. Every time you open photoshop you’re calling millions of instructions written by Dennis Ritchie or Ken Thompson. And most people don’t even know who they are. 


Edit: here’s an interesting story that really gives you insight into compilers https://www.win.tue.nl/~aeb/linux/hh/thompson/trust.html",1539956222.0
liquid_at,"Simply put, imagine the processor as a factory with different lines. 

Your processor gets data as in ""use gateway X, throw Y in there"". 

So one call could be ""Addition, 5, 5"" and the processor would know, that it should choose the path ""addition"" and throw the values ""5"" and ""5"" in there, to receive a return-value ""10"" which is the result. 

&#x200B;

Each call to the processor is an action and the values for that. Processors implement the basic functions. Like ""read stuff from memory"" ""write stuff to memory"" and ""do calculation on data"", while the programming language combines those into functions, that we can use for practical purposes. 

&#x200B;

Very simply put. ",1539946240.0
tastygoods,"> This is an extremely complex topic which cannot be explained in a few paragraphs.

Oh hell Ill try...

Your computer has five basic parts.

The processor or CPU which is like the brain where instructions and tasks work, the hard drive is like a refrigerator where things that aren’t being used are stored, memory or RAM which is like countertop or desk where programs and files you open are pulled from the fridge and temporarily stored, graphics cards or GPUs which are really specialized CPU/processors and are sometimes even on the same chip as the CPU above. Local input like keyboards, mice, and screens + network input like WiFi, ethernet cable, other stuff you plug in.

Basically, the CPU knows how to move things around from all these parts, then it knows how to change the stuff you are moving around, but it only does this using 0s and 1s.

Believe it or not there is an entirely other way to count any number you want by counting a bunch of 0s and 1s. We do this because storing a zero or one only takes a single box to fit it in, so its way more efficient. From those numbers using this other way to count, called binary, we can reproduce everything on a computer just by arranging the counting a certain way, like a recipe or lego instruction manual.

But this is really hard for humans to work this way, so instead programming languages take something that is much closer to work in like English and then convert the English like code into this other binary machine code that the CPU then grabs from the hard drive (fridge) moves to RAM (counter) takes all the buttons you are pressing and everything connected from the network (input above) and looks at and computes the whole system (millions of times per second) then creates a picture image that your graphics card then shows to you on your screen/monitor.

As a whole, computers are very much like a human body actually, the screen/graphics are the eyes, the CPU is the brain, mice/keyboards/touchscreen inputs are the hands and veins and nerves, the network is talking to other people.

",1539948803.0
fourdebt,"Electromagnetic fields are used in hard drives. Ever wondered why a magnet wipes a computer's data? In a hard drive, there is a stack of 4 metal disks called magnetic platters, which contain millions or even billions of microscopic areas which can either be magnetized or demagnetized. These platters spin on what's called a spindle to create an Electromagnetic field. To read, write, and interpret data, there is an thin arm which floats above the platters called an actuator arm. On the end of this arm, there is a small magnet called a magnetic head. As the platters spin, the actuation arm moves along the platter. From the computer, data is sent to the hard drive in the form of bits (which can be a 1 or a 0). A stream of these bits is called binary, which I'm sure you've heard of. As the hard drive receives these bits, it will first store the 1. To do this, the magnetic head magnetizes the area directly under it. When it reaches a 0, however, the area under the magnetic head is demagnetized. This process is VERY fast, and allows you to store trillions of bits. There are faster methods though, like SSDs. In SSDs, there are trillions of small parts called transistors, which are only several nanometers in length. When an electric pulse is sent to a transistor, it represents a 1, and when one isn't it represents a 0. I won't get too much into this because my battery is at 14% but it's much faster.",1539966845.0
cp5184,https://en.wikipedia.org/wiki/NOR_logic,1539946581.0
igglyplop,That's a field of science unto itself my friend. I'd direct you to the internet for that one.,1539944431.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1539935147.0
platinumbulletz,Hi by next Friday I need an interview on a comp sci expert and i would prefer to finish it this weekend but I'm also available during weekdays at night. It will be a short interview of probably 10 min and there will be questions regarding education background and about your job. AGAIN JUST TO LET YOU KNOW THIS WILL BE RECORDED.  If you guys are interested please DM me the only other way I can finish this project is to email companies but that's a lot harder for me.,1540056441.0
cp5184,"Coding != computer science...

If you want to learn coding and want to take the college route, take a few community college coding courses.

I'm no expert, but I'd imagine robotics programming would tend towards the lower end of the language abstraction/altitude paradigm, so typically you'd be using the C language, or even a more rigorous language, like ada.

If you want to do it, do it in the way that best suits your learning style.

What you might try, for instance, is rather than trying to learn java functions, instead, try arduino projects.  Learn on a friday night how to get an arduino board to blink an LED in various ways.  Have the LED stay on, have it blink, make it blink in patterns, make it blink whenever something interacts with the arduino, e.g. if you hook up a serial tty connection, for each keystroke over the serial tty connection.  Build from there, get the arduino to control a motor, get the arduino to control a servo, etc. build on that.

If you learn well in a college environment, take low level programming language and robotics courses in night school/community college whatever.",1539901737.0
Neker,maybe try /r/LearnProgramming ?,1539959822.0
merimus,"Read a book.  That's how thousands of people learned to program.  
If you loose interest when you try to teach yourself then why wouldn't you when you take a class?  
It is all about picking something you want to do, figuring out the direction you need to go, and taking one step on the path.  


Programming robots, and web development are two completely different things.  Pick a project, let me know what that project is, and I can point you towards resources.

&#x200B;

As for how CS is differentiated into a whole degree?  ME and EE are almost exactly the same except for all the ME and EE classes.  The same is true for CS.  The CS classes which differ from ME would include learning at least one programming language, studying programming languages in general, operating systems, systems programming, software engineering, data structures, algorithms, algorithm analysis, and them some more specialized subjects.  


&#x200B;

&#x200B;",1539903983.0
maladat,"My undergraduate degree was a BS in ME, as well.

Years later, I decided I wanted to do theoretical computer science.

I spoke to some faculty and administrator types I knew from prior education, and they said that I was unlikely to be admitted directly to a PhD program, because with PhD programs, unlike undergraduate and many Master's programs, the school doesn't charge tuition and actually pays you a stipend, and that money comes out of research grants, so the school has strong incentive to accept students they are confident can perform academic research.

With no academic CS background and no academic research background, I was too much of an unknown quantity.

However, they thought I should be able to get in to a non-thesis Master's degree program (sometimes called a ""professional Master's"" program), and could take a bunch of graduate CS classes and get involved in some research, and then apply to the PhD program.

So that's what I did, and I did the non-thesis Master's, and now I'm a CS PhD student.

I have occasionally run into holes in my knowledge that someone with an undergrad CS background might not have, but it hasn't been difficult to fill them.

To be fair, before making the decision to go back to school for CS, I did do a year or two of significant self-study in my off time.

My research is on the theoretical side, but I've also taken a number of programming and software engineering courses and haven't had any trouble with them.",1539993871.0
no_detection,Some companies will ask for your transcripts even a year or two out of school. My only advice is to keep putting in effort because it might matter.,1539886654.0
donowhy11,"Stick through it my dude. Even if it means getting bare minimum passing grades. Most companies give 0 fucks about your grades, but having that cs degree checkbox is an enormous foot in the door.",1539888764.0
chromaticgliss,"Those offers are typically contingent upon you completing the degree. There's nothing stopping them from backing out of the offer if they found out you dropped the ball on your last few classes and didn't get the degree.  


Just power through.",1539888904.0
whtevn,"the information is fleeting, but that stupid piece of paper is forever. get that degree",1539889814.0
khedoros,"You've got a bunch of things lined up, but they can disappear if you drop the ball. Sometimes your classes suck, but some companies are sticklers about GPA and transcripts, when they're taking a risk on hiring a new grad.

> especially one about processors, assembly, etc. 

These were some of the classes with my favorite information (although a pain in the ass at the time)! Along with OS, they really helped clarify heap vs stack, the call stack and frame pointer, etc. They're basically the courses that got me into writing video game console emulation.

Pull through; you're almost done! You've got the basics covered, and you're reaching out into core, important topics now. It'd be terrible to waste your remaining time at school!",1539894851.0
teacamelpyramid,"Think of this as a test of character. I can almost guarantee that you will have boring days at your new job, you'll have managers will not inspiring, and some of the tasks you will do will seem pointless. Sometimes there is value in learning how to get through the tedious stuff conscientiously in life, because that's what much of the working world consists of. It's often how you prove yourself to be able to work on the really cool stuff.

Also, I'm 10 years out from a CMU CS masters degree and I am regularly surprised how many things that I previously thought were useless pop up later in my career. ",1539894950.0
Caracharias,"In my personal opinion, classes may or may not matter, but you really need a degree to meet some HR checklists occasionally.

I've talked to older coworkers who went the drop out and work in software route, all of them strongly do not recommend because it limits their opportunities, especially as they look to move into management roles. 

",1539900684.0
lanemik,"My last term was the fucking worst, man. Really. The hardest classes that each took up way more time than anything else I had done up to that point. It was miserable.

But anyhow, I got it done and I'm a full time software engineer. Now I can look back on it and it was all worth it. Just keep swimming, the next part of your life is right around the corner.",1539960341.0
_Bia,"The last classes you take can be the most formative toward your ability to analyze algorithms, be aware of the growth of the field at large, get familiar with learning new languages, understand the connection to higher maths, and many things you haven't gotten fully introduced to yet.  Even if you're getting the job offer you want to ultimately take and even if you never need the degree itself, the education is something you'll regret not completing.",1539889428.0
BuxOrbiter,"Congrats. Relax. Sign up for easy classes, join some clubs, go to more parties if that’s your thing. Just get the grades you need to finish your degree. Last year of college doesn’t have to be hard. I know a guy who took vodka into his AI exam.",1539889413.0
possiblyquestionable,"It's hard to produce new functions if each must be named. At its core, lambda calculus is a computational system for manipulating functions. The primitives in this system is literally the ability to construct a function, and the ability to apply it.",1539877620.0
Vityou,"If you say, `f(x) = 2x`, and someone asks you what the value of `f` is, you can't respond with `2x`, because, that is `f` evaluated at `x`, not simply `f`.  Anonymous function notation let's you notate the value of a function precisely, in this case, `f = (lambda (x) . 2x)` or `f = (lambda (u) . 2u)`.",1539897868.0
combinatorylogic,"Because in lambda calculus you have only one named thing - a lambda argument. If you introduce more different types of named things you'll make it way more complicated to define.

You can make one step further and remove the named lambda arguments too.",1539877399.0
julianCP,"I think you might be confusing a few concepts. For the sake of this comment, lets call the something that is supposed to describe a specific function a ""meta name"". So for example the function ""addition"" is then a function that adds two numbers, commonly abbreviated ""+"". The function ""duplicate"" is the meta name of a function that takes an argument and returns a pair where both entries are that argument.      
Now in the lambda calculus you will not see things like ""+"" or ""add"" or ""composition"" or ""duplication"" instead you see ""lambda"" which I will abbreviate with ""\"" because it looks like a lambda (almost). So instead of calling a function ""duplicate"" you write: ""\x. (x, x)"". (I cheated a little since pairs are not first order citizens in Lambda calculus but let's just not over complicate things.) ",1539877675.0
vznvzn,"its a logical abstraction that led to an entire programming language/ style **Lisp (also Scheme)** aka **functional programming** where functions are constructed with `lambda`. the lambda calculus is **Turing complete.** it was discovered about exactly the same time as **Turings undecidability proof** (1936). just given mostly the lambda mechanism, its not at all obvious one can construct an entire Turing complete system out of it, so the proof is a triumph of mathematics, and has practical applications as well. some of this is nicely captured in the famous/ classic book **Structure/ Interpretation of computer programs by Abelson/ Sussman.** _(free online!)_ https://web.mit.edu/alexmv/6.037/sicp.pdf **undecidability** (the flip side of Turing completeness) remains an active area of investigation https://vzn1.wordpress.com/2016/01/22/undecidability-the-ultimate-challenge/",1539887878.0
versim,"I don't think there's anything *special* about anonymous functions. 
 However, functions in the lambda calculus differ from other sorts of functions commonly encountered in mathematics because they can contain unbound variables.  For example, in the expression

> λ*a*.λ*b*.*ab*

the inner function λ*b*.*ab* contains the unbound variable *a* (note that *a* is not a constant).  Replacing this variable with a different variable would yield a different function.  This makes it particularly useful to define the function within the context in which it is used.",1539879509.0
svick,"When you're building a formal system like lambda calculus, you want to make it as simple as possible. That way any proof you make has to concern itself only with the few constructs you have.

So a the structure of a proof for lambda calculus would be:

> 1. Verify that the theorem holds for a variable.
> 2. Verify that the theorem holds for abstraction.
> 3. Verify that the theorem holds for application.

That's it. If you added a construct for naming functions, it would make the proof more complicated, because you would have to prove your theorem for that new construct too.

On the other hand, the capability of naming functions is very useful in practice,  which is why it's very common to extend lambda calculus with [the let expression](https://en.wikipedia.org/wiki/Let_expression#Let_definition_defined_from_lambda_calculus). With that, the following three examples of using the identity function are equivalent:

* let id x = x    in id y
* let id = λx.x    in id y
* (λid.id y) (λx.x)

But since this kind of let expression is just a syntax sugar, it means you can use it in your code, but can ignore it in your proofs, because the first step of each proof can be ""convert all let expressions to the equivalent form without let"".",1539943170.0
agumonkey,because naming is hard .),1539883864.0
saitouena,"Have you ever written a programming language interpreter?
If your programming language have function definition or variable bindings, your implementation need some kind of environment(mapping from name to value or body of function).
If you analyze programming language model, your language model must treat environment. It complicates model. ",1539918424.0
mdibound,"It would be much easier and much less error prone obtaining the original data, than it would be to extract the text or data points from the PDF. Perhaps the data exists in a spreadsheet, csv, Json, \n document. You want to focus your time.and analysis on thr data, and not on implementing a custom solution for extracting it. Send some emails (or call If that's your thing) anyone from the university whom you suspect could lead you to the data before it was embossed.

Once you have the plaintext data which is layed out in standard form: Json, xml, csv, line break, or what have you, you'll write a script that parses the data and performs database insertions. ",1539841956.0
Wulfnodh,"You can write a small .Net application to do it: 

For the pdf part:
https://code.msdn.microsoft.com/windowsapps/Convert-PDF-file-to-Text-2115897c

You don’t say where the docs are to be downloaded from. If it’s a file share just point your application to the location, if it is FTP WinSCP is pretty good, if it is from a web site, use an automation tool like Selenium.",1539841424.0
ricksauce22,"What form is the data in within the pdf? Is it tabular, text based, graph etc. If it’s text based there are utilities that can convert the files to plaintext at the command line. Otherwise you will end up parsing the pdf encodings. Libraries exist to do this for most popular languages, however this should be avoided any way possible, as it’s time consuming and error prone. ",1539845245.0
bdd4,Use VBA to read line and use the '.' as the delimiter ,1539840996.0
hexaga,"What is the timeframe?

some ideas:

- package manager
- revision control system
- virtual filesystem over cloud providers. deduplication, replication/write balancing, etc etc for stretch goals
- load balancer / proxy for some protocol (kinda simple but meh)
- distributed database of some sort. complex db functionality is non-essential but the basics of concurrency control, maintenance of consistency, etc etc are important. make it multi-writer for more fun
- swarm p2p based content delivery (basically do something with bittorrent). design your own protocol for extra credit
- streaming logs ingestion / analytics / querying framework || server",1539815199.0
deanmsands3,"Auto-scrolling NCurses-based stock-market/crypto-coin price app

Simple chat server/client secured with SSL

A program that passes a complex problem around to host processes written in different languages, each one solving a different part

Android app that passes battery life, orientation, and other statistics over wifi to console app


",1539977957.0
orbat,"The bit about Enigma is so wrong that it's actively harmful. It was the [Polish Cipher Bureau](https://en.wikipedia.org/wiki/Cryptanalysis_of_the_Enigma?wprov=sfti1) that came up with the mathematical breakthrough, but Turing did help build a machine that utilized it. And no, it wasn't ""all messages end in Heil Hitler."" Conceptually yes, in that they knew that certain phrases or words were likely to be repeated at certain positions. There may be more errors, but I just stopped reading at this point",1539808374.0
ZeppelinsAway4,Just read the first part. Very informative. Can't wait to read the rest! ,1539798380.0
spacelibby,"Right, so go learn forth, scheme, haskell, prolog, Coq, and apl and smalltalk and see how they fit into this ""everything is basically C idea"".

When you're used to c, c++, Java, Javascript, python and so on everything does kind of look the same because those are all very similar languages. Once you learn about different ways of thinking about computation (paradigms) programming languages start to get more interesting. ",1539740210.0
thing-a-ma-bob,"It could all have been physics decades ago, right ? I mean isn't everything just syntax sugar over the schrodinger wave equation ?

It's API's the whole way down, if you're working with pointers, malloc, etc, and never with electrons & quarks, then you ARE working in C, not physics with syntax sugar. If you working with type classes, polymorphic functions, etc, then you ARE working with Haskell, not C with syntax sugar.

Sure there's value in unification via some lower-level API, but if done poorly (especially likely by people who think Chemistry, Biology, Haskell, Prolog, etc, aren't really real, or are easy weekend syntax-sugar projects) then what you are actually doing is destroying the ability to think in terms of anything other than the lower level.",1539768327.0
firmretention,https://i.imgur.com/cNFqi.jpg,1539750624.0
east_lisp_junk,"> It seems to me it's all the same, the basic functionality of a programming language.

You might have a tough time writing some fairly run-of-the-mill J, OCaml, Prolog, Scheme, JavaScript, and Haskell programs in C.

> Variable variables

What are these?

> can achieve … encapsulation

How do you propose to do that?

> Then I got to thinking about syntax, the mostly C code could be converted to C with its special syntax that uses a C library allowing OOP and other features but it's all procedural C in a loop at the end of the day.

Quite a lot of language features (implicit data-driven iteration, higher-order modules, unification-based queries, first-class continuations, prototype inheritance, lazy evaluation) are not straightforward syntactic extensions to C.",1539738364.0
zekka_yk,i'm confused,1539732278.0
heap42,This is a really uneducated and ignorant post. ,1539782184.0
MayorOfBubbleTown,"If you go deep enough, every programming language or virtual machine has some ability to call functions in C libraries. If you are interested in this kind of stuff look into the history of C++. It started out as macros to turn C with classes into plain old C.",1539733652.0
thecrazypriest95,"OOda loop.
Up
Down
Forward
Back

Pick one, have we made a wrong turn?

3(4)? chemicals 6 combos, but why?",1539762851.0
thecrazypriest95,#Hcb,1539732184.0
thecrazypriest95,"Congrats, Evolutionary biology.


Find me.

#hcb",1539734409.0
jet_heller,I know that Waterloo consistently ranks fairly well.,1539716089.0
Landonian22,"University of Alberta! Deep Mind recently opened a lab in Edmonton in conjunction with some profs at UofA. Richard Sutton and Micheal Bowling. 

I'm not super familiar with all the specifics but I would recommend you look into it.",1539719441.0
Avrelin4,Waterloo and UToronto I think,1539752876.0
VirtualComposer,"Thank you all! So what I gather is Toronto, McGill, UBC and Waterloo are the best. But given they are very ambitious, are there other universities that are a bit easier to get into but also have equally good CS departments? ",1539851997.0
jmite,"UBC has several researchers in various areas of AI, it's constantly fighting with McGill for 2nd in Canada's University rankings",1539792841.0
thecrazypriest95,"I too request this awnser for another reason, pick it.

You win.

Go.

#HCB",1539714165.0
JoaoVic111,"There is no ""the best language"", there is what you want to do and where you want to go. Do you want learn logic and see if this is an area for you? Try javascript, if you don't have problems with OO then surely programming it's a big deal for you.

 Do you like games? Then go for something like c#, c++, java or the language of the game engine.

Market will always need new programmers, no matter where you live so the best option is to search what would you like to do, that way your job will be less stressful and you'll be happy doing what you like afterwards :)",1539716277.0
_waltzy,"If you're just looking to be more employable:

JavaScript, (+ CSS)

Java

C#

That'll give you what you need for the mass majority of enterprise-y jobs, of which there are an incalculable number. you really only need either Java or c# as they might as-well be the same language for how similar they are. ",1539724805.0
Menliros,"It depends on which area you want to focus on. That being said JavaScript is very much in demand in quite a lot of areas,",1539715858.0
pmpforever,"Learn whatever language is best for the job you want or the field your are interested in. Languages are tools, and good programming skills are largely transferable between them. If you have no idea then take a free course on Programming Languages and try Rust or Haskell.",1539763283.0
callmetom,"This question is very vague and does not strike me as a computer science question, but a programming problem. I’d repost to a programming sub such as /r/learnprogramming or a language specific sub and include a lot more detail about what you’re trying to accomplish. 

My response to this question can only really be “in one of several ways depending on the situation, but given the choice I wouldn’t likely use arrays for CSV records as it then disassociates the value from its header if they existed .”",1539722528.0
orangejake,"It'd be helpful if you could be a *little* more specific with what you want to know (A pitch for our research area? A name for a research area and a list of topics that play into it?).

I'm a 1st year PhD student in cryptography, and am working in lattice cryptography right now (I have some background in my undergraduate in it). This is a rather theoretical area of cryptography that aims to develop cryptographic systems based on *mathematical lattices* (essentially a collection of vectors that you can add together, and constant-multiply by *integers*).

These cryptographic systems have some desirable properties:

1. Thought to be secure against quantum computers (unlike most of traditional cryptography)

2. Nice complexity-theoretic properties even ignoring the above

3. Really cool constructions that have only been instantiated on them (see FHE).

Lattice cryptography has significant efficiency downsides, although progress has been made. Current research involves many things, but includes ideas like:

1. How can we make it more efficient?

2. Do certain techniques to increase efficiency (i.e. ideal lattices) make the schemes less secure, meaning allow attacks that ""traditional"" lattice cryptography doesn't have?

3. How can we make FHE *specifically* more efficient?

A quick explanation of FHE, or ""Fully Homomorphic Encryption"". If we have a ciphertext c = Enc(m), FHE allows us to compute c' = Enc(f(m)) for any computable function f, without knowing how to decrypt c.
This seems like it could have broad applications to the real world, although is currently rather inefficient. For this reason, creating more efficient FHE schemes is an area of active research.

___

Background that helps in this is:

1. Algorithms design knowledge

2. Complexity theory knowledge

3. Cryptography

4. Probability (common to need for cryptography).

5. Linear Algebra

6. Algebraic Number Theory (""Ideal lattices"" mentioned earlier utilizes some rather theoretical mathematical objects known as ""Rings of Integers of Algebraic Number Fields"" to get efficiency gains)

7. Galois Theory (ties into the above pretty closely).

Even with all of the above mentioned, random other parts of theoretical math can be useful as well. Fourier Analysis can show up sometimes to prove bounds on random variables, and I saw group theory randomly show up in a paper lately (Cayley's Theorem that G embeds into S_{|G|}).",1539669777.0
Samrockswin,"That question really depends on the area. I'll give my two cents from a more systems-level experience. My area is High Performance Computing (HPC). In HPC there will often be some problem with scalability, e.g. this physics code is using 2% of the peak performance, how can we fix this? Thus, there is a lot of coding and learning to use academic software. So, with HPC a lot of your time is spent trying to understand what another academic meant with their build instructions.

As for what it took to get there, what it took for me was getting an M.S. and working closely with my adviser and developing a good relationship with him. Then that recommendation and experience allowed me to get into another university's Ph.D. program in HPC, a field I wanted to enter since I took a parallel programming class and enjoyed it.

I think the most important thing for a prospective researchers is to get to know the people doing research. Most likely that will be your professors, so go around to their websites and look at their research, see what interests you, and ask if they have any projects they need help on.",1539669013.0
therealhrj,"All I can say is find the professor that studies the area your most excited about.  Tell him or her that you would like a research opportunity and that your super interested in their field.  They will let you know what you need to do to get into undergraduate research at your institution.  Don't be shy about this.  Seriously, send your professor an email right after you finish reading this.

Also, be prepared to read and learn your little heart out.  It is super tough at times but you really feel like your working on something enriching, and amazing :)
",1539669820.0
PRSGRG,"2 cents from an italian post-doc in CS. (post-doc is the entry level of research professions).

In no meaningful order (that is an accurate representation of when these things happen) the activity of a researcher comprehends: applying for grants, prepearing publications/reports about your work, working on lab projects, working on your own project, doing some teaching, tutoring students, reviewing the work of others, going to conferences, writing software/scripts, waiting for software/scripts to fail, downloading papers you'll read ""later"", struggling because you did not studied enough math, struggling because you did not studied enough CS... 

It may look bad, but is not, because the time you spend doing actual research, meeting people at conferences, helping (and being helped from) students is worth the rest. 

As many said, what I mean with ""research"" actually depends on your field of study. It may be: demonstrating theorems, writing software, doing experiments, writing papers, or (probably) all of the above. 

About the ""how to get there"": After an interesting bachelor degree, an exciting master degree, and a wonderful phd (and related nervous breakdown/impostor syndrome/realization that ignorance is unbounded), I applied for a post doc position: presented a project (in my case: information retrieval and processing of audio signals), discussed it, and got the position.
",1539670520.0
lrem,"I've done my PhD in combinatorial optimization in INRIA in France. All it took to get there were good grade in my MSc and talking to the professors (there have also been some military background checks, but that was done in the background and I was barely aware of them). 

The drill is that professors present interesting problems to students and postdocs. These problems might be things that partnering companies didn't know how to solve in the industry, got brought up at a conference, offiste, by a visiting scientisti... Or outright dreamed up by the professor. Anyhow, you trust your professor to have good judgement on what's worth working on, especially if you're the kind of person that dreams up interesting problems. Hence, choosing your team is the most important decision when joining academia.

Once you have a pool of problems you work on, you start reading up on them. Reading prior art and learning techniques to attack a problem are a major part of the job, for most people up until they retire. Another large chunk is spent on discussing ideas. Usually you work on a thing in a team of a few folk and keep poking holes in everything you think of until it's scrapped or seems bulletproof. Being convinced an idea is good, you write some code or a formal proof. It it turns out right, the last huge chunk is writing up a publication. Here you once again depend on your professor to decide the right venue, that is the right tradeoff between prestige and chances of accepting the resulting publication. If it is rejected, you're back to writing it up better and trying another time, so choosing a wrong venue is quite a setback.

One of the cool thing about doing research is that it's as cooperative as competetive. Yes, it might happen that someone else just published the result you've been working towards for the past year, rendering your publication lost effort (in CS/maths the value of reproduction studies is usually non-existent). But unlike in the industry, your typical reaction to that is get yourself invited, go visit them and write the next paper together.

In general it's a great experience, if you feel like devoting a couple years of your life towards doing something cool. But I don't recommend that as a career path unless you really, really love doing it and don't mind the bureaucracy (which you will learn doing a PhD).",1539676840.0
dwkeith,"Not all research is happening in university. I worked in IoT (Nest/Google) and filed a few patents related to software there. (ugh, I know)

But Google, Apple, *et al* are also doing a ton of research in machine learning, security, and more, some of which they publish.

Presently I am at the Wolfram Tech Conference showing off how my current company uses Mathematica to solve problems with reproducibility in chemistry and biochemistry experiments. (A mix of applied CS, algorithm development, and new machine learning techniques)

Basically, most companies that I want to work for are solving hard problems that have yet to be  solved in CS. Some are for the greater good and published, most are trade secrets.",1539702258.0
CorrSurfer,"I think that PRSGRG's description is spot-on. 

The best way to find out if research is something for you is to try it out. How to do so depends on your academic environment.

- In the US, it appears to be quite common that good Bachelor's students approach professors for additional some research work, so that they can get a first insight into how research works. Normally, this involves helping the professor or some of the professor's PhD student(s) with a concrete question, and if everything works out, you even get a publication out of it. Then you know if you like it or not. Also, the publication will make it much easier to get a (paid) PhD student position later.

- In Germany, it's quite common to be employed as a ""student helping scientist"" - for (small) money. Research groups often have funds for this available, and this is a good way to get some experience. This typically involves helping the research group with the research work in some third-party funded project. Approach professors who are doing interesting work and just ask.",1539679064.0
nablachez,"Ive read a few papers on animation and a lot of them utilize machine learning to pretty good effect. 

You should def sub to twominutepapers on ytube. He is a professor at TU Wien iirc. And shows state of the art research in the area of game/media tech.",1539736498.0
aetherman,"Hello. I'm a 1st year PhD student in database research and the research group I'm in just submitted a paper on a graph database system we built. I joined the Ph.D program straight out of undergrad from an okay university. I had been a statistics and computer science double major.

&#x200B;

I got here by a lot of luck. I had above average grades overall, but I aced the database class in undergrad. This led to an undergrad research project with the professor from the databases class. Even though that research didn't go anywhere, his recommendation and acing the GRE got me into a top 10 program.  I originally was planning on working with machine learning, but I had a good connection with my adviser and my background in databases meant I could immediately begin research. I have been able to bring a unique perspective to the problems because of my strong math background.

&#x200B;

If I could go back to undergrad and do it again, I'd have more proactively sought research opportunities. Many top programs evaluate candidates simply by their undergrad research.",1539747816.0
vznvzn,"have been researching some of the top problems eg **P vs NP** in a nonacademic/ outsider/ hobbyist setting which has had a **$1M prize** almost 2 decades. feel that CS is one of the most beautiful/ deep forms of math. this is beauty/ depth is captured eg in the work of **Turing** et al. a more ""accessible"" problem but still very difficult is the **Collatz** conjecture which also has deep ties to CS. have been banging on it on/ off several decades now. CS is increasingly very crosscutting eg into **physics** esp notable wrt **phase transitions**. CS is the ""field of the future"". more accurately its _inventing the future_. broadly defined (and why wouldnt one?) its world-transforming and will be even more in intermediate future with conquest of **AGI** on the horizon, and **robotics** is popping right alongside it. also consider the extraordinary everyday practicality/ diverse worldwide reach of **bitcoin**! hey what about the rise of **facebook/ social media**? lets face it while sometimes ones own role is a drop in the bucket or feels like trying to boil the ocean, the overall field is incredibly exciting/ engaging/ high energy at times. its a **vast/ thriving/ growing/ _exploding_ playground.** sometimes it seems worth it, other times not, other times one just gets in the ""zone"" and _neither_ matters! how could anyone _not_ be into all that? :) latest on collatz capturing some of the vicissitudes in near-real-time. https://vzn1.wordpress.com/2018/10/03/collatz-fusion/",1539746194.0
pulsar512b,"This is a bloody cool way to talk about the Mandelbrot set, but there's an issue: the mandlebrot set is only for one particular equation, can't remember which.

take the part that explains the thing and you got yourself a nice way to represent fractals in general.

&#x200B;",1539667474.0
simpleIteration,"The question is in the idea, if there is any use for this?

# 

^(Like making a decentralized immutable Mandelbrot set ledger to create a 3D space and build a digital world like Ready Player One's ""oasis""? Sure would cut down computing costs. Nobody would need the whole. Its like ""sharding"" in blockchain technology or ""edge computing."" You'd have your own little universe unless you shared or visited somewhere else. Decentralized and fragmented at it's very core. ""fractal"" comes from fragmented)",1539665628.0
9bob,I have own webspace and use Thunderbird - but i can automatically forward it if necessary. ,1539724231.0
GHC_,"What platform are you using? (e.i. Outlook, Gmail, Windows Mail)",1539642314.0
Swedophone,"> But is there the notion of a kernel in a VM?

Yes, that's the main difference between virtualization and containerization. Containers share the kernel with the host and VMs have their own kernel.

",1539625528.0
TurboFucked,"You might look at researching how Intel VMX and VT technologies work.  Intel and AMD provide instructions for host OSes to hand over access to the CPU, which provides facilities for handling DMA, swapping page tables, etc.

APICv is Intel's facilities for virtual interrupt controllers, which allows the guest OS to handle interrupts directly without using the hypervisor as an intermediary.",1539627798.0
IdealImperialism,"Look up QEMUs TCG, it just directly translates whatever machine code a binary is comprised of to your native machine code. Additionally their full system emulation translates syscalls to the equivalent on your native system.

Some procs also have native virtualization support, but they are still limited to your native machine code e.g. you can only use it if you virtualize x86 on x86. Linux KVM is something to look at for this.",1539637270.0
Vallvaka,Isn't this just gradient descent with a minus sign flipped to a plus?,1539648117.0
liketocumalot,My penis does the same thing  o_O,1539575704.0
TomvdZ,Please define `n`.,1539596732.0
carbonkid619,"If it's adding two matrices containing n elements each, then it's O(n). If it's adding two n x n matrices, then it's O(n^2), since each matrix has n^2 elements.",1539575647.0
santoso-sheep,"The sum of two nxn matrices is O(n^2 ) because you have to iterate over every element in each nxn matrix to sum them.

If you have two 3x3 matrices, then you end up doing 9 sums. If you have two 10x10 matrices, you end up doing 100 sums.",1539568155.0
Liz_Me,"This I would imagine is a great interview question. If the output is n bytes, what is the lower bound on the algorithm? Can you design a O(log(n)) if you need to write O(n) bytes at the end?",1539580245.0
velcrorex,How big is the matrix? How many additions do you have to do?,1539567539.0
JNCressey,Because there are n^2 individual additions to do.,1539568258.0
Rioghasarig,Because there are n^2 numbers in an n x n  matrix. ,1539579177.0
Funktektronic,The complexity of matrix addition depends on the size of the matrices in both dimensions and how you are representing the matrix.  In the usual full matrix representation for a square matrix there will be nxn entries in each matrix and you will require nxn additions.  If you have sparse matrices or banded matrices with O(n) non-zero entries then the matrix addition will be O(n).  Heck you could even get O(1) for matrix addition if you can rely on some special structure of the matrices.,1539590965.0
xanderhud,"“In computer science, big O notation is used to classify algorithms according to how their running time or space requirements grow as the input size grows”

https://en.m.wikipedia.org/wiki/Big_O_notation",1540129866.0
xanderhud,"It’s O(n), where n is the input size.",1539571677.0
cvanolm,"Will he rewrite math theory who knows, stay tuned...",1539580819.0
Pangus1980, M,1539630658.0
LearningMachinist,"TACP is an ongoing, neverending reading, something you come back to over and over again for insight into programming in general. CPL is beauty of immediate, practical use.",1539563139.0
zokier,"K&R is 270 pages of practical programming, TAOCP is 3200+ pages and counting of theoretical CS. Not exactly comparable works. ",1539617674.0
TurboFucked,"The C Programming Language is not a good resource for modern C programming.  [This is a wildly unpopular opinion though.](https://zedshaw.com/2015/01/04/admitting-defeat-on-kr-in-lcthw/)  

TAoCP is essentially your degree in writing.  You'll gain a lot going through them -- it will fill in a lot of gaps from your education.  But since it starts from first principles (numbers, arithmetic, induction) it can be hard to get into initially.  ",1539583501.0
Sir_not_sir,SJF says read Ritchie first.,1539550595.0
LearnerPermit,"I'm curious, what language did your university mostly study/focus on? If it's C++ or Java, the details of the syntax are similar for most stuff. Concepts and thought processes are similar. 


Universities that focus on languages that are fun for academia like Haskell or very high-level languages like python really put their graduates at a major career disadvantage. 
",1539583403.0
acroback,"Read up K&R to understand basics but do not get hung up on undefined behavior and compiler dependency. 

Oh do try those exercises, some of those are difficult to crack.

TAOCP is a good read but very terse and will take you years to masters it. 

Good luck",1539585665.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/u_blackrom0608] [C Programming Language or The Art of Computer Programming](https://www.reddit.com/r/u_blackrom0608/comments/9oczll/c_programming_language_or_the_art_of_computer/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1539612734.0
chamcham123,"Neither. For a modern introduction to C, I would use ""C Programming: A Modern Approach"" by K.N. King",1540161826.0
turning_tesseract,"Both have been beneficial to me - K&R for learning more about C, and TAoCP for learning more about Algorithms & Data Structures.

However, that said, both of them are not the easiest of books to read on their respective topics, and personally I would not recommend either of them to a beginner. As a beginner I had tried reading both those books, but found them too dry and tough to finish (of course, your mileage may vary).

But once I learned a little about C and Algorithms & Data Structures from other books and youtube lectures, the two books above become much more tractable, and very useful for learning the two subjects in more depth.

So now as a general learning strategy, I prefer to learn the basics of any topic or technology using lighter introductory books and YouTube videos, in order to quickly get a high-level idea, and then go back and read the classic books to learn the topics in more depth and to fill-in the details.",1539598652.0
agumonkey,https://mitpress.mit.edu/books/art-prolog-second-edition,1539594110.0
gct,"In general yes, if you're remove the dependencies between instructions then you can execute them in parallel.  Modern CPUs do this for you.  x86-64 has ""16"" general purpose registers, but in reality it has hundreds, the CPU handles renaming registers for you on the fly to enable parallel execution.  This is the crux of a super scalar processor.",1539538583.0
lgroeni,"It’s worth mentioning that most modern processors do actually have multiple ALU execution units per core, precisely for this reason. If you look at the micro architecture diagrams of [Skylake](https://en.wikichip.org/wiki/intel/microarchitectures/skylake_%28client%29#Individual_Core), for instance, you see that the scheduler is connected to multiple execution units that have overlap in terms of what they are able to do.

This allows better instruction throughput than if you, say, only had a single integer ALU per core.

Worth also mentioning is that SMT and CMT approach the same problem by going the opposite direction - instead of relying on instruction level parallelism to extract maximum IPC, you expose more than one front end to the same execution units and allow it to execute work from more than one thread at the same time.",1539542651.0
NicolasGuacamole,"Different op codes do use different registers / bits of logic (hence why it works). As for adding ALUs and such, it seems like you’re basically describing having multiple parallel pipelines going at a time. This is what happens in a superscalar processor.

Someone else may have a better explanation, I’m not a hardware guy.",1539537838.0
combinatorylogic,"It's a very bad idea in general to even have any instructions implicitly writing into specific registers (unless it's a PC, and maybe stack pointer too).",1539550618.0
2Bois1Gril,"Variable names only exist pre-compilation. Basically, they're for the programmer to make understanding and writing code easier. After that, the computer is just looking at the address of the memory location.",1539523227.0
fff1891,"Variable information is stored in the symbol table during parsing. A compiled language uses a symbol table during compilation/linking, an interpreted language will refer to the symbol table during execution. ",1539529329.0
everything-narrative,"The program code is converted into numbers stored in memory, in the form of machine code. The CPU looks at the machine code and interprets the instructions encoded. These instructions say things like ""add the value of memory location A to memory location A"" or ""sore value Q at memory location P.""

Whenever the program code refers to 'x', the machine code refers to 0x00 in an analogous manner. The information that 'x' is associated with 0x00 is a decision made by the compiler — the program that converts the human-readable program code into CPU interpretable machine code.",1539544041.0
bo0mb0om,"That depends on language. 

In C++, x would be at memory location 0x00. 

In Python, the x would be stored somewhere else, and 0x00 would contain the address to that.

In Java, 0x00 would contain the integer. However, if you created an object of your own class, 0x00 would point to the location of your object as with Python.

As to where the reference table is stored, that again depends on the language. For example, java stores its references in the stack. For C++ you create your own pointers, and you can have them anywhere.

Even this is a simplified view. Depending on whether a language is compiled or not, and the compiler options, x might not even exist in the final machine code.",1539524257.0
Indifferentchildren," If x is a local variable in a function, then the address where it points is stored on the stack, in the stack frame where that function is executing. If x is a global variable (in, say C), then there is a separate global space that holds the address. If x is a member of a class that is on the heap, then x is on the heap, inside them space for the members of that class. ",1539529041.0
green_meklar,"The compiler generates code in such a way as to ensure that the right location in memory will be referenced when that variable is accessed. If the variable is a local variable inside a function, the compiler has probably thrown some appropriate constants into the code for that function, which get added to the base address for that function's stack frame.",1539539931.0
hamtaroismyhomie,"Post in r/csmajors or r/cscareerquestions next time.

Looks like a great option in terms of the coursework and skills.",1539531846.0
khedoros,"Some advertise as being remote positions. More commonly, you'll work somewhere for a while, then negotiate that you'll work a couple days a week at home. My last job had a policy of ""we don't care, as long as your work gets done"", so 2/3 of the time, I was the only one on the team in the office (having a toddler at home isn't conducive to having a productive home-office environment!)

More commonly, you'll have daily or weekly meetings that you're expected to attend in person, unless you were specifically hired as a remote worker.",1539485607.0
solinent,"If you're looking to work from home, the surest path is to find work at an office first.",1539480628.0
JimBean,"There are some. But they are hard to find and their investigation into your abilities will be greatly certified before they will even negotiate. (In other words, you HAVE to be a good and well disciplined in your game).

Working from home is actually harder. Ok, remove the good side, no commute, no co-workers to interrupt you. No phones. There are many advantages. But you have to be well self disciplined. It's actually hard to whip yourself into ""getting to work"", actually sitting down to the job. It's also easy to work too much, and not get paid for it. Also, those moments when you do feel the need for human interaction, it can be lonely...

Having said all that, I work my best alone... ;)",1539485753.0
mredding,"There are different and tech centric job sites out there, well above and beyond the likes of Dice or Monster, and those are the sites you want to find and do your job search. Said sites have remote positions as a criteria. I've had some luck landing interviews, but didn't pursue them further because I landed a local role I couldn't pass up.

So they're out there. It does take additional work to find them, beyond what is explicitly advertised as remote. Interviewing is no different, in my experience, than in person - lots of video conferencing and collaborative software.

My recommendation is to have a dedicated office in your house. When you are in there, you are working. Work doesn't leave your office.

Some places also rent out small personal offices, cubicles, and I've even seen these pod things that look like a student desk inside an egg. But some people need that sort of thing.",1539488734.0
Kel-nage,"You are right. The cardinality of A is 3, and the cardinality of P(A) is 8. Either the question or answer is wrong.",1539452929.0
noam_compsci,"Tell whoever made this that if they put the questions as a-d, they should also put the answers as a-d. Lazy is what it is. 
",1539454775.0
Bromskloss,"> I got everything but part d wrong

You mean that you got everything but part d _right_, right?",1539459716.0
_georgesim_,"And also, there's an implicit assumption that e =/= f, f =/= g and e =/= g. So they're distinct elements.",1539457416.0
green_meklar,It looks straight wrong to me. Some textbook writer goofed.,1539460314.0
th3-villager,"Yeah should be |P(A)| so you are right as has been confirmed already. This is the kind of typical nonsense errors that you will see far more often than you should, so be wary and trust if you’re confident it may be a question error ",1539470522.0
JNCressey,"Something to learn is that people making practice or exam questions and answers can make mistakes too. When an answer in a book disagrees with your answer, just take it as a prompt to double check your working out for mistakes and, if there are no mistakes, to try to figure out where the writer made their mistake. 

Some mistakes can even be copying errors when typing something up. Like 3s and 8s being confused by sloppy handwriting in the draft or something.",1539470834.0
space-creature,"Math texts are so detailed that you're bound to find a lot of typos.  Whenever you find one, congratulate yourself on reading carefully.  The person who doesn't notice them is the one who will not graduate.",1539479983.0
fear_the_future,"It could be a typo or it could just be that your prof uses non-standard notation. Notation is often different depending on context and personal preferences, so you should definitely clarify that. For example, some of my profs use # for cardinality.  ",1539516007.0
kbrosnan,"I generally see people use [fingerless gloves](https://www.amazon.com/Thinsulate-Thermal-Insulated-Winter-Fingerless/dp/B01M04H69T/ref=sr_1_6?ie=UTF8&qid=1539450122&sr=8-6&keywords=fingerless+gloves) in this case. 

You can also make them yourself if you don't care about the looks. Just take a pair of scissors and cut through a pair of knit gloves. Just remember that you can't add material back so cutting conservatively at first is recommended. Depending on the weave they can/will start unraveling especially when washed. ",1539450234.0
,[deleted],1539452532.0
asciiontology,"I had a roommate like that for a while. Highly recommend a good blanket or the Snuggie. Then get a heater or something for your desk to keep your hands warm. Or get fingerless gloves. Or get some of those capacitive gloves that people use for touch screens outdoors in the cold. Signed, a guy that has been doing his coding under a blanket ever since the weather turned chilly. God bless.",1539611353.0
richard248,"I was in a similar situation a couple of years ago when I lived in a very cold house. I didn't get around to finding gloves that were insulating enough that my fingers were warm, but slim enough that they didn't get in the way of typing, so +1 for a recommendation from me.

Otherwise, ""reusable gel pack"" hand-warmers were very good. You put them in boiling water for a little bit, then let them cool down to a solid. Whenever you want, you just bend them a little to break up the solid, and they get really hot for a while. Reusable too so you can just put them back in boiling water for next time.",1539449875.0
AgentTin,If you search on Amazon for usb heated gloves you'll find a lot of them of various quality.,1539452373.0
MellerTime,"I had the same problem when I was young because my mom was so cheap she insisted that having the heat set in the mid 60’s was plenty warm enough. I’m not terribly confident that you’ll find any that are thick enough to do anything but thin enough that it won’t interfere with the finer movements required for typing. If you’re set on finding gloves I’d say look for some that have those little tips that let you use your phone’s screen. You don’t care about the tips, but they usually seem to be thinner and made of more flexible materials since they’re geared towards something that still requires the finer use of fingers. 

I think a better option would actually be draping yourself in blankets like an old woman to hold in core heat and then getting a small electric space heater that you can put on your desk and point at your keyboard.",1539450826.0
,[deleted],1539449148.0
HimDaemon,"Web scraping, machine learning.

Reminded me a bit of the [NELL](http://rtw.ml.cmu.edu/rtw/) project.",1539448293.0
NerdAtTheTerminal,"I am amazed at how little research we do before asking out here (including me).
`wget` gets your job done.
Otherwise scrapy is a good framework.",1540569893.0
7yl4r,"I don't have examples, but nowadays PhDs are about publishing and paperwork. ultimately all you need to do this is a committee and department willing to do it. If you can crank out three good publications and wrap it in a dissertation your committee is happy with in a single semester I don't think anyone would complain.  ",1539445865.0
UncleMeat11,"It *is* possible. I know people who had more first author papers in their first three years than I did in six. You can produce enough research to justify a dissertation faster than normal. But this is not a property of working hard. *Everybody* in grad school works their asses off. Finding the right topics and getting papers published has a tremendous amount of luck involved. You cannot just say that you will work harder than other people to get it done. 

But even when it is possible, most people don't choose to do this. Because a PhD is useful for two things: personal satisfaction and an entry to a research career. For the first one, you shouldn't be doing a PhD in the first place if you don't actually want to be there for a while. For the second one, people generally want to take a while in their PhD in order to produce the best research record possible, which makes them much more likely to get a faculty position. You *could* graduate in three years, but it isn't going to get you a job. So why do it? If you wanted to no longer be doing a PhD, you shouldn't have started in the first place.",1539446051.0
chx_,"> for example a tool that could translate tensorflow/pytorch code to fpga cores

Let's run some numbers. Say, a normal PhD takes 10 000 work hours. Rough ballpark, five years, one year is 2000 work hours. Say, half of that is the actual work, the other half is writing, communicating, whatnot. Very rough. Still, 5000 hours. Would you think writing such a tool is worth 5000 hours? I perhaps do not grasp the enormity of the task but I am not 100% this is worth a Phd. 

Of course, I am just a random stranger rambling on the Internet. What matters is whether you can get an advisor accept it. Shop 'round.",1539448593.0
stochastaclysm,"Pretty much every PhD student thinks this. Alas, once you get started you realise it’s not that simple, and why it’s considered such a difficult degree to get.

You’ll probably spend the first three semesters on classes and working out your research project before you even get anything done on it.

Don’t fixate on the time frame too much. Just enjoy your work.",1539449588.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/math] [accelerated phd (cross post r\/compsci)](https://www.reddit.com/r/math/comments/9nuo3y/accelerated_phd_cross_post_rcompsci/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1539445522.0
lrem,"You can make a PhD as quickly as you can crank out an amount of results that other people crank out in a duration of PhD. Twice as smart as the average? Cool, you can take half the expected time. Twice as many hours per day on top of that? Yay, quarter. Twice as lucky too? Congratulations, the time to get a committee running has just become your bottleneck.

There are other questions though: would you really like to do that, if you could? Would your advisor go with it?",1539446485.0
Jaxan0,"If I could, I wish I could extend my PhD life. You have so much freedom (to travel, choose topics, etc). Much more than a professor has, or someone in industry.",1539458023.0
DankKushala,"It's possible, try to publish stuff quickly. But there's no magical recipe that will guarantee doing so. Most people who finish quickly do the same stuff that anyone does, try to work on what they're interested in. ",1539446397.0
accountabilitysuucks,"When I did my Doctorate (Neuroscience), we didn't even have our first exam until after our third semester, and coursework ended after year two.  After that-conducting preliminary research, second exam (oral exam), conducting your research, and writing your thesis it's up to you.  I can't imagine in anyone finishing in a year, or two, even three.  But everyone's different.",1539463648.0
jmite,"I don't think it's possible, unless you've already started publishing.

The thing is, it takes time to learn how to do research and do technical writing. It doesn't matter how smart you are, it's just radically different from anything you're likely to encounter in a Master's or undergraduate program.

It takes time to learn, and the publication process is slowwwwww. Conferences take several months to release reviews for papers and journals are even longer. So even if you do good research fast, and submit publications, there's no guarantee that they'll be accepted.

On top of that, you'll need to read other papers, get familiar with related work, etc. This all takes time, hard work or not.

A single tool might be enough for a paper, but not likely enough for a PhD.",1539825014.0
okapiposter,"You have to assume that the locking transaction can *read* object `O` at any point between `s(O)` and `u(O)`, and that it can *read and write* it anywhere between between `x(O)` and `u(O)`. Since you don't know the actual operations, I would interpret the exercise to ask for *potential* conflicts, so you are not allowed to overlap or skip over an exclusive lock on an object with any lock on that object by another transaction.",1539449876.0
JonnyRocks,"This isnt very acurate and it looks like you only have 2 years of experience? Well from eomeonw with 20 years i can tell. You a fulltime office job varies dramatically between working for a bank, a logistics company, a food service devices company, a software company, and a racing company (a few careers i have had).

Things change based on who your client is and for me personally, writing for different devices is the most fun. ",1539450129.0
bndrz,Hey guys I’ve wrote some thoughts on working with complex software projects. I would appreciate your feedback!,1539414503.0
flexibeast,"Full abstract:

> In this work, we present an approach towards constructing executable specifications of existing filesystems and verifying their functional properties in a theorem proving environment. We detail an application of this approach to the FAT32 filesystem.
We also detail the methodology used to build up this type of executable specification through a series of models which incrementally add features of the target filesystem. This methodology has the benefit of allowing the verification effort to start from simple models which encapsulate features common to many filesystems and which are thus suitable for reuse.",1539399570.0
billccn,So here's the question: is it Turing complete?,1539460496.0
Meliorus,That's not what IQ measures,1539386390.0
panacanny,Is this real?,1539458531.0
flebron,"(a, b) < (c, d) if and only if (a < c) || ((a = c) && (b < d))",1539366294.0
wookie-bowcaster,"Do you mean to ask what is lexicographic order, and how does that apply to ordering a set of tuples? ",1539395901.0
emceekain,"The formal definition, I think, is less helpful the colloquial name, i.e., the ""dictionary ordering.""",1539400836.0
julianCP,Whatever you enjoy the most. I may be biased but I think this is the only acceptable answer.,1539352450.0
jet_heller,"By far, the best one is the one you're doing.",1539352576.0
Marsmell,"The answer to this question depends heavily on you and your criteria for ""best"". We could rank them based on which you would enjoy doing the most, which pays the most, which has more job openings, etc. and it's up to you to decide which is more important to you. I think most of us would recommend the former, do what you enjoy, and no one can answer that question but you.",1539355089.0
iambeingserious,Which ones are you thinking about currently?,1539352799.0
lanemik,Meanwhile all of the python developers I know use pycharm. ,1539348572.0
Xef,No pycharm?,1539348570.0
cawkwielder,Calling IDLE an IDE is a bit of stretch. ,1539350816.0
combinatorylogic,No elpy?!?,1539349196.0
SirBitcher,i like Jupyter notebook,1539351213.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1539323563.0
wischichr,"Depends on the type of malware. Nowadays classical viruses are pretty rare because they are hard to write.

A virus infects other applications - for example it edits a software like Microsoft Word on your device (edits the winword.exe binary directly). This is hard and may corrupt word if done wrong so it wouldn't start any more and that basically would prevent the virus from spreading. (If HIV would kill you instantly it would be much less of a problem because nobody had time to spread it and the virus would stop to exist)

Most malware today are separate programs (worms) and it's pretty ""simple"" for them to spread. They just copy themselfs to other systems and change a setting so the malware will be started again after a reboot.

Most of the time the user is tricked into coping the malware himself (by accident) for example with mail attachments - but sometimes a bug in a specific software is used to spread to other machines.",1539324872.0
qqwy,"At the most basic level, a computer program that is running is able to also access its stored form (as an executablen file) on disk, and copy this to other locations. That's how they can spread to USBsticks, disks, connected cloud drives, etc. Related to this is also the possibility to send an e-mail with the program as attachment from the program. 

Of course, virus scanners know all the simple tricks nowadays, and will notice and warn about them.",1539332655.0
Responsible_Virus,look into quines they are self replicating code the rest of it is going to be about stuff like how to spread the virus or what you want the virus to do.,1539317968.0
eigenman,"Latest book I'm reading and loving it.

*Quantum Computing Since Democritus*

by

*Scott Aaronson*


https://www.scottaaronson.com/democritus/

",1539310138.0
EscDalton,I’m a CS student and I just finished applying to summer internship programs. How long does it usually take to hear back?? ,1539306622.0
batteryramdar,just got Amazon SDE OA.  Anyone have any tips for the first debugging/logic test?,1539306787.0
pulsar512b,"Found a SQL evaluator, if that's your thing.

[https://kripken.github.io/sql.js/GUI/](https://kripken.github.io/sql.js/GUI/)",1539306478.0
ejineta,"If you have good discipline, go solo! It will pay off very well, in the sense of job applications. In Europe at least, the final project is probably the number one aspect of your resume! Just make sure you have a supervisor whom you can talk to every once in a while",1539284038.0
kamanodomino,Solo. You'll most likely end up doing most of the work yourself anyway. ,1539282576.0
NomNom150,"Personally, unless you’re at a good school, people are going to take advantage of you. ",1539282343.0
Darksair,"Do research and read tons of papers, do tons of math (so much that you’ll never forget it in your whole life). ",1539283139.0
khedoros,"Why do you feel limited to reading about it at work? I usually spend at least some time online after my son's in bed, and before my wife and I head for bed. That time's for reading whatever strikes me as interesting, but it's often still something CS-related.",1539279474.0
tutuca_,During my working hours. Half of my productive day is to browse reddit and hacker news...,1539286314.0
combinatorylogic,Not that much is happening in CS anyway. What kind of news you're talking about?,1539349405.0
CorrSurfer,Do you commute by train or by bus? Then there is an obvious answer.,1539270529.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/computervision] [Optimized Image Feature Extraction \/ Categorization](https://www.reddit.com/r/computervision/comments/9n7zqf/optimized_image_feature_extraction_categorization/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1539245573.0
ranieuwe,"No. Interviewing a lot of folks, I really don’t care about certificates for stuff like that. The only thing it shows you were able to sit through a course, which is a nice skill but not one I as interviewer am interested in. ",1539252064.0
djbaha,"Nobody will care about your certificates, they'll put you through practical interviews anyways. If you've learned the skill, just list it in your cv - no need for certification ",1539248094.0
,"Ive paid for two Udacity courses and I was really happy with it.  I know your question is ""will it get me hired"" , and everyone has so far has said ""no"" , but the information you learn in the course \_could\_ get you hired.  Not the certificate itself but the skills you learn while taking it.  


Also, some cutting edge things, like Deep Learning , Deep Reinforcement Learning I think do better in an online course because it changes so rapidly.  This year everyone has abandoned gradient descent for genetic/evolutionary algos, you're probably not going to get that information in a slow moving university.",1539262203.0
helpfulsj,"They will help get past some gatekeepers, just like having a BS will.  

If you take the courses seriously and work on projects applying your skills they will show in the interview. 

 Just like if you take school seriously, it tends to show.

You're never going to do your self any harm by furthering your education. ",1539261764.0
arkrish,"There is a number of non-technical people in the loop until you get to an interview, and having certificates will help you go past them. However the value will diminish once we get to the interview. ",1539269010.0
locotxwork,"Do some projects where your work is the topic not your certification.  It will help out.  Why don't you do a few projects for a non-profit, its' a win-win.",1539278532.0
,[deleted],1539250282.0
stathibus,"In my opinion it does show a degree of self direction and motivation to improve, which is valuable. I don't put any assumptions on what you actually learned from them so I'll give you the same technical interview questions I give everyone else.",1539257458.0
noam_compsci,"The only companies that would value this is a company you probably would not want to work for. Im not talking about 'second tier', I mean truly terrible companies that have no idea what tech is. 

",1539280954.0
sturmhauke,"The courses might be worth it for educational purposes, but the only certifications that are worth anything are ones for specific technologies (like Microsoft's MCSE or Cisco's CCNA). Even then you're going down a highly specialized path that may or may not get you where you want to go.",1539291127.0
namegone,Followup: anyone have an opinion on the new edx Master programs?,1539294260.0
AaronKClark,"Personally, I have added a section to my [cv](https://www.cryptospace.com/~akclark/content/Clark, Aaron K. - CV.pdf) for edX, but I am keeping them off my [resume.](https://www.cryptospace.com/~akclark/content/Clark, Aaron K. - Resume.pdf)",1539250890.0
realFoobanana,"It *might* be the case that an oversimplified, theoretical model of computation constructed in this way could be Turing complete, sure.

But the Venus fly trap itself isn’t Turing complete 😛",1539217407.0
danketiquette,That must have been a long shower you were taking,1539267101.0
feedayeen,They'll be less Turing complete and more functional complete. The part missing from that is some way to invert the signal with a not structure. It doesn't seem like you'll be able have that type of behavior and the buffer will cause some problems as it leads to a proportion delay on par with the reset time. ,1539239955.0
imkger,"The not gate should be possible if I am not mistaken with the addition of a stick. You could connect two traps in a way, where when the first trap would be open, it would push the stick up, that is connected in the middle so it pushes the opposite side of the stick down onto trap 2's hairs and activates that. That way when trap 1 closes, the stick should move back from those hairs and release trap 2.

/>

<

Kind of like that, I hope you can understand.",1539243088.0
c3534l,"If you're using Venus Flytraps to hold binary state, then sure - whatever system you rig up with communicating flytrap-bits could be Turing complete. But the plants themselves aren't Turing complete any more than a transistor or the silicon it's made of is Turing complete.

edit: egregious spelling mistakes made by typing on mobile at 2 am.",1539256938.0
benji_york,"OP, if you haven't read it already, I recommend you read the book Children of Time by Adrian Tchaikovsky.",1539225314.0
GNULinuxProgrammer,"This is like saying`+` is Turing complete, because I constructed a system (say, C++) in which `+` is a part of and is Turing complete. Well, your system is turing complete but `<` isn't. So it doesn't  make sense to say Venus Fly trap is turing complete. No, your language is.",1539293222.0
HimDaemon,"It seems you have described a [domino computer](https://en.wikipedia.org/wiki/Domino_computer).

https://youtu.be/OpLU__bhu2w",1539450180.0
MachineInTheStone,Jesus Christ I am whelmed. OP write up a diagram.,1539228759.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/computervision] [Fast Image Categorization Algorithm](https://www.reddit.com/r/computervision/comments/9n3t5e/fast_image_categorization_algorithm/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1539207732.0
LearnerPermit,"Your typical undergrad level algorithms and data structures class generally focuses on search, linked lists, trees and maybe graphs. Useful for learning the concepts, but professionally probably not used except for niche projects. Those basic data structures and the algorithms need to use them are typically available in standard libraries.

What goal do you have in learning algorithms? Since most of those classes either teach problem solving and various academic concepts.",1539209555.0
positive_X,"There are a couple of free ""MOOC""s with books and everything -  
.  
one uses actual programming language as examples 
(old was ""C"" , now ""Java"") ""Algorithms in C [1990] || Robert Sedgewick {Algorithms 3rd V1 (1-4) , V2 (5) [1997] } 4th ed. [2011] rs@cs.princeton.edu || C , C++ , Java ; Pascal""  
https://algs4.cs.princeton.edu/home/  
..  
the other uses ""pseudocode""  
[Introduction to Algorithms - by CLRS - MOOC DLed]  
https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-introduction-to-algorithms-sma-5503-fall-2005/  
...  
I have found these , but have not gone through either one (yet) .
",1539207838.0
ryanstephendavis,Prolog is such a fucking cool weird language... a person can write a Sudoku solver in around 5 lines of code,1539228637.0
PolarTimeSD,"I got this book at a used book sale for like $2, one of my favourite programming books. Then again, I have a small obsession with Prolog.",1539197702.0
,[deleted],1539229780.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1539187691.0
RickSagan,"* [Hacker News](https://news.ycombinator.com/)
* [MIT Technology Review](https://www.technologyreview.com/) (A bit of ethics) 
* [Journalist Enrique Dans](https://medium.com/@edans)
* [Ars Technica](http://arstechnica.com/)
* r/technology
* r/programming
* r/linux

And a lot of tech-related professionals on Twitter.

EDIT: The result of trying to answer on mobile while eating. ",1539185586.0
jigrafakira,cs morning paper blog is really good,1539189959.0
puppeh,lobste.rs,1539195094.0
Fr3ck,"My favorite is arstechnica.com. It mostly covers general technology, but it is my go to for daily news. ",1539185544.0
agumonkey,"I used to harass lambda-the-ultimate, full of gems. c2 wiki also.

Nowadays I'd rather have a book and exercises.",1539191682.0
MachineInTheStone,This week in machine learning is a pretty great podcast for different applications of machine learning.,1539188980.0
SomeRandomDude5,"There are a couple of blogs I follow in random order

* [https://thehackernews.com](https://thehackernews.com)
* [https://www.theregister.co.uk](https://www.theregister.co.uk)
* [https://www.codesimplicity.com/](https://www.codesimplicity.com/)
* [http://lanraccoon.com](http://lanraccoon.com)
* [https://blog.toggl.com/](https://blog.toggl.com/)
* [https://blog.codinghorror.com/](https://blog.codinghorror.com/)",1539197457.0
svick,"> Is there a Blog or Website you go to on the Daily?

No, because I use an RSS reader.",1539187650.0
Santamierdadelamierd,hackernews is probably what most follow. ,1539185616.0
_m242_,"Hello,

I read HackerNews daily, and /r/programming.
",1539192422.0
bushcat89,"Medium has a lot of interesting blogs from individuals and organizations.[hackernoon](https://hackernoon.com) , [towards data science](https://towardsdatascience.com) and [free code camp](https://medium.freecodecamp.org) are some of the ones I checkout.",1539216164.0
Roachmeister,"I realize it's a humor site, but I actually learn a lot from [the Daily WTF](https://thedailywtf.com/).",1539217349.0
jokasx,"Lobste.rs

It's a better version than hackernews. Really, get far away from HN. Full of politics, american centralized world view and opinions and most think they are pseudo intellectuals that know what they are talking about in the comments. In days past I fell for that thinking ""wow these people so smart"" but after a few years you realize that most of the things said there are lies or technical incorrect information that you know it's wrong but you just don't have the mental energy to write it of or complain anymore.

So it's good if you only look into the main stories but be very careful falling i to the rabbit hole of the comments that so many people say are wonderful and amazing.

I still think reddit is more raw and genuine and pure to have discussions about things. ",1539254193.0
Zaratustra_X,que no ,1539193626.0
rlopu,Devrant cuntttt,1539239280.0
just_a_bit_of_it,I have created http://techblogz.co to keep oneself updated with tech blogs. It is a collection of tech related blogs from technology companies. It also has a powerful search function to search for a specific keywords. ,1539243435.0
jpamata,"* [https://dev.tube/](https://dev.tube/) - best place to find and watch the latest tech seminars, dev tutorials, etc. 
* [https://hackaday.com/](https://hackaday.com/) - for IoT projects.
* [http://news.ycombinator.com/](http://news.ycombinator.com/) - tech oriented but still has a diverse amount of topics. My favourite thing about it are the links to articles and blogs that I wouldn't discover anywhere else.
* [https://stratechery.com/](https://stratechery.com/) - news about the tech industry.
* [https://blog.acolyer.org/](https://blog.acolyer.org/) - from the about page: *""a short summary every weekday of an important, influential, topical or otherwise interesting paper in the field of computer science.""*",1539260666.0
epenaloz,Download flipboard and pick computer science as one of your subjects,1539188692.0
Alex_Dem,"Looks useful, thank you!
But my opinion is Java is very complex language, and you never write a good cheatsheet for it. Or you can do it, but it will be a book:)",1539178690.0
IndependentBoof,"Short answer: yes, there are plagiarism detectors but the most popular one isn't hosted online (it's a command-line script that connects to an online SaaS that runs the analysis). You need to demonstrate that you're an instructor to get an account.

Long answer: If you didn't copy someone else's work, the odds of it being flagged for plagiarism is quite low, to the point where it isn't worth your time worrying about it. Honestly, though, it sounds like you have a guilty conscience. Don't take someone else's work in whole or in part. Cheating doesn't do yourself any favors. Plus, even if you use the tools available, they won't be useful to you unless you also have a database full of other students' answers (including your classmates).

**TL;DR - Don't cheat. If you didn't cheat, you have very little to worry about.**",1539149337.0
cubravk,"9262311516
0207675920
7134133639
9253221335
6975675762
8889295814
5629997532
4131913265
9205934745
1312909170",1539150901.0
Kristler,"My suggestion: learn how to use git if you don't know it already, and use it for all of your assignments. Gitlab or Bitbucket would let you host free private repositories. 

Git is an invaluable industry standard that is very important to know, _and_ your commit log doubles as near-irrefutable proof that you wrote the code yourself.",1539153170.0
tachyonflux,Did you document your code as you created it? It's not plagiarism if you created something just as someone else did but without knowledge of their creation.,1539148633.0
,[deleted],1539580721.0
umib0zu,IRC. Freenode. #python.,1539134263.0
lurkluther,"I’m not a python expert, but your post history says you’re taking a 100 level, so I can probably offer a bit of free tutoring. Feel free to PM me. ",1539135618.0
lurkluther,"Most colleges offer tons of free resources (professor office hours, open lab hours, special workshops, TA sessions, free tutoring centers, etc) that are awesome places to go. Never be ashamed to reach out to IRL resource, they will be much more invested in your success than internet strangers and you’ll see better results!",1539136137.0
GHC_,[https://stackoverflow.com/](https://stackoverflow.com/),1539134541.0
jet_heller,google.,1539135310.0
Zer0897,https://teachyourselfcs.com/,1539148916.0
GHC_,"I have the same problem. Lectures teach me the information, but not how to apply it, how it actually works in a real program. My suggestion: Go through your notes after each class and try to implement each idea into a small program. if you are having a difficult time with a concept, make time to ask your professor for help outside of class (its part of their job). Programming, for me, requires a lot of hands-on learning. You won't understand a concept until you put it to practice and see how it works for yourself.",1539135004.0
Richeh,"Agh, shit mate, I feel for you.  I got a pass when I did my degree, because I didn't understand *shit* about object oriented programming until, like, a year after I graduated.

They assigned reading that admittedly I rarely did, but I can't help but think it might have been helpful to mention that object oriented programming *still* has a procedural component that instantiated the class.  I was sat there baffled thinking ""What?  All of this code runs at the same time somehow?""  If they'd *said* that the constructor is a method that runs at the time of creation of the object instance instead of just calling it the constructor like that's self-explanatory... 

And I could rant for hours about the ""object orientation as a car"" metaphor.  ""It's like a car, see, so it's got parts... like the wheel.. and the steering wheel WHICH IS STILL A WHEEL but different, so inheritance, right..."" which is maybe a clever metaphor but utterly fucking useless in explaining how OO programming works because it's so abstract.  Hah.  Abstract.

Then we get a new lecturer.  Oh goodie, I think, a fresh perspective.  ""OO programming, right, it's sort of like a car...""

FUCK.

Some arsehole's written this in the opening paragraph of a text book and now all these dead behind the eyes postgrads all explain it the same way.

OK, I have a point here.  All of this shit, despite my self-righteousness, *is my fault*.  I never went to my lecturer and said ""look, I don't understand this"" because I was afraid of looking like an idiot.  Which is *ridiculous*.  I paid for the right to be an idiot for three years.

They might not thank you for it, but they won't know they aren't adequately doing their job until you tell them.  (DO NOT TELL THEM THEY ARE NOT ADEQUATELY DOING THEIR JOB).  Be polite, be humble, but be insistent, until you understand.  This is the best advice I can give you, having paid twelve grand for three letters after my name and fuck-all understanding so I had to teach *myself* how to program after I graduated.",1539167211.0
ritwik310,"Hope this will be helpful, https://github.com/ossu/computer-science",1539166864.0
velvet_diamond, openclassrooms or edx.org ,1539151827.0
FreeMulberry,"Khan academy has heaps of easy to understand video courses on youtube for every level of mathematics. It helped me heaps.  If you are just starting to learn programming maybe try a simpler language than C where you don't need to worry as much about memory allocation or where things are stored in memory, python or java are pretty simple. But if you need to learn C for a course you're doing, then what I do is just go through some simple tutorials on youtube to see what the fundamentals are for that language and then look at the docs to further see how things work.",1539158444.0
RickDeveloper,I’m taking MIT6.001. It’s not a hard course. ,1539167250.0
darkone1122,Try Coursera and Edx. They usually have good courses on the subjects you are looking for. Both Edx and Coursera had Good C courses last time I checked. ,1539168617.0
tonicinhibition,"I use [MetaCademy](https://metacademy.org/browse) as a guiding resource, especially for Linear Algebra.  Also look into [Essence of Linear Algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) videos by 3Blue1Brown on YouTube.

You will also find computer science topics on MetaCademy.

It's important to remember that specific computer languages and paradigms in programming *are* **not** *the same* as computer science, which is abstract.  When you learn a ""low level"" language like C, it's built on an ecosystem of existing complexity.  At one point it was considered a ""high-level"" programming language.

You do not need to address lower levels in computer science in order to learn some of the higher level stuff, in the same way that you can learn a good deal of chemistry without particle physics and a good deal biology without chemistry.  However if you want to get rid of the vague feeling of being supported by some magic black box, you might check out:

[Building an 8-bit breadboard computer](https://www.youtube.com/playlist?list=PLowKtXNTBypGqImE405J2565dvjafglHU) from Ben Eater.  Watch the videos as entertainment.

Prefer data structures and algorithms to languages.  Find game programming tutorials because they are fun and satisfying and give easy to visualize examples.  Understand that certain languages are better than others at expressing a concept.  (Scheme > Python > Fortran/c/C++, in my opinion)

Sites like [CodeWars](https://www.codewars.com/) give you a good way to practice simple problems and compare dozens of solutions while you are picking up a language.",1539202036.0
its_ya_boi_dazed,This sounds like homework...,1539127975.0
TheCodeClown,What class is this?,1539139784.0
ggchappell,"> I’ve heard that no one uses openGL anymore

That's certainly wrong. Furthermore, OpenGL is closely related to WebGL, which is the *only* standard 3-D graphics programming API for the web.

> GLUT died in like 2009

Before that, really. But *why* your class is using GLUT is a worthwhile question. I used to teach a graphics programming class using GLUT. The reason I used GLUT was not because GLUT was this awesome thing that I wanted my students to learn. Rather, when we do graphics, we need to be able to make a window, read the mouse, etc., and GLUT allows this to happen in a way that is easy to learn and works the same on MacOS, Linux, and Windows. Using GLUT meant that I could spend my time teaching about *graphics*, rather than the details of how to access the mouse. And I could write system-independent example code that all my students could run on their own machines without modification.

These days, if I taught a graphics programming class, I would almost certainly choose a framework other than GLUT; there are better alternatives out there, I think. But it's not a big deal. If the class is primarily about graphics, and GLUT is just an easy way to handle the windows/mouse stuff, then I don't have a big problem with it. OTOH, if GLUT is the focus of the class, than something is wrong; but I doubt that this is the case.",1539106987.0
PinkyWrinkle,Think about the concepts not technology,1539106968.0
remy_porter,"GLUT is a simple, lightweight toolkit for creating rendering contexts. It was designed for educational and training environments, and it's still perfectly suitable for that.

OpenGL is still widely used. While Vulkan is positioned to supplant it as a cross-platform rendering system, Vulkan remains very new. DirectX is platform specific to Windows.

More to the point: if the purpose is to learn how to create and organize 3D graphics, the most important things to learn are how to interact with transformation matrixes, manage cameras and lighting, and understand how shaders interact with geometry and surfaces, and most important: debug a program that's running on a CPU you can't attached a debugger to (seriously, debugging GPU-based programs is really really hard).

The core concepts aren't going to change that much between Vulkan, OpenGL and DirectX. The APIs will- they can be dramatically different. But as a foundational resource, OpenGL is a fine choice. GLUT isn't really relevant (and anything you do with GLUT would be trivial to port to GLFW, so don't worry about it).",1539107027.0
phire,"Do note, you are not using GLUT in this day and age. GLUT has been 100% replaced by FreeGLUT.

FreeGLUT is source code compatible with GLUT and goes out of it's way to not add new features (just support for modern operating systems), so many people just keep using the original GLUT headers. Latest version of FreeGLUT was released in 2015, adding support for the new Wayland API on Linux.

While almost nobody uses FreeGLUT for a shipping game, it's a nice easy to use toolkit for learning. It abstracts away massive amounts of boilerplate code that you would have to write per operating system.

> I’ve heard that no one uses openGL anymore

Most AAA games use directX, but any game that gets a Linux or Mac port will have an opengl backed. Doom 2016 doesn't have DirextX, just opengl and (in later patches) vulkan.

Many indie games use opengl even on Windows and a lot of professional software (Photoshop, 3dmax, Maya, blender) use OpenGL exclusively.

Emulators and other funky software often use OpenGL as their primary backend, because OpenGL gives you features not exposed on either DirectX or Vulkan.",1539110917.0
dknyxh,No one uses opengl any more? What?,1539106808.0
cocosushi,"Well, I would repeat what other said if I said that OpenGL is still widely used and GLUT kinda deprecated. Nevertheless, if you want to have a more up-to-date replacement for glut, you can still try [GLFW](https://glfw.org/), which hasn't been mentioned here as far as I can tell. ",1539113453.0
sir_sri,"I'm teaching graphics this term, and I'm not using GLUT but that has consequences. 

In the last 10 years they've opened up the graphics pipeline quite a lot, but that means that doing basic stuff takes hundreds of lines of code where basically some magic happens and hopefully you didn't fuck it up or nothing works.   There are other libraries but most of the graphics frameworks aren't intended for teaching, and essentially distract from doing the actual graphics programming.

It's not a good situation, but there aren't many good alternatives, if the prof and TA's are familiar with GLUT you're not going to be substantially worse off than if you used something else.  

Computer graphics as a discipline combines numerical methods, high performance computing, data structures, software design, languages etc.  It's a huge field and many of the 'industry standard' APIs (like OpenGL, Vulkan, DirectX, WebGL etc.) were written primarily to bring all of those things together to let you build something as optimised as possible for whatever problem domain you have.  But in education the most important requirement is ""simple to get setup"", because spending 1/2 of the course before you can make a few objects which can move... you haven't learned anything.  There's so much more (shaders, lights, scene graphs, animation etc.) and time spent learning how to make a window or to handle mouse input or how to render text is time not spent learning graphics things.  ",1539113074.0
Steve132,"OpenGL is *absolutely* still used, and FreeGLUT is new *enough* for an OpenGL beginner.   If you really want to use something else, GLFW is a pretty good option.",1539115270.0
dtfinch,"We used GLUT in my university graphics class. They stuck to the features of OpenGL 1.0. Fixed pipeline, no shaders, no vertex arrays. But that was in 2003, in a lab full of machines from the 90's running Linux.

It was enough for learning concepts, and having a starting point to try new things.

[open.gl](https://open.gl/) has more modern tutorials, completely different from what I learned. Instead of GLUT they favor GLFW, SFML, or SDL.",1539115242.0
Mantipath,"Your teacher is being nice to you by giving you a little playground to learn the stuff that matters. Shut up and learn. 

There's plenty of time to scream at DirectX setup routines, build-time errors, project settings, and inconsistent documentation later. ",1539107459.0
Godzoozles,"First, OpenGL is still very widely used.

I'm no expert but GLUT was released 20 years ago and seems like malpractice to be using it today in light of FreeGLUT and other options, but it's kind of beside the point in some sense.

GLUT is supposed to manage the window and the interface to the OS so that you aren't responsible for handling things like keyboard input or polling the mouse, etc. It isn't the ""graphics"" part of computer graphics. So feasibly one could use it and still learn computer graphics. It's one thing to learn/use the API  (OpenGL) to draw your primitives on the screen, and it's another thing to learn the math, rendering pipeline, and techniques to get it all done. So that being said, I wouldn't worry too much about it.",1539107748.0
FuzzNugs,It’s ok if you didn’t know how popular OpenGL is.,1539136483.0
Fidodo,"University courses choose languages based on how good they are for teaching concepts, not whether they're used in industry. Most of the times that means simpler languages are used because they more clearly teach the underlying concepts without getting you bogged down with unnecessary complexity and cruft which would just distract from the lesson. Tech in industry changes so fast that it's not the goal to teach you a specific tech stack, the goal is to teach you the fundamentals so you can pick up and learn any tech stack more easily.",1539158201.0
santoso-sheep,!remindme 6 hours “opengl”,1539106443.0
Topf_Pflanze,">first week into the class.  
>acting like a smartass.  
Gj",1539129052.0
Milyardo,"OpenGL is used quite a lot, and should be used in instruction on computer graphics, GLUT however shouldn't be used in any modern development, it may be fine for the purposes of instruction.",1539107689.0
jet_heller,"I almost wish that were true. Then what you learned is the stuff behind it and not the things to type themselves. 

The best stuff I ever learned was things learned on simulators. No burdensome specifics, just the how and why of it working.",1539107719.0
bbqgorilla,"Do you go to BRAC by any chance? I am currently taking CG and they are teaching us line drawing algorithms using OpenGL, and I've found it pretty interesting to be honest.",1539113001.0
Overtime_Lurker,"If it's any consolation, so did my university. Took two semesters of graphics, and towards the end of the second semester we got an intro to shaders, switched to WebGL, which is closely related to OpenGL, and touched a bit on General Purpose GPU programming. So by the end of it all I felt like I had a pretty good knowledge base for graphics programming.

And even if GLUT specifically is outdated, as others have pointed out, OpenGL is still used, and you're still getting general graphics experience even if you don't end up using that framework specifically. Honestly, as far as CS courses go, they shouldn't be focusing on teaching one language, you can self teach a programming language easily. What you really want out of your courses are the concepts that will apply to your work later on no matter what language you're using. The language is just a means of functionalizing the concepts.",1539114883.0
PuckFepsi,Apple has deprecated OpenGL. Something to keep in mind,1539121025.0
nemesit,"Even mentioning glut tells me that whatever you will learn is severely out of date, and probably won’t even work on modern opengl versions. I’m also not up to date on modern opengl but https://www.roiatalla.com/public/arcsynthesis/index.html should be a better start",1539113793.0
umib0zu,http://mitpress.mit.edu/sites/default/files/sicp/index.html,1539111704.0
Shitty__Math,Most people generate a standard image from in the smiles and throw it at a convolutional net. There really aren't arbitrary sized input vectors for neural nets (not counting rnn). Some people a fixed number of calculated features and 'fingerprints' and go with it.,1539129730.0
SR2Z,Have you looked into vector quantization? You can change arbitrary length collections into fixed-size vectors of counts.,1539145267.0
puisseance,"How does this: http://www3.cs.stonybrook.edu/~algorith/video-lectures/

Alongside Skiena's Algorithm Design Manual sound to you?

I 'hated' the CSLR text as a self-learner as I'm trying to eat a non-ramen-based diet asap. So it's not for me. And Skiena is just such a nice guy <3

As an aside, I find a wide variety of resources essential, the single textbook approach looks great on paper but doesn't work well in practice. I also recommend geeks4geeks, the Algorithms: Explained and Animated Android app, and this online text (which ramps up very quickly, I found): https://interactivepython.org/runestone/static/pythonds/index.html",1539045160.0
tastygoods,"Tail call recursion or when the last instruction is identical to the invocation will not increase the stack, instead creating a new stack frame while popping the previous one at the same time.

https://en.m.wikipedia.org/wiki/Tail_call",1539040594.0
itsthebando,"Yes*.

*This is a very simplistic answer, but in general, recursive algorithms will use significantly more RAM than iterative algorithms. This is because every function call allocates a ""stack frame"" (think of it as a record of where the execution should go after the work of the function is done). Now, some languages have ways to prevent this through what's called Tail Call Optimization, or TCO, which allows certain recursive functions to be converted to iterative ones by the compiler, but this only works if:

* every recursive call of the function happens at the very end of the function, and
* the language and compiler support it (off the top of my head, I can't think of any widely used languages that support it by default, although I believe Ruby can with a VM flag).

Recursion is a powerful tool for *expressing* problems, but in the real world people tend to avoid it because it can be hard to reason about. For example, [NASA famously forbids recursive code](https://en.wikipedia.org/wiki/The_Power_of_10:_Rules_for_Developing_Safety-Critical_Code), period (as a consequence of them banning any loops without constant bounds, but also explicitly in rule 1, which bans complex control flow).",1539040579.0
plant_powered,"“Engineer” is a title that gets thrown around a lot these days. You needn’t even have studied engineering to be called one. However, it is a protected profession, and there is a licensing body https://www.nspe.org/resources/licensure. 

Note: I am not an engineer.",1539032369.0
,"Ask the professor? Pseudo code is just not formal, you just have to get the idea across",1539027225.0
fearsneakta,There's no standard for psuedo code. You just have to provide a layout for the program. It should look like Python.,1539027331.0
linuxlib,This seems like a major breakthrough. Is it?,1539029227.0
LuckierDodge,Literally came here to post this. Great read on a potential solution to a big question in the theory of quantum computers.,1539026717.0
IndianSpongebob,"> Writing down a description of the internal state of a computer with just a few hundred quantum bits (or “qubits”) would require a hard drive larger than the entire visible universe.

ELI5?",1539109711.0
mrbrightmind,Cue all the PHDs around the world trying to take credit for her incredible work. ,1539045031.0
deadmushrooms,"Of course. It depends a lot of the hardware architecture.

For x86, it's a bit complicated because you need to put the CPU in the right execution mode depending of the stage right after the BIOS tries to find the entry point for the boot.

Som other architectures are made to be run without an underlying OS, such as microcontrollers for embedded hardware.

Also, it depends a lot of the OS provided features you need, such as file descriptors and threads.

TL;DL: yes, languages are just machine instructions abstraction ",1539013213.0
Durahk,"Consider the fact that Linux itself is written in C (and some platform-specific assembly). It's not running on an operating system because it *is* the operating system.

All you need to run a C program is some bootstrap code to set up your hardware accordingly (this varies dramatically based on the specific hardware platform). The only part of C that requires an operating system is anything the standard library does via syscalls (memory paging, I/O calls, etc).

Edit: To clarify, almost every compiler has some way of being told that the program being compiled does NOT have a standard library. GCC for example uses `-nostdlib -nostartfiles` when linking to tell it not to use the default code that generates syscalls and other startup code. In the case you use these options, you have to provide your own implementation instead.",1539012939.0
jet_heller,"Well, since many operating systems are written in C, I would say yes. ",1539017833.0
CyAScott,"I know microcontrollers were said many times, but Arduinos are a perfect example of C running without an OS.",1539019396.0
marssaxman,"Sure. I wrote a little library to make it easier; it's a minimal freestanding C runtime for a generic 32-bit PC:

http://www.github.com/marssaxman/startc",1539024151.0
nerga,"A microcontroller is usually ran this way, which is how many machines and such are controlled. In a lot of simple processors you can set the part of memory your program loads to, so it boots and just runs from there. If you are in school take an embedded systems / microprocessor programming course and you should get some experience in this ",1539013539.0
nicksvr4,"Adruinos basically run on just C code. On a PC, you may want to start with a basic Linux kernel.",1539022842.0
chx_,"Well, no. You don't run C programs. You compile C programs to machine code and then run that.

Once you have a series of machine instructions, sure, why not? There's no magic, everything your computer executes is just that including the boot loader and the OS itself. The computer never knows you started with C, Pascal or Malbolge.

As a simpler exercise, you could rewrite say https://github.com/daniel-e/tetros in C, noone stops you, really. It truly becomes a pointless distinction since most C dialects support inline assembly...",1539021518.0
FUZxxl,"Yes, this option is explicitly mentioned in the C standard.  It's called a *freestanding environment* and quite a few special restrictions apply to such programs.  You can't just expect that any random program runs in such an environment, you have to specifically write your program for a given freestanding environment.",1539026803.0
cirosantilli,Have a look at Newlib with also allows you to use subsets of the C standard library: https://en.wikipedia.org/wiki/Newlib Here is a highly automated and documented runnable example: https://github.com/cirosantilli/linux-kernel-module-cheat/tree/8815312cad053d0284c4d91bfbf36a1e9ea207af#baremetal-setup-getting-started,1539032100.0
mapplemobs,"Yes. Considering that an OS is a program, or rather a collection of programs, just like anything else made in C you would be running directly on top of the hardware. But there are some things to take into consideration if you're going to be running this kind of environment, but to answer your question point-blank, then yes. ",1539037378.0
green_meklar,Of course. What do you think operating systems are written in?,1539044666.0
generic12345689,"With some assembly. I think boot strapping or something along those lines you can start googling from there to answer this question.

If your program has some dependency on an os obviously won’t work.",1539012850.0
jutct,Arduinos and all kinds of PIC processors and many other microcontrollers can and are programmed in C all day long.,1539024841.0
agumonkey,"Any language given you provide some native asm 'bridge'. Search for unikernels (some people are doing pure ocaml application without an OS layer, mostly a bit of OS as library)",1539033572.0
chinpokomon,"I used to have some early games from Sierra that boot directly from the floppy disk. There was a boot loader at the boot sector which would start the game, and no visible executable if you were to try and read the disk from DOS. Actually, I don't even think it was readable in DOS because I don't think it was FAT formatted.

Now, the game could have been written using any language, so I don't know that it was C, but ultimately it is just machine instructions. The operating system provides reusable low level functions which a user mode application can use, but if there is no need to use those functions, then there is no need for an application to require an operating system.

To do this in C, a lot of the responsibility would fall to the linker. Compiling for any platform, the linker sets up how the application talks to the host environment. It handles how to create an application which is structured so that it can be called by the host OS and where the entry point of the application is. Without a host OS, that responsibility would be handled by a boot loader.",1539036814.0
itsmychoiceanyway,Look into Newlib,1539044469.0
jourmungandr,yep. Here is a step by step guide on how to do it when you're writing an operating system for x86 and x86-64 using GRUB as a bootloader: [https://wiki.osdev.org/Bare\_Bones](https://wiki.osdev.org/Bare_Bones),1539015907.0
dinamech,"C++ can't run anything directly. It has to be compiled into assembly code. The assembly code are low level instructions that tell the processor what to do. It's called the executable.

The executable can be run directly on a processor without any operating system if you write it appropriately. This is sometimes called a ""bare metal"" application.  Or, you can run an executable inside an operating system. Either one will work.",1539018760.0
atk93,"If you want to learn how to do this there is an awesome GitHub tutorial on writing your own OS for a raspberry pi.
https://github.com/s-matyukevich/raspberry-pi-os",1539014824.0
rmc0d3r,"When its said that C code can run on bare hardware and thats how operating systems work, I think its good to clarify that its in fact the machine code thats compiled from the C code that is actually run. The actual task of compiling the operating system code from C to machine code happens on another computer.

Even for micro-controllers you usually have a system with an operating system and a cross-compiler tool chain on which you compile your C code into binaries compatible with your target micro-controller. Then you burn it into the micro-controller. ",1539019929.0
tachyonflux,Who is we?,1539037470.0
genr8,"Try includeOS, it supports C++ and you generate bootable images easily. It's a tiny bootloader and kernel , as others have been suggesting to build, but the hard work has already been done.
https://www.github.com/hioa-cs/IncludeOS",1539023789.0
SweetOnionTea,This is literally just an advertisement for coursera. ,1539010192.0
khedoros,"A ""class hierarchy"", I suppose? Although in Java and Python, I think a lot of that is more of a ""package hierarchy"", organizing the classes into namespaces.",1538979274.0
Kambingx,"A [singly-rooted class hierarchy](https://en.wikipedia.org/wiki/Singly_rooted_hierarchy), although I typically call it a unified class hierarchy.  In such a class hierarchy, every class is ultimately the sub-type of a single class, *e.g.*, `Object` in Java and `Any` in Scala.

Recall that if a class `B` is a subtype of a class `A`, then an instance of a `B` object can appear anywhere an `A` is expected.  In Java, we can see concretely instantiate this idea with the `Object` class (the `A` class) and the `Integer` class (the `B` class):

    Object obj = new Integer(0);
    // toString is a method of the object class overridden by Integer
    System.out.println(obj.toString());

In many cases, it is convenient if every object in a program shares a set of common operations, *e.g.*, conversion to a string for the purposes of debugging.  This is easily realized in a unified class hierarchy by factoring those common operations out to the top-most class of the hierarchy.  For Java, you can see a list of these common operations in the [Object class's documentation page](https://docs.oracle.com/javase/10/docs/api/java/lang/Object.html). 
",1539039810.0
PeksyTiger,"I'm not sure what you mean. Most Object Oriented languages have a base 'Object' class that everything inherits from, explicitly or implicitly. ",1538977873.0
milad_nazari,Do you mean object oriented programming?,1538977787.0
gct,"It's an implementation detail.  Languages like Java don't have void pointers, so it's mostly a way to have a ""this can refer to anything"" type.  Though they do stick some implementation details into Object I think.",1539024946.0
,[removed],1538977318.0
chx_,"All the data types C are hardware supported. Yes, the PDP-11 had [floating support](http://gunkies.org/wiki/FP11_floating_point) support in hardware.

Now, to work with fractions you need to implement GCD which is very slow then when adding `a/b` and `c/d` you need to calculate `a*d+b*c` and `b * d` and then run GCD over this. It's slow, slow, slow: https://locklessinc.com/articles/euclidean_alg/ arrives to a version which takes 574 cycles for a pair of 64 bit integers on average. And we haven't even started on the fact that we need three multiplications too -- and those years, not all CPUs, by far had a machine instruction for multiplication. Even in Python fractions only appeared in 2.6 ten years ago almost to the day -- even this high level, ""batteries included"" philosophy language existed happily without fractions for 17 years.
",1538959764.0
flebron,"> It will use twice the space as an int (same as a double) but have vastly more precision than the double.

It wouldn't. IEEE-754 floats and doubles have variable precision over their range, which is not the same as the precision you'd get with two ints (numerator and denominator). It's closer to what you'd get with something like a / 2^b, not a / b. This allows you to get exponentially small and large numbers, as a function of the denominator, instead of linearly small and large.",1538961434.0
joelangeway,"There are programming languages, in fact maybe most non-imperative languages, that implement fractions as a type, using arbitrary size integers for numerator and denominator. They usually get called “rational” if I recall correctly.

The reason they aren’t as useful as floats is because they can’t be made as efficient. Fractions require reducing after every operation, dividing the numerator and denominator by their greatest common denominator. That can probably be skipped sometimes, but eventually it has to happen and it isn’t trivial in any sense. 

Floating point numbers are just like fractions, it’s just that the denominator is constrained to be a power of 2.  Instead of `A/B` a float is represented like `A/(2^k)`. In that way, floating point operations can be implemented just like integer operations, just shifting before and after to make denominators match, and with rounding when shifting would otherwise require too much hardware to represent all the bits.",1538961269.0
Celebrinborn,"Hey, thanks everyone for the in depth answers to my question. I really appreciate it and I'm enjoying the discussions about the reasons behind why this stuff is designed the way it is",1538968263.0
Feynmanfan85,"I think we could recharacterize your question as one instance of a more general point: humans generally perform symbolic computations, using fractions, closed formulas, etc, and manipulate those symbols. In contrast, computers are generally hardwired to perform numerical computations, using expanded representations of those values.

We can make the distinction rigorous using the number 𝜋. The actual numerical value of 𝜋 cannot be stored in any machine (that we know of) because it has an infinite number of decimal values. However, we can store a finite program that calculates the digits of 𝜋 to whatever precision we like. So when we write the number 𝜋 on a piece of paper, what we actually mean is, this symbol represents the numerical value generated by an algorithm. Similarly, 3/4 represents the numerical value generated by an algorithm (e.g., long-division). This makes symbolic computation incredibly powerful, because it allows us to manipulate symbols that would otherwise require an infinite amount of information (and therefore an infinite amount of time) to represent and manipulate. Though we take it for granted, it's actually amazing that we've developed a system of representation that takes an infinite amount of information and compresses it to a finite symbol like 𝜋.

It is easier and more intuitive (for me at least) to perform computation symbolically, than it is to perform actual numerical computations. E.g., applying the rules of integration to a representation of a function is generally a lot easier for a human to do than it is to actually approximate the numerical value of an integral, even if the application of the rule of integration requires some ""creativity"".

""Doing algebra"" is another example of this, where humans, when trained, generally don't find it hard to simplify expressions, even in the absence of a generalized method for doing so. In contrast, teaching a computer to simplify algebraic expressions is no short order, and is one of the reasons Stephen Wolfram is probably quite rich.

So in short, I think your question actually points towards a more general question:

Why is it that computers approach computation differently than human beings, despite being developed by human beings?

My personal view is that the answer is due to our limited understanding of our own abilities. That is, the Universal Turing Machine is the most general distillation of a subset of one approach to computation. So far, no one else has figured out a better way, but that doesn't preclude the possibility that one day machines will, for example, take fractions as inputs, and more generally, solve equations the same, possibly non-mechanical way that we do.

If you're not already familiar with it, you might want to have a look at the Church-Turing Thesis:

http://mathworld.wolfram.com/Church-TuringThesis.html",1538988782.0
vytah,"There's also another issue: what if denominator or numerator overflow? For example you use int32_t as your element type, but I want to calculate (5/4)^(17), which is about 44.4, but to represent it as a fraction exactly it would be 762,939,453,125/17,179,869,184 which is a teeny tiny bit too big for what you proposed.

You could suggest some rounding schemes and what not, but they would make the resulting datatype more complex than floating point and therefore even trickier to use correctly. Or you could use a big integer datatype, which would make the whole thing slow and extremely complex to implement. It's your choice.

Therefore, since everyone has different needs when it comes to a rational number datatype, it would be unwise to impose a single implementation as the default one.",1539005228.0
toastedcrumpets,"Best summary is from /u/chx_ ""All C/C++ data types are hardware supported"".
However, there is the compile-time std::ratio, which implements rational number support which is built into the C++11 standard.

An interesting side note, something possibly more useful than a rational number library is a interval library, like boost::interval. This lets you deal with the practicalities of floating point precision truncation AND it requires hardware support to work correctly (to select the right rounding direction).",1538984960.0
Flueworks,"I guess what you're looking for are https://en.wikipedia.org/wiki/Rational_data_type

Why they aren't widely used are probably a combination of the following issues, and probably some others:

1) Rational Numbers are a subset of real numbers. So by only allowing yourself to store rational numbers, you exclude all the irrational numbers. And it turns out that most numbers on the number line is in fact irrational. Float is much better at handling those cases.
2) Most, if not all, processors today are heavily optimized for floating point arithmetic.
3) Division is the hardest operation to do on numbers. Having to do it for every single number is probably 
4) double can store from -1,79769313486232E+308 to 1,79769313486232E+308. Rational Numbers using the same bits would only be able to store from -2147483648 to 2147483647",1538992453.0
combinatorylogic,"Don't use C-based languages then, if you need rational numbers. There is a lot of languages that support full numeric tower natively.",1538995008.0
iamasuitama,"Even though the answers have been plentiful, let me try to gently point out a few errors in your post text:

> It will use twice the space as an int (same as a double) but have vastly more precision than the double.

Could also be just the size of an int, depending on the required precision. Just use half the bits for num and the other half for den, right? You have to think about the negative numbers too btw. If you simplify, will `-2/-2 == 2/2`? Next, when you talk about precision, I think you mean something like 'exactness', ie 2/3 is really and exactly 2/3. But this gets lost when you need to calculate things with 𝜋. Maybe with 𝜋, your assumption of better precision falls flat and a double can get closer to it than an advanced version of 22/7 (it's been proven to be endless in decimal or something anyway right?)

> math on the fraction will need to be done twice instead of once (performed on both the numerator and the denominator)

Where does this assumption come from? Even just with addition and subtraction, you need to do a *lot* more than twice the operation. eg 2/3 + 3/4, first make them both of the same kind, so make it into 8/12 + 9/12, add up, 17/12. Then maybe simplify after again. Keep in mind that ""making the two operands of the same kind"" is a cool *four* multiplications. When you multiply two fractions, sure it's easy enough, but think about the edge cases where you would want to simplify after lest the fractions become unnecessarily complicated and long with few operations.

Thanks for an interesting post and stimulating an interesting discussion!",1539003169.0
yawkat,"Fractions are surprisingly useless. In scenarios where you need accuracy for decimals, there's better alternatives like fixed point (for finance). For applications where you need accuracy for non-decimals (maths) you usually need more capability than just rationals.

On top of that is that rationals are annoying to work with - they're slow, add annoying problems like number equality, aren't really efficient, have odd range constraints that lead to many of the same problems floats have and so on.",1538987328.0
jourmungandr,"So, every datatype to hold numbers will have holes in it. The most interesting argument I know has to do with the size (cardinality) of the sets of numbers. The cardinality of natural numbers is the smallest infinity called aleph-null (or aleph-naught), the size of the set of real numbers is infinitely bigger. That is if you pick any two real numbers, no matter how unimaginably close you construct them as, there are more real numbers between those two points than there are integers. The funny thing is that the reason Alan Turing developed the Turing machine to prove the halting problem is because he wanted to use a trick he picked up a trick from Kurt Gödel. That is he put the deterministic algorithms (all of them) in correspondence with the integers. That's to say that there are ""only"" as many algorithms as there are integers. So there are infinitely more real numbers than integers and therefore algorithms and almost all real numbers cannot be calculated by any algorithm even given infinite space and time (infinity is weird).

So you cannot even theoretically represent all numbers and so have to pick your battles. The type you're asking about is called a Rational, it exists but it just moves the problem around a bit. 1/3 can be represented exactly but 1/3 + (some very small number) may not be. There are a few very big strengths of IEEE-754 floating point numbers, they are very carefully designed to work very well. They can represent a huge range (double ±2.23×10^(−308) to ±1.80×10^(308)). Second they are very efficient every possible pattern of 64 bits represents a different number, in the Rational type there are lots of pairs of numerators and denominators that represent the same number because they share factors and are therefore not in lowest terms. Comparing Rationals is kinda expensive, you have to reduce to lowest terms either with every single  operation or before comparison.  You can also show that a binary floating point number has the least representation error when used in general arithmetic.

Also the Rationals that we think we ""should"" be able to represent aren't that special. There's not real reason to prefer representing 1/3 exactly and not 1/3 + a tiny number, other than we humans seem to like fractions representable by fractions with small integers in them. Mathematically/physically it would be far more useful to be able to represent say pi or e exactly rather than 1/3. But both those numbers are transcendental so you can't (without infinite time or space anyway any  terminating representation is an approximation).",1538964422.0
NeinJuanJuan,I don't know the real answer. But I guess it's because you can implement a 'fraction' type with the basic variable types yourself.,1538959250.0
smortaz,Check out fixed point math which some languages support. ,1538979500.0
xyproto,"I also like the idea of fractions as the fundamental number type. For long expressions, perhaps some optimizations would be more accessible than for regular floating point numbers.

Here's a Go module I created for using fractions: https://github.com/xyproto/gofractions",1539121766.0
petered92,Here for answers ,1538960345.0
xlhhnx,"I'd rather see Comp Sci/Software Eng on a resume than Info Tech since the former are explicitly geared towards the software development concepts that you will be applying.

Language skills aren't too important imo, once you learn a couple, learning new languages should be straight forward. It's much more important to have a solid foundation in algorithms, data structures, and standard development practices.

None of this is to say that you can't get a programming job with an Info Tech degree, just that I think you'll be in a better position with a Comp Sci/Software Eng degree.

Take it all with a grain of grain salt, do your own research, ask professionals what they or their company would prefer.

Edit: And don't make your decision based solely on Reddit responses ;)",1538959194.0
luxfire,"If you have the skills and desire to program, CS. All day.

The reputation of IT majors is that they are the CS majors that couldn’t cut it. Their resumes go to the bottom of the stack at a FANG.",1538978807.0
LewisJin,What difference with resnet,1538957442.0
beeskness420,"You might have better luck searching things like asynchronous distributed parallel processing. You might be interested in synchronous parallel processing too, but that can often get rid of one concurrency issues. Best of luck! ",1538930554.0
rudedogg,"Was just searching this, and found this resource: https://www.cs.cmu.edu/~scandal/nesl/algorithms.html",1538946952.0
RichardMau5,[Here you go: “Structured Parallel Programming Patterns for Efficient Computation”](http://digilib.stmik-banjarbaru.ac.id/data.bc/18.%20Programming/2012%20Structured%20Parallel%20Programming%20Patterns%20for%20Efficient%20Computation.pdf) ,1538908308.0
Resquid,"Nope, only bad ones.",1538887463.0
Wulfnodh,Microsoft’s Power BI is great for visualizing data in the context of mining. It also works across most non-Microsoft RDMS and doesn’t require star schema database design.,1538887957.0
Noircir,"IBM SPSS is pretty decent for data mining, predictive analytics and statistical analysis overall. More specifically the modeler, I used it in some of my undergrad classes focusing on data analysis and data mining. The software is extremely pricey though.",1538930672.0
khedoros,"How far are you into your degree? Are your grades slumping because your work is too difficult, even if it's enjoyable?

In school, my grades dropped like a rock because I had a couple solid years of deep depression. The CS work really clicked with me though, even though my grades didn't always reflect it.

Finding my first job wasn't *too* nerve-wracking, because I knew I still had a place at my parents' house. When I started, there was a strong ""imposter syndrome"", and some anxiety that I what I had learned wasn't ""real"", and that it wouldn't be good enough to keep me employed. But I shoved that to the back of my mind, and it disappeared over a couple of years of successful work.

So now, 10 years later, I'm laid off from my first employer, basically to improve some numbers on a balance sheet. Couple months of pulling myself together, and then close to 2 months of job hunt so far. I've got about 3 good prospects, and I seem to be doing better at one of them than I assumed (2 remote interviews so far, both of which I thought I'd bombed, but they're calling me in for an on-site!) Anyhow, it's in an industry that I haven't worked in, and that's considered fairly difficult to get your foot in the door. I've got a wife, child, and a mortgage. I know I can program, and work in a software development position, because I've done it for a decade. But there's a lot riding on this, the depression's been making a reappearance, and this time depression brought along its good friend: anxiety.

So, I guess all I can say is: Do the work, study for good grades, tell the naysaying in your mind that it can go fuck itself, and prove it's wrong by succeeding. I couldn't have imagined doing anything but CS in college either, even though it sometimes meant all-nighters to figure things out (recursion was a real bitch; I remember pacing in the study room like a crazy person at 3AM muttering under my breath, then running to my notebook to diagram something out).

If you actually *can't* program, you need to figure out how to fix that, whether it's practice on your own, tutoring, or what. If it's just anxiety holding you back, that's normal to a certain extent, and it's just something you learn to handle.",1538880005.0
PM_ME_UR_OBSIDIAN,"If I understand correctly, you're looking for a project you can do in a self-directed manner, using only resources accessible outside of a university context - so no graders, for example. On the other hand you're presumably outside of school at the moment, so you have plenty of time and energy to dedicate to your learning project. You're looking for something that will help you with internship applications.

I suggest doing the first volume of the *Software Foundations* workbook. It's something I did during some downtime after my senior year, and for about two weeks of work it had a huge impact on my first year in the workplace.

In my experience, the main determinant of how well you'll do in the technical part of the interview is how comfortable you are solving coding problems that require non-trivial control flow. Directed practice is the name of the game. Something like Hackerrank can be helpful as well.",1538867538.0
hamtaroismyhomie,"Very few companies interview in November, since the interview process is interrupted by the holidays.

Many companies will still have applications open in February, but most of the big players who pay for housing will have their internships filled.",1538883062.0
ApertureCombine,"If Siraj just slowed way down, stopped making clickbait-y titles like ""machine learning in 3 minutes!"" and made series' instead of videos in random order he'd be a great resource. He's obviously intelligent and knowledgeable, but I can't stand his teaching style.",1538931809.0
pulsar512b,"Carykh on youtube is also quite good, as well as Code Bullet. They are perhaps a bit different, but good.",1538870793.0
bartturner,"My favorite is

https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A",1538910234.0
Hexorg,"It's definitely a cool idea! I'm more of a signal processing guy, but wouldn't optimization of st.deviation at the end result in essentially finding of high-frequency data? Your images ([especially this one](https://www.researchgate.net/profile/Charles_Davi/project/Information-Theory-16/attachment/5bb2700b3843b006753c2179/AS:673734797049861@1537642301723/image/processed_HW.jpg)) looks like low-res line finding algorithm output (which is peaks of high frequency).

I wonder if instead you can use an actual line finding algorithm and generate a tile-set of square regions of different size (maximize size of regions with low information)? I think this may allow your continuous feature extraction section to have a pixel-perfect precision. ",1538857906.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/computervision] [Contiguous Features Extraction Using Information Theory](https://www.reddit.com/r/computervision/comments/9lypow/contiguous_features_extraction_using_information/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1538854044.0
Taleuntum,"https://en.wikipedia.org/wiki/Big_O_notation Look up the definition of theta under 'related asymptotic notation'. It should be clear that k1=(1/2)^(k+1) and k2=1 and a good  n0 is e.g. 10, a smaller n0 would be good too. 

Note: your proof of the lower bound is not completely correct as n/2 is not always a whole number, and there are more than n/2 numbers between n/2 and n inclusive even if n is even, but these of course are not that important.",1538853253.0
aviihej,"Basically,

f(n) is Big O of g(n) when f(n) <= c*g(n) for all c>0.

f(n) is Big Omega of g(n) when f(n) >= c*g(n) for all c>0

When f(n) is Big O of g(n) AND  f(n) is Big Omega of g(n)

Then f(n) is by definition  Big Theta of g(n)

In normal words this means Big O is the worst case, Big Omega is the Average case, and Big Theta is the average case. 

Hope that helped ",1538865788.0
PM_ME_UR_OBSIDIAN,All of my wat,1538866995.0
Taleuntum,"https://en.m.wikipedia.org/wiki/Strongly_connected_component  in the picture under the definitions part, the middle edge in GSCC is not a bridge. So not always. If you are talking about strong bridges then the edge between scc s is never a bridge.

Note: Every DAG can be the GSCC of some digraph and if you take the edges of a DAG as bidirectional edge you can get any undirected graph. So if it were true that every edge in a GSCC is a bridge that would mean that every edge in any undirected graph is a bridge which is false, there are edges in an undirected graph that are not bridges.
",1538846252.0
p_pistol,"Assuming SCC is strongly connected components, and GSCC is the set of strongly connected components of G, yes. If the SCC is a single vertex, this is not a cut vertex, as it is a pendant vertex in the underlying graph. And yes, a cut edge's endpoints are cut vertices unless it is a pendant edge.",1538840839.0
khedoros,"I kind of stumbled through assembly in my college courses that involved it. I tended to write the algorithm I needed in C, then hand-compile it to assembly (just imagining what I needed to do in each line of C, etc). You can do something like assign registers to represent different variables, and work from there.

Where I really picked up the ability to read assembly though was loading up IDA Pro, opening a binary on my machine, and trying to figure out what it was doing. Lots of side research involved, but it's a really educational experience.",1538776068.0
BiggRanger,"Try playing around with this: https://godbolt.org/ it's an online compiler explorer.  
Enter some C code into the editor and select the compile for MIPS, the ASM updates as you change the code.",1538792570.0
MayorOfBubbleTown,"http://courses.missouristate.edu/KenVollmar
MARS (MIPS Assembler and Runtime Simulator). Full IDE with assembler, simulator, and debugger that runs on JVM. ",1538811262.0
LearnerPermit,"When I took assembly, my junior college required 8086 (yes that long ago) because it was it was a JC, the labs weren't really difficult. When I transferred to a university, MIPS was used in computer architecture & digital design upper division classes. Back then we used SPIM as the assembler, but again most of the labs are the kind you can just stumble through. 

I also realized I had no desire to do anything that low level. Unless you're doing firmware or some really low level hardware stuff, I suspect professionally you'll never use it. 

CS Tutorial center and being the professor's best friend at office hours helped me a lot. ",1538802597.0
Nielskarsten,Know its a bit late but a friend of mine send me this game; [http://nandgame.com/](http://nandgame.com/). Which gives you a pretty good idea of how assembly/mips works at a base level. Wish i had it when i learned assembly last year :D ,1539172303.0
mcintegrate,"Here are a few good playlists. It’s not “pure” assembly but it’ll give you good idea.

https://www.youtube.com/playlist?list=PLRwVmtr-pp05c1HTBj1no6Fl6C6mlxYDG

https://www.youtube.com/playlist?list=PL0C5C980A28FEE68D

There are a few good assemblers out there, GNU has one built in, NASM, MASM etc... each one has a slightly different syntax but logic is the same. Assembly is my favorite language. Powerful but hard to tame. ",1538781148.0
genr8,Yes,1538787830.0
zergling_Lester,"There are two paths in front of you.

One is posting on /r/bitcoin or /r/cryptocurrency but adjusting your question to be like ""Any good sources on industry 4.0/smart industry and why it's based on blockchain?""

Another path is going to /r/slatestarcodex and posting your question titled as ""Any good sources on how decentralised decisions being made by computers instead of humans are going to cause untold misery/heavenly bliss?""

You can take both because we are living in the Dreamtime.",1538779889.0
CorrSurfer,"Well, that's a very broad topic. What exactly are you interested in?:

- Articles that define what ""industry 4.0"" means?
- Articles about its impact in specific industries?
- Articles about the technical challenges (in specific industries)?
- Articles about public funding?
- Articles that tackle concrete research questions?
- ...

Also, I think that summary ""decentralised decisions being made by computers instead of humans"" is not at the core of Industry 4.0, at least how the research funding bodys in Europe see this, which is more about having the machines on a factory floor speak a common language, so that they can communicate, share sensors, simplify predictive maintenance, etc.",1538744876.0
whymauri,"[Interesting, I think I've seen this before.](https://www.reddit.com/r/compsci/comments/9linm4/going_through_introduction_to_theory_of/)",1538712217.0
obp5599,"Reminds of when i learned what a recursive backronym was *cough* WINE *cough* 


I had a line in one of my early cs books that said, “why do computer scientists think trees grow upside down? Its because they never go outside”",1538709264.0
Cocomorph,"#Glossary  
---
. . .  
**Recursion**: see *Recursion*; see also *Tail Recursion*.  
. . .  
**Tail Recursion**: see *Tail Recursion*.  
. . .",1538712599.0
assassinsmead,Oh man I remember this book. My professor used this one because it was offered for free from the author.,1538712548.0
wookie-bowcaster,Huh? I don’t get it I’m Starting compSci next year. Should I be worried? ,1538779078.0
SteeleDynamics,Yep. Classy.,1538753316.0
agumonkey,dogfooding is good,1538770129.0
Zophike1,Can someone give me an ELIU(Explain Like I'm an Undergraduate) on what's going on here and why it's so funny ?,1538793122.0
Gnockhia,"Anyone read XTUML by Julian Miller iirc

The ""Elaboration is stupid"" ant.

I once saw the mug on thinkgeek but can't find the little ant on Google anymore :(",1538732767.0
AbdominalPainPerson,How do people that are professional software engineers code all day long without feeling like their mind is mush by the end of the day? ,1538703513.0
Marsuv1us,"This is going to sound so fake but I promise you it’s true. My computer science teacher was fired recently for showing porn in class. He had a tab open and misclicked while presenting a slideshow. Basically we’ve had a sub and he never had a syllabus or anything so it’s a blow off period. It’s a Java class, and I’m pretty decent in it but now I’m looking for ways to keep practicing it so I can get better and pass my AP test. Is there any place I can look that will give me “assignments” such as , “Create a program that can solve systems of equations”? I just need something to do in that class and I find programming fun. But since he’s not my teacher anymore, we aren’t getting assignments.",1538708553.0
spursup20,"I’m a junior CE major and I’ve been rejected a lot when applying for internships. I feel like I more than meet requirements for most and even have a previous internship, but bigger companies seem to automatically write me off. What can I be doing to get noticed by larger companies. I want to work somewhere that will help me learn in a fast pace environment and where they’re doing something innovative. My last internship was mundane projects not really forcing me to work hard.",1538712210.0
averma7,What are some good podcasts to listen to for Frontend/ Web dev?,1538702050.0
Jack-RGM,"I'm a computer science undergraduate student looking for good note taking applications and/or techniques. I wish to have an application and/or method that can cover all the following criteria:

* Multiple levels of categorisation and good organisation features (i.e. ""Computer Science/Hardware and Software/Operating Systems/Unix/MyNew Page"")
* Open-source and platform agnostic (i.e. Text files organised in folders can be stored online and accessed anywhere, including my phone and computer, but this method lacks features like images and tables)
* Synchronisation compatible (whether it's a single application that syncs across phones and computers or a method can be used with cloud services like Dropbox and OneDrive)
* Somewhat straightforward to use and current (does not involve overcomplicated configurations, isn't half-built by developers with no forseeable updates, and most importantly can be used on-the-fly like quickly jotting down something from a conversation)

I understand it's a pretty picky list, but if there's something available that meets these needs I'd love to know. Currently I'm using Microsoft's OneNote as, while it's not open-source and tied to a company as a product, still fulfilles most of my needs as it touches on most of the pointers above (but not all; lacking Linux support and having inconsistent features depending on platform).",1538853807.0
wakka54,"What does the landscape of CS look like, where 4 year Berkeley CS grads and 12 week Coding Bootcamp grads both land high paying jobs and both call themselves programmer/SWE? What's the difference? I see bootcamp syllabi teach web stuff like Flask - so is a bootcamp grad the modern day equivalent of what 20 years ago would have been called web developer, i.e. dreamweaver user who knows their way around javascript and PHP and SQL? Or is computer science just merging with web development entirely? A friend recommended I do a bootcamp but I don't get why they're teaching website frameworks - why would I want to be web developer? What's that have to do with coding? I get they both use syntax languages but Flask seems like it's mostly repeating tutorials to make generic web apps that have been made a billion times before - and coding algorithms are an afterthought you can duct tape on with little javascript snippets. It seems like a lie to call a web developer a software engineer. Or am I missing something? I'm thoroughly confused.",1539275686.0
nuts69,"I have a question.   I have a TS/SCI security clearance (spare me the opsec brief, trolls) and I was wondering what is a good job market for this specific resume item?   I'm in Denver at the moment and it seems pretty good, although I feel underpaid at 85k.  I'm seeing a lot of good jobs in the DC area, but I was wondering if there's any other hot spots I'm not seeing on job sites. I don't much enjoy the east coast's humid weather and culture. 

My skills are strong in microcontroller programming and back-end development, and I have experience in program management. ",1538705463.0
YEAHILIKEFATCHICKS,"I’m currently working on my B.S. in compsci and I’m planning on doing a 5th year for my MBA, probably with a focus in management information systems. People ask me what I want to do with that and I really don’t know. I was thinking back-end dev but to be honest I’m more into the theory and concepts than actual coding. Just curious if there’s anyone with similar feelings/education/experience that could shed some light as far as career paths go",1538710251.0
Samrockswin,"You might also look up posit arithmetic, introduced by Gustafson and Yonemoto",1538680927.0
gct,Are you thinking of [unums](https://en.wikipedia.org/wiki/Unum_(number_format\))?,1538709652.0
olliej,"It depends what your goal is (the link you have looks like its actually a kind of customizable hardware/hdl floating point implementation?)

&#x200B;

As others have mentioned there are unum/posit systems, but generally they have accuracy issues (they over estimate bounds), and performance issues (as it's more difficult to make efficient hardware pipelines without essentially widening to the maximum precision for the exponent and mantissa).

&#x200B;

The other option you may be thinking of is variable precision floating point, which is basically software implementation of floating point that is effectively using arbitrary precision integers to represent the exponent and mantissa. The down side of these is that you still only represent rational values, so any algorithms you do implement have to be careful to ensure they don't spin for eternity trying to produce a value that actually requires infinite precision. Because of that limitation you have subsequently ensure you don't overstate the precision of any results that then depend on those values (e.g if you compute Pi to 10000 bits, and then do (1 million bits of precision)/Pi your result looks like it has millions of bits of precision, but it's still bound by Pi, and the error around multiples of Pi goes up very quickly (x87 actually has a bug where it does just this for some of the transcendentals).

&#x200B;

Some implementations attempt to resolve this simply by storing fractional values as two separate values and coalescing. I have no idea how the precision ends up working for those though :D",1538761447.0
Andy_Reds,"When people say they're going into a different area for their master's, they usually mean switching from Math to CS, Physics to EE or something like that. There is absolutely no way you'll be able to handle a Master's degree in CS with a bachelor's in music education. If you really are interested in the field, I would suggest doing a bachelor's in CS.",1538680778.0
TKAAZ,If you know nothing about computers and math I highly suggest you do not pursue a master's in CS.,1538673484.0
,[deleted],1538674367.0
,[deleted],1538688117.0
bradcroteau,"The chair of my science department in undergrad also taught most of my CS classes. His undergrad was music and MSc and PhD were computer science. Can confirm, it’s a thing.

My BSc was Computer Science, my MSc was Space Studies with an aerospace science concentration",1538676658.0
tjscollins,"You'll have to take around 30 units of undergrad CS classes before any MS program will admit you, but yes you can do it without a BSCS. ",1538690768.0
futzbuckle,"Look at all the homework, pop quizzes, lecture notes, reading material, etc. that your professor has provided. Professors like to reuse content to punish people who neglected it. That will help more than anything you can find external to the class.",1538668457.0
mrexodia,"If you want some useful experience: implement all the things and try to write tests that exercise as much of the code as possible to understand the edge cases.

If you want to just pass and be one of those people: generally you are not asked to implement something, but rather to find the right data structure for the problem. Probably you were provided with exercises, they should be somewhat representative for the questions you can expect. Do them without looking at solutions first. It sounds like an introductory data structures/algorithms class so it could just be that it’s not very difficult for you.",1538672026.0
no_detection,What was the focus of the course? Implementation details? Choosing the proper algorithm? Proofs of lower and upper bounds?,1538681724.0
ButtersCG,"Understand the code! Learn it side-by-side with the theory of it.

Look at the code and try to understand what happens. If you don’t know what that line or segment is used for, read through the theory to find the answer. 

That’s how I learned it and it simplified matters a lot, since all of the code is connected with the theory.

Of course, that’s assuming your professor explained the concepts with code, which is how it should be imo.

Good luck!",1538687031.0
DenaliAK,"http://www.cs.princeton.edu/courses/archive/fall11/cos226/exams.html

https://courses.cs.washington.edu/courses/cse373/15su/exams.html",1538704958.0
chewyfranks,Easiest for me was to make flash cards and cycle through the algorithms. My courses are all paper we get no computers for any exams and are expected to hand write code and output of specific code. But definitely for studying was to write it in my own words and flash cards then of course practicing implementation by remembering pseudocode. Good luck!,1538798843.0
jpynchon,"Look up any exercises from previous years that you can find, so you can get a taste for what the professor tends to accentuate. The book Introduction To Algorithms by Cormen et al. has great exercises and examples in it, I found. Mostly: really think about the problems, write them out, get a feel. 

Memorize (but also understand why) the space and time complexity of algorithms that will be building blocks for solutions. These include:
- lists (all kinds)
- DFS / BFS
- Flow algorithms (if that is included)

Also for dynamic programming, I found it useful to think about it in an inductive way (especially for the proofs).",1539638063.0
simondvt,Do some challenges on geeks4geeks and hackerrank. ,1538666739.0
nealeyoung,Almost all algorithms texts discuss big-O analysis.  Try e.g. Chapter 0 of Dasgupta et al (e.g. [http://algorithmics.lsi.upc.edu/docs/Dasgupta-Papadimitriou-Vazirani.pdf](http://algorithmics.lsi.upc.edu/docs/Dasgupta-Papadimitriou-Vazirani.pdf)),1538697040.0
charlesbronkowskiIII,You need cache to buy cookies to eat.,1538620631.0
WhackAMoleE,"Two completely different levels of discourse. Cookies are used by web browsers to store user context (ie, who you are and what you're doing). Cache is a general term for any intermediate level of storage where you keep some data close to where it may be needed. In particular, disc caching is when the operating system allocates some memory to retain a copy of data that came from disk. That way if the program needs the same data, it's already in memory (fast) and not way out on the disk (slow).

So these are two concepts that are operating at completely different levels. It's like your pancreas and your automobile. Two different areas of discussion.
",1538620312.0
,[deleted],1538619162.0
zrathustra,"You don't even need a Turing machine for this, you can have a DFA that is in the start state while reading the input. When you encounter the symbol ""b"", move state to b_0. If the next symbol is ""b"", move to step b_1. Otherwise, move back to the start state. If in state b_1 and you see ""b"", then move to the accepting state. Otherwise move to the start state.",1538619340.0
Overtime_Lurker,"Are you asking about the logic of it, like how many states and what each state should represent? Or how to implement it in a language?

For the logic you would have 4 states, let's just call them 0, 1, 2, 3. 0 is the start state and represents no b's. 1 is 1 b, 2 is 2 b's in a row, and 3 is 3 b's in a row. Now you need the transitions to fill out the logic, which would be:

* State 0: 'b' sends machine to state 1, anything else just loops back to 0

* State 1: 'b' sends machine to state 2, anything else just loops back to 0

* State 2: 'b' sends machine to state 3, anything else just loops back to 0

* State 3 (final state): anything loops back to state 3, essentially the machine is stuck here now that the condition is met. No matter what comes in now, the string will be accepted.

Something interesting to note would be the sort of ""negation"" of this machine, strings not containing ""bbb"". To do that, you just reverse the final states: if the machine reaches state 3, the string contains ""bbb"" and the machine will be stuck in a non-final state. Hopefully this helps if your confusion was about the logic of the states and transitions.",1538619020.0
CozyCargo,"If you want to try out your solution, you can find a turing machine simulator here: http://morphett.info/turing/turing.html

I found it useful when starting out in this area",1538685603.0
ballerrrrrr98,Appreciate the help guys!,1538620194.0
,[deleted],1538617524.0
petered92,I am just upset that I have been applying for a week and still have not heard anything back.,1538666735.0
halfeatenscone,"The links in this article are egregious blogspam. e.g. [this solution to ""How do you find the largest and smallest number in an unsorted integer array?""](http://www.java67.com/2014/02/how-to-find-largest-and-smallest-number-array-in-java.html). Not only is the page plastered with obnoxious ads, the solution is incorrect!",1538700760.0
MCPtz,Good thread. Thanks for the link,1538616775.0
anowlcalledjosh,"> The only things that I have done in the past are making basic programs and completing some problems on Project Euler.

Remember: there are people applying for your course that have never programmed before. It's very likely that some of those people will make it onto the course; there's no reason why someone like you can't make it too.

Also, bear in mind that the universities I'm assuming you're applying to will care much more about your mathematical ability than the fact that you haven't done much computer science.

If you don't get many replies here, you could try /r/UniUK or /r/6thform too. Good luck!",1538609863.0
ahfoo,"I worked in university admissions for years so here are some of the basic tips:

First and foremost, be specific by using real examples. What I mean by ""real"" examples is that they have names, dates and places. Don't offer generalizations. Stay focused on concrete examples of what you're trying to communicate by pinning it down with names. 

Second, indicate with named examples why the institution you are applying to is the only school that can match your requirements. This is where many people go wrong because they want to send one application to several schools so they get lazy and try to avoid tying it down to one specific school. Just look up the faculty and see what they're up to and even try to contact someone on the faculty briefly and then stick their full name, department and title in there. This second tip is really just an expansion of the first rule but it deserves a bit more explanation because it's not obvious to most applicants. 

I think if you read the school's instructions for the personal statement you'll most likely see these tips right there in the instructions but so often people just ignore the instructions and dig in thinking they know best. The most common fatal error is keeping things vague in order to blanket several schools. It's not a lottery. By focusing on details you will make your statement stand out. 

EDIT: Adding a third tip you won't see in the instructions. I'll just stick the additional part in here and put it in quotes.

""As an afterthought I remembered the other big tip I used to have to drag out over and over which relates to the style of an admissions letter: the person who will read this is not your friend. That should be obvious but it's hard for novice writers to get a sense of how they are coming across because they're not used to thinking in terms of audience reception. Let me give you a classic example of how this can be a subtle point. I'd often see admissions materials where applicants had written a list of items that end with ""etc"". Don't do that! You can use such devices as ""etc"" in certain contexts but an admissions letter is not one of them. The problem is that it assumes the reader is sympathetic to the writer and knows what they mean and will give them the benefit of the doubt and fill in the blanks on their own. I'll tell you what --that's not how it goes. You are not writing to a friend in this case, you can assume nothing except that they are looking for any excuse to put your application in the reject pile."" 

It's not what you've done that matters as much as how clearly you can put what you've done into concrete terms and make the case that it is relevant to one specific school. So in other words, it's less about how impressive your project portfolio is than it is about your ability to present yourself as being organized and specific. Also, clarity is far more important than flowery technical jargon. Ambitious non-native speakers tend to hang themselves by grasping too hard for fancy vocabulary but native speakers can easily fall into the same trap. Write at the level you are comfortable with and emphasize clarity over buzzwords. 

I realize you were asking for project ideas rather than tips on how to write the letter but the message I'm trying to convey is that the nature of the letter itself could be more important than the impressiveness of the projects that it mentions. Mediocre accomplishments can still make a great letter but a great project that isn't presented according to their expectations isn't going to do much. ",1538621229.0
Barrucadu,What country are you applying to universities in?,1538608728.0
krykanello,"I'm guessing you're talking about Oxford or Cambridge? For your interview, you're not expected to be an expert of any sort. Since you've done a bit of coding already I'd actually suggest grabbing a book and trying to get comfortable with basic data structures and algorithms (and related maths, especially combinatorics). Really internalize these things, and then you'd both be in a better position to learn about and complete more complex projects, and you'd show that you're serious and already quite competent with studying computer science.   ",1538614195.0
TezlaKoil,"> A lot of universities say that no prior Computer Science knowledge is necessary, but I have seen that those who express some sort of prior knowledge through their personal statement have a higher chance of receiving an offer

I've worked at three Russell Group universities: none would force faculty members to read the personal statements of undergraduate applicants. Unless you're applying to some Oxbridge college, you should write like your application is only going to be read by an administrator with no particular background or understanding of CS.

So ""I wrote a Forth interpreter in 8088 assembly"" bad, ""I built a portable speaker from scratch"" good, even though the latter has very little to do with CS.",1538656293.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1538583251.0
danketiquette,Sounds to me like you wasted 4 years of schooling,1538574084.0
mulletlaw,This is kind of a broad question.  After 4 years what sort of things did you learn?  What caught your attention?  There must have been something you came across that you want to explore or develop.,1538577520.0
DontBeSoGrumpy,Steganography. Make a tool that can embed or extract a hidden message in an image. Good luck!,1538577198.0
4144414D,"Take a look here https://www.forensicfocus.com/project-ideas

",1538596253.0
tomdon88,"You could build a computer that is impossible to break into, like pad locks the works.",1538579744.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1538546763.0
mgarde,Learning to play the ukulele. ,1538542134.0
Semi_Chenga,Electronic music production. Really fun crossover of skills between that and my usual programming. ,1538575822.0
Yourothercat,Learned nollie 3 flips on Friday,1538545272.0
onfire9123,Swing dancing. Now I can get women to interact with me without having to spaz out in my head trying to say the right things.,1538593836.0
ice109,"Haskell. I highly recommend the book ""get programming with Haskell"". It's much easier than people make it out to be.",1538599614.0
RosieRevereEngineer,Learning to parent. ,1538551468.0
Ebanflo,Persistence landscapes in topological data analysis. Basically they allow you to do fast statistical analysis on bar code diagrams - which represent the topology of data sets across multiple scales.,1538537686.0
Tr3ytyn,"Quantum mostly. 

Most recently; quantum inertia (QI) — it’s pretty interesting. Was most commonly referred to as pseudoscience and now a military science investor named DARPA just invested 1.3 million into QI. 

From what I’ve read, the theory is breaking physics completely which in turn is why major physicists are calling it pseudoscience. They’re going to try and create a machine that will allow us to launch into orbit without ‘fuel’ using this Quantum Inertia stuff. 

As a side note; QI in Chinese means - the circulating life force whose existence and properties are the basis of much Chinese philosophy and medicine.

Fun stuff ;)",1538545700.0
dazzford,Front flips on skis.,1538591177.0
,[deleted],1540497982.0
xAnonymustyx,Learning to blow shit up with my mind. ,1538584665.0
TKAAZ,I’m sorry but what does this have to do with computer science?,1538548835.0
mrkaikev,Why did you use Python 2.7? For asynchronous downloads you can also look into [aiohttp](https://github.com/aio-libs/aiohttp).,1538556575.0
affinitive2,The code is open-source!,1538534156.0
cheese_wizard,"This... is a goofy question.  Just say that if you are going to implement any of these in hardware, then binary is best because it costs the least and is easy to make in hardware (e.g. existing telephone relays or transistors can be simply used for logic since it can either be ON or OFF).  Binary math also happens to have many nice mathematical properties that are easy for people to understand and show benefits over other schemes, like ternary or analog computers.

Be sure to point out the confusion in the question... that they are all 'machines' but all three are used in different senses of the word.  Von Neumann architecture represent a machine that goes in the fetch-decode-execute cycle commonly found in practical hardware that we use everyday.  State machines are abstractions and are also know as finite state automaton. Lastly, Turing machines are a generalization of a computational machines more or less broken down to first principles, but are not really meant to be realized physically but to reveal principles of computation.",1538545837.0
dman24752,"As I see it, the simplest way to conceptualize a Von Neumann architecture is that it kept data and instructions in the same storage of memory. Other architectures separated the two.

As far as binary goes, you could theoretically make a Von Neumann architecture that uses a different kind of number system. Binary versus anything else doesn't matter. Binary was chosen, as others have mentioned too, that it's just easier to implement. For a bit, you only have to worry about two states (with voltage going through and without) as opposed to a different system where you'd have to track more and have a higher likelihood of error.",1538594818.0
sanyides,Computational history class? That sounds pretty cool. My university doesn't have such class...,1538577276.0
uh_no_,"i think ""good"" isn't really true. von neumann is a type of architecture, and one can be designed in any base.

That said, CMOS is well suited to binary logic, and CMOS is also well suited for von neumann architectures....which is why we have what we do.",1538575154.0
billccn,"I think the question is just poorly worded (or maybe intentionally misleading).

These days ""Von Neumann architecture"" is basically a synonym for stored programming; however, the original [Report](https://en.wikipedia.org/wiki/First_Draft_of_a_Report_on_the_EDVAC) described a very specific architecture. It was perhaps the first time binary was used for number representation in a significant computing project. (Consider that EDVAC's predecessor ENIAC is a base-10/decimal computer.) The benefits of binary was so obvious that the world never looked back.

In other words, there's nothing preventing binary from being used in non-Von Neumann architectures, it's just that where it first appeared.

(Admittedly, depending on how you define Turing Machine and State Machine, they may require more ""symbols"" than the two binary can offer. However, this is be a useless theoretical and literal interpretation.)",1538600280.0
cthulu0,That’s basically what feedforward neural network fior regression tasks  do when trained by supervised learning. They are fitting a high dimensional function to a set of labeled points. So yes I have used curve tracing in the context of improving the performance of a delta sigma modulator .,1538533974.0
_ACompulsiveLiar_,Literally all the time in finance/algos/hft/quant/etc,1538534431.0
RomSteady,I use a variation on this technique to “unwrap” a curved surface detected using OpenCV.,1538533419.0
daedalus4210,Engineering and statistics use curve fitting regularly.  In engineering it’s how you determine the strength (allowable) properties of a material given a large set of test results,1538537370.0
rockinghigh,"Linear regressions are the simplest example of curve fitting and are used everywhere. I've also used splines to clean and compress GPS path data, to remove noisy or stationary points. ",1538547343.0
klausshermann,Often used in financial/econometrics settings. ,1538534271.0
dejafous,"For a more common every-day example, automated alerts based on traffic patterns. Sinusoidal curve fitting is great for this.",1538537690.0
remy_porter,"Yeeeeep. Two scenarios.

I had a panel of LEDs  being driven as a video wall. The power supply in use was rated for 12 amps, and fused at 12 amps. Unfortunately, at full white, or many other bright settings, the LEDs would draw *more* than 12 amps. I needed to know when I was about to send a frame that drew more than 12 amps, and do a brightness adjustment to ensure that it didn't.

So I measured the power draw at various degrees of brightness, plugged it into a curve fitting algorithm, and saved off the coefficients to use in my program.

Similarly, I have a DC motor. It passes through two switches during its rotation. I need to estimate how long it takes the motor to move a certain distance at a certain duty cycle. So there's a calibration step, where the motor moves between the two switches at different speeds and I measure the amount of time it takes. I plug that into a curve fitting algorithm and use that to determine how fast a motor needs to turn in order to cover a certain sweep in a certain time.

In the former case, I just used a curve-fitting tool, since I wasn't going to be measuring these at run time- once I had the coeffs, I could just plug them in. In the latter, I just used a curve-fitting algorithm I found and reimplemented in C++. It runs on an Arduino. ",1538567560.0
elcric_krej,"All of machine learning essentialy boils down to cruve fitting, it's just that the number of dimensions is larger and we don't necessarily try to ""mimic"" a specific type of function (e.g. a polynomial).",1538582269.0
olliej,Do you mean curve fitting on an image (as reddit has decided to have a preview image) or just a data series? Least squares is a more traditional method but I don’t believe it works for concave shapes (so in essence I believe you can only go up/down on N-1 axis of N axis data),1538535256.0
the_engi_nerd,"This was a regular task in my flight test instrumentation job. Some transducers had to be calibrated on the aircraft because they depended on something on the aircraft moving. For example, we installed string potentiometers that measured the extension and retraction of the landing gear. We had to jack up and lower the gear strut, taking measurements of how many telemetry PCM counts (a direct read of the voltage output of the string pot) corresponded to each inch of extension or retraction of the gear. That was a linear motion so a least squares fit did the trick.",1538566628.0
csp256,"Only every Monday, Tuesday, Wednesday, Thursday, and Friday. 

Sometimes the weekends too. ",1538585401.0
sheikhy_jake,Physics. All the time. The exponents and coefficients on various terms often have different origins and meanings. V.v.v. brief,1538596309.0
EmilBourgeois,We use curve fitting algorithms in our instrument calibrations.  The raw instrument produces a Beers curve however by taking several samples from known standards we can produce a curve fit formula from which to extrapolate other measurements.,1538536677.0
IndestructableLabRat,Curve fitting all the time in Igor pro for materials research/physical chemistry.,1538540280.0
cahphoenix,"Building a software controller for a rocket engine.  The fuel ratios and some other datasets were taken from old implementations (same engine, but with different parts from years ago) and we used curve fitting to come up with new formulas for the new sensors/parts.

The formulas were used for things like health monitoring, and making sure the engine was 'where it should be' at a given time.",1538547591.0
pygy_,"I've used curve fitting to remove exponential artifacts when recording nerve signals following an electrical stimulation.

You apply an electric pulse to some nerve and record the nerve pulse a bit further ([with a setup similar to this](http://frontalcortex.com/gallery/pics/jdmiles___0000000000_1254930588_i.jpg))

The stimulation would jam the capacitors of the amp, causing an exponentially decay that would dwarf the actual signal, making it hard to measure its amplitude or post-process it.

So I used curve fitting (using IIRC a linear combination of two exponential functions, that was one of the builtin options in Matlab 10 years ago) on a part of the signal where I knew there was no nerve activity then subtracted the resulting curve from the whole signal, leading to a nice, straight baseline.",1538550055.0
RosieRevereEngineer,"Yes. Big time, for many different applications. One example which hasn't been mentioned here: We create curve fitting algorithms for implementation in products with constrained computing resources to produce on-the-fly models for embedded xontrollers. The idea is that a model which is constructed by ""curve fitting"" data is less computationally expensive then solving complex differential equations and produce adequate accuracy over the control range. ",1538550785.0
bitse,"League of Legends uses curve fitting for compressing skeletal animation data.

https://engineering.riotgames.com/news/compressing-skeletal-animation-data",1538561257.0
SemaphoreBingo,"Yeah, I had noisy data coming from a sensor and needed to (a) estimate a derivative (b) make short-term forecasts and (c) get the data on a regular grid for downstream processing.",1538570035.0
DiggV4Sucks,"We do it to estimate junction temperature from ambient temperature and current LED intensity.

We use the estimated junction temperature to estimate color and efficacy changes in the LED.

This allows us to create more accurate color in our LED lighting.",1538570787.0
railtrails,"The entirety of AI/Machine Learning uses curve fitting all the time. So does finance, healthcare, operational engineers, game designers, and more.",1538577578.0
,Pretty well every sensor needs a calibration curve. Folks don’t care about volts; they want temperature or pressure units,1538580954.0
umib0zu,[You might like this book](https://www.amazon.com/Numerical-Methods-Scientists-Engineers-Mathematics-ebook/dp/B00CWR4M26).,1538581713.0
pulsar512b,There's an xkcd for this: [https://www.xkcd.com/2048/](https://www.xkcd.com/2048/),1538592144.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1538529275.0
MachineInTheStone,Facebook says a lot of things,1538528504.0
janepe4,Compsci has nothing to do with specific library written in specific language.,1538583308.0
Humane-Human,"Can someone break this article down please?

What is it about?",1538525771.0
looselydefinedrules, sounds like a bunch of made up jargon. ,1538555633.0
puisseance,"I'm not gonna lmgtfy but that's what you should do, honestly... Maybe add 'Walter Bright' to your search terms.

Languages are more than just decisions about syntax. It's writing a lexer and parser. It's decisions about the back-end, and the compilation process overall. Do you wanna use the JVM? GraalVM? Something else? Error messages are a thing.

Others know way more about this than I do but I know it's a big task.

There isn't a standard checklist you can work down, more like a massively branching tree of decisions. It's a challenge of great creative and technical scale. Start with a pen and paper and mess around on the creative side before getting yourself in a tangle.

You can always write a Domain-Specific Language in Lisp...",1538504185.0
TheCosmicBagel,"You might find this to be a useful read.
https://increment.com/programming-languages/crash-course-in-compilers/

This isn't a project I've taken on myself, but if I were to start today I'd probably target LLVM IR (as in your compiler puts out).

If you've literally never programmed before, a good language to start out with is python.

Regardless of what choices you make, you're going to doing a lot fo googling.",1538505327.0
Microscopian,"I wrote an interpreter for my own programming language in university and it was one of the most fun projects I've worked on.

This is a difficult project and there are many books and web sites that teach this, but my suggestion for a beginner is http://craftinginterpreters.com/.",1538507410.0
sfultong,"check out /r/programminglanguages where we have a bunch of different people working on their own languages

It's a good place to ask specific questions related to language creation, like parsing, runtimes, etc.",1538510871.0
gabriel-et-al,"First of all, you must decide what kind of ideas will be expressed in your language. For this you need to know what's the purpose of your language.

For example: Why is Erlang and C so different? Because they have different purposes. The purpose behind the language drives the decisions on the languages's syntax, semantics etc.

Once you know in what situation your language will be used (i.e. the language's domain), you have to think in what is the best syntax and semantics for it. Examples:

* If your language is going to be used to introduce kids into programming, you'll probably want to make it a VPL
* If your language is going to be used to numerical analysis, its syntax and semantics must closely match common mathematical idioms
* etc

After you have everything in set up, build the [grammar](https://en.wikipedia.org/wiki/Context-free_grammar) of your language. 

Once you have this done, the rest of the process is really straight forward (doesn't mean it's easy tho). You'll write a parser to transform a text-file (or whatever is the input of your language) into a tree-like data structure (usually called AST), then optionally process  this AST to optimize things (perform removal of unreachable code for example) and finally emit code in other language or interpret it directly (this is the difference between compiler and interpreter, respectively).

Good luck!",1538533917.0
combinatorylogic,Have a look at simple tutorials like JonesForth and MAL.,1538507179.0
nawal86,"https://mitpress.mit.edu/sites/default/files/sicp/index.html

https://compilers.iecc.com/crenshaw/",1538553343.0
TomvdZ,You might be interested in [compiler bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(compilers\)).,1538503824.0
Responsible_Virus,interpreters seem easier but not sure anyway the dragon book and their is a online stanford class taught by alex aiken that's good.,1538691262.0
pulsar512b,"I congratulate you on your handling of English. Sadly, I am only a student.",1538499851.0
jessi-t-s,"Do you have a specific type of employment in mind? (I work in research, not software development or anything like that.)",1538501151.0
espionice,"Jetbrains products. Might not be the fastest, but it makes up for it in features and customization.",1538498326.0
YourUndoing,"Visual Studio and Visual Studio Code. Granted these days I tend to write more .NET code and Javascript than anything else, but I have always found the extra features in the VS IDEs to be super helpful for my productivity. And having easy access to a terminal in VS Code is a developer's dream. As far as in an academic setting, I think VS Code is a great multipurpose IDE to use that is relatively lightweight.",1538494787.0
pirilamp,vim.,1538493704.0
mynewpeppep69,Where my emacs bois at :(,1538516912.0
carryingawand,IntelliJ. Love the time-saving features,1538496866.0
negative_alpha,"VSCode, for everything, C, C++, python, etc",1538493931.0
OneOldNerd,"Intellij.  No, it's not free.  Yes, it's worth it (to me, at least).",1538496269.0
corruptbytes,"Big projects: VSCode

Smaller scripts/while ssh'd: (Neo)Vim",1538514648.0
Screye,"No here uses Spyder for python ?

Can someone sell me on another idee ? I have liked Spyder so far though",1538516646.0
CrypticWriter,"The ones from JetBrains. PHPStorm, PyCharm, etc. are really nice. 

VS code is nice too",1538502050.0
wwitb10,"VS Code is my go-to editor. It’s fast and lightweight. Great plugins for Python, JavaScript and Go for general scripting, API or web app development.

Note: not a fan of any other tools made by MSFT, this one is the exception to the rule.",1538493687.0
yaschobob,Systems software. Emacs.,1538501315.0
zexen_PRO,I’m an intellej and Pycharm man myself,1538508641.0
eikenberry,[Unix!](https://sanctum.geek.nz/arabesque/series/unix-as-ide/),1538505750.0
shahsuraj261,"Atom and Sublime interchangeably

&#x200B;",1538507862.0
jmite,"Agda mode, it's an amazing new way of programming, by holes and case expansion",1538536245.0
TroyHernandez,"Rstudio for data science

Compared to Rmarkdown, Jupyter notebooks are garbage.  Rmarkdown is plaintext that compiles to html (pdf or .doc)... that means:

1. The code you type in them can be easily transferred to and from a real code base.

2. You've got version control.

3. You can use an IDE (and all associated keyboard shortcuts) for both notebooks and regular coding...

I've started using it for shell scripts and even the occasional time when I'm using Python, by sending the script to the terminal that's been newly embedded.",1538501500.0
MCPtz,"Products that have paid and on-going support are worth a lot to professional software engineers.

In University I just edited everything (back then) in XEmacs and some smart, but simple text editors. They completely lacked any code connection features, auto completion, debugging, etc etc. I take all of the latter for granted these days.",1538497552.0
TostitoTheUnblinking,"I previously just used a text editor and the terminal for everything, but in my current internship I’ve been introduced to PyCharm, which I really like!",1538504528.0
mendrique2,"intellij idea for everything, except really large files and temp files, for that I use sublime. I know there are scrap files, but I just prefer it on a separate os icon.",1538509463.0
CapableCounteroffer,"vim for really quick edits or writing simple scripts in python, Visual Studio Code for most of my programming in Rust, C, and production Python. Jupyter Lab for model development and data analysis in Python.",1538519824.0
k4b0b,"**C/C++ = vim**

Very lightweight and powerful. This was my preferred IDE for many years while developing firmware. I often found myself using the Terminal anyway, so writing code on there felt natural.

**iOS (ObjC/Swift) = XCode (not really a “favorite” since you have no other choice)**

Apple simply does not care about developer experience as much as it cares about end users’ experience, so be prepared to deal with some very confusing and annoying bugs/errors with the IDE. That said, there are some very powerful debugging/optimization tools included (e.g. Instruments).

**Android = Android Studio (basically IntelliJ)**

On the other hand, Google is generally very developer friendly. I haven’t had nearly as much experience developing on Android vs. iOS, but I have been pretty happy with the tools here.

**Web Development (JS/Python/HTML/CSS) = Visual Studio Code**

Let’s be honest: Javascript can be a nightmare to work with. IMO, VS Code is by far the best IDE for developing on Node.js, Python, and other web-related frameworks. It also has a vast and robust set of plugins for just about every situation.

Edit: Fixed formatting.",1538533087.0
FuzzNugs,Visual SlickEdit with Emacs emulation. Been using it for years and just can’t find anything better. I’m a windows dev and my pet peeve is anything that requires me to use a mouse. With SlickEdit I can do everything very efficiently without touching the mouse at all. ,1538534197.0
keepitsalty,"Ready for the downvotes but Rstudio is where it's at. This is a case where I believe the IDE actually makes the language phenomenally better. Once Rstudio gets seamless Python integration, I think it will be the greatest Data Science IDE of all time. ",1538534689.0
janepe4,Emacs.,1538582966.0
SilentAmoeba,Eclipse for Java and PyCharm for Python. ,1538510257.0
n008man3,Netbeans. It seems I'm the only one who actually likes it.,1538520496.0
TalenPhillips,"My work recently has been in assembly, C, C++, C# and Visual Basic.

For C# and Visual Basic you just can't beat Visual Studio. For everything else I use Visual Studio Code. I suspect Sublime text would be about as good, but having consistency is nice.

It's gotten to the point where I'll use VS Code even in Linux... Unless I'm not in a window system. Then I'll use nano.",1538502982.0
vsync,SLIME,1538509718.0
imaoreo,"Vim, Atom, or Intellij (if I'm writing java). I wouldn't touch vscode with a 10m stick.",1538521322.0
nickosaur,I recently started using GoLand. Had to switch from vim with my new company’s codebase. Tried vim/atom/vscode. None of those could handle the scale and PyCharm/GoLand just works. Also has a ton of cool code search and navigation features. I spend a large percentage of time reading vs. writing code and I feel like that’s what a good IDE like GoLand does well vs something like vim which is great for writing code. ,1538494577.0
bdtddt,Quantomatic.,1538501853.0
Santamierdadelamierd,I only used eclipse as part of a job and didn't like it. I use sublime and have hopes about using emacs some day in the future. I like the look and feel of visual studio though I never used and think intellij is cool.,1538528711.0
putin_your_ass_,"Recently when I finished configuring Vim for my needs (I had spent almost 3 days for it.. 400+ lines of .vimrc) I proudly showed it to my friend.
Then he took VS Code, added Vim-mode plugin and some plugin for custom keybindings, and after 10 minutes of customizations he got almost the same thing that I got =))",1538529115.0
mediasavage,"i use sublime text and iTerm for almost everything (mostly backend stuff with python and frontend stuff with React). 

i do enjoy using jupyter notebooks when i'm experimenting with new python libraries, sketching ideas, etc.

i also occasionally have to whip out vim when i'm SSHed into remote machines.

maybe some day i will try and become fluent with all the vim keybinds and retire sublime. i do recognize the power and efficiency vim can bring, i've just never gotten around to putting in the time to learn all the keybinds and whatnot :/",1538538272.0
Zophike1,"I like mathb.in for TeXing up proofs, if anyone knows a good LaTeX editor please comment :'-(.",1538543928.0
liquidify,I like to use Xcode or Clion for small c++ projects.  Larger projects I move onto something else. ,1538546626.0
its_not_herpes,"Xcode for objective-c development, VSCode for most other programming (python, c), and Sublime Text for random everything elses ",1538551921.0
Yctallua,"IntelliJ for java and kotlin, VS for everything else.",1538564043.0
__-_-_-_-_-_-_-_-_-,Vim with auto complete and few other plugins for most of the part. Eclipse with vrapper for Java.,1538567725.0
MemesEngineer,Im a Pycharm and Visual Studio normie.,1538583332.0
,[deleted],1538499769.0
githitter,I mainly use Sublime but I’m thinking of switching to Atom. Both are very customizable and have a lot of packages available. ,1538503570.0
metallic,I use a mix of VS Code and an IDE from JetBrains called Rider for C# development.  ,1538499300.0
pulsar512b,IDLE for Python is the default and amazingly simple. ,1538509041.0
NowImAllSet,"ITT: people who don't understand what an IDE is. ~~VSCode is **not** an IDE~~. It is a fancy text editor. Same with VIM, Nano, Sublime, etc.

Edit: didn't know VSCode has so much functionality. I stand corrected. Thanks to everyone who let me know. 
",1538503611.0
Beefin,atom for night mode really lol,1538531102.0
combinatorylogic,You may want to have a look at Wolfram Mathematica. Jupyter notebook is just a pale imitation of it.,1538496230.0
teteban79,"Is this homework? If you have a good OS book around and I tell you that you are describing a *very* famous bakery, that should get you going",1538475417.0
Responsible_Virus, if they are ordering cakes at the same time they could just order more than 25 and then their isn't enough cake. the cake is just an analogy for computer resources like memory and customers are clients are programs,1538691604.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1538457500.0
dankmeems,"The only information I can find on ""dram grading"" is from '99 http://www.simmtester.com/page/news/showpubnews.asp?num=11

It seems only to be related to speed/performance characteristics. It is safe to say that your lecturer is out of date (take all the theory you can, leave behind any proclamations about how industry may or may not work), as you can find many different speeds of dram on the commercial market.

That said, supply and demand still applies. You can't just go and buy HBM2 (a new, fast form of dram) chips because they're all being soaked up by AMD and Nvidia.",1538450331.0
wtallis,"DRAM is generally graded by speed and voltage, but within a single product line there can be quite a bit more than three grades/bins. Flash memory is similarly graded in terms of defect rate; the NAND flash that goes into SSDs is usually a higher grade with better expected write endurance than the stuff going into SD cards and USB flash drives.

Memory is generally sold in one of three forms:

* whole wafers: Big companies like Kingston and ADATA that don't own their own semiconductor fabs will buy wafers and do their own testing and packaging, deciding for themselves what speed/voltage bins to categorize dies into

* individual BGA packages: these are chips that have already been tested and graded

* whole modules: the memory manufacturers allocate some of their output to their own finished products eg. Micron sells DIMMs and SSDs directly to consumers under their Crucial brand, and to PC and server OEMs under their own brand. 

The speed and voltage that a finished DIMM is rated for is not always the same as the rating of the individual DRAM packages were rated for. A company like G.Skill will often produce niche high-end parts by doing further binning on the BGA parts they buy to find the cream of the crop that outperform what Samsung/Micron/Hynix rates them for.

When it's relevant, the vertically-integrated memory manufacturers may hold back significant portions of their output for their own use. This doesn't apply to eg. HBM DRAM because none of the memory companies make GPUs, but Samsung and Intel keep most of their flash memory output for their own SSDs.",1538456091.0
khedoros,"""Binning"" of parts is fairly common, or at least used to be. They'd test a component at the different speeds they want to sell. If a part wouldn't work at the higher speed, they'd clock it a little lower and sell it as a cheaper model. I don't know whether they've graded RAM for reliability like that. I'd tend to think that they'd sell the slower or less reliable parts as ""value"" RAM or something, but that's just speculation on my part.",1538451577.0
thewataru,"Can nodes in the same graph have equal values? How big are your graphs? How endpoints are counted (i assume endpoint is different from all nodes except other endpoints in the same orientation, with the same label and edge color, am i correct)?",1538487028.0
zacharyh211,"This sounds more like a string problem than a graph problem. If I'm understanding correctly, you're roughly looking for the longest common substring between the different ""graphs"". The number of different values are minimized exactly when we maximize the number of times we color equal values the same way (coloring the left and right edges the same). A color in a graph can only apply to 2 vertices if it is the ""left"" for one vertex and the ""right"" for another. This just means they're consecutive in the string. This is the only way we can have 2 adjacent vertices ""removed"" from our duplicate count (Edit: This is the only way when considering two of the graphs in our set).

&#x200B;

For a set with only 2 graphs, the solution would be to find the longest common substring, arbitrarily color those substrings' edges the same way, and then repeat this process for the next longest, disjoint substring. We also don't consider the characters before and after the substring we just colored because they are now ""half colored"" using a color that has already been spent in the other graph. This is OK because coloring always removes at least 1 neighbor anyway, so we can't do better with smaller substrings. Eventually we run out of common values and just randomly apply our remaining colors to the graphs. While it appears to always be better to take the strictly longest substrings when possible, the choice of which one when there are multiple is important. For example:

&#x200B;

1 2 3

3 2 1

&#x200B;

Here, we only have size 1 substrings in common. Coloring the 2's the same would prevent us from coloring the 1's or the 3's the same. Instead, we could color the 1's the same and the 3's the same, leaving the 2's as ""different"" vertices. A greedy approach where we always choose to color from left to right seems to resolve this issue.

&#x200B;

With more than 2 strings it's definitely more complicated due to the metric that we're optimizing for and the possibility of having a color only used for a subset of the graphs. However, looking into longest common substring solutions might be a good starting place for this sort of problem. You might be able to use this approach to get ""candidates"" for coloring and then take a more brute force approach to choose the best of those candidates since that pool shouldn't be too large (compared to the size of the set of graphs and the sizes of the graphs themselves)

&#x200B;",1538500788.0
blexim,"I'm unclear on a couple of things:
* You says each vertex has a left edge and a right edge, does that mean the graphs are all infinite?
* Are the labels in one graph repeated or does each label appear at most once in a graph?
* Do the graphs all the same size?
* Is the number of colours about the same as the number of vertices in a graph, or much smaller?

Some ideas that might be helpful:
* It might be easier to think of a graph as a total order, or as a permutation
* The function mapping a vertex in one graph to a vertex with the same label in another graph is probably a useful one to think about
* The minimum number of different vertices is no more than C+1, which you can achieve by colouring every edge in every graph the same colour, except for the C-1 edges required to meet the ""every colour is used"" constraint. Only vertices adjacent to one of these C-1 edges can be bad, and there are C such vertices if the graph is finite, otherwise there are C+1. So if C is much less than V, this is already potentially a pretty good solution.",1538464858.0
ninjastarkid,"This sounds a lot like walking through a graph. I’m not sure exactly what it’s called but you travel through all the edges in the graph to all the different vertices once, and that forms a path or something. ",1538493848.0
Ikor_Genorio,"I do not quite understand the specification about the 'edge coloring' stuff. Regardless, it looks like each graph is in fact a tree, I would say that this makes the problem significantly easier.

It is a (binary) tree, because you mentioned that the graphs are: acyclic and connected (this makes it a tree) and each vertex has a left and right edge + a max degree of 2 (this makes it a binary tree). I assume the graphs are finite, so there will be some leaf vertices (a.k.a. vertices that only have an edge going into them).

You may wonder what the 'root' of the tree is. Well, any vertex can be the root of the tree. Imagine you have your graph and you choose one vertex. 'pick up' that vertex and let all other vertices fall down by gravity. Now visually also have a tree.",1538467337.0
nebbly,"I created https://cinc.kitchen. I went with storing separately. 

On the GUI, it's just raw text, and I have parsers on both the front end and back end to determine the proper units, and quantities. 

For unit conversion I wrote a units library, which you're free to check out or use: https://bitbucket.org/cinckitchen/kunits/src (python). 

It took a fair amount of work to get this where I wanted it to, but it satisfied all of my requirements: ease of input, persistence of user intent, and arbitrary scaling.",1538451863.0
igglyplop,"I mean a number then a string works, or just a string of quantity and unit (and a parser for reading it).",1538448244.0
Kzone272,"I think I would do a number and a unit enum that translates to a string.

A single number likely won't work because you can't translate all numbers to all units.  For example, some instructions may use mass/weight (grams, oz, etc.), and others will reference volume (ml, cups, fl oz, etc.).

You could do a single unit if you knew the density of all ingredients, and thus could convert from volume to mass and vice versa.  But that's a bit unrealistic and likely more complex than it's worth.

I'd stick with a number and a unit enum.",1538452014.0
metaphorm,"lets break down your problem a bit more systematically and get a schema design for your database that works. 

so you have a couple of different entities that exist in a relation with each other that we can call ""recipe"". what does a recipe have?

Recipe has a name, some text that describes the procedure to cook the recipe, and a list of ingredients and quantities. 

name and description/procedure are just text fields that we can store directly in a database column. the list of ingredients and quantities we have to model with some related tables. lets say we have another table in the same database called Ingredients. that's another entity in our system. what does an ingredient have? 

Ingredient has a name, some descriptive text, and maybe some metadata like manufacturer/supplier/farm-of-origin, etc. 

In this system we also have a relationship between recipes and ingredients that includes a specific quantity (or proportion) for that ingredient in that recipe. For example: a recipe for a cake might call for 1 pound of sugar but a recipe for glazed carrots might only call for 2 tablespoons of sugar. 

so this relationship is something else we want to express as a table in our schema. lets call it Recipes x Ingredients. what does a Recipe x Ingredient have?

A Recipe x Ingredient has an identifier for which recipe, an identifier for which ingredient, a quantity of the ingredient, and a units field for the ingredient quantity. 

That should do it. Three tables in the schema. Recipes, Ingredients, and Recipes x Ingredients (a through table expressing a ""many to many"" relationship). That should meet your data modeling needs for a basic recipe database system.  ",1538448236.0
dillyvanilly123,"If you are looking for a universal representation for quantities a la Unix time then SI units are the way to go. For instance, your base unit of mass could be kilograms (SI standard unit of mass), meaning that any quantity that you are storing in your database is represented by a single number: mass in kilograms. You would then convert to/from other units (grams, ounces, etc.) on the fly. 

As others have mentioned, you have the potential  for issues when quantities of ingredients may appear in recipes as either mass or volume. In my mind, the answer is to use reference densities to convert volumes to masses, then just store the mass in kilograms internally. 

The obvious burden with this approach is all of the machinery required to convert to/from your base unit to other units out there in the recipe world, but if storing a single number is your goal then this is the best way I can think of to do it.

P.s. same goes for volumes, decide on your standard unit (say, mL) and store everything as that, convert to/from other units as needed",1538457996.0
Sniffnoy,"If you want to look at how to do dimensioned quantities, I'd take a look at how [units](https://en.wikipedia.org/wiki/GNU_Units) does it.",1538456933.0
LearnerPermit,"The problem with recipes is they're incredibly imprecise. Recipes from home cooks at best use volumetric measurements, or use vague measurements, a medium onion, a pinch of spice, one bay leaf, a heaping teaspoon, dash of salt, season to taste.

Trying to store that as a universal measurement system doesn't work. Some recipes geared towards professional chefs might be more consistent in their specification by mass but that's not guaranteed. ",1538450330.0
wiener_pancakes,"Not exactly a database, but a related idea I think you'd be interested in: 

[units of measure built in to the type system](https://blogs.msdn.microsoft.com/andrewkennedy/2008/08/29/units-of-measure-in-f-part-one-introducing-units/)",1538455383.0
quentech,"Anyone doing OOP/domain modeling should have a read through Analysis Patterns: Reusable Object Models by Martin Fowler. 

Check out chapter 3

http://uml2.narod.ru/files/docs/13/AnalysisPatterns.pdf

https://smile.amazon.com/gp/product/0201895420",1538456344.0
th0maslv,"Oh I actually built a project in college where we scraped the internet for recipes, populated a db with them. Then you could keep a “pantry” of ingredients you have and find recipes based on them with good ol elastic search.  

We came across this problem too, ended up storing the number as a float, the unit of measurement as a string, then whether it was a fluid or solid so we could convert accordingly. As well as a table of conversions. Including “T” to “Tablespoon”

Some ingredients were weird, egg for example. But our strategy dealt with most problems.",1538458403.0
pardoman,"Entiry-Attribute-Value model approach could work for you. 

https://en.m.wikipedia.org/wiki/Entity–attribute–value_model",1538456749.0
FollowSteph,"Unless your performing some kind of math where you need to mope the exact quantities then why not just store the quantity and unit as a single string, that is either a list of ingredients or all the ingredients as a single string. Again it all depends on what you need to do with the data. As in do you really need to compare 1 tbsp or sugar to 1 cup of sugar? Normally for recipes ta just list of items....",1538458582.0
geon,"My company works with HVAC design and simulations. We call ”quantity plus unit” an Amount. We recently open sourced the library we developed and use in our products; https://github.com/dividab/uom

It supports units in metric or imperial, with conversions.",1538470836.0
Fringe_Worthy,"I think one of the things you should consider is what you're trying to do with this information. And as well, how much do you actually need to do with automatedly? 

Are you looking to be able to upscale / downscale the recipe? How far do you need to go. 

I suspect you could even have a secondary measure indicating how flexible the recipe is. Your stir fry quantities are quite freeform, as long as they fit in the pan, while I suspect bread can be a lot more finicky. Or course, some of those might be tripply difficult since recipe might involve ""Adjust quantities so that dough has proper attributes for current temperature and humidity""?",1538578393.0
,[deleted],1538448719.0
Banality_Of_Seeking,"https://world.openfoodfacts.org/

https://world.openfoodfacts.org/data

https://github.com/openfoodfacts",1538450524.0
JNCressey,"how about just storing the whole string ""one cup of chocolate chips"" and parsing out the specific information when you need it? ",1538475970.0
remy_porter,"I used to support an application which managed chemical formulations of paint.

We had a table called `UOM`, which stored a list of all the known units of measure. Every input or output of the formula had a `UOM` foreign key field, and then a scalar number which was the quantity of the unit.

The advantage here is that it's easy to convert, because you can have a table of valid conversions, e.g. `kg,lb,2.2064`.",1538480462.0
PM_ME_YOUR_FUN_MATH,"Make an abstract ""Amount"" type. This type can be refined into one of several different forms:

A mass type
A volume type
A quantity+size  type
Etc

You'll have a different specific type for each possible dimension of measurement you can have. Each dimension can be set up to translate around within itself. For example, grams to kilograms is no problem, just as cups to ounces is no problem.

For eggs, you can have ""3 Medium"" Eggs. I don't think these should be stored as some axiomatic value, because they're most accurate as-is (since even a ""medium"" egg can vary). You can have an approximate conversion by saying each size is, say, 70% of the next size.

For each new measurement system, you can either see if it matches a unit of an existing dimension (heaping cups could be volume), or add a new dimension.",1538483522.0
practicalutilitarian,"I converted fractions to decimal and everything to  metric units: mL and grams. And you can simplify further by converting mL to grams, if you have densities for all the ingredients. Some recipes provide weights and volumes which you can use to build this table of densities.",1538522807.0
bart2019,"""Quantity plus unit"" is core in calculation in physics. For example,speed is distance divided by time interval, thus its unit is unit of length divided by unit of time. It's like computing with numbers, except it's not.

 I think at least one teacher referred to this as dimensions: time dimension, distance dimension, etc... Volume dimension is length dimension to the power of 3.

I haven't even touched the subject of unit. You can convert from one unit of length to another unit of length; but you cannot convert it to a unit of mass.

Surely somebody must already have thought of a proper way to deal with this kind of stuff in a computer, in a universal way.

",1538452762.0
clintdavis77,"Looking at my Fitbit app it looks like they have three fields, a whole quantity, a fractional quantity, and then the unit description. So your hamburger could be 0 1/3 lb or 2 0 oz. I would think that the whole quantity is just a number field (however big you want to go). The unit description and fractional values I’d think are probably look up tables.  I haven’t really done it, just going by what I see and my experience in the work force. ",1538448056.0
santoso-sheep,I’m upvoting and I don’t know why,1538445654.0
quambothemombo,what is this post,1538445114.0
dankmeems,"Yes, I believe doubling down on the Art of Computer Programming is an item within the set of things you can do.",1538444391.0
acroback,Ummm could explain a little better. ,1538443585.0
AntiGravityTurtle,wut,1538443922.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/computervision] [Fast Algorithm for Extracting ""Large"" Image Features](https://www.reddit.com/r/computervision/comments/9kidr0/fast_algorithm_for_extracting_large_image_features/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1538414971.0
SiliusSodus,You might want to check out Devon Crawford's Youtube channel. He has a bunch of videos when he describes a similar project. I think his uses bruteforce but I imagine it wouldn't be too hard to streamline his methods.,1538445050.0
hindmost-one,"Here you are, solution (may have some +1/-1 errors, though) in haskell language.

    {-
        We store current sequence and maximal length we have already seen.
    -}
    data State a = State
        { seq    :: Seq a
        , maxLen :: Int
        }
        
    {-
        Sequence is two previous elements and its length.
    -}  
    data Seq a = Seq
        { prevOdd  :: a
        , prevEven :: a
        , len      :: Int
        }

    {-
        Given a list [a, b, c, d, e... z], we construct accumulator of State from ""a"" and ""b""
        and then repeatedly ""addToState"" each remaining element along with its index.
        
        (Indices were added to the list by ""zip [2..]"" operation)
    -}
    maxDoubleIncr :: Ord a => [a] -> Int
    maxDoubleIncr (x : y : rest) =
        maxLen
            (foldl addToState
                (State
                    (Seq x y 1)
                    1)
                (zip [2..] rest))

    {-
        We add element to the sequence and assing max(seq length, prevMaxLen) to maxLen.
    -}
    addToState :: Ord a => State a -> (Int, a) -> State a
    addToState (State seq maxLen) (i, a) =
        let
            seq' = addToSeq seq (i, a)
        in
            State seq' (max maxLen (len seq'))

    {-
        Detect oddity of the index, replace given ""prev"" element and correct length.
    -}
    addToSeq :: Ord a => Seq a -> (Int, a) -> Seq a
    addToSeq (Seq odd even len) (i, x) =
        if isOdd i
        then
            if x > odd
            then Seq x even (len + 1)
            else Seq x even 1
            
        else
            if x > even
            then Seq odd x (len + 1)
            else Seq odd x 1

    isOdd :: Int -> Bool
    isOdd x = x `mod` 2 == 1
",1538407205.0
politeCamelCaseGuy,"Let's define f(i, j) as the length of the longest double-increasing subsequence of X such that i < j and A\[i\] is its first, and A\[j\] are its second element.

&#x200B;

f(i, j) = max { 0,  max { 1 + f(j, k) for each k where k > j and A\[k\] > A\[i\] }

&#x200B;

Total solution to the problem is 2 + max { f(i, j) for each i, j where i < j }.

&#x200B;

Feel free to ping if you need further explanation.",1538406359.0
yash_8141,"x\[i\]>x\[i-2\] sorry copy paste mistake.

&#x200B;",1538404315.0
east_lisp_junk,"> I would like to study machine learning.

Can you narrow your focus? Has your time in industry faced you with a type of problem that you can turn into a research program? A statement of purpose that points to a research problem you're interested in tackling can help your case a lot.",1538397444.0
batteryramdar,"you can almost definetly get into a top 20 with those credentials.  Top 5 might be more competitive but it should be in play.  If they didn't accept people from industry who went to industry and thus does not have major research experience, they wouldn't have anyone in their programs lol.",1538380512.0
DoorsofPerceptron,"Which country are you looking at? If you're interested in the UK,  I might have phd funding for EU citizens depending on your experience. Feel free to PM me.

Even if you're not interested in the UK, if you can talk coherently about the pitfalls in deploying ML models in the real world and have the experience to back it up, there is probably a mid-level (but not top tiere) PhD programme out there. If you talk to your colleagues with PhDs that do active research, they can probably point you towards great researchers who are at less prestigious institutes,  and are a bit more flexible in their entry requirements.",1538431159.0
assface,"> Will it be possible to get into a decent graduate program from such a background?

MS -> yes

PhD -> no (you don't have any research experience)

> I am only interested in attending if I can secure funding, since I dont believe in paying for school when I could be making money in industry.

You are unlikely to get funded as a MS student. The programs are short and the funding typically comes from research grants, so you're not around long enough for a professor to justify funding you.

PhD students in CS get full funding at every top school. ",1538392040.0
ejineta,"Experience reveals that the best ideas come around when having conversations with your teammates. You probably all have different specialisms or interests, and the most productive (+fun) way is to get to an idea that suits you all. Good luck and have fun!",1538324836.0
Indifferentchildren,"Does the hackathon not have any parameters for projects?  Usually there are components to leverage, problems to solve, technologies to focus on, etc.",1538351084.0
_ACompulsiveLiar_,">I mostly there for the experience as I am a first year computer science major.

Ok well, make sure you're there for the right experience, not the kind where you're just thinking about your career.

It's a hackathon. It's a fun experience. You are most likely not going to make anything that'll turn you into a godlike coder or will land you into a big n. You also aren't solving any problem that hasn't already been solved.

Meet people, make friends, talk about coding, talk about ideas, and code something you think you'll enjoy coding. You'll make something cool, slightly unique, network, etc. It'll be a good time.",1538353061.0
c_a1eb,"Web scraping stuff is always good fun, or something that runs on other people's devices like a web app to interact with a big display etc. ",1538325798.0
noam_compsci,"Create a predictive model of how many people will come to an after work event, based on the weather on that day. ",1538347284.0
muzwim,favorite one i saw last year was a haiku generator,1538401343.0
hippomancy,"Depends on whether a literature review paper exists for the topic you’re writing about already, and if your technique finds some interesting trend in the papers that currently exist on your topic.

If you’re just writing summaries of papers for a class assignment, I doubt it’s publishable.",1538308874.0
PetrosPapapa,"Literature review papers tend to accumulate many citations, especially if they are done well. Citations translate to popularity which translates to more opportunities, especially in early career. You'll be surprised how many people, even experts in a field, are unaware of entire strands of research or groups of people or pieces of software or just haven't sat down to draw the connections between different sub-fields. People do tend to support and value novel contributions more, but these are rare at the undergraduate level.

It also depends on the area you'll be working on, how popular it currently is, and how established it is. 

There is no formal distinction between 'review' and 'survey' I don' think. Calling it a 'review' probably implies some insightful or critical analysis that adds some value to the paper as opposed to just listing out what's there.",1538326798.0
DreadedDreadnought,"**Systematic** literature reviews are publishable, even in reputable journals. Don't underestimate the time commitment it would require though.",1538320579.0
UncleMeat11,"Lots of conferences publish ""Systemization of Knowledge"" papers. Do a Google Scholar search for SoK and you'll hit some things. 

But these are not published often and can't just mean ""this was the stuff I looked up"". It needs to be a reproducible and systematic literature analysis. Usually you will end up citing several hundred papers in your review.",1538327028.0
dpzmick,"If they are good, I usually appreciate a comprehensive review paper ",1538323355.0
muffinman1000,"They are lower impact than research projects with actual novel results in general, but still useful. When hiring research scientists (In bioinformatics and medical software dev at least)  if an applicant has a list of publications you would generally disregard the lit reviews and see what trey have made / discovered. But for a student I’d definitely try and get it published if it’s possible, talk to your supervisors about it! ",1538311653.0
blufox,"Yes, it is very useful, especially if the field you are working on hasn't had a literature review in the last 10 years or so, or if the field has become diverse enough that the last review has not captured the recent advances.

My field has just recently started expanding, and that shows in the number of literature reviews:
 [Here](https://ieeexplore.ieee.org/document/5487526/) is one such from 2010 that is _very_ influential in our field (1024 citations) and at TSE which is a high impact journal, [here](https://dl.acm.org/citation.cfm?id=2693269) is another of such reviews published at TSE in 2014, here is [another](https://peerj.com/preprints/2483/) in the same field, but in 2016 but not published in a high impact journal, and [another](http://orbilu.uni.lu/bitstream/10993/31612/1/survey.pdf) in 2017. All four are very well written, and can be used as a model for writing a good lit survey. [Here](http://madeyski.e-informatyka.pl/students/systematic-literature-review/) is a good source on writing a good literature review.

Beware, as much as literature reviews are useful, they are a task that requires high commitment. You have to start with a fixed plan, and stick to it.",1538385820.0
PetrosPapapa,"Literature review papers tend to accumulate many citations, especially if they are done well. Citations translate to popularity which translates to more opportunities, especially in early career. You'll be surprised how many people, even experts in a field, are unaware of entire strands of research or groups of people or pieces of software or just haven't sat down to draw the connections between different sub-fields. People do tend to support and value novel contributions more, but these are rare at the undergraduate level.

It also depends on the area you'll be working on, how popular it currently is, and how established it is. 

There is no formal distinction between 'review' and 'survey' I don' think. Calling it a 'review' probably implies some insightful or critical analysis that adds some value to the paper as opposed to just listing out what's there.",1538326887.0
todeedee,Do it,1538358596.0
lxpnh98_2,"It's a way to increase throughput in the CPU.

To execute an instruction, the processor must do several things. First it must fetch the instruction (reading it from memory), then it must decode (knowing what it should do), then it executes it (for example calculating the product of two numbers), and finally it writes back its result into memory if necessary.

These 'stages' use different parts of the circuitry in the processor. Think of it as a car-wash, where each room does one thing.

What pipelining does is take advantage of this fact. Instead of washing one car at a time, you keep pumping cars into the first room when the previous car is done with it.

So instead of having the processor fetch an instruction, decode it, execute it, and write back in one CPU cycle, and only letting it fetch another instruction then, you make the CPU cycles shorter, and instead of processing one whole instruction in one cycle, you make an instruction go from one stage to the next in a cycle.

This means that every new cycle, if things go well, you have an instruction that is being fetched, one that is being decoded, one that is being executed, and one whose result is being written back. This is assuming the processor has 4 stages. Modern processors have more stages than this.

So here's why it increases throughput.

In a processor without a pipeline, each instruction is executed in one cycle. Let's imagine one cycle is 3ns (nanoseconds). So, an individual instruction takes 3ns to be processed here.

In a processor with a pipeline, because the work of processing an instruction is being divided by different stages, the cycle can be shorter. Let's imagine we have 3 stages, and that we cut the cycle from 3ns to 1ns. As each individual instruction takes 3 cycles to be processed, the time it takes to process an instruction is 3ns (3*1ns).

You might be asking, they're the same, so how can one be faster than the other?

The answer is that in a pipelined environment, an instruction is being fetched each cycle, that is, each 1ns. This means that in 3 cycles time from when you start the processor, you will have an instruction being completed each cycle, so that in 3ns you (ideally) complete 3 instructions, as opposed to just 1 in the other scenario.

This is the basic idea. Things don't always go this smoothly though.

What if one instruction wants to read data that a previous, still executing instruction alters? This is called a data dependency.

What if we don't know which instruction should be fetched next because we have a condition (aka a 'branch') in the code? This is called a control dependency.",1538221620.0
GG_Baited,"The previous answers are already correct but I think it is helpful to see a visualization (screenshot from a lecture slide): [https://imgur.com/a/ebRHzCs](https://imgur.com/a/ebRHzCs) 

Note: One „Column“ in this visualization can be executed in one cycle. ",1538224075.0
,"A great simple example of pipelining that I heard when I was in college which might help you remember it better...

If you had to do 3 loads of laundry (wash, dry) how would you do it?

No pipeline: you wash & dry the first, then wash & dry the second, then wash & dry the third.

Pipeline: wash the first, then move the first load over to the dryer. While the first load is drying, you begin washing the second. When the first is done drying, move the second load over to the dryer, and begin washing the third, and so on.

So essentially, barring the first and last load, the pipeline always has both machines running. In the first example, only one at a time is running.

To illustrate the point further, if it takes 30 minutes for each machine to finish their process, the no pipeline approach will take three hours (run each machine three times, never concurrently). However, the pipeline approach will only take two hours (first and last cycles take 30 minutes, then two in between cycles where both machines run concurrently).

As others have already explained, you can see how this relates to executing an instruction in the CPU.",1538228284.0
minno,"Each instruction takes multiple steps. Instead of waiting for one instruction to finish every step before starting the first step with the next one, it starts the next instruction and lets it go one step behind the previous one. This is great when the steps don't depend on each other, but needs some clever workarounds when they do.",1538220214.0
lokithegregorian,"Dude I dont mean to be that guy? But Wikipedia is GREAT for barebones programming concepts like this one. Links to multiple different explanations, animated gifs, the works.",1538246715.0
szienze,"Could you please a bit more specific about what your issue regarding deployment is?

You can obtain a server from Amazon Web Services: https://aws.amazon.com/free/ with a Linux distribution image. Then you'd simply upload and run your Python files there and point your front-end scripts to the server's IP address, instead of pointing to your own computer (e.g. 127.0.0.1).",1538191841.0
spacelibby,"Well, it depends on how strict your professor is. You haven't really said anything about the 5 smallest Fermat numbers. It turns out that they are all less than 65537, but that's not translating the sentence as it's said.

Aside from that, your first predicate isn't a translation of the sentence. It says ""if n is less than 65537 and n is a Fermat number, then it's prime."" this is true, but the sentence says ""ONLY the first 5 Fermat numbers are prime"", and this predicate says nothing about larger numbers.

The second predicate is a lot better. ",1538185619.0
NotImplemented,"I'm a bit rusty, but I think you could also express the statement like this:
There exists no Fermat number that is prime and also greater than 5 different Fermat numbers.

    ¬(∃x∈N: FermatNumber(x) ∧ Prime(x) ∧ ∃a,b,c,d,e∈N: 
       a < b < c < d < e < x ∧ FermatNumber(a) ∧ FermatNumber(b) ∧ FermatNumber(c) ∧ FermatNumber(d) ∧ FermatNumber(e)
     )",1538198090.0
cbarrick,"> “Only the five smallest Fermat numbers are prime.”

I interpret this as the following:

1. There exist five distinct natural numbers which are both prime and Fermat.
2. There is no Fermat number between any of the five.
3. There is no Fermat number less than the smallest of the five.
4. Any Fermat number greater than the largest of the five is not prime.

I would write that like this:

    ∃ (a, b, c, d, e) ∈ N^5:
    
        % 1. There exist five distinct natural numbers which are both prime and Fermat.
        a < b < c < d < e,
        Fermat(a), Fermat(b), Fermat(c), Fermat(d), Fermat(e),
        Prime(a), Prime(b), Prime(c), Prime(d), Prime(e),
    
        % 2. There is no Fermat number between any of the five.
        ∀ n ∈ N:
            (a < n, n ≠ b, n ≠ c, n ≠ d, n < e) ⇒ ¬Fermat(n)
    
        % 3. There is no Fermat number less than the smallest of the five.
        ∀ n ∈ N:
            Fermat(n) ⇒ a ≤ n
    
        % 4. Any Fermat number greater than the largest of the five is not prime.
        ∀ n ∈ N:
            (Fermat(n), e < n) ⇒ ¬Prime(n)
    
I dunno how strict you need the syntax to be, so I've noted what I think to be all of the syntactic sugars I'm using:

- Percent `%` symbols indicate a line comment.
- Commas `,` and new lines indicate ""and"".
- Quantifier scope is written Python style.
- The less-than statements are chained together as a single statement.
- I used a set exponent (`N^5`) to combine multiple existential quantifiers.
",1538245168.0
Arancaytar,"The syntax doesn't quite match what I learned about PL, but that's mostly syntactic sugar and not important.

Your first statement says ""all Fermat numbers up to 65537 are prime"", which may be true, but is not the statement you wanted. The second statement says ""all Fermat primes are less than or equal to 65537"", which is closer to what you wanted, but still not strictly equal to ""the first five"" except that those happen to be equivalent.

> Ive already defined the predicate Prime(n) and FermatNumber(n), although Im not sure if I even need to define a predicate for a fermat number.

It's open to interpretation, but for the sake of completeness I'd say you should. What you're technically writing here is a ""shortcut"" for a less readable statement in more primitive symbols. Since ""FermatNumber()"" is not some arbitrary predicate, but defined in terms of arithmetics, it'd make sense to write that definition down - unless you're publishing a paper where you don't have enough space for the gritty details.

Assuming the predicates `FermatNumber()` and `Prime()`, I would say this captures your statement most closely: 


    ∃x1 ... ∃x5 
    (FermatNumber(x1)∧...∧FermatNumber(x5))
     ∧ (~x2 ≤ x1 ∧ ... ∧ ~x5 ≤ x4)
     ∧ (∀x6 FermatNumber(x6) ⇒ 
             (x6 = x1 ∨ ... ∨ x6 = x5 ∨ ~x6 ≤ x5)
         )
     ∧ (∀x6 (FermatNumber(x6) ∧ Prime(x6)) ⇒ x6 ≤ x5).

Effectively: ""There are five numbers, which are all Fermat numbers, which are distinct and in ascending order, such that all Fermat numbers are either equal to one of them or greater than the last of them (so they're the first five), and such that all Fermat primes are less than or equal to the last of them.""

Edit: ""Only"" is a bit ambiguous. The one I gave says ""there aren't any Fermat numbers other than the first five that are prime"". If you want ""the first five are prime, *and* they're the only ones"", you need ""Prime(x1) ... Prime(x5)"" in there as well.",1538245802.0
twsmith,"So, in mathematics, we'd say something like:

> ∀n∈ℕ  Prime(2^2^n + 1) ⇔ n ∈ {0,1,2,3,4}

or just

> ∀n∈ℕ  Prime(2^2^n + 1) ⇔ n < 5

We can write this in predicate logic as:

> ∀n∈ℕ ∀x∈ℕ ((x = 2^2^n + 1)∧(n < 5) → Prime(x)) ∧ ((x = 2^2^n + 1)∧Prime(x) → (n < 5))

That should be an acceptable answer, but we can shorten it. It is of the form `(a∧b → c) ∧ (a∧c → b)` which can be re-written as `a → (b ↔ c)`.

We thus get

> ∀n∈ℕ ∀x∈ℕ (x = 2^2^n + 1) → ((n < 5) ↔ Prime(x))

Or, to be explicit,  we could list the five integers:

> ∀n∈ℕ ∀x∈ℕ (x = 2^2^n + 1) → ((n=0 ∨ n=1 ∨ n=2 ∨ n=3 ∨ n=4) ↔ Prime(x))

(Everywhere using the convention that ℕ includes zero.)",1538246834.0
rill2503456,"Not sure about the rules of predicate logic/your class, etc., but is there a reason you can't define a binary predicate for IsNthFermat(x,n)?

Then you can basically do

    for all x, for all n
    IsNthFermat(x, n) && IsPrime(x) implies n ≤ 5",1538247518.0
Wurstinator,"If this is about formal predicate logic, it's always important to consider the structure you are working on, especially the signature.

I suppose in your case it's something like the natural numbers with addition, multiplication, and the natural ordering?",1538225707.0
Andy_Reds,"Your first formula is wrong. It doesn't take into account the ""only"" part of the statement. You can keep it and add to it ""if (n>65537 and FermatNumber(n)) then n is not prime.""",1538242376.0
bemend,"No, your first translation means ""if a natural number is between 3 and 2^16+1 and is a fermat number, then it's prime"" and the second one is ""if a natural number is prime and a fermat number, then it is between 3 and 2^16+1"". Your question is actually about a subset of the set of fermat number and its properties. You should do logical analysis for expressing each simpler predicate of the whole formula. For example, ""only the `X`s are `F`"" could be naively translated as ""(for any `x` belonging to `X`, `F(x)`) and (for any `x` not in `X`, not `F(x)`)"", which happens to be equivalent to ""for any `x`, `x` belongs to `X` if and only if `F(x)`"".",1538201943.0
jason2313,My English is terrible sorry,1538195896.0
taauji,"One way I can think of it is that with every iteration merge sort improves upon the structure of the entire array, whereas bubble sort is only concerned with one and only one digit in every iteration. But that is how I can only describe the comparisons in the first step where there are only pairs. I still cannot explain what the combine step does that makes it better than bubble sort.",1538158466.0
Putnam3145,">Bubble sort is straight forward finding the highest digit again and again.

This is actually a [radix sort](https://en.wikipedia.org/wiki/Radix_sort), unless I'm misunderstanding you.",1538174655.0
gaintchicken,"Intuitively the difference is the number of comparisons. If you look at a visualization of merge sort (like on wikipedia: [https://en.wikipedia.org/wiki/Merge\_sort](https://en.wikipedia.org/wiki/Merge_sort)) you'll see that people show the array in levels. On each one of those levels there's going to be on the order of O(n) comparisons. And because you're merging more and more parts together you're going to have log(n) levels, this gets you the O(nlogn) big O time. With bubble sort each time you get one element over to ""bubble"" to the top it costs on the order of O(n). Some of the later ones will be O(n/2) down to O(1) for the last one, but O(n) + O(n-1) + O(n-2)... is still going to be O(n). And how many times do you need to ""bubble"" an element to the top in bubble sort? n times. That's what makes it O(n\^2). ",1538354334.0
_--__,"Maybe a visualization will help:

[Merge sort](https://www.youtube.com/watch?v=kPRA0W1kECg&t=66)

[Bubble sort](https://www.youtube.com/watch?v=kPRA0W1kECg&t=241)

Basically Bubble sort's inefficiency comes from repeatedly ""sweeping through"" the array.  You compare every pair of elements - never making use of the fact that e.g. by knowing a<b and b<c we don't need to compare a & c.

Merge sort (and other recursive algorthms like quicksort) gain by essentially making use of this observation.  The ""merge"" operation (and the ""partition"" principle of quicksort) work precisely because of this property.  Critical to merge-sort's efficiency however, is breaking the problem into **equal-sized** subproblems.  This has the advantage of ""balancing"" the work of sorting across the levels of recursion.  For example, merge-sorting an array of size 16 will have 8 2-list-merges of lists of size 1; 4 2-list-merges of lists of size 2; 2 2-list-merges of lists of size 4; and 1 2-list-merge of lists of size 8.  At each level of the recursion we do essentially the same amount of work - which is how we gain efficiency.

It is worth noting that not all recursive sorting algorithms have this efficiency gain:  for example the naive implementation of quicksort is also O(n^(2)) - because you cannot guarantee the problem will split into two equal subproblems.  You can make quicksort more efficient by using clever pivot choosing algorithms (e.g. median or approximate median).",1538459789.0
DutchDudeWCD,"""cybersecurity"" is a very broad field. There's cryptology, software and hardware vulnerabilities, malware analysis, etc, etc. Some things where machine learning is currently being applied, off the top of my head, are sidechannel analysis, intrusion detection and malware detection.",1538232262.0
cpt_krc,"Give RITA a look from Black hills information security, or maybe it's under Active-counter-measures. Either way you'll find it, they do some cool stuff with ""AI Hunter"" and machine learning with malware beacon analysis. Might give you some idea.",1538233682.0
jmite,"Have they tried to address key pitfalls of previous proofs, or the restrictions from related results? Have they attempted to mechanize any of the critical aspects of the proof? Does it come froma reputable source? Have they submitted it for peer review?

If the answer to all of these is no, there's not much to discuss.",1538184990.0
east_lisp_junk,"> then the only chance to find a disqualification is when the
two numbers x − i and x + i are factors of a multiple of b 

This line of reasoning sounds fishy. Since b is a positive integer, isn't *every* positive integer a factor of a multiple of b?

It would help if when citing a book as the source for a particular claim you would mention the page numbers where that claim is stated and proven, especially when the book is 300+ pages of assorted theorems and conjectures.",1538170152.0
pala4833,I think they use computers.,1538149447.0
IdealImperialism,Do you mean things like 'login with Facebook/Google'?,1538148464.0
ProgrammerBro,"* RDS @ AWS
* Azure Database @ Azure
* Cloud SQL @ Google Cloud Platform
* Heroku Postgres Addon

By ""easy to access"" you mean how with Firebase you can just interact directly with the service in the frontend/browser right? You simply can't do that with SQL so you are going to need to write a backend.

&#x200B;",1538145683.0
johnVanDijk,Checkout Amazon RDS : [https://aws.amazon.com/rds/](https://aws.amazon.com/rds/),1538145203.0
cheald,"If this is a school (or other non-production) project, maybe go the other way - embedded! Look at [SQLite](https://www.sqlite.org/whentouse.html).

The short version is that you won't even need an external database server. It doesn't scale well to multithreaded or larger-than-memory workloads, but for small projects, it's excellent.",1538154566.0
Kel-nage,"What do you mean by ""easy to access""? Most relational databases use SQL when accessing data - would you consider that ""easy to access""? For a hosted solution options, Amazon offer either RDS (which can be backed by a variety of common relational databases), or Aurora, which is their in-house database that uses MySQL or PostgreSQL SQL APIs.",1538145415.0
ricardusxvi,"Google Big Query is easy to setup and use, plus you only pay for queries you run + storage. It’s meant for OLAP though, so it might not have enough performance w.r.t. concurrency & latency.",1538173914.0
felipesabino,"If by easy you mean having an API to interact with data, you could use https://graph.cool, define your relational entities using a graphql schema and also have all sort of extra advantages like permission handling and others.",1538180404.0
TracerBulletX,"I would install a prisma cluster and connect it to a relational db on AWS. Here is a gude. https://github.com/prisma/prisma/blob/master/docs/1.3/03-Tutorials/04-Cluster-Deployment/05-Prisma-Cloud.md#setting-up-clusters-with-prisma-cloud-and-aws-rds

That's probably the closest you're gonna get to a firebase backed by relational db.",1538157972.0
metaphorm,"there aren't any SQL databases that have a browser/javascript client interface so you won't find anything equivalent to Firebase. 

if you're committed to not running an app server your best bet is to use a ""serverless"" setup e.g. AWS Lambda function that interacts with an RDS database instance. 

I recommend just writing an app with an actual backend though. If you're most comfortable writing Javascript there's a lot you can do with Node.js on the server. ",1538152287.0
singham,Would google's spanner be what you are looking for ??  :https://cloud.google.com/spanner/ ,1538145594.0
NomNom150,"In general, if ++a is just a cpu cycle and a+1 is more, why don't we just use ++a-- since it is faster?",1538276999.0
TaXxER,Website looks shady. Recent issues contain mostly papers by authors from universities in Algeria and Bangladesh. I would stay away from that.,1538137423.0
bd-29,"If a feature is deprecated (in all cases in my own experience), the feature is either unnecessary or has been replaced. Even if I want the exact usage of the deprecated feature, I’m more likely to emulate it than directly use it.

Support eventually ends for these features, and you (and your software) need to be ready for that.",1538059945.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/askcomputerscience] [Do deprecated features cause you to react?](https://www.reddit.com/r/AskComputerScience/comments/9jd7ul/do_deprecated_features_cause_you_to_react/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1538057779.0
svick,"That article is light on details and I don't have the time to watch the whole talk right now.

Can somebody explain to me:

* How does an algorithm described in this system look like?
* How do you ensure your code actually implements that algorithm, if the two are described in completely different ways?",1538078554.0
Steve132,">Lamport shows us how to encode an algorithm in a set of predicates: logical statements describing the initial state of the algorithm, the transitions between subsequent states, and the required outputs and conditions for the algorithm to terminate

Functional and declarative(EDIT: and imperative) programming languages are still programming languages, and it's not necessarily the case that they are universally the best style to use for explaining every algorithm.  The procedural version of bubble sort is very straightforward.  I'm not sure the imperative version would be any clearer.",1538054273.0
akai_ferret,"It would certainly be ideal, but I'm not sure enough people in the industry have the mathematical education/familiarity to make that feasible.      

The self taught programmer, or the guy who hasn't thought about such equations since he graduated college 2 decades ago, are going to have trouble making heads or tails of it.   ",1538060738.0
Jerror,"I sincerely agree. It makes a lot of sense in my field, anyway. I do scientific computing and usually write my pseudocode mathematically, with discrete differential operators in index notation. Saves me a hell of a lot of work when I can express say 24 equations in one line on paper and write a function template or macro to express the code corresponding to each in one place. Some of my colleagues seem to just copy-paste and fiddle with indices 'til it (almost) works... ",1538058231.0
Roachmeister,"> we should describe our algorithms not with code – real or pseudo – but with mathematics

Interesting in theory, not so sure about the practice. I'd like to see the mathematics that can describe:

* MS Word
* Angry Birds
* Eclipse
* Linux

Or any of millions of other real-world programs. ",1538095277.0
uh_no_,"lamport is also a bat shit crazy genius.

source - the paxos paper.",1538058107.0
Prcrstntr,Why isn't code mathematics?,1538078477.0
,"He's 100% talking about TLA+

I met him, he loves it.",1538066829.0
bartturner,"The man that wrote part time parliament?

https://lamport.azurewebsites.net/pubs/lamport-paxos.pdf

About as opposite as you can get from using math.",1538052861.0
MisuCake,I rather not revisit the horror that is TLA,1538127334.0
CrackenZap,The pathway to hell is paved with good intentions...,1538139818.0
pcbeard,Terrific talk. Worth your time to absorb all of it.,1538063626.0
trichotillofobia,Still doesn't guarantee there are no bugs in your code.,1538079376.0
practicalutilitarian,"But mathematics notion is ambiguous. Code is precise, repeatable, and has syntax rules and typing that can be checked without even evaluating or executing it. And machines can even compose and optimize code. Though many math expressions can be factored, evaluated, optimized, generated, and simplified by machines, those expressions tend to look more like code than like math.",1538089114.0
boring_dont_bother,"No meaningful contribution here, but I definitely feel like gravitational time-dilation is a case of ""the computer is busy at this time"".",1538150777.0
OnionWayWay,"This looks very interesting, I’ll definitely try to read it at some point even though I probably don’t have the relevant understanding of physics concepts.",1538171087.0
icecubeinanicecube,"This is the most interesting thing I have read in quite some time, thank you.

When are we going to tell the Physicists that we CS guys now handle their work? :D",1538234575.0
The_MPC,How do you reconcile this with the fact that Galilean mechanics are self consistent but don't feature time dilation?,1538328482.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/math] [Using Information Theory to Explain Time-Dilation](https://www.reddit.com/r/math/comments/9jnzig/using_information_theory_to_explain_timedilation/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1538149044.0
Kel-nage,"You're going to have to give more context than that to explain what you're talking about! Are you talking about computer keyboards? What does a keyboard program do exactly? What are strikes? What are you trying to achieve?

But this also seems unlikely to be a topic that is computer science related.",1538045185.0
Seths_Revenge,"The 7 figure Business major likely also have an MBA. Beyond that their success is more likely a factor of their actions after college, than their degree. You’re just as likely to be successful with a comp sci BS and then getting an MBA when your career is ready for it. 

Your career progression doesn’t have to be as rigid as some people seem to think. There’s no reason you couldn’t work in a traditional comp-sci role for a few years then transition to group-lead or another small scale technical leader, and then it’s a fairly natural progression to management, especially with an MBA. 
I want to stress, you should work for a while after your BS, start your career progression and then when your career demands it, get an MBA. The lessons will carry much more weight and you’ll retain the information much better once you’ve worked full time for a few years. Plus, you’ll actually know if you need it or not. 

I would lean toward the BS over the BA, but I’m not really qualified to judge. 



",1538009347.0
winterIsLeaving,"There's really not much of a difference between a BA or a BS in cs (if it's a curriculum worth paying for, the math classes will be required and in your curriculum and also you won't be able to count them towards a bs as they're included in your field of study). Nobody really cares about the details of a degree just that you have it unless you're going further in academia in which case they care what classes you have.",1538010938.0
MachineInTheStone,"If I were you I would pick a project to work on. Just find a tutorial and go from there.

Just to see if you like coding.

After that, I would ask myself if I liked doing it or not. Money should be only 20% of your decision making. I am in school right now and the people that I see failing the quickest were the ones who always talked a big game about their connections, but didn't know what we were learning that day in class. You need passion. You don't need to have it right away, you just need to not hate it. Later on, you can make the decision of whether or not you like coding or not. 

A good starter language is python or java. 

Getting a ba in comp sci in my opinion is stupid. The reason being is that there may be a whole lot of classes that you will want to take in your 3rd or fourth year that you can't take because you don't have the maths. For example, do you like the idea of coding artificial intelligence? Math. Do you wanna build a robot? Math. A whole other more specific stuff require math. I'm not kidding when I say that I've taken at least 3 required courses where they need you to know calculus. 

As a comp sci major you will start out between 40,000 - 70,000 depending on what state/city you live in and your own demonstratable skills. A senior developer can make a whole lot of the moneys but it requires a lot of work. Being a programmer requires a lot of self-learning, thinking, and tenacity.

If you suck at math, I highly suggest getting good. Like really good. There will be people who tell you that comp sci and math are entirely different. Those people will either fail out or prove me wrong and get a degree along with a subpar gpa. If the father of computer science was a mathematician, do you really think that cs has little to do with math?

A big part about college is self learning. You have to be willing to open up a textbook, look at an online course, or start a project.

I'm guessing that you aren't very good at math and that comp sci seemed cool because it didn't have physics, uses computers and they make a lot of money. 

Thats okay. I thought the same things.

I spent a long time trying to get good at math and now I'm currently taking linear algebra. If I can do it, you can do it too.  

You dont need to go to a top school to be a great programmer. I've met literal geniuses who attend my state school. Just pick a good state school that has comp sci and go there. If they have a graduate school for comp sci, then thats a sure sign that there must be something there thats of worth.

Good luck. I'm not editing this as I want to go to bed. Got a algos quiz tmm!",1538035328.0
Jumhyn,"In some cases, the CPU does actually ""guess"" the answer--this is called [branch prediction](https://en.wikipedia.org/wiki/Branch_predictor) and is a foundational concept in modern CPU design. In other cases, the CPU and/or the compiler is smart about identifying instructions which are independent from one another and grouping them so that they can be executed simultaneously. Processors can also detect when an instruction depends on the output of an instruction that is ""in flight"" and stall in order to wait for the input to be ready. You can read more [here](https://en.wikipedia.org/wiki/Instruction_pipelining#Solutions).",1538010443.0
Madsy9,"If we simplify and use the old MIPS RISC model, then the pipeline works by figuring out which instructions in the pipeline are dependent on each other, say if one instruction reads a register which is written by the one before it. Such exceptions are called ""pipeline hazards"" in the nomenclature. In the MIPS model, dependencies like that are resolved by stalling. There might also be dedicated routes back to previous stages to enable some stages to quickly communicate without having to stall. For example, it is common for the load stage (which loads register values from memory) to have dedicated routes back to the stage which reads the values of registers. Without those routes, the younger instructions would either load the wrong value or have to stall almost the whole pipeline.   
Stalling does not necessarily stall the whole pipeline; it might just stall the single dependent instruction while the other instructions advance further in the pipeline if there are vacant stages left. Vacant pipeline stages (no instruction is currently served there) are called 'bubbles', because they ""bubble up"" in the pipeline. A stall at a pipeline stage creates a bubble in the stages proceeding it for each cycle the stage is stalled. Bubbles ""pop"" by having previous stages fill them up whenever a stage further down is stalled. All of this ensures maximum throughput. And of course, if instructions in the pipeline are independent of each other, then they just advance forward with no stalling.

However, in modern superscalar CPUs, the processors have advanced internal logic that are capable of doing instruction reordering as long as the programmer's view of the pipeline is consistent. That is, by peeking at the current state of the pipeline stages, the pipeline logic can move instructions around and turn dependent instructions independent and thus avoid stalls as long as it does not change the final result in any way.

Do a web search for ""pipeline hazards"" if you want to study this topic. It's not super complicated to learn the MIPS pipeline model, but getting implementations right in VHDL/Verilog or RTL is a challenge.",1538010957.0
combinatorylogic,"If your forwarding path is too long, you can delay issuing an instruction until all of its arguments are ready. This is exactly how Out-of-Order execution works. For details, see Tomasulo algorithm for example. While waiting, you can issue the other instructions that do not depend on anything that is in flight at the moment.
",1538039799.0
Kel-nage,"Yes. For example, see https://omereingold.files.wordpress.com/2014/10/sl.pdf",1537997375.0
Jaxan0,"If n is the number of edges in your graph, then a DFS (or BFS) suffices. ",1538000537.0
zcra,"Personally, I got very little of value from the blog post, despite my hopes to the contrary.

I have interests and educational backgrounds in many areas, including engineering and liberal arts, so I'm quite interested in connecting ideas across fields. However, I believe such linkages are best when done carefully, clearly, and insightfully.

In summary, the article was hand-wavy without insight. The offered metaphors were not meaningfully connected to current realities of machine learning. Worse, I am confident one could flip the Greek and Babylonian claims and end up roughly at the same place. Put another way: if a claim can be inverted without changing the impact, it might mean it is so vague as to be untestable.

I have constructive feedback based on what I read. (I can't speak to the author's process or intentions, only what I see on the page.) Here is what I would offer: With respect, put this piece aside for now. Don't try to force metaphors too soon -- get comfortable with the complexity and history; this will help open doors for deeper insights later. What I see right now seems like a metaphor looking for a reality. Instead, why not go deep into some details? Pick a discipline, and look at particular aspects of machine learning from a new perspective. Try approaching it like a historian... or an ethnographer.

For example, I think you can find fascinating material about any of hundreds of sub-topics. Perhaps dig into how back-propagation has shifted and evolved. Or how logic-based approaches wax and wane throughout the decades. What explains the interest in Bayesian methods? Whatever you do, look at how people are involved, how connections are made, where the money comes from, and/or how and why accomplishments are recognized. Once you've got many pages of interesting material, you might naturally start connecting the dots with metaphor. But please don't force it; just let the stories emerge.",1538276233.0
Feynmanfan85,"No, it does not depend upon the particular computer used, as far as we know. 

You should look up the Church-Turing Thesis. In short, it is a hypothesis (not a theorem) that all purported computers are either (x) equivalent to a Universal Turing Machine (i.e., the purported computer and UTM can do the same things), or (y) can be simulated by a UTM (i.e., the purported computer can do only some of the things a UTM can do).

The halting problem is an example of a non-computable problem, which is a problem that cannot be solved by a UTM. Unlike the Church-Turing Thesis, which is a hypothesis that has apparently turned out to be true as an empirical matter (i.e., every computer ever proposed has been proven to be equivalent to a UTM), the halting problem is a deductive theorem (i.e., it is known to be true with certainty that a UTM cannot solve the halting problem).

I wrote a simple proof of the halting problem on my blog, which you can find here:

http://kalleskultur.tumblr.com/post/42465090186/computability-and-the-limits-of-human-knowledge

New models of computation could of course be invented using new processes of nature, and at the risk of self-aggrandizement, I actually kicked off a thread about this exact topic which you can read here:

https://www.reddit.com/r/QuantumComputing/comments/9dhov1/computability_of_nature/

So, in short, the answer is no, it does not depend upon the computer, as far as we know. If, however, someone invents a machine of the type I describe in the thread above, then yes, in theory, we could solve not only the halting problem, but possibly all non-computable problems.

",1537970180.0
flexibeast,"In addition to u/Feynmanfan85's answer, note that the von Neumann architecture dates back to 1945; Turing's proof about the halting problem predates that by nine years.",1537971527.0
sfultong,"To add to the already good answers posted so far:

The halting problem is a question of semantics. If you choose a language that isn't Turing-complete, you won't be subject to the halting problem for programs you write in that language, even if you run them on Turing-machine-equivalent hardware.",1537978586.0
djimbob,"You should also note that while the Halting Problem is provably in the class of undecidable problems (easy to prove by creating pathological case based on assumption that halting problem is solvable), tasks that ultimately boil down to the halting problem are often computationally tractable for common examples.

E.g., asking whether a function/line of code will ever be executed in a problem can be reduced to the halting problem.  To do the reduction -- the halting problem on whether the function `f();` will halt is equivalent to asking whether `g()` will ever be called in the code `f(); g();`  (if `f()` halts, then `g()` will be called; if `f()` doesn't halt and goes in an infinite loop, `g()` will never be called).

However, IDEs and compilers can easily check for many common scenarios with unreachable code (even if they can't diagnose the pathological cases).",1537996665.0
wakka54,"What inspired this question? Everything I've read about turing machines screams universal, so I'm lost and confused what you could even mean by relative?",1538000637.0
Responsible_Virus,no its universal.its however its my understanding that real computers aren't Turing machines as they have finite memory and a model that reflects that aspect of real computers would be a linearly bounded automaton for which it can be decided if the program halts.,1538019093.0
OGsambone,Webdev:*sweating* ,1537978361.0
GreatDaynes,"Thank you for sharing this! 

I'm studying and becoming more and more interested in automated/dynamic UI generation. This is fascinating! ",1537971961.0
zagaberoo,Please use descriptive titles instead of totally generic clickbait. . .,1537981527.0
vReddit_Player_Bot,"Links for sharing this v.redd.it video outside of reddit

|Type|Link|
:--|:--
|Custom Player|https://vrddit.com/r/programming/comments/9j1alv|
|Reddit Player|https://www.reddit.com/mediaembed/9j1alv|
|Direct (No Sound)|https://v.redd.it/x8g50pcr8ko11/DASH_1_2_M|
*****
^(vReddit_Player_Bot v1.2 | I'm a bot |) ^[Feedback](https://www.reddit.com/message/compose/?to=vReddit_Player_Bot&subject=Feedback) ^| ^[Source](https://github.com/vrddit/vrddit.github.io) ^| ^(To summon:) ^[u/vreddit_player_bot](https://www.reddit.com/u/vreddit_player_bot)",1537970823.0
shaggorama,[pix2code](https://arxiv.org/abs/1705.07962),1537990814.0
_georgesim_,What's the point of this sub if half the posts are/should be in /r/programming?,1538006675.0
mrexodia,Is there some actual AI here or just matching squares with OpenCV and then a bunch of heuristics to generate the HTML?,1538037041.0
nicocappa,Developers will put developers out of a job. ,1538037506.0
WebNChill,"Dude. This.  Is.  Fucking. Awesome. 

I've been burnt out recently with my first programming class. Learning C++, and the material didn't prep me for the final project so I'm a little frayed. 

But this video got me pumped. This is why I wanted to learn how to be a software engineer. Fucking wicked man.",1537976787.0
locotxwork,"Uh . . okay . . so . . what was so difficult about pulling up freaking HTML editor (hell even notepad) to create a 6 div page?   How does it handle when you erase the edge? Is it just seeing boxes?  See back in the mid-90's, I saw this cool WYSIWYG html editor that forced the company worked for to spend thousands on a Silicon Graphics machine just because of that editor.  The though was, ""oh it's easy to make web pages, we can do things sooo fast and streamline creating content now"".   However, when we started using it, you couldn't use style-sheets and the render engine didn't know how to handle a crap load of other HTML elements and would crash.  Also, we told them, you are going to need javascript support for what you want to do.  And they were like, ""that's coming in the next version"".  I never came.  So, it may look cool, but is it functional.  What problem is it solving?  (Thoughts?)",1537980890.0
Techno_Jargon,This actually looks like a lot of fun!,1537973247.0
slaphead99,This is fantastic. I hate Visio.  ,1537975018.0
mayhempk1,That is absolutely insane.,1538004258.0
avoutthere,We all knew this day was coming.,1538004569.0
HolidayWallaby,Is this YOLO?,1538040573.0
zed1025,I saw the same post on LinkedIn,1538051094.0
pulsar512b,"While the code may not be quite as high quality, I think that in 10 years, a computer could turn wireframes into code better than humans, and in time, make the wireframe.",1537983234.0
,[deleted],1537972787.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/computervision] [Using Information Theory to Compare Images](https://www.reddit.com/r/computervision/comments/9j24v9/using_information_theory_to_compare_images/)

- [/r/math] [Using Information Theory to Compare Images](https://www.reddit.com/r/math/comments/9jf02u/using_information_theory_to_compare_images/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1537965855.0
613style,"Conditionals are nested like...

    (cond
      [(smells-good? smell) (cond
                              [(food-i-like? type-of-food) (cond
                                                             [(not-too-far-over? days-over) 'eat-it
                                                              else false])
                               else false])
       else false])
",1537913998.0
zokier,"Start building your own primitives from simple conditionals; AND should be easy enough, as is NOT. Combining those two you get NAND, which is [functionally complete](https://en.wikipedia.org/wiki/Functional_completeness). After that you can do whatever complex boolean operations you want",1537914606.0
comtedeRochambeau,"You're on the right track.  If you're not allowed to use `and`/`or`, then some of your tests will lead to new `cond`s.  You can fill in the blanks. 

    (define (can-i-eat? smell type-of-food days-over)
      (cond ((equal? smell ...)
             ...)
            ((equal? type-of-food ...)
             (cond ((<= days-over ...) ...)))
            ((equal? type-of-food ...)
             (cond ((<= days-over ...) ...)))
            (else ...)))

Alternately, as u/zokier suggested, you can define your own and/or procedures although they won't behave exactly the same as `and` and `or`. Your teacher is probably building up to something like this. 

BTW, in Racket and Scheme, names that result in Boolean values end with `?` by convention, and `define` must be lower case. ",1537918286.0
bor0s,"Note how `and` and `or` can be defined in terms of `cond`:
```
(define (my-and x y) ; chained conds
  (cond (x (cond (y y)
                 (else #f)))
        (else #f)))

(define (my-or x y) ; separate cases
  (cond (x x)
        (else y)))
```

Interacting with them:
```
> (list (my-and #t #t) (my-and #t #f) (my-and #f #t) (my-and #f #f))
'(#t #f #f #f)
> (list (my-or #t #t) (my-or #t #f) (my-or #f #t) (my-or #f #f))
'(#t #t #t #f)
```

Here are more generic `my-and` and `my-or` which accept a variable number of arguments, but the structure explained above is a bit implicit:

```
(define (my-and . x)
  (cond ((null? x) #t)
        ((null? (cdr x)) (car x))
        (else (apply my-and (cdr x)))))

(define (my-or . x)
  (cond ((null? x) #f)
        ((car x) (car x))
        (else (apply my-or (cdr x)))))
```",1537950355.0
PinkyWrinkle,I’m in the exact same position. I’m working on a project and I just chose a stack that I thought would be easy to learn. Vue + Firebase. ,1537923703.0
mulletlaw,"Doing a web application in C++ would be pretty heavy.  Im not sure that it's really even done these days.  I would start with something easy to prototype like Django, Rails or if you want to stick with Java you can get familiar with Spring but the barrier to entry is a bit higher and the boilerplate to get it working is more tedious than other frameworks.  If you want to use this on a portfolio pick a popular front end framework like React or Vue. Or just pick something you think is interesting an build it using that.",1537925597.0
NovaX,"I co-authored the paper on TinyLFU. It was fun and we have a follow-up that I can send privately (conference in December). Those try to solve caching problems using probabilistic data structures.

Another paper I enjoyed was on TimerWheels, which I implemented for the cache. Its cute, efficient, and should be more widely known!

Papers on concurrent structures are fun.

Beyond that, you might want to be more focused on a problem domain. There are a lot of good (and bad) papers to sift through. I'd get into the habit of skimming before a full read, even if the abstract sounds wonderful.",1537927912.0
charlesvdv,"The best way would be to ask the company if you can but I don't have a very high confidence of them saying ""yes"" (at least if you have been paid to do the job). 

What you can do in other hand, is describing what you did in your resume, LinkedIn profile or even write a small blog post on Medium. This way, recruiters will be able to see what you have achieve without sharing any company property. ",1537897979.0
future_security,"Dijkstra's algorithm for graphs with non-uniform edge weights. Breadth-first search for unweighted graphs. (The latter is your case, since I see you're only adding one to distance.)",1537902270.0
thehamslammer,"I think the most efficient solution will be to traverse your graph starting at the target node, and then assign distances to connected nodes as you go. So, the target node’s distance in 0, all its connected nodes are 1, all the connections to level 1 nodes that have no assigned distance are level 2, etc. That will make the process iterative rather than recursive, which will always be faster (due to processor design rather than something algorithmic). 

Essentially, make each iteration loop through the nodes connected to nodes that were assigned a level in the last iteration. If a node has a distance, continue. If not, it is at level n, where n is the number of iterations we’ve completed. I’d write up some pseudocode for you, but I’m on mobile and the formatting would be trash :(

In algorithmic terms, I think both your implementation and mine would visit each node no more than once, but I’m not 100% sure about yours from the pseudocode. I don’t think you’re gonna get away with not visiting every edge no matter what you do though (excluding the early out condition of having every node have an assigned difference). In any case, you’re looking at roughly 280 million operations to finish your computation.

Be prepared to leave this running for a while! In something like this, every tiny pointer dereference makes a difference, so try to make the body of your loop as short and sweet as possible. If it ends up taking too long, or you need to do this often, you could look into optimizations like Loop Unrolling to make sure you’re taking advantage of branch prediction. Good luck!",1537901825.0
RandomTutor,"Performing BFS on a slightly modified graph would be the fastest. 
Take every edge in the graph, and flip it. Perform BFS from starting node. And you have the “Fastest” running solution. ",1538236825.0
future_security,"There are (broadly speaking) two types of APIs for things like JSON and XML: streaming based and syntax tree based. 

A streaming API just tells your program what the JSON parser has found in real time. (Via callback functions.) If your program needs to remember what it's seen so far then it is the API user's (a programmer's) responsibility to write the code to save the part of the data they consider relevant. This allows the parser itself to use a minimal amount of RAM. (Just a little bit more than the longer token in the JSON file.) 

A streaming API works like a live radio broadcast. You get information as soon as it becomes available, but unless you have a recording you can't jump around to different parts of the file in random order.

A tree based API either reads an entire file into RAM or creates something like a database. You can do any kind of query because the entire document is available in memory in tree form. It's a lot more convenient but it can involve a lot of overhead if you don't need a sort-of-random-access interface to the entire document tree.

 * Solution 1 - Write an R program that uses only streaming APIs and online algorithms
 * Solution 2 - Use a streaming API to read in JSON then write out sub-sequence of the big array to new JSON files. (It's not even clear what would be a meaningful way of splitting the files up unless most of the relevant data is all in one array.)
 * Solution 3 - Ask your university to use a computer with 16 GB of RAM or rent something ""cloud"" with enough memory.",1537898076.0
skeeto,"How is it formatted? Separate JSON objects? One big JSON object? One JSON object per line?

If it's separate JSON objects, you might be able to use something like [jq](https://stedolan.github.io/jq/) to slice and dice it, or even change formats, before your own application consumes it.",1537896030.0
jmite,"Am a PL PhD student. The only language specs I've been read have been for core calculi that fit on one page.

In general, the key is to understand the concepts behind the languages. Understanding control flow, recursion, types, functions, threads, etc. 

All mainstream languages are built from these same building blocks. When you understand a few, it's easier to see past syntax and know what's really going on.",1538013987.0
flexibeast,"> a new programming language, say ReactJS

Well, er, that's not a programming language; it's a JavaScript library.

At any rate, when i'm looking at a new language, i rarely start with the spec (assuming there is one, which there might not be). i ask things like: Does this language work within a particular programming paradigm, or is it multi-paradigm? If the former, how strictly does it want to stay within that paradigm? What languages have influenced its design? What areas does it want to be used for? What features does it have that help deal with concrete issues i've had when working with other languages? Does it have features that i've found problematic when working with other languages? Essentially, many of these questions are asking: what trade-offs does this language make, and why?

Designing a programming language that people will actively want to use is *hard*. As [Stroustrup said](https://en.wikiquote.org/wiki/Bjarne_Stroustrup):

> Anybody who comes to you and says he has a perfect language is either naïve or a salesman.
>
> There are only two kinds of languages: the ones people complain about and the ones nobody uses.",1537888356.0
east_lisp_junk,"Very few languages' specs are small enough to be a cost-effective way to spend your time learning a new language. Also, a big part of being productive in a language is familiarity with libraries related to what you're trying to accomplish. Having spent a lot of time examining/exploring the design space helps you recognize when something is a new veneer on something you've seen before, but the usual PL research techniques are not themselves all that helpful in picking up another new language.",1537894028.0
future_security,"Well, since I already know Java and C# (plus a dozen major languages other people like that I'd rather not use), often learning the syntax of a language takes two minutes and ""learning"" the basics of the standard API takes 12 hours because the documentation and overall structure is so disorganized.

It always feels really tedious since I'm already used to decent APIs. It's a bit like cooking in another person's kitchen. Except I'm going from neat organized kitchens to less organized ones. PHP, Python, Pearl, and even Rust involves more searching the web for ""how to ___ in ___"" than I would like. 

Things have gotten a much better, but to use the kitchen analogy again... Sometimes I need to find a spatula. I check all the logical locations and can't find one, so I ask the owner. ""Oh. Spatulas. Right. Check in the back of the fridge between the milk and the mouse traps. Up vote if you found this answer helpful.""",1537901310.0
janepe4,"Make a list of things like ""chmod file, read from file, write to file, make a new thread, run code in parallel"" etc. and try to implement it.",1537899064.0
insburgnis,"“I can start an automated machine learning run, go home, sleep, and come back to work and see ~~a good model,~~” windows update is still running",1537884851.0
volcanicBird,“I created a program to build itself and now it’s... oh shit... stop that... no no no... we’re all fucked”,1537907111.0
ctrlnode,From the heading and totally ignoring the article it sounds like they have created AI that is capable of automating the development of itself. ,1537949154.0
bartturner,Sounds like AutoML?    Curious does it use TF or CNTK?,1537957843.0
water-and-fire,"Yeah no other companies like Google has AutoML among others libraries for doing similar things for quite some time. Even open source libraries like hyperopt and sklearn have had the capabilities for hyperparameters and models tuning for a long time.

Not sure what the “news” is here. That Microsoft is finally catching up? 

Weird corporate propaganda.",1537884573.0
justneurostuff,"im having a bit of trouble making sense of what this does. is it saying that if i have labeled data it'll do the model selection, training, and so on for me? does it handle feature selection/extraction, too?",1537884425.0
skulgnome,And so the circle jerks itself.,1537956772.0
abdu1_,This is excellent,1537891717.0
reddstudent,That’s insane.,1537883743.0
TaXxER,"Makes post title:

>What I learning while making an app with 100k+ downloads, ... when I was 17-yrs old

Writes in the blog post:

>Stay humble

&#x200B;",1537960498.0
okkshin,"Empiricism, trial and error, and experience from similar applications.",1537869699.0
CorrSurfer,"Here are a few:

- Memory requirements
- Computation time (Theoretical complexity / ""Big-O Notation"")
- Computation time (measured on similar workloads in practice) 
- Approximation quality (if approximate computation is possible)
- If computation time limit guarantees are needed
- Ease of implementation in the environment in which the algorithm is supposed to run in
- Experience of the engineers who support the application after deployment",1537875826.0
nerdshark,"Uh, that depends on the application. This question is impossible to answer with zero context.",1537858246.0
SubtleGenocide,"print(""hello, world!"") 

I shit you not. ",1537849186.0
TyroneSlothrope,"Not sure if this can be categorized as 'game-changing', but it's a piece of code that I'm most proud of. I wrote a service for executing jobs on a distributed network. Now I know that's not a new idea. There are a couple of softwares that get the job done. But this was specifically targetted at media industry for heavy computational work (the use case was something that hadn't been done yet). My program not only split the execution/tasks among the peers, but also logically split the data to be operated on and merged the final data. Unfortunately I can not share the code because of IP issues. :-)",1538029601.0
snarfi,"cout << ""Din post isch voll daemlich"" << endl;",1537877612.0
zoba,Just so you all know: /r/turingmachines,1537838647.0
NotSureIfImMe,"Hi, this was a crosspost but you can respond here. Thank you :)",1537827533.0
wtallis,"It depends on what you mean by buffers. If you're referring generally to memory spaces allocated for short-term storage, then there's not much room for alternatives. If you're referring to specific data structures, algorithms and access patterns, then there's almost always an alternative if you're creative enough. For example, Van Jacobson has recently proposed replacing FIFO packet queues in network drivers and NICs with [timing wheels](https://www.files.netdevconf.org/d/46def75c2ef345809bbe/files/?p=/Evolving%20from%20AFAP%20%E2%80%93%20Teaching%20NICs%20about%20time.pdf), but packets waiting to be sent still have to be stored somewhere that could reasonably be called a buffer. ",1537819727.0
Jaxan0,"I wouldn’t say Turing machines work on “buffers” (although, I’m not really sure what you mean by it). So, no, they are not fundamental for computation. ",1537824818.0
ryani,"The video glosses over this, but there are (surprisingly small) Turing Machines that are ""Universal"", in that they can be used to simulate any other Turing Machine by starting with an appropriate encoding of that machine on the memory tape.

What this allows is that you don't need to build a custom machine for every program -- you can build one machine and encode the program into its memory dynamically.

This has extremely far-reaching consequences, and forms the basis for how we know that certain values are *not* computable.  A fun example is the [Busy Beaver](https://en.wikipedia.org/wiki/Busy_beaver) function, which grows faster than any computable function.  Here are the first few values for a 2-symbol turing machine:

N | BB(N)
--: | --:
2 | 4
3 | 6
4 | 13
5 | >= 4098
6 | >= 3.5 x 10^18267
7 | >= 10^10^10^10^18705353

Nobody has been able to find the value of even BB(5), and the value of BB(8000) [cannot be found by modern set theory](https://www.scottaaronson.com/blog/?p=2725) because an 8000-state turing machine can encode rules for generating every possible proof in ZFC.  The actual bound where it becomes unprovable is probably much lower, but it's amazing to me that there is such a simple problem that we know mathematics literally cannot solve.",1537808214.0
But_Mooooom,"The elementary place to start for you seems like ""logic gates"". These are the building blocks for implementing ""instruction sets"" of a given processor.

Logisim is a common simulation software for introductions to these concepts via hand-on learning.


Sorry if I completely missed the premise of your question, but it seems to be the direction you're looking for...",1537807196.0
remy_porter,"First thing first: a Turing machine is a hypothetical construct. Turing used it to define what he meant by ""computable"". With that model, he was able to make some very important proofs.

Let's say we wanted to build a Turing machine. And for the sake of argument, our only parts are a pen, gears, a hand crank, an infinitely long tape, and a camera which can read whatever symbol happens to be near the pen. Again, conceptual machine.

In this scenario, how would we build a Turing machine which does unary arithmetic? Well, we'd have to find some clever mechanical solution involving how we arrange the gears. If we wanted to build a Turing machine which finds primes, that would be a different arrangement of gears.

But, what if I wanted to build a Turing machine which emulates an arbitrary Turing machine? Like, why not just have one really complicated arrangement of gears, write down some instructions on the tape, and then load the tape into the machine and let it run? *I can do that*! There's math to prove it. There is a possible Turing machine which can execute any *arbitrary* Turing machine. Call it, maybe, a Universal Turing Machine.

In fact, if I wanted to shortcut this, I could come up with some system for encoding different Turing machines as numbers. Like, Turing machine ""5731"" finds the fiftieth digit of pi, while Turing machine ""12854381"" scribbles a bunch on the tape, but so far as I can see, *never terminates*. So I write ""5731"" on the tape, load it into my Universal Turing Machine, and my UTM pretends to be a specific arrangement of gears which finds the fiftieth digit of pi.",1537808558.0
Gavcradd,"The instructions are just that - instructions to perform an operation. The underlying hardware deals with actually performing those operations. So for example, adding two numbers up can be done using hardware called a full adder (or more accurately, a series of these). Look it up.  Multiplication can be done (somewhat) using shifts, where binary values are shuffled up a register. OTher more complex instructions will have their own hardware, or use combinations of existing hardware.

Have you had a look at the Little Man Computer? Google it, I use it with my Y10 students in the UK (14 year olds). It's a cut-down simulation of a processor in Von Neumann architecture. Each of the instructions in this would be a separate ""chunk"" of hardware.",1537807360.0
theblacklounge,"Didn't watch the video but there are two possible cases:

A simple TM has rules. The rules are part of the TM itself. 

An interpreting TM has rules and reads rules. The rules which are part of the TM itself define how to read and execute rules on the tape. How the rules are notated on the tape is arbitrary and complex but trivially possible so the notation is often left undefined in TM proofs. 

There is no difference except for notation with these kind of interpreters and a practical, really implemented interpreter for Python for instance. And like you mention, yes you can theoretically do infinite regress, but it has to be bounded for any given case. You can make a Python interpreter which runs a Python interpreter which runs a Python interpreter which runs a Python which runs a Python program. Any scheme you come up with, I can trivially find a scheme that goes one level deeper, even though an actually infinitely deep scheme isn't possible. ",1537807882.0
KDLGates,"There were a few hardware electives that were ""expected"" (not mandatory but a good idea) for Computer Engineering degrees at my University, like electrical systems (to study things like the physicality of transistors and how to read a DRAM memory diagram) and VLSI design.

Do these courses also normally factor into an Electrical Engineering degree program? Or do they not cover implementation of logic design beyond logic gates and symbolic design work like done in LogiSim / Logic Friday, reducing truth tables to KMaps and such?",1537808829.0
holden_nelson,"Since others are mentioning logic gates, the book Code by Charles Petzoid was insanely valuable to me in learning this and more about computers.",1537812215.0
sturmhauke,"Think of your basic [Von Neumann architecture.](https://en.wikipedia.org/wiki/Von_Neumann_architecture). The CPU has a set of valid instructions it responds to, and states that it can be in. Memory contains the program to run, and data to operate on (which can even be itself or another program). Other components handle I/O and external storage. This is essentially a more real-world implementation of a Turing machine, the primary difference being that the Turing machine is assumed to never break down or run out of memory.

Once a program and data are loaded into memory, the CPU starts going through each location and looking for instructions. These essentially break down to a handful of possible actions:

* read the contents at the current address
* write something to the current address
* jump to another address
* change the CPU state
* perform I/O
* execute another action
* halt execution

There are complications on top of that when you start talking about actual hardware and software, but fundamentally that's what's happening. Again, this is essentially a practical Turing machine with unavoidable limitations.",1537817680.0
euzinkazoo,"**TL;DR; Moving back and forth between each instruction**

The turing machine does not really exist, it is a theoretical machine a ""step-up"" from an automata, it is defined as an 7-tuple that includes states and transitions, moreover a turing machine is not necessarily an universal turing machine, you could build a turing machine that adds two numbers and it would not be able to divide numbers for example, but why is it so powerful?

A complete or universal turing machine is proved to be able to simulate any other turing machine, and thus if you can construct an algorithm using the turing machine formalism you could take any other turing machine and ""program it"".

So to finally answer your question, I could use another question: **how a program that is on the computer's memory able to modify it?** It uses two different memory sections to do so. The turing machine does the same, using a series of states and transitions to move back and forth between to access ""data"" and ""program"" portions of the tape.  


P.S.: If you want to learn more about this search Computer Theory.",1537826764.0
skulgnome,"Control flow is encoded in the ""next state"" field. IOW, each instruction is followed by a GOTO.",1537845446.0
Workaphobia,"Didn't click the link, but briefly:
Turing machines have a hard-coded program (their state machine) for processing the input. This is analogous to the physical circuits in a cpu.

But one of the most interesting things about Turing machines is that there exist universal Turing machines - machines whose program says ""read another program off the input tape and do what it says step by step"". In this case, the hard-coded program doesn't change, it always says to run the program on the tape, but the program on the tape can be anything. This is analogous to a computer running an interpreter or emulator.",1537880637.0
jhp2000,"Turing Machines have a built-in set of ""instructions"" (state transitions).

If you're passing in the instructions, then you're using the built-in set to emulate another set of instructions.

Many instruction sets are ""universal"", meaning that with the right program they can simulate any other Turing machine.

There's no infinite regress because there is a special set of instructions that is built in. The Turing machine is physically constructed to follow these base rules. The point is that, even though it is constructed with state transition table X, you can use that table with an appropriate emulation program to determine the output of a different transition table Y, which you pass in on the tape, i.e. ""in software"".",1537808365.0
dabombnl,"No, because HTTP clients are typically not servers and can't receive requests. And the HTTP paradigm does not allow for resources to exists on clients. Additionally, firewalls make this extremely difficult and impractical.

But the need for server push without polling is still there, which is why other mechanisms were implemented. Like WebSockets, long polling, and HTTP/2 Server Push. I would look into those instead.",1537804955.0
ACoderGirl,"HTTP is pretty low level. Something like a callback arguably should and sometimes *does* take place at a higher level.

/u/dabombnl points out a good reason that this can't happen simply. To elaborate and use an example, WebRTC is a technology for real time communications in a peer to peer fashion. The nature of it requires that a peer can connect to your computer and that requires a predictable public IP.

NATs can make it such that a computer doesn't actually know its public IP (as the NAT transforms it). [STUN](https://en.wikipedia.org/wiki/STUN) is a technology that WebRTC uses to thus determine the public IP and *if it's usable* (some forms of NATs make it *impossible* to have true peer to peer communications). WebRTC does actually have a way around that, which is using ""TURN"" servers to act as middle men to relay the streams to (but then you need a high bandwidth server). If you're using a TURN server, you're basically just connecting to a regular public server (like any old website). [Here's an article that goes into more depth on the issues that WebRTC has due to NATs and how they are resolved](https://webrtchacks.com/an-intro-to-webrtcs-natfirewall-problem/).

At any rate, as I'm sure you can imagine, if you don't have a publicly predictable IP for the port that would receive your communications, what is the server going to connect to? Now, a callback as you describe would totally work fine between two computers not behind NATs, as is the case for publicly accessible servers. So you could have two servers communicate this way. But ""regular users"" are usually behind NATs and thus it doesn't make sense for the bulk of HTTP traffic. As the links show, there are ways to resolve this issue, but not for all possible NATs and it's so complicated.

As an aside, you can have push-like behavior via using things like long polling or websockets. Websockets requires that you make a connection, then just leave it open. Every now and then you send a tiny ""keep alive"" request so that it won't get automatically closed by the NAT. Long polling is similar except it's a ""regular"" request that just stays open for a long time. When the server has something to say, it immediately returns a response. If it never has anything to say, it sends a response that says ""nothing happened"" and then you make another long polling request to keep waiting for a real response (or to keep a connection for constant receiving of data).",1537811821.0
ArthurTMurray,Modern JavaScript also includes **[Artificial Intelligence](http://ai.neocities.org/FirstWorkingAGI.html)**.,1537707162.0
Cocomorph,"Learnability from positive examples has long been of interest to linguists (cf., e.g., https://en.wikipedia.org/wiki/Negative_evidence_in_language_acquisition for some context). Needless to say, linguistics and formal language theory have influenced each other (see, for example, Gold's theorem), but the perspectives (and respective literatures, for the most part) are different.  ",1537729411.0
,[deleted],1537730563.0
PM_Me_Bayes_Theorem,"Something like this?  
[http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)",1537734533.0
orangejake,"It looks like they use a (forked) version of [MathQuill](https://github.com/mathquill/mathquill), so looking there is a good first step.",1537686001.0
minno,"That's fascinating. It looks like it's doing everything client-side, but the source code is minified so I can't make much sense of it. It's [here](https://www.desmos.com/assets/build/calculator_desktop-721a7d41e28db4feef812af859b17bd2a82ef541.js) if you want to take a look at it.

What it appears to be doing is setting an event handler on the text box that runs every time you type something in. When the handler recognizes a special input like ""sqrt"", ""^"" or ""/"", it changes the elements in the box by adding new HTML elements with the proper CSS to put things like the square root symbol, superscript text, or horizontal line in, along with new text boxes for things like the interior of the sqrt symbol and the numerator and denominator of the fraction.

For example, the expression (x+3)/2 looks like this:

      <span class=""dcg-mq-fraction dcg-mq-non-leaf"" aria-hidden=""true"">
        <span class=""dcg-mq-numerator"" aria-hidden=""true"">
          <span class=""dcg-mq-non-leaf"" aria-hidden=""true"">
            <span class=""dcg-mq-scaled dcg-mq-paren"" style=""transform: scale(0.997621, 1.18573);"">
              (
            </span>
            <span class=""dcg-mq-non-leaf"" aria-hidden=""true"">
              <var aria-hidden=""true"" class="""">
                x
              </var>
              <span aria-hidden=""true"" class="""">
                &nbsp;
              </span>
              <span aria-hidden=""true"" class=""dcg-mq-binary-operator"">
                +
              </span>
              <span aria-hidden=""true"" class="""">
                &nbsp;
              </span>
              <span aria-hidden=""true"" class="""">
                3
              </span>
            </span>
            <span class=""dcg-mq-scaled dcg-mq-paren"" style=""transform: scale(0.997621, 1.18573);"">
              )
            </span>
          </span>
        </span>
        <span class=""dcg-mq-denominator"" aria-hidden=""true"">
          <span aria-hidden=""true"">
            2
          </span>
        </span>
        <span style=""display:inline-block;width:0"">​
        </span>
      </span>

So when I typed ""(x+3)/2"" it created all of those elements in response. They all have CSS classes (and some inline styles) that dictate how they are arranged on the screen, along with those ""aria"" attributes that are meant to be used by screen readers to help people with vision problems navigate the site.",1537686024.0
pgn674,"Well, this is actually kind of interesting. I typed in a test equation: [https://imgur.com/a/ZFQQFcc](https://imgur.com/a/ZFQQFcc)

And looking through the web page's source code, I found this:

    <textarea aria-label=""aria-label=""Expression 1:  ""y"" equals StartRoot, ""x"" plus 3 , EndRoot minus left parenthesis, 2 plus StartFraction, 6 Over left parenthesis, 4 , right parenthesis , EndFraction , right parenthesis  Has graph. To audio trace, press ALT+T.""""></textarea>

Looks like that's for accessibility, so a screen reader can read it out loud. But that's all I've got. I imagine the scripts taking the input and creating the pretty equations are loaded by the page and run locally, but I don't know any web analysis and decomposer tools to go further.",1537684462.0
straught,"If I understand your question correctly, they probably do it with a [lexer](https://en.wikipedia.org/wiki/Lexical_analysis). Using regular expressions you can easily determine what the user means to type, and convert it to the appropriate symbols. For example, if you write 'sqrt' on desmos, it turns it into the square root symbol, right?

Well, with a lexer, you would need to consider 'sqrt' in your regular expression. Then your lexer would produce a token, which you would use to do anything you want, either construct an abstract syntax tree, or use it in a program.

If you really want to have an idea of how to do this, look into learning compilers, it will teach you all you need to know on this subject, and even more.

&#x200B;

EDIT: Looking at some other responses, this is not the answer you might have been looking for, but I'm still gonna leave it here in case someone has a use for it.",1537687107.0
Carpetfizz,Maybe uses something like MathJax?,1537685413.0
48151_62342,"Probably uses a stack to hold the text input, and when it encounters a special combination of entries, like > and = together, it displays on the screen ≥ but doesn't alter the stack.",1537705100.0
alkasm,Might be able to get some good discussion via cross-posting to r/computervision.,1537654974.0
AsymptoticPerplexity,This is really interesting! It could be useful for extracting important features from an image before applying other processing. I imagine you could make an analogue of this for video where you highlight the regions of the video with the greatest information change over time.,1537649111.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/u_technocapital] [Using Information Theory to Identify Image Features](https://www.reddit.com/r/u_technocapital/comments/9i5m1m/using_information_theory_to_identify_image/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1537674420.0
covertfunction,https://toolbox.google.com/datasetsearch,1537676405.0
TaXxER,"Do a Google scholar search on the term ""predictive business process monitoring"", this research area deals exactly with what you describe: forecasting of events from sequences of historical events. Originally this field focused on executions of business processes, but more recent work has also focused on hospital data. Papers from this field also make their data publicly available, so you should be able to also find some datasets this way.",1537703749.0
-smiley--,Making a flowchart really helps a lot.,1537631649.0
Jaxan0,"I usually run the algorithm with pen and paper. For this, you only have to do everything step by step. And at some point I start the understand the algorithm. It especially helps to draw the data structures which are being used. ",1537645445.0
thbb,"Rewrite the code of the algorithm you want to study in your favorite language, test it. Once it works, you will have understood it.

* of course this means when working from the specification, not from a code example in the same language.

For quick sort specifically, you need to study the pieces separately: choice of pivot, partionning (what does the array look like before and after), then recursion.",1537641822.0
eshyDerBerserk,"Especially for sorting and searching algorithms, it helps a lot to apply them to real objects, like cards and run the algorithm on them by hand. 
",1537657471.0
SteeleDynamics,"Pen and paper does work well, as others have mentioned. You can also try using something like GraphViz. That has worked well for me.",1537658731.0
wellnhoferia,"In the beginning each algorithm you learn takes a bit of time to grasp. Be persistent and you will ""get it"" soon enough.
Feeling like you're not smart enough is simply not true and just discouraging! Don't do that.",1537659742.0
future_security,"Quicksort separates the items you are trying to sort into two piles. A big pile and a small pile. If you choose a random pivot and it's close to the median value then your two piles will each have about half as many items in them. You repeat this ""partitioning"" again on each of the smaller piles, except this time it takes half as much time for each one because each one is half the size.

You continue partitioning piles into smaller and smaller sets until every pile has one item in it. If your organize the original large piles ""in place"", pushing small items to the left and large ones to the right, then at the end every ""pile"" of just one element goes from small to big, left to right.

You have to do less work per pile after each iteration, but you do it over more and more piles each time. However the total number of items never grows so you're still do O(n) work. The reason why your average-case run time is (hopefully) O(n log n) instead of O(n^2 ) is because you only need to do as many iterations as the number of times you need to half the original number of items.

If you start with 32 items
 * On the first round you need to partition 32 items into two piles. (Subtotal: About 32 units of work)
 * On the second round you partition two piles of 16 elements into four piles. (Subtotal: 32 + 2 * 16 = 64)
 * On the third round you partition four piles of 8 elements each into 8 piles (Subtotal: 64 + 4 * 8 = 96)
 * On the fourth round you partition eight piles of 4 elements each into 16 piles (Subtotal: 96 + 8 * 2 = 128)
 * On the fifth round you partition 16 piles of 2 elements each into 32 piles (Subtotal: 128 + 2 * 16 = 160)
 * You now have 32 items arrange small to large. The total amount of work was 32 * log(32, 2)

The reason why worst case Quicksort time is quadratic is also simple. Imagine you're extremely unlikely and every pivot-value you choose partitions a pile into the one smallest element on the left and the rest on the right. You put the first element into its final position after each round of partitioning, but you're not making much progress toward making the unsorted piles smaller so you end up having to partition O(n) times. (You may notice how Quicksort kind of degenerates into Selection sort in the worst case scenario.)

Now all you have to do is make sense of the partitioning process. You need to think in terms of arrays instead of piles to truly understand how it works and why it's implemented the way it does. Although I'm a visual learning according to the classic visual/tactile/aural learner test they give to young children, I don't think of topics in terms of visual representations. Most of the time I'm thinking with math as my ""language"". 

In the case of partitioning you need to realize the important details are 1. It needs to be done in-place (meaning you don't need to allocate extra data structures) and 2. You need to prove to yourself that a partition is linear with respect to the input size (meaning if you have n elements then you do at most a number of comparisons and a number of swaps proportional to the value n.)

Some hints:
 * Think in terms of arrays, not lists. (The Python one-liner implementations and similar ""easy"" implements of ""Quicksort"" aren't really Quicksort because they're not in-place.)
 * If they have already taught you loop invariants or proof by induction, then use those concepts as tools for understanding how an array is partitioned into a ""small"" part and ""big"" part.
 * Note that partitioning is useful for things beyond plain Quicksort. For example the QuickSelect algorithm. Or separating an array of mixed positive and negative numbers into two sub-arrays. One with only positives and one with only negatives.

And in general (for other algorithms) don't even worry about trying to ""see"" how things work. Not everything can be meaningfully visualized. (Even if you weren't a 3D creature only capable of understanding things in one, two, or three dimensions.) When you struggle to understand something, try using as many different types of resources you can. Use a paper and pencil. And a step debugger. And loop invariants. And animations. And free lectures posted on the internet.",1537661589.0
CelestialZippyZap,"[https://visualgo.net/en](https://visualgo.net/en)  
I hope this is of some help. ",1537683765.0
szienze,"Perhaps thinking about the [x86 LEA](https://stackoverflow.com/questions/1658294/whats-the-purpose-of-the-lea-instruction) instruction might clarify things: If you have a 32-bit register, you can only store 32-bit addresses in that register, limiting how much RAM you can access.",1537629467.0
skeeto,"Typically memory is addressed via addresses stored in registers. If your
memory addressing registers are only 32 bits wide, you can only address
4GB of memory.

Though that's a simplification, and this isn't strictly true. Some
architectures have schemes for addressing more memory. One example is
the 16-bit 8086, which can access up to 1MB of memory through an address
split across two registers. The x86-64 architecture has 64-bit general
purpose registers, but it currently only has a 48-bit address space
since that's more than sufficient. Some of those address bits are
unused.
",1537629725.0
TomvdZ,"There's no relation between CPU register size and the width of the address bus. They're completely orthogonal concepts. For example, many 8-bit processors can access more than 256 bytes of RAM.",1537629709.0
patsluth,"Think about a 3 bit number.

000
001
010
011
100
101
110
111

There are 8 or 2^3 combinations. With a 32 bit instruction there are 2^32 combinations, which works out to 4GB. ie the maximum memory address that can be encoded in an instruction.  ",1537629807.0
feedayeen,"The typical 32 bit system will use 32 bit pointers to address data which gives a max space of 4GB available to the application and the typical 64 bit system will use 64 bit addresses. There's a layer or two of abstractions like virtual memory separating you from the hardware memory address and this rule does not apply to smaller architectures like 8 and 16bit and there's no inherent reason to use 128bit addresses when processors start using that. Processor and memory development just happened to advance at the rate where those numbers coincided when making the transition ~2 decades ago. 

The bits in an architecture mostly apply to the standard integer size used by the arithmetic and logic unit registers. You can do arithmetic on different sizes, including much larger sizes but it will require more operations. ",1537649413.0
PetrosPapapa,"My problem back in high school had more to do with motivation rather than the level of knowledge. As far as I know this is still the case, as even undergrads (in Reddit) keep asking for ideas for projects. 

Simply put, as much or as little as I would learn, I did not know what to do with it. I wanted to build something fun and meaningful that I could show off. There were 2 problems with that:

First, I was only ever given toy examples, and I could never extrapolate how any of it would be useful in ""real"" software. Give them better examples, solving actual problems! Give them better problems to work on, little games, or small fun apps they can actually use! This is why Logo used to be such a good language for learning, because you have an immediate practical reward.

Second, I was never taught enough to make anything useful. It was like some subjects (like pointers, or I/O, or simple interfaces) were taboo because ""oh that is too advanced"". There is always a huge gap between students who are natural and learn quickly and students who aren't and struggle. This leads teachers to underestimate how much the former can actually understand and learn. Myself and my other geeky friends learned to build Visual Basic interfaces, pixelated game graphics, even sounds and game mechanics all on our own. I remember reading tutorials on a PC magazine on how to do simple drawing and keyboard controller movement in BASIC. I learned so much from that alone!",1537617765.0
s0ft3ng,"I didn't do computer science in high school, but just the self-taught programming stuff.

I actively chose *not* to do the offered ICT class because I was too advanced for it. I like the option that my first-year CS course took: teach functional programming.

It'd be brand-new to both ""advanced"" students and those who've never programmed before. It distills programming to it's core algorithmic parts, and shows *explicitly* concepts like state & abstractions. I never really understood what people meant by ""state"" until my first function programming class, where it all clicked for me.

It's also very easy to test (QuickCheck has a super simple interface) and debug. When writing assignments, you can just specify the type, and let the student go wild.

I'd honestly stay away from object orientation, although it might be useful in their undergrad. In the real world, I avoid inheritance wherever I can, and use composition instead.",1537615652.0
AManIsBusy,"I would like to see resources that don't oversimplify things early on. You can teach complex topics without dumbing them down and still getting younger students to understand them. I think there's too much rush to get students up to speed with using certain tools.

What happens when we simplify concepts is that we have to teach the same thing 5 different times, creating technical debt and mental exhaustion when a student kind of knows something but doesn't know what they don't know about it. They're trusting their prior, imprecise definition for a concept. In order to move forward, they have to stop trusting those definitions, but when they stop trusting those definitions, they get lost in this ether where nothing is real.

You mentioned learning functional and OOP early on. I can absolutely imagine these dumbed down so that students can 'use' them, but then when they start using them, there are tons of 'gotchas' in the real world because they don't understand the full implementation.

For advanced students, I think one of the biggest de-motivators is to realize that you're being lied to, in a sense. Why spend effort to understand something if it's not even technically 'true'?
",1537628573.0
turtle_13,"I personally think processing is a good alternative for teaching code. Although I think functional programming is neccessary to be taught, instant feedback helps a lot.

If you don't have an instant feedback then you cannot visualize what one can do.

And although processing is not that popular I think it would be a great way to teach code.",1537636644.0
surface_book,Bob Tabor's courses are pretty good resources.,1537642563.0
NowImAllSet,"Well I'll be damned. I did a senior project on exactly this. Take a look at [this node package](https://github.com/ageitgey/node-unfluff). It is by no means perfect, but I found it to do the best job. Worked pretty well for most sites. And the algorithm itself is actually pretty straightforward, no machine learning involved. It's well written and easy to understand.",1537621225.0
stalefries,Have you looked to see if this newspaper has an alternative format (like an RSS feed) and scrape that?,1537632529.0
rgb786684,There won’t be consistency across sites in terms of tagging so you’ll have trouble building something that works well on most websites. The best way I can think of getting a system that works in the general case is to use ML or NLP to classify text as good or bad.,1537615492.0
crimson117,"If there are any tags isolating those social media links/words, you can adjust your scraper to ignore them.",1537614392.0
pudu0,"Another option would be to use Mozilla’s library. Can’t find the link right now but it’s the same they use for the reader view in Firefox. 

[Edit] here is the [link](https://github.com/mozilla/readability). ",1537645655.0
edeph,"I think you need a renderer, look into phantomjs
",1537653071.0
webkenth,"You can't generalize websites you have to take them on as completely different and create an Abstract model which describes the basic content, then add Transformers on a site by site basis, after writing about 10 transformers you will get a generic image of how each site presents their content, you can then use Machine Learning to automate the process ",1537624726.0
IcebergLattice,"You represent a relation R ⊂ S×T (with finite S and T) as a matrix M indexed by S and T, where M\_{s,t} = 1 if (s,t) ∈ R and 0 otherwise. I'm not sure how one would write an entire book about that.",1537716344.0
Responsible_Virus,I'm not sure I followed the posts so I might be totally off base about what you are asking but I've read some papers on boolean matrices used for parsing so you could in represent relations equivalent at least to those described by a CFG,1538275448.0
klausshermann,The textbook “Simulation modeling and analysis” by Averill Law has great material on RNG. ,1537549212.0
bart2019,"The LCG (linear congruential generator)  article is, i in my opinion, a classic  and very good as an intro to how pseudo-random generators work. 

I found a site where it's split up over many pages: http://random.mat.sbg.ac.at/results/karl/server/server.html",1537552839.0
future_security,"I don't know, but I recommend against blog posts and stackoverflow. There are good posts, but a person's ability to differentiate good and bad requires knowledge. So they aren't a great learning resource. 

Read a lot. Prefer published academic work. The deeper you want to understand things the more you'll need to read.",1537556420.0
Evbot300,Get-random,1537581855.0
saltybearpuppyguy,Ask your mom how she randomly picks what age guys she’ll suck off at the bar ,1537613674.0
djimbob,"Worth pointing out that graph isomorphism (are two graphs isomorphic -- can you relabel vertices of two graphs so they contain all the same connections) is distinct from subgraph isomorphism (does graph G1 contain a subgraph that is isomorphic to graph G2).  

We know subgraph isomorphism is NP-complete.  There hasn't been a well-accepted complete proof that graph isomorphism is in P or is NP-complete. (Graph-isomorphism is known to be in NP; given a valid mapping between isomorphic graphs you can show in polynomial time that they are isomorphic).  

Note if P != NP, there must be NP-intermediate problems (those that aren't P or NP-complete) and problems like integer factorization and graph isomorphism typically are listed as potential problems in the class.",1537540506.0
m--w,That is not at all what is going on here. ,1537547617.0
DrSweetscent,No he doesn't.,1537543214.0
TKAAZ,Why is this so upvoted? The title is so misleading.,1537602967.0
VishMeLuck,"You don’t have to know all programming languages, just pick one (probably C++) and get to know it heart and soul. Other programming languages work the same way. 
You’re at advantage bc you’re mature enough to understand a lot of things in one day while a student in his freshman ship takes time to build his/her foundation. You’re also driven by passion, not everyone is. 
Tbh, you’re lucky. 
If it takes an extra semester or summer, it’s worth it for your passion. 
I am 26, and moving towards MBA but I realized my true potential at 24 that I wanna be a techie and Database is my passion.
Hope this helps! ",1537534976.0
gibbles_baloney,"Learn one language. Some folks say to learn Python first, and I wouldn't say that they're necessarily wrong. Simpler languages like Python are still very powerful and play an important role in industry. Though, I'd say that if you were looking to form strong foundations which lend themselves to learning *any* language, start with C. The reason for this is mostly historical, as the vast majority of languages out there are either completely build on C, share syntax with C, or are inspired in other ways by C.

What you won't get is an introduction to Object Oriented Programming, and depending on who you talk to this could be a good or a bad thing. Nevertheless, the way of thinking about programming problems and their solutions in C will prepare you for thinking about OOP and for learning pretty much any other procedural language out there with ease. If you're looking for a course, check out edx.org, and look for ""CS50"". It's by Harvard's Extension School and is complete with video lectures, graded homework, and is completely free. In this course they start with C, and move to Python in the second half to teach OOP, etc.

Most of all: have fun! Good luck.",1537536409.0
acroback,"After working for 12 years I still face this problem. People are insanely smarter than me. But I just don't give up, I keep grinding one concept at a time.

I'll catch up to you mofos, sooner or later.",1537558252.0
migomick,"Hi I've been working for 6 years now. Don't worry - your first internship/job will teach you more in 6 months than 4years of school.

As someone in the industry - I'd really suggest focusing on in-demand languages/frameworks, as these are what companies hire for. The truth is that the languages really don't matter as you will learn new ones quite regularly, on the job/at home.

Front end is always fun and very accessible and is not usually taught in school. Look at react and angular frameworks. Use node.js for your server and learn how to use task runners and test suites

Everything now is about continuous integration which means you have lots of tests (unit, integration, smoke, snapshots)

Pick up either Java or C#. They teach Java in school so that shouldn't be hard. I would really advise against c++ as there are very few jobs that utilize it (unless you want to do game dev, even then though there are plenty of tooling jobs that don't use c++).

Most companies will be a microsoft or java shop.

Most important thing is to just program something a little everyday and commit to github.

I like projecteuler for a little fun.

I'd also suggest a javascript game challenge. If you can make war, or minesweeper, or frogger then you can get any entry level job.

&#x200B;",1537552905.0
crakotta,"I switched to computer science at a university after going to college for 2 years.  It did take me 3 years to finish from there, so I took 5 years instead of 4 years to get a degree.  But I just took my time and I compare myself to no one.  I am out of school for 17 years now and I can say that it was worth the time and patience to do what I want.  Is there any urgent reason you feel you need to take an internship soon?  Will you have another opportunity the following summer? This summer maybe you can take summer classes instead of an internship, or see if there is some program at your university to get involved in.  I personally would not have gone for an internship in computer science during the first year of studying programming but everyone is different.  ",1537555154.0
berryer,"As far as a place to start, a project that's relevant to you is a great way to stay interested regardless of how out-of-your-element you feel.

* play any games? Get involved in the modding community!
* like making visual stuff? Learn to work with an arduino & LEDs! Add different sensors, knobs & whatnot as inspiration hits
* in a math class that allows graphing calculators? Learn TI-BASIC and automate some of the more frustrating formulas!
* Set up a home media center with OpenMediaVault, freeNAS, Kodi, or another DiY system! It'll get you started with the linux environment & CLI if you're not already familiar. Setting up a home routing system with OpenWrt could also be a good project for this
* any frustrating issues in an open source project you use? I'm sure they'd be glad to have extra help!

As far as getting rid of [Imposter Syndrome](https://en.wikipedia.org/wiki/Impostor_syndrome), let me know if you figure it out! It's a relatively endemic problem in the industry. I've found it oddly comforting that it seems like nobody actually knows what they're doing (see also: /r/internetofshit )

Edit: as a side-note, as somebody who has been helping screen resumes recently - stuff on your github page that looks like it's there for resume purposes will be identified & mostly ignored. A couple of small, quality contributions to larger projects go much, much farther. (E.g. the best I've seen in months is somebody with three good five-line PRs against some EVE-online tools)",1537574574.0
khedoros,"> but I feel like I my programming skills are lacking and I don't have any full-stack projects under my belt. 

I've been working for 10 years, and I've never built a webapp outside of the few times I've looked at a Ruby on Rails tutorial, or something. Webdev is super popular, and definitely a good way to go, but it's not the only road.",1537574812.0
excitedWallrus,As far as internships are concerned I'd recommend you check out [Jumpstart](https://jumpstart.me/r/wallish).  It'd give you the best chance of getting an internship without a pristine resume.,1537547441.0
ClutchPedal,Which books? Any specific nuggets to share?,1537512951.0
gyromonorail,In general or a specific book?,1537512559.0
tulip_bro,"Is operational semantics used in industry? I am in a compilers course where we are learning a _static_ semantic formalism: attribute grammars. My conjecture is we are learning it due to it's relative easy translation to an implementable algorithm.

I find it less elegant than the dynamic variant of operational semantics, but cannot seem to find many uses of it in compiler design.",1537662776.0
dhutch7813,Is computer engineering an option? It would give you more software classes and possibly have more courses transfer over than what you’d get switching to CS.,1537496680.0
krum,"I was an engineering major that switched to a career in CS.  Not regretting it one bit.  CS is easier, pays more, and in general is more rewarding both financially and personally.   I guess it's not an option to take some core and switch next year?  ",1537492171.0
future_security,"You know engineering does not suit you well. Computer science may or may not suit you well. 

You lose if you stay in engineering. You may win or may lose if you switch to CS.

It's pretty obvious to me what the more rational decision is. Your chosen major is not supposed to make you miserable. Engineering students are the only type of students I've met who take pride in their major being difficult, social-life sapping, sleep depriving, and tedious. If you don't fit in that model it's not because you're defective. Most other students, if they take pride in their major, are proud because they love the subject they're studying.

Read about [escalation of commitment](https://en.wikipedia.org/wiki/Escalation_of_commitment) and the [sunk cost fallacy](https://www.lifehack.org/articles/communication/how-the-sunk-cost-fallacy-makes-you-act-stupid.html). It doesn't make sense to avoid cutting out bad things in your life just because you've already invested too much in it.

It is always better to change your major, leave a cult, quit ""alternative medicine"", or break-up/divorce sooner, *rather than later*. But having not done so sooner isn't a good reason for choosing not to do so *now*. Don't commit to several years of school that you'll still hate just to get a job you will also hate.",1537497000.0
jmite,"CS is more discrete math, logic, graph theory, etc. Linear algebra and abstract algebra don't hurt.

CE will likely have more calculus, real analysis, vector math, differential equations, Fourier analysis etc.",1537493416.0
future_security,I don't believe that there exists a branch of mathematics that can tell you if branch A of mathematics is mathier than branch B of mathematics.,1537494179.0
xShadowProclamationx,both are right. ,1537481500.0
noam_compsci,I did not read the whole thing but I think a lot of consumer data protection theory will eventually come down to internalising externalities via Coases theorem and its enforcement. ,1537717514.0
cristianontivero,"You probably have time if you are only studying (i.e. not simultaneously working, or have some other responsability). 

Internet is truly remarkable. Nowadays you can have access to lecture notes from top researchers in different fields, free books, homework problems with solutions, recorded classes, etc. You can go almost as deep as you want; there is always a huge advantage to having someone who knows what they are talking about giving a hand, but even without I’d say you can manage to get quite far.

You can even download lectures videos and watch them while you commute! There is a lot you can do to squeeze time and be more productive.

Don’t settle with what you are given in class, go way beyond that. In the end, if you truly like what you study and want to learn, you won’t regret it.",1537481046.0
nakayamakai88,"You can try doing what I do:

1) Come up with a great big idea that you can *totally* accomplish in a short amount of time

2) Try writing the code using what you know

3) Fail

4) Repeat step 1

You'll learn why you make mistakes along the way and hopefully ways to fix them (or ways to work around them) giving you a better understanding of what you can and can't do all together. You'll also discover new tools to work with once you realize what you know isn't enough to do what you want.",1537481556.0
khedoros,"Your early classes will focus more on some very basics, and getting you comfortable with using code to explore problems. Algorithms and data structures build the basis of what you'll study later. And overall, an undergraduate degree in Computer Science is a broad overview of the field, either providing a basis for deeper study, or fundamental concepts that are useful in a profession like software development.

But if some aspect particularly interests you, then of course you can also dig deeper into it, with the tremendous numbers of learning materials available online these days. And the only way to get better at programming...is doing programming. Find some articles about design patterns. Think about the kinds of problems that they would be useful for, and think about what tools you know to help you implement those patterns. Then, try to do it!",1537498654.0
MemesEngineer,"Integral calculus is something that is used everywhere. I know computing an Integral is awful but what they represent is crucial. Every single physics equation is derived from calculus, Data modelling and statistics uses calculus, everything that is a system with a differential equation needs integrals. Also, they are good training for problem solving skills. You have the tools and you have a puzzle.",1537481008.0
NEET_Here,"You should look at the PID(proportional, integral, and derivative) algorithm used to control objects.

It's essentially what you're asking about. This comes from numerical analysis and methods. Basically

The integral from a to b of a function f with respect to time is approximately a sum. 
",1537493635.0
DarthSanity,"Calculus and computer science are old friends from a computational standpoint, since the advent of computers. Military and scientific calculations, mostly, such as in the calculation of artillery and torpedo solutions.

The most ubiquitous integral relationship in our society are related to distance/velocity/acceleration. Velocity (mph or mph) is a derivative of distance (miles or kilometers) and acceleration (mph^2 or kph^2) is a derivative of velocity. So automotive systems use calculus when displaying results to the driver. Aerospace navigation systems use partial differentials - velocities in the x, y and z plane. So calculus is everwhere in the many embedded systems present in our society.

IT on the other hand is tied to business processes and solutions, most of which do not require calculus. There are a class of problems in economics, though, that use a form of business calculus to estimate trends in micro- and macro-economics. Calculus also shows up in analytics systems. This is mainly the realm of financial industries.

Does calculus show up at the local Walmart or insurance company? Yes - Walmart will use calculus to estimate trends in product sales so they know how much to order for next time. Calculus also shows up in actuarial tables, using all the factors you check off in the questionnaire you answer to estimate how long you have to live, or how likely you’ll get into an accident.

Make sense?",1537480691.0
SpecialCustard,GitHub ,1537473968.0
dataisthething,You are doing an M.S.? And presenting your thesis project?,1537471035.0
coterminous_regret,"I can't give specific help here as I have not directly worked in this area but there is an entire branch of CS devoted to this sort of thing:

https://en.wikipedia.org/wiki/Natural_language_processing

I suspect if you go googling for ""NLP sentence similarity"" you will find papers and tools for this task 

",1537467802.0
umib0zu,"Letter/word similarity? Sure. Use tf-idf vectors from your entire set of corpus sentences and do the cosine similarity of the tf-idf vectors. Semantic similarity? /r/LanguageTechnology /r/linguistics and get ready for a PhD.

&#x200B;

All these sentences are Letter/word similar. I think you're looking for semantic similarity and that's a hard problem.",1537467753.0
practicalutilitarian,"For accurate sematic similarity you want an embedding (hidden layer output) from an LSTM with sequences of word vectors as the input. They can be trained as an encoder decoder pair on a translation or auto encoder problem. You want to train it on a massive collection of sentences. Then it's simply cosine distance between the vectors. 
But you can get close with a pretrained doc2vec embedding in a python package like gensim. That would be enough to catch the ""not"" term in s3, but it would ignore subtle meaning changes due to the order of the words.",1537485175.0
slashcom,"Universal Sentence Embeddings from Google is the current state of the art. They release a pretrained model of I recall correctly.

It’s very much an open research question.

The tasks your interested in are “textual entailment” (SNLI) and “semantic textual similarity” (SICK STS). These are two common benchmarks related to what you’re asking. These benchmarks have many misgivings though",1537487959.0
helpfulsj,"This is not an easy problem to solve. It really moves into machine learning in AI.

AWS has a tool called Comprehend that does sentiment analysis and will pick out the subjects of a blob of text and give you information like if its positive, neutral, or negative.

Phrases like X will not support Y would have a higher negative rating.

So you could look at all of the phrases that mention Trump, China, and the US and sort them out based on negativity.

All you have to do is upload a text file",1537468278.0
tecnofauno,"The Google Universal Sentence Encoder does exactly what you're asking.
https://alpha.tfhub.dev/google/universal-sentence-encoder/2

In order to try with your own sentences, you need to download tensorflos, the plugin and a little program to encode the sentences.

You can find an example here:
https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb#",1537520337.0
SideburnsOfDoom,If you're asking about strings of text there is [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance). But you seem to be asking more about semantics.,1537476012.0
scottyler89,Cosine similarity of the latent representation of the bottleneck layer of a variational autoencoder (like sent-to-vect).,1537487838.0
hashestohashes,"a quick method would be to compute a dense representation of each sentence, starting from the word embeddings (glove, fasttext, w2v), using SIF [https://github.com/PrincetonML/SIF](https://github.com/PrincetonML/SIF) or p-mean [https://github.com/UKPLab/arxiv2018-xling-sentence-embeddings](https://github.com/UKPLab/arxiv2018-xling-sentence-embeddings), then measuring how distant the resulting vectors are from each other.",1537482296.0
MirrorLake,"Based on your example, I still don’t really know what you mean by “similar”. They all sound like very similar sentences to me.",1537472120.0
postisapostisapost,"This is a grossly simplified technique, but it might help you out:

1. Maps the words in each line to vectors in a semantic space (e.g. word2vec via gensin, etc.)

2. Sum all the vectors in each line to produce a document vector (meaning of the document in concept space).

3. Take the dot product between each vector (e.g. via cosine distance)  to measure similarity. 


",1537477463.0
shponglespore,"Handling the simplest cases is pretty easy, but in general, this is a *very* hard problem. Last time I was paying attention to this kind of thing (around 7 years ago), just uncovering the structure of complex sentences was impossible to do reliably, and that's really important when dealing with modifiers like ""not"". Dealing with synonyms is tricky, and dealing with words that are related by not really synonyms (e.g. help vs. support) requires detailed semantic information about what the words actually mean, that that kind of information doesn't exist in any kind of machine-readable format; regular old dictionaries are the closest thing we have to a catalog of word meanings that's even remotely complete. Another hard problem that needs to be addressed is anaphora resolution (also called coreference resolution), which means figuring out when phrases like ""the president"" or ""he"" refer to Donald Trump.

All those issues are potentially solvable, at least in enough cases to make something useful if it doesn't have to be too reliable, but doing it well is the kind of thing researchers get paid big bucks to try to solve. If you want to take a peek down that very deep rabbit hole, try looking up some papers by my old boss, Dr. Sanda Harabagiu.",1537478680.0
SippieCup,word2vec.,1537495350.0
spacemoses,How would you rate the similarity of your own examples by hand?,1537498821.0
hotrodx,"The topic is interesting. I wonder how it will understand context. For example:

S5: Trump has a collection of fine china.  
S6: China has its trump card.  
S7: The USA is like a bull in a china shop.  
S8: China trumps all other Asian countries.  
S9: There's a new Trump Tower in China.  
S10: Trump towering over China.

I'd guess the AI would probably use points of relevancy, instead of determining what each sentence means ""exactly"". However, some news headlines do use puns, for example ""China has its Trump card"" might be a hypothetical headline in a scenario where Trump will support China.

EDIT: added another phrase",1537500774.0
isaikumar,"Jaccard Similarity is the easiest one. It just takes two sets(unique elements) SET1 and SET2 and calculates the below.

`Jaccard Similarity of SET1 and SET2 = cadinality(SET1 intersection SET2) / cardinality(SET1 union SET2)`

The sets that I talked can be words directly or they can be K-grams based on words or characters. 

For example,

S1 as a set of words = {""trump"", ""will"", ""support"", ""china""}

S1 as a set of 2-Grams based on words = {""trump will"", ""will support"", ""support china""}

S1 as a set of 3-Grams based on words = {""trump will support"", ""will support china""}

S1 as a set of 3-Grams based on characters including space = {""tru"", ""rum"", ""ump"", ""mp "", ""p w"", "" wi"", ""wil"", ""ill"", ""ll "", ""l s"", "" su"", ""sup"".....}

If you intuitively look at it, set of words is nothing but 1-grams based on words.

All the above and many more can be used for constructing a set and finding Jaccard Similarities.

Each have their own advantages and disadvantages.

If you want, I just wrote this program a week ago in Scala for my Data Analytics course Assignment.

Put your sentences in different documents named D1.txt, D2.txt, D3.txt, D4.txt.

If you want to calculate 3-Grams based on words, then set k = 3 and wordKgrams = True.

Run the program on Scala and you will get all possible jaccard similarities between your documents. The higher the value, the more similar the documents pair are.

    // scala KGrams_Question_10.scala
    /*
    * variables that can be modified --> wordKgrams, k, files
    * wordKgrams --> word based or character bases
    * k --> k in k-grams
    * files --> all document names
    */
    import scala.io.{Source}
    import java.io.{File}
    
    object KgramsAndJaccard{
      def main(args:Array[String]):Unit={
        val start = System.nanoTime()
        // word based(true) or character based(false)
        val wordKgrams:Boolean=false
        // k in k-grams
        val k:Int=3
        // all document names
        val files:Array[String]=Array(""D1.txt"",""D2.txt"",""D3.txt"",""D4.txt"")
    
        if(wordKgrams){
          println(""Word Based K-Grams"")
        }else if(!wordKgrams){
          println(""Character Based K-Grams"")
        }
        println(""K = ""+k)
    
        println(""\nK-Grams\n"")
        var kGrams:Set[String]=Set()
        for(i:Int<-0 until files.size){
          // new File(files(i)).exists returns true or false
          assert(new File(files(i)).exists,""File does not exist!"")
          kGrams=getKGrams(wordKgrams,k,files(i))
          println(""No. of K-Grams for ""+files(i)+"" = ""+kGrams.size)
        }
    
        println(""\nJaccard Similarities\n"")
        var jaccardSimilarity:Float=0
        for(i:Int<-0 until files.size-1){
          for(j:Int<-i+1 until files.size){
            jaccardSimilarity=getJaccardSimilarity(wordKgrams,k,files(i),files(j))
            println(""Jaccard Simlarity of ""+files(i)+"", ""+files(j)+"" = ""+jaccardSimilarity)
          }
        }
        val end = System.nanoTime()
        println(""Elapsed time: ""+(end-start)/1000000+""ms"")
      }
    
      def getKGrams(wordKgrams:Boolean,k:Int,fileName:String):Set[String]={
        val bufferedSource=Source.fromFile(fileName)
        var singleLine:String=""""
        var kGrams:Set[String]=Set()
        for(line<-bufferedSource.getLines){
          singleLine=singleLine.concat(line+"" "")
        }
        bufferedSource.close
    
        if(wordKgrams){
          var singleLineArray:Array[String]=singleLine.split("" "")
          for(i:Int<-0 to singleLineArray.size-k){
            kGrams=kGrams+singleLineArray.slice(i,i+k).mkString("" "")
          }
        }else if(!wordKgrams){
          for(i:Int<-0 to singleLine.length-k){
            kGrams=kGrams+singleLine.slice(i,i+k)
          }
        }
    
        return(kGrams)
      }
    
      def getJaccardSimilarity(wordKgrams:Boolean,k:Int,fileName1:String,fileName2:String):Float={
        val kGrams1:Set[String]=getKGrams(wordKgrams,k,fileName1)
        val kGrams2:Set[String]=getKGrams(wordKgrams,k,fileName2)
        return ((kGrams1.intersect(kGrams2)).size:Float)/((kGrams1.union(kGrams2)).size:Float)
      }
    }

&#x200B;",1537518457.0
grabyvc,"Well, you can try the Word distance mover. It's an optimal transport inspired solution to this problem.  You are basically solving the earth mover problem ([https://en.wikipedia.org/wiki/Earth\_mover%27s\_distance](https://en.wikipedia.org/wiki/Earth_mover%27s_distance)) but instead of earth you are transporting the word meanings and your distance is the similarity between words . I have used it previously and it worked. 

For more information, check out this: [https://github.com/src-d/wmd-relax](https://github.com/src-d/wmd-relax) and this: [http://www.cs.cornell.edu/\~kilian/papers/wmd\_metric.pdf](http://www.cs.cornell.edu/~kilian/papers/wmd_metric.pdf)",1537535229.0
NSA_GOV,It depends what tools you have at your disposal. I usually use Fuzzy Lookups with a set confidence interval to determine the similarity between 2 datasets.,1537539683.0
bradfordmaster,"What you really want, as others have said, is _sentence embedding_. That is, take a whole sentence, and turn it into a set of numbers (a point in some high dimensional space) where you can then take the distance and have it be meaningful.

Someone pointed to the Google library which is a good place to start, another is the Facebook library called [InfraSent](https://github.com/facebookresearch/InferSent)

To have any hope of what you want to achieve, you need a machine learning technique. People tried for many years to write down simple rules to achieve this kind if thing and it just didn't work on any kind of remotely nuanced example. ",1537543964.0
Clssq,"OP it's been about two months now, which approach worked best for you? Universal sentence encoder? I have the same exact question.  


Edit: I had high hopes for the Universal Sentence Encoder [but it doesn't seem to be able to notice negations or flipping the sentence meanings](https://imgur.com/a/yt7QzaK)",1543517770.0
beeskness420,Yes ,1537474486.0
pacmanrva,This project from Peter Norvig on translating English to logic statements might give you some clues https://github.com/norvig/pytudes/blob/master/ipynb/PropositionalLogic.ipynb,1537472666.0
knot_hk,"So you want the computer to intuitively grasp what your particular meaning of ""similar"" is?

&#x200B;

To me, the only sentences that are similar are S1 and S4, as those are the only two that have the same meaning, the others are completely distinct. I think you should clarify your question. NLP does have nice tools but you are dealing with computers, you can't expect them to ""guess"" what you want from them.",1537481779.0
Carpetfizz,"you could compute the cosine similarity of the word vectors. wrote something similar a while ago

https://github.com/Carpetfizz/text-sim",1537476287.0
TheCriticalSkeptic,"Your problem is difficult because the sentences are all about the same topic. So a lot of algorithms that detect similarity would put them in the same bucket compared to say a sentence about science and technology. 

It’s possible that sentiment analysis could capture the sentiment difference here. Help is considered positive and not+help could be considered negative.

So you could try some simple string similarity metrics or topic modelling combined with sentiment analysis. That way you’ll know if two sentences are about the same thing but differ in tone.

If you use python look up NLTK and Gensim which contain a bunch of libraries to do this stuff. But be prepared to learn a lot of NLP jargon to make use of them. ",1537481145.0
ShadyDaDev,"Hm, I may attempt at writing this as a side project. Would be interesting for sure. ",1537474365.0
fearsneakta,No,1537472887.0
fnovd,https://www.reddit.com/r/homeworkhelp,1537468699.0
mrexodia,"You can always encode truth tables as a logic circuit (although this isn’t usually the most efficient implementation). In this example, name the inputs a, b, c and then do the following for every column that has outcome 1 (every 1 corresponds to the variable and every 0 to ~variable):

(~a & ~b & c) | (a & ~b & ~c) | (a & ~b & c) | (a & b & ~c)

You can then convert this to a (terrible) circuit manually. You can also ask wolfram alpha to do it for you: http://m.wolframalpha.com/input/?i=%28%7Ea+%26+%7Eb+%26+c%29+%7C+%28a+%26+%7Eb+%26+%7Ec%29+%7C+%28a+%26+%7Eb+%26+c%29+%7C+%28a+%26+b+%26+%7Ec%29",1537468100.0
reasonably_plausible,"If you are having trouble finding a pattern in the truth table listed, try rearranging the order to treat a different input as the most-significant to see if a pattern is more apparent.

.

**2nd-bit Most Significant**  


0 0 0 => 0

1 0 0 => 1

0 0 1 => 1

1 0 1 => 1

.

0 1 0 => 0

1 1 0 => 1

0 1 1 => 0

1 1 1 => 0

.

**3rd-bit Most Significant**  

0 0 0 => 0

1 0 0 => 1

0 1 0 => 0

1 1 0 => 1  

. 

0 0 1 => 1

0 1 1 => 0

1 0 1 => 1

1 1 1 => 0",1537471967.0
eternusvia,I doubt there is a way to effectively solve this problem without machine learning.,1537450650.0
wakka54,"The vibe I'm getting, and I could be totally wrong, is that you seem to think human language interpretation is on the order of difficulty of the pythagorean theorem.

Google is nearing a trillion dollar company and has distilled all their expertise and resources into a little thing called Google Assistant. And its h o r r i b l e. Godspeed, kid.

Fake news detection has a lot of smart teams working full time at all the big tech companies. Some organically grown human brains can't even parse some headlines as hypobole or reality.",1537460664.0
theblacklounge,"You can't do that. You can use a topic models to check whether the content of the article still matches, but that's about it. You can't check whether they say the same thing. 

But this is not just an NLP task, it's heuristics. Fake news comes from similar sources, and uses similar language. It's also not spin, spin isn't fake and most fake news doesn't even try to spin. It's completely made up often, or based on old and unrelated articles. Look for keywords on the site, not just the article. Check the category titles. Check the site's ranking on Google, check the ranking of the links inside. Check the date of the sources. Check the ads on the news sites...",1537452059.0
flexibeast,You might find [this recent research](https://www.eurekalert.org/pub_releases/2018-08/uom-fnd082118.php) interesting.,1537452540.0
almostinvisible,"In the deep learning vein, but have you thought about encoding comparison? Essentially, you encode the reputable news text into an n-dimensional vector and do the same procedure for the fake news text. Then you compare the two vectors. The idea comes largely from [face verification](https://sandipanweb.wordpress.com/2018/01/07/classifying-a-face-image-as-happy-unhappy-and-face-recognition-using-deep-learning-convolution-neural-net-with-keras-in-python/) (see the 'Encoding face images into a 128-dimensional vector' heading).",1537455945.0
bradcroteau,"With my highly trained integrated bullshit-o-meter. It’s an implementation of the extraordinary-claims|extarordinary-evidence algorithm, but the user interface requires application of critical thinking so it’s best to include user training in the licensing contract.",1537467509.0
Andy_Reds,"No, an analytical approach is very, very far away from reaching the level where it can solve problems like these. Machine Learning is well suited to such a task, as others have said.",1537467567.0
WhackAMoleE,"Here's the classic case to test your design on.

During the run up to the Iraq war in 2002, the New York Times ran a series of articles by Judith Miller claiming that Saddam had yellowcake uranium and aluminum tubes to build WMDs. Turned out this was a neocon lie, 100% fake news to start a war. 

But hey, it's the New York Times, right? So your algorithm or method should be able to identify fake news not only in the National Enquirer, but in the ""paper of record,"" the New York Times, which is the world's greatest purveyor of fake news. How many wars has the National Enquirer lied us into?",1537468441.0
exorxor,"If you are working on it, why are you asking us?",1537634653.0
llN3M3515ll,"There are a number of really good pod casts, personally like coding blocks. Noting they mostly discuss compiled languages.  They do cover news, but they have main themes per podcast, and go over books and other design and best practices.  I have listened to around 70 of them and have found it a pretty decent supplement to development.",1537452188.0
tfofurn,"I haven't listened to any of these; I'm just providing links to podcasts mentioned in other comments for folks on mobile.

* [Coding Blocks](https://www.codingblocks.net/)
* [Test and Code](http://testandcode.com/) (appears to be Python-specific)
* [Talk Python To [sic] Me](https://talkpython.fm/)
* [podcast.\_\_init\_\_](https://www.podcastinit.com/) (Python)
* [Javascript Jabber](https://devchat.tv/js-jabber/) (part of a larger  network of programming podcasts: [Devchat.tv](https://devchat.tv/))
* [Ruby Rogues](https://devchat.tv/ruby-rogues/) (also Devchat.tv)

edit: rouges -> rogues
",1537455414.0
type1advocate,"Test and Code, Talk Python to Me, Podcast Init, JavaScript Jabber, and Ruby Rouges all have elements of this. But they are all definitely talk show format. It would really be hard to make a podcast focused exclusively on what you seek without it becoming a snoozefest. Hopefully someone will post something ITT that proves me wrong, but I've listened to probably over 1k podcast episodes and haven't heard anything like you seek.",1537450567.0
ogre14t,"Coding blocks has a multipart podcast covering clean code, another for clean architecture, and even another on algorithmic complexity. ",1537454941.0
Isvara,There's computer science gossip?,1537458171.0
LongjumpingEnergy,"I’m glad you asked this. I don’t know any podcasts but am interested to hear about them also. 

You could check out Jack Ganssle, ganssle.com - he has a weekly newsletter and tons of articles on his site. Some videos also. He is in the embedded space specifically but is always writing about software quality and best practices in general. Highly recommended. ",1537461025.0
insburgnis,Don't get me wrong but I think what you are looking for is easier to find in video series from CS courses/classes.,1537462643.0
mcandre,"Doing What rustc Tells You, no commercial interruption",1537462950.0
SmallSubBot,"To aid mobile users, I'll link small subreddits not yet linked in the comments

/r/DigitalPhilosophy: Digital philosophy is a direction in philosophy/metaphysics that relies on computer science and theory of computation. It commonly assumes discrete and finite/countable ontology.
---------------------------------------------------------------------
Posts about digital philosophy together with posts close in spirit are welcome in this subreddit. For example posts about philosophy of artificial intelligence, artificial life/open-ended evolution are also welcome.

---
^I ^am ^a ^bot ^| [^Mail ^BotOwner](http://reddit.com/message/compose/?to=DarkMio&subject=SmallSubBot%20Report) ^| ^To ^aid ^mobile ^users, ^I'll ^link ^small ^subreddits ^not ^yet ^linked ^in ^the ^comments ^| ^[Code](https://github.com/DarkMio/Massdrop-Reddit-Bot) ^| [^Ban](https://www.reddit.com/message/compose/?to=SmallSubBot&subject=SmallSubBot%20Report&message=ban%20/r/Subreddit) ^- [^Help](https://www.reddit.com/r/MassdropBot/wiki/index#wiki_banning_a_bot)",1537429335.0
eternusvia,There are lots of challenges scattered over the web where you try to accomplish something with the fewest lines of code or the least memory. Could be fun.,1537414049.0
wizcheez,Maybe something like a security CTF to promote collaboration with others.,1537423314.0
,[deleted],1537430996.0
ghedipunk,"A single, uncompressed bitmap of a 1920x1080 screenshot in true color (32bpp) mode would take up, on average, 0.004 square inches on the latest generation of consumer hard drives that have a density of 1.34 Tbit/in2.  


Converting that bitmap to a .png file, in the case of my own desktop in an experiment I did while writing this, decreases this size to about 0.00000003 square inches.  (My screen has a LOT of same color space, so compresses down quite far.)",1537396259.0
nerdshark,"That's impossible to answer completely without knowing the specifics of the drive the image is being stored on. But in general, the bits would be so densely packed together you wouldn't be able to see them without a microscope.",1537394028.0
Vityou,Take the size of a transistor times the number of bits in the image.,1537394254.0
clownshoesrock,"It's almost too intuitive for me to explain..

But think of the counter-consequences.

If a computer could simulate itself faster than it could run, then you could run a faster simulator inside the simulator, and in turn have an ever increasing speedup.

Anyway, the simulator has to do things like fetch memory, but fetching the memory in simulator always takes as long as doing all the prep work in the simulator, then doing a memory fetch in hardware equivalent to what is being simulated..  So basically every thing you do has to be done in hardware anyway, but with more overhead on everything
",1537390308.0
TheMasterBaker01,"You could really just think of this like, if a computer can only physically process 10 bits per second, then no simulation inside that machine could ever process faster than 10 bits per seconds just due to physical limitations on the hardware.",1537390628.0
hypermog,In this house we obey the laws of thermodynamics,1537392172.0
nanodano,Could you pull a wagon and have the wagon go faster than you?,1537395245.0
Dev__,"You can use a light bulb to power a solar cell that powers a light bulb.

The second light bulb will never emit as much energy as the first.

Or you could go with a photocopier analogy - if you copy a copy of copy there will be a reduction in quality. There will always be a loss. As the other commentor suggested if there was some kind of improvement or increase within the system the consequences would be absurd. ",1537391008.0
olliej,"There are different reasons, such as the tautological/obviously not happening: if a system can simulate itself to run code faster than it could run the code itself, then you would just have the simulator run the simulator, recursively to get an huge performance win.

Another trivial way: take the fastest/shortest runtime instruction, and run it in the simulator. The fastest thing it can do is to just execute the instruction natively, which is obviously not faster than itself.

Now you might be thinking “the simulator could optimize the code before executing it”, but if it does that it could still just run the optimized code directly rather than going through the simulator.

Hence the absolute best case performance of a simulator is identical performance to running natively. To achieve that the simulator cannot do *anything* other than run the code directly, which means it’s no longer a simulator.

Therefore a simulator of any machine, running on top of a real version of the machine must be slower than the host machine itself.",1537404475.0
p_pistol,"I'm surprised to not have seen this higher up in the comments.

This is very closely related to the halting problem/Rice's theorem.

https://en.m.wikipedia.org/wiki/Rice%27s_theorem

Consider asking whether event X occurs at the end of the program.

A short proof would be that the length of the simulation = an overhead period of length t + the length of the simulation = t + t + length(simulation) ad infinitem = nt as n goes to infinity = infinity

Clearly t cannot be zero, or the length of the simulation would be zero (if we are assuming the simulation exists/is not trivial). 

Then the length of the program to compute the simulation is infinite. Therefore the function is undecidable as the algorithm to compute its output is non-terminating.",1537424072.0
alvahrow,"If the real computer is spending cycles running a simulation how can it run something faster or as fast as itself?

Think of it that way. ",1537391172.0
Dustin-,"For a super simple example, a NOT gate (a switch who's output is opposite of its input) can be ""simulated"" by stringing 3 NOT gates together in a line, but it would take 3 times longer to run than a single one. ",1537391853.0
ralphredosoprano,listen man how do you fit a box inside another box when the box is bigger than the box its not disgusting its just reality peace out,1537467552.0
spacemoses,This isn't to say that a computer couldn't be used to develop a different computer that was faster than itself.,1537406951.0
Reddit1990,"Its like running an emulator... Except the emulator has the same specs as your computer, so time needs to dilated for it to be able to run properly. I mean you only have one machine, you cant have two machines for the price of one. That would break the laws of conservation of information (pretty sure thats a thing, or it should be).",1537392923.0
SteeleDynamics,"That's because the real computer must use multiple clock cycles just increment the simulated clock 1 cycle. I think this is the most basic unit of any CPU.

I could be be way off, though...",1537397362.0
almostinvisible,"Modern computers are based on the Turing Machine theoretical model. By theory, this model allows a computer to simulate another computer. However, the simulation task itself takes up some of the computer's resources. The original computer's resources (processing power/memory) are thus split in the following manner: resources directed towards keeping the simulation running and resources available to the simulated machine. This means that a simulated computer's resources are always fewer than the original computer's total resources. Thus the simulated computer will always run slower than the original computer.",1537404743.0
macnlz,"Simulations are inherently inefficient: in addition to the work (W) the simulated computer is doing, there's the additional simulation stuff (S) that the hosting computer has to do.

So the work the host has to do is H = W+S

To increase the efficiency of the simulator, you'll want to reduce S as much as possible, so you get as close as possible to H = W + 0.

The most efficient ""simulator"" is to just build the machine you wish to simulate and use it. That way, you get H == W, and no extra work S is necessary!

Now, if the host computer is identical to the simulated computer, that means that the speed of the host is the upper bound for the speed of the simulated computer. And unless you're just using the host as a plain computer, any additional simulation work S will only slow things down compared to the original.",1537405720.0
wannagetbaked,It's like building a box to fit inside another box. ,1537405772.0
teawreckshero,"For a more tangible example, you could go download an xbox emulator, compile and run it on an actual xbox, and play games through it, but you should expect it to perform strictly worse than if you just played the games on the xbox itself without the emulator.",1537411307.0
tsvk,"As the simulating computer has to execute both the code of the simulator itself *and* the code of the simulated computer, it means that the speed at which the simulated computer runs must be strictly less than that of the speed of the simulating computer, since the resource usage of the simulator itself is always nonzero.",1537416623.0
brett_riverboat,"in short, every kind of simulation/emulation/virtualization will have overhead.",1537418628.0
agumonkey,"A practical side effect of this, someone tried to make a physical simulation of a snes cpu (to make a 'real' emulator), and it couldn't run fast enough on a fast cpu from 2006~. Others emulators are full of shortcuts (which sometimes leads to bugs because games often relied on weird hardware structures to do visual effects)",1537427985.0
Laafheid,"first we have to state that simulating oneself is not the same as just acting like itself, but some simulated action (as per defenition)

this means, in the case for paralel programmign for example, some resource has to be spend on making sure the simulation runs as it is supposed to, thus leaving less resources for the simulation, resulting in a simulation that has to run slower than the computer itself.

this has consequences for any **natural** simulation. whereas a game like chess can move from state to state unrestricted by time, a simulation of a physical system, in which part of the simulation is things that move around per time(unit), the simulation can never do this, as an extra action of keeping the simulation running is always required.

this does not mean that we cannot model natural processes faster than they occur. we can use paralel computing to compute faster than what a single computer could, (or any physical system), but we cannot do it within the same machine.
",1537441558.0
superjimmyplus,"Let's make this simple:

It can't be bigger on the inside than it is on the outside. ",1537453894.0
MjrK,"I think ""emulate"" is more appropriate for this context than ""simulate"".

I'm not aware of any ""law"" which makes the assertion. I think an easy way to express the concept is that an emulator cannot require zero resources.

In order for a computer to emulate another computer (virtual machine), it must run an emulator program which is what executes the actions of the virtual machine. The emulator program must add some overhead in terms of memory or CPU cycles.",1537454002.0
JusteEnzo,"Just simple Turing machine law:

&#x200B;

[https://www.quora.com/Can-I-input-a-Turing-machine-into-another-Turing-machine-If-yes-how-And-if-no-why](https://www.quora.com/Can-I-input-a-Turing-machine-into-another-Turing-machine-If-yes-how-And-if-no-why)",1538041655.0
taratoni,"never heard of it, but now that I think about it, if the reverse was true, then a computer would run an emulator, that is faster, that will run an emulator, that is faster... infinite recursion... any computer could be the fastest computer in the world, it makes no sense.",1537411485.0
I_edit_comments_bad,Yeah i have no clue what your professor is talking about. Whenever a game is running too slow on my computer I just simulate one that's faster and with better specs.,1537407226.0
pandasashu,Which is why the movie inception was backwards I am afraid...,1537414566.0
ocean07,This is basically saying we can never truly simulate the universe if the universe is a simulation. ,1537405181.0
Kulhu,I completely disagree. ,1537425022.0
fourdebt,"Let me give a quick example. Say your computer can only process 50000 operations per second (just an example). You wouldn't be able to create a process *on* that computer that processes *more* than 50000 operations per second.

Here's another example. The programming language Lua is written in C#. It is therefore not possible for Lua to run faster than C#.",1537392950.0
generic12345689,Your using physical capacity to run the simulation. So the simulation would be the remaining capacity on the hardware. (Maximum available capacity is less). That’s how I interpreted it not sure if true.,1537389532.0
Causative,"Why it can't be faster has been explained well by other commentators. There are two important exceptions though:

- You *can* have a computer run a simulation of itself that gives the same *results* and runs faster if you can improve the internal process used in the simulation to reach those results. 
- You can also get (almost) the same results faster by limiting the simulation to the observer.


**Examples**

**Improve the algorithm**: The simulation prints 1,2,3,4... but counts up to 1000000 between each number. Simulating this in a simulation would still require the counting up to 1000000 making it slower when run in the simulation. However - if you rewrite the simulation to skip the counting to 1000000 part between each number you could run it much much faster in a simulation but still get the same results. Note however that this is not the *same* algorithm, it just gives the same *results*. If the original is already the most efficient version this won’t work and you can't keep repeating this since all further optimization could also be included the first time.


**Limit the simulation to the observer**: If we apply this to a universe simulator the alien race may have decided to go for a simulation that gives similar results to real life. If they track every human, camera or recording device and only bother simulating parts of the universe that we are actively looking at, experiencing or recording in the resolution that we are doing this then they could save a lot of processing power and technically run an earth simulation faster than it would run in real life, especially the early part of history. As humans start looking at stars and studying atom collisions the simulation would have to get bigger and the resolution higher to maintain the illusion of a full consistent detailed universe. 

Computer games already use these concepts all the time causing a passing car to suddenly disappear after it turns a corner - or - damage you caused earlier can be gone moments later. The more stuff the game or simulation tracks the more realistic and consistent it seems, but also the slower it will run.


The issue with cutting corners in a universe simulation is that you have to stay ahead on everything to keep it consistent and maintain the illusion. Didn't bother to fully simulate the surface of the moon before telescopes were invented? Humans might notice 10000 years down the line that there should have been much more dust on the moon initially when compared to all the other moons they have studied in that time. (Not that anyone is going to take that as proof of a simulation, but it is a deviation from the original results which the simulators don't want.)


Finally the fastest way to run a simulation is not to only try to stay ahead of everything, but to actively influence perception and memory in the simulation targets. Anytime someone notices an inconsistency just wipe that memory. Humans will not notice that they missed something because if they do see it again it will just be wiped again. Of course if this becomes a daily occurrence that may cause the simulation to deviate too much from the original, but for specific scenarios this may be a perfectly reasonable approach.


So are we living in a simulation? Mathematically it may be highly probable. Perhaps we are in iteration #234563 of an alien grad student's project studying the ratio between humans obliterating themselves before getting off earth and humans successfully colonizing other planets. The ratio is not looking good, but the alien grad student is probably getting a good grade.",1537442442.0
roscoe_e_roscoe,Virtual machine?,1537457072.0
voneger,"Imagine a simulation that can do all that computer does. There has to be some overhead work to make it (the simulation program) run. For example, simulation will have a bit less memory available at any given time than computer would because simulation program itself takes some. Same goes with a lot of  potential actions that would probably require an extra layer of indirection that costs resources (even if only a little).

Hence, simulation necessarily runs on less resources than ""raw"" computer, and therefore can be reliably predicted to always be ""slower"".
",1537459081.0
znegva,"No, it's false. We can construct a computer made up of two subunits A and B. A and B can each execute the same programs, only B is faster than A. We can feed a program to the computer and instruct it to run on A. Meanwhile, B is idle. Now we can simulate this execution using a simulator program that runs on B. Since B is arbitrarily faster than A and simulating B's idleness requires no computation, we're faster than the original execution.

This is exactly what happens when recompiling a program so that it uses faster combinations of instructions.

The key here is to realize that a simulation consists in running a *model* of a machine. Depending on the complexity of the model, ""running the model"" may be somewhat slower or faster but it's different from replicating the original machine with infinite physical precision, if there were such thing.
",1537463736.0
TheoreticalFunk,"For the same reason you can't create a device that generates more electricity than it consumes.

Physics.  There is always going to be a loss.",1537467818.0
djimbob,"I think it depends on what you mean by computer can ""simulate"" itself.  If the simulation simulates every calculation that the real machine does, plus adds any small overhead to manage the simulation, the simulation will necessarily run slower.  If it doesn't have any overhead - e.g., the simulation does everything the exact same as the real machine -- I'm not sure how it is a simulation -- isn't it just the native machine running and not a simulation?.

That said, you can come up with scenarios where you can compare apples and oranges (that both accomplish the same thing) where a simulation works slightly faster.  E.g., you could run unoptimized code natively on a linux machine.  Or you could run the same code in a VM running on the same linux machine, but run the code through say a JIT compiler, and find it runs faster in the VM.  But if you think of it, while the same *user* code is running on both, when in one case you are running JIT compiler code and not doing a simulation of the same thing.

You could maybe come up with a scenario where you cheat a little (e.g., the simulation gets access to more cores or the simulated IO is faster because in simulation it's not doing the same thing), but again this is besides the point.  That point is the act of simulating hardware doing the same thing as native hardware, should only slow it down (unless in the simulation you somehow simplify what is actually being done).
",1537468607.0
Veedrac,"Unfortunately most of the answers here are a mix of subtly wrong and completely wrong. You are better off ignoring most of them. Unfortunately I haven't gotten to grips with this entirely myself, though I think I've got a slightly better idea of it than most of the people here.

As stated **this claim is incorrect**. There is a likely a correct version of this claim that your professor was attempting to get across.

Consider a simple program written for a simple kind of computer with a few simple instructions.

          set r1, 1000000000000
    loop:
          sub r1, 1
          br_if_nonzero r1, loop

A computer is an interpreter for its language. Generally speaking, they execute code in a straightforward manner. In this case you would expect the computer to execute 1000000000000 iterations of the inner loop; this is slow.

Now consider an emulator of this instruction set built in this same language. This might be implemented as a JIT compiler, and have basic optimizations. The optimization passes might notice a loop like the above, and after a comparatively small amount of analysis compile it down to just

          set r1, 0

Thus it is entirely possible for an emulator to emulate **a specific piece of code** faster than the host environment. No logical contradiction is involved here. In fact, this is practical reality; IIRC certain JIT compilers have been known to execute native instructions faster than the hardware, primarily by converting dynamic information into static information (eg., branch locations).

Instead, your professor is likely taking his comment from a subset of computer science that is primarily concerned with computers as tools to solve decision problems. Case 1 would be rather vacuous in this context, because you're not concerned with the speed of a particular program `P` that solves decision problem `D`, but **the intrinsic complexity of the decision problem `D` itself**.

This is a much more intuitive question, because the trick is really simple. Given an interpreter `I`, if `I(P)` is faster than `P`, then the program `I(P)` is a faster solution to the decision problem `D`. Then if `I(I(P))` is faster then `I(P)`, we can perform the same trick and just use `I(I(P))` as the program directly.

But it's still more subtle than that. Note that this only holds as an argument if there doesn't exist an infinite chain of progressively faster programs [`P`, `I(P)`, `I(I(P))`, `I(I(I(P)))`, ...]. Consider such an infinite chain of asymptotically faster programs, for example of runtime `O(n^2/1)`, `O(n^3/2)`, `O(n^4/3)`, etc. For any specific given input, we would still expect some finite member to be optimal; for this not to be the case, you would have to have a model of time that allowed infinitely faster operations; if your timesteps are instead discrete then it's obvious the chain can only be finitely long *for any particular input*.

Allowing for non-discrete time steps, can we make a computer model for which every program is part of such a chain? I believe you can, if you're willing to be aphysical. Consider a simple computer where every typical operation has a typical cost of 1. There is another instruction, `FASTER`, which takes 0 time and increases an ""oracle"" count by `2^-k` where `k` is the number of `FASTER` instructions executed total, starting at `1` for the first `FASTER` instruction. While the oracle counter is greater than 0, the time for any instruction to execute is reduced to a minimum of zero, and that reduction is subtracted from the oracle counter. Every program is required to end with the instruction `END`, which takes one cycle.

The overall speedup from `FASTER` instructions converges to one instruction, but more `FASTER` instructions is always better. Because the speedup is capped at one instruction, you cannot profitably put `FASTER` instructions in a loop; no number of them will compensate for a single control flow instruction. However, the program `{FASTER, P}` is always faster than the program `P`. This produces an infinite chain for *any* legal program `P`.

Of course, people don't use models like this. They use models with discrete timesteps that in some sense model a count of reductions applied to the program. In such a case you obviously don't have these infinite chains for every program. People also use models that have at least some relevance to physical systems; it is exceedingly unlikely that an instruction like `FASTER` is physically realizable.
",1537441867.0
NowImAllSet,"This isn't a law, just a clever musing. The first part is a funny nod to the notion of [turing completeness](https://simple.m.wikipedia.org/wiki/Turing_complete). If a machine is turing complete, it can do anything a turing machine can...that includes even the machine processes themselves (simulation). The second part is just a reflection on the limitations of that. Think about it intuitively. If a machine could run a simulation of itself at equal (or greater) efficiency, then it would be infinite computing power. Just spin up a simulation, and a simulation inside that, and one inside that, etc.

An analogy could be to imagine a 3D printer (computer) that is capable of 3D printing a replica of itself (simulation of itself). Now say you have a small object you wish to 3D print. It's never going to be equally or more efficient to print another 3D printer to then print the object. There's not really a formal law that states this (to my knowledge), it's just basic logic. ",1537395706.0
CowboyFromSmell,"No, definitely not a law.

[PyPy is a Python implementation](https://pypy.org/) in Python that can run faster than the C implementation of Python with less memory and better concurrency. So that’s a similar problem, if not the same problem - virtual machines are machines too. 

You can [read more about how PyPy works here](https://stackoverflow.com/a/2592094) but the gist is that they used higher level abstractions available in Python to make optimizations unavailable to C. 

This really isn’t unlike CPUs. Several Intel CPUs have included a decoder step where it converted regular x86 op codes to simpler RISC-based op codes. This enabled them to make optimization’s they couldn’t have otherwise. So in a sense it happens at a hardware level already.

They also keep runtime metrics about branches to make better predictions and do speculative execution. It’s by no means unreasonable to have a VM do a better job of predicting branches. It’s just a matter of running the optimizer at a level higher.

So yes, we have built virtual machines that can emulate themselves faster than the original. I see no reason why this absolutely can’t be true for non-virtual machines as well.

Edit: clarity

Edit:

Alright, everyone’s heavily downvoting me, so let’s set some bounds. If you read to the bottom I have an example of such a hardware simulator.

A simulator mimics the **behavior**. For a CPU, this means it implements the instruction set and each instruction returns the same result for each instruction. You could even argue that it needs to have the same clock speed, but you can’t ever argue that it has to have the same instruction throughput. Otherwise, it would be almost impossible to build simulators because they would usually have a lower throughput (be slower). 

Intel has successfully used clock speed as a measure of CPU performance ever since the Pentium 3. While clock speed has implications on overall performance, it’s far from the truth. 

Long ago, to achieve instruction throughput, CPU manufacturers started pipelining. Simplistically, this means that each instruction gets partially computed on each cycle. Suddenly, instead of processing one instruction at a time, there are lots of instructions in flight. On modern Intel CPUs, there can be as many as 168 in flight instructions at any given time.

So now the problem is keeping the pipeline full. If it stays full, all the hardware is busy. If a “bubble” forms, we’ve lost opportunity to optimally utilize the hardware. This is a very hard problem.

IF statements are one of the primary places bubbles would form. You don’t know which instruction to load until the IF condition is fully evaluated. This is where speculative execution comes in - you keep track of which branches are usually taken so that you can make better guesses. 

The problem is CPUs have a hardware budget - there are a finite amount of transistors they can use for keeping branch statistics, but a much larger number of branches to be taken (and scenarios under which branches execute, etc). 

A software-based simulator should have a much larger amount of memory available to keep branch statistics, so should have a larger capacity to make speculative execution decisions.

If I were to make such a simulator that executes code faster than the original, I’d do it very similar to what JIT compilers do (e.g. JVM, CLR, etc).

1. To start, don’t pass instructions directly through to the CPU. Instead, put instrumentation around branches to keep track of statistics. 
2. After about 10,000 iterations of a loop, “rewrite” the instructions that are passed through to the CPU such that the CPU makes much better speculation decisions.

In step 1, the simulator would be significantly slower than the original. In step 2, the hot code could be significantly faster than the original CPU would have executed it.

Again, this isn’t new. It’s what JIT compilers do. You can argue if they’re *effective*, but you can’t tell me they’re *impossible* (which is fundamentally what OP’s professor said).",1537407803.0
Andy_Reds,This might be better suited to r/programming. ,1537467365.0
Clockwork8,Is this a referral link?,1537376526.0
watsreddit,This is not computer science and doesn't belong here. Try /r/programming.,1537377558.0
testq90,"> I know that it is important that schools needs to be ABET accredited.

Actually, it's not. In engineering maybe.",1537296999.0
onebignerd,"I'm a CS major at University of Maryland University College.  I'm 45 with a family. I've been in and out of college since 1991. I transferred all my credits to UMUC 10 years ago and decided to go back for a career change. You would have to check if all the needed CS courses are available online. My current class is a hybrid where I drive to College Park once a week, but all the content and classwork is online. I know UMUC isn't the best school for CS, but it fits my life well. ",1537313996.0
honduranheat,"can you attend a local community college online? finish your associates, then transfer anywhere else. ",1537326884.0
crossCounter23,WGU comp sci ,1537323020.0
millenial2go,University of london is offering an online degree in cs through coursera this 2019. Google for details. You cannot use FAFSA. ABET accreditation is unimportant for CS,1537331732.0
jordanaustino,"Dakota State University, Regis University, University of Illinois - Springfield, for degree completion there is a program at Cal State Monterey Bay.

Regis is very proud of being the only fully online ABET accredited program, but for CS that doesn't really matter like it does for engineering.",1537333395.0
ice109,"georgia tech online ms cs http://www.omscs.gatech.edu/

no question",1537318314.0
mitchpllease,"Typically you wouldn’t do either. You would have a separate privilege table and a table to join your users to privileges. The pair of primary keys of your user and role would be the the only information in this table and each row would be unique. If you wanted to find out the privileges a user had you would select all rows in the joiner table with the users primary key to retrieve the corresponding keys for the privileges. This way you only input a privilege once, reducing the chance of introducing a rouge privilege on each write to the database. You also save space as you do not have to store x instance of the string “user” or whatever roles you have... just a number ",1537284562.0
JCaesar13,"Web caching, multicast, dns, content distribution systems. ",1537312036.0
spicy45,Oh boy this takes me back,1537289585.0
jx4713,"(loves(john, mary)) AND (∃!x : (x != john AND loves(x, mary)))

&#x200B;

Note that there is an order of precedence on these relations and quantifiers but I am being purposefully scrupulous in order for you to see exactly how the sentence corresponds.",1537277104.0
Crysis456,(For all x such that x is in the set of people) (x loves Mary implies x equals John and vise versa),1537290422.0
UnconstitutionalSir,"Also note: the first symbol means (for all values of ''), the second (there exists (ie there is at least one '')), and I believe the last one means (there does not exist (ie there isn't a single one of '')).",1537277101.0
UnconstitutionalSir,"For all John's there exists Mary. AND for all but John, there does not exist Mary.

I think that's the translation. Replace it with symbols and you should be fine.

Important: this is a mental exercise to get you use to logic notation. It is important to do yourself. I suggest doing at least another 3 in place of this so you can get used to it.",1537276677.0
cirosantilli,"My setup might also be of interest: [https://github.com/cirosantilli/arm-assembly-cheat](https://github.com/cirosantilli/arm-assembly-cheat)

Features:

\- runs both on RPI and QEMU user mode from x86 host

\- build and run all examples in a single command

\- GDB step debug just works

\- dozens of easy to use assertions to test your expectations",1537280467.0
Gonffed,We're doing this at UCSD using a 200 node pi cluster if you're interested in comparing notes,1537331210.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1537244823.0
mr_clauford,Depends on CPU architecture because amd64 provides more registers and more advanced instruction sets that may increase performance but not ARM.,1537242693.0
ReginaldIII,The field of Computer Science will never be the same.,1537196857.0
AndASM,The major change here is that window functions were added.,1537195765.0
dabombnl,Awww yasssss column rename... And I'm spent.,1537198956.0
ZaynThomas,"Thankyou for update I've been looking for it.

Visit our new app - knowledge search engine app 

[http://www.heuroapp.com/](http://www.heuroapp.com/)",1537266911.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1537168246.0
jaktonik,"I think you'll get a much better response in r/learnprogramming - best of luck, and searching for python tutorials is a great place to start",1537163092.0
sjh919,Pro tip: search engines are your friend,1537171388.0
krimin_killr21,Go to office hours. Your professor or TA would be very glad to help you out I'm sure.,1537158638.0
dnabre,Better to try  [/r/ProgrammingBuddies/](https://reddit.com/r/ProgrammingBuddies/),1537151950.0
mostlikelynotarobot,"I have very little programming experience and am currently learning C. Mind if I just sort of sit in your discord and watch development? Just as like a way to learn? My C is fairly rudimentary atm, but I might be able to help a little bit.",1537236674.0
jourmungandr,"Sounds like you're taking about the Needleman–Wunsch algorithm. The edges of the table count up from zero for edit distance. They represent a insertion/deletion at the beginning of the respective sequence if I remember correctly. 

",1537150629.0
c0deb0t,"If you are asking about what the values in those 3 cells you mentioned mean, then I can explain a little.

Recall that the edit distance algorithm is the edit distance up to (eg. some prefix of the strings up to) a certain index i in the first string and an index j in the second string.

As such, the top left cell is the edit distance with both strings being empty, which is 0. The top right cell would be the edit distance between the full first string and an empty second string. Finally, the bottom left cell would be the edit distance between the full second string and an empty first string.

I hope that helps!",1537160838.0
fokinsean,"Postgres 

/thread ",1537123548.0
drvd,"Any.

Your requirements do not impose any constraints in the database.",1537168237.0
IdealImperialism,"If you go beyond simple tagging searches and into the space of relationships I'd look at graph databases. Apache Tinkerpop is a good option, if you're only every storing datasets with a set number of tags then something SQL based like postgres.",1537124950.0
Wulfnodh,"If it is just the tags you want to index, any relational database will do. For more sophisticated text search, Microsoft SQL Server and MongoDB both offer good full text search capability.",1537129991.0
CPROGRAMMERS,U can use my sql,1537125483.0
,"I don't like to outright give you an answer: but here is a clue.

1 ) Repeating digits in hexadecimal will lead to repeating digits in octal : could you use that fact to argue how many potential answers there are?

2 ) F in hex is 17 in octal. What could you add to that to get a digit with 2 7's ?",1537119653.0
mystic_thunder,"Think about it in binary, you want 4 adjacent ones in the correct position for the F, and 3 adjacent ones for each 7. So 0b111111 = 0x3f = 0o77 = 63 works",1537119852.0
htotheinzel,"You need to chop it up using CIDR. If they aren't specific on block sizes, a /26 breaks evenly into 4 x /28s. Each has 16 addresses, though not all are usable",1537039120.0
,[deleted],1537098191.0
_lll-_-lll_,Looks cool. I will try and see ,1537103374.0
AdituV,"The dots do not denote function composition, they instead provide scoping as a shorthand where it is unambiguous, as it can get hard to read with parentheses instead.  Rewriting it with parentheses, you get `0 = \f (\x (x))` which is indeed the same as `0 f x = x`",1537018504.0
whymauri,"if you're interested in deep learning, Ian Goodfellow's book expects absolutely no background beyond high school algebra.",1537160820.0
keksper,"3Blue1Brown's video series is a good conceptual introduction.

https://youtu.be/aircAruvnKk",1537192347.0
-S7evin-,"I think I've already heard it...
http://futurama.wikia.com/wiki/What-If_Machine",1537009278.0
BobFloss,You couldn't *possibly* have linked to the actual tool!,1537023685.0
maelstr0m1337,[link to the tool itself](https://pair-code.github.io/what-if-tool/),1537025077.0
voronaam,Google reinventing the wheel again. Have they seen [Orange3](https://orange.biolab.si/)?,1537029784.0
pulsar512b,"Saw ""What-if"", thought I was in /r/xkcd and got very confused.",1537052086.0
xasteri,"PhD student at top 20 school here.

Starting from the bottom:

If you don't want to do research then a PhD is a waste of time and money. I can't stress this enough. People think that a doctorate is a degree ""above"" MSc and that it's ""better"" to have. Truth couldn't be far from that. It's a *different* kind of degree that shows that you can do original research. Frankly, there will be no advantage from getting a PhD if you want to be a software engineer.

As far as school name goes:

Yes, people will stop more frequently and pay attention when your CV says ""Harvard"" instead of ""University of Shitville"". But that's just it. If you are good at what you do and you get involved in projects so that employers are able to see your work then that's what really matters in terms of job prospects. Loaded GitHub repos with interesting stuff show a lot more than your grades and your school name.

Good luck :)",1536951444.0
noam_compsci,"Both sides have really good arguments. 

You cant deny that Harvard, Yale and Oxbridge have an unbeatable clout that is a huge leg up. 

On the other hand, 99.999999999999% of people in the world, and probably like 99% of successfully people, did not go to these places, hard work after the first few years of uni can bridge the gap. 

I cannot stress enough how '99%' of successfully people did not go to a top uni. At the same time, on average going to a top uni is the best bet. All things else the same, going to a better uni is better than going to a bad uni. 

But that just means, dont keep 'all things else the same'.

Read something along the lines of ""A year out of uni, its all about your school/degree/score. 5 years out, its about what you worked on and experience you gained. 10 years out and its about the people you know"" on LinkedIn the other day. 

If your uni/college is not 'that' great, focus on perfecting 'working on good projects of high value' and 'networking and building relationships'. ",1536975493.0
acroback,"It depends.

It is easier to get into industry with a degree from top school because internal circle and brand name. 

But experience trumps everything. Down the line, no one cares where you studied as long as you are good and can deliver.",1536973849.0
plgeek,"Free advice, go get a paying job. Most good industry positions will pay for the MS degree. If you really find the desire to get a PhD, you can do it after a few years of industry experience. That experience will make you a better student. The only challenge is learning to not get used to being paid well, so you have the ability to take a big pay cut when/if you go back to grad school. ",1537130313.0
hindmost-one,"Replace `pow_2( x, n / 2 ) * pow_2( x, n / 2 )` with `sqr( pow_2( x, n / 2 ) )` and you're fine.",1536996492.0
c0deb0t,"I think you are supposed to save the result of the pow call in a variable so that you don't need to call it twice! Also for your analysis the ""depth"" of the function call stack will be log n so it would be 2^(log n) = n calls

Edit: also your your calculation for when the exponent is odd is slightly off, it should be a little more similar to the even case",1536956508.0
soto_okami,is this for Yu Chen?,1537156433.0
Edumacated1980,What did you end up getting for this? I got (logn)\^2 for running time. I really had no idea about the space complexity. ,1537223301.0
hextree,"Consider what happens when n is a power of two, and apply Master Theorem or a more simpler argument. Runtime is O(n), space complexity could be O(n) or O(log n) depending on how stack and garbage collection is handled in the language.

It is an intentionally bad algorithm for computing pow. Computing pow(x, n/2) only once instead would make it O(log n) runtime.",1537287815.0
wtallis,"The OS sees hardware threads as individual CPU cores. There's no distinction along the lines of cores 1-14 being physical cores and 15-28 being ""virtual"" cores; they're just two equal front-ends to shared execution resources. If the OS interrogates the hardware it can discover which cores are sharing resources behind the scenes, much like it can discover NUMA topology information, but while this information may be useful for the OS scheduler to have, it is not necessary for actually using all the cores.

The distinction between hardware threads and OS threads is the same as the distinction between CPU cores and processes—and many operating systems don't even have a real distinction between threads and processes. The number of CPU cores/threads is fixed in hardware, and the number of software threads/processes is unrelated. You can have fewer processes than cores in which case you'll have some idle cores, or you can have more processes than cores in which case the OS's scheduler will have to divide time processor time among multiple tasks waiting to execute.",1536948882.0
evil_burrito,"Just to add to what /u/wtallis wrote, the OS can make more threads available than there are physical CPU cores because, generally, processes spend more time waiting for I/O than they do waiting for a CPU instruction to complete. Therefore, the OS can swap out a process which is waiting for I/O and swap in a different process to the same physical CPU core. This allows more efficient usage of the physical CPU cores.",1536949249.0
CaptKrag,"The multithreading capabilities in hardware are effectively just hardware context switching. A single core shows up as two and software (os) can treat it as such, but it's actually just a single core with hardware optimized context switching between two threads.",1536985877.0
m1ss1ontomars2k4,"CPUs don't really have threads. They have cores, each of which can usually run 1 thread. On some CPUs, such as yours, enough of each core is duplicated (while still only counting as 1 core) that CPU manufacturers and OS writers are willing to present 1 core as 2 to the OS. 14 cores 28 threads is a way of saying there are only 14 complete cores, but each one has enough of another core on it that you'd probably see a performance gain running up to 28 threads.

The OS has threads.",1536989814.0
SanityInAnarchy,"So far, I don't see anyone explaining what CPU threads actually *are.*

So... yes, the OS does think of all the CPU threads as individual cores... mostly. If it's smart, it'll know about the real cores and use that to decide which threads to use, so that if you only have 14 OS threads/processes to run, they'll each get their own core.

So, a CPU core has a bunch of individual pieces of hardware attached to it that does different things. There's an actual circuit that just adds integers together, and another that multiplies them, and another that compares things, and so on. To grossly oversimplify, a CPU thread is the thing that translates that instruction into sending electrical signals to the actual hardware that carries out the computation you're asking for. But it only does one thing at a time. If it's dealing with an instruction that says ""Save this value to RAM,"" it could sit idle for a *crazy* number of cycles waiting for that to happen.

CPU threads do a lot of work to keep the core busy anyway. They'll break instructions down into tiny pieces that touch different parts of the core, so it can run those in parallel. (Maybe one instruction is loading a value, and another one is doing some math...) They'll cheat and read ahead to see what you're planning, and rearrange those instructions to optimize your code on the fly into something that can better utilize the CPU. There's more to this, and [Meltdown and Spectre](https://meltdownattack.com/) happened because CPUs are so dependent on this for performance that when they reach a branch (like what you get from an if statement), they can't wait to find out whether the conditional is true or false, so they just guess.

Having *multiple* CPU threads is just another way of trying to keep the core busy. One thread might be stuck waiting hundreds of cycles for something to load in from memory or whatever, but that means the other thread is free to use all the other bits of the core, like the ""add two numbers"" circuit, as long as it has some instructions to process. (Of course, if both threads are trying to use the exact same hardware at the exact same time, one of them wins and the other has to wait its turn.)

That's why they pretend to be cores as far as the OS is concerned. If the OS has 28 threads/processes that all need to do stuff at once, it can just have each of them run on a separate hardware thread.

---

Also, all this stuff about OS threads and processes missed an ELI5 option. Think about single-core machines with a single hardware thread -- those machines could only do one thing at a time. So if you give the OS two processes (or threads) to run, it just switches between them really fast, fast enough that they each feel like they're running on a CPU that's half as fast as the one you have. (And that's only if they all are actually trying to use CPU at once. Often, one process wakes up for a bit, does something, and goes back to sleep...)

Your CPU can do 28 things at once if they're different enough. So if you have more OS threads than that all trying to use CPU at the same time, your OS will *multiplex* them onto the 28 cores -- it'll do 28 things at once, and then, dozens to thousands of times per second, it'll pull one or more of those OS threads out of the CPU to make room for another one. (Or, one of the OS threads will go to sleep, so the OS will wake up to swap it out for a thread that's still ready to run.) And then, behind the scenes, your CPU is doing the same thing with sharing *pieces of a core* between two hardware threads.

---

It's turtles all the way down, by the way. Languages like Golang and Erlang that are designed to handle large numbers (tens of thousands or more) of *language-level* threads will tend to launch one OS thread per hardware thread, and then run all those language threads on the OS threads.",1536992209.0
gabriel-et-al,"Others have already provided a nice explanation, now I'd like to point you to the Round Robin algorithm. The geeksforgeeks site has (as always) an article describing it.

https://www.geeksforgeeks.org/program-round-robin-scheduling-set-1/",1536976423.0
acroback,"CPU thread is a hardware construct while OS thread is a software construct.

Today multicore processors are capable of running independent tasks simulatneously. 

But this also means a lot of context switching can happen between tasks. This switching happens because CPU has finite number of registers, then finite amount of Data and instruction Caches. When a running task's next instruction or data set need to be loaded from memory, it is basically not doing anything useful for current tasks, so it is context switched out. Now CPU thread means we have double the registers and other low level constructs but still single execution core. It now can store double the tasks in it's lower level hardware. OS doesn't care, it sees 2 sets of registers it assumes 2 cores and assigns 2 tasks to same core.

OS thread is a misnomer. You mean software thread, it is similarly a software analogy of what I just explained except now each thread has it's own stack and context but shares resources like open file handles and heap memory. Some OSes see these user threads as regular tasks and schedules them on logical cpu cores which use CPU threading. 

Tldr; CPU thread is a engine, OS thread is what runs on this engine. OS decides what gets to run on which core. CPU decides to kick things out of it's core on it's whim.",1536996141.0
,"Theory isn't procrastination. In research we say ideas are cheap, but it's no slouchy gig to have to actually \*prove\* your theories. It's not as if theorists walk around all day only thinking about stuff, they have to produce something in order to contribute to research as a whole. Do you know anyone who works in this space/understand what they do? 

&#x200B;

There is a symbiotic relationship between industry and research, with many companies integrating research into their companies (all the Big 4). Industry gets the research really late in the game; for example most ML stuff used right now is super duper old research. And you could also look to the origins of blockchain as another example.

&#x200B;

That being said - I love making things!",1536938336.0
,[deleted],1536925098.0
Ryuzaki_us,"I like what I read and simply wanted to add that you should make things you are passionate about at the moment. 

I started with no knowledge but in time, artificial intelligence started to peak my curiosity. Now I code on that and it has been interesting.",1536923496.0
Ruko117,"> Why do you educate yourself?

> To make yourself more employable, to earn more, to make yourself valuable. Right?

Anyone else creeped out by this line?

",1536962940.0
CubKits,"So, what should I make? (I should add I'm just starting in the computer science field). I feel that when it comes down to it, I simply have either too many big ideas that would require years of development, or I have no ideas. Where do I start with making my portfolio? What kinds of things do people just ""make""? Does it have to be new and revolutionary?",1536931654.0
Mr_Again,I know I'm reading a high quality medium/dev.to article when they spend the first 15 paragraphs trying to convince themselves their idea isn't worthless before diving into their 3 weak bullet points somewhere near the end. ,1537007856.0
crossCounter23,Great article! Thank you!,1536926638.0
cioio,Is tour target audience people who are before/between jobs or business? - because the rest are making things every day..,1536962374.0
Aeshma_,"Actually I like more abstract topics in CS, they are not pracrical though, but I feel more comdortable with them. May be its just me.🤔
",1536929134.0
_m242_,"I totally agree with you :)
",1536944888.0
,[deleted],1536962825.0
skulgnome,"Fuck you, I won't do what you tell me.",1536928689.0
,[deleted],1536941418.0
jmite,"1. Everything is a tree, except when it's a graph
2. The essence of recursion isn't self reference, but breaking a problem down into smaller parts, using the recursive call, and reassembling back into your answer
3. Code is primarily for humans, not for machines, and should be structured to be read and and understood by humans.",1536904332.0
purleyboy,Everything is a tree,1536891894.0
skulgnome,Doing your own homework. Not looking for shortcuts. Showing up for lectures.,1536926752.0
zetsubou-tan,"1. abstraction
2. everything is a graph
3. languages/hardware/things were built to be intuitive",1537518583.0
igglyplop,"What...? Well, I guess Calc 3, statistics, and discrete math.",1536888512.0
ChipMania,"Hi guys, just looking for some advice.

Finished my degree around 4 months ago now and managed to get a first class honours in CS and maths! I've only just got back from travelling about a week ago as I didn't want to go into the industry straight away and took a 3 month long holiday. Now that I'm back I'm basically wondering how long I should spend brushing up on data structures, algorithms, personal projects and interview technique before I start applying for jobs? I'm living at home currently and it's getting frustrating telling my parents that no, I've not applied for any jobs again today because there wouldn't be any point with how unprepared and rusty I am.

Any help would be appreciated! ",1536913195.0
Robbebeest,At university we have to ask and answer a research question about Fourier transforms. Does anyone have an interesting research topic about Fourier transforms?,1537112500.0
wtallis,"Your measurements may include compiling the GPU kernel and definitely includes moving the data between system RAM and VRAM. These are far more time-consuming than the trivial addition task. 

This is a great example of how benchmarks may not measure what you think they're measuring, and why micro-benchmarks in particular are hard to construct in a way that they produce meaningful relevant results.",1536865901.0
mrgreywater,"First of all, I have no idea how well optimized and parallelized that compiled cuda code is, but I assume most of the time is wasted by transferring the roughly 400-650MB of data between the ram and gpu-vram over pci-express. 

edit: see https://code-maven.com/showing-speed-improvement-with-gpu-cuda-numpy",1536865826.0
LgDog,"As many has commented, there are lots of overhead you are not accounting for.
You can amortize the overhead cost by repeating the operation:

    count = 1000
    start = timer()
    for i in range(count):
        C = VectorAdd(A, B)
    vectoradd_time = (timer() - start)/count

assuming that the code is compiled only once and the contents of A and B are transferred to GPU memory only once then the overhead cost will be divided by __count__ and you will get a closer value to the actual GPU computing time.",1536866626.0
,"If you have a GeForce GPU like you said, I recommend the library [Cupy](https://cupy.chainer.org/) for that sort of computation. It's sort of a Numpy-clone for the GPU and deals with interesting concepts of allocating space in the GPU for parallel computations and so on. ",1536877800.0
Jonno_FTW,"Use pycuda to make a better benchmark, you can have fine control over copying things between host and GPU. Then you can time only the add operation on the device. ",1536874957.0
csp256,"As an addition to what everyone else has already told you:

If you need speed on a GPU, don't assume a library will handle things gracefully... Especially not a python library which may or may not be written for convenience and consider 2x a good speedup on embarrassingly parallel tasks.

Prefer to write performance sensitive GPU code yourself. I've always opted for cuda and I understand there are performant ways of using it from python.",1536876897.0
devssh,Have you tried giving it a larger load.. instead of 0.02seconds how about giving it a job worth 20minutes. Maybe gpu just takes more milliseconds to initialise and then it's way faster,1536902325.0
mrDanglingPointer,"GPU's are much better at handling linear algebra given the nature of how a lot of gaming engines rely heavily on vector geometry to calculate position and 3D space. [Source](https://drive.google.com/file/d/1Na6r1d4RRP_GoSJ-x1gAj8TsyIeF53s1/view?usp=drivesdk). Not to mention that there are significantly more cores in the GPU that are much much slower than the cores in a CPU because they're specialized to handle a smaller subset of calculations very well that pertain to gaming more than general processing. That's why hashing can be faster on a GPU than a CPU because it often involves the same calculations over and over again. So depending on how the code or libraries you're using are optimised, will depend on how much advantage they can take of the actual power of the processor type it's running on.

Also, depending on how your processor is handling the calculations, and what the environment is looking like as the code is running. Your processor may be context switching your code out to run system tasks much more frequently than your GPU, even though it's a multi core, whereas your GPU might have less load on it to run basic system tasks, so is able to give your program more time on it's main cores, or bigger time slots depending on how it queues processes. Also, that's why your times won't be measured accurately if you're basing this off of time taken in real time (by the looks of things you record an initial time, and then calculate the difference) because that time will also likely include things like context switches (and time allocated to the scheduler in the CPU) and times that your processor spent running other system programs, or even having a browser window open while running etc. You might want to try calculate it using clock times instead of physical time [resource for python](https://www.pythoncentral.io/measure-time-in-python-time-time-vs-time-clock/). 


",1536915533.0
reallyserious,Did you run the exact same code on the old setup with i5 and gtx950?,1536910242.0
DebateHelper,"The GPU only realizes its full potential when you can effectively use ""shared memory"" (it's really a per-block cache). In your program, each element is accessed only once. Therefore, every access is to global memory. If your calculation used the same data more than a few times, moving that data to shared memory (cache) would speed things up drastically (shared memory accesses take only a few cycles, whereas global memory accesses take roughly on the order of 100 cycles). In cases where data is used only once, it's most likely not worth offloading the calculation to the GPU.

Also, CPUs are really good at vectorized operations like that because caches are pretty smart (in this case, the cache would take advantage of spatial locality).",1537398428.0
cp5184,"Might be just how hard nvidia crippled it's fp32 performance to protect it's tesla and quadro sales?

Or maybe the 1080 needs a little more handholding in issuing the instructions in parallel as ""wraps""?",1536872881.0
ReservoirPenguin,"I'm a Computer Scientist, not a ""programmer"" #notmyholiday",1536837737.0
olliej,"A filter is just time/space optimization you put in front of your regular data structure. It depends on you expecting the majority of lookups to bee false because you're replacing

&#x200B;

`lookup(foo)`

with

`if (filter(foo) is present) lookup(foo)`

The idea is that if most of your items are likely not going to be in the backing data structure, then the cost of filter + optional lookup is on average faster that simply performing the lookup. It gets even better because if you don't have to perform as many true lookups you can change your ground truth data structure into something that is slower, but uses less memory.",1536814919.0
klausshermann,[https://www.amazon.com/Java-Foundations-Introduction-Program-Structures/dp/0134285433](https://www.amazon.com/Java-Foundations-Introduction-Program-Structures/dp/0134285433),1536785570.0
trackdaybruh,"Oof, you gotta do your own due diligence.",1536856208.0
symmu,"authors: John Lewis, Peter DePasquale, Joseph Chase",1536784378.0
CPROGRAMMERS,"Its Okay can you give me url of this problem.  Because if inserted two same keys that its satisfied binary tree property.  If you have any suggestions please give me me url.  Or insert sequence of data.  And thanks for advance. 
☺️
",1536780565.0
Parizivial,It should be Press start,1536763369.0
Elekhyr,I'm in the wrong subreddit.,1536764259.0
TaXxER,"> cryptography 

.

> too easy

What? Crypto is one of the most hardcore math subfield of CS. If the basic stuff gets too easy for you you can go into elliptic curve cryptography or post-quantum cryptography, which are hot research topics within cryptography at the moment.

&#x200B;",1536735265.0
Andy_Reds,I'm pretty certain you have no idea what cryptography is.,1536773546.0
noam_compsci,"Do something that has been done, but do it with blockchain. ",1536978696.0
paypaypayme,"you can approximate the orbit of the earth using discrete differential equations. use the equation for gravity to calculate the force on the earth by the sun. choose a step size so that the earth moves some small distance (this is the discrete part, the number of steps per orbit should not be too large). In the simple case, the earth's orbit will be circular. If you add a couple other planets though, the orbits will become elliptical.",1536717874.0
jakemar5,You could provide a proof that shows that unique solutions exist for the Navier-Stokes Equation!,1536718322.0
ahelwer,"A lot of scientific simulations approximate differential equations which are too difficult to solve symbolically. For example, you can simulate the propagation of electromagnetic waves through space with the [finite-difference time-domain method](https://en.wikipedia.org/wiki/Finite-difference_time-domain_method). Ever wonder what would happen if you turned on a lightbulb inside a box lined with perfect mirrors? Discretize Maxwell's equations onto a grid, run the program, and find out!",1536719030.0
whymauri,Mathematical finance prediction models using the [Black-Scholes](https://en.wikipedia.org/wiki/Black%E2%80%93Scholes_equation) equation would be pretty neat.,1536723473.0
pykidd,You can try doing deep learning programming. It involve some calculus in finding the optimum parameters based on the errors. (gradient descent),1536718073.0
jourmungandr,"Look up velocity-verlet integration and do a ball and springs force-directed graph layout engine. 

Edit: can't spell but that's old news",1536719031.0
vikingville,Any N body problem. An example would be simulating the evolution of a protoplanetary disk.,1536718902.0
todeedee,"I'd take a look at SUNDIALS: it is a pretty huge diffeqn solver package. 

They have a boat load of applications listed here: https://computation.llnl.gov/projects/sundials/uses-sundials",1536726374.0
crwcomposer,The SIR compartmental model of the spread of disease is a perfect contender.,1536726678.0
v3nturetheworld,"Solve temperature differentials of an ice cube on a hot plate in two dimensions using [Poisson's Equation ](https://en.m.wikipedia.org/wiki/Poisson%27s_equation?wprov=sfla1). I did this for a numerical methods class, honestly it's pretty involved and it is a partial differential equation but it was really fun.",1536729249.0
nate_12,You could simulate the double pendulum using lagrangian mechanics,1536735942.0
LearningMachinist,An integral solver?,1536748201.0
overweight_neutrino,"Sorry to add on, but does anyone have any interesting projects that incorporate probability and statistics? ",1536720027.0
stefantalpalaru,Why would you repost this off-topic shit here?,1536674353.0
solinent,"Programming is one of the core aspects of getting a computer science degree. A program is the manifestation of all of the CS theory, and the beauty about computer science is these theories about computation are all able to be brought to the machine, ultimately.  The machine is not necessarily a concrete concept for more theoretical computer science topics, but to do well in these areas you will need to be exceptional at mathematics.

If you don't enjoy programming, I suggest you find a project whose ends make you happy: computer graphics is quite fun and can get you into more serious topics, for example.

If you are looking to become an academic beyond your undergraduate education, it is possible to get by without coding too much if you focus on the purely mathematical side of things. However, I don't know any successful computer science majors who make it in the professional world and cannot code.

Depending on the programs available to you the theoretical approach may not be viable at all. Most theoreticians I know are exceptionally good at both--programming in addition to mathematics.",1536628882.0
hextree,"If you've given programming a serious attempt, and think it isn't for you, then probably Computer Science also isn't for you. It's not clear why you would even be interested in CS if you say you don't have a liking for programming, are there aspects of CS that attracted you?

Nevertheless, you aren't really *meant* to have extensive programming experience at pre-university level, since you learn all that stuff properly in your first year. So the fact that you haven't been taught good programming isn't a major issue. But your lack of enthusiasm for the field likely would be.",1536757461.0
Doberman123456,Computer Science without programming is like doing Physics without Maths... (Or CS without Maths for that matter!),1536667034.0
noam_compsci,"week one of my MSc Comp Sci at my uni: Program the game 'chess' in python, basically from scratch...

Thats like 20-50+ hours of hard core python writing, debugging and repeating. 

I think Comp Sci is very different to IT. A masters in Computing or IT may be better?",1536978864.0
JCaesar13,"Here's my 2 cents. 

According to what you mentioned, your dislike for programming could be a result of the fact that you feel you weren't taught programming the right way. So, before committing to a decision (about uni) either way, maybe try learning programming from someone who is good. Maybe hire a tutor for a few sessions, maybe check out some reputed free online courses. You don't have to pursue it until you become proficient. Most universities have beginner level programming courses which will help you get there if you choose to join a uni. 

The purpose of this exercise is only to figure out if you have an aptitude/liking for programming after being taught well.",1537312341.0
Ravek,"It's obviously not true that a polygon can have only two triangulations, so I don't know how you're planning on proving this. ",1536672216.0
igglyplop,"Take it during college, because that way you've had the same foundation as your peers for the professor to build off of.",1536604914.0
delarhi,At least they get a correct error code return.,1536607304.0
oohshandan,Nested (for)klifts?,1536611875.0
lissofossil,This is why men die earlier that women,1536603844.0
stringParameter,I bet their boss threw an exception,1536616809.0
ummmIdontknow1332,r/OSHA,1536608270.0
chidoOne707,Why didn’t they just used the bigger forklift? There was no need for the smaller one.,1536611222.0
natinusala,`fork | fork`,1536665982.0
manys,That's a paddlin'.,1536622921.0
jmwozniak,"If the middle guy fell, it would have scored as success.",1536604788.0
its-ya-boi-uhhh,I expected to see two more forklifts lifting up forklifts but i guess these people like simplicity ,1536618222.0
jokerfan1988,Going up. And uhoh,1536608611.0
Gordon-G,It was close ,1536624981.0
leafiest,set -o pipefail,1536626580.0
Jezon,Thought this was /r/2healthbars ,1536666705.0
stefantalpalaru,50shadesofofftopic,1536620525.0
Liz_Me,Evolutionary algorithms are a last resort antibiotic.,1536581160.0
fanmanutd11,"This is a really cool concept, but how many practical applications are there? It feels niche to me. But I could be wrong.",1536599660.0
E46_Nerd,We don't do your homework here. ,1536510692.0
pilotInPyjamas,"Yes. There is a unique minimum spanning tree if all edge weights are unique, so any algorithm that finds a minimum spanning tree will find the same tree. The Wikipedia article on minimum spanning trees has a proof of this under ""uniqueness""",1536496931.0
TomvdZ,"Just FYI, at our university we run automated plagiarism checks on all student submitted code. If two students submit similar code, this will be noticed. If it's for an assignment, make sure your instructor is okay with sharing code, because we'll catch you and report you to the disciplinary committee for academic misconduct.",1536485748.0
theemptyqueue,"Finally, Iv'e been waiting since windows 97!",1536474329.0
notkraftman,I thought the two file systems didn't mix?,1536475022.0
sinjp,"You've always been able to do this with a registry edit, I had to hardcode the username but this is it [https://github.com/Manouchehri/bash-WSL-context-menu/blob/master/Ubuntu-bash.reg](https://github.com/Manouchehri/bash-WSL-context-menu/blob/master/Ubuntu-bash.reg)",1536482485.0
Sjeiken,You can already do this today.,1536488155.0
Fjolsvithr,"ITT: No one knows Linux Subsystem for Windows exists, misunderstands that this announcement is just a small edition to the context menu.",1536504179.0
ebookit,"In the late 1990s Spencer The Cat in Computer World heard about a Microsoft project named Winux that used the Windows GUI over a Linux shell. I guess they scrapped that for WSL?

Still I like to dual-boot Linux and Windows or run Linux in VIrtualBox.",1536478754.0
Wulfnodh,I never thought I would see the day - this is awesome!,1536475728.0
ccundcf,"Not quite the same as the real thing, fortunately / unfortunately .",1536476684.0
rsgm123,How is this related to computer science?,1536499466.0
glha,"Even though my current needs are fulfilled with cygwin, I guess a full featured bash would invite me to try more fancy stuff.

  


I'm really liking this little steps towards OS's integration. Being able to develop the same thing using MSSQL at workplace on Windows and on a docker at my home Linux setup is already great, now this. ",1536496965.0
b1ack1323,You can add anything to the context menu with the registry.,1536506474.0
doyoueventdrift,So how do I start Linux on windows 10? What variant is it?,1536511684.0
MotionKing,Is there a downside to just opening powershell here then typing “bash” to get bash up?,1536475977.0
kunaldawn,Zzzz.. install Linux for pits sake,1536474898.0
phunkygeeza,extend embrace extinguish ,1536484526.0
grizzly_teddy,Omg awesome!!!,1536483846.0
justsomeguyiguess87,"I don’t get it.. is this a joke? Is it emulation? Why would Microsoft bundle this, it can’t be a common request?",1536489751.0
RepliesOnlyToIdiots,"Depends on the domain of computer science you’d like to work in.

If computer graphics and gaming, yes it’s useful. If doing related scientific work, obviously yes.

I have physics in addition to CS, but because it’s a personal interest.

I don’t need it for my work (compiler and platform library support), but I do wish I had better stats to improve my performance analysis work.

So, where do you want to work? What domains would you like to work in? Find the companies in that intersection that you’d like to work for, and then find out what they are looking for to hire someone doing the work you want to do, including which fields, courses, universities, etc. that they’re interested in, etc.

Don’t aim wildly and hope — learn what you need to take you to your goal.",1536445703.0
reality_boy,"I would push on through with physics. Not only is learning perseverance its own reward but it helps you see a practical application for calculus.

Me personally I have been a developer for 20 year and have been a game developer for 9 years. I took plenty of math and physics in college but still wish I had more.",1536449587.0
noam_compsci,"I really do not think so. I am starting a masters in computer science, with an econ/finance background. I was told I needed A level maths (the schooling right before uni) to be considered and at least 1 year UG maths to be competitive. I had 1 year UG pure maths /stats and 3 years of metrics/financial maths. 

As part of my masters pre reading, I've been told to brush up on Discrete maths and told Grossman's Discrete Mathematics for Computing is a good benchmark of what might be needed. 

Not sure about France, all my experience is the UK. 

edit: A lot of posters from the USA are saying their experience is very different. For reference, I went to a social sciences uni for UG which had no physics/science courses and so possible that we covered similar topics but not under the branch of 'physics'. Not sure. Sorry for not being more help! ",1536443528.0
nah248,"Depends where you live, for cal state schools(where i go)  they require two physics classes. Both involve using calculus. ",1536444101.0
error_0x00,A CS undergraduate will most likely require physics. Mine required physics 1 and 2,1536457009.0
Zuitsdg,Robotics require some physic basics.,1536446643.0
andyligent,Depends on your school Cal States do require it but some UCs don't ,1536447999.0
jeanbria,I was required to take physics one and two. I was considering sticking with it ,1536450557.0
hextree,"No, you don't need physics, just focus on maths.

Even if you choose to go to a uni program which requires a small element of physics, high school maths is far more helpful for this than high school physics.",1536536851.0
heap42,"No. beware though you need maths, so if the math of physics is the problem, then you might have trouble",1536655194.0
RacerX5150,"Yes, math and physics are the cornerstones of any good computer science/engineering program. ",1536449218.0
obp5599,"What country are you in? The previous guy isnt from the US so it can vary. As a senior in comp sci, in the US youll probably have to take a physics 1 & 2 course but thats it as far as classes go. You wont actually need it, its just a req for some reason",1536443730.0
acroback,"No, don't think so.

But Physics is cool nevertheless, who doesn't love good old classical mechanics.

Algebra, calculus, probability and some statistics is all you will need.",1536458147.0
krum,You should ask your professor.  ,1536440021.0
WebNChill,"Emulate anything you can find similar online. If he doesn't like what's in your report, use that opportunity to request for feedback. Yes, you might take a 'hit' in the first round but it let's you get feedback for the next report and if adjusted accordingly it shows to him that you're able to learn. Possible points in his book. 

It's definitely difficult to say what he wants because every teacher has their own 'way' something should be taught.  

Hopefully that helps some.",1536444980.0
hr0m,"Web app shouldn't be a first programming project.

&#x200B;

CS researchers don't ""research"" popular ways to program web apps,",1536438859.0
Gavcradd,"This whole question shows a lack of understanding of how the web works. Everything about the web is built on incremental improvements and ""hey, how can we do this..."" hacks over the past 30 years. Someone developing a reasonable web app needs to be familiar with at least 4 ""languages"" (I use that term losely) - HTML, CSS, a client side language (JavaScript) and something server side (PHP?). Add on various frameworks, databases, server software etc and you've got a huge range of knowledge needed.

The thing is, you can't replace all of that with one ""thing"" because they each do a separate job. It's like asking for one car that you can use on the road, and to go inside supermarkets, oh and be able to fly. And float. You have cars, trolleys, planes and boats that do those things perfectly well.

And I agree with the other poster - it's not a suitable first project. But it's far from a disaster. I'm an experienced programmer and I find it lovely and comfortable. Stop putting your own thoughts onto others.",1536439874.0
combinatorylogic,"Because web is an overengineered pile of shit, and no sane researcher will ever touch this crap with a 10ft pole. This leaves only the not so sane ones.",1536500339.0
lrem,"That would be an industry/engineering interest. Research/science is about finding out what is possible, not about making polished products.

You might have better results asking this on /r/programming.",1536439006.0
dmead,"There actually have been a bunch of attempts at this.

this is one i know how to find off the top of my head.

http://links-lang.org/

edit:

but like the other comments say, most of the web has been build on incremental improvements. ",1536454497.0
Gavcradd,"What? I love creating web apps, using PHP in particular but also Python + Django. How on earth is it a disaster? It's grown organically definitely but not sure how you got this conclusion?",1536437886.0
FormofAppearance,"I started my cs degree at 26. My interest in cs grew directly out of my interest in philosophy. I don't feel like I made a transition but rather that I'm just getting deeper into things I've always been into (creative activities, philosophy, politics)",1536438972.0
WompusCatt,"I’m not a CS major, but i do have an IS degree and write software for a living.  I started my college career at age 38. When I turned 18 it was 1987 and my parents didn’t set aside money for my school, and back then it was still possible to get a somewhat decent job without a degree, so I started working for a tile installer.  Did that for a few years then started waiting tables/bartending. Although it was very fun, I eventually figured out that I couldn’t do it forever, so I made the decision to go into the military.  After 7 years in the military I had 2 back surgeries and had to get out. I loved being in the Navy, and hated that I had to leave, but it did give me the opportunity to finally get into college. After getting all my basics out of the way I took a couple of entry level coding classes, javascript and the like, and found that I absolutely loved to code, especially troubleshooting bugs.  And thats what I do for a living now.  I have been working for The Walt Disney Company for 5 years now, writing and fixing software for their finance people and I couldn’t be happier! ",1536447368.0
sderyke2002,"I focused on Science in HS, got into electronics and Nuclear Power in the Navy, then got my BS and MA in Nuclear Physics.  Only got into Computers around 35 years old because I saw no opportunity for high energy Physics in the community my wife and I decided to call home.  I have been doing it for 28 years and have gone from Network Admin to Cyber Security (CISM and CEH).  Make a lot more money than a Master of Physics makes - so not a bad decision.  Hope to retire on the next few years, I am 63 after all.",1536449791.0
MrAckerman,"There are only a few comments, so I’ll pitch in:

In high school I was smart, but had a chip on my shoulder.  I had no discipline or direction and barely graduated. 

Spent some time after working random jobs to support myself and taking a community college class here or there.  Nothing came of it though, because when I wasn’t working I was getting high, chasing girls, and being an all around shit head. 

Late 20’s hit and I was tired of working for peanuts.  I met a girl that I knew I was going to ask to marry me one day and knew if I wanted things like a house and a family I would need a career.  CS seemed like a practical choice, but still really interesting to me personally.  Started bartending to pay to start college again. It was hard, because I never knew how to be good at school.

At 34 I started at my first job.  It’s nothing glamorous.  I write almost exclusively test code at a large company you’ve probably never heard of.  It was hard to get that first job, but now that I’m established it feels like I’ll never have to worry about a job again.   I’m making great money for the first time in my life.   I’ll probably be able to afford to buy a house in Southern California within the year.  I never though I’d have a chance to do that.  

I had a non traditional path and work history.  I felt I could use that to my advantage getting through the early interviews.  I also participated in a robotics club, which gave me a lot of ammo for interviews with engineers.   Life is good now and I wouldn’t have done a thing differently. 

Sorry for bad grammar, typos, or unclear writing.  I just tried to bang something out real quick on my phone while waiting for a haircut. ",1536450284.0
xasteri,"I was in my 4th year of my 5-year diploma in Mathematics when I took a class in Cryptography offered at my university. At the end of the semester we covered some topics on e-voting and I was fascinated by the huge impact that a complete (meaning, one that would deal with all the issues that electronic voting has) scheme could have on democracy. It would become way easier for people to vote from the comfort of their homes. Not only participation in elections would increase significantly, but also authorities could hold referenda for important issues with minimal cost compared to today's elections.

Now, I'm actually doing my PhD in Cryptography (with some side of Complexity Theory).",1536443787.0
,[deleted],1536446430.0
WompusCatt,You’re never to old to go to school.  I still take classes to stay current on new tech,1536457342.0
beyphy,"It's actually funny. I had always loved computers as a teenager and was actually a comp. sci major before switching to philosophy. I tried my hand at several programming languages with web dev and I hated it. I took all the prerequisites to code in college but never took an OOP class. After switching my major to philosophy and didn't touch coding for several years. 

In my mid 20s, I got a job as an analyst and I started learning Excel really well. Learned a ton about the program. Then got into macros. Once I reached my limit with macros I started reading an exhaustive VBA programming book. After what felt like banging my head against the wall for several months, things finally clicked one day and things all made sense. I was able to automate routine aspects of my job which took several minutes in seconds. This saved me time, which gave me more time to code, which saved me more time, etc. 

This was maybe three years ago. And I've been more or less studying programming since. I generally learn programming by reading books and I've read a few thousand pages in the last few years. The topics I've read include python, SQL / databases and I'm currently trying to learn real OOP in C# (At around page 450/800 in an excellent but difficult C# book.) It's so crazy how much learning programming has changed me. I never would have guessed that it would change the structure of my thinking.

I don't know if I'll go back to school for it. I may try to get a masters of CS if I end up finishing this C# book and can really do OOP. I've been trying to switch industries for some time now (I work in accounting) but I've been having difficulty. I think I'll be taking some SQL Server database courses I found online and will try to switch into doing that. That'll probably be some time early next year if I keep reading this C# book until I'm done with it.",1536461035.0
mkhanreddit,I moved from statistics to Computer science.I don't know why i moved but then i realize. Now you have moved so move on and don't think about too much about that i did wrong or right.,1536602035.0
Crypto_Alleycat,"I took some cs in college and got way to stressed out. Now I’ve learned it all on the go or through online tutorials. Just keep picking projects, playing with things, and asking questions. 

Whether you go an official route or not is totally up to you! ",1536511946.0
minno,Video encoding is something that people need a lot of CPU power for.,1536435826.0
bluehavana,"Compression (say LZMA) could be an interesting application as it's not easily parallelizable and isn't already covered by mass manufactured processors like GPUs.

I'd definitely love to see the database stuff though ",1536438643.0
jugglist,"I sometimes see FPGA stuff from high-frequency trading company blogs. It’s probably worth it to figure out exactly what the application there is. I assume the meat of their work is finding the fastest path to read something from the network card and then decide when to send a good, already formed response as quickly as possible...",1536444253.0
unit_circle,Protein folding,1536462811.0
Plazmatic,"A small idea, constant time sorting small arrays, 

A really big one would be swapping FPGA implementations for different tasks, for example, can you, in code, use the FPGA for one instruction, and switch to another completely different code path for a later part of a problem, enabling the speed bonus of an FPGA for multiple programs within the same single program which utilizes the FPGA. ",1536458961.0
mogorrail,Is this a masters thesis?,1536460016.0
MCPtz,And compare it to a CotS video card,1536456288.0
MarcoPoloC,ray tracing I doubt it would be faster than doing it on a gpu but it would interesting to see how fast,1536451575.0
combinatorylogic,"The only thing that is really interesting with such a hardware is the minimal latency and maximum throughput you can achieve between your CPU core and the FPGA fabric. Previously it was far from impressive, making those integrated solutions not any better than simply talking via PCIe.

Also, I'd recommend to stay away from OpenCL on FPGAs. Just learn an HDL already.",1536500567.0
PsychoticallyAmiable,"Maybe test the load and execute times for the Neural Nets you're testing. Load a net, run a single set of input through, load a new net, cycle rinse repeat. Would be nice to see some data for various flavors of neural nets as well.",1536460210.0
,Many ANN applications.  Training.  Classification.  Etc.  The training especially is terribly expensive computationally.  But FPGAs have the right structure to perform the tasks in parallel.,1536462526.0
spinwizard69,This is a bit different but you asked.   This is likely only feasible if the FPGA has access to its own I/O but it would be cool to see the hardware implement much of the real time requirements for a CNC controller.   Think about LinuxCNC and the FPGA support cards for that software.,1536500356.0
spinwizard69,"Another idea, work with one of the SETI teams to do signal processing for the search for ET.   

&#x200B;

I look at it this way find something fun that you are interested in.   Your list is nice but lets be honest it holds many of the common FPGA uses that are already researched extensively.   While signal processing is also heavily researched at least you can have some fun with SETI, maybe even get some sponsorship.   In any event the idea is to get off the beaten path.",1536500767.0
floridawhiteguy,"I think your ideas are all great choices.

I'm curious how well the FPGA talks to the registers, and if certain crypto routines can be more effectively implemented there.
",1536512710.0
zamadatix,VM networking performance. I believe Intel already has integration between it's FPGA and DPDK/Open vSwitch.,1536514702.0
Samrockswin,This is a pretty canonical benchmarking suite: [NAS parallel benchmark suite](https://www.nas.nasa.gov/publications/npb.html),1536514946.0
olliej,Raytracing! I used to dream of making an FPGA accelerated raytracer. Of course that was back when I was at university. At that time we had the glory of register selectors for “dynamic” hardware t&l :),1536516863.0
Darkknight512,"Mathematical modeling and simulation. Finite element analysis for aerodynamics, electromagnetics, etc., Simulations of systems described with ODEs. ",1536524518.0
iwantashinyunicorn,Constraint programming. There are some lovely bit-parallel propagation algorithms that are just slightly too fast on a CPU to be worth offloading to an FPGA when the FPGA is external hardware.,1536590091.0
sevhead,Can it run Crysis?,1536445950.0
pcbeard,"Try implementing the CPU from this book:

https://www.nand2tetris.org",1536513865.0
Kamajal,"MINING, FUCK JUST USE FOR CRYPTO-MINING. ",1536444568.0
PM_ME_UR_OBSIDIAN,"Sipser's Theory of Computation; Pierce et al.'s Software Foundations workbook.

Have fun!",1536413500.0
arichi,"What topics do you want to learn?

_Computer Systems: A Programmer's Perspective_, by Randal E. Bryant and David R. O'Hallaron is amazing.  They have their programming assignments online, too, if you want to do them.  Not sure if that counts as ""extra lectures"" or not.",1536429624.0
swift-swoop,"Any of the dinosaur OS textbooks by Silberschatz and ""Introduction to the Theory of Computation"" by Sipser. ",1536420098.0
jet_heller,"I had made it through the 3 Internetworking with TCP/IP books before my school even offered a networking class. When they finally got it and I took it, book 1 was the textbook for it. I laughed.

As I worked at a computer store that had more computer book titles than software titles at the time, I had at least cracked more open and perused them than I ever even had textbooks in college.",1536439026.0
SteeleDynamics,"SICP, CLRS, Dragon Book, Cinderella Book, ... that's all I got for now.",1536443469.0
acroback,"Has to be Morris Mano's book on Digital Circuits. What a well written book.

SICP for pure Comp Science.

Algorithm book by Robert Sedgewick is another gem. I mean the old one which has 2 parts. Better written than cryptic CLRS.",1536458022.0
foreheadteeth,Serge Lang's Algebra is excellent but if you can read it cover to cover without lectures you may be a genius.,1536419086.0
TaXxER,"Currently reading ""Introduction to Process Algebra"" by Wan Fokkink without having lectures on the topic. It's a bit dense, but if you have a proper math foundation it's very readable and I think a very good introduction into the topic.

[https://www.springer.com/gp/book/9783540665793](https://www.springer.com/gp/book/9783540665793)",1536917160.0
net_nomad,There's only 6 nodes. It's solvable.,1536381331.0
dasdull,"Correct me if I'm wrong but I think for wheel graphs like this the problem can even be solved in linear time. 

Assume we do not want to visit any vertex twice (the task does not specify that, but the graph seems to satisfy the triangle inequality).

Then w.l.o.g. the only valid routes are going clockwise around the circle and using the inner node D to skip exactly one outer edge.

That makes O(n) routes, each of which can be checked in O(1).

Edit: changed star to wheel",1536388911.0
marvel2010,"There are whole books on how to solve the TSP in practice. There are lots of questions you can ask, including:

1. Are there special graphs where we can solve the TSP in polynomial time?
2. Can we find an approximation algorithm that quickly gives a good solution that is guaranteed to be within a certain factor of optimal?
3. How can we find ""good solutions"" quickly (without theoretical guarantees)?
4. If we really want the optimal solution, how quickly can we find it?

Each of these questions leads down a slightly different path:

1. I'm not aware of any terribly useful results in this direction (but good results are known for other NP-hard problems).
2. On metric graphs, the famous 1.5 approximation is the best known.
3. In practice, people use simulated annealing or ant colony optimization or genetic algorithms. All of these are slight variations on a theme: take a bunch of good solutions, study them, and try to make them better.
4. When people really want an optimal solution, the best tool is Integer Programming. There are various IP formulations. Some are provably better than others, but it could also depend on the structure of the graph.",1536390147.0
BloodOfSteel,">Is there a solution to the Travelling Salesman problem   

of course there are many solutions but this one is ""NP-hard"" which means there is no fast solution. for example Held–Karp algorithm solves the problem in time O(n^2 2^n )",1536381463.0
Neker,There is a subtle complexity with the *F* node : it's in another dimension and the weights of the vertices leading to it are not determinist ;-),1536390547.0
denny31415926,"Well this got more attention than I expected.

I guess what I really wanted to know; is there a simple method I can teach to a 17 year old who has never even heard the phrase ""graph theory"" before, that won't take hours to find? There's been several people chiming in to say there's ""only 120"" possible paths to check, but that's still far more than can be reasonably expected for a high schooler to be able to do.",1536397534.0
Ikor_Genorio,"The main distinction between this problem and the travelling salesman problem, is that this does not have a (direct) path from each node to each other node. This makes the problem easier.

I believe the solution you want to find in this case, is a some kind of Euler circuit with the extension of minimalizing the weights.
This is a lot easier than the TSP.",1536398530.0
Akari_Takai,"So, given the fact that the graph is a Halin graph (specifically a wheel), the complexity of TSP in this case will be O(n) due to the number of tours in a wheel being O(n).

However, that doesn't matter since this graph has so few nodes that even the most disgustingly inefficient TSP solution will solve it immediately.

I quickly [wrote](https://gist.github.com/akaritakai/8ea515dd20821568a842323dd241b379) such a program, and the shortest route is ABGEDCA (and its reverse ACDEGBA), which has a distance of 66.

&#x200B;",1536422057.0
lpprof,"in ocaml:

    let g = [
    (1,[(2,11);(3,14);(4,9)]);
    (2,[(1,11);(4,12);(6,8)]);
    (3,[(1,14);(4,18);(5,22)]);
    (4,[(1,9);(2,12);(3,18);(5,8);(6,6)]);
    (5,[(3,22);(4,8);(6,7)]);
    (6,[(2,8);(4,6);(5,7)])
    ]
    
    open List
    
    let voyageur s0 g =
      let cmin = ref [] in
      let pmin = ref 100 in
      let lsg = sort compare (map fst g) in
      let circuit c =
        let dep::_ = c in
        let fin::_ = rev c in
        dep = fin
        && (for_all (fun s -> mem s c) lsg)
      in
      let rec parcourt s c poid =
        let arcs = assoc s g in
        let arcs1 = filter (fun (s1,p) -> mem s1 c) arcs in
        let arcs2 = filter (fun (s1,p) -> not (mem s1 c)) arcs in
        let arcs = arcs2 @ arcs1 in
        iter (fun (s1,p) ->
                     let c1 = s1::c in
    		 let p1 = poid + p in
    		 if circuit c1 && p1 < !pmin
    		 then (cmin := c1;
    		       pmin := p1;
    		       Format.printf ""poid %d\n"" p1);
    		 if p1 <= !pmin
    		 then parcourt s1 c1 p1)
    	      arcs;
      in parcourt s0 [1] 0;
      (!cmin,!pmin)
    ;;
    
    voyageur 1 g;;
    (*
    # voyageur 1 g;;
    poid 89
    poid 85
    poid 76
    poid 72
    poid 69
    poid 66
    - : int list * int = ([1; 3; 4; 5; 6; 2; 1], 66)
    *)

&#x200B;",1536523989.0
Mysterious-Stranger,"As an addition to other comments, there is an approximation for larger graphs using [Christofides algorithm](https://en.wikipedia.org/wiki/Christofides_algorithm).",1536416102.0
GNULinuxProgrammer,"There are very good heuristics for TSP. UWaterloo even has a TSP solver, here: http://www.math.uwaterloo.ca/tsp/concorde.html",1536420754.0
GandalfTheBored,Abgdeca is the answer is it not? ,1536466824.0
jmite,Throw it at Z3 and you'll get a solution quickly enough,1536473002.0
SiliusSodus,It's considered an np complete problem meaning that there's no solutions better than what we already know. At least that's what I remember from my linear algebra class ,1536397012.0
lavatory_member,"I think there are 2 solutions - 
ABGDEC
ACDEGB",1536398756.0
TheRealRosepierre,We had a question like this on the ACT in 2017... I’m pretty sure I got it wrong... ,1536383614.0
abramN,Acdegba,1536390144.0
vsuontam,ADEGBACA ,1536389915.0
paez74,"I have been to a couple of Hackatons in my student life ( currently a softmore) I have been 2nd place in most of them. The way I get my Ideas is by talking to people about which problems they might have, looking for a necessity or if there is non creating a necessity they might not see. Ex. I did an application that follows the buses from my University in real time for a hackaton after talking to students about any problem they think needs fixing. They dont even need to be computer science students they can be anyone really. Hope this helps :).",1536349529.0
Cheddarlishous,I once did an Alexa skill that connected to GitHub! It didn't work... But if it had.....,1536348445.0
kyle1elyk,"My team won a couple times last year, look at some of the sponsors for inspiration.  
Mango hacks last year was partly sponsored by Express Scripts so when we got there we brainstormed ideas in the health field and ended up making a pill dispensing Raspberry Pi. We used AWS for Rekognition and were able to dispense pills based on who was requesting it. https://devpost.com/software/drug-dealer  
Also take a look at problems relevant that week; things that solve problems that judges have on their mind effectively are pretty good",1536352881.0
WhackAMoleE,The first rule of discreet mathematics is that we don't talk about discreet mathematics.,1536348747.0
thatfreakingguy,"What is your n in this case? The number of tokens? In that case, you're handling each token exactly once, and all the used functions can be considered constant, which would result in a runtime of O(n).

But the functions will certainly have a runtime dependent on the number of characters, so if your n is the number of characters, you would have to look up the implementations of atoi, c_str and the stringstream.",1536314971.0
reality_boy,"Generally big O is concerned with the big picture. You have to imagine what the trend would be if you put in a string with a million characters in it. In this case the time taken would grow linearly so it is O(n)

The key point is big O is not a measure of efficiency but complexity. It does not care if you do 1 or 1,000,000 operations per character. It only cares if the complexity goes up with more characters. 

It answers the question ‘is there an upper bound on my data set size’. For big O(n) there is no upper bound but big O(n^n ) would max out any future computer relatively quickly.",1536321079.0
metaphorm,"lets develop some thinking tools to figure this one out. 

when the recursive call is made what is the difference in the size of that input compared to the input that the parent function was called with? looks like input is getting bitshifted right by 1 on each call, so the input string decreases by 1 bit in size each time. 

when the function is called, how many times will it recursively call itself? looks like two times in this function (except in the terminal case, where it is not called recursively at all). 

what does this mean for runtime? it means that we have a runtime of 2*n. we have to recurse at least n times to process the entire input string, and we run through this twice on each recursive call because we consider two operands on each operator. 

in big O notation we can ignore the constant scalar though so it's just O(n). ",1536343611.0
,"Assuming that all sub-functions are o(1) then the entire function is o(n): You call it once for every token in the input stream. 

If any of the subfunctions are m = o(n) or worse then the function is o(n \* m), since you'll call that slower function n times, once for every input.  ",1536319129.0
everything-narrative,"Interpreting arithmetic is generally difficult to do correctly and in a time complexity other than O(n), n being the number of tokens.

It would be clearer if you implemented your function as a loop, using a vector to store operators you cannot immediately satisfy, paired with their 'lone' operands if any. This would very clearly show that you run over the string exactly once: O(n).

You can test your algorithm by generating strings: set the number of operators to be k, and the number of numerals to be k+1. Write either an operator or a numeral to the string, all the making sure the number of operators you have left to write is always at least one less than the number of numerals you have left.

Last, avoid using `atoi`, as it cannot report errors. Write a helper function that uses an `istingstream` to attempt to read an integer, and reports an error if the operation doesn't advance the stream.",1536349817.0
Vitus13,"Think about it like this, there's no way you can find the answer to the equation without looking at every part of the equation at least once. So it's at least linear (O(n)). Sub-linear algorithms (such as log(n)) have a way of not looking at some of the data. In a binary search, you rule out half of the remaining data each time you make a comparison. You can't do that here. Next you want to consider if it's superlinear (you have to consider each input multiple times). A strong indicator of super linear behavior is nested loops. In this example, after you evaluate an operator you never need to evaluate it again, so it's probably linear or very close.",1536381482.0
trout_fucker,"If you want to, do it. Nobody cares about your age.

Just don't be like the jackass who interned for us over the summer, then threw a temper tantrum and quit because the person set to manage him was 19 and he was 32. When you do your internships and become a junior realize that it's a training opportunity for you and the people working with you.

Edit: and yes it's worth it if your passion is development.",1536292232.0
PhoneyBone,If you’re passionate about CS I’d switch. Make sure you leave school with the degree you want. If programming is what you want to do go for it. 2 years in isn’t to late for a switch. If you’re worried about the age thing for interviews don’t worry about it no one is concerned about your age coming in just what you know. ,1536293478.0
rbobby," CS or software engineering? CS to me has always been the study of algorithms and pretty math focused (e.g. the most efficient way to do X). Software engineering to me has always been about the best ways to build applications/games/systems. And there can be lots of cross over (data science, machine learning, etc, etc) that really requires both.",1536293810.0
MemesEngineer,"Does your school offer a minor in CS? Most of CS isnt coding, its math and algorithms. I would recommend a minor in CS with your finance degree, but if you really hate your degree then switch. Finance is a good degree to accompany with CS and you will totally be able to be a software dev. SQL is a nice language to learn for you as a finance/data language, but make sure you learn the basics first.",1536296338.0
shuerpiola,"Im 28 and have been pursuing my BS in CS even after I completed my BA in English. Best decision of my life! I am currently leading the VR development team, am some yet-to-be-determined position in the VR and innovation club.

My VR development team is being sponsored by Amazon! 9 weeks and we present our stuff. Wish us luck!",1536301668.0
Prof-,It’s never too late! I’m non trad myself coming off a previous degree. In the end age I’d just a number. If you enjoy it keep going! ,1536296111.0
NarwhalSquadron,"If you’re worried about being limited in what you can do with the degree, then go CS. There’s much more possibilities with what you can do with a CS degree. Naturally I’m biased, but I think you’d actually learn more skills with a CS degree and get more bang for your buck. ",1536298008.0
Itakitsu,"If you do, make sure a CS major at your school is what you want it to be. Often you can take the CS courses you want without being in that major. And often a CS major will require you to take courses you don’t care about (e.g. theoretical comp sci). 

There are also options like dev boot camps, or you can just self study and start applying for roles and see what happens. 

Edit: Wait just 1-2 semesters? That’s like no time, if you aren’t worried about tuition/board costs just do it haha",1536298715.0
zvekl,"Been pondering this too, I already graduated an eon ago but at 40 I’m still yearning to go into programming. Always had a passion for it and I still do lots of basic front end and some backend stuff for work web development as side projects. Had a mcse before 2000s and know basic php JavaScript... just never had formal training and need to develop good base. Didn’t major (but almost minored) in CS because it had lots of math requirements that, though probably academically passable, didn’t fit with my interests. 

Now as my kids are growing a bit older, I wanna see if I can rectify the “do what you love and never work a single day” issue I’m having. ",1536299841.0
baetylbailey,"A CS degree is not mandatory for many careers in software development.  I'd closely examine the requirements of the jobs you might want. You can take the core courses, and pick up the rest later.",1536333593.0
Vinality,"Hey, [Geeks for Geeks](https://www.geeksforgeeks.org/java/) got basically everything you may need to learn Java and other important concepts on data structures later on. ",1536285271.0
thesia,"There are plenty of tutorials that can teach java. I find Khan Academy is usually a good starting point for most people. Another good one is this guide at [https://www.tutorialspoint.com/java/](Tutorials Point) which will transition to more complex topics.

Since you're in high school, you should also check if your high school offers AP CS-A, AP CS-B, and AP CS-P. CS A and B will cover programming, AP CS P is more of a principles class, which will teach computational thinking. You should also look into getting into calculus as well as you can avoid a lot of classes in your college career that way and save some money.

Lastly, you should check if there are local universities because in the past 5 years there has been an explosion in CS outreach programs. You may be able to find interesting classes or summer camps in areas like game design, app development, and supercomputing that are made to be accessible to high school students.",1536285442.0
FattestPupper,"Ap Computer Science A? Because they have a specific subset that is really good to memorize. I used something called “edhesive” for my AP CS class. It’s not free, but the best I can recommend if you want to hold on to the knowledge for a long time and have good practice in everything. ",1536285523.0
Prof-,">>I plan to make a career out of this. I’m learning java and learned about casting and how to do arithmetic using doubles and ints.
  
You finding joy in casting and doing arithmetic with primitive data types made me smile. I'm glad you're enjoying it!   
  
As for some free resources, I've heard very good things about the University of Helsinki's object oriented programming with java course. It's free to do and I scanning the course it looks to cover the jist of an intro class: http://moocfi.github.io/courses/2013/programming-part-1/  ",1536287567.0
Tank_full_of_dank,"Thenewboston on youtube is really good, I’m also a high school student (Gr12) who started off with java in gr10. Don’t stop at just one program, defiantly learn multiple others so you have a vast knowledge of languages and are able to utilize them down the road. Do projects and stuff too, but other then that have fun man comp sci is great! ",1536285572.0
Erwin_the_Cat,"Yes! Tons of resources are available freely online. My advice would be to mainly think of things you want to make and try to make them. When you get stuck Google specific problems and you have a good chance of finding answers. 

Ultimately the best resource will be finding an open source project you are interested in and trying to contribute. This is likely to be *incredibly* hard but it will give you experience working with a large code base which ultimately in my experience is a talent employers value more highly than even a degree. 

When the time comes getting a degree is also a great move. But it will focus on lower level and more theoretical concepts than you will generally encounter in the workplace. It's super interestingl and useful knowledge! But again. Working with open source is the best way to get a feel for what life will really be like working as a programmer professionally.",1536287633.0
___Ultra___,Woah I’m in 11th grade and it’s my second year of compsci,1536288948.0
Vawqer,"My AP Computer Science course frequently used this: https://practiceit.cs.washington.edu/

I found it quite helpful. You may also want to see if /r/learnjava or /r/learnprogramming have any resources for you.",1536288758.0
offByOone,Udacity has some pretty good courses you can take free so long as you're ok with not receiving a credit for them. There's also plenty of video tutorials and lectures on YouTube. ,1536289389.0
Paparabbit,I finished AP CSA my sophomore year last year so lmk if u need some help lol. Pm me and I’ll do the best I can ,1536291518.0
PhoneyBone,Derek Banas on YouTube has a very good java tutorial series from basic concepts to some more advanced stuff.  The videos are usually relatively short and he has many different tutorial series you can try when you want to step out of Java. ,1536291978.0
jdavey35,W3 schools is nice,1536285294.0
StealthWingX,"First and foremost, take a deep breath. As a former comp sci major, working for a large software company, I can say that this is the first of many times in this career path where you will be challenged and pushed beyond your comfort zone. It sounds like you have a good head start. Being successful in the tech industry and life in general means being able to identify a weakness and form a plan of attack on how to close the gap. Those that can successfully do that have a very bright future.

If you’re serious about this, I suggest you do some research and practice areas you are struggling with. I am sure your university has areas you can tap into and growing up in the internet age allows self teaching (YouTube) for free.

Keep your head up, commit and you will do great!",1536285794.0
inspired2apathy,"Don't worry!

There's a terrible pattern some places with assuming people have experience prior to intro CS.  However, you'll quickly catch up and any gap you have won't last past 101.

Feel free to ask questions on /r/learnprogramming or /r/learnComputerScience 

The terminal is a ""command-line interface"" where instead of using a mouse to click menus, you use the keyboard to type instructions.  There are several different ""shells"" but the one you're using is probably called ""bash"".  ",1536286012.0
eternusvia,"Don't give up. Buckle down. You can do it. Your current lack of knowledge in these areas just makes the class 10x more valuable to you. 

The most important thing is to take these areas of weakness one at a time. Don't say to yourself, ""I don't know X Y and Z"". Instead say, ""What is X? Where can I find examples about X? Can I write a short program demonstrating X?""

Of course, you need to have free time to devote to content exploration. If you're taking too many credits, drop a class. It's OK.

If you handle one thing at a time, day after day, you'll be flying. ",1536286424.0
mrcruton,"Lol shits the same for my introductory CS class (which was stressed that no experience was necessary to take this class). 

First lab day the prof runs through the whole assignment that will be due at midnight. 

First she boots up xubuntu, tells us to use the terminal to make a couple directories and subdirectories, all using bash commands the majority of the class have never used before (lucky for me I got some linux experience so I was in the clear). Then while still solely using bash commands makes a file, opens up nano editor, writes a simple C++ ""Hello World"" and then compiles it using g++ all while through the terminal. Fucker did this whole ""explanation""/example within 30 minutes. And then tells us to do it and screenshot it and submit it.

No one was able to do all that within the lab time (couple people couldnt even figure out how to login to the lab computer).

So she just tells us if we couldnt complete it in time to just remote login to the lab servers terminal from our own computers since she didnt yet know when the labs would be open. Like yeah lady, none of the class will be able to figure out how to remote login with only the host id.

Everyone was just fucking lost.",1536287060.0
chinmaygarg,"Having been a TA in cmpsc classes, I’d say just talk to one of the TA’s or professor and try to ask as many questions that you can to clear whatever you feel is stopping you from understanding. They’ll also be able to try to connect some similarities between what is different in Python vs Java and how you can understand it better without letting your knowledge of Java confuse you further than helping you. ",1536286630.0
shoshy566,"Coming from a female computer engineering upperclassman who's been there, you'll be fine. I would be willing to bet money that at least 1/3 of the class was just as lost as you but trying to look busy. Since intro CS courses are some of those courses where you have people with 10+ years of experience as well as people with literally no experience you often get this sort of mentality in the class where everyone is afraid to ask for help. My biggest piece of advice (which I really do wish I took to heart my first year) would be to just stop caring what other people might think or where your skill level is compared to others in the class and just ask for the help. You've got the upper hand on a lot of them since you know a bit of Java but that might come as a disadvantage at first to you since now you'll be learning Python. They are very different languages and if you're a beginner switching between the two of them can be very confusing.",1536289790.0
7itemsorFEWER,"Currently a senior in CS and I was in just about the same boat. I will definitely say I agree with the comment that says just take a few deep breaths. My friends that were in that class with no prior knowledge or experience were struggling in the same manner as you. What you have to realize is that the professor is probably throwing a lot at you as a sort of gatekeeping. They want to scare people, they don't want people in their major that can't handle it. And let me tell you right now not a day goes by that I don't think ""how did I get here, us this really right for me"", but that's because we picked a hard major. 

My suggestions: go over the syllabus and figure out what you will actually be needed to know: both the prerequisite knowledge as well as the course. Next, learn to self teach. I've had a lot of bad teachers, not just in the CS field but others as well and it's just a valuable skill to have. Give it a couple more weeks and if you still feel behind, delve into your assigned books. 

Regardless of what you do, just Google ""beginner Python projects"" and try to accomplish them on your own. I agree it's not the ideal thing to be doing on your first college weekends but if you want to get a leg up on the information and your professor seems to be no help, it's the best way. And many times you'll find it fun! I love figuring out algorithms and solutions, it's very gratifying and makes you much more confident. 

Next, many times your professor, recitation sessions, TAs, graduate students, and lastly OTHER STUDENTS IN YOUR CLASS are your BEST resources. They are willing and able to help and are a great source of knowledge. There is no substitution for experience, which many of them have. Make friends with your classmates. 

Just know that everything will come together as long as you get on top of it and don't let it get on top of you. Be proactive and everything will come to you and remember DON'T EVER BE AFRAID TO ASK ANY QUESTIONS, because I guarantee that somebody else has the same exact question. ",1536288143.0
Noxagen,"Make lists of terminology you dont understand completely. After class ask the prof, TA, or other students to explain those to you. Email them if you're uncomfortable talking about it in person but I've found that going through the unknown stuff face-to-face with some really helps.

Once you understand it then sit down and try to do it. Very important that you do it in that order.

A lot of the advice on here is really good. Don't over think it. Simplify every problem to its core and put all your energy into solving that first. Remember every challenge can be beaten by breaking it up into parts and solve them one at a time. You dont need the answer right away just somewhere to start.

Good luck! And remember you are not alone!
",1536289391.0
ReedOei,"Like everyone else said, you’ll be fine. Don’t worry, just start studying and you’ll get it soon enough.

Also, there’s another sub, r/learnprogramming, with lots of people who would love to help you out with questions you may have.",1536288385.0
Cocomorph,"Speaking as someone who came to computer science through mathematics and accordingly was in a not particularly dissimilar boat, you'll be fine. Find your TA. Go to their office hours (or the professor's). They can help you get oriented. Feeling overwhelmed with the peripheral technology is perfectly natural if you're unfamiliar with any of it to start.  
  
You aren't stupid. You made it successfully through a year of Java. Don't panic. It will get easier (and then it will get harder again, but you'll be prepared for it).  
  
Your problem is normal, expected, and fixable. The less anxious you feel, the easier it will be to fix, but it'll be fixed either way! :)",1536289566.0
forbin895,"Several years into a rewarding career in CS here. Your situation reminds me a ton of my first ever CS class.

I had always liked computers before college, but hadn't really ever done any programming. At my school, there were two ""intro CS"" courses, one for majors and one for non-majors. Since I planned to major in CS, I signed up for the majors class. The first day I was completely lost and confused. Everyone else seemed to follow along just fine, but I left convinced I would have no clue what was going on and enrolled in the non-majors course instead. Still struggled at first, especially with setting up the environment we were working in (the IDE, shell, etc you're mentioning), but stuck through it and once I'd gotten over the confusing noise of getting used to the tools and workflow, I could totally handle the problem solving. In retrospect, that was the same trouble I had with the majors class too: I was just overwhelmed by the new environment! I would have done fine if I'd stuck with it. 

As others have said (there's some great advice in this thread), it's totally normal in this field to go into situations where you're totally out of your depth, and at the beginning that's really stressful. But it does give you opportunity to grow and in time you might even find yourself enjoying being in that kind of situation: when you have confidence in your ability to figure things out, it can be quite fun!",1536289651.0
chadwickofwv,"You'll be fine.

Python has very different syntax, but it isn't that different overall.  I'm sure someone here will send you to a good source for a crash course.",1536290018.0
uncleXjemima,Don’t panic. ,1536290118.0
offByOone,"An IDE (integrated development enviroment) is to programming as Microsoft word is to writing. It has a bunch of tools that make it easier but not really necessary. I'm guessing you used some program to write and run your java code (Eclipse maybe?) if so then that was an IDE. I'm less certain on the definition of a shell but I just think of it as an interface for running programs (command prompt on windows or terminal on linux). Although it could also refer to the python interpreter.

Most importantly: google is your friend. If there's something you don't understand, google it and you likely find an answer. And don't get too discouraged about the shell and what not. That's just getting you setup to program and not what your courses are going to focus on for the most part.",1536290649.0
masterpirateking,"I think its totally fine to be scared or intimidated at first, also one thing I learned most of the time people are in same boat but afraid to ask. there might be many more people in the class who will not be having any idea what they are doing but just sitting there silently.

For anything you dont understand, google will be your friend just google it and see how to do it. I learned most of computer science by trial and error, the only thing you need to understand is just not to give up. After a while everything will start looking interesting and easy.

About dropping CS, I believe majoring in CS has the most rewards among all STEM courses and better job opportunities and better pay. Don't drop the major thinking you will not succeed, you have already proven yourself by learning Java and getting into CS majors in pretty big university, you will definitely succeed by putting some efforts.
",1536291252.0
tastygoods,"You can PM me if you need private help otherwise what are you exact questions? 

Shells are small wrapper commands and programs around whatever functions are inside the operating systems kernel that you typically type into. Think like how the steering wheel, pedals, and buttons inside your car operate the big complex motor and brakes and everything else inside the car.

Scripts are just collections of Shell commands much akin to a large recipe, that you can run, schedule, put to sleep, and give to your friend.

The basic hierarchy is:

Hardware > Kernel > Shell > Commands > Apps > Programming IDEs.

IDEs are integrated development environments and are usually mouse driven apps that basically have a bunch of the stuff shell scripts would be used for inside a nice clickable interface.",1536284861.0
_1000011,"IDE - A text editor with bells and whistles to help you understand the code

Terminals / shells - pretty similar to each other, they allow you to interact with the program through text

I recommend running ""python"" in your command line and just playing with IDLE, which is like command prompt for python",1536288302.0
Obamendes,"Recently, we've seen many papers applying Convolutional Neural Networks (CNN) to solve computer vision problems.

&#x200B;

Has anyone read something about limitations or difficulties that CNNs might have. I believed they are not perfect and may not be provide excellent results in every type of image, but I've never found any paper related to it",1536589655.0
Ragnarock14,Can someone direct me to a resource that involves learning space complexity and maybe run time complexity? ,1536618465.0
ringraham,"Can anyone point me in the direction of some resources where I could learn about applications of game theory in compsci? I’m an undergrad majoring in econ minoring in compsci. I’m taking the class right now, and I really enjoy it, but I asked my prof about it, but he’s an econ prof and didn’t really know of any resources. ",1536771945.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1536262127.0
sievo,"I did CS and philosophy as a double major.  That doesn't mean you do double the classes, you just have to meet the requirements for both majors during your (usually 4 year) degree.  At least that's how my school in Alberta Canada was.

The two go very well together, and though philosophy hasn't served me much professionally, and I haven't studied much since finishing my degree, it gives you a lot of skills and knowledge that you wouldn't get otherwise.  All CS grads should be getting more ethics IMO.

If it interests you, go for it!",1536264286.0
pythonicus,CS + philosophy sounds like a wonderful way to spend university,1536254750.0
Doberman123456,Oxford (and Edinburgh I think) offer a CS and Philosophy degree,1536243172.0
necheffa,"> I feel that Computer Science and Philosophy is a double major that I am really considering.

Don't forget, some schools let you minor in a subject. So if you are concerned about the course load, a minor might be a good way to lower the degree requirements but still enroll in higher level courses from each subject.",1536276901.0
pvc,"At least for non-profit 4 year liberal arts colleges in the US, mist should allow this in 4 years easy. At least I know mine does and others near me. 

Technology and philosophy should go together more, so that we might not be in the mess we are in now. Philosophy is also good at teaching writing, which is very important in technology careers. ",1536269943.0
noam_compsci,Strange you mention this as I was just on the Oxford compsci website and saw that they do[ CompSci and philo](http://www.cs.ox.ac.uk/admissions/undergraduate/courses/computer_science_and_philosophy.html). ,1536246667.0
Sammy1Am,"Double majoring doesn't usually equal twice as many classes since your general-ed classes will all the be same, but if the majors don't have a lot of overlap (Philosophy and CS for example) then you *will* have to take twice as many major-specific courses (think overall maybe 150% as many courses instead of 200%).  Definitely look into the requirements for maybe a philosophy *minor* instead of a major.  If available, it'll be significantly less classes but still gives you that taste of philosophy you're looking for.

From a personal-growth perspective I definitely think you should at least try to take some philosophy classes regardless.  My biggest disappointment from university was that basically all of my classes were computer-focused and I didn't have much opportunity to explore other fields.",1536266839.0
semicc,Formal logic and discrete math are fundamental in a theoretical computer science program. It is a good foundation if you intend to do research in a graduate program later on. It helps you to reason about other research and express yourself in publication. ,1536281357.0
LuisG_07,"I am doing a CS degree right now, if I start doing a phylosophy degree too I will have 10 hours of classes some days. Maybe is not a good idea.",1536284199.0
barsoap,"You should've put a trigger warning, there, (/s) seeing you just made me recall a ""discussion"" I had on some philosophy sub about ""mind as computing machine"". While I wouldn't ever want to rail against philosophy as a whole as there's very acute and thorough work being done there, there's also a staggering amount of arrogant know-it-alls more interested in smelling their on opinion farts devoid of any of the background knowledge necessary to form an informed opinion on the matter. Like, in this case, understanding what is meant by ""computation"", babbling about ""naive mechanist world-view"".

With that in mind: Give them hell.",1536316463.0
nsblackwidow,"I was initially enrolled in a double major program for computer science and philosophy, but I decided the course load was too much for me and opted for computer engineering instead (computer engineering is an interdisciplinary course between computer science and electrical engineering at my university).

I plan on minoring in philosophy if I can learn to balance school work and my internship, but if that’s not possible I can always go back to school someday. 

I say don’t worry too much about what major you’re going to choose until you’ve gotten into college and have taken some courses. It’s much different than any schooling I’ve had. You can always change too. Good luck!",1536254174.0
Oh_Petya,"It's definitely doable (in the US). I wanted to do this myself, but had decided on my CS major too late to properly plan for a double major.  


You won't take double the classes, you'll just spend more of your electives on major courses rather than just whatever you want to take/general education courses.",1536268701.0
wolfpack_charlie,"Generally, when people talk about philosophy and CS, they're referring to AI. If thats your end goal, I'd suggest you consider a math major or minor. Most algorithms used in Machine Learning tasks (and AI in general) require a background in various areas of math. I've noticed that my CS classmates who dont have a major/minor/strong interest in math struggle with a lot of the math-heavy concepts. 

Not saying to necessarily drop philosophy, but if you want to get into AI, I'd highly suggest taking some math classes. Statistics is also a great choice. ",1536280964.0
hleehowon,"If you can make it to Stanford, do Symsys.

https://symsys.stanford.edu/",1536282933.0
l_lecrup,"In the sidebar you will see a link to the textbook SICP. I think if you read that, you will notice a distinctly philosophical tone. Being a computer scientist, or even being a good programmer, is not about learning a set of tools and techniques and applying them in various ways. You will have to think deeply about complex problems, and you will have to present arguments and even proofs, if you really want to do something interesting. I think philosophy would be an excellent combo with cs.",1536305274.0
bigbadmangos,"My school, UIUC, just approved a dual degree of CS + philosophy! We're a top 5 CS school in the US, check it out. Currently doing a CS + Ling degree.",1536305701.0
Blue_Q,"I majored in CS and mathematics, but philosophy would have been my second best choice. I highly recommend it, it ties together in many foundational questions. There is a great article on it; https://plato.stanford.edu/entries/computer-science/",1536306920.0
Godzoozles,"At my university I minored in philosophy, and double majored in political science and computer science. I enjoyed it. The philosophy logic class was a joke since discrete math is an important part of computer science, and I would say that's where the strong overlap between the subjects ended. I imagine you won't find yourself thinking too much about computer science while reading David Hume, for example 😛.

That said, I enjoyed the holistic education I received, and I take pride in the fact that writing a bunch of papers and critically reading texts in my liberal arts studies sharpened my skills in writing and reading. But there are other combinations that are also very good and more tightly connected to CS, e.g. Math & Computer Science.",1536265939.0
danyamachine,"Do it! Also, you might want to post the same question in a philosophy subreddit to get a different perspective. 

University of Toronto is very highly ranked in both CS and Philosophy. ",1536273519.0
IAmALinux,Are you familiar with the free software foundation?,1536273659.0
MemesEngineer,That would be interesting. Make sure you focus in AI ;),1536274526.0
48151_62342,"There is absolutely no value or payoff in a philosophy degree. It's a huge passion and hobby of mine, too, but don't waste your money getting a degree in it; it will never pay off. Anyway, you can learn way more on your own online than you ever could from university. There is 0 reason to get a degree in it.

CS on the other hand is an extremely smart thing to get a degree in.",1536267512.0
Sjeiken,"Philosophy makes you retarded it prohibits you from thinking, even Socrates said this. 
You can always read philosophical articles in your spare time. Also there are no jobs to get from philosophy. what are you going to do,  contemplate the existence of this fucking for-loop? 

The main selling point of philosophy is that it ”trains your mind for critical thought and core logic”

If you're after the critical and logical thinking you get from philosophy, then you're better off picking a discrete mathematics book, at least then you can prove it.  Google ’George Boole’ if you're interested. 

Go pure CS and math if you want to do something interesting with your life.

Whoever downvoted this: 
Go fuck yourself, philosophical peasant. You know deep inside that your life is fucked beyond repair.",1536276469.0
Concorp,"McGill University has a strong computer science degree. Its located in Montreal, Canada. Its in the top english schools in Canada

https://www.cs.mcgill.ca/",1536260122.0
Domva,I would agree with you. I don't see the difference between boolean and logic examples.,1536231119.0
FUZxxl,"In classical logic, &not;(*a* &and; *b*) &harr; &not;*a* &or; &not;*b* holds.

In intuitionistic logic where the axiom of the excluded middle *a* &or; &not;*a* does not hold, only &not;*a* &or; &not;*b* &rarr; &not;(*a* &and; *b*) holds, &not;(*a* &and; *b*) &rarr; &not;*a* &or; &not;*b* does not.

So for logic, the answer is a clear “it depends.” For Boolean algebra it does hold though.  Perhaps the textbook meant algebra over **Z**/2^(*n*)**Z** where 2^(*k*)2^(*l*) = 0 if *k* + *l* &ge; *n*?",1536234517.0
Ibot02,"Sets (specifically, subsets of some set with intersection as AND, union as OR and complement as NOT) are an example of a Boolean Algebra. Thus, since there is no null factor law for Sets, it does not hold true in all Boolean Algebras.",1536258636.0
TheSeanis,"The short answer is yes, here's a nice [graphic](https://imgur.com/HnjF9KR)",1536277657.0
Abstracticon,It was my understanding that the algebra of sets under the operations intersect(and) and union(or) is an example of a Boolean algebra. So your reasoning from part b) could actually apply to part c) as well.,1536293394.0
raghar,For sets multiplication means Cartesian product. So *A x B = empty set* means *A is empty set* or *B is an empty set* which is true.,1536266177.0
arcangleous,"Yeah, the book is basically wrong here. There are two-value algebras which do have the behaviour described, but no one refers to them as a ""Boolean Algebra"" in practice because both computer logic and circuits are designed to obey the logic rules and it gets really confusing if two people use the same words to mean different things. Those algebras are generally referred to as fuzzy logic specifically to make them distinct from traditional Boolean logic.",1536280571.0
Seth_Jenkins,"Would this not hold up due to Complement Law in Boolean algebra? 

Since AND is our equivalent to multiplication in Boolean algebra as well, you can have the case where A.Ã = 0 (Sorry, I don't know how to do a proper bar on keyboard). Neither x nor y is 0 and yet we still get 0. I would assume this would be why it doesn't hold up.

I'm not an expert so I could be completely wrong, but I hope this helps.",1536232883.0
acroback,Not this again.,1536206225.0
Godzoozles,"I hate these fluff tech articles that are just complete gibberish. I'm a sincere fan of Rust as well, and learning it molded the way I think about coding, but if you want any idea of how, why, or what Rust actually does, you won't learn it from reading this junk. Why does crap like this constantly get written?

>1. Resource Management
>This is the most outstanding feature of Rust programming language. Instead of using the brute force algorithm like other new languages, Rust uses mutable borrowing algorithm. This algorithm ensures that the resources are managed through a special system known as Resource Acquisition Is Initialization. The language also uses stack allocation of values to manage its resources.

Like this shit means nothing, and tells you nothing.",1536266334.0
skeeto,"Here are reasons to hold off learning Rust for now. I'm sticking to the
practical implementation and usage issues without addressing anything
about the language itself:

* Overall the language and ecosystem is still very unstable.

This by itself is a deal-breaker for me personally. I can't build stable
software on an unstable foundation.

There were major breaking changes less than three years ago, and there
are minor breaking changes happening all the time. Don't expect older
code to compile with a recent Rust release without adjustment.

Applications and libraries written in Rust quickly become dependent on
new changes and features. If you depend on any libraries, you're going
to have to spend time keeping up with the latest releases, too.

For example, to compile Firefox right now, you need to [use a Rust
compiler no more than a few months
old](https://wiki.mozilla.org/Rust_Update_Policy_for_Firefox).

Contrast this with C or C++, as the article does. You don't need the
latest release of GCC to build the vast majority of C programs. A 15
year old version of GCC would probably work just fine. The C++ standard
has evolved faster, but most C++ programs would compile just fine with
at least a 7 or 8 year old compiler.

* There is currently only a single implementation of Rust.

To compile a Rust program you need to use *the* Rust compiler. There is
no language specification, and there won't be one any time soon since
the language is changing so quickly.

Contrast this with C and C++, where there are three major
implementations (GCC, Clang, VS) and dozens of minor ones. They (more or
less) implement common standards.

Since there's only a single implementation, there's [limited support
outside of the x86
family](https://forge.rust-lang.org/platform-support.html).

* Compiling Rust programs is *really* slow.

C++ can be pretty bad about this, too — especially with heavy template
use — but Rust really takes the cake, especially for larger programs.
All that required static analysis comes at a huge compile-time cost.

Compiling a C program is blazing fast, even with optimizations enabled.
An entire 100 KLOC C program compiles single-threaded in a few seconds.
",1536233837.0
cirosantilli,Give me the source code :-),1536215001.0
flexibeast,"In general, i feel it's better for one's code to enforce what you \*do\* want rather than trying to enumerate and avoid all the things you \*don't\* want. That is: whitelist valid inputs, rather than trying to create blacklists of invalid inputs.",1536133316.0
kcbh711,It'd be much easier to catch everything that's invalid rather than prepare for every little invalid string that's inputted. Using a net is more efficient than tweezers. ,1536441969.0
IdealImperialism,"""evidence is endless"" -- provides no evidence",1536129556.0
rick2g,I find your ideas intriguing and would like to subscribe to your newsletter. ,1536128446.0
,"Sure if you don’t believe our government.

Guess the moon landing was fake to. 😂",1536133362.0
flexibeast,"[Sometimes they are indeed called 'words'](https://en.wikipedia.org/wiki/String_(computer_science)#Formal_theory).

Note, however, that in computing, 'word' can also mean ""[the natural unit of data for a given hardware architecture](https://en.wikipedia.org/wiki/Word_(computer_architecture\\))"". i also seem to recall authors sometime using 'word' to mean ""a valid sequence of allowable symbols"", e.g. ""jyxz"" might be a valid *string* but not a valid *word*, whereas ""word"" might be both.  ",1536134836.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1536080769.0
DrKarlKennedy,"You don't need to know anything about computer science, but basic high school maths is assumed and any prior knowledge of programming will make your first year easier (and your later years if you take advantage of your momentum).",1536122085.0
fakefactfrank,"My university doesn’t require Computer Science majors to have any prior programming experience. It is a plus, but not a requirement.
I can’t be sure that it’ll be the same where you plan to attend, but I bet they wouldn’t have extensive experience in programming as a requirement.",1536080320.0
4sieke,"Honestly, as long as you meet the prerequisites for each class you will be fine. ",1536080721.0
Free_Math_Tutoring,"I'm not aware of any substantial connection between them. Neural Networks are firmly rooted in Linear Algebra, while Cellular Automata  usually follow much more basic transformations.

The states of neurons in a NNs are continous, while cells in CEs have discrete states. In Neural Networks, we only care about the output, not about the state of individual neurons, while the output of a CE _is_ the collection of its states.

Some CEs are technically Turing Complete and could, technically,  theoretically, implement neural networks in them, but that hardly counts as a relationship.",1536093722.0
okayIfUSaySo,"Originally published in Latin in *De dignitate et augmentis scientiarum*. This image is from the [1640 English translation](https://archive.org/stream/ofadvancementp00baco#page/266), entitled *Of the Advancement and Proficience of Learning*.

While Bacon recognized that a binary encoding could be useful for a wide variety of purposes, the specific purpose he had in mind was a [steganographic cipher](https://en.wikipedia.org/wiki/Steganography).",1536071859.0
Scentable,"Impreffive work, Francif!",1536090632.0
rozling,> Knowledge is power. France is bacon.,1536101905.0
dragonnyxx,So obviously I understand the value of a binary encoding in the modern world. But do we have any idea what value was perceived in this all the way back in 1624?,1536071758.0
bhez,That's really interesting. I see there is no representation of the letter I or numerals or spaces.,1536117123.0
dances_with_poodles,"His writing also uses dots to separate characters in this illustration. I wonder if he thought about the problem of misalignment? 

This is particularly problematic if you have bits as things likes torches on or off, or trumpets sound or no sound. E.g. a leading “off” will be ignored unless you have a specific “begin transmission” code - and of course timings are an issue too. 

Nonetheless really cool, just wondering how far he went. ",1536134682.0
Praxon1,"Why are the letters ”J” and ”U” missing? Or is it just me? 

Nevertheless, really cool. I instantly wanted to write a secret message in this binary alphabet (à la da Vinci, but without the mirrored text), but then I realized I have nothing interesting to secretly write down lol. ",1536149872.0
nah248,Wow that’s cool,1536095812.0
sweetsmellingrosie,Shakespeare's published original tomb marker (pre 1820) had an inscription (epitaph) that is reportedly coded using Bacon's bi-literarie alphabet. To gain the a/b form you must substitute capital letters and small case by b and a respectively. It yields yet another cryptic puzzle in the form of a block of characters that have rows and diagonals that all sum up to 53 (of masonic interest) when one applies the standard letter to number cipher with 24 characters.,1537232994.0
lissofossil,Why is Reddit so interesting ,1536083229.0
DevFolks,"You could possibly do something with the targeted advertisements that prey on inexperienced users, such as fake download buttons, fake flash updates, fake virus pop ups, etc... this could also tie into what you already mentioned with not having an anti virus",1536071778.0
Zulban,"The hardest part about documenting your projects is actually writing out the documentation. Do it in a open format like markdown. After that, choosing a technical solution to host or organise it is nothing compared.

You should take that first step before worrying about the last.

To answer your last question - markdown, code comments, and docopt in Python. I recently migrated from github to gitlab and it was painless.",1536016847.0
Spoogly,"I would suggest using markdown, as others have mentioned. Makes porting everything to a new platform super easy. If you want a software solution for displaying and searching it, I liked tiddlywiki for a local wiki solution. ",1536022689.0
Transfinity,"GitHub will let you put together a full-fledged wiki for your project - there's a tab that says ""Wiki"" at the top of your repo's page. If that doesn't cut it, there's dozens of services out there that will host a wiki for you - a quick Google for ""free wiki hosting"" reveals several.",1536016485.0
chipstastegood,Consider gitbook.com,1536040288.0
valdogg21,I've been using DokuWiki for the last few months and love it. Open source with available Docker images to get up and running quickly. Simple UI and syntax. Very easy to use Python library if you want to progrmatically upload pages. Tons of plugins (I recommend nspages and EditTable).,1536029914.0
garblesnarky,"I'd say the ""easy way"" is to create documentation as you go, regardless of whatever other system you might use to store or present it.",1536037478.0
LinuxCodeMonkey,"You can install mediawiki easily on a Pi or something else, then make it as thorough as you need, but I love just jotting down a few thoughts and return to them later. You can lock it down if you like, and backups are a quick Mariadb dump.",1536041431.0
trytrytrytrywow,"I'd suggest [Notion](https://www.notion.so/). I use it as a personal wiki tool for documenting software that I work on. It supports wikis, markdown and is pretty flexible with inline tables, kanban boards and even calendars! The UI is pretty good too. :)

Another thing I love about it is that it has an option to export your whole content as markdown (or what have you). I find this really important because I don't want to loose all my content in case I decide to move from Notion ever in the future.
",1536042793.0
hnerixh,https://tiddlywiki.com/,1536049935.0
DemonInAJar,"You can use gitlab. It provides free private repositories, it has wiki pages simiar to the Github ones but also provides a free domain for your project in which you can upload anything you want via your CI script. What I do is document my code with doxygen and then upload the html output to the project page.",1536069431.0
AuzSSF,Checkout mkdocs,1536069573.0
timmyotc,"This is the graduate level computer science subreddit, where we talk about **graduate level topics**, not copying and pasting from blogs you find https://yagisanatode.com/2018/02/23/how-do-i-change-the-size-and-position-of-the-main-window-in-tkinter-and-python-3/

You know that there are rules against self-promotion https://www.reddit.com/wiki/selfpromotion because someone mentioned it a few weeks ago to you and you said you ""Didn't know""

I don't think you understand what [""a lot of value""](https://www.reddit.com/r/Python/comments/9apsz6/get_started_with_cryptography_using_python_in_5/e4x727k/) even means, nor what an actual skillset looks like when considering programming. Please stop posting, as you're just creating garbage for moderators to clean up. If you want to produce content, you should learn more about the field so you actually have context for what might be valuable to certain people. 

",1535996342.0
lrem,"There's no taboo here. That's quite a regular industry, on both sides of the cat and mouse game. Covered by a bunch of academic courses and countless commercial books.

For a good read I believe both books by Michał Zalewski (lcamtuf@) are pretty nice (The Tangled Web probably more interesting to a newcomer, due to a more mainstream subject). Until very recently he was the chief security person at Google. Disclaimer: I haven't actually read them, just going by recommendations and other contact with the author.",1535969752.0
DrKarlKennedy,"Universities will gladly teach you how to find and exploit security vulnerabilities, for the purpose of teaching you how to discover and patch those security vulnerabilities in your own software.",1535971012.0
Howtoeatpineapples,It's an incredibly broad topic (that's why they teach it in college) I suggest you look at your course guide and see what assignments you'll be doing to get an idea of what you'll be learning. Then decide to get a book on one topic,1535969795.0
ntopower2,"Knowledge is neutral. Maliciousness comes from applying that knowledge to cause damage/harm.    
My advice is to start acquiring basic security knowledge by reading books (/r/infosec has a good list but you can broaden your search). Then you have to practise those skills in actual systems (i mean vms obviously ;) ) vulnhub, hackthebox, ctf competitions. Start reading write ups from other people, study their techniques, take notes. That path never ends, new exploits arise, procedures get automated but it all comes down to scanning, enumerating, planning attack vectors and trial-error. You may find a job while in the process but your target should be obtaining certificates like ocsp, cissp, etc.",1535986258.0
TheReddHaze,"As some others have said, start with the fundamentals. Make sure you understand the 7-layers of the OSI model, how to read traffic/network flow on a computer or from a network, etc. Without the fundamentals, you will get lost in this field when asked to troubleshoot an issue or write a rule to detect specific kinds of traffic.

If you want some suggested books, I would suggest grabbing a Security+ book and reading over it. Its a great overview imo, not to mention some jobs actually require the certification as a prerequisite into the field. I would also suggest looking at places like Udemy which provide online classes for subjects like cyber security. If you're interested in malware and traffic analysis, grab Wireshark, or other traffic analysis tool, and go to malware-traffic-analysis[.]net to grab some packet captures/read some blog posts about the subject. It was a great resource in understanding what to look for when I became interested in the field. (Note that malicious artifacts can exist within the packet captures - Be Careful).",1535990406.0
camoeron,"Start with the fundamentals, networking, and software engineering. Once you know how it works you'll know how to break it.",1535984508.0
,"It's hard, but no harder than any other major if you really are interested. Just expect a couple of all nighters beating your head against a metaphorical wall.

You're going to have to get used to logical thinking and being able to think as a computer would think: one step at a time. It requires brushing up on your mathmatical problem solving skills and being able to understand the principle behind the coding, not just seeing it as a ""do this to solve this problem"" kind of deal. It's like a puzzle where creativity meets pure logic. I still remember spending a good 5 hours into the night at the dining room desk, surrounded by a mountain of paper trying to decipher assembly, one instruction at a time. Either that or writing a console chat-application in C. Think that's the only time I've ever pulled two all nighters in a row (with a little nap over the day to not kill myself.)

Give it a go if you're interested: it's a wast, diverse field with endless possiblilities. Even the most difficult classes I took could be fought with a heaping spoon-fill of determination and work, and it was the most satisfying thing ever when it eventually clicked.  
",1535938782.0
JustAnotherNerdyGirl,"It definitely requires a certain type of thinking but in general I’d say it’s no more difficult than any other STEM/engineering field. Its really more about your personality than pure difficulty. I know many people who were certainly “smart enough” for computer science but decided after interning or taking some higher level classes that they just didn’t like sitting at a desk and coding for hours on end. So if computer science is something you are truly interested in then I’d say go for it and don’t let the difficulty dissuade you, but if what attracts you is solely the money than you probably will not like it.",1535937027.0
Ra_Ra_Rushing_B,"It really depends on the person in my experience. Some people have an easier time at it than others but if it's want you want to a major in it then clearly you like it enough to invest the time into learning it. It can be hard but the cool thing about computer science is everything has a reason for being the way it is so you can always reason about why this were designed the way they are. In this sense it can be easier than other STEM majors where a lot of the times things aren't known for certain. Then sometimes it can be more difficult as well because when programming there are so many layers of abstraction sometimes it's easy to get lost.

&#x200B;

Ultimately if you're willing to put in the time there's no reason a hard working student couldn't succeed. ",1535938461.0
zielony,"Computer science isn’t that hard, what’s hard is putting in the programming time necessary outside of class to become a good developer. A CS degree is just part of what you need to get a job.",1535938698.0
genpfault,"> MODOS

MOPROBLEMS",1535946556.0
irabonus,"I think it very much depends on the other parts of your application. The GPA itself doesn't matter that much if you can stand out in other ways.

I had a terrible undergrad GPA and I'm starting my PhD at a very competitive university now. I think that's mostly because I was able to explain why I'm a good fit for the program.",1535936197.0
jessi-t-s,"I’m in Canada where our GPAs go up to 4.0, so I’m not sure exactly how to compare yours haha (where are you from?). But either way, grades are important but having a good relationship with a potential supervising professor is more important. Get in touch with professors who are doing research you’re interested in and talk to them about what you want to study. If you can, try to get a research assistant position in a lab you’re interested in (again, talk to profs to make this happen). If you’re in a lab with multiple profs it will be easier to make connections with more of them.

The cliché, “It’s not about what you know, it’s about who you know” is definitely applicable here. (Of course... you do still need to know stuff :P)",1535924847.0
MTheProphet,"....country?
I mean, it will depend on the program you want to join and the area you're in.... also if its a 0 to 10 system, how come a 8.4 be a bad grade?",1535990657.0
my5cent,"8.4.. dam, you need a 3.0... but schools are willing to work with you. Just have to ask.",1535926185.0
rosulek,"It would be awkward to prove this via the pumping lemma for CFLs (you are right that the complement is easier, unfortunately CFLs are not closed under complement).

The simplest way to show it would appeal to [Parikh's theorem](https://en.wikipedia.org/wiki/Parikh%27s_theorem). From this you can conclude that every CFL over a *unary* alphabet is actually regular. So it follows from the closure of regular languages under complement and the fact that {a^n | n prime} is not regular.",1535918461.0
SteeleDynamics,I love this subreddit.,1535929442.0
CookieLust,Doing stuff & things with computers.,1535903172.0
dasdull,"To study up for CP, I can recommend reading the relevant bits of CLRS (you can skip the proofs), and also the book Competitve Programming 3 by Halim & Halim.

Give it a try, it's great fun!",1535907809.0
hextree,"> Again, don’t use Python, Ruby, etc, for CP. These are very high-level languages that won’t give you as much control over your code as is needed. And they are painfully slow.

I always use Python, and haven't had any problems at all. The ease and speed of writing Python code is great for speed-coding. I wouldn't dream of trying to do string manipulations or custom sorting on-the-fly in C++ or Java. And occasionally there are problems involving large integers, which are far better handled in Python. The only problem I've hit is lack of balanced binary search tree in Python, so I have a custom class on Github I use whenever that comes up.

The run speed is not an issue, they generally set a higher Judge timeout for Python than for others, and if you get timeouts it's because your algorithm complexity was too high, not because of Python.

Personally I mainly do Hackerrank competitions. I tried Codechef but found a few of their problem sets to be buggy. Maybe I'll give it another try. Never tried Codeforces so I'll try that too. Used to do TopCoder many years ago, but haven't heard anyone mention it in the last few years, and their website looks like garbage now, so it's dead to me.",1535928190.0
sunjaun2,"Thanks for the post, will check out Codechef and Codeforces",1535883558.0
,Should really recommend something like golang/rust.,1535869451.0
NMDA,This is spam. Reported,1535861342.0
MithradatesMegas,"When they learned how much experience I had with servers, Burger King was happy to bring me in as a shift manager",1535974462.0
,"I'm a vulnerability researcher, I specialise in applying mathematical theory/logic to the field. For example can AI find vulnerabilities and exploit them is a question we'd like to answer.

SAT/SMT solving, stochastic methods and neural networking play a large part. It's all applied though. ",1535836145.0
reality_boy,I have a rather funky CSE degree (computer science and engineering dual major). After 20 years of programming in many industries I can tell you that my cs side has never been used but the ce side is my bread and butter. The only good part of cs was understanding big o. Unless your heading towards a PhD cs has little value,1535990512.0
hjqusai,"This is a great post.  I especially liked ""none of us is as smart as all of us"".

I transitioned from programmer to manager a year and a half ago, and I've been really back-and-forth between loving it and hating it.  On one hand, I miss sitting down, programming for hours, then looking back and feeling really accomplished.  The reward is so much more concrete - you actually built something cool!  On the other hand, management is so much more challenging and requires such a different set of skills, which is a fun challenge in itself.  The reward isn't as concrete - mostly you look back on a day and think ""what did I even do today?"" - but if you do a good job you can really feel the appreciation and respect from others.  

The part about it being way more stressful is so true, in a way that I never would have understood until I got here.  Being responsible for a team of people means being responsible for their happiness.  Making a mistake doesn't mean you have to spend a few extra hours debugging, it means that someone is unhappy/miserable as a direct result of your poor leadership.  That's a huge burden, and it really stresses me out, especially in the beginning when I was learning how to deal with different personality types.  

For those of you that have good managers that you appreciate, please tell them that, and tell them often.  It's pretty much one of the only ways to give them that feeling of accomplishment that, if they're like me, they desperately miss.",1535837478.0
ibsulon,"For those making the transition, “The Manager’s Path” by Camille Fournier is an excellent expanded book on this topic. ",1535834058.0
uh_no_,sounds like a demotion to me...,1535831009.0
Demon_Axe87,Looks like an old vent duct,1535832014.0
gct,"It depends on the operating system.  I'm familiar with Linux so I'll speak to that.  In Linux, the kernel only has one notion of an executable ""thing"", which it calls a task.  Processes and threads are both represented to the kernel as tasks.  The user-space library simply provides an API for accessing and modifying these tasks.  Tasks have their own associated address space they execute in, but the meta-data about the task lives in kernel memory and is managed by the kernel.  

Going further, certain languages that run their own VM or scheduler like Go can have their own threads, generally referred to as green threads.  These live and are scheduled entirely by the user-space program.  If we want them to actually execute on multiple processes, then we have to spawn one or more _real_ (kernel) threads to execute them.  This is where the question 'how many green threads should we run per kernel thread"" comes up.  After you decide that, then green threads will run in one of your kernel threads, no copying needed.",1535816609.0
reality_boy,In my mind I think of them as heavy vs light threads. Kernel level thread switches yield time to the scheduler and can be expensive if your executing lots of switches.,1535990280.0
balefrost,"Probably not the best sub for this kind of question. /r/AskProgramming or /r/learnprogramming or maybe even /r/cscareerquestions would be better. This sub is for (and I quote the sidebar here) ""high quality posts focused directly on graduate level CS material. We discourage most posts about introductory material, how to study CS, or about careers.""

Unless you get a career in some industry that itself uses a lot of calculus, or get a job doing some branch of CS that involves calculus, then you probably won't use your calculus skills day-to-day. But there is value is learning calc in that it forces you to develop good problem solving skills. The math that's more directly applicable to CS is discrete math.

> I often have worries that school my ruin my passion, because it's definitely happened before with previous interests.

If you think that you might lose your passion due to poor instructors or slow classes, then you don't really have any passion - you have something more like infatuation. If you're really passionate about CS, you'll figure out how to enjoy things *despite* bad instructors and slow classes.

If you're worried that school might ruin your passion, then just wait until you've been out in industry for a few years. CS is not the same as programming, and if you love CS, you will be disappointed that a lot of professional programmers spend a lot of time doing non-CS things. Even if you love programming, professional programmers spend a nontrivial amount of time doing things other than programming. ",1535811659.0
my5cent,I suggest self learning on the side. Free code camps along with some paid on the low end like udemy 10$ deals and pace yourself in a way so to not burn out. ,1535810206.0
sayubuntu,"1. You are going to have a lot of bad professor’s everyone does. Just accept it and work harder, you are getting a grade your professors already have their degrees.

2. If your passionate about the field seek out research opportunities, CS research is a very fulfilling experience.

3. Start an ambitious side project and just go for it. Use all of the tools you can. You like python? Start a game in pygame, unit test the shit out of it with pytest, document the hell out of it following a PEP or googles python style guide. Meticulously manage it in a GitHub repo using issues, referencing issues in commits, making pull requests, etc.
 ",1535810338.0
IamNotAHobbit,"From my experience the 1st semester or so is to just weed out everyone who can't do it. It did start a little slow for me too, but I was completely new to programming. It did pick up kinda fast in my 2nd semester however, a little too fast I might say. Of course your program may be different than mine.  Just stick with it and if it is too slow you can always try getting further ahead on some other skills. Good luck! ",1535810349.0
Lemmings4Friends,"senior in CS degree here, at least at my school the CS curriculum is drivel compared to anything else you will ever do in an internship, community project, etc. Explore your interests in CS outside of school, get the piece of paper, and move past it.",1535813759.0
Saikyun,"Imo the best part of getting an education is to get to know people. If you feel that you learn better online, focus your study sessions on that material, and use some of your time at school to get to know others with the same interests. It's a lot more fun to do programming or complain about teachers together with friends. :)",1535814150.0
alecthomas,"A shadow data structure is a representation of the ""main"" data store, used for some other purpose. In this case it sounds like they maintain an in-memory copy of their on-disk store, to mitigate disk latency and additional locking.

The most famous example of a shadow data structure is the ""shadow DOM"" (search for that term).",1535793274.0
fnordstar,Not even putting Paxos in the title of your post...?,1535792647.0
GayMakeAndModel,"Developers have a difficult time naming things, but when we do name things, we use ‘cool’ terminology such as shadow copy.  We also try to use analogies to describe something completely made up in order to ground code in something that exists in the real world.",1535811095.0
Workaphobia,I don't think the L-system was clearly explained at all.,1535819710.0
nightwood,Nothing new.,1535869536.0
non-deterministic,"Yeah, 'cause if there's one data structure I needed help visualizing, it's a linked list...",1536333403.0
Anotheravailablecant,"Depends on context. A personal one are often forever put on hold. As most people either lack the skill to complete them or the motivation to do so. In a business setting lack of communication is a major reason. If workers don't express what they need to do, how to do it, and reasonable expectations of completion the codebase could end up as a very large, unrefactorable, unscalable  mess that no one on the team understands how its structured. Eventually it just becomes easier and cheaper to scrap.
Unrealistic expectations are a big killer. Particularly in a smaller sized company that can over estimate what they can accomplished. The scale of the project makes it much harder to plan, and typically harder and longer shifts for workers. Which over a longer span of a project of that scale tends to lower morale, slowing the project, and in the worst case cause workers to quit. The project gets delay after delay until something gives.",1535781523.0
ISNT_A_NOVELTY,I think you're looking for /r/programming,1535783624.0
jokoon,"Trying to do too many things at the beginning, not having a working version with a simple use case.

Premature optimization and premature anticipation of dealing with specific situations, by constantly trying to theorize solutions to new problems that are not important yet, which slows you down.

It's also important to accept the fact that you must throw away code, like a body ejects old cells. Code is cheap, what matters is the time spent reading code to understand what to keep and what to remove.

Making code reusable is also about using the concept of input output, which will untangle your code. Data oriented programming is the best way to write good code, to me. Avoid abstraction.",1535786086.0
opus-thirteen,"Really, Expectation management and Time management.  That's really what it comes down to.",1535787492.0
GNULinuxProgrammer,"Top 10 looks good to me, almost exactly how I'd order them. Anything non top 10 will be a little more subjective since the density of ""giants"" will be close between unis. You can check their computations [here](https://github.com/emeryberger/CSrankings). Also, in tables like this 1 or 2 wouldn't really matter so being #18 (Northeastern) and #19 (UCLA) is not really a difference we can verbally quantify, their exact methodology will determine it. Also, I don't think Northeastern is ""relatively unknown"" it's a pretty internationally famous university in the heart of Boston. Sure, it's not as famous as MIT or Berkeley, but their research output is good.",1535750227.0
Screye,"IMO, CSrankings is a lot more reliable than USnews. Especially if research outcome is of interest to you.

The way rankings are computed is fully transparent, so you know the exact criterion on which universities were evaluated. (ie. publications and citations at tier 1 journals/ conferences)        
You can also filter based on recency, specific research areas and conferences. The code and data is openly available as well.

No ranking system is ever going to be perfect, but csrankings delivers on exactly what it promises. An ordered list of universities based on their research outcomes.

The reason for NEU's high rank is clearly visible in the ranking itself. It has a massive number of CS faculty. Harvard has close to the same research output with half the faculty. CSrankings doesn't say anything about the quality of teaching, student to teacher ratio, or funding given to grad students. ",1535752888.0
east_lisp_junk,"Looking over areas/labs/people I'm familiar with, it seems to be missing a lot of data.",1535761683.0
jmite,"Northeastern is a bit of a hidden powerhouse for CS. In particular, many of the world's best Programming Languages researchers are there. Matthias Felleisen is the creator of Racket, was huge in the development of Scheme, and is an author of How To Design Programs. Amal Ahmed is another there, she's a very accomplished PL researcher, and is giving a keynote at Strange Loop this year.

I'm sure there are other big names there who I'm not familiar with, particularly in other CS areas.",1535912465.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1535737114.0
devsdb,"I'm a newbie myself with some experience in the industry. 

What I've learned recently is to delay coding as much as possible. 

- think about the problem
- ask for clarification and try to get the problem as clear as possible because without it coding is impossible
- come up with a naive solution
- try to find edge cases that will break that solution
- try to optimize the solution using any data structure or algorithm that might fit the usecase
- when you're confident you've understood the problem and can solve it, then start coding.

At least that is what I currently try to do. Suppress the instinct to write code as much as possible and think about the problem at hand.",1535704553.0
BubblegumTitanium,Start writing down problems on paper. Even if it’s easy and you can obviously do it in your head. It’s just a good habit don’t worry what others think. ,1535706212.0
jkuhl_prog,"Here's what I do:

1. What tools does the language have that can solve, or at least help solve, the problem?  
Grab that tool, see how it works, play with it if need be, then implement it.  It might be something as simple as a native method in your language (like say, JavaScript's \`Array.prototype.sort()\` method) or something as complex as a full framework (I need to turn objects into tables on a database, maybe Java's Hibernate?)  

2. Reduce the problem into atomized steps.  How do I step by step approach the problem with each step being one instruction of code?  
This is probably the hardest part, but start thinking procedurally into smaller and smaller steps.  In order to do the thing,  I need to do X, then Y, then Z.  In order to do X, I need to do this, this, and this.  And so on and so forth until you can't go any smaller.  

3. Describe the problem to yourself.    
Write it out, write it out as if you're going to post on Stack Overflow for help.  You don't know how many times I've answered my own questions by trying to write a forum post at Stack Overflow or the Vue.js forums or anywhere else.  

4. Walk away  
This does not mean give up.  Sometimes you just need to clear your head.  Walk away and come back.  You'll possibly see it at a different angle and it'll click  

5. Play with the language  
The more you know the language, the better you'll understand what tools are available for addressing the problem and the less you'll get tripped up on syntax errors.  Sometimes what looks like a logic error is actually just a misspelling, which sucks because you start looking in all the wrong places for a solution.  You can mitigate this by better understanding the syntax.

&#x200B;",1535716788.0
umop_aplsdn,"1) Practice.

2) Imagine you were given this task and you didn't have the aid of a computer. How would you do it? Try solving the problem by hand for larger and larger inputs - then an algorithm should come eventually. (e.g. you can find the maximum of a 5 element array in a glance, but you can't do the same for 1000 element easily.)

3) I don't think you should delay coding as much as possible. If you think you might have something -- even if you're not sure if it's completely correct -- at least try coding it to see what happens. Then try to break it. This way you get much more practice than if you thought hard beforehand. It also better reflects how software is often written in the real world: [make it work, make it right, make it fast](http://wiki.c2.com/?MakeItWorkMakeItRightMakeItFast). For more complex problems, your first try is generally wrong or incorrect, sometimes in subtle ways, so it's good to get in the habit of trying to break your code.

4) Eventually you'll start to recognize common patterns in a problem and know instinctively how to break the problem up into smaller problems and solve each one. But this comes only with a lot of practice. ",1535707436.0
alex-o-mat0r,"If possible, I take some instances of the problem and try solve them manually. At this point, I don't have any algorithms in mind, I just try to solve each instance anyway I can. Usually I'll notice some regularities, which I then can make use of to find a generalized solution.

&#x200B;

If the problem or its instances are too big for me to solve by hand in a reasonable time, I try to split it into smaller parts and stages, where the result of each stage is the input of the next one. I usually start at the end and try to figure out the least amount of information necessary to get there. Then I repeat that with this least amount of information until the start. If these stages are still to big, I repeat this process. If a stage is small enough, then I'll try to solve it manually as described earlier.

&#x200B;

While studying, problems usually were small so solving them manually was the most common task for me. When I started work, splitting bigger problems into smaller ones became the most common task instead. The smaller stages I now end up with are comparably easy, though, so the generalized solutions are usually already obvious.",1535709494.0
Blue_Q,"I usually start to make schematics with pen and paper to visualize the problem. I often think about how I can transform the problem into simpler problems: for example if a differential equation is given, try to transform it into something solvable, compare to similar equations with known solutions or try to find some algebraic identities. More CS specific, for any given problem I always keep in mind the common datastructures (e.g.: Trees, Stacks, Linked Lists,...) and algorithms (search algorithms, sorting algorithms, graph algorithm) used in CS and whether and how I could apply any of them to a given problem to solve it most efficiently. Generally, I try to find any exploitable pattern in a problem to build a most efficient solution. ",1535710056.0
oprimo,">How do you break a problem into more manageable pieces? 

1. Break problem into a piece small enough to do *one single thing*, e.g., calculate one value, get one input, do one sweep on a data set, etc.

2. Can you implement said piece? 

If yes, go ahead. 

If no, take said piece and use step 1 on it.

&#x200B;

I often get stuck trying to find the *optimal* way to do something, instead of just doing it the ugly way, then refining it later. As Kent Beck says, ""make it work, make it right, make it fast"" - in this order.

&#x200B;

You can also try to think *backwards*, like, ""how would the solution to this problem look like?"". Then you break the solution and start building from there.

&#x200B;

Something that I find particularly helpful is to have a toolset of abstractions you can apply to most problems. Those are called design patterns. I learned those from the [Gang of Four](http://wiki.c2.com/?DesignPatternsBook). They changed my life.

&#x200B;

Finally, look at how *other people* solved a problem. As I was learning Javascript I was often on /r/dailyprogrammer looking at the solutions to their programming challenges. The solutions are small but often quite creative, and I learned a lot from there.",1535721752.0
blueish101,"This is usually more applicable to algorithmic problems (was part of my algorithms class), but I've found the approach scales to any problem:

http://blogs.ubc.ca/cpsc3202017winter2/files/2018/01/how-to-solve-320-problems.pdf

## Basic summary of the steps:
1. create/solve trivial instances
2. Design the input of the function, and the desired output (usually a function stub). You can write down notes about constraints here!
3. Have you seen this problem before? If you've solved it before, just use that solution and you're done!
4. Try a brute force algorithm. It will be terrible, but that's okay! And if it's not terrible, you can use your brute force. Just writing a brute force solution usually will reveal some insights for the next steps.
5. Improve on the brute force, either by adding some constraints, simplifying the steps where you can, or using an insight to rewrite the approach.
6. Keep iterating till either you're happy, or at this point usually you can feel comfortable bringing it to a colleague/friend to help you find more insights.",1535729611.0
Vallvaka,"I've often found that understanding basic data structures is super useful in breaking down problems into manageable pieces. I believe it's the Unix philosophy to think of problems in terms of the data structures to use, instead of the algorithms.",1535738649.0
Slavadir,"Reduce edge cases.

Instead of designing a solution as a single piece with many edge cases, make each edge case a solution by itself. This usually means less code duplication and less conditionals in the code.

Also, some edge cases can be avoided completely. Sometimes, seemingly different problems can be handled by the same piece of code.

I usually spend a large amount of time on planning and writing POCs, where I mostly do just that. It really saves time on maintenance in the long run.",1535721011.0
MyPythonDontWantNone,"I have more of a math background than a CS background. Here is my system:

1. Look for problems that I have solved before. Sometimes it is just an application of things that I already know (math formulas, algorithms, or libraries).
2. I figure out my inputs and outputs on a piece of paper.
3. I figure out how to break the problem down. This often includes finding the pieces that are repeated. I then write out my function names and maybe a docstring (depending on the size of the project).
4. I repeat the previous steps with each function and solve it piece by piece.
5. I then try to break everything by using absurd inputs (0, 1, -1, very large/small numbers, strings, floats, ints, and nulls are good starting inputs).
6. When I get stuck, I either google the error message or the most specific version of the problem that I can find.",1535730300.0
petz87,"Problem solving skills cannot be described with simple steps. Its because programming is as much technical discipline as it is an art. And to master programming you need experience. Lots of experience.

Acquiring ecpertise in a field is like collecting pieces of a puzzle. Once you have enough pieces, then ypu begin to see the overall picture. 

So  my advice is to just practice. Read solutions by others, reimplement solutions by others, read books and think on difficult problems on your own. After some time, once you build up the intuition, it will be easier. Henri Poincare (the mathematician) is famous for having documented his problem solving process. His strongest weapon was his subconsciousness, which after a period of incubation would often guide him to the right solution. But guess what! To build this intuition he practiced. ",1535737970.0
PingasKhan,"If you hate penning things out, I highly recommend the [rubber ducky ](https://en.m.wikipedia.org/wiki/Rubber_duck_debugging) method. Put a pot of coffee on, get out of your chair and pace like a mad man while talking the problem out. All but one programmer at my office has adopted this method which makes it feel like we’re in a mad house, but hey, it works. If you’re actively engaging a problem with speech and motion I’m confident you will be more likely to overcome it. ",1535740623.0
hardwaregeek,"I like to break the problem into a conceptual model. For instance, if you're calculating all the partitions of a number into odd numbers, there's a bunch of different ways you can think about it. You can think about it as ones separated by even numbers of + signs (1 1 + 1 + 1) vs (1 1 1 1). You can think of the partitions as a strictly monotonic sequence with odd numbers. You can think of them as a trie that holds all the potential partitions.

Each of these conceptual models can work. What I like to do is work with one, try it for a bit, then step back if I'm not getting anywhere, and try a different one.

Another fun thing you can do is try to find a mathematical tool. Math is your friend! So much of CS is reinventing or reusing mathematical concepts. And since you're not in school anymore, you're allowed to Google as much math as you want. For instance, the number of odd partitions of a number is equivalent to the number of unique partitions. Would this help? Maybe. Maybe not.

Finally, just get something out there. Write some code, even if it only passes 5% of the test cases or even if it's super simplistic. Just write it and iterate on it. Figure out what's wrong, then fix that piece. Don't try for some grand vision or all encompassing solution. Often times the first solution won't be the most elegant. But as long as you're progressing, it'll help.",1535741710.0
khedoros,"> How do you think about problems to help you solve them better?

Start by looking at the shape of the data. The structure of the algorithm will match it. A list of items is most likely a loop, for example. Something tree-ish will probably use recursion.

> How do you break a problem into more manageable pieces?

Part is that you learn to see patterns in the work. It's useful to learn a bunch of the common design patterns for the technology that you're working with, because they'll generally be good matches for the kind of work that the particular tech stack is usually used for. Often, when you break a problem into sub-problems according to some pattern, then those sub-problems can also have some standard pattern applied to them as well.

> How do you figure things out?

If you don't know how to start, start with some trial and error. Throw things at the wall until something sticks. In the process, you may find a pattern that didn't initially occur to you, and the pattern might be useful in breaking down the problem.",1535746067.0
evil_burrito,"Isolate the variables. Try changing one thing at a time to triangulate the problem. If the needle moves at all when you change a single variable, have a think about what that might mean. Form a hypothesis based on the data you have (log files, tests, etc). Gather evidence that confirms or contradicts that hypothesis. Rinse and repeat.

The most important part of solve a problem is to enthusiastically resist the urge to make any assumptions about the problem. Over the years, my assumptions about what the problem is or isn't have caused me 80% or more of the trouble. Don't assume anything that isn't supported by data, and, even then, resist the urge.",1535752186.0
acroback,"Think of it as a system. 

What is a system? It is a abstract notion of a logical machine. Can be a function, can be a distributed node, can be a library and so on so forth.

Each system takes some input and sends out an output. It may or may not change it's state in response to the input (s).

Now, find out what a system is supposed to do?

Each system can be made up of multiple more systems. That is how I have been debugging and solving problems for years. Works out great. Sometimes it takes a lot of time, which is the only sad part.",1535757913.0
curt94,ALWAYS wrtie the comments before writing any code.,1535774107.0
CowboyFromSmell,"Sleep.

Being totally honest, I try a lot of the stuff in the other top level comments, but for the hard problems, it’s not until the next day that I have a good answer. [Rich Hicky’s talk Hammock Driven Development](https://m.youtube.com/watch?v=f84n5oFoZBc) talks a lot about this.

It’s a real thing. REM sleep, which happens mostly in morning sleep, helps us process those hard problems that we’ve been banging on. You can read more about it [in this book](https://www.amazon.com/dp/1501144324/ref=cm_sw_r_cp_api_YmHIBb27NKFGA).",1535775581.0
ReverseEngineered,"My process is similar to what others have said. I also find it helpful to use what I was taught in college as ""the engineering method"". Writing out these things in order helps clarify the problem:

 * What is the problem we are asked to solve? In engineering, this is usually a single, specific problem, with known quantities. In computer science, you would want to solve an abstract problem, but you can still follow these steps by using a few concrete examples to help formulate and test the abstract solution.
 * What's the ""goal""? What should the solution look like?
 * What are we given explicitly?
 * What else do we know about the problem? Assumptions, estimations, techniques - brainstorm related things that might end up being useful.
 * Come up with a set of steps to solve the problem using those tools. Basically, come up with the algorithm and describe it.
 * Work through the algorithm manually showing each step leading up to the final solution.
 * Show that the solution does in fact solve the problem, and in doing so accomplishes our goal.

That part in the middle about coming up with algorithm is often the hardest part, but the steps before it are meant to help prepare you to do it and the parts after it are to make sure you haven't missed anything.",1535736284.0
metaphorm,pseudocode on paper before real code in text editor ,1535719150.0
se7ensquared,"I'm learning about Test Driven Development and while it seems a bit tedious compared to what I've been used to, I think it helps me to break down the problems better ",1535722224.0
Alextherude_Senpai,"As a newbie: 

&#x200B;

Suffer.

&#x200B;

Google.

&#x200B;

Attempt to apply related google knowledge to issue.

&#x200B;

Introduce multiple bugs after fixing issue.

&#x200B;

Crtl+Z.

&#x200B;

Attempt method 2 of said related google knowledge.

&#x200B;

Bugs.

&#x200B;

Crtl+Z Crtl+Z Crtl+Z.

&#x200B;

Go insane with random hard-coding until success.

&#x200B;

Success.

&#x200B;

Question how that even worked.

&#x200B;

Suffer, Rinse, and Repeat.",1535774074.0
reality_boy,"I iterate over the problem many times. It is nearly impossible to get things right the first time so making passes at it, improving the logic each pass is important.

I also keep a few problems in play at once. That way I can bounce between them when I hit a wall. Often I find that time away from a problem yields the best ideas.",1535990719.0
BuxOrbiter,"I don’t know how to fully codify my thought process. There will be some parts where my brain just connects the dots, and I can’t explain how this happens. 

To solve a problem, I must determine the correct abstractions. This is some generalized technique I’ve see before (e.g an algorithm, data structure , mathematical tool, etc.). If I don’t know the abstraction because I’ve never seen it or studied it before, then inventing it could take me a long long time, and I might never solve the problem in my own.

When I am stuck, I have a few tactics. First, work through some examples on paper, and work out obvious edge cases. Next, try to figure out properties of your problem, or properties that all solution have. By properties I mean logical statements that are always true. Knowing the properties can sometimes give you enough insight to jump to the correct abstraction. (Since abstraction are often stated in terms of properties)

For example, I ran into a problem on Reddit a few weeks ago. It was a demo Google interview question: Given a n x n matrix of integers, determine the longest sequence of adjacent consecutive integers (cells are adjacent to their horizontal or vertical neighbors, not their diagonal neighbors).

So I thought about this, worked on some examples. I then started to think of properties

* A matrix can have multiple disjoint sequences
* If the cell does not have any neighbors with value plus or minus one, then that cell forms a sequence of length 1.
* Every sequence has an end: A largest value in the sequence. Therefore, the finding the longest sequence is equivalent to finding the cell with the longest sequence of consecutive decreasing neighbors.
* If I know that the longest consecutive decrease sequence leading up to a cell, and the cell had a neighbor plus one, then I can create an even longer sequence.
* The longest sequence leading up to a cell is one plus the maximum longest sequence leading up to any of its consecutive neighbors.

At this point, a couple abstractions became obvious to me.

(1) This is really a graph theory problem: I can represent the cells as a DAG with edges e(x, y) if x, y are neighbors and value(y) = value(x) + 1. I just need an algorithm to find the max path on a DAG, and I know a linear time algorithm exists (in terms of number of edges). 

(2) I can express the solution recursively.


    MaxLength(cell)
      v = 1
      for n in neighbor(cells)
        if value(n) = value(cell) - 1
          v = max(v, 1 + MaxLength(n))
      return v


I just need to run this solution on each cell in the matrix, and pick the largest result.


    MaxLengthSolution(matrix)
      v = -1
      for cell in matrix
        v = max(v, MaxLength(cell))
      return v


Now I know this has a linear solution (in number of cells), and notice that MaxLength(cell) may be recomputed. Ok the worst case this makes my solution quadratic. 

Fortunately I know about memoization: So all I have to do is pass in a data structure to lookup and save the result of MaxLength(cell). This is can be a hash table or an array with length equal to the number of cells in the matrix.
",1536050369.0
_1000011,"Weed or beer, helps me distract and think in a different context",1535745309.0
justinba1010,"I was just wondering, what are some of the lesser known open problems in computer science. I mean outside the P vs NP vs EXP. Maybe even beyond NP vs BQP. But what are some of the little problems in CS? Like the Collatz Conjecture, or subsequences in Pi?",1535679592.0
SCElectrons2025,I'm generally interested in people's thoughts on the merits of Skiena's Design Manual vs CLRS for algorithms.,1535681027.0
TyroneSlothrope,"So I've been doing some elementary work in machine learning for a few weeks. I won't say I *hate*   it, but I definitely don't see myself working in machine learning for   long time. I have genuine interest in artificial intelligence, and from   what I see in industry, machine learning is goto technique for many   (almost all?) AI applications. It's not useless. Nobody can deny the   fact that the things we have achieved in field of AI wouldn't be   possible without ML. But I do think that ML can not be the way to take   us all the way to strong AI. I genuinely think we need to move on from   ML approach (aside from where it is needed - data dependent apps) to   something more sophisticated, something data-independent. I don't have a   clear idea about what it could be (maybe symbolic computation?   something like that?), and this is just my gut telling me. I am not an   expert in ML or AI, but I would love to hear your thoughts and arguments   that refute or support mine.

​

TLDR: I think ML is way too data dependent to be our approach to achieve strong AI.",1535696661.0
tulip_bro,"There seems to be a dearth of operating systems research, literature, etc. from a _theoretical or mathematical_ perspective. I've always been interested in the math behind Computer Science ideas, e.g.:

Want to know the math behind compilers? See regular language theory, automata theory, proofs, etc. 

Want to know the math behind functional programming style? See lambda calculus, program language theory, etc.

Implementing Operating systems can presumably dwarf the code base size, or general complexity of the aforementioned examples, and most of the literature I've read so far is highly technical.",1536009106.0
LearnerPermit,"Most of those certs are good only if you want to work extremely low level desktop support, usually for a contractor or OEM. Replace broken parts on laptops and desktops. 

Those certs aren't good for even server room support. Personally I wouldn't get it. Unless you want to do that kind of part time work while in school. Never mention it after you graduate with a degree in CS or whatever engineering program you pick.",1535671711.0
SporadicallySmart,"Are you getting a degree in computer science? I've honestly never heard of any certifications that meant anything for a CS degree. I don't have knowledge in other fields, but as for software engineering, I would run the other way if a potential employer even asked me about this certification. It would just tell me they had no idea what they were talking about.",1535671656.0
,[deleted],1535675820.0
jourmungandr,"Read ""The Design of Everyday Things"" and ""The Humane Interface"".

&#x200B;",1535658158.0
ibcooley,"One Best practice tip: get your users hands on it early to ensure it is intuitive, and user friendly.

Because of the way people are used to interacting with devices and machines, there's a certain bias for ease in understanding how things should work.  Generally, the fewer clicks or menu navigations to do what you want, the better.  Context and guiding the user matters a lot.
",1535652081.0
GoAwayStupidAI,"(with a bit of jest)
It's all message passing in the end! Get a book on development with actor systems. 

No idea what book tho. I use Akka but a book on erlang might demonstrate the concepts easier. 

For a good intro with suitable depth into distributed system theory I highly recommend 

Programming Distributed Computing Systems: A Foundational Approach (The MIT Press)
 https://www.amazon.com/dp/0262018985/ref=cm_sw_r_cp_apa_XsvIBbN40QJR0",1535726820.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1535643810.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1535643152.0
shuerpiola,"This is standard practice in critical safety systems, such as insulin pumps or pacemakers.

Edit: The *entire system* is has to be axiomatized, down to its circuits. This is far too costly to do for ordinary applications.",1535596700.0
PetrosPapapa,"Programming by proof is a big part of my job/research. This is a whole field, so I'll try to keep it short, but please feel free to AMA.

# Formal Verification

Generally speaking, there are 2 types of formal verification: theorem proving (logic reasoning) and model checking.

* Theorem proving has to do with specifying your software/hardware in terms of mathematical logic. You can then prove things about it (Does it terminate? Does it actually calculate what you want it to calculate? Can you ever get a null exception? Can you ever get an uncaught exception? etc).
* Model checking typically has to do with a state-based representation of a system and you can verify quantifiable properties or properties about relative time (Do I always get this result if I give this input? Is it possible to get money out without ever entering your PIN? etc.)

Both of these are complex and time consuming. Although tools for both have vastly improved in the recent years, the complexity of the verification is very high, especially given how large and complex modern systems are.

# Program By Proof

The ""Program By Proof"" paradigm has to do with constructive type theory and the Curry-Howard correspondence where you mathematically define a type system and perform mathematical proofs of various properties which then automatically translate to executable code. Coq is the most developed system that actually focuses on this. The main issue with it is that the generated code is typically very inefficient.

Another approach is one where you implement algorithms in a functional way directly on the logic of a theorem prover and prove properties about it. The proof system can then translate your algorithms from mathematics to code. This can be quite effective, but as with everything else, it doesn't scale to large systems.

# Successes

In contrast, formal verification has been very successfully used in the development of modern programming languages, especially functional ones. Haskell and F# (among others) have very strong links to formal type theory and they start from there to develop new features.

Formal verification is also adopted to deal with safety-critical software. NASA, for example, uses it a lot and so do companies that develop UAVs and Air Traffic Controller software etc.

Both Intel and AMD have saved themselves millions by catching hardware errors in their processors early, in some cases just before sending mass produced faulty processors to the world.

# Verification vs. Testing

Formal verification is the ultimate way of ensuring the correctness of your algorithms. It gives you absolute, mathematical guarantees for anything that you are able to prove. However, to accomplish that you have to build a model of your system, which means some parts need to be abstracted from the real world. Dealing with unknown unknowns is always a problem.

Moreover, you also need to describe the properties you want to prove using logic. It is quite possible to create and prove a specification of a property, but that doesn't fully describe your system. As a silly example, a system may adhere to the property that it always terminates, but maybe that's because you forgot a ""return"" statement in the 2nd line and your algorithm doesn't really do anything!

This is the main reason why verification is not enough on its own and you need to complement it with testing. Testing on its own isn't enough either, because you can only test for specific inputs, whereas verification gives you properties for any possible input.",1535621440.0
jacksonmills,"I'm not sure they will replace unit tests. Maybe they will be a complement, but there are a lot of things in a practical application that aren't necessarily \*provable\* per se, they are just desired. Permutations of conditions in the application may not necessarily also have continuous, provable, or ""logical"" desired output. So in some sense, especially around very fluid and abstract business logic, it might be a hard case to see.

Also, I think a good set of unit tests kind of reads like a proof anyway. It's more like saying ""when I do this, I expect that"". Really good sets of tests read like a paragraph, such that you sort of understand what the object is claiming to do, and them passing is proof that it can at least do that.

I think testing tools can improve a lot though, and there's probably more that can be done from an automated side on that end. I'm just not sure it's proofs or proof-based languages.",1535582257.0
mazesc_,"Not sure which paradigm in terms of formal correctness you mean exactly, but yes there are absolutely applications where unit tests are not sufficient and serious effort is put into proving properties of the program. One example is https://project-everest.github.io/",1535582863.0
SrPeixinho,"I, perhaps coincidently, have just written a post on how formal proofs will be used to replace tests in a browser being developed for Ethereum, where bugs can real money losses. [Here it is](https://medium.com/@maiavictor/updates-on-ethereums-moon-project-535f8c0497ef).",1535581535.0
swimmer91,"This is a topic I'm particularly interested in, but still learning about.

To start, [proofs are programs and programs are proofs](https://en.wikipedia.org/wiki/Curry%E2%80%93Howard_correspondence).  When the type system of a language is sufficiently powerful (containing more types corresponding to useful mathematical objects), [we can create theorem provers for functions defined in the language](https://en.wikipedia.org/wiki/Coq).  We can then guide these theorem provers to useful lemmas.  Then we get code which is proven correct and unit tests are obsolete.

This is an active area of research.  Researchers are working to expand the capabilities of theorem provers as well as [to make these more powerful languages more approachable](https://www.idris-lang.org/).

Like I said, I'm still learning about this stuff.  If I've said anything misleading or incorrect, I would appreciate someone cluing me in!",1535582926.0
tulip_bro,"Commenting to elaborate further, but for now, I will say that if you accept the Curry-Howard-Correspondence, in that programs are equivalent to proofs, then all coders program by proofs, by default.

Additionally, it's hard to compare unit testing with formal verification. Unit testing is largely a black box testing technique with a regression suite being a side effect. At a high level, unit testing is saying ""I can prove an absence of this particular bug occurring for the program, but that's it.""

Whereas verification/ proving is more general, powerful, and expensive, paraphrasing that ""I can prove an absence of all bugs for this program, in all runs.""

​

**Edit 2:** IIRC, one of the biggest issues with formal verification is at what point do we stop to _prove_ a program formally? E.g., great, I've formally proved a program does _x_. But what about it's compiler? Do I need to formally verify the compiler? And then the operating system?",1535583099.0
FUZxxl,I used to prove the correctness of C programs with proof assistants using [Frama C](http://frama-c.com).  Oh boy was that hard.  The easiest things were just impossible to prove.,1535628848.0
remy_porter,"Unit tests are a low-key attempt at imitating proofs- they're ""proof by doing it a bunch of times with mildly varying conditions"". From a mathematical perspective, that's not anything like a proof.

But from a *practical* perspective, it's arguably better than a proof. Here's the thing: I don't actually care if my code is ""correct"" or not. What I care about is this: given the planned execution of my code, does it behave the way I expect it to behave?

It's a cost/benefit problem. While unit tests are *tedious*, they're also quick and easy. I can take a set of expected inputs, their matching outputs, and whip up a plausible unit test in less time than it takes to implement the code.

Writing unit-testable code is more complicated than simply writing code. Writing proveable code is at least as complicated as writing unit-testable code, and usually will be much *more* complicated.",1535593854.0
ccundcf,"Cardano use proofs in their work. This is a good video about their approach - https://www.youtube.com/watch?v=TZGVgNsJSnA . Oxford Uni comlab used to believe in the utility of formal proof in general industry but one of the main drivers of that (Prof. Hoare) reversed that stance a while ago. It is still a very mathematically driven comp. sci department however. https://www.cs.ox.ac.uk/people/tony.hoare/ . 
",1535584671.0
acroback,"I find that a lot of programmers have never heard of state machines and k maps to manage complexity.

Some decently complex system can be coded as a state machine or it's variant.  Helps a lot IMO.",1535598107.0
NerdAtTheTerminal,"Proofs cannot eliminate need of debugging or errors during implementation: They just verify the algorithm works properly for all possible test cases i.e inputs. 

Otherwise, you cannot test the algorithm for all possible test cases;

Say for example: I need to write a very simple program to compute sums of powers of a natural number (input, say `n`) from power 0 to nth power;
i.e 1+x+x²+....+xⁿ

Instead of computing the `n`th power of x in every iteration, we can multiply the sum up to previous iteration by `x` then adding one in each step, i.e:

sum = (sum*x) + 1

I don't elaborate on complete algorithm here -- Just took this example because it is fairly simple, and removes the computational overhead due to the most obvious method;

actually, this method is fairly simple and won't need a proof, still if you are in doubt, you can verify by mathematical induction..

However, it becomes significant for more complicated, real world algorithms to verify that the algorithm behaves the way expected for all input (Possibly non mathematical ones as well), and testing all cases is not practical. Mathematics provides an easy way for verification...",1535696849.0
agumonkey,"there are research fields about program synthesis, I think they do so.",1535599621.0
eigenman,"Sort of off topic but I rant.  Regarding unit tests.  Aside from test driven design concepts and more in practical terms, unit tests are certainly overdone in many cases and underdone in many cases but at the core, I like that when I build the code it runs some of it.  Such that you get a sort of runtime error check.  It does occasionally catch things that are not obvious on a build and occasionally it informs you of a better way to program something. On the other side having many useless tests mean you have to modify twice as much code for each function modification.   Thus doubling dev time for no extra gain.   If some tests are pure redundancy like prove this function takes some arguments of some type that's literally defined in the function definition then remove it.   Those tests are actually slowing you down for zero gain.",1535602437.0
experts_never_lie,"I do, sometimes, for particularly important or unusual sections, but many of the things we would prove are handled by the programming constructs we use.

For example, one big requirement in proofs is loop termination, but it's handled for us in a ""for (Element element : collection) { … }"" or ""collection.stream().forEach(…);"" syntax so we don't need to prove it.  Similarly, while a standard for loop (""for (int i=0; i<n; ++i) { … }"") does demand a proof, that's trivial by inspection (variant is n-i; termination condition is n-i≤0; progress is guaranteed by the increment, unless someone is mucking with i within the loop).  I do prove loop termination if I have some complicated variant, because otherwise it's too easy to miss a case that you don't see.  And if you don't see it you're less likely to cover it in unit tests…

Proving invariants isn't unusual, but those tend to be documented, and partially enforced, as codified assertions.

It comes down to ""sure, when needed"".  It also depends on the culture around you.
",1535610286.0
gabriel-et-al,"Quality [in software] is closely related to trustability. A software is high quality when the user highly trusts in it. 
Most users don't need to trust in their software to the point of making mathematical correctness a must, so it's understandable that this technique is not common in industry.

However there are users that do need to trust that much, such as users of [this bad guy](https://en.wikipedia.org/wiki/Medical_ventilator).",1535634283.0
Farsyte,"Formal proofs of the design of a code module are a fantastic way of improving your confidence that you have the correct solution. They are a very useful tool for your toolbox -- but like a really fantastic hammer, they do not replace the screwdriver; you have the very real risk that you have proven your algorithm solves almost but not quite exactly the problem you intended to solve.

Additionally, if your prover does not start from the source code you are maintaining, then you still have the question of whether or not your code as written actually implements the algorithm you presented to the prover. Fortunately, it does not take many simple test cases to gain confidence that your code conforms to the model.

And nothing raises confidence in a working program than actually seeing it work; and in a robust fault tolerant service, by observing it continue to work as your chaos monkey kills nodes. Proof is a great place to start, but if you stop there, you are leaving a huge value on the table.",1535641063.0
TopIdler,"I really liked this talk, it talks about using proofs for machine learning implementations.

​

[https://www.youtube.com/watch?v=-A1tVNTHUFw](https://www.youtube.com/watch?v=-A1tVNTHUFw)",1535649284.0
chaotic_david,"I got taught to do this, and use other methods to verify code, while attending university. But most work done in code is for prototyping, non-safety-critical areas, or projects without the funding and time to verify properly. Proofing code is for the really important, really well funded stuff. Medical devices, aeronautics and space exploration, and military applications.",1535660478.0
solinent,"After reading that Lockheed did this for the space shuttle software, as well as learing this practice formally in school (given a C-like language with simple semantics), I've personally always used preconditions & postconditions and briefly informally prove to myself that the postconditions will be met given the preconditions.

I don't do anything formally on paper, but I've found I can write extremely high quality code quite quickly almost without bugs in this manner, since there's so little time actually discovering bugs required after implementation. I take much longer in the writing code stage as I just construct simple proofs in my head, though if I get in the flow it can be almost as fast. Since the preconditions and postconditions can be explicitly included you can quickly find the source of the error if there is one still using testing or even automated testing depending on the language.

Unit tests help you find problems with your code that you already knew could exist, so hopefully you've designed against them. I usually just keep whatever testing examples I use when constructing the code as unit tests though by no means do I exclusively use tests to drive my development.

I can't find the exact spec I read some years ago.

Obviously this only works if you don't have rapidly changing requirements: I only do it for software which needs to essentially never fail or there could be expensive consequences.",1535679726.0
Blue_Q,"Most of the important aspects have been mentioned already, so just for completeness, there is an ongoing effort to transform any informal mathematical proof into a formal proof written in a Hilbert style axiomatic system; http://us.metamath.org/index.html
",1535732811.0
jonathancast,"> Unit tests seem appealing to me if you a doing a contract style project that you are aware of all the requirements beforehand, otherwise it's super tedious.

If you don't know all the requirements up front, write your program a bit at a time, and get the requirements for each bit before you write it.  Then show it to the customer and, if they don't like it, negotiate new requirements and then change your program to fit.  With practice, you can get adept at writing tests to the current requirements and adjusting them as the requirements change.

> Can making formal/informal proofs reduce the need for testing in software (which doesn't really seem to be taught)?

Informal proofs probably won't replace tests; too much potential for mistakes (most math proofs have small mistakes in them, and quite a few significant proofs have major holes; for math theorems that's usually ok (since the theorem is probably correct), but for programs (that probably aren't correct) small holes in the proof are the same as having no proof).  A unit test is a kind of formally-checkable (decidable, even) correctness statement, and as such it's much more useful than an informal proof.

Formal proofs are more valuable than tests, because they prove much more than a test can (infinite properties / exhaustive correctness vs. just a few special cases), but fwics they seem to be much harder to write than unit tests.  They're not used to reduce the burden on the programmer, but rather to increase the value of the final result.",1535585345.0
skulgnome,"I'd love to, but who has time for pedantry? Or a maths toolkit for C types?",1535590224.0
sobeita,"Disclaimer: not a pro and dropped out of college. Take with grains of salt to taste.

I'm getting there with template metaprogramming. If you create types with certain traits, you can create aggregates of these that have the same traits, you can compose them to combine traits, etc. Due to strong typing, using types without some traits in a context that requires them will cause a compilation error. If your guarantees are not met, there is no program, and conversely, if there is a program, its existence is proof of your guarantees.

Sadly this breaks down at least where your code meets third party. If I try to prove my program is leak free by using resources only in RAII types and their aggregates, I'm at least relying on the functions I call to clean up to actually do their jobs. I aim for guarantees with explicitly stated predicates so that those predicates are the first place to look if/when something misbehaves. That can make for a tremendous reduction of testing, especially for new permutations of proven components.

Your comment about changing requirements is certainly valid. I would say that an added requirement could be integrated through the same kind of composition, but new requirements could conflict with the old ones, potentially invalidating a lot of code that relied on the old guarantees. All I can really say is that it isn't always the right tool, and even when it is, it's essentially never the only tool you'll need. From what I've learned, it seems best suited for long-term development and maintenance, especially things like support libraries. YMMV.",1535594877.0
gruehunter,"One of the cardinal rules of professional programming is to keep things simple.  But sometimes you have to break that rule.  When those times come, declaring some invariants and showing how those invariants are met can give you confidence that you didn't screw it up well above and beyond what you can gain through testing.",1535596513.0
Vityou,"Unit tests also help with readability of code.  If an internal function isn't documented, unit tests can help clarify it's meaning.",1535586682.0
fiskfisk,"There's [an in-depth article at jaxenter](https://jaxenter.com/oracle-closes-fortress-language-down-for-good-104777.html):

> Steele went on further to say that the team were “now unlikely to learn more (in a research sense) from completing the implementation of Fortress for JVM” so its the end of the road for their part. But Fortress will still remain available as an open source language like it has since 2007’s olive branch to the community, just before a 1.0 version appeared for the JVM.

It also mentions that Scala and Clojure adopted many of the ideas central to Fortress.",1535477065.0
zem,i hope fortress's [operator predecence system](http://lambda-the-ultimate.org/node/2943#comment-43411) gets picked up by someone else; it's one of my favourite small pieces of language design,1535506802.0
zokier,"I thought Fortress ended up cancelled because Sun ""lost"" DARPA HPCS competition to Cray and IBM, who developed Chapel and X10 languages for it respectively.",1535484002.0
Bromskloss,I remember seeing mathematical expressions in Fortress being rendered as properly typeset mathematics. That's what got me enthusiastic about it. :-),1535485706.0
zck,"I feel like this article doesn't really explain what the two sides it's debating are. From the opening paragraph:

> It is quite surprising that even a significant percentage of experienced programmers still avoid using the command line interface (CLI or CUI). They instead prefer developing applications using the graphic user interface (GUI).

The first sentence talks about ""using the command line interface"". This, to me, reads like it's talking about using a shell to do things like list files, or commandline git. The second sentence talks about ""developing applications"", or doing the typing part of programming.

Later, the author continues to glide over the actual details he's using for his argument.

> Command line interface gives you better control than the GUI. Whether you are writing a code or just trying to instruct the computer to execute an action, you can be sure of the command lines. They are capable of performing any action.

It restates the argument a few times, never actually giving any real details. Similarly, details that counter the argument are ignored: 

> There is no doubt that navigating through different icons on your computer can be slow and tedious. Some icons may be small and you may spend a couple of seconds before you locate them. With CLI, you simply need to type a few commands you want and you will get the results instantly.

He's comparing ""finding an icon you don't know where it is"" to ""typing a command you've memorized"". That's not a reasonable comparison.

Finally, the article wavers between what it's actually advocating. Compare the suggestion to use the two in a balance here:

> In this article, we are going to look at reasons why programmers should work on CLI more than GUI.

with the all-or-nothing from two sentences before:

> [Ardent programmers] know the importance of using CLI in doing everything, even when there is an option of using GUI.

This article would be better if it argued for the first quote with detailed arguments, not arguments from authority aimed at convincing people to never open up a window.",1535470228.0
enchufadoo,"> CLI has won the hearts of many programmers because it is more secure than GUI

They are both interfaces, none it's more secure than the other just because the way they display information or receive input!

This article is nonsense.

",1535471408.0
athousandcounts,"When using a CLI you can send someone a script instead of saying ""click this button, then this one, then hover your mouse over there, and so on"".",1535461185.0
dala-horse,"So which one is the best?

The article gives really good reasons why you should know the command line. It should be an important tool in your toolbox. If you are aboiding the command line, probably you are missing its advantatgdes.

But, you need to choose the better tool for each task. I love to use an IDE for development and I always use the command line for remote server access.  With Git I mix a GUI with the command line, I love to visually see the branches and browse and compare commits, but I am quicker in the command line to add files and set up the environment. If I use an IDE I use its own GUI to work with Git because I get so much for free.

Know your tools. Know your shortcuts. Know the command line. That will make su effective.",1535462547.0
svick,"> Ardent programmers won’t argue over this issue. 

You must have a very different definition of ""ardent"" than I do.

And the rest of the article is mostly nonsense too.",1535474465.0
raghar,"Adding a new option to CLI - several minutes with testing.

Now, adding new flow to GUI and testing it...

Anther problem - think about calling one program from another, piping results and stuff... With GUI that would create a whole new levels of complexity to provide even half the features of CLI.",1535461979.0
fearsneakta,dumb,1535467306.0
viralmonk,I prefer using command lines ,1535466070.0
OptimusPessimum,"Text is the ultimate interface. And, when combined with the possibility of composing functions, say, via piping, achieves a level of flexibility that no GUI has.",1535466500.0
geo_prog,"Meh, I've tried both after having started with Emacs. I'm much faster in a GUI IDE than I am in CLI.",1535462335.0
shookees,"Indeed, enabling (to be a power user) tools give a nice productivity boost",1535465765.0
Skywise,"Right tool for the right job.

I prefer GUI for source control checkouts and merges within my IDE.  That’s somewhat a matter of taste but I can get a better read on the status trough the GUI than the CL.

For service level stuff (starting servers, batch actions, service configurations like net stack setup) the CL is far superior because the GII can’t keep up or gets in the way.  It also impresss your non-tech friends “you’re like some kinda computer guru” no - I just typed ls.",1535473743.0
ccundcf,he is right and HE is wrong.,1535476399.0
LifeOnDeathRow," Powershell, especially for Exchange, does plenty of things that can be done for things that have no GUI for. Running Core instead of GUI will conserve resources. Running scripts in Python for management tasks on a Cisco router/switch infrastructure is pretty handy.

&#x200B;

If you are going to create a directory structure, md one, two, three, four, five, six, seven   is much easier and faster than rt clk>new>folder> type name,  repeat",1535485676.0
NeoNagapJoe,"Because I can get my work done. I do inital basic text analysis, filtering, file manipulations, system control, remote system control, glueing simple things together - basically most things related to my work at the command line. For the rest I use neovim and ipython/notebooks - In case of e.g. Java also an IDE.

I really don't know how people are able to work efficiently with a gui on such things....",1535465521.0
wyldcraft,"In text you can fit a whole lot more data on screen at once. This is why ""graphical programming languages"" fail. ",1535461881.0
combinatorylogic,Why anyone besides GUI designers should work *on* a GUI?,1535467923.0
Perfect_Wave,Is this just affiliate links to Udemy courses?,1535484458.0
Crysis456,As long as the binary tree you use is self balancing it will have a worst case of O(n log n). It's actually a very nice sorting method because it will maintain a sorted list even as you add and remove items (although extracting the list is O(n) each time you do it).,1535465428.0
,"I am not sure treesort with a balanced tree is O(n^(2)); the only case for a treesort to get O(n^2) is if your input is already sorted and your tree is simple Binarysearch tree in which case you are just appending to a linked list.


As for treesort with a balanced tree, it should be Θ(nlogn) because in all cases, inserting remains a Θ(logn) operation. Repeating for ntimes gives you a Θ(nlogn).",1535467264.0
vinaych,"Thanks for the feedback, I guess this method will be fastest if the list to be sorted has maximum element much greater than the maximum integer size supported by the language being implemented in. Otherwise, counting sort would be the faster one.",1535473998.0
CorrSurfer,"When writing to a dynamic RAM, you either ""empty"" a capacitor or charge it. These two things can take a different duration of time.

Ultimately, it doesn't matter, though, as in reality, such circuits are clocked in a way that the clock cycle length is sufficient both for writing 0s and 1s. While you could speed up the clock cycle temporarily when writing uniformly only 1s or 0s, this is a rare event and would require complex circuitry.

Also, having the clock cycle long enough that the computation steps finish regardless of the concrete data words being transmitted or computed allows makes microprocessor design much much much simpler, so that's the abstraction that is normally used.",1535463560.0
fiedzia,"Depending on particular technology,there might be some difference, but you won't be able to ever observe it or benefit from it.
Access to memory is managed by clock, and this is designed to hide any possible differences.",1535463366.0
TomvdZ,"0s and 1s do not correspond directly to anything physical. They're just the way in which we theoretically describe (on a high level) the workings of a circuit. There are many ways in which a circuit can be physically implemented, and the way a 0 or 1 is represented varies. If in one place of your computer a 0 corresponds to a particular capacitor having a particular charge in it, in another part of your computer that same charge might actually correspond to a 1 - or a 0 or 1 might not correspond to a charge at all, but to current flowing in a particular direction in a circuit (e.g. SRAM).",1535455580.0
drvd,No.,1535718620.0
GuyOnTheInterweb,"Depends what medium we are talking about, disk medium like SSD have a [trim command](https://en.wikipedia.org/wiki/Trim_(computing)) to release space, which can be very fast compared to overwriting.

It is not actually zeroing out anything (unless you do Secure Erase), just removing the space from internal mapping so the controller can re-use the actual bits as it wishes.",1535491581.0
,[deleted],1535455157.0
baxter001,"> I now realize that since i was talking about acidity it was my fault for using the word Soda which could easily have meant any chemical compound which contains Sodium. I was using the term to refer to carbonated beverages or Soft Drinks. I have corrected the term to be more clear.

He thinks that's the flaw in his tortured rambling?",1535440075.0
cenderis,"There are surely other relevant lessons?

According to at least some studies, even experts (some of the time) can't tell that white wine coloured red is actually white wine; they're not that consistent in judging it (so in blind testing, they often don't notice when an identical wine is present more than once).

Maybe most relevantly, ordinary consumers (maybe experts too; I forget) tend to rate wine as better if they think it cost more.
",1535632981.0
cyber-clown,It's number 323.,1535438133.0
anaemicpuppy,"Not so much ""What You Need to Know Before Considering a PhD"" as ""I Don't Think You Should Get A PhD."" 

At least have the courage to present your opinions as that – opinions – instead of trying to masquerade it as concerned advice. The current presentation is incredibly dishonest.",1535451881.0
MinuteTradition,Can you not just do a PhD for pure enjoyment?,1535439199.0
noam_compsci,"Look man, this article is just total bull. 

The Jeremy Howard guy is shady AF imo - just look at his blatantly falsified wikipedia and linkedIn. (No way in hell he earned $200k a year at age 19 at McKinsey... only to leave McKinsey once he graduates from a very very low tier univeristy, to then join 'Australian Consulting Partners'. The plummet from McKinsey/$200k to Australian Consulting partners is pretty violent to say the least). In any case, just calling bullshit on one 'fact' that you mention in your article. ",1535448085.0
l_lecrup,"This article is absurdly one sided. It starts with a warning about selection bias, and then dives headlong into several anecdotes with glaringly obvious selection bias (""my colleagues without PhDs are better than my colleagues *with* PhDs!! Dum dum duuuummmm!"").

Of course there are loads of issues with doing a PhD. Of course you can learn loads of stuff without doing one! But if you have the opportunity to do a PhD, there are things that it can offer you that nothing else can, in my opinion and experience. I spent four years really understanding something at the deepest level I could, with nothing but my own higher internal existence driving me to understand things better. I understand that that is not everyone's experience of doing a PhD. But there was (almost) no profit to be made doing what I did, so it is precluded by the path advised in this article.",1535432691.0
RavlaAlvar,Reading these type of article really hurts for people like me that wants to do a Phd but struggle to find any opportunity.,1535439951.0
AaronKClark,"I'm in a terminal master's program, so I don't know what a real graduate student has to deal with, but check out [this post](https://www.reddit.com/r/GradSchool/comments/9avhu0/the_grad_student_grind/) today from /r/GradSchool",1535464157.0
Jaitez,"Teach what you can’t do, that’s the saying right?  

Anyways, unless you plan for a career in academics you’re only limiting yourself by doing more academics. Usually by the 4th year most students tend to realize their potential or strengths and can do without more studying, testing or writing etc.  

Sure, there are perks to more education just like there are perks to stopping after a 4 year degree. This old debate will never end, personally all the people I know with masters and PhD have expressed it was a waste of time. I’m sure there are people who will say otherwise.  People should just do what works for them.",1535434485.0
poizan42,Millions? You can fit billions on the size of a poststamp. You can surely fit trillions in there.,1535415556.0
USDAGradeAFuckMeat,"Millions? Try trillions if we're talking volume. It's insane, really.",1535415467.0
zsaleeba,"I hate to be that guy but these are valves, not transistors. They serve a somewhat similar function in circuits but valves operate on a completely different principle. The term transistor only refers to solid-state circuit elements, which these are not.",1535416011.0
hackingdreams,"Those are vacuum tubes, so they used a million times as much power (and cranked out much of it as heat), were flakier at switching, routinely burned out and needed to be replaced, and were way slower at switching.

Early commercial *silicon* transistors came in [cans, little smaller than a dime around and maybe a few dimes thick](http://semiconductormuseum.com/Transistors/LectureHall/JoeKnight/JoeKnight_EarlyPowerTransistorHistory_BTL-WesternElectric_Page14.htm), so much, much smaller than what you see here... a whole modern cellphone CPU would easily fit inside of one of those cans today - so more than a billion times more efficient at packaging them up too.",1535416166.0
tutuca_,"You linked the profile page, bro. Not the project",1535406793.0
timee_bot,"View in your timezone:  
[September 28, 2018 at 11:59pm PT][0]  
[0]: https://timee.io/20180929T0659?tl=Software%20Engineering%20Internship%20at%20Google%20Winter%202019


*****

[^^delete*](/message/compose?to=timee_bot&subject=delete+request&message=%21delete+eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJlNHhvbW1pIiwiYWN0IjoiZGVsZXRlIiwib3AiOiJhbGlzYWxlZW03NDAiLCJpYXQiOjE1MzU0MDAzODB9.nP9ghhPOboMGbhfUb__F0FlCTO35p6QtgYttNfb9BAg)
^^|
[^^reprocess*](/message/compose?to=timee_bot&subject=reprocess+request&message=%21reprocess+eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJlNHhvbW1pIiwiYWN0IjoicmVwcm9jZXNzIiwib3AiOiJhbGlzYWxlZW03NDAiLCJwbm0iOiJ0M185YXNhcWYiLCJpYXQiOjE1MzU0MDAzODB9.aMkqiFqIyyFoFzYpGGtn_iqDCRF_Cp-TBEeDcKKOxq8)
^^|
[^^ignore ^^me](/message/compose?to=timee_bot&subject=ignore+request&message=%21ignore+me)
^^|
[^^help](https://www.reddit.com/r/timee_bot/wiki/index)

^^*OP ^^only",1535400379.0
east_lisp_junk,"This ""opportunity"" [is a joke](https://i.imgur.com/Vzhaw7T.png)",1535402304.0
gabriel-et-al,"You can create a tool to apply sentiment analysis in comment sections of a given website and return some insights on what posts generate certain sentiments. For example, your tool can tell posts by journalist A tend to attract hateful comments, while posts by journalist B tend to attract happy comments.

It's basically using NLP to transform comments into data and then feeding a BI with it. You can actually make a lot of money with it, I guess many news portals would love to have this kind of information.

The point is to test many different combinations. Author of the post, datetime, title, color of images, length of text, layout etc.

Don't forget not to sell your tool, just ""rent"" it for some months, so the same portal may contract your service every once in a while and you get money everytime.",1535398099.0
deanmsands3,"How would you detect the fake news? And then, once found, could you detect the fake news and then run statistics on how people react to it vs real news?",1535396593.0
hoopyhooper,"Use a-frame to share 3D models in VR between users.
One user starts the server drags the model into the web browser, sends a link/invites someone else to the instance and they can both add annotations and highlight areas.

Then pitch it to F1 companies",1535398365.0
qqwy,"something else, but also very interesting and research-worthy are distributed applications. ",1535401889.0
MajesticMwe,Mental help AI run on a server?,1535407888.0
krum,I wrote a CMS backend entirely in C++ to demonstrate how much less it cost to run at scale than comparable node/JVM applications. ,1535413424.0
putin_your_ass_,"""Code: The Hidden Language of Computer Hardware and Software"" by Charles Petzold",1535332265.0
,[deleted],1535335453.0
twktue,[Computer Science Distilled ](https://www.amazon.com/Computer-Science-Distilled-Computational-Problems/dp/0997316020/ref=mp_s_a_1_1?ie=UTF8&qid=1535335984&sr=8-1&pi=AC_SX236_SY340_QL65&keywords=computer+science+distilled&dpPl=1&dpID=51TC80IuOSL&ref=plSrch) ,1535336053.0
pohart,Why do you want to learn it? And what is your Computing and math background?,1535334626.0
player2,"Example of least privilege: admins do not have write access to all files on a shared drive just because they are admins of that shared drive.

Example of need to know: admins don’t know what’s on a shared drive just because they set it up.",1535331522.0
ctchalk,"Not an expert, but the differentiation seems to be ""least privilege"" relates to writing data while ""need to know"" relates to reading data. IMO ""least privilege"" can be phrased to relate to both (knowing is a privilege) but I guess the authors find some value in separating the concepts.

Edit: Please see /u/dr1fter 's comment, I think he clarifies a more important distinction.",1535328906.0
tbrownaw,"They're the same basic idea (""only what you need""), but one is applied to *doing* things and the other is applied to *knowing* things.",1535336615.0
Dr_Legacy,"""Least privilege"" relates to what can be *done*.

""Need to know"" relates to what can be *seen*.",1535341714.0
vorlik,"I don't think these are being presented as two different approaches to access control. I think the text means ""access controls should use both of these principals, which are already pretty similar.""",1535328740.0
A_Crazed_Hobo,"User access controls is giving the users only the rights they need, and only giving them the ability to view the data they need. Similar but not quite the same",1535328898.0
paypaypayme,"Least privilege has to do with access to systems/hosts e.g. ssh sessions, credentials, root access, etc. need to know is more specific to data access",1535329767.0
jhanschoo,"Another example of least privilege: remote users do not have the ability to execute certain system programs. This clearly does not fall under ""need to know"".",1535334821.0
rotharius,"Least privilege focuses on *ability*: restrict what a user/system can *do* to a minimum. A visitor should have different rights than an administrator.
This makes it more difficult for an attacker to exploit given abilities.

Need to know addresses *knowledge*: don't give away information the user/system does not need to *know* for their role in the process. A non-engineer should have less information about infrastructural details (hardware,  networking, services, environment, program execution) than a random user. Also, only store and expose data if it  is necessary. This is somewhat related to privacy by design: only store user data you need for the purposes of your application (and the user agreed to share).
This makes it more difficult for an attacker to gain information about possible vulnerabilities and could reduce the scope if there is a breach.

In a way, need to know is a species (subtype) of least privilege (reduce the ability to gain information), although stating them as separate principles makes their intent more explicit and increases the chance of compliance.",1535350961.0
cos,"There's a lot of overlap between the two ideas, but they're not saying the same thing exactly.  Let me give you an example to illustrate how one of these addresses an issue the other does not...

I'm part of a team that administers a public-facing computing service which uses an internal database to store state.  As part of my job responsibilities, I sometimes need to do things that involve altering data in this database.  However, it's also risky to alter this database - if I accidentally do the wrong thing, I could break the product for customers, or cause them to lose access to data.  So, the internal group that my team are members of, doesn't actually have permission to do most alterations on this database, though we *do* have permission to look at this database and to make certain kinds of less risky changes.  Instead, we run some internal servers under a userid that is a member of another group - a group with no human members - that has full access to this database.  We put routines in these internal servers to do the kinds of things we need to do for maintenance and troubleshooting sometimes, along with error checking and consistency checking to prevent mistakes.

Here, we're using the principle of least privilege by deciding that it's safer for the humans not to have direct access to fix or break the database.  Instead, we mediate that through routines whose code is reviewed, then tested, before going into production.  But as you can see, this has nothing to do with ""need to know"" - we do have a need to be able to inspect this particular database, and when we need to, we can do so.  But we still use ""least privilege"" to reduce the risk of harmful mistakes.",1535339010.0
MattiasInSpace,"I think the question has been answered already, but a hot tip here is that the author made the first sentence of each definition parallel to highlight the differences.

“Principle of least privilege—Grant users *only the rights and permissions* they need to perform their job and no more.”

“Principle of need to know—Grant users *access only to the data* they need to perform their job and no more.”

So least privilege is about what they can do, need to know is what they can look at.

I agree that it's a questionable line to draw since, most of the time in computer science, reading data is itself considered an action. It seems unnecessary to think of users differently in this regard than one thinks of programs, processes, etc.

One place the distinction is useful, perhaps, is when talking about the permissions themselves. Admins can manage the permissions of other users, but it might be useful if users don't even know what those permissions are. That could help make the system as a whole more secure because it would be less obvious to an attacker what the main lines of attack are.



",1535394922.0
missing_in_time,These are two considerations of the same topic of access control. Simple office example. A company has two floor of a building. Seperate departments are on each floor. Aa new employee can only access the floor that their department is on. This is least privilege. Now if one department is Human Resources (HR). HR does training and payroll. An employee is tasked with training company policy and they are locked out of the file room with payroll info as they have no need for payroll data to accomplish training. This is need to know. An HR employee cannot say I have floor access so I should be able to see payroll data. These concepts are really a matter of granularity of access controls.,1535336822.0
link23,Least privilege is about which activities a user is allowed to do or to cause; need to know is about what data the user is allowed to see.,1535337544.0
dannybullis,"Privilege: what you can do
Need to know: what you can know
—
An example of least privilege is setting up port access for, say, a web server. You start by restricting access on all ports except for 80 and 443, for example, and could go even further by only allowing traffic from IP addresses within a specified range. The “do” part here is what ports on your server users are able to interact with various services on. They can submit an HTTP request for data on port 80, for example. That’s an action, and they are given the privilege to perform the action successfully only to achieve their need to make HTTP requests.

An example of need to know is a database with a bunch of different tables containing various data about a company. Some tables contain data pertinent to, say, payroll...salaries, wages, and the people in those positions. You’d only want to allow access to that database to people within the company who have a need to know that information (like an accountant). There may be other tables in that database shared with other users throughout the organization too, like all of the projects in progress and the hours of each employee. Maybe team leads or managers are granted access to various project tables so that they can report on how much time is going into their team’s projects. These individuals could be granted access to these tables, but wouldn’t need to be granted access to the payroll data of the entire company. 

Good luck!",1535358055.0
deftware,Execution permissions vs data read/write access.,1535363497.0
Kins97,One is for permissions one is for data access thats the difference ,1535363744.0
,The privilege pertains to “rights.” The second principle pertains to “data.” The first restricts access and capabilities. The latter restricts the set of information passed to the user.,1535372315.0
qwazwak,"I'm an some asshole on the internet so take this with salt..

But I think least privilege applies to application/commands/ect

Need to know applies to data/info

Maybe",1535376966.0
its_joao,What book?,1535352835.0
ljcrabs,"Note this is in conflict with digital transformation, which says to give everyone access to everything unless it's explicitly private.",1535356998.0
Cocomorph,"I personally believe that no one should be able to graduate from a CS program without some exposure to HCI. Maybe not a mandatory full semester course in the general case, but there is a lot of low hanging fruit there and the field as a whole desperately needs it. ",1535308686.0
UncleMeat11,"PL PhD here. 

I think that a good understanding of compilers is incredibly powerful and I think that at a graduate level formal analysis becomes an immensely useful technique for a number of applications. But if you are just getting an undergrad degree, it is likely not especially useful. Only a small number of engineers actually work on compilers of static analysis and to make a *good* system you generally need a lot more experience than a single class. Knowing how pointer analysis works or how galois connections can prove that your abstract interpretation framework is sound just isn't going to come up in the day to day. 

OTOH, front end work is common. Knowing the basic principles of HCI will let you dodge a lot of dumb errors when it comes to front end work. If you will only take one course of either subject, I'd guess that HCI is more likely to be useful in your career.",1535319859.0
b4ux1t3,"HCI is a class that has the potential to be a *lot* of fun, but usually ends up being a discussion of ancient UX and UI design philosophies.

Note that these philosophies are good to know, but are often intuitively understood by younger aspiring programmers. They've usually been looking at UIs,good and bad, their whole lives, and typically know when something doesn't feel right.

A *good* HCI class would delve deeper into things like Fitts' Law, and examining how it applies to more modern interface mechanisms (like touch screens or AR/VR). Unfortunately, you all too often end up just getting a generic look at how to space things out on a page, with no real depth.

I liken it to trying to teach someone to program by showing them the solutions to a bunch of programming challenges. Sure, they might then be able to solve these problems, even with slight variations, but they haven't learned how to fundamentally solve problems.

Program Analysis is *really, really* cool stuff. It's hard to make it boring, assuming you're the type of person who plays Zachtronics games, is in to things like the [1K Challenge](https://hackaday.io/contest/18215-the-1kb-challenge), or enjoys sites like [Dwitter](https://www.dwitter.net/). It's not going to be for the feint of heart, but it will be a hell of a ride. 

EDIT: Fixed some formatting.

EDIT 2: FYI, I would recommend taking both before you graduate. Both classes will help you solve real world computing problems, which cannot always be said about some of the stuff you take in CS programs. 

EDIT 3: [Disclaimer](https://www.reddit.com/r/compsci/comments/9ahh9t/not_sure_what_to_take_program_analysis_vs_hci/e4vkg6q/) elsewhere in this thread.

EDIT 4: Added further clarification re: HCI.",1535308390.0
jessi-t-s,"If the HCI course is project-based, I don’t think you will have to worry too much about the negative things some people here have said, eg only learning graphic design or discussing outdated philosophies. You’ll probably build an entire UI and learn first-hand what does or doesn’t work, how to evaluate the interface, how to do user research, how to specify project requirements, etc. It will probably be VERY useful for project planning.

During my undergrad, I took a course in Software Analysis (probably similar to your Program Analysis?) and a course in HCI. Software Analysis was actually a prerequisite for HCI. I think both are important but my overall impression based on my own experience is that program analysis will teach you how to make sure a system is effective and efficient, whereas HCI will teach you how to make sure a system is effective, useful, and usable.

Either would be good but I vote HCI.",1535310754.0
mattrepl,"Program analysis. You’ll have a better understanding and appreciation of how languages you use to write code are implemented. There are all kinds of fun problems and solutions to learn. Compilers and PLT aside, program analysis is also used in computer security.

HCI is useful, but it’s easier than program analysis to pickup outside of a class.",1535346775.0
,"Between the two I'd take Program Analysis, it sounds like it'd help you with debugging. And HCI sounds like a pain to me. Don't be dissuaded by the ""senior level"". A year from now you'll be smarter, but as long as you have all the pre-requisites you aren't so much dumber now you'll have a hard time. A lot of times rankings area meaningless anyway. I took a senior level course in network security last semester, and it was boring and unengaging. ",1535306409.0
diegobust4545,"Want an easy A, learn about design fundamentals and good practice for designing a good UI? Take HCI.
 
Want to work hard for that A and learn about program optimization and program correctness? Take Program Analysis. ",1535307438.0
MikeyLifeCerealQuery,Program analysis is very useful. Human computer interaction is less complex and you probably can understand it from a book and some online videos. ,1535320414.0
,[deleted],1535309816.0
Flutterwry,"Post this in /r/ProgrammerHumor, not here.",1535302157.0
noam_compsci,This is the most uninformative post i've ever read. ,1535295490.0
vorlik,yes,1535268511.0
omon-ra,Yes. Also try his MOOC on coursera.,1535270437.0
obp5599,"Everyone will say math, but as someone in their senior year of cs, id say a minor in EE or CE would be great",1535268026.0
IAmAStickAMA,"Cognitive science or psychology are great choices, especially if you're interested in AI or HCI. Schools offering cog sci majors/minors will often let you count some of your CS courses for cog sci because of the overlap.

Philosophy also works well as both it teaches skills in logic and problem solving that will prove useful in CS. Your philosophy department might even offer courses in linguistics (for natural language processing) or ethics that overlap with your CS studies. Obviously, though, you should only pursue this if you're actually into philosophy.",1535273398.0
SCElectrons2025,"If you have an interest in physics along with programming, coupling a physics and CS major isn't a bad idea. The two majors complement each other very well. A lot of condensed matter theory or astrophysics is more about programming than proofs, for example. If I could do it over again, that's what I'd do.

But, I stress: only if you have an interest in physics. If you prefer to focus more on the computers themselves than applications, math (algorithms) or EE (kernels, internal stuff) might be better choices.",1535269694.0
noam_compsci,"Depends what you want to do? If you want to keep your options open, I'd do CS + Math or even CS +Philosophy. If you want to become an EE or CE, then obviously those. If you want to go startup or consulting, finance, accounting or econ. 

",1535295734.0
realFoobanana,(math but I only love math so I’m super biased),1535298624.0
foreheadteeth,I did math+CS and now I'm a math prof in university. I worked in industry for many years but never ever used math of any substance when I did.,1535302362.0
CheapChannel,"I did EE & CS (basically a CpE degree) and it had a big impact on my career. It gave me exposure to more hardware oriented things and that's where I steered my career towards. 

It probably sucks hard though if you don't end up liking low level programming and all that comes with it. ",1535313734.0
Zulban,Often programmers are terrible teachers. Absolutely terrible. I say this as a programmer and a teacher. [This video](https://www.youtube.com/watch?v=CjYEpVNbM-s) goes over lots of critical things programmers often miss about teaching. Also consider [reading this short article](https://qz.com/691614/american-schools-are-teaching-our-kids-how-to-code-all-wrong/).,1535236218.0
sayubuntu,"The most difficult part is not going to be the actual “coding” rather it is explaining things in a way the people can understand. Learning how to use metaphors/abstractions effectively is the single most valuable skill you can have, particularly if you hold office hours.

The trick to explaining things like this is understand where your metaphor works and where it doesn’t. 

“Think of a class as an instruction manual. Now unlike an ikea manual when an object is instantiated it will be exactly as described—no extra screws—but like an ikea manual ambiguity can create havoc. The real idea though is that your class is not a tangible constructed object, it is just the instructions for how to build and use one.”

Teachers tend to have a favorite metaphor, you have car people, sports people, food people, etc great teachers are flexible enough to find the right metaphor for both the subject and the student. Maybe I’ve never built an ikea desk, hit me with “it’s like a recipe” though and I go “ahah! I cook all the time tell me more about how object oriented programming is like the kitchen.”

Another thing you really need to do is watch out for saying discouraging things. “Oh it’s easy it’s like....” see I’m not listening to you I’m self deprecating because apparently this is easy but I’m over here struggling. 

The last thing I would suggest from personal experience is to look into the concept of “talking to a rubber duck.” I forget what book I stole this from (pragmatic programmer maybe) but it worked like you wouldn’t believe. I literally left a little box with a few rubber ducks outside of my office and would require people to explain their issue to the duck before coming in. Literally over half of the people that did this would just figure their own problem out and walk away. The other half actually knew how to tell me exactly what was giving them problems—90% of my work was done.

tldr: As a TA you are going to be a lot more familiar with the material then pretty much everyone in the room. If you want to work at being a great TA work on communicating effectively.",1535241104.0
poodleface,"It will be very tempting to just tell students the answer when they ask questions about why their code is not working. I strongly encourage you to not do that, even though it is faster. Learning to program is also learning how to recognize your own mistakes. This doesn’t mean you don’t help at all, but I found myself mostly asking students questions about their code. e.g. If they are missing a semicolon, I might say “The previous line works, but the next line doesn’t. What is different about them?” 

How much you push this is really dependent on the student and if this is their only programming class versus needing to truly understand it for future classes. Obviously you give them both the same care and attention, but frequently the personal motivation is completely different. This is just something to be conscious of. ",1535248559.0
braedont_ask,"Hey, I was a TF for my college's intro CS course (CS50 @ Harvard) and did struggle with getting high class participation. I took the laid back approach with students and treated them how I would have wanted to be treated: not cold-calling, being flexible with assignments, and joking during class. I can confirm that this was NOT effective, at least among my students. It was actually deadened my section and made for an abysmal semester. My students held me in a low regard and saw me as unhelpful despite dedicating countless hours for office hours among other things.

My recommendation (first few are only relevant if you teach a section):

1. Be well prepared for the lesson and ensure that you are only delivering information you know as fact (this should sound obvious, but can slip your mind when trying to answer some questions).
2. Make sure you know the problems inside and out so you can answer questions to the best of your ability.
3. Keep a HIGH standard on what you expect participation to be in your office hours, section, or lesson. The worst thing that you can do is let students fall off the edge because the do not actively engage on their own.
4. Cold call if necessary - honestly its brutal but useful.
5. Flipped classroom mechanisms are great for small lessons and teaching (IMO). Try to get students to try a simple, but related, idea if they are struggling.
6. Make sure that any extra time you put in is apparent to your students. I honestly believe that it will help them appreciate you better.
7. Make sure students are constantly aware of what resources are available to them. For example, if you are holding extra office hours or if there is some sort of study mechanism they can reference repeatedly bring it up.
8. This may not concern you (but is something I care about), but I advise you think critically about how approach URM's in the class (i.e. female, black, hispanic, etc). An intro CS course will always have the highest concentration of these students, and as a teacher I believe that there is a strong obligation to make sure they feel comfortable to learn. It has become clear to me that the support of these students is the most rewarding because of the societal impact and because if you are catering to students that deviate from a usual CS student you are likely covering most potential edge cases (as a good computer scientist should!).

Sorry for the rant! Good Luck!",1535252918.0
DavisNealE,"[This book](http://teachtogether.tech/en/) by Greg Wilson, the founder of Software Carpentry is an excellent summary of pedagogy.  It's more focused on workshops, but much of the wisdom will carry over to discussion sections as well.

His rules for teaching:

1.  Be kind: all else is details.
2.  Remember that you are not your learners…
3.  …that most people would rather fail than change…
4.  …and that ninety percent of magic consists of knowing one extra thing.
5.  Never teach alone.
6.  Never hesitate to sacrifice truth for clarity.
7.  Make every mistake a lesson.
8.  Remember that no lesson survives first contact with learners…
9.  …that every lesson is too short from the teacher’s point of view and too long from the learner’s…
10. …and that nobody will be more excited about the lesson than you are.
",1535246518.0
agumonkey,maybe htdp/htdp2e books from felleisen et al ?,1535237894.0
knot_hk,"Normally schools only allow those who have taken the class to TA, so just read over your course materials. If not, then ask for them from the prof.",1535233832.0
evil_burrito,"The syllabus, first and foremost. I would also become intimate with ""Effective C++"" by Scott Meyers, if you're not already.",1535244127.0
redditEnergy,"I often go by the philosophy of 

""If you couldn't invent it you don't know it.""

I generally don't give advice unless I can break it down to such a level that anyone could understand. 

Also don't assume three persons knowledge of anything. You could skip over the very part they are struggling on.

Ex: a friend needed help with physics. He just couldn't get it. And I asked 

Is it the formulas? No.
Is it what the formulas mean? No.
How to derive the formulas? No.
Having trouble interpretting the question? Kinda (finally getting somewhere)

How would you approach the problem? See that's the thing I don't know how to start. BINGO.

I found where he struggled by prodding and understanding that misunderstanding can happen anywhere in the learning/ problem solving process. 

",1535286840.0
ImaginationGeek,What are your job responsibilities going to be?  Which aspect of the job do you want tips for?,1535235972.0
NeoKabuto,">as a soon-to-be-instructor?

Are you actually acting as an instructor for the class? Most TAs grade and do office hours while the professor does the lecturing. Maybe running recitation sections if the class has them.",1535242719.0
TopBeginning,"Pointers, Memory Management, Style guidelines, and Object Orientated Programming. ",1535251739.0
mc8675309,"I led two group tutorials in college on software engineering and programming as I had been both before I went to college and our school didn't have a CS dept at the time. 

The two most important things I tried to remember is what the students are expected to know and what they are meant to learn. Then I took them on a guided journey. 

The syllabus for the class should outline those two things and you should be able to follow the syllabus through to whichever week your students are on. For an intro CS course this should be pretty easy since the prereqs should be minimal. The students who need a TA should mostly be clean slates rather than a class full of people with different levels of understanding and different backgrounds. 

That said, they will have different backgrounds and goals. The math major whose had some theoretical math will understand abstractions in a way that lets you say, ""this is an abstraction, these are the rules, this is how you use it, here's an example"" and bam, off to the races. The first year student with no background at all may need a lesson on computer memory layout and registers to start to grasp pointers. 

Students will have different interest levels. Some may just want to get the thing to work and others may really want to grok the CRTP and the assignment will be an afterthought. They take different paths on their journey. ",1535254684.0
supercool5000,"Used to be a TA for a C/C++ course back in 2002-2004, 4 semesters. Every student has a different level of skill and interest. Some will need hand holding, some will whip through homework/labs in no time. Your job isn't to force them to work your way, but to figure out how to teach them individually. Just be patient and make them feel good about the smallest accomplishments. That's the best way to keep them engaged and interested.

Anecdotally, one of my students was a high schooler doing PSEO. He was incredibly bright, went to UC Berkeley after for undergrad and I still keep in touch 14 years later. He owns his own dev company now. I don't take any credit for his accomplishments, but he's thanked me for getting to where he is. You never know who you'll teach, but it'll have an impact on them all.",1535258205.0
allenguo,"Highly recommend MIT's *[The Torch or The Firehose](https://ocw.mit.edu/resources/res-18-004-the-torch-or-the-firehose-a-guide-to-section-teaching-spring-2009/)*. It's humorous, straight to the point, and full of practical advice (e.g., how to use a blackboard effectively, how to structure lessons, how to deal with ""invisible students""). Easily readable in a few hours.

(I was a TA at Berkeley for six semesters.)",1535258378.0
flipcoder,"Probably the best way is to write a few programs, even a small game that uses the concepts that you'll be focusing on in the course.  You may not realize what you're rusty on until you start coding again.",1535263962.0
Mortdeus,"you guys teach C++ in an intro CS course? Do you guys like medieval forms of torturing your students to convince them why they may or may not be cut out for the tech industry? 

Like, ""weed them out early. if they can't sit here and power through learning compsci via C++ without blowing their brains out, they aren't ready to try and build a smarter search engine at google""? ",1535266549.0
deepakdai,Hey!! I know a guy who just got hired as a TA. Reply if you know my username.,1535276002.0
onetwosex,"Since none mentioned it, the following link has some nice examples. I used it when I was a TA for an introductory C++ class:
http://www.cplusplus.com/doc/tutorial/
http://www.cplusplus.com/reference/

You might also find useful this cpp shell for interactive problems, examples, whatever: http://cpp.sh.

A similar website is this one, more difficult to use but doesn't break that often: https://ideone.com

Finally, from books I enjoyed Stroustrup's ""A tour of C++"".

Have fun !",1535408730.0
SiliusSodus,"I would review what Google says programmers need to know to get hired. It's a pretty comprehensive list for programming. I got these resources from a contact at Google who suggested these before trying to get a job there. Sadly I didn't get it, but hopefully you can help some kid get there. Best of luck! 

Sources:

[Google for educators](https://edu.google.com/computer-science/)

[Google Tech Development Guide](https://techdevguide.withgoogle.com/)",1535613403.0
trout_fucker,"Designing applications in ways that make sense and not sticking to a specific dogma is the way to go. Usually that ends up being some mix of OOP, functional, and procedural.

Otherwise you end up with shit that looks like this: https://github.com/EnterpriseQualityCoding/FizzBuzzEnterpriseEdition

There's good reason why even languages like Java are succumbing to functional paradigms.

edit: When I saw this there was only a title and no content. I think RiF bugged out. Whatever, point still stands. ",1535218706.0
future_security,"""Why aren't you totally starting over with us animals too?""

""NPCs would take a lot of work to rewrite. Besides they already kind of use inheritance and work most of the time.""",1535230708.0
Wulfnodh,Clickbait.,1535209017.0
philipwhiuk,http://catalog.illinois.edu/undergraduate/las/academic-units/stats/statistics-computer-science-major/,1535186951.0
dudeimtrying,Required? No. Recommended? Yes.,1535218798.0
SCElectrons2025,"I've done a little bit of ML on my job. Generally speaking, the main thing is to have a solid grasp of linear algebra and data structures/algorithmic analysis. Nothing too complex for the day to day jobs, I can't speak for the ones that require doctorates. But for the former, you have those nailed, and you'll be fine.

​

That being said, I'd highly recommend doing a course on operating systems. Not so much for the knowledge, per se-Three Easy Pieces is online and quite comprehensive-but because you'll often do a cool project at the end with other people. Great for experience and resume building. I'm a non-student trying to learn OS and I can appreciate how much better I'd be at doing it if I had someone to build stuff with. ",1535267522.0
,/r/iamverysmart,1535179897.0
spinwizard69,"Spend the time in class learning the good habits that hopefully the program promotes.    One of the worst things about the self taught is the bad habits they pick up and the difficulty others have in getting them on track.   Ideally you would just forget everything you think you know and start at the ground level.

IF you have no bad habits and can fly through class, I highly suggest joining a study group where you might be able to help others.   The goal being to develop the skills needed to work with people.",1535188138.0
khedoros,"The temptation may be to judge the class assignments as trivial and blow them off for a while. I did that a fair amount at university, and it repeatedly bit me in the ass. So, watch for that. If you're so far ahead already, it'll be an easy A as long as you do the work.",1535180019.0
Kibouo,Did you make chrome? I'd finally understand why it uses so much RAM.,1535193262.0
noam_compsci,"I know what you mean man. I am in a similar situation. This lecturer told me I had to take Calc 1 and Calc 2, before Calc 3 but I read the entire wikipedia page for Rinemanns Hypothesis This is gonna be a fun semester. ",1535200399.0
umib0zu,[See this](http://colah.github.io/posts/2015-01-Visualizing-Representations/),1535172897.0
ImaginationGeek,"There are a good number of different techniques in machine learning.  Some are more opaque than others...  so for machine learning as a whole, the answer is “it depends”.

Also, machine learning techniques are not 100% accurate.  When you train a machine learning algorithm, you then test it to measure how accurate the trained algorithm is.",1535176013.0
spinwizard69,">Does machine learning give you a 'solution' with no understanding?

Doesn't this depend upon what you mean by ""understanding""?   Does a computer ever really understand anything no matter the techniques used to program it?

At least with today's state of the art and any near term potential tech, computers have no ability at all to ""understand"" anything.   They are simply dumb bitches that do our bidding.",1535188954.0
QSCFE,I crosspost it to /r/MachineLearning subreddit https://www.reddit.com/r/MachineLearning/comments/9apx5h/does_machine_learning_give_you_a_solution_with_no/,1535384095.0
ImaginationGeek,"That’s definitely food for thought.  It puts a finer point on an idea I’ve been thinking about for introductory CS, which is to get students to *reason* about their code and use logic to determine that it is correct *before* running it (merely to confirm their reasoning).

Of course Dijkstra is taking that idea a step further here.

(I’m also not saying that I’ve achieved this in class, just that I think I should work towards it...)",1535171866.0
iamasuitama,That guy was something else,1535191889.0
IndependentBoof,"Because CS is a leader in innovation (and very visibly so), every few years there's a new area that everyone gets hyped about.

Right now it's Machine Learning. A few years ago it was Big Data. Cloud Computing. Cybersecurity. Bioinformatics. Human-Computer Interaction. Artificial Intelligence. Virtual Reality. The longer you go back, the longer this list becomes.

My main point is that all these areas were the ""hot topic"" in CS at some point. They're all still around and important subjects making real contributions to the broader field. People get excited about them when they're new and that's a good thing. However, don't be surprised when Machine Learning is no longer said with so much excitement because there's some other *hot new sexy topic*. That isn't to take away from ML or any of those other topics, it's just the nature of our field.",1535149115.0
iconoklast,In this house ML is a programming language! (sorry),1535153562.0
Kris--,"Machine learning has its limitations:

**Narrow Application Focus**

Machine Learning techniques suffer from a narrow application focus, as such their problem solving abilities are not generalised. All machine learning techniques are designed to work within a limited domain. Programmers cannot utilise one specific method, designed to analyse text for example and then feed visual information into the network. The network would simply output nonsense. This narrow application focus cannot be overcome by training, the programmer can only make more complex networks and extend their usability. This poor reusability is the greatest weakness of current machine learning techniques. In the real world problems are multifaceted and situations change, meaning the network will need to be re-designed when new parameters are added.

**Difficult to Maintain and Debug**

Due to his narrow focus networks used for complicated tasks are usually complex and employ multiple techniques. These techniques themselves may require individual programmers who are experts in that area to work on the code. This creates difficulty for programmers maintaining and debugging neural network based programs.

**Limited Computing Power**

Despite advances in hardware computational limitations still exist due to the scale of learning required for complex problems.

**Correlation not Causation**

Neural network methods encode correlation within the data they process, they do not understand causation or relationships of data. Despite their successes, they are still a ‘dumb’ technique akin to historical brute force methods relying on computational power. This limits their abilities and they require trained individuals to draw conclusions from the data they produce.

&#x200B;

*However*, there is a reason machine learning has gained attention recently. The historical example of Deep Blue vs AlphaGo is a good example of why. Deep Blue, a chess playing computer created by IBM in 1996 and demonstrated to the world by eventually defeating Chess Grandmaster Garry Kasparov. At the time this was a global news story, a computer defeated a human at what is generally perceived to be a game requiring human intellect to win. Deep Blue didn't have human-level intellect, it relied on explicit programming and computing speed to win. 

Google’s DeepMind’s AlphaGo, a similar game-playing AI, leveraged neural networks and reinforcement learning, to defeat human players of the Chinese board game Go in 2015. AlphaGo is entirely different to Deep Blue, it is not explicitly programmed - it learns. Deep Mind taught AlphaGo what the game of Go looked like. It was exposed to a large number of amateur games, before it was pitted against different versions of itself to learn from its own mistakes and incrementally improve.

**Explicit program**

`If email contains p!lls`

`Then mark spam;`

`If email contains  ...` 

`If email contains  ...`

**Learning program**

`Try to classify emails;`

`Change self to reduce errors;`

`repeat;`

It's a pretty powerful difference.

However, it suffers from the above limitations.",1535152902.0
Carpetfizz,"If you’re a student, focus on learning fundamentals in math and engineering. It will allow you to self learn the basics of the next big thing.",1535160943.0
TheBlackElf,"There is no doubt that ML is a big an important CS and engineering field. However, these are my quarrels with it:

**It's like astrology**

You can't really say if it's going to work or not until you do it. Did it work? ML is great! It didn't work? Oh well, it isn't magic after all. 

**It's like alchemy**

The hard truth is that we don't *really* know how it works. At least, not at the same mathematical / analytical rigour that CS usually delivers. In a way it's actually really impressive that these complex numbers actually magically work, but it's daunting to develop something which is quite black box.

**It's like marketing**

It's quite ridiculous how much AI and ML get overhyped in the media. I understand that this is how funding is acquired, but sometimes it's borderline [disingenuous](https://medium.com/@josecamachocollados/is-alphazero-really-a-scientific-breakthrough-in-ai-bf66ae1c84f2). ML is lately plagued by lack of reproducibility, which you hear from fields like psychology but not something acceptable for any CS area.
",1535158479.0
tgaz,"""AI"" has been a thing since at least hobby magazines of the 70s. Turned out to be harder than expected. Once in a while there is a popularized breakthrough, and we get a few years of frenzy. It's getting worse because

1. companies play on press releases more and more, and
2. the war on attention in the media is serious business.

ML actually has useful applications in image and sound (signal) processing, where there is lots of redundant data to do pattern matching on. We used to do that using rigorous rules, and now we can ""see what sticks"" (a.k.a. evolution) using ANNs and similar algorithms (a.k.a. brain).

I'm gonna go on a limb and say that the latest craze was started by [Deepmind's Atari paper](https://deepmind.com/research/publications/playing-atari-deep-reinforcement-learning/) in 2013. It popularized deep learning through press releases. The upswing then was probably fueled by the increase in processing power that modern graphics cards give (and the PS3 Cell processor). It made deep learning a reasonable alternative.

Unlike ""blockchain technology,"" which is equally hyped, but so far without any useful application for it. A normal centralized database seems better in most cases. I've only seen one exception so far: fully transparent voting systems.

I'd say ML definitely is useful right now and next year. But it can't solve all problems, and it's right now hard to know which problems are easily solved by ML techniques. Trial-n-error is expensive, but needed at this point. That makes it boring to most, except researchers. Maybe in 20 years it will be like civil engineering today; just look up your scenario in a table and implement, with a 99.999% certainty that it will work.

Blockchain... Uh, yeah, when someone finds a good application for it that isn't just an over-engineered database. Not this year, and probably not next.",1535150794.0
EVE_ddred,"I don't have a huge experience with Machine Learning, but I think it does have very useful applications for very specific tasks.

Machine Learning / Optimisation algorithms can basically increase the performance of tasks that a deterministic program would struggle with. I realised the strength in these algorithms in my CS classes when performance at solving NP problems started to hinder performance of the program. At that point, it was better to use a genetic algorithm to find approximate solutions for a certain amount of time, depending on the scale of the program, before feeding these solutions into different threads of a deterministic algorithm which effectively allows you to bypass a lot of the hard number crunching and in some cases allow you to solve NP problems in P time.

To me, machine learning is a very useful tool with wide applications and is worth learning even outside of research.",1535151012.0
noam_compsci,"I dont think so. A lot of the latest trends in what is 'profitable' (if such a thing can be defined) are really about capturing someones attention. 

ML/AI are ways to assist in capturing attention. But in on itself, I dont think it is a game changer. It is not the new 'internet'. I think it will be like the advent of a more powerful computer chip

edit: I realise how vacuous this comment is. To elaborate, I think AI and ML are like biotechnology and in particular, DNA based medicines (so you have a medicine that matches your very DNA). It is clever tech, it could change the world, it should change the world, we get excited about it changing the world - but it is just so far away from changing the world. In the mean time, it will just slightly improve stuff in some areas. ",1535151784.0
HugeRichard11,"In 20-30 years if you are only a ML engineer that might be bad if tech moves in a different direction as you never know what's coming next. Could it be ML possibly, but I kinda doubt it since the investment cost are so high it generally takes too much capital for one company to be successful without a lot of failures.",1535167656.0
mimighost,"ML is a useful technique, no doubt. Whether it is Overhyped, that is a perception problem, or a sales problem, has surprising little substance to do with ML itself.

However, back to the basic, what does ML offers but previous technology struggles to handle? It is not hard to see that ML fits its position, offer prediction/optimization based on past experience, which surprisingly is an old business. It is used to be the job of Shaman/Priest/Historian, etc. If we human still believes in that history has generalization power to indicate what is in the future, ML will keep stick around for a very very long time.",1535168800.0
combinatorylogic,"It's a cycle, repeating over and over again. The previous iteration of this hype cycle was called ""data mining"". You can see how it all ended.",1535201582.0
spinwizard69,"Ml can be a very good solution to some types of problems at other times it looks like a fancy way to do a database look up.    I woulnd't get too wrapped up in being a ML expert.   A lot of ""experts"" end up without a job when the current hot thing becomes a big chill.    

Instead focus your professional development on a array of skills.",1535157271.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/u_kamranahmed_se] [Is Machine Learning Overhyped?](https://www.reddit.com/r/u_kamranahmed_se/comments/9a3vtf/is_machine_learning_overhyped/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1535168282.0
iamasuitama,"Heard that same quote, it's not useful. I don't think that's true. There is a specific type of problem that you can now readily solve quite well with ML libraries. Something like, processing a large and steady stream of input sensor data to a stream of output sensor data. A ML solution is now a possible solution for that type of problem, where before we would not have a solution at all. I'm into music software, and even though I would not have thought so, even in that field it has really interesting applications. (sadly, can't find the link to the talk I'm thinking of rn)",1535190874.0
jokoon,"Yes, but for several reasons.

* ML is a set of methods you can apply to a variety of problems, but it depends on the quality of your data.
* ML is as good as your linear algebra and mathematics knowledge. It can get complicated very quickly, and you will often be limited by your hardware.
* There are a lot of ML stuff that is still experimental, so when something works, you don't really know how/why it work because it's just a black box, you cannot extract or synthetize knowledge from ML results.
* ML is just brute forcing data to create AI. So while you can always manage to get some results, you cannot really prove you have some working AI, you just have an average/approximate version of it.

ML is still a breakthrough in the AI field, just so because computing power (especially GPU) is cheaper and that ML is a brute force method.

&#x200B;

Anyway, you cannot really deny that ML is still very practical and useful for prediction, especially in the medical field. Although I'm not sure it would make good prediction in economics, business, politics, etc.",1535203939.0
bartturner,Do not think so.   ML is going to make it possible for us to have cars drives themselves.   That alone is just huge.    But just one application.,1535374888.0
CaptainHatdog,"It is for me at least. I find the research topic extremely fascinating and even though I tend to do bottom-of-the-barrel ML applications, as a data scientist, I tend to help solve fairly core business problems and answer core questions in a way that sounds like magic to most people.

​

And ML will be ""the next big thing"", just not for the typical applications that we have been using for the last 25 years. It will be the next big thing in automotive, advanced robotics, medicine and more. The main new drivers here are super strong machine vision and reinforcement learning with advanced simulations.

​

EDIT: oh yea, also expect machine learning to be a central topic in the next big arms race. When nations realize the impact of weaponized AI, that snow is going to ball like it's 1945.",1535152400.0
DeltruS,"Machine learning may have found a way to connect mathematics and intelligence, and that way is through statistics.  It may be that statistics is the true form of intelligence, the way that physics is the true form of the natural laws. 

Joscha Bach, an AI scientist, once said ""If a model gives better predictions than a statistical model, it is a better statistical model.""

Is this true? I'm not sure, but if it is then it lights the way for decades of progress and the eventual singularity.  ",1535154044.0
Blue_Q,"Undeniably, machine learning especially with neural networks boosted things like image recognition or natural language processing, I regularly use it on my own. However, I don't think you necessarily achieve the best results for general purpose AI by mimicking the human brain and thus all its flaws, from trivial things like seeing a face in a cloud (We already see some deep learning programs ""hallucinating"" things that aren't there) formation to making complete irrational decisions during heated moments. In the end, the most impressive AI systems like Watson combine multiple AI techniques, from classical deductive expert system, to statistical/Bayesian inference, neural networks, evolutionary algorithms,...  ",1535156314.0
underwhere,Karpathy once tweeted 'Gradient descent can write better code than you'. I found his ideas on ['Software 2.0'](https://medium.com/@karpathy/software-2-0-a64152b37c35) interesting. ,1535165969.0
DBZard27,"I am a freshman, computer engineering, and I am learning C++ rn (DS+Algorithm). Should I learn ML after that ?",1535168264.0
Number1ess,If we had enough computing power machine learning would be truly revolutionary ,1535157405.0
Sjeiken,Hype comes from retardation. ,1535156941.0
p_pistol,"1) yes to both, if there is no goal node specified then the algorithm will search the whole graph.

2) I have no idea what you are talking about.",1535127505.0
ItzWarty,"DFS is an abstract algorithm for visiting nodes in a graph:

    # Graph DFS Implementation
    visited = {}

    fn DFS(node):
        # add returns false if node already contained, else true
        if !visited.add(node): return
        F(node) # visit node (some abstract operation)
        foreach child of node: DFS(child)

    DFS(root)

Or tree:

    fn DFS(node):
        F(node)
        foreach child of node: DFS(child)

    DFS(root)

There are many things you might do in F:

1. Traversal: Print node labels to console as DFS traverses the graph.
2. Ordering: Insert nodes into a list ordered by DFS visit
3. Search: Determine whether a goal node G is reachable (and if so, its path)

There are many variations of DFS, such as:

1. Iterative DFS implementation (rather than recursive)
2. Tracking the predecessor of a node - that is, the node whose DFS iteration first led to node
3. Tracking depth during traversal


        fn DFS(root):
            res = { root: (None, 0) } # indexing is order-preserving
            for (i = 0; i < res.length; i++):
                node, (pred, depth) = res[i] # indexing yields (key, value)
                foreach successor of node:
                    res.add(successor, (node, depth + 1))
            return res


4. Terminating if goal reached, then backtracking to find path from root to goal.

Timestamps aren't a DFS term - that's some extension by whatever book or class you're working off of. ",1535142415.0
FormofAppearance,I learned to use booleans instead of timestamps.,1536265752.0
,"You can definitely load each instruction directly from the harddisk, until you are done loading the second, you can go pick a snack, watch a movie on netflix and by then, the instruction will be fully loaded.


Okay jokes, aside, your cpu doesn't read the instructions from memory. The cpu reads the instructions from the cpu cache. The cpu cache consists of hierarchies, the lowest level is the register, then you have L1 cache, L2 and L3.

The higher up you go, the more memory you have but also the slower the memory is. Your cpu pulls from the ram into the cache and then does what it's instructed to do. The issue is that not everything can fit into the cache. For this reason, you have the ram and when the ram can't contain all the necessary data, the swap is filled. The same limitations apply, the bigger capacity you have, the slower the speed is.

Your cpu already spends much of the time in idle waiting for memory to arrive, having everything on the drive ssd or hdd, only makes things worse.",1535123400.0
cogman10,"To answer the question, it is all about speed.  A CPU COULD run everything from the hard drive, but that would be way too slow.

A 10MHz processor can run an instruction every 100 nanoseconds. (yes, MHz, not GHz).  On the flip side, A modern SSD has read latency in the order of 31,000 nanoseconds. (0.031ms).  A spinning disk is worse at around 3,000,000 nanoseconds (in the best case).

That is to say, by the time a 10Mhz processor is done with one instruction, it has to wait for the time that it could have executed 309 more instructions for an SSD to give it the next instruction.  

That means, a modern SSD will slow down a 10MHz process if it read solely from the SSD.

This is why the memory hierarchy exists in modern system (even in embedded systems).",1535145171.0
IcyWaffl,"So I’m no expert, but I assume it has to do with with the speed of RAM vs storage. Each time a program needs a piece of info, it’s much faster to grab the info from ram rather than a hard drive. I’m assuming this was the question you were questioning.",1535122880.0
barsoap,"That question is rather specific so I don't think you'll get properly in-depth answers, here.

The trend is definitely towards reliability and in particular crash-resistance, though, I don't think there has been a new non-journalling file system since ext3, which is close to two decades old now: Those filesystems technically speaking never lose integrity, they just roll back operations which were in-flight during the crash, and provide full ACID guarantees if you do things right (including calling fsync after you're done for the ""D"" part).

...provided the underlying media is operating flawlessly. Nothing but RAID and checksumming will protect you against bit flips and such. ZFS still rules supreme there, I think.


---

On the off-chance that someone knows something about these things here, though:

Suppose I have an ext4 filesystem on an unpartitioned disk, created by some idiot (possibly my past self). From what I gather resize operations generally take space off the end of the file system, in other words: They don't move the beginning of the filesystem forwards such that space for a partition table could be created.

What are the odds of catastrophical data loss were I to fdisk a partition table over the beginning of the ext4 and then fsck? I don't mind the odd lost file, none of that data is particularly critical at all, I just don't want to lose the whole disk. Or copy it all over. 

Surely fsck is going to find a backup superblock, but would it figure out that every index is off by some number of bytes?",1535118580.0
FUZxxl,"The trend in file systems seems to be moving towards ensuring that the data structures cannot ever be inconsistent, even in the face of crashes or hardware failures.  While the file system drivers are often resilient against defective structures, there might not be a way to fix them.  Instead, restore your data from backup.

Though of course some modern file systems can be fixed.  As a good example for both techniques, Berkeley FFS uses a sophisticated write ordering scheme (soft updates) to make sure that each disk write leaves the file system in a consistent state.  At the same time, a set of utilities (`fsck`, `fsdb`, `clri`, `badsect`) exist to manipulate the file system to repair defects, almost all of which can be fixed.",1535120459.0
jbelshaw55,"Modern file systems in the prescence of corruption in memory, network or in the io path will cause issues but will report an error so your applications should know about it and can correct. So if you get unrecoverable corruption during a write then that bit of data will be corrupted but the rest of the data will be fine.  The data is first written to a log area of the file system and then when the io is complete it is committed to the rest of the disk do this makes it less likely to have io half completed.",1535133857.0
tobias3,"I don't know much about APFS, but here is what I know:

All modern file systems do not become damaged when power fails if the hardware works as intended.
If hardware corrupts parts of a file system, one needs redundancy to repair it in general. Simple, heavily used and older file systems (like ext4) have good repair tools which can recover some data without ext4 having metadata redundancy (e.g. move file data into lost+found), but that is not guaranteed to work.
In the last few versions Linux XFS got a online metadata scrub that repairs metadata using metadata redundancy (reverse-mapping B+tree, free space tree).
Then there are obviously cow-file systems with optional redundancy (RAID) which can repair both metadata and data (ZFS and btrfs).
APFS only copy-on-writes the metadata, and is probably a bit more resilient to hardware failures because of this, but I don't think it has a scrub like XFS, btrfs and ZFS. Apple obiously also has control over the used harware, so they can trust the hardware a bit more.

At least most Linux file systems are now-a-days heavily stess tested with corrupted file system images (fuzzing), such that most corruptions are handled correctly. This means corruptions don't crash the kernel, return localized EIO, or get remounted read-only.",1535137034.0
50shadesofbombay,And a RuntimeException breaks through the window and knock the kid out ,1535111641.0
brett_riverboat,"No log statement, so nobody knows why the 0330 is running late.",1535114520.0
teteban79,"It's just a try-catch doing nothing to actually solve the cause, the kid is actually a deformed half-burned almost-aborted hybrid fetus of a genetically diseased human and sadistic alien killer machine

&#x200B;",1535199747.0
trout_fucker,lol,1535081583.0
Verioc,"Oh jeez, I wonder why",1535081945.0
knot_hk,What does this have to do with computer science?,1535084072.0
Uranium-Sauce,"Hi guys, I need an api to get an answer to the universe, can't seem to find any. Help pliz?",1535087396.0
sjh919,Ez just write your own api for it,1535089914.0
CorrSurfer,"I think that chriscoskenobi deserves a proper answer.

Publicly accessible APIs are put online by individuals and organizations who have an interest in their APIs being used.

You are requesting access to a potentially large database, and the question is who would put them online:

- Either volunteers could track prices in stores and put them onto some website. This does not appear to be a project that is likely to be sustainable, so it's no surprise you didn't find any such API. Who has fun doing this? You can get some rough and potentially outdated data from ""cost of living"" websites, such as https://www.numbeo.com/cost-of-living/in/Toronto -- But note that the operators of these websites typically have no interesting in allowing you to scrape their page.

- Possibility 2 could be the supermarkets exposing their price data. But why would they do this? It costs money (Servers) and allows the competition to automatically match prices, which they have no interest in. So you are unlikely to find such a API even for a single super market, let alone all of them.

",1535108089.0
TKAAZ,Wroooong sub!,1535096016.0
mcdowellag,"The 1980s bred a weird and wonderful collection of would-be supercomputers, including some machines with large numbers of simple processors at relatively slow clock speeds. Normalized to 1GHz they might do surprisingly well - see https://en.wikipedia.org/wiki/Goodyear_MPP and https://en.wikipedia.org/wiki/ICL_Distributed_Array_Processor. If you are really asking about some sort of measure of architectural efficiency, you could look at the array processors of that era. See e.g. https://en.wikipedia.org/wiki/FPS_AP-120B and its exotic relatives such as https://en.wikipedia.org/wiki/CDC_Cyber#Cyberplus_or_Advanced_Flexible_Processor_(AFP). The problem with these is that the compiler technology of the time could access only a fraction of the available power. Lessons from these machines, together with better compiler technology, was sure to transform the world of computing - https://en.wikipedia.org/wiki/Itanium

Thinking about this, another way to cheat is to take and old processor where the difference between cpu and memory speed is not so huge as it is today - perhaps even http://www-03.ibm.com/ibm/history/exhibits/mainframe/mainframe_PP2091.html just as an example - and claim that you can scale the memory speed up to 1GHz with the cpu. You can probably find workloads on which the relatively fast memory gives it an advantage.",1535088394.0
ejaculat0r,"Define 'processing power'. The processor in your computer (x86) is very generalized, it does everything okayish. And that's good, you want your computer to do multiple things and you want them all to perform well enough. That being said, you can make processors optimized for one thing, and presumably it'll be able to do that one thing possibly multiple orders of magnitude faster than the processor in your computer. Tell me, which one has better 'processing power'? 

Processors/architectures are apples and oranges and cannot be compared like this.",1535086499.0
bartturner,"Can't measure in clock cycles.    What is done in each of those cycles?  

Today with instructions pipelines and speculative execution it all comes down to cache hits for speed.  Memory access times are the problem.   Not executing instructions.

So here for example is an interesting approach to that problem.

https://www.anandtech.com/show/13241/hot-chips-2018-the-google-pixel-visual-core-live-blog",1535112830.0
neptoess,Coffee Lake,1535085162.0
Wulfnodh,"[Best CPU 2018](https://www.techradar.com/news/best-processors)  is quoting the Ryzen Threadripper as being best for performance at the moment for micro architecture.

There is another category of processor architecture, namely instruction set architecture (ISA). IMO it is unfortunate that we ended up being stuck with x86 as the dominant ISA for personal computers because of an early decision IBM made based on cost.",1535088953.0
olliej,"it’s actually super easy as a concept if you start out by ignoring permissions and processes etc

Let’s start out with the basic no-VM scenario. In this case an address/pointer is literally just an index into the physical memory.

When you tell the cpu to load memory at an address /x/ it is literally sending that number over a set of wire to your ram. The ram responds with the data at index /x/.

Now when you have virtual memory, each address in your program is now virtual, and does not necessarily have the same physical address. To do this our exceedingly dumb v1 cpu has a map that says for each virtual address, here is the physical address.

Let’s imagine that our dumb v1 cpu says that physical memory is in reverse order from virtual memory. Say we have 4 bytes of memory, the cpu has a table with 4 entries:

  0->3
  1->2
  2->1
  3->0

Now when our program says load an address /x/, the the cpu finds the entry for /x/, and then asks the physical memory for the data at the real location. Eg your program says load address 0x0, the cpu looks that up and find that the entry for 0x0 says to use location 0x3, and so asks the physical memory for the data at location 3.

Importantly the cpu doesn’t care about the ordering of the map, it just a looks up the virtual address in its table and then uses what ever the table says.

This means you could do:

  0->2
  1->3
  2->1
  3->0

And it will still just work, except now when your program asks for location 0x0 the cpu asks the memory for the data at 0x2.

So that’s the basic idea. Now this map has to go somewhere in ram because you want the OS to be able to change it which gets us to the reason for “page tables”: our current map has one entry for each address, that means you need as much memory for the map as you have ram in the first place. That is clearly insane.

The solution is to stop mapping individual bytes, but instead map continuous regions of memory. We call those pages.  Let’s upfrade our dumbass machine to have 32 whole bytes of memory, and say the pages are 4 bytes. So now our machine has 8 pages, so we can have our dumb reverse memory map like this:

  0->7
  1->6
   ...
  7->0

Our map is now a page table! Importantly (though unfortunately for example continuity) only the pages are in reverse order, the bytes in each page are in increasing order.

Now say we want to load the address 23. We can’t just lookup 23 in the page table there are only 8 entries, and anyway the table is for pages, not bytes. So we have to find the virtual page for our virtual address. 

This is super easy! We know each page is 4 bytes, so we just have to divide by 4. If we do this we find that the virtual page for our address is page 5. So we look up the entry for page 5, and find that it says that we want physical page 2.

The only remaining step is working out the actual byte we want is. We divided by the page size to get the page, so we obviously want the remainder to know which byte we want. The remainder for 23/4 is 3. So now we know that the physical page we want is page 2 and we want the 3rd byte in that page. We turn all of this into a single physical address by doing 2*4+3. So now we now our physical address for 23 is 11, and that’s what we ended up loading.

So in our example we have a single level page table, but modern hardware (for a number of reasons) uses multiple levels in a tree, but you’re still basically doing the same thing for each level. The other things you know about like access permissions are just additional flags in the table, eg instead of address->address you have address->(address and permissions)


I hope my writing is remotely clear to you :)


",1535081275.0
Simonzicek,Try Crash Course Computer Science on YouTube.,1535071384.0
Andy_Reds,Operating Systems: 3 easy pieces is an excellent book that is available for free online. You might want to check it out.,1535107178.0
darpp32,Computer science needs to be renamed to abstract computer science.,1535079179.0
Beastrik,Why do I see an occasional face pic on this sub like it's tinder?,1535305040.0
linearmodality,"What are the ""normal"" programs for which this approach is faster than GHC? Are there benchmarks posted somewhere?",1535144675.0
Prof-,"Correct me if I am wrong, but OP's affiliate link is the one shared. If you buy the book after clicking the link OP gets commission (which is cool if they had disclosed that). Great book though!",1535067142.0
pa07950,"Great book and don’t forget the author also has most of the questions up on hackerrank.com

",1535061571.0
stefantalpalaru,off topic,1535067253.0
sheikhy_jake,"I assume the only time you need to convert a dictionary to json is to pass an object/dictionary to some JavaScript in a template or you have a json field in the database?  I've been working on some flask projects recently with a lot of JSON fields in the db and passing of objects to render on an HTML canvas with javascript and can't say I've found this aspect troublesome. 


What about your problem isn't solved by Json.dumps() and json.loads() ? ",1535170543.0
1Tikitorch,"What type of Skillset do you have?
I’m a Carpenter/Electrician & I Help with Habitat for Humanity.
The YMCA at certain times of the year they have Fun nights for the kids, Halloween is coming up & many of them have a party called Hallaballu, I used to volunteer as well as my kids to.
Soup Kitchens are another, what I did as well is to sit with Patients receiving Chemotherapy, talk & keep them company. 
Feel free to get back too me
👍🏼👍🏼🍀",1535075194.0
,https://www.hackersforcharity.org,1535046475.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1535036245.0
vidro3,"love these, especially the Anki cards. 

",1535046663.0
ricanontherun,I just took a job working with systems at a scale I am not used to. This project saved me when I first found it,1535057989.0
cntrlnmbr1,"Sorry for noob question. How can I learn from these? I am a computer science student with basic understanding of programming languages like Java, c++, and python. I look up the codes and always see some files that I don't know how to use. Any tips on what to look for in these kind of stuff. Thanks :)",1535071108.0
foreheadteeth,"I'm a math prof now but a looong time ago, I did a double major in Math+CS and it was nice to do two things. In my case I ended up enjoying the math a lot more. :)

I'm not an economist but I've published a couple of papers in econometrics and in retrospect I wish I had learned more about economics when I was a young man. I don't think it will ever be useful to you as a computer scientist but there's a lot of super fun jobs outside CS that you can do with economics, e.g. World Bank etc...

The only problem with economics from my point of view is that the math they teach is too weak. The professors themselves can be quite strong mathematicians but the students don't have the chops for it so you end up with a ""thin gruel"" version. The ""crazy good version"" lies underneath it but you won't get it as an undergrad.",1535019594.0
PrivilegedVoyager,"Good idea. Cs skills are vital today in finance and firms are hiring coders to trade stocks through auto mated systems.

With knowledge of basic econ, math that you learn in you CS major and coding chops, you might be looking ahead at a career in algorithmic trading too.

No need to mention the high paying, early retirement kind of profile it offers.
",1535020497.0
Niaso,There is a lot of crossover between IT and Finance departments in business. I don’t believe economics would be as useful as Accounting unless you’re working in a specialized field building economic models. A programmer that understands accounting is worth a lot of money.,1535023480.0
Weed-,"The problem with most Economic departments is they teach an ideology, and not actually how our macro-economy works. They still teach that money evolves from barter, an idea that has been completely debunked by historians. As well, they teach that the US taxes and borrows in order to generate revenue to spend, which is not accurate for a monetarily sovereign State.",1535028676.0
PinkyWrinkle,"Since you go to UCSC, you should know that Dunne or Miller or whoever is teaching it 100% considers this cheating and will fail you for it. 

Further, this is an easy assignment. If you can’t do it, you for sure can’t do the following ones. ",1535061707.0
khedoros,"I remember my assignments like that. We had to use an ARM simulator written by the course's professor. It wasn't pleasant, but it wasn't horrible. Just a little user-unfriendly.

If I remember correctly, what I ended up doing was writing the program in C, and then convert it into the subset of ARM that we were allowed to use. MIPS is a somewhat-similar load/store architecture, and the same kind of approach should work well.",1535008135.0
SteeleDynamics,Was going to say pretty much all of it.,1534988319.0
ItsAllOneGhettoMan,"Linear algebra is just about taking vectors to other vectors (via matrices). Lots of kinds of information are just rows of numbers, which are vectors. So to learn how that information transforms, you need linear algebra. 
Watch 3blue1brown’s series on neural networks. ",1534990665.0
rockinghigh,"Just to name a few: matrix multiplication, inverse, and transpose for regression or simple optimization problems. Eigendecomposition and singular value decomposition for recommendation systems (see SVD++). To be honest, if it’s your first class, you’re likely to use everything you learn.   ",1534993925.0
its_ya_boi_dazed,Which part of med school is used in surgery? - Op,1534996558.0
eternusvia,"Because machine learning is so broad, it is likely that many many many aspects of linear algebra are applicable. It would probably be more difficult to find a linear algebra topic that is not related at all to some portion of machine learning.",1534987223.0
NicolasGuacamole,All,1534989271.0
abadams,Try to get a good understanding of what linear transformations do to data sets. E.g. how do they change the covariance? How can we understand their effect via decomposition into eigenspaces? Can we use them to make the measurements in the data set less or more correlated? ,1534996830.0
acousticpants,ALL OF THEM. ALL THE PARTS.,1535022690.0
Peter-Campora,"I can't help but feel the reductionist responses to OP's question that essentially say ""all of it"" are misguided. Linear algebra is a broad field, and I'm sure there are some people that use things like how module and ring theory relate to vector spaces and fields in their research, but I can't imagine it is common. 

Consequently, I'd say you don't need to learn these topics to do machine learning. So if you use machine learning, please just say the math you commonly use and don't give a snooty response.",1535036254.0
alreadyheard,"You’ll find linear projection in algorithms like the perceptron, and eigenvectors/eigenvalues in dimensionality reduction algorithms. Those two initially come to mind but as others have mentioned pretty much all of it. ",1534992229.0
possiblyquestionable,"If you're interested in the computational side, gaining more depth in the various decompositions, perturbation analysis, subspace methods, and different iterative methods is very useful. Additionally, there are a lot of elegant reductions to linear systems and invariant subspace reductions that are good for general algorithmic knowledge.

When I took computational linear algebra, there were a ton of focus on priming your system by preconditioning, regularization, and other tricks to improve conditioning. My understanding is that these techniques have their neural network counterparts, with similar intuitions for developing regularizers.

Finally, linear algebra is a great segue into other aspects of algebra, particularly thanks to its illustrative nature. For example, computing Givens rotations and corresponding decompositions gives a great introduction to the power of canonicalizing your algebra into orthogonal (or some other special) bases and additionally gives you exposure on how to compute these bases as well.",1534998181.0
shaggorama,"I think the biggest topics are:

* matrix multiplication
* norms
* decompositions (esp eigen, SVD, NMF, QR)
* vector calculus with an emphasis on optimization

Additionally, graph theory makes heavy use of linear algebra black magic.",1535016746.0
NytronX,Here's a good resource. This guy writes textbooks and has MIT OCW lectures on the subject: [http://www-math.mit.edu/\~gs/](http://www-math.mit.edu/~gs/),1535022319.0
Invoke_Gaming,"Dimension flattening and working with numpy arrays. Honestly, a general knowledge of Linear Algebra helps to grasp a significant amount of machine learning concepts. ",1535033754.0
GNULinuxProgrammer,You should breathe and live linear algebra. Your blood should be composed of red blood cells swimming in a sea of linear algebra. Having sex with your SO? Too bad you cannot finish because your mind is occupied thinking LA.,1535041974.0
fluffycatsinabox,"Would it be more precise to say that machine learning deals more with ""matrix"" algebra than linear algebra? If we're being pedantic, linear algebra deals with the study of linear transformations on vector spaces, and matrices are one type of object that can belong to a vector space. Even when we do stuff like calculating eigenvalues and determinants, we're still mostly doing them on matrices, right?

&#x200B;

I don't know enough about machine learning to know whether or not the broader class of linear algebra is used in ML. ",1535076303.0
WhackAMoleE,"I hear there's a lot of matrix multiplication. There's a whole discipline of computational linear algebra in which they try to multiply matrices fast. Computer graphics is all about that. Big chunks of ML I hear too, though I'm not a specialist in ML.",1534995813.0
jazimov,"All of it. You can use machine learning without knowing linear algebra but you won't really be able to understand the cutting edge or contribute anything new without a grasp of it.
Reading *""linear algebra and it's applications""* is highly recommended if you want to go beyond the surface level.

But please don't let this put you off, you don't have to be a genius or all that smart to be a good ML researcher/practitioner. Like anything else worth doing, it requires a passion for what you do and the willingness to apply yourself.
Go at your own pace. I'm so tired of insecure people showing off how much they know. Even if your contribution is modest it's worthwhile. Don't be afraid to get things wrong or ask questions! ",1535015024.0
barsoap,"There's not much to linear algebra: It's all about composition of linear transformations, that is, composition of a specific type of functions. Machine Learning algorithms then use those types of functions in specific ways instead of general ones because things built out of linear transformations have properties that make them really nice to optimise.

All that matrix stuff is just mathematical assembly language covering a large, but not complete, section of possible linear transformations. Unless you need it to understand the actual algebra of things it's safe to ignore it and just use BLAS. Unless, of course, you want to re-implement BLAS, which you probably won't. Not unless you're planning on making that one specific thing your academic career, that is.",1535023691.0
andimmike15,"Focus on the notation. Try to understand it. Many underestimate the power of notation, but by just looking at the formula, you can grasp the main idea of the topic.  
Then learn about regularization, vector, and matrices.",1535055865.0
Python4fun,"while I completely understand the substance of the article, I don't see the value in using a modulo of a negative number",1534963411.0
Bromskloss,No plots?,1534972627.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1534939504.0
j03,"How fun! I've added support for colour output using ANSI escape sequences: https://github.com/JosephRedfern/asciify/commit/95dc78be10a274f893da0971a6d2931a4fe83ba9.

If you'd like me to open a PR then I can -- but it has the caveat of requiring a terminal with Truecolor support. I was going to add this as an optional flag (using something like argparse), but didn't know if you wanted the additional complexity?

EDIT: also, this might be more suited to a subreddit like /r/python, perhaps.",1534933928.0
Pfaeff,Have you tried using something like gradient direction instead of intensity? I think you could get some awesome results with that.,1534960129.0
,You should read clean code.,1534970489.0
stefantalpalaru,You're in the wrong subreddit.,1534937572.0
frenris,Cool. I wonder if it could be extended to stream movies over ssh to an xterm. I think that would be even more practical. ,1535003817.0
vorlik,maybe there is some info about the method used at http://detexify.kirelabs.org/classify.html,1534874670.0
jet_heller,What's the difference?,1534877878.0
ketjapo,"As long as you threat them as black boxes, the same techniques apply.",1534879084.0
chrismamo1,"Very similar, only difference is the performance of your application code.",1534883448.0
gianm93,so I can talk about web server's performance,1534884755.0
gianm93,so I can use what I find for web servers?,1534887173.0
Woedica,No.,1534851129.0
frugal_warrior,"Nope, they don't expect you to know. I had zero experience myself when I started my first intro course. You'll be totally fine - learn as the course progresses.",1534851506.0
sniperr69,I had never experienced coding before i started the university. There wasn't any problem. But i recommend you to begin coding now. It will help you on your courses.,1534851602.0
chhuang,"Tldr : no

Idk about other unis. But from the one I graduated from, we were not required to have any knowledge for taking entry cs courses. 

That said, I took cs in highschool so it was a easier run. Tho I still learned something new and getting rid of bad coding habits. 

My opinion, I don't feel like I had a huge advantage besides knowing the basics. And some of those who started out fresh didn't necessary underperform at later stages.

",1534851703.0
Stenwalden,"Probably depends a lot on the uni and the country, but I knew nothing about programming when I started.",1534852379.0
CatchingZzzzs,It would help so youre not struggling too much with concepts but otherwise no. ,1534853437.0
,"No, at least not where I graduated. It helps a lot, but if you pay close attention it isn't going to hamper you.

""Programming 1"" was a required course in the first semester that assumed no programming experience and taught you c++ from the ground up. For people like me who did come into university knowing a programming language it was essentially a free credit. For others it helped establish the ground fundamental skills you'll need as you pick up other languages trough your studies. 

Most other courses will teach you the required parts of the language they use as they go on and need them, but once you know one language the others are easy to learn the basics of.

(Except assembly. Nothing can prepare you for assembly.)",1534853515.0
keksper,"No. It might actually better to go in without having written any code, as you won't have bad habits. Formal classes will (hopefully) teach you logic before code, which will help with producing readable, efficient programs.",1534853857.0
sailorcire,"No.

But it doesn't hurt to know some basic programming before hand in Java, C++, C#, Python, or even BASIC.",1534893686.0
nexvanshisha,"Usually not, but I would suggest you to look into some basics, because you might find yourself in a class with people with more experience and the professor then maybe explain the basic faster than you can catch them. 

I'm a third year student who didn't have any prior experience before the school, and I found myself in the above situation sometimes.

Some cool beginner's stuff you could check out could be:

- try writing some C,Java,JavaScript,Python or any language you might have heard of code, and try to print something on the terminal. (google: Hello World ""name of the language"").

- also try to understanding how variables and loops work which are common tools in programming

That might help you starting with an edge.",1536004398.0
noam_compsci,"I think most courses make it clear that you do not need, but strongly recommend it. 

I'd recommend doing a MOOC or free course like Codeacademy. Just make sure you do the right language. 

If you are asking this question, I think you know  you need to :p ",1534851160.0
havokang,Linux is wat I use for school. I use it on a $300 laptop I bought four years ago. Still runs well ,1534848169.0
igglyplop,"Ubuntu is to Linux what Coca Cola is to soda. Just a specific product in the genre.

I started school (CS degree) with a Surface Pro running Windows, and it was fine (didn't like the flexible keyboard though). I then upgraded to the surface book just because I wanted more power (again with Windows) and it was still fine as far as school was concerned.

HOWEVER I found that setting up development environments in Windows is very touchy. I tried Ubuntu as well, and I've found developing on Ubuntu is much easier than Windows. I prefer Ubuntu over other flavors because I'm used to it, but basically any flavor of Linux will work fine ",1534848908.0
Quexth,"I have experience with Windows 10 and some Linux distros so I can't speak for MacOS. From what I have seen from my friends using Mac, it does not give a larger control over your system than Linux and for some software we used they had to install Windows separately anyways because the software didn't support Mac. You should speak with people from higher grades to see if they had similar experiences.

My experience so far is that it is easier to install and manage most proprietary software on Windows. So if you are going to deal with software products a lot Windows will be sufficient for you and you may get a free copy with Dreamspark if your school partakes in it. However, if you are going to delve into the lower levels of CS (I made the switch because I had some very interesting problems trying to use the C++ compiler from Windows) I find it easier to use Linux because most people who work on those subjects also use Linux so support is better with it.

Overall if you are going to use IDEs and stick to popular languages go with Windows but if you wish to go deeper into the background of things instead of learning just how to do them I would advise you to use Linux.

So far I used Ubuntu and Fedora as Linux distros. I like Fedora more than Ubuntu and I don't think Fedora is harder to get into than Ubuntu. Only difference for a beginner is that Ubuntu has a larger community so you can find answers to your questions easier. Also, it may take a while to properly set up your Linux OS depending on your hardware (generally everything works out of the box but it may take some adjustments to make it the way you want it) but you can do it in an evening.

My final word is between Windows and Linux, you don't have to make a choice, you can do a dual-boot setup (which is easy to do) and use them both.",1534851629.0
Jaxan0,"I did my studies with a dual boot Windows / Ubuntu laptop. I mostly used Ubuntu. It is great for programming (similar to MacOS) and writing.

There have been only a few occasions where we used proprietary software which only ran on Windows. But even in those cases, WINE could emulate those apps. ",1534857034.0
keksper,"People will give their gospel regarding their preferred OS but the reality is you can use any OS you want. Linux or Unix (MacOS) may be easier for some tasks, but Windows isn't nearly as bad as it used to be. I prefer Linux but my home computer is Windows since it's a gaming rig, and I just use the git for Windows tool set in Powershell and use VS Code as my IDE.

CS is about figuring out hard problems-- dealing with OS quirks should be one of the easy ones ;)",1534853506.0
stvaccount,Build your own.,1534876290.0
zumu,"Ubuntu is a [Linux distro](https://en.wikipedia.org/wiki/Linux_distribution). 

If you're new to Linux, Ubuntu is a good choice.",1534897362.0
ccundcf,Linux /end,1535155588.0
TopBeginning,"Computer won't make much of a difference as long as you have decent memory, ram, and processor. Worse case you install a virtual box for Ubuntu (coding locally) or AWS cloud9 (coding on the browser).",1534859732.0
mrexodia,Get a standard Windows 10 laptop and run Ubuntu for Windows to get the linux experience if you need it. You can always install a different OS if you feel like switching.,1534895534.0
cpplinuxdude,"Go with a good MBP. You'll be sorted for at least the next decade, if not much more.",1534847968.0
Dodobirdlord,"Can't really go wrong with Ubuntu (which is a kind of Linux, FYI). For the most part unless you end up doing iOS or .NET development Linux will be a large part of the rest of your life. Might as well get good at it now. 

If you can get over your issue with the keyboard then OSX is an equally good substitute for a Linux machine given that it's Unix based. You'll pick up all of the same knowledge about permissions and file systems and shells and commands from OSX that you would from a Linux variant with the added feature that there's a lot of out-of-the-box support for functionality that you might not have the experience to set up for yourself on Linux. The existence of iTerm alone is almost reason enough to opt for a mac.",1534868210.0
greenwizardneedsfood,"I would go Mac 2015 and before. To me those are the pinnacle of laptops and are much cheaper now that a few years have passed. MacOS tends to crop up a lot, so being familiar with it is a good idea, plus the ideas translate into other common operating systems (like Ubuntu)

Edit: Why all the hatred towards macs? They’re just straight up good computers",1534863133.0
net_nomad,"You should get a windows 10 machine, so that you can have the strictly office products (word, excel, powerpoint) as not all professors will accept the open versions or web versions

You should get a virtual machine up via VirtualBox or VMWare, which has a version of linux on it (get multiple if you can: rpm-based, apt, yum, etc), since the package manager makes big changes to how you use the system.

",1534870124.0
RexPowerColt69,"Unless you can get Linux factory installed on laptop, go with Windows or Mac.  Reason being, Linux still doesn't have great support for laptop hardware and you may run into problems getting it to work properly.

Additionally, you can create a Linux virtual machine on your laptop, and most schools also have remote Linux access for cs students to do their development in addition to computer labs.

Personally, I used Windows based laptops in school with Linux virtual machines if my work had to be done in that environment.  I tried getting a pure Linux laptop working, but it would always end up being bricked requiring a full wipe.  This is as of a year ago.",1534863507.0
Gnockhia,Not sure you should be doing a CS degree if you think a standard IO device has any relation to an OS.,1534851729.0
PronayPaulus,"Oh my god, that indlish gave me cancer.",1534847279.0
flexibeast,That i've never heard of it.,1534841616.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1534827881.0
boilerup800,"OS would certainly be better for your computer science skills imho. Job scheduling, parallelism, and memory layout will all be really relevant if you’re trying to get the most out of the algorithms you design. HTTP is better learned outside of school.",1534822197.0
Segfault_Inside,"I'd go with the OS class, tbh. Understanding the basics of how the OS you're using works is a pretty useful skill in most parts of software engineering. Networking, in my opinion, can be learned on the fly a lot easier when you need it.",1534822495.0
arxlight,"I've taken undergraduate and graduate OS, but not networks. I found OS super-interesting both times, so I'm biased. My assignments in OS are mostly emulating different aspects of the OS.

I've heard from others that networks is not too difficult and a lot of the content is about protocols (anecdotally). I agree that OS will likely be more intense. I think OS will sharpen your skills more.

Putting aside the practical skills to be gained from each, I feel that OS may give you more inspiration content-wise. I don't have any idea of your thesis, so take this with salt. Operating systems may have more in common with a brain model at a functional level, than a network server. More concretely, the models of memory management, process management, and input/output may be more relevant (you can judge that yourself). The idea of ""virtualizing"" a resource may also be more generally useful.

I am having trouble thinking of a downside to OS. Sorry. Maybe someone who has taken networks can shed light on some of their concepts, and intensity.",1534823146.0
magnificentbop,"Take the OS class.  Networks had a few important concepts, but most of it was trivia.  A lot of the magic of computers was cleared up for me by OS, compilers, and computer architecture.  I highly recommend all three if you get the chance.",1534846695.0
aleksir,Totally agree that OS is the way to go to build programming skills and a massive understanding of base compsci algorithms.  OS was a tough but massively fun course during my MS.,1534824170.0
pirate_starbridge,100% OS.,1534834212.0
spinwizard69,"Go OS!    Why?

Well pretty simple, if you actually follow through with biomedical engineering you could end up easily involved in custom operating systems.    Or to put it another way, biomedical engineering could easily mean embedded programming of sensors, pacemakers, bionics and similar technologies.

Beyond that many engineers are not real familiar with the operating system in their desktop.    Being able to leverage your given OS can lead to more effectively using that computer as an engineering tool.",1534866470.0
YourUndoing,"Having taken both, I'd say that academically OS might be more interesting, but I think there are quite a few practically useful things that you can learn from CN, especially in terms of how various handshake protocols work, how packets are sent, received, and verified, how routers and switches work and how queuing of packets work across distant networks, and things like headers and server responses among many other useful skills.  

if your interest is purely academic, you might find more juicy meat in the OS course. If you have any interest in actually doing network related work in the future, you might really find the CN course useful though. A lot of this also depends on how rigorous the classes are.",1534824289.0
very_sneaky,Por que no los dos? 🤷‍♂️,1534842755.0
Darkfeign,"Didn't you already post this, either in this sub or another sub?",1534890108.0
mraheem,"OS since I'm taking it semester
",1534903173.0
poshpotdllr,"OS course of course. networking is easier to learn on your own from a couple books. 

>The Computer networks course looks pretty interesting and I would build an HTTP server in Java.

java is shit. the language is shit. the ecosystem is shit. the vendor is shit. the existing code base is shit. learn C, assembly, perl, rust, go, and stuff like that. java will never stop decreasing in relevancy. get out while you still can. ",1534879043.0
bargle0,"My personal preference is for the MBP. However, it really doesn’t matter.",1534819967.0
hebrewer13,"I had the same choice to make last year and it doesn’t matter that much but I went MacBook Pro. The major advantage of a MacBook Pro will be the ease of accessing the terminal. Additionally most of your classmates will have one too which will allow you to eliminate some environmental variables in group projects.

On a personal note, don’t buy a computer for school with gaming in mind. College is an amazing time to meet people and try new activities. Games will always be there but the social opportunities of college won’t. I uninstalled all games from my laptop after the second week of college and I never regretted it.

Good luck with school!

",1534820240.0
RandomHero492,"Most of the students and staff in my CS department use MBPs. They’re just high quality laptops. Huge plus for terminal commands because they’re the same as Linux terminal commands. Just earned my Master of Science in Computer Science and used my 2011 MBP for both undergrad and graduate school. 

Only time I needed something stronger was when I trained a machine learning model to determine if a given news article was truthful or not. But at that point, I had to rent out GPU time from Amazon Web Services. So no home laptop could have done the trick. 

Good luck and get your ass to class, no matter how tempted you are to sleep in! ",1534820668.0
ActiveSelf,"I own both the MBP and the SB2. Recent CS grad, 3 years in industry(DevOps/SRE/Sysadmin), have developed on MBPs at work for those three years. SB2 is a more recent acquisition, with \~3 months of use.  


Wanna graduate? get the MBP because nogamez.   


On a more serious note, development in both environments is apples to oranges. First off, MBP is a \_solid\_ piece of hardware (at least the 2015 model - haven't used the new ones) and using MacOS can be a wonderful and gratifying experience if you get sufficiently familiar with it (there's a learning curve for first time mac users). MacOS has traditionally been the easy choice because it makes the command line more friendly to you. Moreover, there'll probably be a better paper trail on stackoverflow for ""how do I get X to work on macos/Unix/Linux"" than Windows. Realize some software you can't use on a mac. But also be aware that some software runs better / smoother on a mac.  


However, the SB2 and modern win10 in general is pretty slick too. Nowadays you can get a full BASH/Linux command line (which you'll learn is very powerful) on Windows rather easily with WSL/Ubuntu (whereas you'd have to use VMs for that in the past). I'm developing a react app on Windows now - I must say Microsoft has been killing it lately with giving developers what they want/need. That said, it is still these days somewhat wonky and idiosyncratic (to Windows) getting a development environment ready on Win10. The touchscreen is pretty killer, too - I use it for drawing. The GPU is good enough to run shit like GTA5, Dark Souls 3, and similar games at very high settings  


TLDR? IMHO this one comes down to your personal preference. There is no clear winner, and whichever you end up getting, be ready to troubleshoot their respective annoyances - both of them have 'em.",1534822253.0
Einstein_Reborn,"To preface, sorry for the long post and if I reiterate some points already made, but here goes! First off, the Surface Book 2 is an amazing laptop. It comes with fairly decent Nvidia discrete graphics and an Intel CPU, and you're pretty much guaranteed to have the latest driver updates as the vendor is none other than Microsoft. Moreover, for Linux needs the Windows Subystem for Linux can let you install your choice of Debian, Ubuntu, or a few other well-maintained Linux distros and access it via the bash shell. The only strong caveat I'd have with the SB2 is that it's a relatively hefty construction, and it may not be the best to carry around as you're walking around a campus.

For this reason, I would definitely recommend getting a 13"" version regardless of whether you buy a Windows or Mac (Trust me, at first you'll definitely feel the pangs of not having a 15"" laptop, but I think you'll find the still significant screen real estate and the lesser price of a 13"" model to be really worth it in the end). 

Ultimately, I would advocate for a MBP since I find MacOS's visual and user experience in general to be more seamless. On top of that, having a native, Unix-based OS is a boon when you start programming and installing libraries. You can still back up your files with OneDrive (it's free on App Store) and get all the Office apps with a license. For those rarer instances in which you'd need to use a particular IDE or something like Microsoft Visio that's only available on Windows, you can still dual boot Windows through Boot Camp. If you go this route, preferably get a 512 GB SSD for the storage space, get 8 GB RAM and the lowest available Intel processor, both of which I find to be sufficient even for programming purposes, and just give that Windows Boot Camp partition a solid 100 or 120 Gigs of storage space so you don't have to worry about running out of space. 

With those specs and an Office and Windows license I believe you'll end up paying $1850 for the 2018 13"" MBP. To be sure, for a 13"" Surface Book 2 with roughly equivalent (minimal) specs and Office license you'll pay $1350, but if you search for and find the 13"" 2017 MBP with the above specs on Apple's Refurbished store, you can notch the Mac's price down to $1590, including the Office 365 subscription and Windows license. Although I have never bought a refurbished laptop from Apple and can't honestly attest to how good they are, this may be a good option as some MBP 2018 owners have experienced crackling audio issues, which apparently may not be solved [easily](https://www.pcmag.com/news/363048/2018-macbook-pro-owners-experiencing-crackling-audio). (If you can get a brand new 2017 13"" MBP, all the better).

Summary: Go for a 13"" laptop to minimize potential strain as you carry your machine around a campus. See if you can get a fresh, unused 2017 13"" MBP from Apple. If not, do some research to see how people's experiences with refurbished Apple products are and if desired get a refurbished version. If it turns out that is not a good option or you're just not comfortable with that, probably go for a 13"" Surface Book 2. No matter what route you go, I think you'll be pretty happy either way.",1534825512.0
madahdz,"i just graduataed this past June with the xps 15 9550. I got because I loved the new design at the time and I had a really old hp. i never really used it to it's full potential. i recommend get something cheap for notes and taking to class. if you really care about taking digital notes. i loved the iPad pro and the apple pencil using notability during my last year. If you really want a workhorse, a desktop is the way to go. I prefer PC over Mac but if you are already in the ecosystem, then get a mac the linux commands are lovely to use there and most of my class mates did have Mac's. I like the 15inch but due to price I would just get the 13 inch and a monitor for whenever you're at home. Best of luck in college!!!",1534823598.0
salvor_hardin1223,"If you’re going to be doing your programming assignments on it you’re going to want either the Mac for it’s Unix based system or to get the Surface and do a dual boot with Ubuntu or some other Linux flavor on there.  Unless you’re working on a .Net stack (which I doubt you’ll do in college unfortunately), then running Windows can cause some headaches for a lot of CS work you’ll do.  You can absolutely do everything you’ll need to on the Surface, but I think you’ll find most programming related tasks easier on the MacBook. There’s a reason that pretty much every company gives their engineers Macs instead of Windows machines. ",1534820146.0
knot_hk,"Why do you need something expensive? You aren't going to game on your laptop, the processor will never matter for any type of programming you will have to do. If any of your classes need heavy lifting you will get an account to ssh into your school's lab workstations. If you are planning on gaming, you should just get a desktop anyway (surface book and macbook will not cut it).

So why not get a used thinkpad (x220 or t420 comes to mind) for \~$100, and if you need a bigger computer down the line build a desktop. Plus thinkpads are great linux hosts for when you inevitable decide to switch.",1534822561.0
Bey0nDPhant0m,"I'm finishing undergrad and going to grad school, so take my comment with that in mind.

I think you should go with an HP or MSI gaming laptop. You're paying for the brand when it comes to Microsoft or Apple laptops. I had a Surface Pro 3 through most of my undergrad and it was , well, okay. I bought an MSI with a GTX 1060 for  less than what you would buy the MBP or the Surface Book and have a more powerful laptop.

Your 15"" Surface Book with i7, ""discrete NVIDIA"" graphics would be about [$2200](https://www.microsoft.com/en-us/store/config/surface-book-2/8MCPZJJCC98C?cid=surfacebook2interstitial&selectedColor=)

Your 15"" MBP with (I assume)i7 8750H, Radeon 555X would be at minimum [$2300](https://www.apple.com/shop/buy-mac/macbook-pro/15-inch-space-gray-2.2ghz-6-core-256gb#)

A 15"" MSI with GTX 1070, i7 8750H, 256GB SSD, 1TB HDD is [$2000](https://www.amazon.com/MSI-GE63-Raider-RGB-012-Performance/dp/B07BBCMKLW/ref=sr_1_3?s=electronics&ie=UTF8&qid=1534820133&sr=1-3&keywords=msi%2Blaptop%2B1060&th=1)

A 15"" HP Omen with a GTX 1060, i7 8750H, 16GB of RAM, 128GB SSD, 1TB HDD is [$1200](https://www.amazon.com/HP-i7-8750H-GeForce-Anti-Glare-15-dc0020nr/dp/B07CZCFGX8/ref=sr_1_3?s=electronics&ie=UTF8&qid=1534820300&sr=1-3&keywords=hp%2Bomen&th=1)

Look up speed comparisons, because at the end of the day, that's what matters, imo.",1534820788.0
Screye,"Dell XPS 15 / Think Pad X1 carbon > Matebook pro / SB2 > MBP

I like the XPS & X1Carbon for linux support. But, as a windows machine Matebook pro / SBP > everything else.",1534820304.0
modestbeachhouse,"If you're ever going to program iOS apps get a Mac. IMO I think Mac's are more convenient for CS because you of the bash command line out of the box. But I also don't really care about note taking on a screen, and I have a separate (old) gaming rig that works fine. ",1534820299.0
tempus-temporis,Get a Thinkpad! Save $ and have a fantastic laptop.,1534822209.0
grmdgs,MacBook has a longer real life battery. The MacBook Pro is probably overkill though. Samsung and Asus have some amazing laptops out. I run windows and do Linux development. It’s a non issue. ,1534822168.0
LearnerPermit,"I personally don't really like the surface book. The compact keyboard makes it difficult to program, at least for me. In my opinion they make good second laptops for use on the couch or bed, not necessarily a primary work station. Unless you're mostly writing essays.

I preferred the larger keyboards, in your dorm room a normal keyboard/mouse will be there. 

I know some people swear by one note for note taking and maybe that's you. My learning style and brain really need me to take notes on paper or print outs of the power point. 


As for gaming, i'd skip that as a primary focus, aside from u/hebrewer13 said, about 30% of college freshmen drop out at the end of the first year. Parties and other non-school activities are the primary reason. My advice would be to get an inexpensive refurb laptop that you won't mind getting high wear and tear as you toss it in a backpack full of other stuff. 


Also there are advantages to not having the coolest computer in the dorms. There's always some idiot who will have a giant flat screen tv, sound system, gaming console, etc. Their room will be the hangout room, then go back to your own room or the library for peace and study time. 


For example: https://www.dell.com/learn/us/en/22/campaigns/outlet?c=us&l=en&s=dfh

https://www.dellauction.com/
",1534822954.0
tylerbmx777,"**Which OS do YOU like?** I think this is understated here. Evaluate your ability to be productive and efficient on the computer and OS you choose. You will spend a TON of time multitasking and googling a thousand different things about your assignments (The time and frustration can add up if it feels like you're fighting your computer). For programming, at some point or another, you'll most likely be in a linux environment which can be accomplished either way. On that note, I like MBP for aspects like OS, battery, and build quality. 
",1534824807.0
ponycomplete,"If you're a CS major, you almost certainly want a UNIX machine rather than a Windows machine. And if you value your time, you should probably buy a Mac, since Macs are UNIX machines that *mostly* ""just work"". There are reasons why almost all serious computer scientists and software engineers I know have Macs.

That said, the current line of MacBook Pros is kinda overpriced and has notable design flaws (the keyboard on my 2017 MBP is way worse than the one my 2010 MacBook Air, and the TouchBar is worse than useless). If you're price-sensitive and can somehow wait a couple of months for the new MacBook Airs to come out, that *might* not be a bad idea. Honestly, my beloved, relatively slow 2010 MacBook Air is still OK for most work-related tasks...the main pinch points that got me to upgrade were screen real estate and disk space. Failing that, an older MBP (used or refurbished) might also work. ",1534826661.0
zumu,"Honestly, I'd just use your 2013 MacBook til it dies. Maybe upgrade the SSD if necessary.

I have a new MacBook Pro 15"" and I kind of hate it. I much prefer the pre-2016 versions.  ",1534828148.0
sir_sri,"I'm a CS prof, and we get this question a lot.  

It depends what your programme uses or tends to favour, and how much disk space you need and how much money you want to spend.  

Where I am we happen to use C# in first year, and we pretty much insist on visual studio on windows for it, because we have far too much trouble with trying to do what we want on Mac and keeping our TA's (who are about 99% windows or Linux) up to speed on even basic mac stuff is ridiculous.  On the other hand, the maths department is all macs, so our students who are windows users need to use labs for mac stuff in first year maths.  If you've ever watched a Windows user try to explain to a Mac user what a file is, what a file type is, how to find a file, how to edit that file in a text editor, and how to upload that file to our learning management system you'd understand.  

There's no right answer.  The best thing to do is not buy anything yet, take your old machine with you, and in your first or second week buy something based on what staff and more senior students tell you is most useful in your programme (and that might depend on which courses you take from whom).  You might be able to find the departments computer science club and see what they are using or what people struggle with. 

I'm in Canada, so pricing might be different wherever you live, but macbooks are much more expensive than a Surface (especially with the sale on Surface laptops) for the same hardware.  128 GB of disk space is probably not enough for anything, and the Surface is an expensive machine.  If I could prevent students from showing up with less than 512 GB of disk space I would.  That pushes a purchase into an expensive price bracket.  Between all of the various development environments we use, the various random shit all of the other departments have them use, general productivity, multiple OS partitions and whatever other random crap ends up accumulating on your computer over time and you could find yourself short even at 256. 

That said, it also depends when you plan to buy the next machine, in 2 years, or 4, or... neverish?  If you can buy something else in two years to meet needs as you discover them, that's better than trying to buy something today that will survive the whole 4 years.  But that costs more money potentially, and where you aren't sure what you'll want to do.  
",1534829427.0
NovDavid,buy the mac,1534838934.0
falicor,"A surface is too small, and a macbook is too much for what you get. I've seen many people sell their surface after a year and get something else 

Just get a Spectre or an XPS13, or a Lenovo thinkpad. 
",1534820782.0
paypaypayme,"When I was in engineering school they had a preference towards windows for many classes. A couple classes used microsoft software like visual studio as a requirement. Most of the time it was possible to get things working on mac as well, but the thing is the TAs and professors were used to setting things up on windows. So it was much easier to get troubleshooting help on windows if you couldn't set up your environment correctly. That being said, as a professional I prefer developing on a unix-like operating system by far, so either a macbook or a notebook with linux on it would be my preference. It also depends on how comfortable you are setting up software on your own, and what types of programs your classes will need.",1534823465.0
Messigood,"Sophomore in college. Get the surface. It is better equipped for a college student and MacOS doesn't play nice with a lot of cs programs. For example, you can't get visual studio enterprise on MacOS. Even better, maybe get a HP spectre x360 or a surface book. They are much easier to type on and still allow for that note taking on a tablet feel. ",1534819927.0
404-CodeNotFound,"Personally, I'm not going to be buying another MBP until they fix the keyboard, port selection and awful repair service that you're forced to deal with.

So with that said, i'd go with the Surface Book 2. It's a great 2-in-1, albeit pricey.

Regarding your concerns about CS and which operating system is better, **it honestly doesn't matter**. You can get everything on Windows that you could on OSX and vice-versa. You'll also likely be dual-booting (or running in a virtual environment) a Linux distro at some point in your degree, as it has a number of advantages over both Windows and OSX.",1534820069.0
Zulban,"You just need a durable laptop with a good CPU and lots of RAM. OS is whatever you're most comfortable with.

It is far, far easier programming on Linux than on Windows or Mac tho.",1534820688.0
YourUndoing,"That is a question that is best answered by ""It depends.""  

If you are going to be a consultant, it probably doesn't matter. When I worked for a consulting company, I probably used at least a half dozen different languages with varying frameworks. I would say one of C# or Java is probably a good one to have, and knowing scripting in bash is probably going to be useful if you are doing a lot of work in a Linux environment. Javascript is important if you are doing any web development, and an understanding of how HTML and CSS work goes hand in hand with that. But there are many people that do serious software development that never touch any of these. Of course you want marketable skills so it really helps to look at the job market and see where a lot of the need is. That can vary depending on where you live/where you want to live.  

Understanding development concepts and paradigms is probably more important than knowing specific languages. I have to teach myself new languages I'm unfamiliar with on a regular basis as new work projects come up. Have to learn Ruby for a hand off project. I'm not really nervous about it because a lot of the concepts in Ruby exist in other languages that I've worked with. When it comes down to it, once you've worked enough with various languages, learning new ones is more about finding the unique things in that language versus the concepts you are already familiar with in other languages.  

In school, you likely don't really have much of a choice, depending on the program. But if I was to offer a legit order, I would say, learn C or C++ first, then Java or C#, then Javascript, then from there it depends a lot on the variables like job market and what you find interesting. In the end, don't try to learn something just because you think it'll help get you a job. Chances are good that fringe languages/frameworks may come and go in terms of usefulness. Just ask the various developers that became experts in Silverlight development.",1534785630.0
bargle0,"It doesn't matter, as long as you can find your passion for programming in that language.

Maybe try Python first? It's very forgiving and widely used. However, I don't think there's anything pedagogically superior about it.",1534785999.0
tobysmith568,Whatever order your educational institution teaches them in,1534785183.0
Stanky_Nuggz,I'd say Spanish. Alot of Spanish speakers here in the states,1534785460.0
OrangeAndaHalf,"What kind of software development are you trying to do? That is going to heavily influence which languages you learn.

Also maybe I should mention its very hard to get a job without a degree, depending on the type of software development. Check out /r/cscareerquestions and look at posts from people who self taught and got jobs.",1534786010.0
PowerfulAlternative,"The language is secondary to a strong grasp of fundamentals such as data structures and algorithms.  Understanding the core concepts of object oriented programming, for example is more useful than learning just python, for example, because it allows you to more easily switch from python to OO perl, to java, or C++ (or even well-organized C, in some cases).  The same basic approach applies to all of these languages, so some of perl's (and C's) weirdness aside, all would be easier to understand and move between.",1534786113.0
_deepthought42,Always test files first. ,1534767823.0
fortyeightD,"I got stuck half way through. There was no button to continue after I said that we don't review test files.

I'm using the browser inside Reddit is Fun on a Samsung S7.",1534763641.0
jmd27612,NOT computer science. ,1534719479.0
xShadowProclamationx,"i used a 13” mac air while getting my cs degree. i had VMs for linux and windows. i now have a 15” MBP. would like to go back to a 13” MBP. 

pm me if interested in a trade
",1534719933.0
LeafGecko,"I live for split screens. Increases productivity. I would get the 15"" (as i have one now). Graphics card is also better than the 13"" for certain models (im abit out of touch with mac stuff atm)

Hope you make a choice thats happy for you.",1534734850.0
MotoDJC,"Great article, thanks! 😊",1534717753.0
umib0zu,Check out /r/physics. A guy is challenging the results and it's actually extremely clear the data is fabricated.,1534631511.0
SamStringTheory,"Wrong sub? This has nothing to do with computer science (or even quantum computing, as your title implies).",1534631763.0
Bromskloss,"Typically, yes. In principle, though, that depends on how the programming language in question works.",1534604018.0
krimin_killr21,Yes. You'd probably be interested in learning about value vs reference semantics. In every language I'm aware of assignment simply copies the value of the variable at that time.,1534604021.0
A_Crazed_Hobo,"The question usually is, ""what is y = x doing?""

If you can say it's making y point to x, then y will change as x does. 

Or you can say that y is pointing to the value of the variable at x, which would assign it 5 and not change with x.


x = 5
y = x
x = 6
",1534605474.0
homomomorphism,"Correct me if I'm wrong, but if you do this in python with lists then y also changes with x?

Edit: Just did an experiment
x = [1, 2]
y = x
x = [1, 2, 3]
Doesnt change y, while

x.append(3) as the second line does change y",1534605257.0
drvd,"That _only_ depends on the semantics of =.

It could also mean that 5 equals 6. Which is nice as it makes proves quite simple.",1534746447.0
Lanfranc_di_Cambria,"if x = 5 and y = x, then the value of y is 5. 
",1534989489.0
Dramattick1,"depends on a deep.copy vs a shallow copy, and how the language you're coding handles these references. ",1534604428.0
GayMakeAndModel,"In general, primitive data types (int, long, float, bool, etc.) are allocated on the stack as value types which means that a copy of 5 was assigned to Y and that Y doesn’t point to the memory location of X.  Google “pointers” and read about them but don’t get sucked into C/C++ pointer hell.

Edit: as mentioned by another poster, Y = X is not a declarative statement that Y is always equal to X.  It’s assigning the value of X to Y. There is a disconnect between Algebra and what code looks like it is saying.  I almost want to say: think of each line of code as a command to the CPU and not an immutable statement of fact, but I would be flamed to oblivion for doing so.",1534607479.0
sailorcire,"Better question, why would it not be 5?

If this was dealing with pointers, then yeah, y would be 6, but as written, why would one think it is not 5?",1534614937.0
,[deleted],1534548239.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/python] [Bresenham’s Circle Drawing Algorithm](https://www.reddit.com/r/Python/comments/986qq8/bresenhams_circle_drawing_algorithm/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1534545886.0
khedoros,I believe that [this section](https://en.wikipedia.org/wiki/Midpoint_circle_algorithm#Variant_with_integer-based_arithmetic) of the Wikipedia article covers the derivation of the algorithm's decision variable. I'd read the non-integer version first; it looks simpler.,1534578809.0
foreheadteeth,"I'm not sure if the thing you linked is 100% correct, but here's a rational explanation for how this algorithm works.

http://run.usc.edu/cs420-s15/recitations/circleRasterization.pdf",1534579575.0
eternusvia,How to Solve it by George Polya,1534531741.0
hardwaregeek,"The Design of Everyday Things. Great book on design and usability. Really helpful when designing interfaces, whether for users or for other programmers.",1534541872.0
VibrationalSage,"Gödel, Escher, Bach: An Eternal Golden Braid. 

This is a life changing book that opens your fucking mind, man. Pulitzer prize winning book. It takes a stab at consciousnesses and relates it to systems by breaking them down into the sum of their parts and does so much more.

It looks at AI, recursion, cognitive science, reduction of problems and even Meaning. It is dense but an absolutely fun and fantastic read.",1534539622.0
,"How to win friends and influence people.   
I believe that being capable of communicating your thoughts and needs properly as a software engineer is as essential as knowing how to type.",1534527009.0
Tushta,"[Surely You're Joking, Mr. Feynman](https://en.wikipedia.org/wiki/Surely_You're_Joking%2C_Mr._Feynman!)",1534533152.0
Sqeaky,The Mythical Man month. It's a series of essays the cover a bunch of successful and failed software projects. It can be summed up as agile and waterfall are both b******* every project is unique because every project is unique.,1534537004.0
kfarr3,"Checklist manifesto

How To Win Friends and Influence People

Thinking Fast and Slow

Deep Work

The Wealth of Nations

",1534530318.0
flipcoder,The Bible because in debugging and deadlines sometimes all you can do is pray,1534541809.0
YakumoYoukai,"The Edward Tufte books on information design.  His thesis is that the purpose of presenting information is to help tell a story, so your choices should be tailored to the specific story you're trying to tell.  Not only is it directly applicable to the data we deal with all the time in tech, and in UI design, but can be applied to other kinds of design choices (software, code, product, etc).",1534570218.0
delpee,“Thinking fast and slow” by Kahneman! Gives you some mental tools to detect false claims and unfounded facts. Also a really good read apart from programming.,1534542302.0
hal00m, How to Prove It: A Structured Approach  -[Daniel J. Velleman](https://www.goodreads.com/author/show/392007.Daniel_J_Velleman) ,1534570934.0
johnydoe666,"The Phoenix project. Had it on my night stand for 3 years until I finally started. When I finished it 12 hours later, I wished I had done that 3 years earlier",1534530221.0
caiocaio,"At the end of Little Schemer, there is a reading list of non-programming books he suggested before moving onto the next book. It was inspiring to me, so I'll reproduce it here:

1) Flatland  
2) Alice's Adventures in Wonderland and Through the Looking Glass  
3) Naive Set Theory by Paul Halmos  
4) Grooks by Piet Hein  
5) Goedel, Escher, and Bach  
6) Goedel's Proof by Ernest Nagel  
7) How to Solve It - Gryogy Polya  
8) To Mock a Mockingbird - Raymond Smullyan  
9) Introduction to Logic - Patrick Suppes",1534541265.0
motorsports,"Either Absalom, Absalom! or The Sound and the Fury by William Faulkner. While the content has nothing at all to do with programming, these books taught me some valuable skills that are crucial to programming. Both books are simple from a narrative standpoint but are both told in very convoluted, non-linear way (each one being quite different). In order to get through them you need patience. You need to give up the notion of immediately reaching the end of the story. Instead, you slow everything down and enjoy the process. Once you turn off that part of your brain that just wants to figure out the big picture you start noticing the details and intricacies of the process. It's only at this point of letting go that you start having ""a-ha"" moments and small breakthroughs. Just like in programming, if you can't immerse yourself in the minute details of the process and enjoy the twists and turns of the journey then you'll end up with a very shallow grasp of the concepts once you reach the end (if you ever do reach the end). These books taught me patience and an appreciation of style and technique as opposed to immediate gratification.",1534600807.0
cosmicroamingoctopus,"The Age of Spiritual Machines - it’s not a programming book, but the way it explains computing principles as a whole is better than any other book I’ve read. Great choice if you’re interested in neural nets, AI, or just generally the exponential progress of computational tech. It’s also quite funny in some places and has a great degree of readability. ",1534537899.0
ezrawork,"How Language Works

The Bone People

Decolonizing Dialectics

Oscuramente fuerte es la vida",1534549481.0
the-other-phil,"The Road Less Traveled, Timeless Edition: A New Psychology of Love, Traditional Values and Spiritual Growth https://www.amazon.com/dp/0743243153/ref=cm_sw_r_cp_api_yOYDBb781X0B6",1534536325.0
lryyy,Algorithms to live by ,1534575658.0
M51_8427-21u,"*Hard Boiled Wonderland and the End of the World* by Haruki Murakami, It helped me understand the importance of paperclip maximizers.",1534542818.0
sv0f,"Back in the heyday of patterns people would recommend the architect Christopher Alexander's works on design.

[The Timeless Way of Building](https://www.amazon.com/Timeless-Way-Building-Christopher-Alexander/dp/0195024028/ref=sr_1_1?ie=UTF8&qid=1534549469&sr=8-1&keywords=timeless+way+of+building) is pretty inspirational.

Or else Stephen Grabow's book-length interview/exposition [Christopher Alexander: The Search for a New Paradigm in Architecture](https://www.amazon.com/Christopher-Alexander-Search-Paradigm-Architecture/dp/0853621993/ref=sr_1_1?ie=UTF8&qid=1534549483&sr=8-1&keywords=grabow+alexander), whch is an even better place to start.

How these ideas got imported into computer science is an interesting story. I've always loved [Richard Gabriel's](https://www.amazon.com/Patterns-Software-Tales-Community-ebook/dp/B003TJ9FGE/ref=sr_1_1?ie=UTF8&qid=1534549498&sr=8-1&keywords=richard+gabriel+patterns) version.

Another excellent book on design more generally is Simon's [Sciences of the Artificial](https://www.amazon.com/Sciences-Artificial-Herbert-Simon/dp/0262691914/ref=sr_1_1?ie=UTF8&qid=1534549723&sr=8-1&keywords=sciences+of+the+artificial). This is especially true if you like cognitive science.",1534549665.0
Andrin6,Leadership and Self-Deception by the Arbinger Institute,1534565589.0
scotty3281,Steven Covey’s book The 7 Habits of Highly Effective People is a great read for anyone. ,1534549188.0
RunnyPlease,Enders Game,1534533648.0
tailoredbrownsuit,"Algorithmics is not strictly a programming book, but covers a lot of fundamental Computer Science ideas. ",1534583459.0
JustSomeBadAdvice,"1. The Design of Everyday Things.
2. The unwritten laws of engineering.
",1534583757.0
memoryspaceglitch,"- ”The signal and the noise” by Nate Silver;
- ”Chicago Manual of Style” (as reference for writing English); and
- ”Factfulness” by Hans Rosling",1534586331.0
Blue_Q,"Frege's ""Begriffsschrift"", which virtually introduced formal proofs in mathematics and was laying important foundations for CS. Also ""on computable numbers with an application to the entscheidungsproblem"" by Turing, it really nailed the low level description of algorithms mathematically. ",1534597932.0
murdho,[Mastery: The Keys to Success and Long-Term Fulfillment](https://www.amazon.com/Mastery-Keys-Success-Long-Term-Fulfillment/dp/0452267560) by George Leonard,1534601515.0
jmickraut,"[Smarter Faster Better](https://www.amazon.com/dp/0812983599/ref=cm_sw_r_cp_api_JMeEBb9PWPYWJ) 

The most challenging problems in software engineering are people problems. This book is an excellent breakdown of the things that make individuals and teams perform at their best, backed by examples and data. It spans various industries, providing guidance applicable in all fields.

The explanation of how teams of average performers can blow away teams of rockstars blew my mind. Mutual respect, empathy, and trust are the biggest success indicators, not IQ and sheer coding badassery.",1534611525.0
bytelord,"To engineer is human (Petroski)
The essence of chaos (Lorenz)
Metaphors we live by (Lakoff & Johnson)
The information: a history, a theory, a flood. (James Gleick)
Thinking in systems: a primer (Meadows)
And one of my favorites: The challenger launch decision (Diane Vaughan)",1535236085.0
Nopaste,"[The music of primes](https://en.m.wikipedia.org/wiki/The_Music_of_the_Primes)

Our professor, [Janos Korner](https://www.itsoc.org/portal_memberdata/jkorner), suggested it. Awesome human, awesome professor, awesome book.",1535386524.0
bart2019,"Technically it is about programming (in the FORTH programming language)... But not about coding. ""Thinking FORTH "" by Leo Brodie. It's about how you should think about writing code, design, (re)factoring, naming variables, commenting style, etc. in order to be able to write better code.

Also, [this cartoon](https://imgur.com/a/FyCO1w7) is burned into my mind.",1534597749.0
HRK_er,"Trust me, second every book being mentioned here and read the book “Design of Everyday Things” before u read anything else as a developer. Design is definitely beginning to take seats at the tables of many companies and itll only progress. More devs need to be woke! :) good luck bruddduh",1534603289.0
3d3d3_engaged,title says hardly anything this post is useless,1534523481.0
partyinplatypus,About 30 seconds,1534523464.0
Jakeob22,"Do you mean understand what it does? Or understand each part of the code? Which language are you looking at? There's a big difference between understanding print 'hello world' in python and understanding exactly what each keyword in java/c++ is supposed to do when you set up the main method. In either case, it should still be pretty straightforward..",1534532223.0
drvd,7.,1534747795.0
WhackAMoleE,By understand do you mean knowing every detail of how it works from parsing theory down to the electrical engineering of the chips to the underlying quantum physics of the universe that makes it all work? Nobody understands it. ,1534618999.0
knot_hk,"just wait until you get to fizzbuzz, godspeed",1534822729.0
Jaxan0,"You don’t understand it, you just get used to it ;-).",1534578616.0
YourUndoing,"Can you give more background on this course? Is it high school level, college, etc? Are the students expected to have any prerequisite knowledge in any specific areas like math, logic, etc? What is the ultimate objective of the course?",1534525892.0
istarian,"Other than being ""fun"", what's the objective of the course? If someone is just interested in doing something as opposed to theory, most stuff may be kinda blah.",1534522794.0
Meguli,SICP ,1534530482.0
theboywholovd,How to have sex with a computer,1534534192.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1534504202.0
DannoHung,"I figured it had something to do with structure sharing yesterday. I should have said something but I just didn’t feel confident enough given I was not familiar with Absal.

Do you think it is possible to leverage this interesting result to non-lambda encodings of data? Or to separate the savings associated with the lambda encodings from the rest of the graph structure when comparing cost?",1534511759.0
jfb1337,An article on lambda calculus is not somewhere I expected to find a panenkoek reference,1534512750.0
zergling_Lester,"So, how does it reflect on the viability of Absal?

If I understood it correctly, while the computational net representation allows for easily planning the best possible evaluation, now it turned out that finding the best possible computational net representation is a very much nontrivial problem.

As in, appending ""and then copy the result once for no reason"" to your function results in a randomly different representation that, for no easily understood reasons, retains constant size under repeated self-application, unlike the original.

Looks like someone tried to sweep the Actual Hard Problem under the rug and it came back with a vengeance, more incomprehensible than ever.",1534539021.0
noam_compsci,Is there a python equivalent for this? ,1534499446.0
cottonycloud,[Here is the best I could find. ](https://visual.ly/community/infographic/technology/mobile-broadband-what-can-you-get-your-gigabyte) ,1534494373.0
,"Maybe you could overlay a grid on some monospaced text, explaining that the computer treats each *letter* as the smallest possible unit that could exist.

Then, in a paint program,  show her a high-res image. Zoom in until the pixels are the same size as the grid you placed over the text. Explain that for images, one *pixel* is the smallest possible unit. It's sort of like the image equivalent of a letter. Have her note how many pixels there are.

Also, you could explain that English has only 26 letters, but there's a huge number of colors.

(This explanation doesn't include anything about compression, but one step at a time.)",1534493987.0
NowImAllSet,"Make an analogy to money, which most people have some kind of intuitive grasp on. Equate a single letter of text to one dollar.

So imagine you have to pay a dollar for every unit of text. A unit of text is a letter, which is the smallest thing we can break text into. If an essay is 500 words long, with each word averaging 5 letters, thats (500 x 5) x 1 = $2,500 to store or deliver that essay to someone. Definitely not insignificant, but let's move on... 

Next, explain that the smallest unit we can break a image to is called a pixel. A single unit on an image is about THREE dollars worth. And a typical image is a grid of these units, the most common being 1280 x 720. At 3 dollars per unit, this quickly adds up to (1280 x 720) x 3 = **2.75 million dollars**. Now that's serious money!

Note that I pulled the numbers for essay out of my ass, and not sure if 720p is the most common. Of course, encoding matters for text, bit depth and other stuff for images, etc. But I think this drives the point home, and you can extrapolate as needed. ",1534501959.0
ryani,"I think a simple counting method works best.

How many letters are there?  26?  Well, Unicode has basically all the letters in every language and it has ""only"" around a million possible ""letters"".

How many letters are there in a book?  Well, the entirety of Shakespeare is [around 800,000 words](https://www.opensourceshakespeare.org/views/plays/plays_numwords.php) and a word is, lets say, no more than 10 letters on average (to be extra conservative).

So to represent text at least as big as Shakespeare's combined works, you need about 8 million letters, and each of those could be one of a million possibilities.

How many colors are there?  For most images stored on computer the answer is ""around 16 million"" -- 256 shades of red, green, and blue.  256\*256\*256 = around 16 million.

It's pretty common for a smartphone camera these days to be around 8 megapixels -- 8 million pixels.

So a single selfie is around 16 times bigger than the entire works of Shakespeare.  (Actually a lot bigger, because Shakespeare doesn't use many Unicode letters whereas the selfie probably uses a big range of colors)",1534504950.0
Stavronius,"You'd probably benefit from an analogy or example/exercise of how much data is required to represent an image or video.

Computers cannot visualize things; they can only read your instructions and show you the result on a screen.  Create a list of bullet points for \*drawing a simple straight colored line\*, then have your mom try to recreate the line - she is the computer!

* The dimensions of the piece of paper
* The color of the piece of paper
* The position on the piece of paper to start at
* The angle to begin drawing
* The width of the line
* The distance to draw
* The color of the pencil
* How hard to press the pencil (keep it simple - soft, medium, hard)

To get more advanced, do it on graph paper and then describe each of those bullet points for each graph square the line is drawn in

For video: Multiply this by 30 times for each second that passes.

So you can use the space to store all these bullet points to draw a simple straight line, or you could use it to store your favorite book!",1534499090.0
oliverjohansson,"I would recalculate proportionally to kg, taking into account that 5kb is like a post card so 5g. 

You can also go volume, or just like this: Video or picture is as big as a TV set, modern jpg is like lcd, old fashion tif and bmp like old TVs. and so on. ",1534505635.0
dudewhoisnotfunny,"A character in text is like a dot on a picture 
A picture is a million dots 
A video is million pictures 

I'm just using a million to get the point across obviously.",1534506610.0
net_nomad,"Hello. I'm trying to find a computer science sub where all the newbie posts are deleted, and instead focuses on providing interesting information rather than continuous Q&A format.

Does such a place exist? If you need an example, /r/programming.",1534918842.0
sagaciux,"I think your problem as currently stated is too open ended. Since your goal is to build a specific mathematical model, you're going to need a precise definition of what you want to achieve. Right now, the phrase ""create the simplest model possible in which the evolution of the laws of nature arises from the natural selection of structures"" is too ambiguous for me to unpack: what laws of nature are you looking to express? What structures are you selecting from? How do you define the process of evolution/natural selection? How would you know if your model was simpler or more complex?

The problem may become more clear if it is separated into smaller parts. I think philosophy can be open-ended and contradictory, but a model needs precise definitions. At minimum, a model needs rules and an initial state. Before trying to figure out these things, I would want to know: what do I want my model to demonstrate? Given a particular state, what should the next state look like? If the model should be simple, then I would want to include only the most relevant behaviors and states. What information does my model at minimum to function, and how much of it?",1534437456.0
WeirdEidolon,"NEAT might check a lot of the boxes you're looking for (I haven't browsed through your link yet)

https://en.m.wikipedia.org/wiki/Neuroevolution_of_augmenting_topologies",1534433283.0
Rococoon,"When do you want to start working on it? I think it is super interesting and I would like to help you think about it, however I am super busy right now... I do think that I might be able to help though given my background. ",1534429201.0
pdxdabel,"I'd suggest taking a look at [Les Valiant's paper on Evolvability](https://dash.harvard.edu/bitstream/handle/1/2643031/Valiant_Evolvability.pdf?sequence=4) \-- it investigates questions relevant to your agenda about the relationship between computation and evolution, grounded in Valiant's early framework for understanding machine learning from a theoretical perspective, [PAC Learning](https://en.wikipedia.org/wiki/Probably_approximately_correct_learning).",1534464777.0
UnderTruth,Sounds like you should talk with /u/userdna46 -- see if you two can come to consensus.,1534469752.0
noam_compsci,Page not found on the kiwi link,1534429402.0
,[deleted],1534429474.0
GayMakeAndModel,"Divise a Universal Search algorithm that utilizes a select set of modern programming techniques. 

Then devise a Universal Search (US) algorithm for Universal Searches.

Edit: clarity and to add that bonus points are awarded for using a finite, partially-ordered set of Hermitian operators to move from time(0) to time time(N)",1534462193.0
,[deleted],1534492966.0
Meguli,Chaitin might have material that can inspire you. ,1534498525.0
quiteamess,https://link.springer.com/chapter/10.1007/978-3-642-28111-2_12,1534504490.0
zergling_Lester,"1. Much cleverer people tried that before, what makes you think that you can do better? Ignorance.

2. Go pirate and read https://en.wikipedia.org/wiki/Gödel,_Escher,_Bach, this will get you up to speed with the 1970s state of the art of that stuff and make you realize how much you don't know in the process. Also, it's so damn enjoyable, to be honest with you fam. Anyways, it'd provide a perfect starting point into more serious inquiries.",1534539290.0
kiwi0fruit,"**(Update 3)**

I started to think that this my idea of self-justification can be split into (1) something incomprehensible OR like provability (that seem useless untill analyzing the complete model). So it's better to throw it out and go with fallibilism and falsificationism. (2) something about not using global laws but instead having algorithms inside individuals that change environment. It's not really clear too but... This second part is only a hypothesis under (1) govern: the main point if the model works open-ended. Then if our universe is possible in the model, etc... (3) Using simplicity and Occam's razor a lot.

What I first and foremost want to create is a framework within which every ""why?""
 (""because of what?"") question has an answer stating how historically (via natural selection) it came to existence.

But no matter how I think about it I cannot image any justification for ontology postulates except ""to make it simpler"" or ""as much entities as possible should have a history how they appeared - instead of postulating them directly"". This seems interesting but it's not what I expected - I wanted a better justification.

Man, this article needs a concise rewrite...",1538071397.0
criticalcontext,So you want to make a theory of everything. Good luck...,1534482389.0
kiwi0fruit,Comments from other sources:,1534754412.0
kiwi0fruit,"> [The Origins of Order: Self-Organization and Selection in Evolution](https://www.amazon.com/Origins-Order-Self-Organization-Selection-Evolution/dp/0195079515).  

> I'm not aware of too many articles, but you could try [this one](https://www.sccs.swarthmore.edu/users/08/bblonder/phys120/docs/kauffman.pdf) co-authored by
Kaufmann a few years before the book was published.

(u/[deleted])",1537510484.0
kiwi0fruit,">Hello, the title of this post caught my eye. I knew it would be some sort of ultimate question
>about everything.

>I love how ambitious you are - trying to define the problem and solve it in one post on
>reddit). Perhaps you’re underestimating the complexity of both processes.

>This topic, or problem, you’re talking about is so complex that it’s extremely hard to define it
>in words that would describe its true nature. It’s interesting how people can still understand
>what you’re talking about. It seems to me you’re looking for a not just a theory, but a
>mathematical model of everything. It is important to note that this question deals with
>consciousness because the nature of universe consists of objective nature (quantitative)
>and subjective nature (qualitative).

>Let’s start by defining the problem correctly...

>Some people who have commented claim that your original question is too ambiguous to
>be the definition of the problem being solved by a computational model. You mention
>“evolution of laws of nature” and “natural selection of structure” and it doesn’t seem clear to
>me what exactly you’re talking about.

>You seem to be trying to define current state of the world, universe, or everything. With this
>information you could predict how it originated and how exactly it will change in the future.
>This is the “simple” model you’re looking for.

>I think a better way of phrasing this problem is to be less ambiguous and more precise with
>what you’re talking about. If you want a simple answer, ask a simple question.

>How - exactly - does everything operate, based on the current state of everything?

>Despite the lack of specificity, would you agree that this a more well-defined problem? To
>me, using the word “everything” is easier and more useful than trying to define everything
>because we all can agree on what we’re referring to when we say “everything”: the universe.
>The universe is an example of a complex system. A complex system is any system featuring
>a large number of interacting components (agents, processes, etc.) whose aggregate
>activity is nonlinear (not derivable from the summations of the activity of individual
>components) and typically exhibits hierarchical self-organization under selective pressures
>Saying the laws of nature and structure leads one to think these systems are separate when
>they are in fact both part of one complex system we can refer to as “everything”.
>Understanding exactly what everything means requires an unimaginable amount of power.
>Everything includes every single thing in existence and everything at once - everything that
>has ever existed and everything that will exist. Everything is an objective thing with
>quantifiable features like the laws of physics that is only observed through subjective things
>like human beings and other biological organisms. It is important to note that the definition
>of “everything” is different from person to person, however everyone can agree that the
>word makes sense to represent everything in their world (or perception).

>So, you might ask, if everything is so hard to define, what would be computed to predict the
>future?

>Well, some things just don’t need to be defined by all of its physical attributes to be used for
>some purpose. Usually, complex systems are defined by emergent properties that come
>about because of interactions among the parts. A classic traffic roundabout is a good
>example, with cars moving in and out with such effective organization. How can people
>predict the flow of traffic to drive safely to their destination? This seems obvious if you have
>experience driving on a populated roadway. These drivers don’t know everything about this
>roundabout (how it was built, the names of the drivers in the other cars), but they know
>how they function. This only requires part of an understanding of a roundabout. Another
>example the phenomenon of life as studied in biology - it is an emergent property of
>chemistry, and psychological phenomena emerge from the neurobiological phenomena of
>living things.

>From Wikipedia, “Emergence Theory” - Whenever there is a multitude of individuals
>interacting, an order emerges from disorder; a pattern, a decision, a structure, or a change
>in direction occurs.

>(I’m only quoting Wikipedia because it’s an example of an emergent property of human
>communication and organization.)

>I think you would be interested in researching complexity theory as well as computational
>complexity theory.

>“Complexity theory is the study of complex and chaotic systems and how order, pattern,
>and structure can arise from them.”

>“Computational complexity theory is a branch of the theory of computation in theoretical
>computer science that focuses on classifying computational problems according to their
>inherent difficulty, and relating the resulting complexity classes to each other.[1] A
>computational problem is understood to be a task that is in principle amenable to being
>solved by mechanical application of mathematical steps, such as an algorithm, which is
>equivalent to stating that the problem may be solved by a computer.

>A problem is regarded as inherently difficult if its solution requires significant resources,
>whatever the algorithm used.”

>Something I’ve derived from studying complexity theory: An interesting relationship
>between objective nature and subjective organisms is that as the environment becomes
>increasingly complex, so does the organism.

>Also, research the hard problem of consciousness.

>:)

(u/[Deleted])
",1537510774.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/digitalphilosophy] [Introduction complete rewrite: The simplest artificial life model with open-ended evolution as a possible model of the universe](https://www.reddit.com/r/DigitalPhilosophy/comments/9k6n4x/introduction_complete_rewrite_the_simplest/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1538317039.0
SnowceanJay,"This is a really interesting problem!

Regarding the compsci side of this project, the most obvious things to look into are, imho: evolutionary algorithms (of course), multi-agent systems, emergence and self-organization.",1534429372.0
kdotddot,FreeCodeCamp and The Odin Project are pretty good for me.,1534379599.0
combinatorylogic,https://mitpress.mit.edu/sites/default/files/sicp/index.html,1534413638.0
kicked-off-facebook,Udemy,1534415073.0
Lucretia9,What language? YouTube probably has something.,1534386937.0
DezzaJay,"Depends on what level you want.

If you're looking for a very basic introduction you could try the CS50 course.

www.edx.org",1534418438.0
CanadianDevHead,"Laurier just launched their regular MCS course online: [https://online.wlu.ca/programs/master-computer-science](https://online.wlu.ca/programs/master-computer-science)

&#x200B;",1536673117.0
FuturePrincessLeia,I'd suggest CodeCombat. Then fun UI makes it easier to stick to in my opinion. I did their python tutorial and liked it.,1534396169.0
lambertb,Cleverprogrammer.com,1534423093.0
WhackAMoleE,"A disreputable site would work as well. Pick up a book, start on page one, and work your way through it. ",1534619134.0
inephable,Wrong subreddit.,1534374150.0
jessi-t-s,"Apply for internships that start after you finish your first year, that way you’ll have acquired some skills. But I think it’s more common to do an internship after your third year. Doesn’t hurt to apply though!",1534374222.0
samboy218,"Apply for government stuff. They like to get people early and are always wanting good technical people.

If you're looking for an internship the places that would take you on aren't really looking for people who know a lot, otherwise they would just be hiring you for a real job. Get aquatinted with the things you commonly see as requirements. Learn a bit of python and go and you'll get through an interview just fine. Express an interest to learn and grow, that's what an intern's real job is.",1534387294.0
zookiethewookie,"Try out /r/csMajors/, that community is more geared to these kinds of questions :) ",1535672930.0
criticalcontext,Seems fishy.,1534373167.0
Steve132,"From https://github.com/MaiaVictor/abstract-algorithm

""Moreover, it is capable of automatically exploiting any inherent parallelizability of your program, since interaction nets are a naturally concurrent model of computation.""

Iterated ""not"" can be computed in O(1) time pretty trivially, and in O(log(n)) time pretty trivially with reductions, so the fact that it goes ""logarithmic"" in that case is pretty uninteresting.

More importantly, addition can be done in parallel O(log(n)) paralle steps, this is how it's implemented in hardware with reductions.

It seems like whatever counts as an 'operation' is done in 'parallel' under this model of computation, which makes the 'O(log(n))' reductions impressive that it discovered them, but not terrifically surprising.

Wake me when it can 'run' a subset sum problem in O(n^3) time.",1534390161.0
noam_compsci,"Would you care to do an eli5 of this? 

I do not understand what those graphs do. In one of the last paragraphs you mention that there are, say, 4bn calculations occurring. Is the number of calculations and the number of graphs different? Are you measuring the number of calculations or the number of graphs being made?

This seems really interesting but would love it if someone dumbed down parts of it. :)",1534406427.0
TheVicePresident,Judging by the typos im guessing this is a mistake and the editor of this news source didnt check it well enough,1534374175.0
mmm_toasty,"Oh hey, I can answer these.

**Foundational knowledge**: Get the CLRS Algorithms textbook. If your math/proof skills are up to date, this will be invaluable. If they're not, get them there.

**Practical programming skills**: Leetcode, Hackerrank, Codingbat, and personal projects. I have more recommendations if you want them.

For both of the above sections, *read the sidebar here*. The resources there are really, really good, and might actually be more geared toward specific questions you have vs. this sub.

**Course recommendations for next semester**: OOP in C++ and Data Vis. The ML/data mining course in R will likely be incredibly useful, too, and R is one of the primary languages for stats, ML, and data science.

**Notes**: Don't bother with OS and networks at this point. They're hard and really not things you need to know right now. Focus on the things that'll actually help you, and if you want to know these later, then come back to them.",1534372198.0
Aleriya,"Courses that strike me as being relevant or helpful for your thesis:

> ML/robotics type class in C++/Python

> Data Visualization

You should really know this to be a well-rounded programmer, but out of the list, these are the easiest ones to teach yourself without needing a formal class in it:

>Computer Networks

> Databases

Operating Systems is really interesting, but it's not a class where you learn practical day-to-day programming. It's sort of like taking philosophy/logic as a math major. It's also a common undergrad course and there are a lot of lecture series online if you ever wanted to learn it on your own.

Useful for almost any programming job:

>Parallel Programming Techniques


I'd put these at the bottom of the list (due to scarcity of jobs and/or needing more specialized knowledge to be a viable career path):

> Systems programming 

> Graphics

Both of these are super interesting, but Graphics is an entire course of study and I don't think you'd be a strong job candidate with only one class in it. That said, it's a very mathy area and you'd probably do well there. Systems programming is notoriously experience gated. Ex: you're often expected to have 10+ years experience before you delve into systems programming as a career.

>computer architecture for AI

I don't know enough about this to really comment, but this seems to be the same field as Intel's Loihi project: ""a self-learning neuromorphic chip that mimics how the brain functions"". That might dovetail into your area of interest nicely, and it sort of bridges EE to ML/AI.",1534373426.0
travisk1154,"For computational science programming, I would take a look at the parallel techniques course, or at least the topic in general. A lot of large scale scientific programs are written to be run in parallel on graphics processors. Learning GPU programming like CUDA would also be useful",1534379175.0
robotix_dev,"Out of your available choices, based on your interests I would say OOP and Databases are probably the first two to take. If you don’t feel comfortable with programming though, maybe take the undergrad course first (just to get your feet wet).

I would also check out sites like Udacity and Coursera for updating your programming skills. ",1534375210.0
pdbeard,"I don't have recommendations for courses, but thought you might be interested in event I helped out with a few years ago: 

[https://brainhack.sice.indiana.edu/#Home](https://brainhack.sice.indiana.edu/#Home)

One of the organizers worked with my group and also develops an open source MRI Visualization tool in python ([http://nipy.org/dipy/](http://nipy.org/dipy/)) that presumably is used mainly for brain things. 

May or may not be useful, but your thesis reminded me of it!",1534402396.0
millenial2go,Grad school is probably not the best place to gain foundational knowledge of a subject,1534367928.0
ikdc,"From an information-theoretic standpoint, ""negative complexity"" is nonsense of course.  The only thing this means is that the original reduction strategy for computing ""add"" was suboptimal, and wrapping it with the silly-looking copy function allowed the reduction algorithm to find a faster strategy.

As a side note, highly parallel models of computation like the one discussed in the paper are one reason why Area*Time complexity might be the right notion for analysing the realistic costs of computing functions.",1534366105.0
krimin_killr21,If I hear one more buissiness major talk to me about crypto I'm gonna suck on a pipe of exhaust.,1534266128.0
,"/r/vpn

/r/vpnreviews

/r/nordvpn",1534237567.0
easeypeaseyweasey,Cant answer your question but be sure to search for the coupon deals.,1534242213.0
_--__,"Let L = {01^(i)01^(j)01^(j) : i,j>0} &cup; {w ∈ {0, 1}* : w contains 00}.

You can show that L is not regular using the Myhill-Nerode theorem.  But you can show that L satisfies the pumping lemma with pumping length 3 (i.e. for any word in L of length 3 or longer you can break it up into uwv such that uw^(i)v is in L for all i).",1534242463.0
BuxOrbiter,"Models of Computation is the course that broke me into a man.

Recall the equivalence between regular languages and finite state machines. Consider the graph representation of a finite state machine. Take any string that is in the language but has more symbols than the machine has nodes. To accept the string, there must exist a node that the machine visits twice. Therefore, a loop exists in the state diagram (if the machine can revisit the node once, it can revisit (aka pump) an arbitrarily large number of times). 

The pumping lemma basically says that if you take a machine with n nodes, to accept any string of length k > n, the machine must revisit m nodes, and therefore there exists a subsequence in the string that can be repeated, and that the machine must accept.

TL;DR to represent an infinite language with a finite state machine, that machine must have an infinitely repeatable loop.",1534233227.0
taejo,"Let A be regular and B non-regular, and let # be a symbol which is not used in either of them. Then A#B = {a#b : a in A, b in B} is not regular, but satisfies the pumping lemma.",1534229469.0
shakespeareanseizure,"Great question! Thinking about converses usually helps me better understand a theorem.

First, think of some examples of languages that aren't regular, the simpler the better. Do you see any patterns that appear in that language that could be pumped indefinitely and still be in the language? What's the length of that pattern?

To be honest though, it's not as easy as it may seem. Do you know the Myhill-Nerode theorem?",1534219791.0
uncleXjemima,My worst nightmare ,1534241217.0
jx4713,"Nice! But, people shouldn't be scared to pick up a graph theory textbook, either.",1534185536.0
zettasyntax,"What level of degree? I don't know of many undergrad CS programs that are entirely online. Only places like SNHU or GCU come to mind. At the master's level, places like USC offer their master's degree entirely online. ",1534126101.0
Rizean,"Regis https://info.regis.edu/lp_bs_computer_info_systems?cid1=cpc&cid2=google&cid3=co-computer_info_systems_bs&_vsrefdom=CO_CIS&gclid=Cj0KCQjwtb_bBRCFARIsAO5fVvF5RHAi4J16EJPBzcpQLzEbbAilA-rmSDsAayCjerTe27-t5vhkyVUaAgx_EALw_wcB

That's where I got my degree. For reference making 115k with in 5 years of grad. However, I had 20 years IT before switching to CS and eventually DevOps.  Regis CS program was solid but I did a ton of study outside school. They will teach you CS... NOT software development.",1534127159.0
lair001,"If you already have a bachelors in some field:

[http://eecs.oregonstate.edu/online-cs-students](http://eecs.oregonstate.edu/online-cs-students)

[https://www.omscs.gatech.edu/](https://www.omscs.gatech.edu/)

I'd suggest the masters if you have a strong STEM background.",1534127570.0
hawkman561,"This is how those list articles should be written, just tell me the damn list lol",1534102015.0
caiocaio,That's awesome. I'd love to see more accomplished people share their favourite programming or math books.,1534101643.0
acroback,"SICP is more about separating tools from problem solving but it opens your eyes in a way no other book does.

The part about recursion and it's cost and how that blends in with a tree structure is a great example of good book writing.",1534110268.0
Santamierdadelamierd,"Everybody recommends the same books and the prices on amazon rise. The hardcover copy of one of those is $200, although it's available online for free.",1534125129.0
p_pistol,Everyone should have CLRS,1535127780.0
ElHermanoLoco,"One thing that will add valuable industry skills is to extend functionality or refactor your old projects, maybe to be more straightforward to extend or just light up new functionality. Something you haven't touched in months or years, ideally, so it isn't fresh in your mind. You could take one of your old apps and tweak the premise or task. Or try to make it testable, if you haven't written tests. Keep track of what's easy to change and what's hard, and what decisions you made incorrectly. It won't teach you new algorithms, but will help focus on maintenance, self-documentation, abstractions, etc.

I don't know how it's done now, but all of my school projects we're basically one-and-done, when nearly all industry work happens iteratively on existing and potentially unfamiliar code, plenty of which you will have written and won't remember.

Otherwise, consider spinning up docker containers and create distributed versions of your existing apps, or maybe a statsD type of metrics collection and reporting system. Anything where you have to make other systems work together. Lots to dig into there, and relatively simple to start with.",1534056835.0
fantasticpotatobeard,Contribute to open source.,1534060842.0
afiefh,"Make a game.

Depending on your choices it can be as complex or simple as you want, performance matters so you have to deal with that, and you are going to need to make lots of decisions.",1534049890.0
nevabyte,"A game with libgdx! It is a great framework to use as you can literally program to interfaces as opposed to being forced to use certain classes.

The above as well as the fact that you can ship to desktop, android and iOS will give you some insight into all of those OSs and how they operate! 

Use other libraries as well such as Ashley and Box2d! Creating your own projects is great, but getting accustomed to using others’ work is also a valuable skill and one which is no doubt needed in the industry.

Plus it is really fun! I made an a little alphabet app for my son to use! Have since made quite a few games! 

I am a dev with a good few years of experience, this is what I would suggest! Good luck ",1534071818.0
jmite,"Write a compiler. I suggest the Tiger Book by Appel. Compilers touch on almost every aspect of CS, and will give you plenty of data structures practice, as well as architecture decisions to make.",1534259797.0
obp5599,Do a project with a real language :/,1534085739.0
anotherseemann,Can you please write full words?,1534071054.0
Tagedieb,"15 can be represented in 4 bits, 25 takes 5 bits.",1534047515.0
CeilingRepairman6872,"1. You have to guess when the data is not likely to be needed in memory. It can't be too short that the cache is useless, and too long that you'll get a memory leak.

2. The underlying data might get changed by another process and then your process that uses the cache will be working with incorrect data

That's all I can think of right now",1534002364.0
unlocal,"Success in a caching implementation involves seeing into the future.  Or multiple possible futures.

This is hard in any non-deterministic situation. Sources of non-determinism include both your own workload as well as other workloads that may affect the cost of things you depend on (cache fill / eviction actions, for example).",1534004380.0
timmyotc,"Shopping cart example: 

Let's say User1 adds 5 different things in their shopping cart. The inventory count of those items are cached in Redis or Memcache.

While the user is checking out, a Manual Inventory has been completed and gets loaded into the system. A manual inventory is the result of someone walking through a warehouse and counting items by hand. They discover that there are actually zero of a certain item that happens to be in User1's digital cart. The inventory person obviously doesn't know that.

User1 pays for the items that the system says they still have inventory for. Ideally, an error occurs as the purchasing system attempts to move a number of items from ""in stock"" to ""sold"", as there are actually none ""in stock"". This happens a lot when one item is next to another item and they have very similar packaging. Warehouse employees will accidentally ship the wrong item. If the cart checkout code was less defensive, it may just set a negative ""in stock"" value and the company would have a more serious problem of selling something they didn't own. 

Hopefully, the person who wrote the Manual Inventory Entry code knows about the caching that the shopping cart does and sends them some sort of message that ""inventory has been updated. Please invalidate these cache entries."" But that requires that they know exactly what is being cached and how it's being cached; otherwise they may not give the right information to ensure that data is being invalidated. ie- ""Please invalidate the following SKU's - "" vs ""Please invalidate all SKU's associated with Vendor XYZ"". 

",1534005945.0
dominic_failure,"A bit of wisdom by someone more eloquent than I:

http://thecodelesscode.com/case/147

http://thecodelesscode.com/case/148",1534003305.0
gct,"Cache coherency is the overarching difficult problem, not just invalidation.  You have to build a distributed consensus protocol, in hardware, and it has to be flawless (and very fast).",1534005690.0
pardoman,I’m unclear whether OP is refering to CPU’s L1/2/3 cache levels or a server side caching data layer. ,1534006743.0
lgroeni,"I’d also add that since the cost of populating a cache is almost always higher than an uncached request, with the cost amortized over subsequent uses of the cached object, unnecessary cache invalidations tend to hurt performance.

That gets worse with shared caches, as a cache invalidation usually involves either implicit serialization or synchronization of pending accesses by other consumers.

Precise cache flushing/invalidation is great if you know, beyond a shadow of a doubt, that you don’t need that data anymore and you want to hide latency in subsequent operations, but mistakes or bugs or even poorly tuned implementations can easily degrade performance under common access patterns.

Complicated systems involving caches often result in chaotic dynamics. They’re hard to get right, yo.",1534013897.0
cognificent,"I really think the joke needs to be updated to read 'distributed cache invalidation'. Local CI might be hard, I don't really know all the details, but the distributed version seems much more 'no-win' to me.",1534040362.0
metaphorm,"any model you might make of data access patterns is just that, a model, real life situations are always going to be different than idealized models and you will always get more cache misses than in an optimal scenario. ",1534005741.0
VVVDoer,"Are you writing an operating system? Needing to interact with the cache in the software layer is typically to help reconcile changes made during interrupt contexts that will otherwise prevent stale data from being correctly re-read.

Most of the problems of cache coherency and performance are built into the processor and don't require software intervention.",1534008901.0
MeanFoo,https://martinfowler.com/bliki/TwoHardThings.html,1534043781.0
dusklight,"For the longest time I thought it was about machine caching and how to decide when to clear items out of the pyramid of caches we have in modern computer systems, from the register to the l1 cache to l2 and ram and then harddisk and so on.

Nowadays how I apply it is to the cache inside my own head. As a programmer we have to remember many things and there are only so many things you can remember, especially the stuff that you remember in your ""muscle memory"", the part of you that thinks fast enough to be able to make intuitive decisions. So, choosing what you have to remember, and being able to choose what you can forget to free up more brain space to remember new things, is really an artform. In particular writing code that you can forget what you wrote and you can come back to it a few months later and you don't get lost and can easily figure out what is going on, that's an artform also. So as much as possible, the goal I set for myself is to write code such that I can randomly jump to any screenful of code, and I can more or less understand what is going on without remembering anything other than that one screenful of code I am reading right now. 

Choosing the right names for your variables/functions goes a long way towards making this happen. Here's a video that covers some of the things you should be thinking about when naming things if you haven't seen it before: https://www.youtube.com/watch?v=CzJ94TMPcD8",1534004395.0
ggPeti,"Don't take that saying too seriously, part of the humor in it comes from the contrast between jargon ""cache invalidation"" and common language ""naming things"". There are lots of hard problems in computer science, many of them harder than these two.",1534004058.0
Akeshi,Affiliate spam...,1533994683.0
VaginalCrease,Was really hoping for some free shit on this one. You let me down Carl,1533995520.0
TrevorMcDonald,"This probably belongs in r/learnprogramming. 

Hint: what happens when you’re in the first loop and you don’t hit any of the cases?",1533966150.0
lmericle,How is it possible that a robot can have so much character with only a hand?,1533947091.0
IndependentBoof,"Haha I like it!

Are there any papers on this project? Or even better, some videos of it performing it on more Where's Waldo pictures?",1533944012.0
zchompy,Sure thing.. BUT CAN HE FIND WOOF THE DOG?! HUH?!,1533944806.0
firmretention,Did they find the only copy in the world where some asshole hasn't circled Waldo's location?,1533951042.0
RyanCacophony,I assume it needed a decent size training set to extrapolate how to detect a waldo- how did they get enough instances such that we can be sure it wasnt trained on this exact waldo? Are there really that many waldo books?,1533960245.0
sparr,"How to find Waldo with Mathematica operations:

https://stackoverflow.com/a/8479757/13675",1533961066.0
rlundegard14,Robot with a severed hang thing creeps me out. Don’t think it was that necessary.,1533952722.0
absurdness,Now just be on the lookout for weapons that provide a simultaneous head shot to 250 enemies on the battlefield without the need for a soldier to fire a gun.,1533955334.0
pm_me-your_tits-plz,Why is there even a hand? It could be a laser pointer. Or a stick.,1533955258.0
flekkzo,T-800 prototype.,1533951529.0
DensityKnot,This is beautiful,1533961435.0
Alienithinkimlost,Is it just me or does this robot have so much sass packed into such a small volume ,1533978042.0
Nerdlinger,"https://www.newscientist.com/article/mg22429992-400-googles-new-bot-trap-trains-machines-to-see-the-world/

https://www.techradar.com/news/captcha-if-you-can-how-youve-been-training-ai-for-years-without-realising-it",1533929744.0
TomvdZ,"It is quite uncommon to publish anything before you start your PhD. Don't publish something just for the sake of publishing; a bad publication will do you more harm than good.

There are a lot of ""predatory"" conferences and journals (especially in your part of the world) that charge a fee for publishing a paper, but whose peer-reviewing is basically non-existent. They will basically publish any paper, no matter how bad, so long as you pay. Publishing any paper (even a ""good"" one) at one of these predatory venues can harm your chances significantly.

If you do want to publish something, you'd need to pick a subject area to study, find a mentor at your university that works in that area and can give you some problems to work on, and then you'd need to be lucky to get any publishable results. I think it would be better to just focus on writing a good master's thesis, if you're lucky then perhaps some results from your master's thesis will be publishable.",1533924161.0
jessi-t-s,"Are you in a thesis-based master’s program? If so, you’ll already be doing research! You should speak to your supervisor about appropriate conferences to submit your work to, since there are many, many options and it will depend largely on which area of computer science you are in.

Generally speaking, your research should contribute something novel (you don’t need to re-invent the wheel, just further our knowledge base in some small way). Do a lot of background reading on topics you’re interested in and try to determine what hasn’t been done yet or what has been done that you could make significant improvements to.

Depending on your particular research domain, you may benefit from conducting user studies of the system you’ve built and presenting the results in your paper. No matter what you’re doing, you will likely need solid quantitative data that can show something statistically significant.

If you’re in a course-based program and are not already doing research, you will likely need to get in touch with a professor doing research you’re interested in, and propose a topic. If they are willing to supervise you, they can likely guide you toward good, publishable research.

One thing to keep in mind though, is that pretty much all venues that publish research will have a very low acceptance rate for research papers. This keeps the quality of published research quite high. Whether or not your submitted paper is published will be largely determined by the individuals who will review your submission and their own knowledge, opinions, and biases. If your paper is rejected, take your reviewers’ advice and comments into consideration so you can improve your work and resubmit! Don’t let rejection get you down. Even really good research will likely get rejected because acceptance rates tend to be so low. Keep working at it!

It’s hard to give really specific advice without knowing your research domain (I’m in HCI myself), because computer science is very broad. So everything I’ve said here might seem very general/vague. Let us know what your research interests are and someone from your particular sub-field can probably give you more specific advice!",1533924551.0
thbb,"You have to have the subject you want to work on *before* you start considering writing a paper. You work on something that you like to explore, and when you've found something you think will interest other people, then and only then can you consider publishing it. 

At first it may not be that great, or publishable material. So you can consider making a small blog, or exchanging with fellow students and professors at seminars, for instance. This is how good ideas for research, hence papers, emerge. 

Now, you may argue: but there are so many things I'd like to explore, I have no idea what direction to take, what will be fruitful? This is where a tutor/advisor is needed. Someone who talks with you, has some experience, and can point you to papers or colleagues whose ideas may resonate with you. If there's no one who can coach you on the topics you're interested in, read papers, and email to the authors to engage in a conversation about their research. Researchers love to discuss their research. If' you're thoughtful, inspiring, they'll consider giving advice on how to go further in their area of interest, and that's how you get to chose a topic.

So remember: you don't ""write a paper"" because you want to get good grades so as to go to a university. You write a paper because you have found something cool that you'd like to share with a community of like-minded people. The invitation to follow-up comes naturally after that.",1533924609.0
rosulek,"In order to have a research paper, you need research to write about. In order to qualify as scientific research, a contribution *must* be (1) interesting, (2) novel. And if you want to actually succeed, (3) within your technical abilities. For someone in your situation, simply identifying something that meets all 3 criteria is generally just as hard as actually making the progress. Without serious exposure to a research area, you will have no way of measuring 1 & 2, and (depending on the area) your undergraduate preparation has probably not equipped you with enough technical tools for 3. 

This is not even accounting for the actual writing (most people start out as pretty bad technical writers) and publishing (you have to know how to choose the right venue and understand its conventions).

This is why you need a mentor: to help you identify a problem, direct you in digesting the relevant literature, and coach you on developing technical skills. ",1533926627.0
jessi-t-s,"Adding a second piece of advice: since you’re just starting out in research, it might be helpful to aim for smaller milestones! This can still help build your research profile. For example:

-Many conferences have workshops with high acceptance rates. While these papers are not exactly “published”, they are still appropriate to include on your CV (under a “workshop papers” title) and attending the workshops will also give you opportunities to collaborate with other researchers in your domain.

-Many conferences will also publish extended abstracts and works-in-progress. These are also maybe not as impressive as full-paper publications since acceptance rates are generally higher, but they can still be valuable and flesh out your CV! They are usually peer-reviewed (albeit to a lower standard than full papers) and published alongside conference proceedings. For something like this, you probably want to try submitting to bigger conferences since they are more reputable.",1533926863.0
jmite,"As for the actual mechanics of putting together a paper, \[this\](r/https://www.cis.upenn.edu/~sweirich/icfp-plmw15/slides/peyton-jones.pdf) is the best guide I've seen (video version [here](https://www.microsoft.com/en-us/research/academic-program/write-great-research-paper/)). It's a fairly opinionated guide but it covers the basics well.

The hard part is doing the research that you can write a research paper about. Have you done any research as part of your Master's? Do you have a supervisor you can talk to? Writing the paper is easy, compared to doing the actual research, so that you have something to write about. ",1534133084.0
KalimasPinky,[like this](https://media1.giphy.com/media/ule4vhcY1xEKQ/giphy.gif) ,1533935574.0
localoptimal,"Like others suggest, it would be quite a task and not really doable on your own. You'll need significant mentorship from an adviser which is definitely possible; you should look into that yourself by talking with some of the profs at your school. 

That said, what you can do yourself is build up a portfolio that shows you can at least understand current research. You might look into some recent papers and recreate their results from scratch or apply the methods to a different dataset or something like that. Then publish this on github and/or your own portfolio website - this shows not just your technical competency but also that you can communicate results clearly.",1533960096.0
dabombnl,"You are going to have to assume that file is never changing.

And if that is true, might as well just have it cache all over the place, which is what people do today.",1533913017.0
ChayceH,"This is what multicast solves. By sending out one packet (or file) the protocol copies that file at different nodes in the network depending on the number of receivers that request the file, and where they are located in the router topology. This prevents the root from sending out n packets as this vastly increases bandwidth.

I worked on the multicast team so pm me if you have any questions.",1533915197.0
GuyWithLag,Look up multicast.,1533911606.0
peer_gynt,"If you distribute data over the network, and assuming this is not about special purpose network infrastructures, then your disk seeks are mostly made irrelevant for two reasons: caches and latencies.  Always first measure, then optimize - if you *have* measures, then please do post what you found.

I'd also recommend to look into multicast for this problem space (as suggested by others) - although general support is very poor in WANs.  P2P approaches may also apply, depending on the specific use case.

A classic read and highly relevant for anybody who wants to look into distributed computing: http://www.stuartcheshire.org/rants/Latency.html

",1534064454.0
UncleMeat11,It is not possible to answer this question without seeing your code.,1533918799.0
carette,"Is it just me, or is the linked article entirely devoid of actual content beyond the title?",1534084963.0
slayersaiez,That was pretty damn hard for me to understand. Does anyone have an even more beginner friendly intro to lambda calculus?,1533846727.0
Aureliony,Initially I thought the thumbnail was a fidget spinner,1533891292.0
lgastako,This one looks pretty good: http://www.cs.unibo.it/~sacerdot/fli1718/IntroductionToLambdaCalculus.pdf,1533881154.0
HeyGuysImMichael,Cool!,1533871081.0
,What is lamba calculus and why should I learn?,1533930693.0
,[deleted],1533874450.0
spooker11,"Damn I really wish my linear algebra teacher prioritized teaching over speed, this stuff is really interesting and I want to learn more about it. Sucks that all that tuition money and time was pretty much wasted by a shit teacher and now I have to self-study anyways in order to understand this article.",1533923860.0
NovaX,"Caching is fundamental and keeps getting better. See this [example](http://highscalability.com/blog/2016/1/25/design-of-a-modern-cache.html), though slightly outdated as TimerWheels were later added for expiration. You can follow its links to papers to dig deeper into what interests you.",1533827717.0
KevZero,"You should write [a book](https://www.goodreads.com/book/show/265456.The_Tao_of_Programming), or [maybe a website](http://www.mit.edu/~xela/tao.html), and call it *The Tao of Programming*. ",1533783513.0
flexibeast,"Heretic! The Way is the Way of Lambda, for indeed, its forms can lead us to the Functions Not Constant.",1533784147.0
pulsar512b,"That is soo awesome. I have a different translation that wouldn't work as well, but it would do the trick.",1533783356.0
Lemmings4Friends,"Well yeah... the Tao is an attempt to interpret ""natural order"" (modern day physical sciences, physics, chemistry, etc) With no concrete knowledge all one can do is speak in abstractions, so it would obviously be synonymous with high level programming. Abstract description of a system? Programming or metaphysics, whatever word floats your boat. ",1533790383.0
possiblyquestionable,"Is the whole conclusion of their research that computer vision can be improved by adding recurrent edges to their networks? That seems really anticlimactic.

Edit: actually, I just saw that this was published in a cogsci journal which does make this research more valuable. Their aim is to identify why biological neural pathways are recurrent and they used the lack of the ability of a non-recurrent artificial network to have higher order representations of objects as a cue to the role of recurrent edges. Though this article's claim that this will help further research in artificial computer vision seems ambitious at best. The subject matter of the research is well known to the community. The discussion in the paper itself however is very thoughtful.",1533773834.0
woojoo666,"Reminds me of [this neural network](https://youtu.be/DglrYx9F3UU) that was trained to recognize ""similarity"" between images in the same way that humans do. I wonder if this technique would fare well on the problems presented in OP's paper",1533809887.0
flikibucha,"This is why I’m a little skeptical about self driving cars. The car has to understand what is going on if someone tries to like hijack the car, or someone’s running in the street crazily. Otherwise they may just end up using routes like buses to avoid novelty.

And I wouldn’t be surprised if the human brain is more efficient than something we could build realistically.",1533801101.0
DreadedDreadnought,"To read the value of a register you need 1-2 CPU cycles, for L1 you need more (4+) cycles. 

https://stackoverflow.com/questions/10274355/cycles-cost-for-l1-cache-hit-vs-register-on-x86 ",1533749901.0
soluko,"- L1 cache is slower 
- registers must be individually managed, allocated and saved/restored by your code _(well actually the compiler)_, L1 cache is managed by the hardware and is _(mostly)_ transparent to your code 
- as a consequence of this, an architecture will have a _(more-or-less)_ fixed number of registers but L1 cache size may vary between different models of the same CPU architecture
- there are a lot fewer registers than there is L1 cache. 
",1533749953.0
kyuubi42,"Simplifying a bit, each cpu core has a fixed number of registers with defined names and sizes, and in some cases defined behaviors (such as E/R/FLAGS on x86, which is automatically set and can be read from to determine if some exceptional condition has occurred, such as an integer overflow).

L1 Cache just caches data from RAM for faster access. In most cases this is completely transparent to the programmer, although some chips do allow the programmer to specify caching policy for specific reads and writes.",1533750186.0
Glacia,"What do you mean ""what is the difference""? They are completely different things. ",1533747936.0
combinatorylogic,"Register file usually have far more ports than L1 cache, and is addressed differently (no tag lookup required, but may be banked). Otherwise it's the same SRAM block.",1533890536.0
MINIMAN10001,"A register is where data is stored that instructions can be ran, it can be controlled by the programmer.

add eax, ebx

add eax, 4

There is a small amount of registers and instructions

L1 cache is a slower large block of memory ( usually 32 KB ) which the programmer has no direct control over

Each successive cache is larger, and further away from the registers than the previous which makes them take longer to access.",1534102697.0
balefrost,This would be more appropriate in /r/programming (or /r/learnprogramming) than in /r/compsci.,1533733668.0
Nadamir,"The crux of the statement is that for the most part the default method of generating 'random' numbers on a computer is into random enough to fool humans. It is not random enough to be cryptographically secure, which is why there are special packages and instruments that use truly random natural phenomena to generate truly random numbers.

Most people, laypeople and programmers alike, mean pseudorandom when they talk about random numbers generated by computer. Unless there's a specific cryptographic context, I assume random means pseudorandom.",1533705659.0
jdionne100,"There's also the fun story of how back when iPods were a new thing, people would have music on shuffle and often have multiple songs by the same artist in a row and complained to Apple. Apple then changed the algorithm to make it less random, to make it feel more random to people.

The thing is I've always had a hard time with randomness too, and never really got it until I took calculus based probability, which is where I got that ^^^ from. True random is so different from how we perceive random that if we flip a coin and see five heads in a row, it might seem nuts, but in all reality can happen a lot more than would make sense.

To answer what seems to be bothering you (reading the rest of the thread here), honestly I think there are just two different definitions, and it's all context based. Randomness in a sociological experiment might need to appear more random than it really is, while randomness in mathematics should be truly, scientifically random.",1533707726.0
ParanoydAndroid,"Oh god this post is a mess.  I never knew this sub had so many non-CS people in it.

OP, ignore any comment that sounds vague, wishy-washy, or says things like, ""my intuition is that"" or ""from my perspective randomness means ...""  

Randomness in a broader sense has some fuzziness to it, but in the realm of CS it is a well-defined and rigorous phenomenon.  Please do not get confused by the multitude of shitty posts clogging your question up with fake answers.

My main recommendation is to read [Claude Shannon's (famous) paper](http://math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf) where he introduces the concept of _Shannon Entropy_, which is essentially a measure of randomness.  There's also the related concept of _surprisal_.

I know recommending that you read a 1948 academic paper seems pretentious, but it's a highly readable paper and honestly a better introduction than you'll get from most sources.  You can also check out [the Wiki article](https://en.wikipedia.org/wiki/Entropy_(information_theory\)).

In short, randomness, in CS terms, is a measure of how much information you expect to get from a string of bits.  According with our intuition, randomness is maximized when all bits have the same probability of appearing in our string.   A good, quick way of assessing randomness is to ask yourself the question, ""how many bits are required for me to store the potential result?""  the answer to that question is not identical to the randomness, but is a good first-order approximation.  For example, a coin flip can have its answer stored in 1 bit (H == 1, T == 0, for example).  By comparison, it takes <3 bits to store the results of a dice roll, and ergo a dice roll is ""more random"" than a coin flip; which again, conforms to our intuition, because it's harder to guess the outcome of a dice roll than a coin flip.

The longer, less hand-wavey version is that to actually measure the randomness, instead of just looking at roughly how many bits you need to store the result, you instead look at _every, individual_ result and for each one you calculate how many bits it would take to store that _single_ result and multiple that by how likely that result is.  Do that for each one and then sum that all together (and also multiple by -1 because you're working with reciprocals in a log).  _That_ is your measure of randomness in CS.  Specifically, the more bits this measure gives for some process, then the more _information_ is provided by some specific outcome of that process, and randomness (again, in CS) is the property we measure when we measure how much information you get.  The more bits you need, the more information you get, and the more information you get from a specific event, then the more random the process that generated that event was.  

To put it another, another way: randomness is a measure of uncertainty in some outcome, and in CS the more uncertain of the outcome we are, the more bits we need to use to capture all the information about that outcome -- e.g. if something can equiprobably go 2 ways, we need one bit; if something can go 1,000 ways, we need 10 bits.  Ergo, we can use bits to measure randomness and this is what _Shannon Entropy_ does.

As for your specific question, most CS programs (really, all of them that I'm aware of) will have a defined model of computing they're using when they talk about ""what computers can do"".  In most common terms, computers cannot generate random numbers, because usually that reference is either to a fully deterministic Turing machine, or to a Von Neumann architecture computer (which is more or less our computers: a couple of peripheral HIDs, a processor, main memory, and storage).  Neither of these types can do the task.",1533736431.0
tldrtfm,"I don't have much to add to this discussion, but I hate to see a good [reference](https://xkcd.com/221/) go to waste.",1533715411.0
thbb,"You're opening a can of worms here. I'm just going to drop a few considerations and viewpoints on randomness, and let you make your own interpretation.

From a philosophical standpoint, a good definition of a random event is [the encounter of chains of *causality*](http://sheynin.de/download/cournot.pdf). This is intellectually satisfying, but of little use in concrete settings.

In computer science, a good definition of randomness arises from [Kolmogorov complexity](https://en.wikipedia.org/wiki/Kolmogorov_complexity): a string of digits is said to be random if you can't find a combination of algorithm+input string whose representation is *shorter* than said string and that *produces* said string. 

Finally, the statement ""Computers can't generate random numbers"" means that computers are *deterministic*: given the same input state, they always produce the same output. In order for a computer to produce a truly random result, you have to introduce a source of randomness, such as a [lava lamp](https://blog.cloudflare.com/randomness-101-lavarand-in-production/). But then, the lava lamp becomes part of the inputs of your system, the computation stays deterministic.",1533723222.0
khedoros,"""Random"" means that when you're generating a sequence of numbers within a particular range, the next number generated has an equal chance of being any number in that range. There's no pattern or predictability.

Difficulty: your computer is designed to work in a deterministic way. Given the same initial state and the exact same inputs, the outputs will be exactly that same. Doesn't sound very random. So, for random number generation, we've got a lot of functions where you specify a starting point (a ""seed""), and it will generate a set of numbers that behaves like it could have been generated by a truly random process. We call those Pseuso-random number generators. A common seed is the current system time. That's usually sufficient for non-cryptographic uses.",1533707835.0
lazylearner,I dunno what to answer but just say I love these deep threads. I hope you find the answer you're looking for. :-) ,1533714236.0
Awia00,I feel like you would like this video https://www.youtube.com/watch?v=KjeKiIa7XEk&t=1s ,1533727308.0
tsphillips42,"It is impossible to deterministically generate random numbers. That is, if we know how the numbers are generated, then they are not random. 

A couple links that may be helpful - the first on what random means, and the second on randomness in quantum mechanics.

* http://mathworld.wolfram.com/RandomNumber.html
* https://en.wikipedia.org/wiki/Quantum_indeterminacy

The key idea here (from the first link) is, ""It is impossible to produce an arbitrarily long string of random digits and prove it is random."" In computer science, we look for algorithms that produce a series of numbers that is random enough for our purpose. (This refers to pseudorandom numbers.) Consider how random a video game needs to be compared to how random a cryptographic key needs to be. The first problem needs to generate data that are entertaining, while the second problem needs to generate data that cannot be predicted by teams of mathematicians.",1533745052.0
jnwatson,"The statement that ""Computers can't generate random numbers"" is true only for a particular abstract model of computer that doesn't match today's modern laptop or cell phone. Today's modern chipsets contain ""true""\* random number generators that derive their values from a physically random source that ultimately derives from quantum mechanics.

(\* true if you believe the manufacturer's claims. It is relatively straightforward and hard to detect for someone in the chip manufacturing supply chain to compromise the hardware RNG). ",1533755338.0
metaphorm,"computers can generate random numbers but to do so they must be connected to a source of real physical entropy that they can sample from. 

absent a source of this kind they can only generate pseudorandom numbers from an algorithm that, while deterministic, is extremely hard (nearly impossible for most cases) to guess what the next output would be. ",1533762057.0
shadow54015,"I'm coming into my junior year as computer science.

So the question being, is anything in the world truly random?

I don't believe that there is. We as humans just don't understand all the variables, reasons, and science. Even some a lot of the science we believe to be true has errors or flaws in it. It is just perceived as random.",1533762546.0
Volcano-squared,"This entire thread is a hodgepodge of confusing contradictions, most of which are incredibly detailed and refer to papers and articles. 
  
For a variable to be completely random it has to be impossible to recreate it even if you had complete knowledge of everything else in the universe. Randomness is the attribute of being random, and things can contain a varying amount of randomness with something with greater degrees of randomness requiring more outside information to determine the value of than something with less degrees of randomness. 
  
Classical computers cannot create a truly random variable if they don't leverage quantum principles, (which are the only things currently understood to be truly random). 
  ",1533764672.0
PM_ME_YOUR_FUN_MATH,"I see this question a lot. People use the word random to mean different things, as I'm sure you've noticed.

One definition that I see a lot is where random means *uniform*. For example, a machine that spits out numbers that has the same odds of giving you any number. A simple loop of 1, 2, 3, 1, 2, 3... would satisfy this.

Another is *pseudorandom*. This is when you get an output that appears to follow no pattern. Usually these are uniform, and usually that is a hallmark of their quality, but not always. An example of a pseudorandom generator would be a function such as taking the input number, multiplying it by 3, then finding the remainder when you divide it by 7. It would turn 5 into 1, 1 into 3, 3 into 2, 2 into 6, 6 into 4, etc. If you were just given these numbers, then you might think they were random, but the truth is that there's a function behind them.

To be pseudorandom means that, given one number, it is possible to predict the next with absolute certainty.

The other big one is true randomness. This is where, given one number, you absolutely cannot predict the next one absolutely.

Does true randomness exist? It's hard to say, as everything follows some physical law which we may or may not know.

Therefore, we say things are truly random which may not be. A coin flip is very random, even though if you knew all inputs you could theoretically calculate exactly how it would land. Something like nuclear decay is incredibly random, and I'm not even sure if anyone knows how to predict that.

While it may not be true in the absolute strictest sense of the word (due to as-of-yet unknown physical laws), we can reasonably say that phenomenon such as nuclear decay are functionally truly random.

Hopefully this is just enough detail to help you out.",1533765167.0
themuffinking,"What they mean in first-year CS classes when they say ""computers can't generate random numbers"" is actually ""computers are deterministic"". Deterministic means, exactly, *it is always possible to determine their output given their input*. Anything that looks nondeterministic either: 1. has inputs you didn't think of, or 2. includes some source of nondeterminism.

The reason they're telling you this is because understanding CS requires understanding that there is no hidden magic waiting at the bottom. First-year students really have to grasp that the field they're entering *covers the whole thing*. It's all just code, all the way down to transistors, and we know how transistors work, and they work that way because people designed them to. And the code that runs on those transistors works the way it works because people wrote it that way, and the code built on top of that works because people wrote it that way, and so on all the way up to the present day and *you* writing code. So when you encounter something behaving ""randomly"" in your program: assume it's not random. Assume something, somewhere, is getting different input. 

In theoretical CS, that assertion is really true. The theoretical Universal Turing Machine can't generate random output at all. Of course it can't, because you could run the same program with the same input a billion times and get the same output a billion times, no matter what. You'd have to attach an actual source of randomness if you wanted random output, and even then, it's really just serving as a new source of input.

In practice, computers have things like I/O lag and clock jitter that can introduce nondeterminism, and you can access that nondeterminism in code (you can even read it out of /dev/urandom). So physical computers *can* generate nondeterministic, irreproducible, ""real"" randomness.",1533769235.0
bigblackcuddleslut,"If you think of random like heat... as being a quality a sequence of numbers can exhibit.  ""Randomness""

A nuclear physicist may say an oven can't produce hot items.

While a cook would say it can.",1533769938.0
jokoon,"Computers cannot generate random numbers because computers are designed to behave in deterministic ways.

Also randomness means something that cannot be anticipated, calculated or understood. A little like a chaotic system.",1533770126.0
dominic_failure,"From a practical point of view:

Statistically Random - a sequence of numbers that all have the same chance to occur, but the current and past numbers have no influence on the next number in the sequence. (yeah, it's terrible, so sue me)

A truly random set - even if you know your position within a random sequence of numbers, you have no way of knowing what numbers will come next.

A pseudorandom set - if you know your position within a sequence of pseudorandom numbers, you know what numbers will come next. Also, if large enough, the sequence will wrap around to the beginning again.

For an example, if I take Python's pseudo-random generator and seed it with a set number (thus putting myself at a known position within the sequence of generated numbers), I will always get the same sequence of numbers following.

    import random
    random.seed(12345)
    random.randint(1,100)
    => 42
    random.randint(1,100)
    => 2
    random.randint(1,100)
    => 83
    random.seed(12345)
    random.randint(1,100)
    => 42
    random.randint(1,100)
    => 2
    random.randint(1,100)
    => 83

This happens for the computer's random number generator (i.e. /dev/random) as well, however the position within that sequence of numbers is changed by sources of entropy. A terribly incorrect example - whenever the letter 'i' is typed into the keyboard, the position within the number sequence is incremented by two. Whenever the letter 'e' is typed, it's incremented by five. Apply this to all possible inputs to the computer, and you get /dev/random (more-or-less). That means that it is theoretically possible to predict the next number if you know the entire state of the computer, but that's impractically hard to do. Hard enough that an outside entity (that's us) can't ever predict the next number to come out.

What does this mean from a practical point of view? Use the OS provided sources randomness whenever you don't want a repeatable sequence of random numbers (cryptography, etc), and constant ""seeds"" if you do want a repeatable, but statistically diverse set of numbers (such as a seed for a randomly generated videogame map you want to be sharable).",1533774829.0
atoponce,"You might want to check out r/RNG. I'm the moderator there, and it's specifically devoted to all things randomness. Hopefully, we can answer your questions there, and you can find some stuff that interests you.

However, I'll answer your question:

> When I was taking my first year of Computer Science, we were told that ""Computers can't generate random numbers"". 

This isn't true. Computers can and do generate ""true"" randomness, in CPU jitter, temperature fluctuations, user input timings, disk IO, and a number of other hard-to-predict chaotic or quantum mechanic noise. The problem hinges on the operating system sampling this noise correctly, extracting the raw entropy from the noise, and using it as a source of randomness. Every major operating system does this in kernelspace. Microsoft Windows provides this functionality via the CryptGenRandom API call, and macOS, Linux, and BSD provide `/dev/random`, `/dev/urandom`, and appropriate system calls.

Off course, that the noise is collected into an entropy pool, where it is then fed through a randomness extractor, and used as a seed for a cryptographically secure pseudorandom number generator, of which continuously provides pseudorandom output to userspace. But the core thing here, is that the hardware running the operating system is exhibiting noise, and that noise is being collected and extracted in kernelspace for use in userspace. But, once sufficiently seeded, the kernelspace CPRNG will almost indefinitely produce random data that is indistinguishable from perfect randomness.

It's further not true that computers cannot generate random numbers, in that we have all sorts of high quality pseudorandom generators, pseudorandom functions, and pseudorandom number generators, that although 100% deterministic, provide randomness that passses suites of statistical tests for randomness. Mersenee Twister, xorshift128+, LCG, MCG, PCG, block ciphers in counter mode, stream ciphers, even hashing functions, are all deterministic functions that are or can be used as pseudorandom number generators.

When talking about ""random"", it's less important that we're talking about unpredictability, and more that we're talking about uniformly distributed outcomes that are all equally likely, but behave in a chaotic manner, such that behaves like something that is unpredictable, even though it may not be.",1534117453.0
MattiasInSpace,"Setting aside how to define “random” exactly, we do know that the result of a deterministic process is not random.

If they're working correctly, all the building blocks of a computer are deterministic: the clock, the logic gates, the cache, the disk, the buses, etc. It is impossible for a system built out of deterministic building blocks to give a non-deterministic result, so the outcome of any computer program is exactly predictable from its instructions and data.

That is what is meant by “computers can't generate random numbers”. For an output to be random, it has to be impossible to exactly predict from the input, and computers are predictable by definition.

There are two families of exception: one when there is a technical failure, like a cosmic ray flipping a bit, and one when the computer is connected to an outside stream of unpredictable information, such as weather patterns (I've even heard of a visual feed of a lava lamp being used). In the first case, the outcome is unpredictable, but it's only because the physical hardware of a computer is diverging from what computer scientists mean when they say “computer”. In the second case, that is a legitimate way of generating random data, but it is not the computer that's doing it, only the external data stream.

However, most programming that relies on random data does not make use of such a data stream. That is because, if randomness is defined as an even, unpredictable distribution, it's possible to generate a stream of numbers that *appears* random to another process via a deterministic algorithm. This is much cheaper and faster than relying on data from the outside world.

Because the process that's asking for the random stream knows nothing about how the data is generated, it can treat it as random and get most of the benefits of using a truly random sequence at a much lower cost. That is what is meant by “pseudorandom”.",1535500540.0
zokier,"Depends also on what you mean by a ""computer"". Pure turing machines are bad at randomness, but real world computers have special hardware for generating true (in pretty much every sense of the word) randomness.

https://en.wikipedia.org/wiki/RdRand",1533711004.0
ncbrown1,"Here's my take.

Classical computers can't be truly random, as all state can be reproduced (I can set your clock, I can copy your code, I can replicate your stack + heap + pc, etc). Since state can be reproduced, ""random"" number computation can not be consistently random.

In reality, there are infinite factors which lead up to the current moment, and there are infinite possible futures. No situation can be reproduced exactly, so randomness in reality is true random and probabalistic. 

If you're interested in random, you should read a book (which I loved) called The Drunkard's Walk by Leonard Mlodinow. He talks about randomness and statistics and probability, and pretty much how randomness rules our lives.",1533707672.0
fiedzia,">When I was taking my first year of Computer Science, we were told that ""Computers can't generate random numbers"". 

Taken out of context, its just wrong. Computers are perfectly capable of generating random numbers, all you need is access to proper source of randomness. Many computers are equipped with one, and if they aren't, one can attached as peripheral device.

What you can not do is to generate random data from device that has nothing random in it.

> If, 3000 years ago, humans couldn't predict the lunar cycles, were they ""random""? 

For all intended purposes, at that time, they were (assuming they really couldn't, I am not sure about that).

> And if we do understand them now, did they become less random over time as our understanding of them became clarified?

Yes, any 3000 year old encryption method based on lack of knowledge of lunar cycles will not be very useful today.

> Darwin claims that one of the main mechanics for evolution is ""random mutation"". 

We don't know enough about mutations to know how random they really are. We found some that are less random than we previously thought, so there is a chance for more surprises.

What you are missing is the usecases for randomness. We don't have it for the sake of it, we use to
various purposes (encryption, bias removal, data distribution etc.). What matters is if it works and what guarantees do we have about that, not the definition.

",1533736410.0
pratik_mullick,"Here is my take on randomness. There is nothing truly **random** in the true sense of the term, since even highly random phenomenon (weather cycles, stock patterns, even quantum states) exhibit a relationship with other underlying variables, and show a **pattern** over the course of a large number of iterations. The randomness is dependent upon the ability of the observer to process the data and the entire data frame. The greater the number of variables introduced into the system, the lesser the randomness perceived for the entire system (considering a closed environment).

Coming to computer science, almost all **random generators** are purely pseudorandom in nature, due to the fact that they use mathematical models to derive a random value. Hence, given enough iterations, those random values will eventually form a pattern (Sometimes the pattern can be greater than the lifetime of the universe, but that is a topic for a separate discussion!)",1533709605.0
echo_oddly,"Let's dive a bit into quantum mechanics and what it means to take a measurement. I'm going to start with a basic concept and work from there to my main point.

###Basic setup

Suppose you have an particle in your experiment which, after you measure it with some apparatus, will either be spin up or spin down. Note that I said after you measure it. Before you measure it, NONE of the following are true:

* The particle is spin up
* The particle is spin down
* The particle is both spin up and spin down at the same time

What is true is that there is a quantum state `|up>` and a quantum state `|down>` and the state of the particle is in a linear combination of the two, `|state> = a|up> + b|down>` where `a` and `b` are complex numbers such that |a|^2 + |b|^2 = 1.

The probability of measuring the particle in the up state is |<up|state>|^2 = |a|^2 . The notation may not be familiar but what this means is that we take `|state>`, project onto `<up|` like a shadow, and measure the length of that shadow, which in this case is |a|^2. `|up>` and `|down>` form what is called an orthogonal-normal (orthonormal) basis, just like x and y unit vectors on 2d plane. The quantum states, however, are complex valued.

###What is going on

From the experimenter's perspective, the process of measuring the spin is entirely non-deterministic. You might think, maybe the particle has an internal note which has either `up` or `down` written on it and that determines which outcome the particle will display. Nope. That ends up voiding other assumptions I've implicitly made in this discourse. See [Bell's Theorem](https://en.wikipedia.org/wiki/Bell%27s_theorem). The particle really is neither up nor down. The particle IS the superposition of up with down.

###What about multi-worlds interpretation?

An interpretation of quantum mechanics is how a person can relate the physics of quantum mechanics to a system of philosophy, and what that means for questions about free will, determinism, reality, knowledge, etc. An interpretation doesn't really offer any predictions but it does relate to the discussion.

###Free will

A very exciting result in quantum mechanics is the [Free Will Theorem](https://en.wikipedia.org/wiki/Free_will_theorem). What this boils down to is this: assuming 3 simple and reasonable axioms in physics, if experimenters have free will, then the particles in the experiment also have free will. In a sense, this result is evidence for the existence of free will. Which is not the same as randomness but it is related. John Conway gave a few lectures on the topic which are very interesting.

###Conclusion

This comment is kind of a ramble, but my hope is that it answered a few questions. Questions about determinism vs randomness vs free will strike deep at our understanding of physics. We have some answers and limitations to apply, but there are still many open questions.",1533730408.0
Arancaytar,">""Computers can't generate random numbers"". 

>Do you agree with that statement? 

This depends on how you define ""random"" and ""computer"". No, the idealized model of a Turing machine can't perform any computation that doesn't depend, deterministically, on the input. It can use the input to generate bits that appear unbiased and without a pattern, but all identical Turing machines given the same input generate the same numbers.

In a real computer, this still sort of applies, but ""input"" gets a lot fuzzier: It now includes data from any sensor that the computer possesses, and these produce noise: Small variations in temperature, microsecond timing on keyboard/mouse events, radio noise from the wifi antenna, literal noise from the microphone, etc.

In practice, ""random numbers"" just need to be generated in a way that a potential adversary could not guess, even if they know the code and almost everything one can reasonably know about your system.

For example, if you base them on the millisecond timestamp at the time the function is called, this might be enough for the RNG in a game - but when generating a secret key, it would allow an attacker to test the relatively small space of timestamps (86,400,000 if the attacker knows what day you generated the key, less if they can guess the approximate time).

On the other hand, it would be impossible for an attacker to predict the exact digital noise produced by your computer's sensors at a particular time. For practical purposes, that is ""random enough"", even if we live in a completely deterministic universe where true randomness does not exist.",1533733893.0
foreheadteeth,"> Randomness is the lack of pattern or predictability in events.

[(Source: Wikipedia)](https://en.wikipedia.org/wiki/Randomness)

Some computers have [genuine random number generators](https://en.wikipedia.org/wiki/RdRand) (or [maybe not](https://www.theregister.co.uk/2013/09/10/torvalds_on_rrrand_nsa_gchq/)).

[Pseudorandom number generators](https://en.wikipedia.org/wiki/Pseudorandom_number_generator) are predictable, which is probably what your professor was saying.",1533735616.0
philthechill,"Some people mean ""a sequence generated by this method will match a common statistical distribution of ""random"" numbers.

Other people mean ""the next number in the sequence could not be predicted by an attacker who knows everything except the current internal state of the RNG"".

Those are the two most common meanings when we talk about getting random numbers from a computer .",1533735888.0
MjrK,">""Computers can't generate random numbers"". Do you agree with that statement?

Instead of talking about randomness, I think it's more intuitive to talk about predictability. In which case, randomness would just be a measure of the lack of predictability. An important point to note then is that the predictability of anything depends on what someone knows about the situation (or in generalizing, what is possible for anyone to know about whatever we are talking about).

Isolated-computers are completely deterministic; if you know everything about some initial state of an isolated-computer, you can predict (simulate) every future state of that isolated-computer.

Open-computers are deterministic to the extent that you know what input they have gathered (or in generalizing, the extent that you are able to know what input they will gather). The less information you know (or will be able to gather) about what input they will acquire, the less you can predict the behavior of the open-computer.

If you know how the open-computer is gathering new information from the environment and you can monitor or predict this new information, then this will allow you to predict how the computer will behave. The amount of information that a third-party has about the state of the computer and the environment determines how random the computer will seem to that third-party.

It isn't possible to make an absolute statement about how random a computer might seem to all possible third parties, without being more specific about what third-parties you are talking about and what they do / will be able to know / predict.

However, if the open-computer is gathering information from a source that is extremely difficult to predict (ex [quantum randomness](https://en.wikipedia.org/wiki/Hardware_random_number_generator#Quantum_random_properties)), then we have a lot more confidence that it will be difficult for a third part to predict the behavior of the computer.

For a mathematically rigorous explanation for why you can't characterize the absolute randomness of a string without being explicit about the third-parties you are concerned with, refer to [Kolmogorov complexity](https://en.wikipedia.org/wiki/Kolmogorov_complexity).",1533743971.0
Quintic,"I would define true randomness as anything that is not deterministic. Thus it is important to define the concept of determinism. For example, I might say a system is deterministic if it is possible to define a function that takes the current state of the system, and outputs the next state of the system.

Any system that such a function can be defined, we will call a ""deterministic system"", and any system that is not deterministic, we will call a ""random system"".

I think it is a deeply philosophical question on whether or not random systems exist in nature. Note that my definition is independent of us being able to construct the function. It only needs to be possible to construct such a function.

Now given that we don't know if actual random systems exist. From a practical standpoint, I still want random numbers. In this situation, a sufficiently unpredictable number tends to be sufficient. It is my understanding that this is what computers do.

I think computers collect measurements that they can reasonably assume are not easy for others to measure, and add that to a pool of ""randomness"". From there, they use that data to seed pseudo random number generators (which are completely deterministic), to give numbers that behave how we expect random numbers to behave.",1533751471.0
mremachine1,Any computer generated code will follow the pattern that the code instructs and would not be random. You could hypothetically find a way for a computer to sense input from an actual random process and use that information to generate randomness digitally. Here is the wolfram [definition](http://mathworld.wolfram.com/RandomNumber.html). It also gives a little information on psuedorandom ,1533757355.0
ModernRonin,"> ""Computers can't generate random numbers"".

I would amend his statement to say ""*software* can't generate *truly* random numbers."" There will always be some kind of a pattern in software-generated generated random numbers. No matter how slight or difficult to detect. (Though in practice, the psuedorandom numbers generated by heavily-tested algorithms are good enough for nearly everything.)

As far as your conceptual question... My AI prof in college told me that one of the easiest ways to think about randomness is in terms of ""surprise"". For instance, suppose you have something that generates a stream of bits. The more surprised you are at the pattern of bits that comes out, the more random it is.

Say the stream of bits is 111111111111. You aren't very surprised. They're all 1s. You wouldn't be very surprised at a string of all zeros either. Now say the string is 10011001100110011001. This is slightly more surprising, but there's an observable pattern - ""1001"" over and over again.

Now what if the stream of bits is 1111101101001. (Taken from https://qrng.anu.edu.au/) It's fairly surprising. No obvious patterns. And just try to predict what the next bit is going to be!

[ParanoydAndroid's comment](https://www.reddit.com/r/compsci/comments/95j4zl/struggling_with_randomness/e3top1f/) explains this in more detail.",1533759595.0
drvd,"> ""Computers can't generate random numbers""

The truth of this claim depends on three things: Your notion of 1) ""computer"" and 2) ""random number"" and 3) the plural in ""random number*s*"".

* A touring machine can cannot produce something humans would consider a sequence of truly random numbers.

* An actual computer connected to a geiger counter (or something else fundamentally QM
in nature) has no problems generating of truly random numbers.

* A touring machine can produce a short (!!) sequence of random numbers. E.g. the sequence <7>.

* A touring machine can produce a long sequence of pseudorandom numbers.

* A touring machine is not able to produce any random number.

While these sentences seem to contradict each other they are all true depending on your definition of random numbers. Sequences of Pseudorandom numbers achieve very high randomness if you look at one (not too long) sequence only. Unfortunately Touring machines have no way of random seeding such a sequence so while the sequence ""in it self"" is random it is deterministic. 
",1533717496.0
rectiddly,"My opinion: Absolute randomness can't exist in a deterministic universe. Any time we find a process that produces different results based on what seems to be magic, there is scientific work ahead of us. The last stop could be mechanisms that are not reproducible/observable/understandable by us before we go extinct.

Here is the question on Ask a Mathematician:
http://www.askamathematician.com/2009/12/q-do-physicists-really-believe-in-true-randomness/

I also found the comments on that page to be very interesting.",1533728056.0
renmana74,I think there is only pseudorandom. All actions are an inevitable result of complex chemistry and physics. For example if you roll dice the instant they leave your fingers the numbers they will land on is determined by physics and the  actions leading up to rolling the dice are a result of your brain chemistry sending signals to your muscles. If you follow this logic back far enough it ends up concluding that there is no fate and all being is tied to the history of the stars and this is a thought I struggle with frequently. ,1533732716.0
WiggWamm,"all that it is saying is that computers have finite memory, so they can’t choose from all numbers when they pick a random one (because computers don’t have enough memory to hold infinite numbers). Don’t think too hard about it",1533745984.0
swardson,Nice editorialized title OP but the guy hasn't worked for Microsoft in 5 years.,1533707730.0
WhackAMoleE,"I tried to read the paper but got the blue screen of death instead.
",1533746098.0
My_reddit_throwawy,Can it predict the spread of antivaxxers and the subsequent increase in deaths?  Nice work btw,1533716214.0
DontSingTheSongs,"Depends what your goals are, currently ‘Big Data’ pays quite well so I’d do “Parallel and Distributed Computing”, “Machine Learning”, “Data Warehousing and Mining”. 

It’s up to you and what you’re interested in specializing in",1533645516.0
Jaxan0,"They do not give more computational power as we can simulate such a network on a single machine (for example a Turing machine).

However, I believe there is work on theory of execution and theory of interactions. Both of which talk about (infinite) sequences of messages instead of a single output.",1533637348.0
xasteri,"I'll try a different approach. I'm going to keep it super high-level without definitions, proofs and/or rigor. If you want more details please feel free to ask.

As previous answers stated, there will be no difference in **computability**. That is, the set of functions that can be computed by a network of computers, is *exactly* the set of functions that can be computed by a Turing Machine.

You can ""visualize"" this by imagining a multi-tape Turing Machine. On each tape you can simulate the computation of each computer in the network. It is very easy prove (see Sipser's textbook) that any multi-tape Turing Machine is equivalent to a single tape Turing Machine.

In terms of **efficiency** things are a bit different. For example we have the complexity class **IP** which is the set of problems that have interactive proof protocols. In this case you have 2 communicating machines: Prover **P** (usually of unbounded computational power) that wants to prove a statement and Verifier **V** (usually a probabilistic poly time machine) that verifies whether what **P** is saying is true. It is proven (again, I recommend Sipser for an easy to follow proof) that **IP=PSPACE**. That is that the class of problems that have interactive proof protocols is exactly the class that can be solved in polynomial *space*.

Now, **PSPACE**  is a pretty big class which shows that adding interaction to our computation, actually offers us a lot of benefits.

I could go on about other interactive classes but I think it goes beyond the scope of your question. If you are interested, check out the **Arthur-Merlin** games. By the way, adding stuff to computation doesn't always give us an advantage. For example, it is widely believed that, from a complexity theoretic point of view, randomness is useless, aka **BPP=P**.",1533841029.0
Jaxan0,"You could make an animation where you slowly change the shape of the triangle, and see how the path changes. A bit similar to this:
https://en.wikipedia.org/wiki/Logistic_map#/media/File:LogisticCobwebChaos.gif",1533637885.0
stevenxdavis,"It's an interesting suggestion that I haven't really thought about before. I think one approach would be to incorporate ethical considerations when discussing case studies in courses. An example that came up in one of my classes was the [Morris worm](https://en.wikipedia.org/wiki/Morris_worm), which like a lot of early malware was an academic experiment that got out of hand. Thirty years later, we still face the same kinds of problems, e.g. the WannaCry ransomware attack. Connecting CS to real-world consequences would make the ethical aspects more clear. 

It would also help for teachers to explain *why* things happen, because so many news stories say things like ""Company X got hacked."" and leave it at that. It's critical to understand that sometimes the problems are with the machines, but more often they are due to human-centric attacks like spearphishing.

In the brave new world of constant government surveillance, it would help to discuss the role that technology plays in law enforcement and to encourage people with technical skill to engage in the political process.",1533600713.0
jeffbell,"In the case of privacy leaks at Cambridge Analytica, it was an oversight in the terms of of use.  thisisyourdigitallife was allowed to collect data from your friends as well.  The terms of use were drawn up by lawyers, and lawyers are required to take ethics classes, so if classes are the answer then it must have been OK.

I've seen long [discussions about the ethics of self driving](https://www.usatoday.com/story/money/cars/2017/11/23/self-driving-cars-programmed-decide-who-dies-crash/891493001/) cars and harm avoidance that correspond to the ""trolley problem"".  If a self driving car gets into a situation where it will probably hurt someone, how should it respond?

In contrast, the current state of the art is to scream and lock the brakes.  The state of the art I'm referring to is human drivers.

How much higher should the standard be for software?

If we manage to get the accident rate to be 90% lower than humans, is it still ethical to allow human drivers?",1533597011.0
thbb,"Just visit /r/computerethics there is a lot to read on the topic. 

The ACM just updated its code of deontology & ethics, and courses in ethics are now mandatory in PhD programs in Western countries.

As a teacher in the area, one of the main problems is that now trainees and interns have more competence on proper conduct and protocols than seasoned professionals.",1533596347.0
bemend,"Having a code of deontology can be seen as a move for professionals to control what each other are doing, to exclude unqualified people so as to obtain a monopoly over an activity, and to gain the trust of the public and make the profession prestigious. Typically, lawyers and physicians. Maybe programmers don't need a code of deontology yet because the salaries are still very high compared to other occupations and it has a lot of prestige, so few people feel the need to start regulating the profession and how it is seen by outsiders. If programming keeps getting more and more taylorized, if there keep being scandals due to poor practices, and if people start being more demanding towards software they use or have to suffer against their will, things may very well change.",1533595789.0
Bromskloss,Where would it end? What activity would _not_ have a board of moral exemplars?,1533606258.0
,"because computer science is the biggest, newest way we are getting cocked in the ass and learning to love disgusting things. Once we are all forced on the cloud we will then have to listen to an ethics board. For now, we can enjoy the big corps.",1534010013.0
,[deleted],1533596557.0
MostlyHarmless___,"I'm assuming you've taken the standard data structure and algo classes. For real experience:

OS and Compilers

Very useful: Networks",1533595325.0
trout_fucker,"Data Structures is the single most important class in college. It is probably the only one that will be consistently useful throughout your career, no matter what industry you get in to.",1533594335.0
oakles,"Data Structures, Algorithms, Computer Systems (low-level programming, memory, etc), Operating Systems.",1533597636.0
wayoverpaid,"Data Structures and/or Algorithms.  Sometimes these are the same course ""Data Structures and Algorithms"" at two levels, or sometimes they are split.

No matter what, you need these.  And they are generally useless without one another.   No point knowing that a hashmap has constant time access without understanding what that means, or understanding why the ability to keep a B-tree sorted without an O(n) insertion is a big deal.

These two give you the foundations of all computer science which will likely last longer than any specific language or knowledge.

But you said third year, so I hope you already took these two.

The most useful course I took after that was a low level C programming course, where I spent a lot of time hacking away with valgrind to debug memory leaks.  Useful skill if you need to go low level.

If you have a machine learning course, I would advise you to take it, it's the future.",1533595382.0
AsiansInc,"In my opinion the most useful classes I ever took were Database Design classes. I am defining ""useful"" as something I learned that I reflect back on the most.",1533596047.0
LagrangePt,"One of the courses I remember and value most was a course that went from transistors up to assembly coding, and had coding projects at each level up the stack.  Hugely valuable to have an understanding of what is happening from the atoms in the chip all the way up to compiled c++ code.

I really regret not taking classes on: Compilers, Operating Systems, and Networking.

I see a lot of programmers who have no background or understanding of how gpu's work and how graphics are rendered - if you're going to be working in anything related to that field, it'd be a good idea to take a class or two on it.",1533606451.0
khedoros,"Algorithms and data structures. Whatever course teaches you about runtime complexities of algorithms (we touched on it in my A+D courses, then went further in depth in a later course). Compilers (lexing, parsing, grammars, and so on). Depending on the software you want to work on, networking would be really useful. Operating Systems would help demystify the system further. Understanding system calls, virtual memory, and generally the way an OS works can be applied to troubleshooting your programs sometimes.

Beyond that, if they have an actual ""software engineering"" course, that ought to cover the differences between a few different development methodologies, and hopefully would hit on revision control. For the most part, I had to learn those on the job, but they would've been good to know before I started.",1533598684.0
markth_wi,"It's a heresy but I'll say 5 or 6 classes but my having found out about the various subjects in these particular classes in NO way means you will find these subjects or in part or whole. Some classes may be combined or some sub-component may stand completely as a separate class.

I've found repeatedly high value in knowing information presented in these classes above all.

- Data Structures & basic algorithms - Lists, pointers, arrays, popping/loading a stack, managing a queue, traverse a tree, the BASICS of algorithms, sorting

- Data structures and advanced algorithms - Recursion, looping, advanced sorting,efficiency/resource use, hashing, graphs, dynamically linked lists ,agent design, weighted networks, neural networks

- Database/ relational design - how to create indexes, databases, organizational design, create a small stack, interface with things, store,process,report on data.

- Operating Systems - Basics of I/O, memory utilization, stacks,queues, inter-process communication, process threading, memory sharing/violation/usage and management,BASIC systems operations, tools for observation.

- CS/Operational Statistics - calculating basics , Z, averages, weighted averages, sum of squares, normal vs. non-normal data, Gauss/Bayesian Logic, Combinatorial Math, Odds/and Probabilities, ANOVA

- Research and Application Development - SPC, BASIC Operations Analysis vs. Business vs. Research applications, Code management/Revision Control, Differential Comparison, BASIC DOE/Experimental Design, Testing Methodology/Tests and Experiments, Estimating/Observing/Assessing.",1533599332.0
electricmammoth,"This question gets posted a lot and everyone says Data Structures / Algorithms. Yeah that stuff is important but they are required classes almost everywhere. So far in my short career my Software Quality Assurance class has come in handy the most often, followed by Software Engineering. It's much more useful to understand the jargon your product owner speaks and be somewhat familiar with Agile than it is to know the best and worst case run times of heap sort. ",1533607603.0
RainbowDasher,"Funnily enough Software Engineering class, I had no idea what I wanted to do with my CS degree but then I took SE and found that I really enjoyed working with a group to deliver something ",1533602077.0
MarsLanded,"OS, algorithms, distributed systems if you can.",1533599885.0
vaelfyr,"Computer Systems (sometimes bundled into an OS class but we didn't have one). Mostly because every time I run into a knowledge gap, I vaguely recall the topic from the class just not the details.",1533603809.0
boottrax,Computer Architecture and/or Compilers.  Without taking Computer Architecture you are a programmer.  The most successful and talented software engineers are those that understand what they are designing runs on a real hardware machine.  ,1533619158.0
valondon,Data Structures is very essential to CS and a career in programming. However operating systems taught me how to actually use those data structures to solve problems.,1533596728.0
Erwin_the_Cat,Take whatever interests you most and make sure to work on side passion projects outside of what school requires. This is really the only way you'll leave college being significantly more talented than the average grad. It's cliche but true.,1533602701.0
Askee123,"We had this programming languages core class that threw 2 super obscure languages at us, then an object oriented and functional language. 

We had to learn them all quickly and were tested in some brutal exams.

If anything, it taught me specific language proficiency isn’t as important as people make it out to be.

",1533604329.0
GNULinuxProgrammer,"Data structures, Algorithms, Operating Systems. If you'll be a data scientist then Machine Learning, linear algebra, probability theory and optimization.",1533608485.0
valriia,"Design Patterns. I didn't take it in uni, though I had that option, and I regretted later. It's a fundamental way of thinking, you need those kinds of courses.  
  
Programming Languages Categorization - if you have any kind of course that compares different types of languages, different categorizations, those are very useful. The languages change in time, but the categorizations still remain useful to understand them better.  
  
Also, while in uni, push yourself to your limit to take math courses. Because the more math you know, the more doors can remain open for further specializations; and math greatly benefits from organized education, rather than self-training. Even things like Design Patterns and PL Categorization you can learn on your own later, but math is much harder to learn this way.",1533634130.0
agumonkey,"it's a bit too opinionated and subjective :

- fp for its emphasis on inductive thinking (said this often but it made my brain 10x more capable at solving problems because it gives them a tiny finite description)

- interpreters: you have induction from above, now you can encode tiny recursive algebras to talk about your problem

- compilers: you have high level encoding from above, now you write a program that map what your interpreter choreography to hardware and fill in the precise low level details. It's very freeing, you do whatever you want or need with the machine.

- a bit of discrete math, because you'll like being able to cut through structure in terms of numbers too (combinatorics)

- prolog: bidirectional programming, near pure, terse, very different way to understand the word programming (https://www.metalevel.at/prolog/)",1533638748.0
SanityInAnarchy,"Essential for what? I can think of three possibilities:

1. Essential for getting hired
2. Essential for being good at your job (once hired)
3. Essential for understanding what computing actually is, and what all of this actually means

My answers for these are pretty wildly different. I'd say, if you have time, you probably want to do all of these at some point -- #1 and #2 are important for your career, obviously. #3 isn't, but if you think you'd be interested (and it's interesting stuff), university is probably the best place to learn it.

---

**Essential for getting hired:** Honestly, whatever's easiest and fills a requirement. You need to pass, you need a degree, and it can't hurt to keep your GPA high. If that's what you want recommendations for, if linear algebra can fill any requirements for you, do that -- it tends to be shockingly easy for comp sci students. Beyond that, it probably depends too much on your school's comp sci program.

I'd put algorithms and data structures in #1, but any decent comp sci course will require data structures. I guess algorithms, if that's somehow an elective, but it shouldn't be.

---

**Essential for being a good software engineer:** Anything that involves substantial amounts of actual programming, or other hands-on experience with tools you'll end up using day to day. A team project course is probably the best, if you get a good team, but a bad team can be problematic here. Other than that, try a course in programming languages (or compilers), OS design, or *maybe* databases.

If you have free time, learn Git. Learn more Git than you think you'll need, so you don't end up like [Randall here](https://xkcd.com/1597/). The basic ideas should translate to whatever you actually end up using, and it's pretty amazingly useful for school projects, too. And you want Git specifically because it a) is insanely popular (see: Github), and b) you don't need a server to get started, it'll just run on your laptop.

---

**Essential for understanding all of this:** So, so many options here. These classes taught me how a computer works:

* Physics -- I took two semesters, and the second semester focused on electromagnetism, so you get some idea of how a transistor actually works. (Plus you learn a bunch of other stuff about how other things work, of course.)
* Digital Logic -- probably an EE course, not comp sci -- this is like the first half of [nand2tetris](https://www.nand2tetris.org/), only it starts with transistors, not NAND, but it ends up in about the same place.
* Machine Architecture (the name is probably different) -- this was a low-level course that taught mostly assembly language, but also taught about things like instruction pipelining. You can barely spot a seam between this and Digital Logic -- the final project of Digital Logic is the simplest possible CPU that machine architecture talks about.
* Any good course on C, or C++, or both. You want to know these anyway, but I ended up with a class that also talked about things like how garbage collection actually works, or how you might implement `malloc`.
* Operating Systems -- the other side of this: How do you construct the environment that those C/C++ programs run in?

Aside from this, Theory of Computation is essential for understanding what computing actually is, in a mathematical sense, including what sorts of problems are even feasible to solve with *any* kind of computer. Stuff like P-vs-NP, turing machines, regular expressions, and [why you can't parse XML with regex](https://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags).

I found compilers to be fun, but not quite as essential, since most of us don't write compilers, and it's almost always more important to understand what your program is doing, rather than what the compiler is doing. Also, nand2tetris filled in some gaps on how stack machines work, and gave me some idea why so many bytecode engines use them, so it might be worth doing even though it has a bunch of overlap with the above.",1533641706.0
CSMastermind,"I'll take it in a different direction: Discrete Math.

That's the stuff you'll learn during a computer science degree that you won't learn on your own.  

I get why everyone is saying Data Structures and Algorithms.  Both those classes are super important but, in my experience, you learn all the data structures and algorithms you need to know just by programming on your own for a little while.  

Discrete math, on the other hand, can really change the way you think about software and it's not something you'll just bump into unless you get a CS degree.",1533648055.0
montibbalt,"In terms of Computer Science? Data Structures.  

In terms of actually working with other people someday? Take a Technical Writing course",1533651568.0
DarkDaemon,"Robotics/mechatronics, or some other applied course. If you're going into development, classes tend to provide an ""idealized"" view of engineering. Something like a combined mechatronics class can give experience with real-world problems, and a primer to electrical engineering, physical devices, microcontrollers, etc.",1533600187.0
EdgarVerona,"To me, it was Data Structures.  Not because the direction you go in will necessarily require you to *re-implement* any of these specific structures, but because it teaches a lot about the problem-solving mind and approach of computer scientists who went before you.  It also teaches you about algorithm efficiency analysis, which will prove more practically useful on a day to day basis.

The pace is fast, the content broad, and at least where I went the professor was unforgiving.  It was a real trial by fire, but in a positive way.",1533607437.0
AaronKClark,"While I think DS&A is important, I think Discrete Math is equally important. Learning to wrap your head around Truth Tables and long chained AND OR NOT statements is pretty invaluable.",1533611657.0
dusklight,"The compilers class. You should do it in year 3 because it is usually a very hard class and it will be too stressful to do it in year 4. 

I found the compiler class to be super valuable not because you are likely to be writing a compiler after you graduate but because it teaches you the processes by which higher level programming code becomes low level machine code, and by writing a compiler you are actually doing it, you are not reading about the bicycle but actually riding the bicycle. By writing a program that takes an idea at a higher abstraction and breaks it down into pieces that can exist in a lower abstraction, I started to understand how that process first happens in my head, when I would take an idea that could be expressed into words and then break it down into (in my case) java, and before that, when I would take an idea fresh out of my mind and break it down into words. ",1533615086.0
groggyjava,"stats and linear algebra are the underpinnings of all the tech that makes headlines, so make a point of not just passing the classes, but really understanding them, and good paying jobs will find you. (seriously)

* design & analysis of algorithms
* data structures
* some kind of software engineering course
* math: 
   * calculus, 
   * probability and statistics, 
   * linear algebra
   * discrete mathematics",1533656718.0
InTheFiveByFive,"Linear algebra.  The world needs another sort or tree implementation like a hole in the head, but new ML algorithms, which are heavy in linear algebra are needed all over.",1533606656.0
stochastaclysm,"Algorithms and data structures, discrete maths, low level programming (probably C). Then loads of actual building stuff in general purpose languages.

Also: 
- Requirements
- Agile
- Testing
- Architecture",1533594713.0
davecrist,Public speaking and business writing.  As a manager with more that 10 years of technical development experience I am often frustrated by working with brilliant devs who are completely unable to communicate their ideas well ( therefore lack the ability to positively influence the technical implementation of the products we mKe) and/or work effectively with non-technical people to figure out what they want in a way that the user can easily use. ,1533617433.0
skulgnome,Combinatorics.,1533604508.0
mfiels,"Data Structures and Algorithms
Operating Systems
Networks",1533605704.0
IronTooch,Analysis of Algorithms was really solid too,1533607201.0
spinwizard69,"I guess the number one thing here is that it depends upon the type of software engineering you intend to do.   However the overwhelming advice on data structures and algorithms is very  important and should not be ignored.

Any classes that deal with low lever computer programming is also of high value in my mind.   You would be surprised at how much trouble some graduates have with communicating over a serial port using OS supplied facilities.   Ask them to write a driver and it is all over.   Allied with this wold be courses focused on micro controllers and other low level devices.

You need to follow your interests in regards to choosing the CS courses but don't forget math.    You need or likely will have requirements to fill but even after that is taken care of take at least one math series a semester.   It will provide a good background and keep you from getting hung up on a job assignment that comes out of left field.   Statistics is often mentioned but keep an open mind.",1533607704.0
RumbuncTheRadiant,"Symbolic Logic.

You won't find it in the CS department, try maths department, or bizarrely enough, philosophy.",1533607958.0
Nodebunny,"CS 101 -- aka Data Structures

CS 102 -- aka Algorithms

also Computer Architecture (maybe)",1533609638.0
PM_ME_UR_OBSIDIAN,"In my experience, from most important to slightly less important:

1. Whatever your functional programming class is called (""programming languages"", ""paradigms"", etc.), to teach you control flow and principled thinking about code.
2. Data structures and algorithms, probably more useful than functional programming but it's eminently easy to learn via self-study, you don't need a prof for this.
3. Operating systems, databases, telecom networks/distributed computing; to have a holistic understanding of the systems you're working with.
4. Compilers, or some other large project class.",1533609790.0
dwkeith,"Prefix this with the fact that I am a high school drop out and former senior engineer at Google/Nest/Apple. (Now Eng Lead @ Emerald Cloud Lab, we are hiring)

On the job experience matters most, in school you can get this by working on open source, non-profits, and internships. Masters level CS projects may qualify if they have well written source code.",1533622035.0
tetroxid,"Data structures and algorithms, and database design.",1533624463.0
,[deleted],1533624807.0
deltaSquee,[Systems engineering](https://en.wikipedia.org/wiki/Systems_engineering). Good project management is absolutely crucial for good software.,1533626392.0
Markus_Antonius,"Dissenting somewhat here by the looks of it but especially OS and in particular memory management. I've seen the most horrendous results from not understanding these. If you have no clue as to what is happening at this level you're not someone I would hire. Know memory management and I/O and understand how modern programming environments still depend on those even if it seems all that is something you no longer need to understand. Abstraction is only useful if you understand what is being abstracted. If not, it's just a gateway to mordor 😉",1533626591.0
ryani,"I think a lot depends on your interests, strengths, and weaknesses.

For example, for me, Computer Architecture was probably the most valuable CS course.  Before that I kind of understood low-level programming, but during that course it really ""clicked"" for me.  I think Operating Systems and Computer Architecture were the most valuable two courses in hindsight.

But for a different person, I could easily see the answer being Graphics (at my school, was too focused on film techniques for my liking -- I wanted to do realtime), Compilers (didn't click for me until much later in life), or Programming languages (if the teacher is good and you haven't been exposed to many programming paradigms)

And you mentioned, you already took it, but linear algebra gets an honorable mention.  If you really grok how matrices work you can find applications for them everywhere.

Also important, I think, is to find some courses in another discipline that really interests you, and figure out how to apply your CS knowledge there.  For me this was formal logic, which led into an interest in computational logic and proof systems.",1533631802.0
ecb29,"The most fundamental was the course where we built from gate-level logic a five stage 32 bit MIPS processor and simulated it. 

https://www.cs.cornell.edu/courses/cs3410/2018sp/projects/p2/",1533635224.0
alex-o-mat0r,"I guess, in the 2 years you already covered data structures & algorithms. Other than that, I'd suggest parallel & distributed computing.",1533636189.0
ha1zum,"Design and Analysis of Algorithms.

Image Processing.

Operating System.",1533638005.0
nipuL,"As you've said you're entering 3rd year. I'm going to assume you've already done a lot of what people are suggesting. In my degree a lot of it was 2nd or even 1st year. Data Structures, Algorithms, Compilers, Operating Systems should be core for any decent CS degree. Parallel programming and Non-Procedural Languages should also be part of your standard CS education. If you have avoided these, you're already at a disadvantage.  


3rd year subjects should focus on your desired graduation path (in this case software engineering). Obviously take some actual software engineering courses. Being able to write code is a very different skill to being able to design, build, test and maintain a product of sufficient complexity and size. These courses are usually boring as hell, but will be extremely helpful in the long run.  


You'll also want something that allows you to apply your craft in the real world. For me, I found a multimedia unit the most rewarding and challenging (working with encoding and compression algorithms). I also dabbled in electronics, but didn't enjoy the course work.  


Taking some 2nd and 3rd year mathematics course is also a really good idea. I don't think it really matters, just something you'll enjoy. If nothing else, they're just really good for mental gymnastics and problem solving skills. I did multi-variable calculus and number theory and enjoyed them thoroughly (so much that I still consider going back for a 2nd bachelors in mathematics).",1533639210.0
bartturner,"That is a tough one to just list one.

But would probably say data structures.",1533644245.0
WArslett,"I'd second Data Structures, Design Patterns and Algorithms etc. but one which is still surprisingly uncommon is Software Development *Methodologies*. Many universities still don't teach it but when I took my first job out of university they were massively impressed at my grasp of Agile workflows and how to actually get work done objectively. It's one of the biggest criticisms of students coming straight out of university in my experience that they still need to learn this stuff.",1533644639.0
trollpan,"Lots of people have mentioned Data Structure course which I couldn't agree more. Yet, I would argue that a communication course is the most important course for a CS graduate. Just about every CS graduate knows Data Structures and Algo but very few pass out with effective communication skills. ",1533652841.0
Isgrimnur,Business Communication.,1533657243.0
J0hnnySp4rkles,"Im seeing a lack of agile in the comments. I dunno if it's integral to all computing courses or not, it's not the case for the college I went to, but it's a key skill to have as a developer. Being able to work and learn from the team and embracing change is highly desired. ",1533659451.0
dauchande,I would also recommend you learn some functional programming as it is likely the future of programming.  You can do this on your own time reading the SICP book - [https://www.amazon.com/Structure-Interpretation-Computer-Programs-Engineering/dp/0262510871/ref=sr\_1\_1](https://www.amazon.com/Structure-Interpretation-Computer-Programs-Engineering/dp/0262510871/ref=sr_1_1)? and watching the MIT OpenCourseware for the course here - [https://www.youtube.com/watch?v=2Op3QLzMgSY&list=PLE18841CABEA24090](https://www.youtube.com/watch?v=2Op3QLzMgSY&list=PLE18841CABEA24090),1533664689.0
LongUsername,"I'm in embedded so my view is tainted by that:

Best class I ever took:  
Microprocessors. We built a simple processor on a bread board using gate chips, then used another micro to drive the inputs. Taught me a ton about the internals of how a computer actually works.

Classes I wish I'd taken:  
Networks. They are fucking everywhere.  
Compilers.",1533676444.0
Blue_Q,"In conjunction with my university course in Algorithms, Formal Proofs and Data Structures I learned most by building an interpreter which can evaluate simple mathematical expressions. It taught me essential things: Formal language theory to efficiently tokenize and parse the mathematical expression, Bytecode generation and optimization gave me understanding of trees as data structures and how to operate on them and finally the Interpreter taking in an array of bytecode and executing it, corresponding to how a register machine is implemented and the working of the von Neumann cycles. All of it also gave me deeper understanding of formalized math and symbolic logic and in conclusion the Hilbert program.",1533762282.0
casino_r0yale,Compilers,1533979324.0
rombios,"I do embedded work but the most important courses in my CS major two decades ago where (in my opinion)

* microprocessor architecture.  
* compiler construction
* data structures and algorithm 1 + 2


Processor architecture is important to help you visualize the system you are working on especially in a resource constrained environment. 

Compiler construction was a mix of theory and practicals (language definition and structure, token parsing techniques etc). I later wrote a working 6502 assembler years later for a hobby project I did largely based on the knowledge i gained from that course

Data structures and algorithms is self explanatory. There is a lot of data exchanged between embedded systems and maintained by state machines controlling the device and its interaction with the outside world. 

Knowing how to organize and process this in a timely fashion can make or break a system",1534281802.0
SOberhoff,"I don't understand your argument. You observe that Σ log(x) = log(Π x) and continue to state

> If you cant compute the log base, you cant find patterns between things its the log of. Drop lower binary digits to approximate infinite precision with finite precision. So P != NP. Its still exponential.

I have no idea what this means.

Also there is no full agreement that pseudorandom number generators exist. Proving their existence would have proving P != NP as a prerequisite.",1533590234.0
jet_heller,Are you asking for peer review for a new proof? ,1533589015.0
future_security,"The scientific method is actually applied to hypotheses, not directly to a subject of study. It requires a person think of a hypothesis first. At the end of the process you get an answer to whether or not the hypothesis appears consistent with reality.

(Inconsistent with reality meaning the hypothesis is false. Consistent with reality means that the experiment did not prove the hypothesis false. To determine something is true requires additional experiments.)

If there is no apparent pattern to something then it is difficult to create a hypothesis. You can't do the scientific process without a hypothesis, so if it's difficult to create a hypothesis it's difficult to come to a conclusion. Real scientific work requires the intellectual honesty of the people involved such that they have to say ""We don't know"" or ""We don't know yet"" before there is evidence that a hypothesis is true. Ideally the conclusion is ""The hypothesis is probably true,"" ""The hypothesis is false,"" or ""No conclusion"".

The scientific process is **not** like myth making or neural net training. You don't give up and come to a bad conclusion just because you've tried for a long time without success. Hypothesis testing should prevent the community from unjustified conclusions.

By the way,  quantum physics is random ***and*** confusing.",1533590383.0
dhjdhj,"Quaanthm physics does in fact imply some underlying randomness 😀

However, I think the more general thing to understand about the scientific method in general is that while you can prove that a model is false (by showing a counter example) you can’t PROVE a non-trivial model to be true (because there always might be a counter example lurking out there)",1533586965.0
,another interesting question: how can you apply the scientific method to itself?,1533587841.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1533579780.0
,"I have a 16gb that I have my Tails drive + related encrypted documents on, including private stuff and copies of important documents.

I also have a 16gb that I keep the same copies of important documents in, and use as a vehicle of documents/files for presentations in class and such.

So, two.",1533577365.0
Cathercy,"I have two but I have no idea where they are. So, I guess 0 actually. It's a shame because every once and a while I do need one.",1533577675.0
realFoobanana,"Kinda got shit on with the downvotes, so I'll post a quote here that comp sci people might enjoy to make up for it :)

***

""The theory of computation has traditionally been studied almost entirely in the abstract, as a topic in pure mathematics. This is to miss the point of it. Computers are physical objects, and computations are physical processes. What computers can or cannot compute is determined by the laws of physics alone, and not by pure mathematics.""

–David Deutsch

Source: Cited in chapter 4 of *Quantum Computation and Quantum Information*, by Michael A. Nielsen and Isaac L. Chuang",1533594208.0
AddemF,"I volunteer teach inmates at a prison through Bard University's program, the [BPI](https://bpi.bard.edu/).  They are desperate for CompSci teachers--in fact, I teach Math and CompSci but I know very little about CompSci, I'm a mathematician.  While tons of humanities specialists and some Math specialists volunteer, CompSci is such an industry-focused field that almost none of them volunteer.  (At least that is my speculation for why the stark disparity exists.)  But the inmates' greatest demand is for CompSci courses because it has the most promise of landing them some kind of work once they're out.  Hence, I'm among the only people they can find who can offer any instruction in the field.

If you live in the upstate New York area, you could volunteer with them; I know that California also has a prison education program, and wherever you live there might be something similar.",1533526232.0
kinow,"I worked for a while for banks & telecoms, which gave a good financial return. Now I am working for a government agency that works with atmosphere & water research, mainly around the Pacific & NZ. Salary is not the same as banks & telecoms... but I feel extremely happy and satisfied whenever I can contribute to some work that may benefit local communities, world weather model, climate monitoring in Antarctica and whatnot. Sometimes the work is Open Source'd, which is a bonus.

Pretty much everyday I have a look at a few Open Source projects. Started with docs, tests, and small issues, created some OSS libraries, and now spend most of my time on Apache Software projects, or looking for interesting projects around scifi, japanese, linguistics/nlp, or just something related to work (e.g. am now comparing Symfony's validator with Hibernate validator... both follow JSR-303 pretty well ,but Symfony validators don't seem to work well with JSON property mappings from JMS Serializer... so am trying to find a solution based on what I'd use in Java). I also feel really happy when I see a project I started or contribute to popping up in some articke/stack overflow/or even if it is a CVE (you still learn a lot from experiences like this).

Other than that, I never do anything that I consider unethical or consider incorrect or not professional (turned down some roles and projects in the past). And I've met a few people, surprisingly a lot, in pubs, who were looking for developers for some idea or code... which I ended up implementing for free if it was an interesting idea or if I could learn, or just met for coffee and spent some hours going through his/her ideas, to validate, see what was doable or not, etc. Some of these pub conversations ended up with me helping researchers and being mentioned in dissertations/theses.

My lifestyle is pretty simple, so probably that's why I could say no to some financially interesting proposals, reject projects, and look for a role both interesting from a technical point-of-view, but also that would give me this feeling of helping society & humanity. Might be more complicated for people with family, complicated situations, or different lifestyles or background. But there should be some way, be it helping local communities, helping researchers and students (there are heaps of students right now, with no idea how to implement a project that would die for a 30 minutes chat with a professional software dev), or just looking at open source projects that need help (look at NASA, NOAA, CSIRO, Nieman Lab, Knight Foundation, etc... they have some interesting technical projects, that are used for good).

Ah, one more thing, my family has a few memes passed over, such as not celebrating birthdays, this simple lifestyle, and never ever giving money - however, if people ask for food or help with something, we have to help if we can.

Hope that helps, and good on you for wanting to help more!",1533534446.0
86LeperMessiah,"I am on a similar situation, just graduated but don't want my work to feed ""the machine"", I want my work to help others growth as human beings. I'd love to make educative software to help people realize how wonderful life, the mysteries that are in them; It is a tough way to get there, what keep me going is that love is the answer, I wish you find purpose my man.",1533524611.0
jaycrossler,"Work at an FFRDC -a Federally Funded R&D Center.  They are not-for-profit organizations dedicated to improving society. They employ many CS experts, Systems Engineers, etc.  They are the trusted advisors that help improve health care, fix our tax systems, and catch bad guys. The pay isn’t stellar, but it’s above average and you are always working on important problems. ",1533519165.0
shakespeareanseizure,"The most effective thing you can do by several orders of magnitude is to donate 10% (or more) of your annual income to non-profit organizations or charities, ideally rated highly by CharityNavigator. You can read about [effective altruism](https://www.effectivealtruism.org/), and it almost always comes down to ""cash is the most effective way to help, unless you're [skilled in certain special areas](https://80000hours.org/career-guide/high-impact-jobs/)."" Other than that, you want to focus on (making) and giving as much as possible.

>Effective altruism is about answering one simple question: how can we use our resources to help others the most?

>Rather than just doing what feels right, we use evidence and careful analysis to find the very best causes to work on.


This probably won't make you feel any better though: for that there's open source contributions, community participation (teach a intro to coding class at your library), and finally, volunteering at a non-profit like Code.org, Girls who Code, or something local in your free time.",1533510651.0
sailorcire,">I hope this doesn't make me sound like a fat tool,

Nah, you just have have a conscience. 

Now that I'm back on my feet, I need to start donating to charity again. Last year sucked, but that's all personal stuff and not really appropriate to share on the internet.",1533512198.0
CMFETCU,"Applying your skills in an area you care about is one option. I switched from a Fortune 100 chemical engineering company, to work for a small biotech company that creates software for clinical trials. I am in a clinical trial myself, and I can see that the stuff my team works on helps get medications to people in need faster and more effectively. 

Other sectors where you might find passion and cross over to helping people are in government data analysis, Nonprofits like the red cross, space exploration (NASA, Space-X, Boeing, etc.), or doing work for agencies directly involved in hard science that helps people. THE EPA does some really neat stuff to further understanding of public health risks from exposure to a myriad of things. AI and automation can improve the safety of people's environments, infrastructure projects for nuclear power plants, or designing software for engineering of major public works projects. Even if it is their procurement software, mundane sounding I know, you can contribute to furthering things helping people if the end result is something like a greenbelt highway. Think outside the box. ",1533514344.0
Pinewold,"Check out americanwell.com, we do telehealth medicine. You help people in remote places see a doctor!",1533514596.0
sidekicker6547,"This is actually a bit if an odd qeuestion since software devs are working in ALL industries. Just change to an industry you like to work with. Here are some industries which intetests me: 

* Space
* Electronics
* Clean energy (solar)

Do a similar list where YOU want work then contact some local companies in the field and ask them if they need some special requirement. Now you have a goal which you work for by learning anything you lack.

Ps. Dont underestimate yourself and always have a personal goal with your work.


",1533539247.0
AndreasKralj,"I'm a Systems Engineer, not a software engineer, but working in healthcare really makes me feel like I'm making a difference because I'm writing scripts that deploy infrastructure and software that helps clients diagnose and assist their patients better. You really feel that you're positively impacting real people's lives and helping to solve their problems when you work in healthcare, at least in my experience, so I wholeheartedly recommend working at a company that delivers healthcare solutions. In addition, several of these companies will have opportunities to volunteer your time with their clients on a couple work days out of the year, allowing you to feel even closer to the people you develop your software for. ",1533526046.0
atxweirdo,Scientific software. Look at science gateways and portals through projects like Xsede and SGCI. Lots of research institutions are doing great work on helping researchers solve problems but a lot of these researchers have no experience in software development. ,1533558585.0
howaboutudance,"I literally just got hired to a biotech firm to clean up the mess researchers made in some data... And work with a co-worker to build a tool to submit the data properly....for a cancer drug... That cures childhood luekima😭😇, I mean I have some relevant experience and am a recent college grad...but it's possible. I have a good friend who works for a major hospital customizing Epic for them. And another who works on software to securely manage and warehouse clinical trial data for a major cancer research center",1533522924.0
Magroo,Use your expertise to better open source software. There needs to be a good small business point of sale for example. Or working on a pre-existing project as well can be very beneficial to many people.,1533529642.0
vrockai,"I've been lucky enough to avoid this terrible feeling during last 10 years. I've been working in DNAstack, a small genomics startup, last three years. Before that, I'd been working in Red Hat - a huge open-source company. 

You don't have to contribute to open-source in your free time and deal with all these ""small emperors"" yourself, you can get employed by a company working in the open-source field - my obvious choice would be Red Hat, of course. But as others said, it would be the best for you to pick something you care about. If I personally was about to look for another job, I would definitelly check open positions in companies that do something I care about. Something like Patreon (since they allow many of my favorite YT channels to exist), Cypress (since they work on a very useful project and have great developers) or even Red Hat. I would also checkout open positions in genomics companies, since I tend to like this domain. Having said that, we got some open positions ourselves at DNAstack, *wink wink*.",1533544748.0
ADBEmemewars,Teach others ,1533558883.0
probs7311,"I’m in IT too and have been struggling with that. After 10 years in the biz and a CS bachelors, I am contemplating changing careers.  We are benefiting society, with some IT jobs. Its impossible to see that as problems/issues are always right in our face. Plus most of us don’t work directly with other people. ",1533511409.0
generic12345689,Take a pay cut and work for a non profit. ,1533510689.0
jringstad,"as a government contractor you can make a very nice wage and also benefit to important parts of society such as fighting transnational crime or  human trafficking. You do need to find the right project through, not all governments and government projects are good/competently run/pay well.",1533544034.0
OishiiYum,"The thing is, you probably won’t gain satisfaction solely from work. I used to think that’s possible but realized that there are fundamental contradictions between wage labor and meaningful work. 

I don’t know if you necessarily need to use your degree to feel a sense of contribution/satisfaction. If you don’t mind doing things other than coding, I’ve found being part of organizations that help people to be meaningful, whether nonprofit or local grassroots or even more informal groups. Things like helping people with computers, teaching local youths, help people apply to jobs/find housing, etc. 

If you are looking specifically for CS things, I think disability technologies to be a neat field and have a lot of potential to make a big difference. I don’t know too much on specifics, but enjoyed coding projects that target accessibility. There are also groups that partner with local schools to teach kids about technology. ",1533574371.0
ChrisC1234,"I work for a medical school, and I LOVE it.  For what I do, I get paid OK (I could go elsewhere and make more), but the benefits are great, and it's extremely low stress.  Now, it's not for everyone, but if you can find a way to keep your sanity while working within a bureaucratic entity, it can be worthwhile.  One of my favorite things that I get to do is that I get to come up with solutions to the problems caused by the school's bureaucracy (and additional problems bestowed upon us by government bureaucrats and accreditation bureaucrats), instead of being part of the problem.  And it is nice knowing that most of the people that I interact with work at the school for the same reason.  All of the instructors and administrators could make much more money somewhere else, but we're all there because we like working for the school.  And when things do get a little crazy and I have to put in extra time to get something done, it's nice to know that I'm not doing it just to make somebody else richer at the end of the day.",1533518450.0
joshu,https://www.lesswrong.com/posts/3p3CYauiX8oLjmwRF/purchase-fuzzies-and-utilons-separately,1533523790.0
LowConclusion,"This is why I'm a teacher, instead of a developer. 

Still have the opportunity to create your own projects and do freelance work for extra cash. 

Teaching is a livable wage, but not a great one. CS teachers are also in high demand, it's such an important industry, but it pays too much for most to consider teaching. Most secondary CS teachers are actually teaching business skills or they're math teachers who were recruited cause they once took an intro to CS class",1533560588.0
thedomham,"You described your goal quite vaguely, so I'm not sure this is what you expect, but a really agreeable option for a software developer is always to *donate* your skills and time to software you believe in.

You can join or create an open source project or support a charity that would otherwise pay for custom software.

We are in the pleasant position that we don't have to quit our jobs to give something back. You don't even have to drive anywhere.",1533577065.0
,I find a lot of guys satisfy that desire through working on open source software.,1533577405.0
locotxwork,"I hear you.  I'm CSE degreed.  First off, get a good job to pay your bills.  You cannot help others, if you don't have your own bills/business/family taken care of.  When it came to technology, I have vast amount of experience in web applications development and to be successful in that game you have to know programming, graphic design, databases, system design, etc...   All of those projects maybe me think a different way and I apply that to everyday problems and ""troubleshooting"".  After I gained all this experience, it was easy to support people who needed help without worrying so much about getting paid.  I've ""donated"" my time to many causes and there is always a non-profit in need of help.  I've helped with photo scanning and video editing for a historical society and helped back up their archives.   I've helped create flyers and helped promote (email/social media) for a non-profit trying to advertise their golf tournament.  Just the fact that you WANT to help the betterment of society/humanity puts you in the right path.  Good luck.",1533580872.0
DASoulWarden,"I know it sounds simplistic, but with a CS degree and some experience you can get a living wage pretty much anywhere. The difference will lay on how much a living wage is to you, really. 

Think of the stuff you know how to do really well, and find a way to apply that in a way that helps society. It may not seem straightforward at first, like saving the whales or feeding children, but helping with scientific progress is one of the best ways. Plus, if you feel like you're more an academic type, you'll be comfortable in that space. If you feel more ""hands on"", you can find smaller companies working on almost anything, from medical to environmental and such",1533582818.0
water-and-fire,"You may want to read the 80000 hours site or the podcast 80000 hours for some ideas. 

https://80000hours.org/

It talks about how to best use one’s career to benefit the society. 

PS I am not necessarily advocating for 80000 hours but I find their ideas quite refreshing. ",1533590062.0
kc3w,You could try to work for a company that is committed towards open source.,1533592839.0
pfannkuchen_gesicht,"Working in biology fields probably. Helping to build software that can detect various carriers of diseases. Building classifiers for diseases, ailments or viruses etc. pp.  Still lots of research potential there which help not just us humans.",1533596069.0
darkspine94,"Find an open source / community projects on github/gitlab or any similar site. Some of the projects are for fun, like games, and some for scientific purposes. Get a clone of the project, see what you can improve or add new features then request a merge, if the owners like it, they will merge and do you'll be helping the community",1533621672.0
MeanFoo,"Work in Infosec (white hats), and donate time to worthy open source projects. ",1533517717.0
jhaluska,Look into biomedical software development.,1533518805.0
bopub2ul8uFoechohM,"Donate time to work on FOSS projects (most companies allow it as long as it is somewhat related to your work area, especially if it's a project your company is using).  Stand your ground on issues of software quality and security.

It's not like large tech companies aren't contributing to the betterment of society.  If any one of them disappeared overnight, even something like Uber, a lot of people are going to suffer because they depended on those services.  By helping develop these services and influencing them where you can, you are helping society.  Some people might try to guilt trip you because you work for a tech giant, don't listen to them.",1533515834.0
TheCarnalStatist,Find firms who do things you find ethical. Work for them. ,1533524358.0
ac-asks,"Hi, not an answer to your question but I'm kinda in the same boat studying a CS degree but don't want to be doing it for the money and want to make an impact. Can I ask whether you initially went into software dev for the money or whether you had the desire to make an impact/work on something meaningful before you went software dev? Or did you only start feeling the meaninglessness  after years of corporate?",1533526876.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/u_qseds] [How can I use my CompSci degree to work for the benefit of society while still making a living wage?](https://www.reddit.com/r/u_qseds/comments/94yeee/how_can_i_use_my_compsci_degree_to_work_for_the/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1533531449.0
t_montana,"as other ppl have mentioned, healthcare/biotech is a good option. I guess it also depends on what you define as 'benefitting society'. If the bigger issue is that you feel like a small cog in a big machine, then consider switching to a smaller company. ",1533531818.0
tenshi_anjiera,"There are also software companies that encourage / coordinate corporate charitable work, like Benevity (based in Canada).",1533534492.0
ewhim,"It is possible to find meaningful work in software.  

Education software in the K-12 market (think Blackboard) is big business, and serves a higher purpose.  Very interesting space.

Medical device software is an area where your individual impact is meaningful and saves lives.  Take a few moments and chat up your bio-engineering chums and see what kind of stuff they're up to.

",1533563681.0
aspvirx,Lots of good suggestions posted already. I will add joining or starting some kind of open source project. ,1533565354.0
zombiecactus,"There are people using big data to help! Here's an article about data-driven triage for a text crisis hotline: [Crisis Text Line!](https://www.kveller.com/how-you-can-prevent-suicide-with-a-text-message/). Maybe you can find a group like that.

I understand your frustration. They teach us in school that technology is a tool to help solve societal problems, and that's part of why I studied computer science and engineering. But we act like we're doing a good thing just by building software, and we're not. Often we cause harm instead.",1533566410.0
kwuz,"There are a lot of programs cropping up in the south/old coal country that's focused on helping coal workers transition into tech. If you enjoy teaching and are around there you could check it out.

",1533569020.0
morewaffles,"I started to feel the same way just ~2 years out of school. Depends on what your plan is, but I have been looking more and more into getting into teaching. Regardless of what people will tell you, there is a growing need for CS in schools. A few districts in my area actually are hiring CS teachers at the high school level. Short of that, moving to math is a also viable option, if not to just get your foot in the door. After spending some time on education subreddits, I have seen a lot of people also find a lot of CS positions in private schools. This is good because you don't need to certified to teach at those. There are certainly a lot of hoops either way but I think the payoff is worth it (and probable paycut). There is no one more important in society than teachers (at least in my dumb opinion).

Besides that, like tons of people have mentioned, you can't go wrong in medical or other parts of education. If teaching doesn't work out for me, I am most likely going to look into a job in medical.

Also, you should consider volunteering at organizations that you think make the world a better place. I do a weekend or two a month and it's great! It's obviously different from a full time job but every little bit helps!",1533574213.0
MC_10,I've had similar thoughts. If I could do that and still be able to pay my bills/loans and live decently... I'd switch in a heartbeat,1533583272.0
veltrop,"I work with humanoid robotics that do not have any military uses nor systematically take jobs from people.  My company's robots are here to entertain, assist, be used for research, or be companions.",1533588498.0
aedinius,"There's always government work, but there are fine lines between helping and hurting people, depending on your point of view, and far too often just wasting money.",1533592903.0
wTheRockb,"Lot of great ideas in this thread, but I wanted to share my story. I now work in a startup that largely supports entrepreneurs and other small businesses selling on Amazon. This is interesting, because instantly one could think ""Oh Amazon, remorseless tech giant"". You'd be right, but a majority of the products sold on Amazon (even those through Amazon Prime shipping) are from 3rd party sellers. A lot of those sellers are entrepreneurs selling their products through online retail channels (checkout r/entrepreneur or /r/FulfillmentByAmazon  for a ton of great content on amazon-based businesses!). I really enjoy the fact that I work on a product that helps the small guy more often than helping the big guy, relatively speaking (a lot of our engineers came from ad-tech companies when they wanted to actually do something meaningful). 

It's not 100% non profit work of course. We have investors, we are making a profit, but we are doing it in a way that is symbiotic to entrepreneurs, and I really consider that a success. Keep a lookout for healthcare companies, social good companies, often just generally startups, and blockchain based technologies. A lot of these companies have potential to make awesome impacts on our society. Good luck with your search!",1533514590.0
LLLJJ,Start a non-profit that helps improve your local communities! ,1533515788.0
strongside71,"Maybe consider reaching out to some non-profits you care about to donate some of your time/skill for free? I work at a non-profit and I’m learning web development in the hopes that I can build something useful someday - but the need for tech savvy folks to help out in non-profits is substantial. 

I’m not saying you can’t find companies with good causes and impacts on their community, but I’d say keep your high paying job and try to find a cause you can contribute to. As a social worker who’s trying to break into something more in your field, my work is very meaningful and helpful but doesn’t pay great. It’d be awesome to find something in between. ",1533524940.0
TacticalGeekBC,Join fsociety!,1533573633.0
AaronPDX,"Donate money and find a non-profit to volunteer for in your spare time, make them software to increase their efficiency/effectiveness. We are in one of the highest paid fields, and are modern day magicians who can make technology do things every other group can't. Funnel money from wealthy conpanies to non-profits via your paycheck, and put your skills to work. ",1533525479.0
SanRenei,https://80000hours.org,1533522746.0
peachers21,"I ask myself thus everyday. Wonder if I should switch careers, sometimes. Glad you made this thread.",1533527397.0
jrrjrr,Another resource to find an employer to feel good about: https://bcorporation.net/,1533530414.0
kicked-off-facebook,Create the next Facebook without the BS....,1533551248.0
daixso,I have donated my time and skills to local hackathons. Helping non profits and government agencies.,1533520479.0
rippingbongs,Go camping prolly ,1533537287.0
MikeyLifeCerealQuery,You've been brainwashed to think that operating within the free market is not serving society's benefit. Open your eyes to see that the reason you are paid for your work IS that your work benefits society.,1533580784.0
McDrMuffinMan,"It's not an answer that's popular on reddit given the politics, but work in the free market and make people happy. 

Bill Gates has made a larger change in the world than any of us hope to, he's made quite litterally billions of people richer and made their lives better.",1533537480.0
rekt_brownie,"Work on improving the ways we target ads, make a ton of money, then use that money for whatever social good you want. ",1533565623.0
foxh8er,"Fuck it. Working for the ""benefit of society"" is a massive scam. You'll make more impact by making more at Jane Street or Google and then donating the rest or starting your own fucking foundation. ",1533542321.0
Humane-Human,"Who knows?

I feel like humanity is mostly shit.
What action could you do that transcends that general systemic shittiness?

Try to remain morally innocent by ignoring reality and focusing on pure maths, maybe your work will be essential in some scientific application in 100 years useful for constructing a new type of atomic bomb.",1533552026.0
,[deleted],1533523517.0
CuriousJordanL,All work is meaningless in the end,1533549410.0
MBLBOSS,"You are right now. You're developing a product that billions of people will use. Your work improves the lives of billions of people. 

If you want to help people directly, then I suggest you should read the Bible everyday. Once you are devoted to Christ, then you can help the ministry.",1533514581.0
nerdshark,"It depends on the job. Without details, it's impossible to say.",1533510239.0
skaz68,"From my experience working at both roles, consultants travel and spend time with customers, while developers typically attend internal meetings and write code.",1533511072.0
,"This reads like someone who encountered a few bad apples and developed a bone to pick with university education. It's also interesting how in one breath he bashes academia and abstractions, then in the next talks about happily hiring physicists. I'm not bashing physicists (I ought not to, seeing as I am one), just pointing out that this article is pretty stupid. The author won't even stand by their opinions publicly. ",1533501653.0
tedl999,"\*Pursues CS degree, very excited\*

\*Finds out its difficult and drops\*

""Yea CS degrees are bogus""",1533747239.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1533492668.0
tipsandtricks1977,CPU Registers? Types of Registers. How Registers works?,1533492720.0
enzlbtyn,"I think the easiest way to think about DP is just a specific instance of recursive problems. Where the recurrence can be represented as a DAG and has no overlap (is not a tree). IMO I think you should've put more attention toward this rather than just one sentence. Of course the second property is not a requirement.

That is, if there's no overlap between sub-problems then there is no point in memosing solutions to sub-problems: just evaluate it when required. I believe this occurs when your recurrence is a tree. For example: f(n) = f(n-1) + 1 => a recurrence which is a tree (degenerate; linked list/chain), which has no overlap between sub-problems, i.e. memoisation is not helpful here, unless of course you require to evaluate f for multiple values of n.

When looking it from the DAG perspective, all your recursive algorithm is doing is traversing the DAG in reverse topological order; as this is what a post-order DFS does by nature.

The hardest part with DP is, of course, finding the recurrence. Recognising you can use memoisation (i.e. the recurrence is a DAG and there is overlap) is usually trivial, but of course sometimes is not obvious depending on the way you have structured your problem.",1533715432.0
commitpushdrink,I want to see experience relevant to the job responsibilities you're applying for. I don't care if it's a job or side projects.,1533491449.0
lrem,"You probably wanted to ask this on /r/programming, but well...

I don't think it really matters that much. If you're having trouble fitting into space limits with relevant things, it means that any number of interviewers can pick something interesting to talk about. If you care about the CV screening phase, small things probably won't make a difference. The sourcers I've seen appear to be executing a query in the wherebaouts of `worked in F500 for >1y OR graduated from an university I heard about OR holds a PhD OR was referred`.",1533501089.0
AlexFromOmaha,"First things first, try /r/cscareerquestions

Hiring manager here!

Experience isn't one-size-fits-all. Depending on the company, sometimes experience *in the target tech stack* is the primary qualifier for entry level. Sometimes, experience is interchangeable if you have a history of success or accomplishment. Regardless of the company, you're going to be judged by your ability to deliver before your first year is over (no one new delivers much of substance in the first few weeks - don't get overwhelmed too soon). So, you have two primary goals: 1) get in the door, and 2) be prepared to not suck

You're going to want to decide where you want to start. Not necessarily in terms of which company you start at, but pick a field that has a decent presence in a city you're willing to work in. Then, tailor your experience to that.

In a general sense, if you've *shipped* something, you've done good. Open source project ready for consumption, raspberry pi embedded robot into some competition or expo, put a website into production, got an app on the app store, all that is a very good sign for a newbie if it demonstrates relevant skills. Emphasis on *shipped*, though. Kids rolling in with their school projects have shown very little skill, no matter how much skill they think they've shown. Include it if you have to and you're trying for the long shot. Better nail that interview if you get one.

Teaching is a tough one. Teaching kids is probably not relevant experience in the vast majority of fields. Making apps for kids, working at an education company, sure. Most everyone else, eh. Teaching adults might be, but the bar for entry is higher, and you'd still have to demonstrate relevant skills. On the plus side, if you're teaching skills an employer wants, you've demonstrated both tech and soft skills, and that's a big win. If you have enough qualifiers for relevant skills and you want to show that you also have good soft skills, then teaching kids might be worth advertising.",1533504190.0
IndependentBoof,"> things like instructional roles teaching kids/teens programming concepts (albeit the teaching program no longer exists), or any other non-technical role that still involves CS in some way.

Definitely include them. It shows a passion for the field and other intangibles that employers are looking for (communication, teamwork, etc).

Definitely include side projects as well. They're a lot more meaningful than the 20th applicant from University of X showing their Project 2 from CS 400.",1533491220.0
ninijay_,"In my case I tried to combine the both.

I wanted to get a job in infosec. 

So side projects in this area are like ""find 0day"", rank in some CTFs, do hacking challenges online, maybe set up a lab.

filler would be something like teaching 'hacking', managing the guys that manage the system security, etc..

I combined teaching with CTF and hacking challenges by doing yt videos where I solve said challenges.

That said teaching can be fun, as well as coding an app. My tip would be to go with what you like most. But I'm fresh in the area and not to experienced with hiring processes.",1533488375.0
stanley_reisner,Look at the difference between depth first search and breadth first search. ,1533488868.0
snowman4415,"Stacks make sense because of execution order, FILO. So the analog would be any sequential execution I guess, being FIFO. Basically any synchronous single threaded execution. I think you’re overthinking this honestly ",1533490502.0
linus_rules,"Asynchronous producers and consumers use queues for message  passing. For example, consider a distributed system, for example a weather collection data with several weather stations at distant points, and a server for aggregatting data. The weather stations sends packets with measures and timestamps. The packets are enqueued at the server before processing.",1533488787.0
morth,"Go channels are sort of like this, I suppose.

Perhaps you should specify a more closely how you mean. Cooperative multitasking could also apply, for example. ",1533483686.0
orig_ardera,"Could you be more specific? In what aspect should they be similiar / analog?
Also, as far as I know, function calls / recursion are solely implemented using stacks, so why should there be something similiar with queues?",1533482760.0
tgaz,"You could view each execution stack level as a queue of instructions to be executed, aside from the return address. Each [basic block](https://en.wikipedia.org/wiki/Basic_block) is pushed atomically, and any branch instruction (the end of each basic block) just enqueues the operations of the next basic block.

In this model, the point of recursion is to inject operations at the front of the queue. Something you couldn't do with a flat queue and the mechanism described above.",1533490925.0
VibrationalSage,"You can use recursion to implement queues too. You’d just need 2 stacks. 

An answer to your question could be iteration, but anything that can be implemented iteratively can be implemented recursively, so they’re really analogous to each other.",1533496072.0
loamfarer,"It depends on what you're trying to describe. If you mean how stack frames store program counters to come back to (FILO), then probably channels holding lambdas. Or any other sort of meta-programming that builds up something that is semantically sequential operations. Like repeated map calls on an iterator. If you mean more how data within each scope is managed, just how a stack is storing local values to be popped in reverse order, then the analog to queues are probably streaming iterators.

Corecursion (I hadn't heard of them before) looks to just be a diverging function that needs to keep track of it's past states to produce future states. It's almost like the extreme case of memoization.",1533496467.0
VorpalAuroch,Iteration through a loop.,1533491759.0
uh_no_,iteration,1533492141.0
istarian,A circular queue/ring buffer? The question seems a little unclear. Maybe try to explain your example?,1533488857.0
ThePillsburyPlougher,"One way of looking at a program is a graph. The path of execution would just be a path in that graph, or a list of nodes. All recursion would be is a loop in that graph. ",1533493885.0
sannysanoff,"Cooperative multitasking implemented using any suspension method. Chunk of code is executed until suspension point and then put to end of queue. First element is taken from queue and resumed, then stopped and put to the end. Put it in a loop, you've got multitasking. In fact, any sort of multitasking is like that. Run queues it's called.",1533498607.0
remaze,Reminds of procedural generation of data from some seed value,1533499366.0
gabriel-et-al,"Doesn't it depends on the moment you visit the element?

    def fifo(arr, visit):
        if arr:
            visit(arr[0])
            fifo(arr[1:], visit)

    def lifo(arr, visit):
        if arr:
            lifo(arr[1:], visit)
            visit(arr[0])

    arr = list(range(1, 6))

    fifo(arr, print)
    print()
    lifo(arr, print)

It prints:

    1 
    2 
    3 
    4 
    5 
    
    5 
    4 
    3 
    2 
    1 
",1533504594.0
thbb,"Production systems, or forward chaining is what you're looking for.

https://en.m.wikipedia.org/wiki/Production_system_(computer_science)",1533533726.0
possiblyquestionable,"> Edit: found this: https://www.wikiwand.com/en/Corecursion

Can you elaborate? A recursion scheme that explores branches via a queue rather than a stack is still constructing a least fixpoint of the model in question. E.g., recursively computing, say, a solution to the equation foobar = 1 + foobar, will still fail if your recursion scheme is implemented using a BFS strategy versus a DFS strategy because no least fixpoint exists. However, there is a coalgebra analogue of nat/unit-list/several other isomorphic types in which this object exists, and it is perfectly fine to define this corecursively, as long as you don't unfold foobar explicitly. The unfolding strategy itself does not make sense in this model.",1533586902.0
InjuringAxial,Go to can work wonders you might to import some packages so that might be problematic if that’s not fancy ,1533490963.0
GayMakeAndModel,A stack is a queue with items removed from the back of the queue.,1533482977.0
,[deleted],1533493930.0
JC_Admin,We all know how to troubleshoot and fix everyone's computer problems.,1533447354.0
cp5184,"Well, a big one would probably be about how video game development works.  Most of it's not game engine programming, and even if it is game engine programming, then you probably want a degree in optical physics or something like that.",1533452370.0
nightwood,"That we are autistic no-lifers with bad health (thanks Hollywood)

That, if you have seen a feature in some software, that it's automatically available to us at no cost.

That we can hack part security and write viruses.

That the time it takes to make a piece of software is the sum of the parts.",1533456637.0
sailorcire,"Every computer scientist loves programming.

There are those who really like theory, or architecture, or math and so on .",1533450471.0
santoso-sheep,im not tech support!,1533455445.0
chx_,"That can we bill by the hour. My job is not typing, my job is to _think_ and my brain is not a stopwatch to stop and start as I want. Want me to bill the leisurely strolls on the beach? The gym? The shower? My best ideas certainly came me to at one of these places.",1533456058.0
bdd4,We're all superior hackers who could bring the NYSE to its knees. ,1533463487.0
,[deleted],1533481363.0
evil_burrito,I know how to make your goddamn printer work with Windows.,1533477879.0
ughit,That programming is a solitary activity. One of the aspects I enjoy the most is learning about other people’s problem and fixing them with programming (if applicable).,1533451164.0
tjclccs,That we’re virgins ,1533462593.0
swxxii,That you need to be good at coding or that you need to learn a specific language. You need to be good at problem solving. The problem solving skills I learned in CS have helped me in all other areas of IT. ,1533452861.0
allkindsofcrapog,"How many times i get, “oh my god and you’re a girl” when i tell people what my major/degree/profession is. ",1533484835.0
misingnoglic,"People who are not social make good software engineers.

At least at my job, half of the job is communicating your ideas and concerns to product people, writing good design documents, good emails, good ticket comments, etc...",1533479277.0
acroback,"That if you are great at algorithms and data structure puzzles you are a great programmer. 

Seen again and again, this is rarely true. Clever is never better than clear. ",1533447828.0
serial_wanker,we know the best anti virus and how we can make your computer super fast,1533466186.0
isnopitag,"In my case, that you need to know everything about everything, if something has a LED panel you'll need to know it how to use/repair it, ex: Home theaters, printers, phones, TV's, tablet's",1533475141.0
a88smith,"Typing Speed matters. 

It might depend on the job you have or the  things you're using to build your program, but for most of us, it doesn't matter. I'm a horrible typist and I get my job done fine.",1533490895.0
hanimal3,I DO NOT WORK IN IT. ,1533482902.0
RonnieVanDan,We know how to socialize.,1533482209.0
agumonkey,"that binary logic based devices yields absolute certainty

-- system theorist",1533466623.0
aspvirx,"\- That we are all super smart...

\- That we are all neds... 

\- That we are all introverts

Sometimes true, but not always .... lol",1533483925.0
lxpnh98_2,"That you have to be ""inspired"" to program well in a given moment. More specifically, to pass on an algorithms (or any other programming class) test. This is more a jab at some software engineering students than at the general population.

You either know it or you don't. If you make a mistake due to distraction, or have a blanc during the test, that rarely should make you lose more than 10% of the grade.

But some of my colleagues routinely say they weren't ""inspired,"" and if they were, they wouldn't have failed the test. The problem is that the grade wasn't even close to 50%, they just didn't know the material. That's just lack of studying and paying attention in class.

On a broader point, programming is not art. It's engineering. That doesn't mean there aren't any artistic elements to it, after all, we can enjoy a well-crafted bridge, or the internal mechanics of a specific car.

But the engineers who built that bridge didn't need inspiration to figure out how it would be supported and not collapse, and the engineers who built that car didn't need inspiration to know how the engine and the gearbox work.

Inspiration, or creativity, is needed to solve new problems or solve old problems in new ways. If you know basic algorithms and data structures, and you're being asked to solve basic problems relating to them, you don't need inspiration.",1533490997.0
naasking,"That software development has very little interaction with people, that you're holed up in a dark room for hours on end. Depending on career choices, there's often considerable face to face time while you're understanding a business and solving their problems.",1533476543.0
tiko23867,They expect you to know what sniffing packets is without ever hearing about it before hand. ,1533486981.0
mraheem,It’s IT,1533491676.0
pants75,"This:
https://youtu.be/u1Ds9CeG-VY",1533504933.0
GoodLifeWorkHard,"I thought computer graphics was doing photoshop stuff, the professor laughed at me and told me that I was totally wrong haha",1533683001.0
allkindsofcrapog,I am female. They mostly are surprised i would even pursue. The fact that I’m successful is out of the realm of comprehension. ,1534450812.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1533409304.0
tipsandtricks1977,What is CPU? How CPU works? Explained in detail.,1533409342.0
Illhaveanearbeer,"If you are going to get a masters degree, you should be working while you do it with reputable companies. 

Having real world software development experience (if that's what you want to focus on) is going to be more valuable to employers (and yourself) rather than learning from some professor who hasn't worked in an agile/scalable environment in years.

",1533397065.0
PrivilegedVoyager,"It depends on what job you are getting after your bachelors and if the opportunity cost of masters is worth it.

Generally speaking, if you have done bachelors from a decent University and are getting a good paying job, go for masters only if you are sure that it is the only way to get the job/ career that you want.

In comp science almost everything is available online and if you have a solid knowledge of your bachelors, you can self learn skills, after all most courses we learn by our own anyways..

",1533397520.0
kdhindsa,Pretty cool! I have had an idea to build something like this myself but nowhere nearly as fancy and feature full as this. Thanks for sharing!,1533397511.0
lgroeni,"Oh, is this the one that uses Node.js and V8 as an install requirement?

Edit: survey says yes:

    dghost@Voyager ~> brew info npm
    node: stable 10.8.0 (bottled), HEAD
    Platform built on V8 to build network applications
    https://nodejs.org/
    /usr/local/Cellar/node/10.8.0 (4,022 files, 48.7MB) *
    ... 
    icu4c: stable 62.1 (bottled) [keg-only]
    C/C++ and Java libraries for Unicode and globalization
    http://site.icu-project.org/
    /usr/local/Cellar/icu4c/62.1 (250 files, 67.3MB)
    ...
    Version	0.1.1
    Total size	1.02 MiB
    Total dependencies	121
    Tarball size	10.2 KiB
    Direct dependencies	4
    Dependencies
     signale @ 1.2.1 -- 717.2 KiB (23 deps)
     update-notifier @ 2.5.0 -- 271.8 KiB (75 deps)
     meow @ 5.0.0 -- 150.1 KiB (46 deps)
     chalk @ 2.4.1 -- 32.2 KiB (6 deps)
```

Soooo, it's a 10KiB terminal todo app with 1MiB of direct dependencies that requires a 100MiB runtime.",1533406655.0
Jusjee,"Very cool, will give it a go. Thanks for sharing!",1533402319.0
fdcooperiv,"Meh, Node (the new reinvented visual basic), I'll pass. Besides there is TaskWarrior is all you need without another layer of JavaScript. ",1533423678.0
vscxz384,"That awkward moment when u click this post for the dog picture thinking it’s was a dog community your following, but then you find out it’s not",1533414803.0
Sjeiken,I thought it was a good idea then went to GitHub and it’s JavaScript... What have you done???!,1533432229.0
stefantalpalaru,[Software Engineering is Not Computer Science](https://www.gamasutra.com/view/feature/131817/software_engineering_is_not_.php),1533411157.0
trevorscoot,What terminal is that on the readme?,1533439841.0
i_am_nicky_haflinger,"Engineering is the measured application of prior art and experimentally derived information to define a product that meets a set of constraints. Its output is a specification. 

Programming is turning that specification into running code. 

Software development is the part where we define the practices and processes by which we do the best engineering and programming possible with the time and money we have available. 

if this was a bridge I’d say designing it to carry tractor trailers for 50 years and not fall down during earthquakes is the engineering. Digging to bedrock, pouring concrete, operating cranes, and installing rivets is the programming. Software development is supply chain management and ensuring there’s a port-o-let and adequate lighting on site for the crew to work. 

Our industry is still so young that most jobs will have some blend of all three, but the blend varies quite a bit from company to company. Use the interview to figure out what ratio to expect. ",1533396804.0
,[deleted],1533394772.0
kernel_s4nd3rs,"They are the same thing -- or at least the way people use the terms, anyways. Especially if you are thinking of job titles (which mean very little, are different from organization to organization).",1533395090.0
mrexodia,It’s all the same. Just make sure you get the title that pays the most.,1533399240.0
curiousGambler,"Assuming you meant software development, not soft-development, that’s the same thing as software engineering. It just sounds fancier and IMO engineering is a more accurate description.

Programming is just one facet of software dev/eng. other facets might include design, testing, support, etc. depending on the project.",1533394301.0
ummwut,"Something to do with Forth not in /r/forth??? Good heavens! Prolog too?????

The paper looks really solid. Strongly recommend anyone to read it who is interested in either language mentioned, computer logic, or compilers.",1533364816.0
,"Great! As someone who still (digging into CT and Haskell currently) wants to dive into both Forth and Prolog, this is a nice link between those topics for sure! :)

Thank you for posting this!",1533557220.0
sailorcire,Lol,1533345584.0
jmercouris,"I’m not sure what you are talking about is a specialization of programming knowledge or techniques. It seems to me that it is a specialization of application, and just about any path would allow you to work on this as a profession.",1533367743.0
Nerdlinger,"> I mean why can't we say that solving the problem will take a longer POLYNOMIAL time .

We can. ",1533310158.0
nomad42184,"\> why verifying a solution in polynomial time implies that finding the solution can't take a polynomial time

It seems you may have it backward.  Problems in P can also be verified in polytime (obviously, since you could verify a solution simply by computing it).  The question is whether verifying a solution in polytime implies that computing a solution will always be polytime.  We don't know --- which is why it is an open question.  However, most theoreticians believe that P != NP for many good reasons.  The intuition you point at (which is not, itself, a strong reason; intuition fails all the time) is that verifying an existing solution somehow seems easier than discovering one.  So, it would make sense that there would exist classes of problems where discovering a solution is hard, but verifying one that you've found isn't.

\> I mean why can't we say that solving the problem will take a longer POLYNOMIAL time .

We do.  For example, verifying a path of length <= k between 2 nodes in a weighted graph can be done asymptotically faster than than finding such a path, even though shortest path is obviously in P.  The P vs. NP problem doesn't really touch much on the asymptotics of the algorithm, other than to say that in one class it is a polynomial function of the input size (P) and in the other, we don't yet know (NP).",1533323768.0
Jaxan0,"> why verifying a solution in polynomial time implies that finding the solution can't take a polynomial time

Well... It's still an open question. So it might be that P = NP, and the implication does not hold as you suggest.
But as it stands now, there seems to be no way to turn a verification algorithm in a algorithm which also finds the solution (in the same complexity class), so there is no reason to believe P = NP yet.

It should also be noted that many people have very different reasons to believe P != NP.",1533319185.0
OpiaInspiredKuebiko,The program is just what you're expected to learn. What really matters is what kind of work you want to do specifically in the IT world and if the program cover the 'entry level' requirements to walk into that role. ,1533316612.0
GoodLifeWorkHard,Associates won’t open you any doors like a BA will,1533440843.0
LakeBug,"I would ask what their placement percentage is by field in this program and the average pay. The program has several tracks, which is why I mentioned asking what tracks have the most placement.  The schools relationship with local/regional companies is really the biggest help for placement. 

Having hired and interviewed more people than I can count, I never cared about paper certificates, only your ability to learn and think critically. I’ll admit most places aren’t as progressive, but I’ve found little correlation to schooling , gpa, and successful developers. For reference, I have a BS in CS so your hearing that from someone who went to school.",1534219545.0
endiaga,You would probably want to crosspost this to /r/ECE if you wanted more digital logic design feedback.,1533295952.0
mumrah,Nice! Now let’s see it on a breadboard 😁,1533297736.0
inephable,Now this is good content. More of this!!!! No more intro programming questions!!,1533294782.0
xoniwqe,Software used?,1533295128.0
Blue_Q,"Good work ! I always think about building a simple computer from scratch, but implementing a 16 bit on a breadboard with TTL logic components would be quite a task. There is a Google engineer who build his own CPU from scratch using TTL logic, it can run the Minix operating system. He adapted LCC compiler so that it can target  his own architecture and the compiled Minix. https://www.youtube.com/watch?v=0jRgpTp8pR8
There is also a guy who creates CPU with Lithography (the standard way of doing it) https://spectrum.ieee.org/semiconductors/devices/the-high-school-student-whos-building-his-own-integrated-circuits He bought old equipment from Intel on ebay. ",1533301419.0
Fidodo,"I got to design an 8-bit processor with a team for one of my classes in my degree. It was my crowning achievement. I went into CS because I wanted to know how a computer worked from the lowest level and up, and that class was the culmination of it all. We were the first team to 100% that class. I know it's bragging but taking that class and putting my all into it and having it pay off made me so happy.",1533325512.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/ece] [My bare bone 16 bit calculator!](https://www.reddit.com/r/ECE/comments/948usj/my_bare_bone_16_bit_calculator/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1533296632.0
xemasiv,"WHAT THE ACTUAL FUCK?

NEVER THOUGHT MANUALLY MAKING A CALCULATOR IS REALLY THAT COMPLEX LOOKING.

",1533314160.0
seaweavle,This is really great content. Wish redditsilver was still a thing ,1533296893.0
pilko909,"This is amazing, Logic gates have me hating my embedded system unit ",1533297830.0
project2501a," > bare bone

for a moment, i thought this was Hex from Unseen Academicals.",1533300408.0
BaleZur,"/r/ElectricalEngineering and /r/Electronics might like this.

Also WOW that's some work! Have you considered using this as an ALU for a computer?",1533304832.0
SteeleDynamics,"Very cool, OP!",1533305095.0
mantrap2,Nice!  Good job!,1533312381.0
trkeprester,"my hat is off to you this is awesome, i have some latent fear of <=assembly programming/logic design born from my early days of programming education i huddle comfortably around the glow of my c compiler",1533321485.0
Silvia923,"Wow, this is amazing! Everything is so abstractified these days, it's rare to see things like bare-bones 16-bit calculators -- a beautiful sight.  There's a certain joy in getting to see the technologies that power our world in their most bare-bones, un-abstractified form. I'm actually a CS student, and covered Full Adders, etc., -- it's cool to see them in your logicly diagrams. Pretty impressive stuff, thanks for sharing with us :D ",1533326615.0
rAaR_exe,"Also if you huys like this search bean eater on yt, he has very in depth vids about this.",1533352181.0
Staticbox,What software are you using for the design?,1533309015.0
cip43r,What software did you use?,1533367895.0
E_kony,"Jesus christ dude, learn Verilog. Most FPGA toolchains come with free simulator.",1533324535.0
flexibeast,"Firstly, coding is only one part of computer science; check out [this recent thread](https://www.reddit.com/r/compsci/comments/92ahi9/would_you_say_computer_science_is_like_writing/) about computer science.

Secondly, in terms of qualifications, which country are you in?",1533287542.0
nefrany,which year are you in?,1533291677.0
Jaxan0,"I'm sorry I cannot help with recommending books (I am a non-self-learner, I really like attending lectures and doing the given exercises).

But for the project: We made a compiler in the Compiler Design course. This was immensely satisfying, as we started with nothing, and then built every stage of a compiler (parser, typechecker, code generation). In the end, we had a compiler from a pretty high level language down to assembly!",1533319557.0
nexvanshisha,"Tomorrow I have and exam on Operating Systems, and we used the following book: 

http://iips.icci.edu.iq/images/exam/Abraham-Silberschatz-Operating-System-Concepts---9th2012.12.pdf

It's easy to understand and probably one of the most used books for this course. You can also find all solutions to the exercises online.",1536004699.0
random_forest97,"You got database as ur course, thats great.. u have to focus in that area and try to learn sql,mongodb etc
As a project u can build a website ( a full stack website )",1533293129.0
CorrSurfer,"The arguments in the article can go both ways:

- Without a solid unterstanding of the theory, the software developer is doomed to trial-and-error when he/she needs to deal with difficult computational problems.
- The so-called ""academic"" programming languages are taught for a reason! Functional programming forces you to think about *what* you want to achieve, to only then think about *how* this is to be achieved. This training is insanely helpful to write code for complex applications that is low in bugs.
- Frameworks such as ""Node.js"", ""React"", and others are not taught on purpose. Such knowledge is quickly outdated and they are comparably easy to learn by yourself. So why should an institution waste time on teaching this if they can teach material that is hard to learn on your own and will remain relevant for a very long time?

Also, if the author wants to hire software engineers, why not hire software engineers rather than computer scientists? ",1533285545.0
bigmell,"> Intellectualism rarely produces results

As opposed to what, stupidity?  The idiots are producing top flight code again!  Nonsense.",1533490555.0
lucsturci,Anyone played around with Google Cloud Speech Recognition API?,1533273381.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1533253944.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1533248046.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1533232383.0
pinano,"What’s the way around some numbers not being representable in decimal?

e.g. 1/3 = 0.33333333333333333333…

You approximate it, or you use a different representation, like a fraction.",1533232928.0
rep_movsd,"No matter what base you use, some numbers are infinite sequences, some are irrational, some are transcendental.

You have to use some sort of alternate representation like fractions for recurring decimals",1533236248.0
zokier,"Depends on what you mean by *""binary""* (and also ""representable""). Floats are not just straight up general binary numbers, they are very specific format. From a mathematical point of view I think binary numbers are about as ""expressive"" as decimals.

In practical terms using rationals gets you pretty far alredy. Other types of things you might encounter are fixed point numbers (really just a special case of rationals), BCD encoding, and decimal floats. They all have their applications.",1533236533.0
CheifOfFury,"Hands down the best text on OS concepts is Operating Systems: Three Easy Pieces found at http://ostep.org

It’s available for free online chapter by chapter and in includes some sample projects to apply the concepts on an educational OS called the xv6. ",1533215837.0
,[deleted],1533218147.0
flexibeast,"In terms of books, there are a number listed in [this post on Quora](https://www.quora.com/What-is-the-best-book-on-operating-systems); i can personally recommend ""Operating System Concepts"" (""the Dinosaur book"").",1533211582.0
JimmyPepperoni,Here’s a good collection of notes of OS https://github.com/darshanime/notes/blob/master/operating_systems.org,1533209446.0
nedimm,"Videotaped lectures, sample exams and programming assignments are available at [https://www.ops-class.org/](https://www.ops-class.org/) ",1533234957.0
TooTryJund,"The xv6 book is less than 100 pages. xv6 is a simple Unix-based OS developed at MIT for their OS course.

[the site](https://pdos.csail.mit.edu/6.828/2017/xv6.html)

[the book, revision 10](https://pdos.csail.mit.edu/6.828/2017/xv6/book-rev10.pdf)

It doesn't beat OS:TEP, but is much less daunting.",1533227880.0
Slukaj,"[Operating System Design: The Xinu Approach by Douglas Comer](https://www.amazon.com/Operating-System-Design-Approach-Second-ebook/dp/B00UVB2YA2). 

Of course, I'm a Purdue graduate, so I could be biased, but this is one of the few CS textbooks I ever had that felt like it was worth its weight in gold. ",1533239543.0
GNULinuxProgrammer,"UC Berkeley [CS 162: Operating Systems and Systems Programming](https://inst.eecs.berkeley.edu/~cs162/fa17/) I find these slides very concise and accurate. It's a good survey of fundamental concepts of (mostly Unix-like) operating systems. The project of this course is an open source project (basically a half-made operating system intended for undergrads). Search PintOS and you'll find it on github (along with many student and TA solutions from all around the world). Do that project, I found it very helpful myself. (The project is in C, and it's notoriously hard (especially Project 3, virtual memory) so make sure you already know basics of machine structures, C programming, low-level programming etc... and be very familiar with gdb)",1533252134.0
acroback,"Mccusik's Design of BSD operating system. I have fond memories of getting knocked out around by that book.

Very practical book.

If you need just the design aspect, nothing even comes close to Mauirce Bach's design of  Unix operating system.",1533227560.0
reiger,I'm a huge fan of the Linux and BSD implementation books as they seem to be a nice balance between theory and practice. ,1533223075.0
GloriousLaserChicken,I've found this site to be a pretty interesting resource: [https://wiki.osdev.org/Main\_Page](https://wiki.osdev.org/Main_Page),1533240661.0
NerdAtTheTerminal,Also check out MIT OCW.. There might be some others OERs as well. I think Caltech has all previous year lectures & notes available on cms.caltech.edu ....,1533261217.0
Habikki,"If I may ask a clarifying question, what part of the OS are you interested in and from what perspective?

Personally, I find thread schedulers and resource managers fascinating but I will never attempt to write my own.  Conversely, I loathe writing kernel mode modules but will do so when the need calls for it.  ",1533221634.0
cirosantilli,"I have created the following resources which might be of interest:

- https://github.com/cirosantilli/linux-kernel-module-cheat
- https://github.com/cirosantilli/x86-bare-metal-examples/graphs/traffic",1533276403.0
sai_yerni_akhil,"Operating system concepts by Silberschatz, Galvin, Gagne is a good book tho. ",1533279898.0
couple_moment,Anderson and Dahlin's Operating Systems is a pleasure to read. It gives you the big picture and the lower level mechanisms.,1533283798.0
Nerdlinger,"It’s a lot closer now than it had been, but some of the reasons that notion rightly existed are:

* Android did have quite a few holes back in the day, these have been cleared up pretty well.
* Developers tended to give their apps far more permissions than needed, so if they were exploitable, the attacker could do more on the device than they should have been able to.
* The google App Store is *much* more permissive about what it allows than the Apple App Store, so a lot more malware got distributed via “official” channels with Android than Apple.
* Fragmentation. This is the big one. There are still a ton of evinces out there running old, vulnerable versions of Android. IOS, on the other hand, has extremely rapid uptake for software updates, so vulnerabilities get patched a lot more quickly and broadly.",1533170865.0
HelloAnnyong,"[iOS gets encryption very right, out of the box. Android not so much.](https://blog.cryptographyengineering.com/2016/11/24/android-n-encryption/) mind you I don’t know if this situation has changed in the last year and a half. But there’s a reason why you only ever hear about the government needing help breaking into iPhones.",1533205277.0
AncientPC,"If we define hacking as in ""messing around"" instead of ""malicious attacker"", then Android is much easier to hack because you can gain root easily (on purpose) and the OS source code is open source.

If we define hacking as ""malicious attacker"", then the most likely culprit is that many device manufacturers (e.g. Samsung, HTC, LG, Motorola, HuaWei, etc) are responsible for testing and pushing security updates to users' phones. That means dozens of companies need to regularly maintain hundreds of new Android devices per year. Contrast this with a single company (Apple) only having to keep a few dozen devices up to date.

A more apt comparison is the security of Apple devices with Google devices (e.g. Pixels). In this example they probably have an approximate same level of security.",1533195107.0
hnerixh,"Both systems are resonably secure, but fail in different ways, in my opinion. As previously stated by others, Andriods ecosystem is less strict and more spread out. Apple owns the better base system here. But Apples QA is horrendous. Just read the source code from `goto fail;`, it should have been caught by the compiler, a code review, and the most basic tests you can write for a SSL suite.

Breaking Apples data security or gaining root is hard, but a smiley can be used for denial of service. Android seems to ve the other way around.",1533199725.0
Tacticus,"Patches and support.

The amount of garbage devices (basically * outside the google made ones) out there, shitty telcos that block updates and short lifespans for what ""support"" there is leads to large numbers of poorly secured by default devices that just get worse over time.

there is some pressure to fix this. Googs move to pushing app updates through play store instead of bundled in the OS and their push to get security updates as something that vendors can just consume is a good start. They still leave too much control to the device vendor and telco to block updates.


",1533189731.0
ParanoidDrone,"I was always under the impression that Apple products aren't *harder* to hack per se but rather that their user base is comparatively smaller and therefore pursuing vulnerabilities isn't always worth the time.

That said, I'm not sure how iPhones, tablets, and the like would factor into this.",1533178257.0
chiveygnome,"The original rumor started way before the mobile era, back when personal internet browsing was becoming popular.  I believe it came from an apple commercial, or something similar.  (Cant find it now, may update later if I have time).  Anyways, the basis for the claim was the market share.  At that point the hardware and OS differences in machines made programming cross platform difficult, as there were too many to really code for.  
Anyways, the basic idea was since they were small, apple machines were a big target because typically, it wasn't worth it.

Add this to the early releases of android being insecure and you have your answer",1533209673.0
sayubuntu,Ask the DOJ,1533179170.0
aelsilmaredh,"It's not necessarily a big difference, but it's always been my impression that the BSD core of Apple's OS's and BSD in general is pretty much the most hardened system that exists, even more so than Linux.  It seems deliberately designed that way.  It's pretty well-known for it's rock solid built in encryption tools, for example.",1533184688.0
pratik_mullick,"I believe that is more about perception. While both Android and iOS share the same UNIX core with Android based on the Linux kernel, and iOS using a customized FreeBSD kernel; the Android app ecosystem is more open than that of iOS. One can develop Android apps that can access much more system data than iOS apps.",1533169511.0
enp2s0,"Andriod as an OS is probably harder to hack than iOS, because it's based on the Linux kernel. However, Apple places strict regulations on apps, while Google dosent. This makes it way easier to install malware on andriod than iOS. However, as long as your not an idiot and don't install or download shady stuff, andriod should actually be more secure. If your really worried about security, AOSP andriod is probably even more secure because it lacks all of the Google bloat/backdoors/data collection/etc",1533196464.0
Wetmelon,"I still have this comment by u/aplejax04 saved:

https://www.reddit.com/r/AskEngineers/comments/3adekw/how_are_plans_of_huge_asics_stored_you_dont/csbnuuw/

>Yes, there is a final single file with all 8 billion transistors on it that is sent to the fab. The file can be many gigabytes large, and has the extension .gds. This gds file contains the coordinates for all of the different metal layers, polysilicon layers, nitrite layers, diffusion layers, etc to build the actual transistors.

>The design of modern processors is actually quite interesting. With so many transistors on a modern chip, the whole processes has been abstracted to coding. Basically if you can code, you can design a full computer processor. You do not need any electrical knowledge. The process is called VLSI Design and it works as follows:

>You discribe the functionality of whatever you want the chip to do in a high level hardware discription language, such as Verilog. Verilog is just like any other programming language, so you can program in algorithms, state machines, whatever you want. Now you can test and simulate your design with programs such as NC-Verilog.
Once you are happy with the design you have to turn it into hardware. This step is called synthesis. Someone else (usually the foundry) designs electrical circuits to do boolean logic operations for you. So the foundry will provide you with a hardware adder, and gate, or gate, etc. This is called a standard cell library. Now, you use a computer program such as RTL Compilier, which will read in your verilog code, and compile a netlist of and and or gates with the same logic functionality. The output of the program is usually another verilog file containing only the boolean standard cells which perform the same logic functionality. It will also output report files saying what cells it used, and will estimate power, speed and area of the design.
Now that you have a standard cell library to use, and your synthesised verilog, the next step is to combine the two. This step is called Place and Route (PnR). Basically a computer program will read in your sythensised verilog, and place in the standard cells according to what your synthesised verilog says. The defacto tool for PnR is called Encounter. The output of encounter will be a finished chip. It will output a final gds file containing every transistor on the chip, a final PnR verilog which you can use to simulate the chip with, a schematic in spice format, which you can simulate with, and report files, which will estimate area, power, and speed of the completed chip.

>Once you have the final gds file, you must verify it works before sending it to be fabricated. The gds file can be checked with a general purpose electrical CAD software, such as Cadence Virtuoso. In Virtuoso you can to make sure it passes all the design rules with DRC rule  checks. And you also wanna check that the final spice netlist matches the gds layout with LVS checks. Also, you want to run a full electrical simulation on the schematic that encounter outputed to you. This will give you actual power number, and delay numbers for your design. Two big tools for performing these simulations are Synopsys Primetime and Synopsys Nanosim.
Once you are happy with the results you send you completed gds file to the fab, and wait 6 months or so until the chip gets fabbed.
Notes in the Margin:

>*I use alot of these tools any day, so feel free to ask me anything about them

>*These tools are all REALLY REALLY REALLY expensive, and can cost $10,000s a year for the licence fee for any single tool listed above. So companies spend millions of dollars just on the licence fees for the programs they use to build computer chips.

>*The documentation for most of these tools are either crap, or non-existent. And if you email the company to ask how to do something, they ignore you, or answer your question wrong. And, no there are no free alternatives to most of these tools.

>*Because these tools are really really expensive, and very very hard to use, if you know how to use any of the tools, you are very very valuable, and will be paid very very well.

>*All of these tools are linux based, not windows, so Yay linux.

>*Alot of the tools are command line based, and you use with scripting languages such as TCL, perl, python, ruby, etc.

>*Each simulation can take hours/days to complete. It can take along time to place 8 billion transistors.

>*TL:DR Computer processors are designed by writing code.

>Edit: If your really interested in how chips work you should check out /r/chipdesign, its a small subreddit for people in the industry to hang out and talk about integrated circuits.

>Edit2: alright guys I have got to go to school, so I'll be back in 10 hours or so to continue our little discussion, but I really gotta leave reddit and be productive today. See ya later.

>Edit3: Wow, gold, thank you kind stranger for the gold :):):). This is a subject I am passionate about, and it was nice to feel like a celebrity for a day. Anyway's I'll be around reddit today to answer any lingering questions. Also, check out /r/chipdesign, there a fascinating subreddit on this exact topic, and they desperately need new subscribers.",1533179257.0
mantrap2,"It's a bit more complicated but from orbit, it's like that.

It currently costs $500,000,000 to **merely design** a 8 nm IC.  That does not include ANY cost of goods or manufacturing capital equipment or infrastructure/running costs.  The typical Super Fab (the only kind now made), runs >30,0000 300 mm wafers per month and often it's over 100,000.  Such fabs cost between $30B and $50B to build.   So that much money means a lot of process complexity to pretty much everyone from conception to delivery to customers.

Often processes include iterated loops.  For example, just to bring a process up, you have test structures used to validate each step of the process.  

Each of those takes up wafer real estate so there is an iterated ""discussion/argument"" about which test structures to include and which test algorithms to run on those test structures.  So there's always discussions like ""Can't you drop those capacitors now? We can shrink the drop-in smaller and add 2 more product die!""

Some test algorithms can run a very long time but throughput is so critical that you despite the fact those long tests provide critical and essential information, you can not actually do many of them.  

So there's a negotiation process that can take months and gets revisited throughout the life of the process.

And this is just a minor operational corner of a fab and a process running in it (ofter fabs run more than one process with different steps which are controlled on-the-fly via the MRP system).  

Product design is similar where die size is a negotiation between requirements and physics - it's NOT decided and set in stone up front because engineering realities can necessitate increasing or enable decreasing the die size.  

And the moment you change die size the entire economics of the product change.  Of course, before any of that, the marketing manager has run all those numbers with variances and tolerances to anticipate this and that in turn can drive changes to other parts of the process.  It becomes a ""bump in the carpet that propagates"".

So yes, from orbit it's as ""simple"" as ""decide build something and then build it"" but it's NOT remotely that simple in reality.   Most design or manufacturing processes involve iteration of decision making and are highly networked between many, many departments and groups in the fab.",1533221981.0
duskhat,This has nothing to do with computer science. It's also poorly written and this user has posted this article in 10 other subs in the last 2 hours,1533164313.0
maq0r,"I've been doing infosec for close to 15 years and currently am at one of those FANG companies. 

What others have said isn't quite right. I have 0 certs (tho I do have a bsc and a master's, which came after already working on the field). Regardless, you say you're one of those webdev bootcamp folks without a ""proper"" CS background, and that can be very valuable still; you see, infosec is essentially tasked with securing every aspect of computer science. You do NOT have to learn how firewalls work, or how to do a pentest per se. Your knowledge of web development gives you a leg up on Application Security which seeks to find vulnerabilities in web applications and I recommend you start there. 

Reading materials: OWASP Top 10 and learn how to use BurpSuite, check out some web app pen testing videos and the like. You can make a Lot of money finding issues for companies through bugcrowd, trust me, many companies that pay out bug bounties you've found will try to hire you giving 0 fucks to whether you have a degree or a cert

Infosec has MANY entry points, network engineers can go into that route (setting up vpns, firewalls, IDS, etc), sysadmins can go the system hardening route, and developers can go the app testing route. 

Start with learning how to secure what you do in your field (whatever that might be) and after that, if you're really digging it you can learn other infosec ""paths"". The concept of a vulnerability is the same whether you're a webapp tester, system pen tester or security network engineer. ",1533159057.0
sailorcire,"By reading:

[Hacking: The Art of Exploitation, 2nd Edition]( https://www.amazon.com/dp/1593271441/ref=cm_sw_r_cp_apa_SPEyBb15C89E4)

[CEH v9: Certified Ethical Hacker Version 9 Kit]( https://www.amazon.com/dp/1119314003/ref=cm_sw_r_cp_apa_LQEyBb0E15854)

One of my favorites: [Designing BSD Rootkits: An Introduction to Kernel Hacking]( https://www.amazon.com/dp/1593271425/ref=cm_sw_r_cp_apa_nSEyBbA59B1F4)

Then set up your own lab (can just be a few VMs) and hack yourself.

Then take your Security+ and CEH exams. And don't forget to subscribe to the 2600!",1533143859.0
michelolvera,Use 12345678 or admin as a password.,1533164658.0
AllowItMan,"I moved from software engineering to application security/dev sec ops. I'm now learning cyber sec on the job, whilst adding value of ensuring best practices are being followed interns of secure coding and secure delivery.
I know it's not exactly what you want, but it's a career path that might work for you if it's available to u.
Good luck!",1533185496.0
explodycat,there is a really good book collection up on humblebundle.com right now. Several top tier books in it and definitely worth a look,1533219760.0
kittytheexplorer,"I always encounter these 2 resources on the web:  r/https://www.udemy.com, r/https://www.lynda.com  
Aside from learning from these platforms, doing a self-study can significantly help you as well. Join professional groups online and get some insights from them. You will also find job opportunities there.  I'm not sure if the reviews about Udemy and Lynda are good. You will see the feedback of their students if you do a research. One of the positive sides of these resources is that they are very specific in their programs. I think that's a good place for you to start. If you become excellent in your chosen field, then you will always get a job in the IT world. This type of profession will still be in demand in the coming years. Technology is always upgrading; thus, companies should always improve the level of security in their business. You have a good career choice. Keep it up! To motivate you more, here is an article which states the beauty of IT career: r/https://www.infotechresume.com/it-career-advantages/   
 ",1536040406.0
uncleXjemima,Follow the white rabbit ,1533155381.0
its_joao,"Cyber-security and information security will require you to have formal education (usually a masters) and lots of certifications. 

I highly doubt that you can get a meaningful position in this field without any formal qualification and without certifications. 

Maybe look into a course at you uni, if you have a BSc in any field you might be able to do a MSc in information security.

Quite frankly, I am not the best person to advise. The above is based on  my own experiences and from friends' experiences. Maybe someone will be able to help out here more than me.",1533156411.0
,"Requires a talented brain for this, doubt ur boot camp material",1533178931.0
possiblyquestionable,This is a surprisingly good profile coming out of Quanta,1533140118.0
hrsvrdhn,Search for udemy 100% free coupon. You will get a lot of them. Apply them at Udemy and have some certification for free.,1533140744.0
Resquid,"See if your employer will pay. Besides that I'm not aware of any free certifications. It's definitely an industry.

PS keep track of your expenses. You can deduct them from your taxes in some scenarios.",1533122786.0
sailorcire,"1. Amazon Prime.

2. MSDN\Dreamspart\whatever it's called this year.

3. I didn't have it at the time, but look for AWS.

4. See if your school is partnered with VMware. I didn't use it, but might be helpful.

5. See if your school provides MS Office for students at a reduced rate.",1533120962.0
RunThread,"Chegg, it gaves you As.",1533131868.0
olliej,"As primary goal? Honestly i think it depends on how good you think you would be. Which sounds dumb.

In my experience if you’re doing a masters for the purpose of employment you really want to be able to count on the work you do for your research/internships(if you’re in the US), or the people you’d interact with specifically getting you an offer.

Although the specific topic of my thesis is completely useless in the real world (I think that this may be a requirement), in doing it a learned things that were useful in getting me job offers, and actually useful in my job. But on the other hand I was extraordinarily lucky in getting the interviews I did - that was purely happening to have direct connections to the right people. One company through my supervisor (advisor? In the US?) and the other through interacting with engineers and managers in an opesoirce project.


Err, the tldr is basically:

You need your MSc to teach you things that you can’t trivially read from blogs*, and you ideally need your thesis (publications, conferences, supervisor) to give you connections to employers looking for the skills you [will] have.

* I’ve noticed a theme in AI/ML “courses” and “majors” to basically be “here’s how to clump things together in trnsorflow” rather than “this is how tensorflow works”, and for the big companies you need more of the latter. For the former blogs give you the same information for free.",1533091292.0
bkalle,"My 2 cents from someone who works in a AI focused robotics startup in SF (with PhD in your field), and interviews a lot of graduates like you: By now almost every CS graduate can clobber together a tensorflow model and has a reasonable AI background. Your chances and earnings will essentially not be better than a normal CS master student. I.e. still good, but AI focus doesn't pay extra, unless you come with proven special knowledge (PhD). If you're in it just for the money, i.d say get working experience without the masters. Or, consider a masters in Europe (were your total cost of tuition would be a fraction), or aim high and get a full grad school program aimed at research and PhD.",1533103509.0
merimus,"You don't need an expensive education to get a good job.   
While a big name school might give you a slight head start, that quickly disappears in the workplace.  
After 2-3 years your education doesn't even matter much anymore and your job experience is much more important.  
",1533089673.0
,"I'm curious too, I've been considering this myself.",1533086896.0
sailorcire,"I had the most fun doing my Master's. Took two years and bam!

That said, I also had my Master's paid by the school for being a TA and teaching. See if that's an opportunity for you.",1533091110.0
DoesNotLikeBroccoli,Link to the paper https://eccc.weizmann.ac.il/report/2018/128/,1533064231.0
autotldr,"This is the best tl;dr I could make, [original](https://www.quantamagazine.org/teenager-finds-classical-alternative-to-quantum-recommendation-algorithm-20180731/) reduced by 86%. (I'm a bot)
*****
> In 2016 the computer scientists Iordanis Kerenidis and Anupam Prakash published a quantum algorithm that solved the recommendation problem exponentially faster than any known classical algorithm.

> At the time of Kerenidis and Prakash&#039;s work, there were only a few examples of problems that quantum computers seemed to be able to solve exponentially faster than classical computers.

> Kerenidis and Prakash proved that a quantum computer could solve the recommendation problem exponentially faster than any known algorithm, but they didn&#039;t prove that a fast classical algorithm couldn&#039;t exist.


*****
[**Extended Summary**](http://np.reddit.com/r/autotldr/comments/93l71i/teenager_finds_classical_alternative_to_quantum/) | [FAQ](http://np.reddit.com/r/autotldr/comments/31b9fm/faq_autotldr_bot/ ""Version 2.02, ~339911 tl;drs so far."") | [Feedback](http://np.reddit.com/message/compose?to=%23autotldr ""PM's and comments are monitored, constructive feedback is welcome."") | *Top* *keywords*: **computer**^#1 **problem**^#2 **quantum**^#3 **Recommendation**^#4 **Tang**^#5",1533092536.0
nighthawk55555,"Has anyone started an implementation of this yet in python, go, rust, etc?
",1533153684.0
celerym,"I was told that Quantum Computing will bring about the Singularity we've all been hoping for and that the NSA has quantum computers beyond our wildest dreams, cracking my porn history in mere nanoseconds. What is this?",1533145346.0
TheWass,"Compilers are really cool and I think don't often get away much attention as they should. Especially parts like the parser that many consider ""solved problem"" but in reality we don't have very good ways or expressing complex grammars in a algorithmically good way. Plus getting the grammars right and their implementation right to prevent security issues is a big deal, check out the LangSec workshop for more info.",1533065359.0
FieryPhoenix7,"Here's a link to the book in the title:

https://www.amazon.com/dp/012088478X/",1533066416.0
DiputsMonro,"Oh, Keith Cooper was my compiler professor in college!  I loved his class and definitely recommend this book (though I may be a little biased).  Compilers have been one of my favorite topics ever since!",1533094041.0
barsoap,"Millions of lines of code...

The thing is: Don't be intimidated by that. [You can do a compiler in something like 300 LOC](http://www.bayfronttechnologies.com/mc_tutorial.html). That's for metacompiler + compiler + VM, a metacompiler is a self-hosting compiler compiler. That last term -- compiler compiler -- is nowadays usually called a parser generator. 

And that's also the limitation of the thing as implemented by Schorre: The parser doesn't generate an AST which is then fed through half a gazillion of pipelines, you just output code directly, in a single pass. That's sufficient for pretty much all of ALGOL60, it is nearly sufficient for Pascal and other languages you can compile in a single pass... the notion of what a compiler is supposed to do has changed quite a lot since the 60s. Bells and whistles such as optimisation, type inference, you name it, which require information flow that METAII just can't model. Also: Sensible error messages.

But this basic stuff is a good place to get started: The essence of a compiler is if not simple then concise, and definitely elegant.",1533150264.0
kromlic,I used Cooper and Torczon's book to write a toy compiler and self-teach myself compiler fundamentals. I'd highly recommend it to anyone looking for a similar challenge.,1533092039.0
TaXxER,"One of my favorite courses of my comp sci bachelors. Hard as fuck, but very rewarding.",1533106889.0
RottenZombieBunny,It also teaches you about programming languages (and not just the ones your compiler compiles to or from).,1533105497.0
combinatorylogic,"And on the other hand, compilers are among the simplest things out there. Literally. It's really hard to find something that is simpler than a compiler.

As a consequence, if your problem can be represented as a form of compilation, you can eliminate all the complexity out of it mechanically.",1533119356.0
Marcus_AureliOS,"Is there a place/forum for people independently working through the book?  A solutions manual would work as well.  I haven't been able to find either.  I realize I could ask questions here or on math/stackoverflow, but that would be far less efficient seeing as how I only want to verify my solutions.",1539291622.0
crono731,"Hey, I went to Pitt! cool to see some older stuff. I wasn't too fond of the modern day version of the class though, I hate low level programming -_-",1533056849.0
figurehe4d,"I'm about to take assembly and machine architecture this fall, not sure if I should be excited for it. lol. ",1533061756.0
jnwatson,I was OK until it asked for the fixed point square root. Use Newton's method?,1533073218.0
NeoMarxismIsEvil,I got to type a punch card recently!,1533099215.0
MrHanoixan,That April 1st assignment...,1533093714.0
ibcooley,Which ones?,1533016928.0
forhire123,How do we get the free help?,1533017905.0
vorlik,the background on that website makes me want to die,1533010980.0
harakka_,Computer science != software engineering.,1533034559.0
barsoap,In other words: Too many smiths who can't forge their own tongs.,1533019200.0
flekkzo,"I distinctly notice a difference between self taught/boot camp developers and holds a real engineering degree developers. There's a lack of deep knowledge replaced by either just scraping by searching on the internet or worse, an inflexible mind and a pain to work with.

There's also the slightly rarer don't hold a degree but wants to learn it all. I like that a lot better. I don't care if people hold degrees or not per se, but if you are trying to win arguments by telling me what others think and throw silly rules at me instead of having a friendly productive discussion were you understand the concepts I'm going to loose respect and intrest.

Comp sci/software engineering makes a difference. It's well worth learning it.",1533011162.0
iamLurch,"Well stated.

I recently went through (the recordings of) MIT’s intro course for electrical engineers, in which somewhere the professor says students may wonder why they have to do all this calculus and learn FET models and so on — in real life don’t you just wire chips together? And he points out that MIT degrees are for the people who make the chips.",1533061860.0
EmbeddedEntropy,"I was surprised at the ending where he used the doghouse/skyscraper analogy.  I've used the same analogy many times myself many times before when trying to explain the differences.

I used to use doghouse, however, I think doghouse is a little insulting, so now I usually start with ""garage"" and work up from there though ""house"" and ""apartment building"" before getting to ""skyscraper"".",1533058444.0
f4hy,Websites have lost their readability.,1533060547.0
TheExplorativeBadger,"I did a double major with computer science and math. Was able to find a bunch of overlapping classes and finished in 4 1/2 years. Math was always my passion though, so I would just suggest doing what you absolutely love to do.",1533003145.0
kupo1729,I did a math/cs double major and was super happy with it. In hindsight I wish I'd also done a religious studies minor. The classes I loved (both in and out of major) are the ones I still remember. ,1533003900.0
,"I did biophysics, CS, and a third major in a mix of EE and applied math. The latter two have served me very well in my biophysics career, which has actually included some computational neuroscience research. If you're interested in neuro, having the CS skillset will put you in high demand for grad school/industry. ",1533007365.0
Optimus_Pr1me,"I’m currently trying to transfer from my Computer Science (Honours) to a double degree in Computer Science and Medical Science.

I’ve heard double majors or double degrees especially in a Science reveal major opportunities for research programs. ",1533008351.0
norse_dog,"Having done a glorious Math/CS double major, I'd strongly advise against it. It is a lot of fun until you hit the final stretch and realize that you've roughly doubled the workload. You are faced with the choice to either spent more time than average to graduate or give up one of two subjects you at worst love and at best have invested a lot of time into.

Ultimately, a major has more to do with signalling (I can complete a complex set of requirements in an intellectually challenging area) than growth and learning. Just one major satisfies the signalling requirements; you can learn and grow without the stringent requirements associated with a full second major.",1533078590.0
AforAnonymous,"Text of this comment shamelessly stolen from /u/arXiv_abstract_bot, with some light editing for MathJax & better paragraph breaks:
>Title: What Does This Notation Mean Anyway?
>
>
>
 Authors: [David Feller](http://arxiv.org/search/cs?searchtype=author&query=Feller%2C+D), [Joe B. Wells](http://arxiv.org/search/cs?searchtype=author&query=Wells%2C+J+B), [Fairouz Kamareddine](http://arxiv.org/search/cs?searchtype=author&query=Kamareddine%2C+F) (ULTRA), [Sebastien Carlier](http://arxiv.org/search/cs?searchtype=author&query=Carlier%2C+S)
>
>
>
>Abstract:  
>Following the introduction of BNF notation by Backus for the Algol 60 report and subsequent notational variants, a metalanguage involving formal ""grammars"" has developed for discussing structured objects in  Computer Science and Mathematical Logic. We refer to this offspring of BNF as Math-BNF or MBNF, to the original BNF and its notational variants just as BNF, and to aspects common to both as BNF-style. What all BNF-style notations share is the use of production rules roughly of this form:  
>  
>[;\bullet \mathrel{::=} \circ_1 \mid \cdots \mid \circ_n ;]  
>  
>Normally, such a rule says ""every instance of [;\circ_i;] for [;i \in \{1, \ldots, n\};] is also an instance of [;\bullet;]"".  
>MBNF is distinct from BNF in the entities and operations it allows. Instead of strings, MBNF builds arrangements of symbols that we call math-text. Sometimes ""syntax"" is defined by interleaving MBNF production rules and other mathematical definitions that can contain chunks of math-text.  
>There is no clear definition of MBNF. Readers do not have a document which tells them how MBNF is to be read and must learn MBNF through a process of cultural initiation. To the extent that MBNF is defined, it is largely through examples scattered throughout the literature and which require readers to guess the mathematical structures underpinning them.  
>This paper gives MBNF examples illustrating some of the differences between MBNF and BNF. We propose a definition of syntactic math text (SMT) which handles many (but far from all) uses of math-text and MBNF in the wild. We aim to balance the goal of being accessible and not requiring too much prerequisite knowledge with the conflicting goal of providing a rich mathematical structure that already supports many uses and has possibilities to be extended to support more challenging cases.  
>
>
>
>
>[PDF link](https://arxiv.org/pdf/1806.08771) [Landing page](https://arxiv.org/abs/1806.08771)
",1532995566.0
NotJoshua-,"I quickly skimmed through the paper, and I do not understand what the authors are trying to achieve or what problem they are trying to solve (the abstract is very confusing and there's no proper introduction).

Could someone that understands the paper try to explain it?",1533046499.0
laptop_overthinker,Why does the author Fairouz Kamereddine have (ULTRA) after their name?,1533018306.0
possiblyquestionable,"I skimmed, I reread the abstract, I even read the first few pages in its entirety. Still, I have no idea what MBNF is, why it's interesting, how it's different from BNF (beyond producing ""math-texts""), and what the authors mean when they claim that most papers out there seem to specify mathematical structures in a BNF-like form. Am I just not reading enough modern papers to see this trend of explicitly ""syntacticizing"" everything or are these implicit constructions? Skimming through the remaining sections seem to suggest that MBNF just gives interpretation to your garden variety operators in various contexts, which seem to suggest that the aim is to explore the idea of implicit constructions, but this is not spelled out anywhere.

Edit: I guess this may be been worded poorly. I am interested in this area, and some of the examples within this paper sounds very intriguing. Unfortunately, I'm having trouble understanding the motivation and the core aspects of this paper. I would love to be able to chat with the author (if OP is an author) about its motivation and its ideas if possible.",1533022533.0
cbarrick,"It would be way more informative if the rotations were animated. That's by far the most important part of AVL.

As I see it, the rotations are instantaneous, so it's hard to see the relationship between one frame and the next.

The only part that's animated is walking the tree, which isn't so important.",1532996764.0
shakespeareanseizure,Are [zoos](https://complexityzoo.uwaterloo.ca/Complexity_Zoo) out of [style](https://math.nist.gov/quantum/zoo/) now?,1532940687.0
DevFRus,"Oh gosh, metaphor-based algorithms...",1532986235.0
aunva,[Relevant xkcd](https://xkcd.com/1827/),1532970364.0
green_griffon,"The cool thing about AVL trees is you can track and update the balance factor without knowing the depths on either side, just knowing the previous balance factor and what operation you are doing. ",1532906869.0
faintedremix009,This was the hardest data structure to implement during my data structures class. Great article btw!,1532914588.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/u_ang3lplay3r] [Tutorial on binary trees](https://www.reddit.com/r/u_ang3lplay3r/comments/932tle/tutorial_on_binary_trees/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1532945375.0
,"[GEOHASH](https://redis.io/commands/geohash) has some interesting properties. For example, you can remove characters from the right hand side to get a hash that points to the same area, but less precisely. Maybe you could draw inspiration from that.

However, keep in mind that clever methods exist for unmasking users based on ""anonymized"" data. See the Netflix Prize incident for an example.",1532880643.0
zergling_Lester,"> a project that would allow sites to publish some sort of ""token"" along with user posts that would provide partial information about the poster's identity, without completely unmasking them.

So like https://en.wikipedia.org/wiki/OpenID ?

> Does anyone have any thoughts on a system that might eg: allow geolocation down to a country level?

Why would you want that if your goal is persistent identity without self-doxxing?",1532904725.0
stufuller,GNU = GNU's not Unix.  It's recursive. ,1532834827.0
enp2s0,"GNU HURD kernel

HURD stands for HIRD of Unix Replacing Daemons
HIRD stands for HURD of Interfaces Representing Depth.

2 mutually recursive acronyms",1532848151.0
,[deleted],1532839846.0
Neker,"Back in the days, Bell Labs had a big serious corporate project in the work, an OS enabling time-sharing, that is allowing one computer to be used simultaneously by *multiple* users. Accordingly and quite unoriginally, said OS was christened *[Multics](https://en.wikipedia.org/wiki/Multics)*.

Meanwhile, a pair of renegade programmers were left to their own device in a forgotten office and started a pet project that they called *Unix* in jest. The rest is history.",1532847147.0
SupremeEntropy,"I know some pretty bad puns. Here's the one:

""What tree is often being mistaken for a bamboo?""

""Sssplay tree""",1532858069.0
bartturner,"Windows NT was designed by David Cutler that also did VMS.

So WNT was thought to come from V - W, M - N, S -T.

Not super convinced.  Not really a pun.  but thought I would share.

BTW, a techie and not a word person so all I got.",1532868321.0
ichweisswodeinhauswo,https://en.wikipedia.org/wiki/Set_cover_problem,1532822635.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1532814923.0
trichotillofobia,"Perhaps it would be nice if the article mentioned that AVL trees are O(log n) for insertion and deletion, whereas perfectly balanced trees are O(n log n), IIRC.",1532882973.0
jasoncarz,I guess women stopped? How is this homophobia? I think it’s called a manner of speaking. 🤷‍♂️,1532810631.0
even_though,Downvote for homophobic slur.,1532805592.0
fallen77,"Can't load the page. 
""An appropriate representation of the requested resource could not be found on this server. This error was generated by Mod_Security.""",1532804723.0
s32,Decent writeup but I disagree that load balancers are great because they can deal with ssl. I'd rather terminate tls at the host layer with my lb being essentially TCP pass-through. Usually when folks terminate tls at the lb they end up with connections between backend hosts and lb running over http. ,1532876817.0
krapht,"CS degree with math minor, because there are far more data engineering positions than data scientist (what is that, anyway - basically a fancy name for a statistician with some computer programming ability).

P.S. If you're dead set on the data analysis and modeling part of data science, get a graduate degree. M.S. is sort of a minimum hurdle here unless you luck out with the correct internships.",1532782042.0
Sparklingcobweb,Study something stats-heavy like physics or actuarial science and learn to code in R and Python along the way. ,1532787681.0
omon-ra,xUnit Test Patterns: Refactoring Test Code by Gerard Meszaros ,1532756883.0
tontoto,Depending on what type of code you write might factor into what recommended reading you might want,1532787454.0
a-buttclown,"Pragmatic unit testing with java 8 version 2

This is a great book to learn test driven developmemt in java! ",1533065418.0
khedoros,"I'd start at r/learnprogramming. You'll get lots of answers for what language to start with, especially depending on what you'd eventually like to do. If you haven't decided what your final goal is, Python3 would be a good place to start, in my opinion. However, expect other people to have other opinions. And expect it to be a lot of work. It takes a while to learn the basics of your first language, even longer to start seeing how to break problems down, and even longer than that to actually be good at doing it.",1532741615.0
xShadowProclamationx,"pick a language, get a book, do the lessons... rinse repeat. 
",1532741624.0
therealtonyryantime,"Udacity. Intro to Computer Science. Strongly recommend. Teaches python, but  the language itself isnt the biggest reason I’m recommending it. The pacing and level of difficulty was perfect.",1532742905.0
Lord_kobok,"Python should be good. It has a simple syntax, but you will learn the basics of programming fast. If you really, really want to go deep into programming you can lateron learn languages like C++.

Scientists often use Python with libraries like Numpy: Numpy is written in C++ and therefore is very performative, but is used in python with the simple syntax, therefore being comfortable to use.",1532767648.0
Jaxan0,"I started programming in GameMaker (it has its own scripting language). It’s nice, because every code you write has a direct visual influence. That way it’s quite easy to see what each construct does.

In the end, the choice of language is not too important. As long as you can make something you care about or are interested in. ",1532779445.0
keksper,r/cscareerquestions,1532721138.0
tjscollins,"From someone who switched majors too many times (and after my 3rd year to boot), the sooner you switch the better. Don't drag it out. On the other hand, be sure it's what you want, so you don't keep switching. ",1532725972.0
javaHoosier,"Since you’re looking for just advice. What school? Some key things to consider. A lot of people ask if Math is necessary. While it’s not crucial to be amazing, some math will pop up here and there. In my experience mostly linear algebra, Stats/Probability theory, and should have a discrete class. 

The tug of war between programming and the theory. Everyone will struggle with some of the theory at some point. CS by it’s nature will have a lot of homework and struggling to understand some concepts. Be prepared to just push through.

I recommend trying the Java helsinki MOOC as a test drive to see how well you can focus on the material.",1532769060.0
elcric_krej,"Try teaching yourself Cs & programming and keep doing chemistry, the skillset you come out with could actually be more valueable.",1532789181.0
tedl999,No,1532820923.0
fiedzia,"> Has there ever been any successfully created operating systems in recent years that don't have ties back to these giants?

There were few, but I'll start with Linux. While describing as ""based on unix"" is technically and historically true, being one of most active software projects ever, large parts of it have been and keep being redesigned and reinvented, and today have absolutely nothing to do with its roots. To very large degree Linux provides an environment for research in OS space.

Some somewhat successful oses created recently include Fuchsia, many members of the L4 family, Redox.
There are many more in the embedded space, and several new oses created for teaching purposes.
But nothing on the scale of Linux.

>And if not, how hard would it be now for someone to even have a chance at starting an open source project to create an OS from the ground up collaboratively, kernel and all?

Creating an OS is trivial. All that needs to be done has been done 1000s of times before, is well documented and accessible. Getting other people to use it is where the challenge is. Github is full of operating systems that boot, have some device support and some novel ideas - but nobody cares. OS is a social thing far more than a technical one. My apps work on Linux. My company standardized on Linux. My hardware supports Linux. I know how Linux works. People have support contracts for Linux for next decade and will use for the products they will build years ahead (you can replace with Linux with Mac or Windows, the point is that there is no place for new OS).
Anything else would be a hard sell even if it was 10x better, faster or better designed.
Existing systems provide 99.99% of what everyone needs and allow you to add the 0.01% of customization when required, so there is absolutely no point in redoing all of this.

You may succeed with new os in very few scenarios:

1. In a niche existing systems can't fill (IoT, embedded devices, robotics)
2. In a very large and influential tech company where own OS may provide competitive advantage
3. In situations where ""your own"" is more important than ""good and cheap"" (governments maybe).

Overall, the need for new OS is not great. I'd love to see more competition, but it's very unlikely.",1532721474.0
TheyOnlyComeAtNight,">“If AI researchers really care about being covered thoughtfully and critically, they should come together and fund a publication where writers can be suitably paid for the time that it takes to really dig in.”

Or maybe journalists should ask for comments from actual AI researchers instead of going for high-profile celebs that don't work in the field.",1532701481.0
redteddy23,Too much trolly folly and abstract AI ethics. I can tell you what AI drivers will do under all circumstances. Whatever reduces insurance premiums.,1532701129.0
Andernerd,Maybe someone shouldn't have decided this should be called AI. It's obvious that non-CS people would see it this way.,1532701699.0
EmperorOfCanada,"As AI proceeds to become easier and easier to implement against complicated data sets. The whole Citizens Index scoring crap will get easier and easier. 

Right now it is pretty hard to incorporate school records, payment records, travel records, driving records, etc but slowly AI will make this easier.

If a government can do this why wouldn't they do this. Information is power. The slowly but surely this will start to factor into life. School admissions, job background checks (insurance and companies that have you drive want a drivers ""abstract"").

Then there will be the conservative desire to punish the sinners. Thus people with low CIs shouldn't get full access to society's goods if they aren't going to play by society's rules (i.e. their rules) so no or delayed passports for people with poor CIs. 
You go to court and they look at your CI. You do poorly.
You are going through customs and they look at your CI.

This is the sort of crap that AI will deliver where most of us loose.

",1532732387.0
jonathancast,"Machine learning weights are _not_ algorithms.  We have words for a reason.  For someone so concerned about precision in language, this author isn't very concerned about precision in language.",1532701516.0
Kins97,Because algorithmic discrimination doesnt really hurt the rich so politicians dont give a shit ,1532706944.0
mikeiz404,">  “What really needs to happen is better training of journalists and more integrity,” he says. “Until that happens, my blog is just a pebble in the rapids of crap. I’m not altering the direction of the stream.”

Love the imagery here.",1532716749.0
PunsForHire,I love when people (EDIT: journalists who write on emerging science for a popular audience) seem to have never heard of STS as a field of study.  Asking an AI researcher about the past and potential impacts of AI on society is so misinformed about how the field works.,1532722984.0
rhoark,"When journalists write about discrimination in algorithms, they really mean **non**\-discrimination in algorithms.",1532721779.0
lawnm0werMan,The next time I write something for school that generates the wrong output I’ll just argue they don’t understand my language smh,1532707556.0
dom85esc,If these ai are capable of crunching unfathomable data sets as I believe your saying. Is it that much of a stretch to say that ai is creating all the plank pixels in this enormous ai generated hologram of what we believe to be reality?,1532757555.0
Triple_Elation,"Algorithmic biases and discrimination are real issues that we should contend with, and not sweep under the rug. That said: when someone looks at the incredible inflection point in history that is the emerging improvement of AI -- all the opportunities, all the pitfalls -- and then they proceed to discard 99% of all that in order to get conspicuously upset about discrimination and algorithmic biases -- that tells me more about where that person is coming from than it does about AI.",1532772919.0
VorpalAuroch,"Neither of those things is important. ""Not ending the world"" is important. Discrimination, robots; these are just deck chairs on the Titanic.",1532722332.0
TotallyNotARobit,"Computer science is a wide and diverse subject in the same way Math is. Just as doing a proof is significantly different than solving algebraic functions, one part of CS will be significantly different than another part. 
However, again like with math, almost all more advanced CS topics require a basic skillset. This would be your arithmetic and basic numerics equivalents. 
This isn't to say that introductory CS is like relearning arithmetic in terms of how you have to think. 
To finally get around to answering your question, basic CS is like learning a set of tools and then figuring out what combination of applications of those tools will get you to your goal. I've always viewed generall CS as a puzzle game where you have to construct a path from start to finish with obstacles and resource limitations that are situationally dependent. In that sense, it is like solving either geometric or algebraic proofs where you have something you start with and are trying to transform that something into something else. You have to learn how the tools work together to produce different results and how to efficiently and progressively transform your initial situations into situations that are closer to the end situations. 

Hope you can glean something from that. Are you considering taking an intro CS class? Feel free to ask any other questions you have.",1532678502.0
flexibeast,"At the very least, i don't feel it's synonymous with 'software engineering', whatever various courses might imply ....",1532677567.0
,"Would you say mathmatics is like writing proofs for a geometry class or breaking down functions in algebra? You can see that this question is an oxymoron: both those fields are mathmatics so it isn't more like one or the other. CS is the same way, it's too diverse to neatly categorize. 

However many of the proofs I had to write were a mixed bag where the proofs were very verbose algebra statements, where you'd write three parahraphs of text with random letters replaced with the greek alphabet. The more ""mathmatical"" proofs were more like algebra. The more ""computer-y"" proofs were more like geometry. The most commonly used proof was a form where you state a problem, introduce a bunch of symbols you'll use in your proof, and then ""prove"" things mostly via making logical statements and saying ""If that holds true, then it must follow that F(X) is also a member of Y for programs produced by A"". 

Computer science is just another field of mathmatics, it's the mathmatics of computation, algorithms, and a bunch of related fields mixed into the lot. Computer science often involves graph theory, discrete mathmatics, logic, theory of computation (Turing machine proofs are black magic and nobody can convince me otherwise), and a bit of abstract algebra depending on how far down the rabbit hole you go. Additionally CS has the practical fields added to the mix as well, such as programming, system architecture, operating systems, software engineering principles, and so on and so forth. 

A standard CS degree would start with you taking a bit of calculus and an intro programming class. Then you'd move into data structures (The different ways to store and orginize data), advanced programming, and system architecture (Learning how a computer works under the fancy facade of the UI). Eventually you go to more theoretical aspects, such as logic, AI design, Operating Systems, theory of computation (What *is* a computer, and how do we reason about different types of computers and algorithms), algorithm design and proofs, and so on and so forth. ",1532685586.0
,"The content tables from Wikipedia's entry for Computer Science might give you an idea for what's computer science (this is from the section ""Areas of Computer Science"").


    4.1 Theoretical computer science
        4.1.1 Data structures and algorithms
        4.1.2 Theory of computation
        4.1.3 Information and coding theory
        4.1.4 Programming language theory
        4.1.5 Formal methods
    4.2 Computer systems
        4.2.1 Computer architecture and computer engineering
        4.2.2 Computer performance analysis
        4.2.3 Concurrent, parallel and distributed systems
        4.2.4 Computer networks
        4.2.5 Computer security and cryptography
        4.2.6 Databases
    4.3 Computer applications
        4.3.1 Computer graphics and visualization
        4.3.2 Human–computer interaction
        4.3.3 Scientific computing
        4.3.4 Artificial intelligence
    4.4 Software engineering
",1532683363.0
LifeHasLeft,"I like to think that physics is applied math, and chemistry is applied physics, and biology > psychology > sociology etc. But l also think that computer science is in a sense, applied mathematics. The same way mathematics is not just about numbers, but about logic, proofs, and solving problems ",1532698983.0
bargle0,"Sometimes.

Sometimes Computer Science is like a physics experiment.

And sometimes Computer Science is like banging on an engine with a wrench until it starts.

Sometimes you do all three things in one day. It keeps things fresh.",1532691349.0
Meguli,"To me, CS is constructive mathematics. ",1532685397.0
throwme2DstarsNback,"CS grad here. i have a very different interpretation. For me it's like an art class. At the beginning, you learn theories and styles and history. Then at the latter part, you do your own artwork, your own programs with their own functions. Just like in art where train yourself to imitate the world, in CS you train computers to try to imitate the world, or at least follow how we humans think. Not sure if I made sense, but I think CS is beautiful.",1532681295.0
Artemis__,Computer Science is the science of solving problems.,1532687758.0
mdillenbeck,"From my CS program, if say yes. However, from some biology courses I took, you need to remember that mathematics doesn't always look the way you expect.

So one aspect is learning theory, such as algorithm design - and in doing so you are learning the mathematics behind it. You'll learn about comparison complexity, run time, boolean algebra for circuit design, prepositional logic, ands a sampler math course on discrete math topics.

There is some applied aspects to the course work, like understanding data structures - and a lot of that work ties into discrete math - and courses on instruction set code. As you can see, some work is to establish an understanding of how modern computers work.

Of course, CS isn't standardized. Some schools use to mean programming courses, some use it to mean IT courses (maintaining servers, designing networks, etc), and some use it to mean software engineering.

Basically, look at different school programs and the course work to determine what they mean.  Mudding it, many CS students go on to work in fields that are more about applying their knowledge to a vocational skill (programing) and not pursue any academic goals of ""Computational Theory"".  However, in my personal opinion if you aren't learning about algorithms and their complexity or how to determine your own solutions's correctness, completeness, and complexity then you aren't learning what I know as computer science.  Essentially CS to me is about learning how to solve problems (and show why your solution is better than others out there).",1532684539.0
GayMakeAndModel,"I like to think of CS as applied philosophy.  There will be truth tables, proofs, and graphs; they will be used to solve practical problems.",1532689452.0
Chandon,Have you ever played a video game (or even a tabletop game) and studied the game mechanics in detail to try to take advantage of them? Computer science is more like that than like anything you'd do in math class.,1532706289.0
engineering_stork,"If you are looking for the connection between maths and cs, this book has come highly recommended to me:  [Elements of Programming](https://www.amazon.com/Elements-Programming-Alexander-Stepanov/dp/032163537X?pd_rd_wg=1Wxl9&pd_rd_r=60b6312d-76ca-401a-b697-81944566122f&pd_rd_w=GF20K&ref_=pd_gw_cartx&pf_rd_r=5WCCHJTZAYTV76Q8RD7D&pf_rd_p=6b27b606-ba71-500d-b5f8-3510f86504b4) by Alexander Stepanov

Written by the man who created the C++ STL, it examines computation by breaking it down into its mathematical underpinnings. ",1532874664.0
dmercer,"Basically, everyone here (so far) is either wrong or just being pedantic. To answer your question, it is more similar to geometry proofs. You've got a place you're trying to get to, and you pick your starting spot(s) and work toward it step-by-step.",1532696144.0
Blackweb_CLX,What language can you recommend for an upcoming computer science student? Ill take it next year (hoping that i will pass the entrance exam without knowing anything about cs),1532656054.0
Will301,"For those that started programming late, how did this affect your pursue for your degree in CS and job opportunities? I'm asking this because I recently switched from Finance to CS with no programming experience what so ever and I feel like it will affect me heavily being a late starter.",1532656758.0
buddy-pls,"Hi everyone, I'm a cs undergrad applying to grad school this fall. I'm currently working as a software development intern at a large (20000+) IT company.  They have basically already offered me a full time position for when I graduate, but unfortunately I let slip to my boss that I might be planning to attend grad school. They told me that because I'd be full time for less than a year, they won't be able to give me an offer if I do go through with it , but in this case I can continue working as an intern until June, when my contract will terminate.  My question is: should I be applying for full time roles for the upcoming year and give my 2 weeks when I need to leave,  or just trying to land an internship for next summer? Applying for a research residency is an option as well but it won't contribute to my grad school applications since the application cycles' timing are basically the same. Thanks for reading.",1532664943.0
NicktheSwimmer,"I'm a highschooler learning computer science on my own right now and in the AP class this coming school year. Is there any advice you can give, perhaps things you wish you had done during highschool?",1532667214.0
Firetaffer,"I've been OUT of computer science for 2 years, having gone to philosophy.

I had a great time but now I'm returning because I want MONEY and already have invested into this skill set. I am loving the come back, and am learning C# which is new for me as I mostly did Python.

Can anyone recommend a refresher? Perhaps some materials, a course, something? I plan to enter the job market at the start of next year. In Software Development.",1532934119.0
,Nice spam. Not even a related sub,1532645585.0
lrem,Looks like you've sent this to the wrong sub?,1532637721.0
kevln02,/r/lostredditors ,1532638474.0
T_OHAIRE,If you’re serious about joining the air force do it now before you finish school and have them pay for it. Best bet for you IMO would be ROTC at whatever school you are at. ,1532616376.0
karma000,"Comp engineers can learn software engineering pretty easily, but it’s a lot harder for a computer science student to learn hardware stuff. But if you don’t have a passion for hardware, there’s no point sinking time into that degree. 

Comp engineers work harder than computer science students based on what I’ve observed. Of course some CS students are work horses too (you get out what you put in). 

Whether you can balance sports and a hard science degree depends on how smart and disciplined you are. If you aren’t sure if you can perform without smoking weed, that’s a red flag in my opinion.",1532620232.0
_theFaust,"I studied computer engineering in undergrad and the focus was chip design, computer architectures and VLSI. I realized a little too late that I would’ve preferred to have done full electrical engineering or full computer science. I’m a software engineer now. ",1532621590.0
jmd27612,"This may be more appropriate in r/datascience, r/analytics, or even r/machinelearning. Manually marking the data is part of data preprocessing to build a training data set. You build a model on the training data, in this case a classifier, and then test and deploy. “Pre-handling” the data is 90% of any analytics project. Coding and building cool models is barely the other 10%. ",1532554749.0
TomvdZ,Why don't you try calculating how large such a table would be?,1532543670.0
paul_miner,"[181^30](http://m.wolframalpha.com/input/?i=181%5E30) for a full table is approximately 5 x 10^67, gonna need a much more sparse or lower resolution table, or drastically reduce the number of dimensions. ",1532545648.0
disposable_silence,"A lookup table is unlikely to be feasible, as u/paul_minor pointed out. If you really have 181^30 different answers, without any structure, I have no clue what you’re trying to do—there’s no chance in the lifetime of the universe your users will request a reasonable fraction of those.

However, if there is some structure to the answers, I see two possible solutions.
1. Try to come up with a formula that relates the dimensions, so you can calculate the answers.
2. Use a decision tree. Find out the few (less than 180) different relevant cases, such as “below 30” or “divisible by 15” for the first dimension, and depending on the answer, move on to one of a few next steps in your tree.",1532552736.0
theblacklounge,Just following all references will be more costly than calculating a hash in place. ,1532553655.0
BenjiSponge,"This guy is either a troll/satire or a relentless hack. I think he's trying to show everyone that it's really easy to be respected if you just say controversial things on Medium, but it's not working because everything he says is complete nonsense backed up by nothing.",1532540361.0
jamespunk,Lol,1532540411.0
dro80,"No new information presented to further argue their point, just a lotta shitposting, dude, how much money did u lose? ",1532541624.0
ooa3603,Probably saw GeorgiaTech's OMSCS program making bank and wanted to get in on those sweet sweet dolla dolla bills.,1532535293.0
ChrisC1234,"This is NOT a masters degree in Computer Science.


CIT 591 - Introduction to Software Development (one of the required courses)
 
>This course is an introduction to fundamental concepts of programming and computer science. The course starts off with an introduction to modern programming languages and aspects such as basic data types, loops, and conditionals. The course will cover features of Object-Oriented programming languages including objects and classes, inheritance, and interfaces. In addition to programming, this course will also focus on best practices and aspects of software development such as software design and software testing. We will use Java for the entire course. 

and

>There are no prerequisites for this class. No prior programming background is expected.

In all honesty, my **bachelors degree** in computer science was more rigorous than this masters degree.

",1532540165.0
vengentz,The degree is Master of Computer and Information Technology. This is not REMOTELY the same as a master’s in Comp Sci.,1532536274.0
fnybny,Degree mill,1532545633.0
so_zetta_byte,"Alright, there are a lot of questions and assumptions being thrown around here. I'm very familiar with Penn and the department (I did both a Bachelors and Masters in CS there) and can probably clear things up.

MCIT was generally billed as a Masters degree for people who didn't do CS/Math/Engineering as an undergrad. I've known many people in the program. It's roughly equivalent in knowledge to a CS bachelors, minus a few electives. The undergrad core courses at Penn all essentially had MCIT equivalents. While I never took them, from what I heard they tended to be slightly easier than the normal undergrad equivalents, though it's not clear that it was because they were watered down. The class sizes (I assume) for most of them are likely smaller, and since MCIT students already have bachelors degrees they don't need to spend time on more general requirements so they're taking less classes at once.

I don't think it's fair to call this a ""Masters in Computer Science."" When I was at the school there was a big and clear distinction between CS and MCIT grad students/degrees. I think MCIT was/is best for people who don't have a backround in CS who are looking to switch careers. 

Anyway. If anyone has any questions about the program, school, or department I can hopefully clear things up.",1532588721.0
MirrorLake,"This is disturbing to see an ivy league school acting like a degree mill. Are other reputable schools doing this, too? It’s barely a bachelor’s degree.",1532583925.0
,[deleted],1532535788.0
atli_gyrd,I see all your comments about how this situation kinda scammy. Any advice on an online cs degree? Seems like most online only courses are in IT not CS.,1532568141.0
metalloidica,Not Computer Science... if you want to spend the rest of your life at a customer support desk....go for it..,1532561468.0
imgyatso,Done. I'd love to see the results!,1532527799.0
grelphy,"Minor methodology note:

> I am scared of new knowledge and ideas that I do not understand

…is extremely leading. Very few people will agree with that, even if it is accurate. I don't know enough study design to say how it *should* be phrased, but I'm sure that's not ideal.",1532530563.0
DevFRus,"Chaitin's metabiology is one of the many factors that inspired me to look at evolution through the algorithmic lens. But I had thought that Chaitin's approach was mistaken. 

Two of my earlier blog posts on algorithmic biology aimed to explain these mistakes back in 2012. In the first, I reviewed _Proving Darwin_: [Is Chaitin proving Darwin with metabiology?](https://egtheory.wordpress.com/2012/06/29/proving-darwin/) and in the second, I responded to a blog comment of Chaitin's: [Critique of Chaitin’s algorithmic mutation](https://egtheory.wordpress.com/2012/07/08/algorithmic-mutation/). It was nice to see these posts cited in the OP paper. And the positive reaction of this subreddit to these posts years ago is one of the things that encouraged me to continue blogging. So thank you, Reddit.

Six years later, I would express myself differently and focus on slightly different aspects of Chaitin's work. However, I still think it is inspiring but not able to achieve its aim. Theoretical computer science as a whole, though, has a lot to offer evolutionary biology.",1532502040.0
INDEX45,Thanks for this. I am a big fan of his earlier work and didn’t realize he had been working in this area now. ,1532520647.0
shakespeareanseizure,"For basics (undergrad level), Theory of Computation by Michael Sipser. Starts at automata theory, then computability, then the last part is complexity theory.

The Arora and Barak is actually really good and readable. The proofs in there, for at least the beginning parts are not really new to complexity theory (things like SUMCHECK, PSPACE in IP, Karp-Lipton), so if you need a more extensive proof you can just look them up and find lecture notes.
You can skip around mostly too, the introduction lists the dependency graph.

Just make sure to get the latest version of the Arora and Barak. The preprint one is easily accessible online for free but it's full of errors (grammatical and mathematical) that are confusing. The first edition fixes all of them.

There's an older text by Papadimitriou but it's not as modern or accessible as the Arora and Barak. Finally, I've heard good things about Oded Goldreich's Computational Complexity: A Conceptual Perspective, but I haven't read it myself.",1532492452.0
PunsForHire,"I'm late to the party, but Sipser is THE undergraduate/early-graduate text in the field as far as I'm concerned.  Besides introducing most of the fundamental notions well, it's jauntily written and helped pioneer the Theorem/Proof-Idea/Proof structure.  Alongside Spivak's Calculus text I think it's one of the most engaging, thought-provoking, well-written texts out there.  And the questions in the back of each section range from trivial 2-minute exercises to some interesting former research problems (I'm thinking in particular of the question asking to prove that any language decideable in o(n log n) on a single-tape deterministic Turing machine is regular).",1532723496.0
rosulek,I have used Jonathan Katz's lecture notes before as a course reference.,1532494100.0
DoesNotLikeBroccoli,Stanford has a course on their online platform lagunita https://lagunita.stanford.edu/courses/course-v1:ComputerScience+Automata+SelfPaced/about,1532543066.0
Chizzyfricory,This set of lectures helped me the most: https://www.youtube.com/playlist?list=PL7HjUNIdk93ThXvz2Oa_g30Jt3Owwm4HZ,1532503415.0
jet_heller,Is that 512k for everything or dedicated to each core?,1532471638.0
GNULinuxProgrammer,What is this? Intel fanboy thread?,1532475025.0
Cras_es_Noster,"For comparison:  

Intel(R) Xeon(R) CPU E5-2699 v3 @ 2.30GHz 

Cache: 46,000 KB.

e5-2699 is a HEAVY BOY, though.  I'm not sure what a more fair comparison would be.",1532471435.0
PinkyWrinkle,I use sublime text 3 for almost everything ,1532458497.0
Andy_Reds,"No, I don't. I use pycharm for python, and intellij for java, they're incredible. I develop C++ in XCode, though. I've tried using a text editor such as atom, but I just find it to slow me down too much on large projects when compared to a good IDE.",1532460466.0
specification,vs code only here ,1532464636.0
wsqmkgecb,"I switch between different ones. Xcode for c++ and IntelliJ for Java. 
For anything else I go with emacs (or sublime more recently)",1532461511.0
,"I'm programming Elixir at the moment, and using vscode, and when I was using PHP I was on PHPStorm/IntelliJ. I've only ever used XCode for iOS programming specifically because I think getting an IDE with the best language specific support is going to give me the best experience.",1532461817.0
mrexodia,"I use Visual Studio for Xamarin, Atom for go, PyCharm for python and Qt Creator for C++ (not just for Qt projects). For other stuff I use vim or sublime text, whichever is more convenient.",1532464999.0
C1995O1,"I just like to use what I'm used to so I stick with vim for everything except intellij when I do jvm related things.

Just try and use something that can do most things instead of switching between a ton of tools for specific task imo.",1532476524.0
brettmjohnson,I use Emacs for almost everything.,1532497740.0
Nerdlinger,"This isn’t really a question for /r/compsci. /r/LearnPython or /r/LearnProgramming are better locations.

Having said that, look up “short circuit evaluation” and what values evaluate to _false_ and _true_ in Python. That should lead you to the answer to your question. ",1532453286.0
combinatorylogic,Reading and writing are also considered a necessity. Do you feel threatened by the amount of competition?,1532442417.0
khedoros,"I'm primarily a C++ dev with experience on half a dozen Unixes, creating and maintaining build systems, an interest in virtualization and emulation, and a history of jumping into parts of a project that the other devs on the team don't want to.

>where coding skills are considered a necessity and every other person is into programming

Programmers, especially decent ones, are a relatively scarce resource. There are plenty of dabblers, but not enough professionals to satisfy market demand.",1532452773.0
future_security,"There is no shortage of terribly implemented programs in the world. One solution to the problem is to hire as many programmers as possible to keep up with all the bugs and vulnerabilities without much regard for skill. I don't think it's a good solution, but it's evidently the solution industry uses for pacemakers, smart phones, routers, operating systems, cars, IOT, and just about every other type of technology that manages to stay vulnerable and buggy or grow more vulnerable and buggy. 

Even though it seems competitive, there are worse industries to be in. This one has high demand for labor. There are ""unskilled"" jobs that are more competitive in part because they're on the verge of becoming (or past the point of becoming) unnecessary. You have to be a much more perfect employee in that world than in coding.",1532481120.0
BenRayfield,"How would the algorithm perform on the numbers log(multiply of any primes), first d binary digits of each, of which no set of them summed (if infinite digits) equals any other (because every positive integer is the multiply of a certain bag (set with dups) of primes)? For example, log(10)=log(2)+log(5) for any log base.

I dont think very well, nor for any other algorithm, since log usually gives back irrational numbers.",1533586931.0
Periplokos,Excellent result,1542330087.0
ProgramTheWorld,">	#But why?
>
>	Mainly because if we make Rockstar a real (and completely pointless) programming language, then recruiters and hiring managers won't be able to talk about 'rockstar developers' any more.

",1532408580.0
BaleZur,"Yeah but if you take away people being able to use the label ""rockstar developer"" while simultaneously developing Rockstar, doesn't that make this person THE Rockstar developer? Thus getting rid of competition for the title while making your claim to fame more authentic?

I dunno guys and gals, I think this is a trick!",1532433612.0
uhoreg,"Or, if rock and roll isn't your thing, you can try https://en.wikipedia.org/wiki/Shakespeare_Programming_Language",1532403429.0
jaybol,"*My world is nothing without your love*
Initialize *my world* with the result of subtracting *your love* from *0*

",1532417993.0
thewmo,Halfway through this post I was entirely prepared to hate this; then I totally came around. #unexpected #moresensiblethanyeslyrics,1532444187.0
datanerd840,"I did not know what to expect, but I loved this and thought it was very cool. Great idea. ",1532456690.0
paxromana96,I went ahead and made a \[syntax highlighting package for Sublime\]([https://github.com/paxromana96/sublime-rockstar-syntax](https://github.com/paxromana96/sublime-rockstar-syntax)) for it. Hope it makes being a Rockstar developer a little easier!,1532493412.0
learnie,This is a good idea. Thumb up!,1532417897.0
LelouchVB,I love it. Its like encoding your source code in a maybe beautiful poet. Haven't played around with it yet but the docs and examples looks really cool!,1532474521.0
thenefilim,"Great, I hope ""Animal, Fuck Like a Beast!"" was an influence...",1532485387.0
pilotInPyjamas,Cool story bro.,1532387349.0
beknowly,You might find identifiers written in other languages but language features / existing things are not translated automatically. That would be a nightmare,1532375578.0
zokier,"Welcome to the world of US cultural imperialism. Most people just suck it up and learn (either by rote or by learning English) the English programming languages as-is. But the fun part comes when those people decide that enough is enough and use their native language for all identifiers and code. Makes sure no offshore consultancy/sweatshop is going take over that codebase! 

Of course the other alternative is not much better; they write incredibly clunky and half the time misspelled English identifiers etc. Or mix the two.

There are some (historical) non-english programming languages too; Wikipedia conveniently has [a list](https://en.wikipedia.org/wiki/Non-English-based_programming_languages)

> In English, Algol68's reverent case statement reads case ~ in ~ out ~ esac. In Cyrillic, this reads выб ~ в ~ либо ~ быв. 

Beautiful.

Conveniently languages that were not spoken language based in the first place such as APL were truly international, in the sense that the code was equally incomprehensible for everyone.",1532381845.0
Nodebunny,"most programming languages dont have internationalization... and come primarily from the US and are thus linguistically English-like.

Other countries have to deal with it, which they can use their own variable names but keywords cant be changed (if,for,etc.). Accounting for grammatical structural differences in languages seems an unnecessary complexity to introduce. Various other countries (Russia for example) do have their own languages we just dont use them in the US for obvious reasons. 

Scientific endeavors make more sense when scientists use the same lingo, which is why they use latin for biological classification and greek symbols in math... similarly English is a common ground language... in comp sci why? thats another topic entirely

chase down computational grammars and youll see its all about creating patterns with symbols... which features like keywords are essentially immutable symbols. You can use anything for a symbol but having something like Java in 5 different languages would be impractical, especially from a shared code perspective",1532385964.0
uh_no_,"nah. i just store all my data on tape. it's so dense and ought to be fast enough for anyone.

Anyway, this article is pretty good....despite my skepticality at the title .",1532374133.0
TheTarquin,"Caches are one key to scaling. But you need to know when and how to use them. If you think that the way to scale your systems is just to rub some cache on the problem, then you are going to have a bad time.",1532384115.0
,[removed],1532459031.0
falicor,/r/sysadmin is a thing already. ,1532345727.0
gnullify,Why does the state matter if it's 100% online? I know PSU offers an online BS in software engineering but not CS. IIRC ASU and Texas A&M have online CS programs.,1532294712.0
mediocre_desi,University of London also has a CS Bachelor's program offered through Coursera: https://www.coursera.org/degrees/bachelor-of-science-computer-science-london,1532315240.0
hapihakr,"Colorado State University - Global Campus, Bachelor of Science in Information Technology. https://csuglobal.edu/undergraduate/bachelors-degrees/information-technology",1532319462.0
anotherdonald,"Coincidence? https://www.reddit.com/r/compsci/comments/91sa0w/university_of_pennsylvanias_school_of_engineering/

Aha, there's also criticism: https://www.reddit.com/r/compsci/comments/91sa0w/university_of_pennsylvanias_school_of_engineering/e30olwx/",1532606156.0
purleyboy,"OMSCS Georgia Tech. Very affordable, ~$8,000, targets full time workers.
But, it is a Masters, and not an undergrad. ",1532297365.0
zinbiel6171,WGU just started a computer science program- very affordable and accredited.,1532312309.0
falafel_eater,"If you're studying online, are you sure a formal degree is what you want? I would especially check whether an online BA/BSc in computer science will allow you to be admitted into post-grad programmes in the future.  
If not and you are only interested in the degree for an industry job, it could really be sufficient to study the syllabus and follow online courses in relevant fields (data structures, algorithms, operating systems, computability & complexity) without a formal degree. For industry jobs, a degree is very often not a strict requirement if you have the equivalent knowledge and experience.",1532358795.0
green_meklar,">Can there exist two seed values which generate the same random number sequences?

Maybe. It depends on how the generator is implemented. Some generators may guarantee that you won't get the same sequence from two distinct seeds, but you should not assume that this is the case unless it is specifically stated.

>What exactly does seed do?

The details depend on how the generator is implemented. But basically, the generator performs some mathematical manipulations on the seed in order to create a 'starting state' for itself, and then proceeds by advancing the state in a certain way for each successive number you draw from it.

>Also, given 'n' sequentially generated random numbers, can we calculate the original seed?!

Maybe. It depends on how the generator is implemented. Clearly, you could only do this with perfect reliability if the generator algorithm were of a kind that guaranteed that every seed gives a distinct sequence.

>If yes, then a bit of explanation / pseudo code / approach would be great.

It would depend entirely on how the generator is implemented. Without knowing the generator algorithm, it would be tough to make any headway on this problem.

With some clever code, you could conceivably analyze long sequences from a generator and start to make predictions about the pattern and the algorithm behind it. If you knew how many numbers had been drawn so far, you could possibly even work backwards and determine what the first number to be drawn was. But this still wouldn't tell you anything about the seed with any great degree of reliability, because the initial manipulations used to set up the starting state for the generator could still be anything and your analysis doesn't address them at all.

Moreover, this is far harder than it sounds. Good generator algorithms tend to create *extremely* random-looking data for billions of draws in a row, and some algorithms in common use have been expressly designed to make it difficult for a hacker to analyze their patterns. This is not the sort of thing you could just sit down and do in an afternoon.",1532246711.0
vorlik,"a simplified version of many pseudo random number generators is something like:

next_number = complicated_formula(previous_number)

where the formula involves multiplying by big numbers with certain properties.  In this case, the ""seed"" is just the number that the program pretends was the number before the first one it emits.

In this definition, it's certainly possible for two seeds to produce the same sequence (since if they produce the same first number, then they produce the same sequence). But we can choose the complicated_function so that this is extremely rare for a randomly chosen seen (just like we can devise hash functions that are ""almost"" perfect)",1532243790.0
raevnos,"1. Depends on the generator algorithm.
2. Depends on the generator algorithm. With many non-crypto pRNGs, given enough outputs (sometimes just one number is enough) you can predict all future numbers it returns. If you know that no numbers have been generated between seeding and the ones you're looking at, you might be able to go backwards one to get the seed, again, depending on the algorithm.",1532242124.0
Kel-nage,"To answer your first question more fully, a (pseudo) random number generator works by maintaining some form of internal state, which is used to calculate the next number it provides and then advanced (how it is used and advanced varies by PRNG).

Providing a seed initialises that internal state to a particular state (how the number translates into the state is again dependent on which  PRNG is being used). Since the advancement methods are deterministic, you now know that the PRNG will always provide the same series of numbers. However, it is entirely possible that two different seeds will produce the same internal state - or that they produce two different states, but after some iterations they converge on to the same state and henceforth produce the same numbers.",1532244100.0
rosulek,"For your example (draw 3 integers in the range 0-9) there are only 1000 possible outcomes. So if you just tried 1001 different seeds you would definitely find two that produce the same sequence. Even if you try just 50 seeds, you will have a [very good chance](https://en.wikipedia.org/wiki/Birthday_problem) of finding two that produce the same sequence.

This doesn't mean that the two seeds will produce the same output *for all programs*, just for this specific program. If two different seeds are 100% identical for all programs, then it is a sign of a weak pseudorandom generator.

The other problem you described (given PRG outputs, determine the seed) is a computationally hard program for good cryptographic PRGs. You should not expect such a method to exist, as it would represent a major breakthrough for a good PRG.",1532264662.0
jeekiii,"To illustrate, let's use a shitty random algorithm that squares the number, and then returns the digits in the middle.

23 (=> **52**9) => 52 (=> 2**70**4) =>70 (=> 4**90**0) => 90

The number generated here are: 52 70 90

This algorithm has many problems and is not used in real life, but it illustrate what a seed is, and what a random number generator is.

The seed designates the first number fed to the algorithm that generates random numbers. Here it is 23.

The important thing to realize is that there is no ""real"" randomness involved. Given the same seed (23), the exact same set of operations will take place.

And yes, in this algorithm and many others some number give off the same sequence. (ex: 10 & 90) therefore the seed might be impossible to guess.

Another important thing: eventually all of these algorithm form loops, therefore the next number will be predictable. Fortunately modern random generator's loop is huge and not commonly a problem.

",1532266709.0
N00banator912,"also, since you're only generating 3 random numbers of 0-10, you have a 1/1331 chance of generating the same 3 numbers each time. in other words 1/1331 seeds will produce the same result. Obviously though, the more numbers you generate the less seeds will produce the same result.",1532271660.0
fuzzynyanko,"> Can there exist two seed values which generate the same random number sequences? 

Warning: site tends to have a lot of ads: http://dilbert.com/strip/2001-10-25

This joke sums it up well. If it's trully random, it can be hard to tell. How many numbers will be the same? Did you end up in the same spot in the random generation sequence? 

Sometimes algorithms don't need to be accurate, just ""good enough"", then combine that with the chance you run into some weird-ass edge case, then yeah, it'll probably happen. You might run into a case like ""what if you have an embedded device that only uses 4 bits out of the seed for RAM reasons?"" and so forth

A ""good enough"" example would be like a single-player video game where speed is more important than random number accuracy. ",1532286667.0
SirClueless,"You seem to be talking about things you're not too familiar with:

> This is why the bubble sort is mostly used when you have a small amount of elements to sort, and it is the preferable method of sorting for small amounts of elements because of it’s simplicity to program and it’s unnoticeable runtime.

Bubble sort is almost strictly worse than insertion sort, and is almost never used by any sort implementations. It is mainly of interest for academic reasons as it is a good example of worst-case and average-case O(n^(2)) behavior, which is generally regarded as the bare minimum for a sort algorithm to be acceptable.",1532231024.0
lawnm0werMan,"Why’d you choose a theme with go daddy instead of making your own? No offense, but if you’ve got your own site up, get creative with it! It’ll say as much about you as your content. ",1532238022.0
jmite,"An equivalent description, that isn't so loaded with philosophical connotations, would be ""functional Turing Machines"" versus ""relational Turing Machines"". That is, a DTM is one where the transition between states is a function (one output for each input set), and with an NTM it is a relation.

The definitions of P vs NP *do not care* about oracles or free will or anything like that. All they care about is, is there a path from a start state to a halting state, that is valid with respect to the transition relation (function), and what is the length of that path relative to the input size.

Whether the universe is deterministic or not is completely independent of P vs NP.",1532237992.0
znegva,"The concept of free will doesn't show up in math, computer science, or even physics because it's dependent on a model of cognition, which typically isn't formally defined, if defined at all, and certainly not universally accepted. On the other hand, nondeterminism in computer science is defined unambiguously wherever it's used. That makes it not equivalent to free will, which remains undefined because computer science doesn't define cognition.",1532279840.0
l_lecrup,"A normal Turing machine has a single instruction for each combination of state and current symbol. If the current symbol is s and the state is q, then the Turing machine is obliged to write a specific symbol s' and change to a specific state q' and move right or left (it could be that s=s' or q=q'). 

A nondeterministic Turing machine can have multiple different instructions. That's it, that's the whole difference. As long as there always exists at least one ""computation path"" that solves the problem, we say that the NTM solves the problem.

There is no connection between this and the concept of determinism in philosophy, other than the fact that the current behaviour of a deterministic Turing machine is completely *determined* by the previous configuration. The same goes for a deterministic universe.",1532263474.0
HeraclitusZ,"1) The usages of the word ""determinism"" here are connected in that they mean the exact same thing: One state of a system *determines* the next completely, such that there is one path of state evolution.

2) Determinism is not the opposite of free will. You are confusing it with *hard* determinism, the name of the stance that there is not free will due to determinism. However, *compatibilism* is the name of the stance that determinism is *compatible* with free will. 

3) Philosophically speaking, P vs NP is sometimes associated with the difference between the creative spark necessary to create a solution, and the mechanistic coldness that is required to check a solution. It also has impact on understanding of logic through Fagin's theorem in descriptive complexity theory. But most directly it just expands upon understanding of the power of non-determinism. In some systems, non-determism adds no power (like DFAs vs NFAs), and in some it adds (like DPDAs vs NPDAs). 

Edit: missed a “not”",1532211612.0
GayMakeAndModel,"Take a deterministic finite state automata.  Then allow one state to « branch » into one of two states by whatever means whether it be probabilistically or via some oracle that chooses the correct branch that leads to an accepting state every time with decision time being constant.


Also note thar every non-deterministic finite state automata can be made deterministic but with a cost assuming that NP complete complexity cannot be reduced to polynomial time on deterministic machines.

The key difference between NDFAs and DFAs is that NDFAs can move from one state to many via some mechanism.

Edit: more words.  Also note that I understand that I simplified the hell out of this, but I think the main ideas are conveyed.",1532219258.0
BenRayfield,"A clique math problem of npcomplete has a factorial number of equal views by isomorphism of the nodes, between each pair may be an edge or not, and you find a set of some size (bigger is harder) of nodes which all have edges between eachother. A nondeterministic computer could loop linearly to check if each next node has an edge to all previous nodes, branching both ways at every node. There is nothing actually nondeterministic about it since all factorial (except duplicates) of those could be sorted and explored in that order.

I dont believe in determinism or nondeterminism because they both say one thing changes to another thing, and they disagree on the ways of changing. If every possible thing were represented as some number, then a (if chance from-to is same as chance to-from) markov view would be which possibilities are nearest which other possibilities and how much.",1533586003.0
stefantalpalaru,This should clarify things: https://en.wikipedia.org/wiki/Non-deterministic_Turing_machine,1532213512.0
Nodebunny,that's what xml was. ,1532199663.0
jx4713,"I have not thought about this problem, but if you weren't aware I remember seeing snake cube puzzles in the MIT opencourseware lecture series ([6.890: Algorithmic Lower Bounds and Fun With Hardness Proofs](http://courses.csail.mit.edu/6.890/fall14/)).

Failing that, below is a paper which goes the other way and establishes the NP-completeness of snake cube puzzles. Perhaps this might give you some inspiration.

[http://erikdemaine.org/papers/HamPath\_JIP/](http://erikdemaine.org/papers/HamPath_JIP/)",1532276015.0
Hendrikto,"Take this Haskell program:

    map (+2) [1..10]

In Python instead of this

    map(lambda x: add(x, 2), range(1,11))

you could write

    plus2 = functools.partial(add, 2)
    map(plus2, range(1, 11))

Note that

    (x+2 for x in range(1, 11))

would probably be more pythonic, it’s just an example. Python has first class functions (like `map`), so passing curried functions around can be useful.

Another example: Imagine you have a function like this

    def make_person(first, last):
      return {“first”: first, “last”: last}

and you want to use it in tests, so instead of duplicating

    make_person(“example”, “user”)

everywhere, you could instead use

    make_example_user = functools.partial(make_person, first=“example”, last=“user”)

Now you can create example users easily

    make_example_user()",1532295291.0
FailsTheTuringTest,"Currying is very convenient when you like to pass around functions to other functions.

One of the applications I work on is written in Scala, and we make frequent use of curried functions. It's a very convenient way to partially apply functions without wrapping things in lambdas all over the place. As a toy example, let's say you have a function to compute a tax on some item: It's 5% on the first $1000, and nothing after that. And then you have a List of values to calculate tax on. You might write your function like:

    def calculateTaxCents(taxRate: Double, maxTaxedCents: Long, amountCents: Long): Long = {
        min(maxTaxCents, amountCents) * taxRate
    }
    
    val amounts = List(1000L, 5000L, 600000L)
    amounts.map(amount => calculateTaxCents(TAX_RATE, MAX_TAXED_CENTS, amount))

But that lambda passed to List::map is a bit ugly. Can we use currying to make it prettier?

    def calculateTaxCents(taxRate: Double, maxTaxedCents: Long)(amountCents: Long): Long = {
        min(maxTaxCents, amountCents) * taxRate
    }
    
    val amounts = List(1000L, 5000L, 600000L)
    amounts.map(calculateTaxCents(TAX_RATE, MAX_TAXED_CENTS))

If currying didn't exist in Scala, the first example would be a fine solution. But the second example is more concise. And it also makes partial application even easier: Say TAX\_RATE and MAX\_TAXED\_CENTS come from a configuration parameter to your application (since your local government changes tax rates frequently and you want to make changing this easy). A common thing to do--and we do this all the time in our app--is to pass the *partially applied* function around instead of passing all the myriad configurations around:

    val taxFn = calculateTaxCents(TAX_RATE, MAX_TAXED_CENTS) _
    val purchaseFn = doPurchase(taxFn, inventoryFn, creditCardFn) _
    
    // ...
    
    purchaseFn(order)

This isn't necessarily how I'd go about implementing this specific use case, but it's certainly a way. And when you have a big app with lots of configuration, this pattern of doing things is really intuitive and saves you a lot of ugliness (and can turn functions with twenty friggin' configuration parameters into something more manageable).",1532134293.0
rcfox,"It allows you to take functions with similar (but different) signatures and use them with a common interface.

Say you're parsing a line of text, with a different handler for each keyword that might appear:

    with open(filename) as f:
        text = f.read()
    handlers = {
        'foo': lambda x: x + 'bar',
        'qux': lambda x: x + 'hep',
        'asdf': lambda x: x + 'blah'
    }
    for word in text.split():
        yield handlers[word](word)

(I'm using lambdas here so that the code is shorter, but pretend we're using proper functions since parsing foo/qux/asdf is *clearly* a non-trivial problem.)
    
But then you realize that the filename should be included when parsing a 'qux'! You don't want to add filename as an argument for all of your handler functions since most of them won't use it and that's ugly. Instead, change your 'qux' handler and curry it:

    import functools
    def qux_handler(filename, x):
        return x + 'hep' + filename

    def parse(filename):
        with open(filename) as f:
            text = f.read()
        handlers = {
            'foo': lambda x: x + 'bar',
            'qux': functools.partial(qux_handler, filename),
            'asdf': lambda x: x + 'blah'
        }
        for word in text.split():
            yield handlers[word](word)
    ",1532146993.0
raufer92,"Currying is a really elegant mechanism. How is it useful? Have a look into this small library I have created to compose ML pipelines:

https://raufer.github.io/2018/02/08/poc-dataflow-for-ml/

Currying is really the major building block there.

I have done it just to play around and experiment with possible currying-APIs.",1539207912.0
Nerdlinger,https://www.reddit.com/r/programming/comments/181y2a/what_is_the_advantage_of_currying/,1532130211.0
justneurostuff,Would recommend to someone interested in the next step up in terms of level of detail into this topic to read Computers Ltd.: What They Really Can't Do. ,1532138462.0
jx4713,"I'm not really sure what the value of this article is, in my opinion the definitions are far too vague. I understand that it is supposed to serve as a non-technical overview of some arbitrarily chosen complexity classes, but the definition of a complexity class is not really a place where accuracy nor rigour can be skimped on.

It's really of no use to anyone to memorise these high-level definitions, if somebody is genuinely interested in complexity theory I would suggest they take a course or read a book which will offer a give a sound introduction to the area and eventually offer meaningful definitions and theorems concerning the classes that the article mentions.

For example ""BPP: Problems that can be quickly solved by algorithms that include an element of randomness."" This sentence could equally define RP or ZPP yet they are not necessarily the same thing and their potential differences are the source of much interesting discussion.  I think most computer scientists or programmers should be aware of P and the notion of NP-hardness, anything beyond that deserves a detailed and rigorous treatment to be of any use, and taking descriptions such as this with anything beyond a pinch of salt could ultimately be misleading.",1532168347.0
anamorphism,/r/codereview/,1532117652.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1532102476.0
Ottstar,"Maybe you should check out some stuff by prof. Mutlu. There are some good lectures at his channel. 

https://www.youtube.com/playlist?list=PL5Q2soXY2Zi_QedyPWtRmFUJ2F8DdYP7l",1532096936.0
Andy_Reds,Computer Architecture: A quantitative approach by Hennessy and Patterson is the best modern hardware book that I know of. They won the turing award this year for their contributions to the subject.,1532116899.0
LouMM,"Interesting question for the compsci sub, but informational nonetheless. Being an EE and a garage maker, i can tell you there are a ton of publications and content online that covers the latest and greatest in design, Discrete parts design, and more. Electronic Design covers the technologies required for engineers to make the right design decisions at the right time – quickly & efficiently. http://www.electronicdesign.com/about-us . Also Planet Analog is an online community for designers and engineers – both newbies and wizards – interested in discussing and advancing the state-of-the-art in analog design techniques, technologies, integration, and application. https://www.planetanalog.com There is also , https://www.powerelectronicsnews.com. 

Another easy way is to review adafruit and spark latest posts, videos and products, then go out and find their designs and spec sheets online. Have fun with it! Build some stuff! Make it so!",1532096099.0
_georgesim_,First you need a solid background in digital electronics and then you can pick up a good Verilog/VHDL book.,1532128922.0
disposable_silence,"That's a very broad question. You may be interested in [From Nand to Tetris](https://www.nand2tetris.org/), which shows building a computer from first principles.

Other than that, you'll need to be a little more specific. Are you looking for mechanics or electronics? Interested in tradeoffs like power vs performance, or producibility? With a slightly narrower scope, others may be able to point you in the right direction.",1532095336.0
sarcasmasaservice,RemindMe! 2 days ,1532100529.0
hokiecmo,"This doesn’t really warrant a full thread, but I’m heading back to school for compsci in a month for my first semester of college in 11 years. It’s also been that long since I’ve so much as looked at a laptop (just had gaming rigs since I started working). I could really use some recommendations for laptops under $1000 (but preferably cheaper) that would be good for school work and the occasional game of hearthstone between classes. I have a general idea of what I want spec wise, looking at an i5 or better, don’t really care about a dedicated gpu, 8gb of ram at least and I’d prefer a SSD.  I mainly just don’t know what brands aren’t pure garbage anymore. Any help would be appreciated. ",1532573056.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1532022728.0
munro98,Gleb Alexandrov has an excelent video on photo scanning using only free software. https://youtu.be/GEAbXYDzUjU,1532019603.0
WhackAMoleE,"I did one too. Pseudocode:

    forever do:
        print ""Be alarmed!""",1532023179.0
,[removed],1532008091.0
Andy_Reds,"Off the top of my head, some variation of k-nearest neighbors might work. Explain to the user what the features of a few universities are, and ask if he likes each or not.This will form your data set. Then, for each remaining university in your list, classify it as being a suitable choice for the user or not according to k nearest neighbors, and then output some list of the universities that were classified as being suitable. Look into SciKit K-Nearest Neighbors in python for the actual programming, it's pretty easy to pick up. ",1532033807.0
simplethingsoflife,"Veteran architect here. For those of you that might be discouraged at computer science... I urge you to not give up until you've at least taken data structures. Years back in college, when I was close to losing hope in computer science, I took data structures and struggled at first. I kept at it and I still remember the moment when it all ""clicked"" like the end of the matrix. It sounds lame, but I've heard it described that way by other professionals I know. For me...it was in our computer lab and we drew out a linked list on the board. 

Data structures are key and take you beyond just knowing how to sling some code. If it weren't for that class I might not have the life I'm appreciative for today. Stick with it and don't be afraid to ask questions. You're entering a field where you'll be learning the rest of your life. Get used to asking questions, being confused, tinkering, and trying over and over and over.",1531974933.0
dreymatic,"Dude fucking thank you for this website, I did not know it existed and I’ve been really struggling with algorithm comprehension, I read the introduction and I know this site is going to clear up so much for me. REALLY appreciate it",1531963351.0
that_dude_dane,"yeah, this is legit.. great work",1531971291.0
not-just-yeti,"A fine article, with a slight blemish that there's no mention of how to represent an empty tree.  (There is a class for non-empty nodes, which in turn contain a list-of-non-empty-nodes.  That's fine if your definition/use-case for trees doesn't include an empty-tree.)

Any sentinel will do.  People often use `null`, but that's error-prone; better to make a `class EmptyTree {}` with no fields (as a singleton-class).  And then if you want [traditional class-based] inheritance, you could have both `TreeNode` and `EmptyTree` extend `Tree`, if you have a type-checker to satisfy.

For purposes of such a tutorial, maybe a line ""this tutorial represents non-empty-trees only, for simplicity"" with a footnote about also handling empty-trees?",1532015912.0
uncleXjemima,"Are trees something that come up often in web development/JavaScript? 

Edit: answer is no, okay got it ",1531978325.0
LouMM,"mm, Bvyi vu6kh yjtj ff mn",1532097311.0
rmc0d3r,Code by Petzold,1531933092.0
combinatorylogic,"Project Oberon, NAND2Tetris",1531939530.0
skeeto,[*The Elements of Computing Systems*](https://mitpress.mit.edu/books/elements-computing-systems),1531954333.0
wildeye,"I second Petzold. NAND2Tetris also but possibly just browsing, since it takes you from NAND gates all the way to Tetris, so it's rather in depth homework.

Possibly too non-technical for you, but by one of our most brilliant computer scientists (copy-pasta from Amazon):

""The Pattern On The Stone: The Simple Ideas That Make Computers Work"", by W. Daniel Hillis, Revised edition, 4.5 out of 5 stars:

Most people are baffled by how computers work and assume that they will never understand them. What they don't realize -- and what Daniel Hillis's short book brilliantly demonstrates -- is that computers' seemingly complex operations can be broken down into a few simple parts that perform the same simple procedures over and over again.

Computer wizard Hillis offers an easy-to-follow explanation of how data is processed that makes the operations of a computer seem as straightforward as those of a bicycle.

Avoiding technobabble or discussions of advanced hardware, the lucid explanations and colorful anecdotes in The Pattern on the Stone go straight to the heart of what computers really do. Hillis proceeds from an outline of basic logic to clear descriptions of programming languages, algorithms, and memory.

He then takes readers in simple steps up to the most exciting developments in computing today -- quantum computing, parallel computing, neural networks, and self-organizing systems.

Written clearly and succinctly by one of the world's leading computer scientists, The Pattern on the Stone is an indispensable guide to understanding the workings of that most ubiquitous and important of machines: the computer.

About the Author

As an MIT graduate student, W. Daniel Hillis designed the first practical massively parallel computer, the Connection Machine, and in 1983 co-founded the world-famous Thinking Machines Corporation to produce and market this device. The co-founder of the Long Now Foundation, Applied Minds, Applied Invention, and other technology companies.
",1531940794.0
MirrorLake,"The CS textbook that’ll teach you this is *Structured Computer Organization* by Tanenbaum, specifically [chapter 3](http://www.cse.fau.edu/~jie/teaching/fall_2006_files/3.ppt) which goes over the most basic logic gates and how they combine to do things. It builds up to the design of the ALU (Arithmetic Logic Unit) with full circuitry. Focus on learning boolean algebra for a few days. In my class at school, we specifically were tasked with taking random truth tables and turning them into basic circuits (it’s easier than it sounds!)

Learn those basic gates (AND, NOT, OR, NAND, NOR, XOR, XNOR) and their truth tables, and you can build up to understanding the ALU. For me it only took a few days of class and homework for it to click—much faster than I was expecting.

So the path to understand it might be:

Learn those gates and their truth tables.

Practice some boolean algebra/circuit problems.

Check out how a mutiplexer works. Check out how a full adder works and half adder.

Finally, the Tanenbaum’s ALU diagram should start to make sense.


Edit: Buying the textbook is probably not necessary, and yeah, Petzold’s book is MUCH friendlier to read. Neither book will force you to practice boolean algebra. But using boolean algebra or playing with circuits/gates, though, is what will help it click the most. ",1531962921.0
OneOldNerd,"Computer Systems: A Programmer's Perspective.  


[https://www.amazon.com/Computer-Systems-Programmers-Perspective-3/dp/9332573905/ref=sr\_1\_1?s=books&ie=UTF8&qid=1531958631&sr=1-1&keywords=computer+systems+a+programmer%27s+perspective+4th](https://www.amazon.com/Computer-Systems-Programmers-Perspective-3/dp/9332573905/ref=sr_1_1?s=books&ie=UTF8&qid=1531958631&sr=1-1&keywords=computer+systems+a+programmer%27s+perspective+4th)",1531958662.0
token_internet_girl,"I think my Operating Systems Concepts textbook covered this exceptionally well, but from the perspective of how an operating system works. Maybe something you'd read after Petzold. ",1531960780.0
salgat,"Have you learned Assembly? This helped bridge the gap between computer hardware architecture and software for me, since it outlines pretty explicitly what the CPU is doing (at least for simpler architectures).",1531980531.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1531931563.0
Codearella,What type of ideas? Are you looking for fun project ideas?,1532231545.0
MKEEngineerDude,"As a physics major with extensive post-bachelors education in computer science: yes. However, your best bet is to speak with your community college’s advisor and not strangers on reddit.",1531930275.0
kiotoarigumi,"If you're not planning for a career in computer science why not just cherry pick the topics that interest you and research them individually? You don't need a thorough understanding of compsci to talk about block chain.

Aside from that, how limited is your understanding of computers? Do you have basic knowledge of computer hardware?",1531923348.0
GetOnMyLevelL,"I often feel that a lot of people who invest in crypto hardly understand the technology behind it. Yes they can tell you the goal of the company behind the crypto etc. But most of them wouldn't understand what is said in the Whitepapers.
Computer science is a fairly broad major and people go into all kinds of different directions. You can look up the typical courses for a cs major and pick out the most interesting one's for you. Getting through those courses will still take a lot of time.

For AI you at least need to know Linear algebra and statistics.",1531933700.0
lrem,"A thing you forgot to mention: how strong is your mathematical background? Turns out understandingevery interesting computer science area requires a different subset of math topics on at least a basic level. E.g. Blockchain itself would be simple algebra and numbers theory.

If you're interested in a very specific thing, Wikipedia usually provides references to a minimal set of reading material. But it might be hard to read without the background.",1531993873.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1531916760.0
allensaakyan,"I've been following Rhonda for a while, she's cutting-edge in protecting clients’ systems and data through industry leading security, regulatory compliance, and risk management practices. She has also been approved over 150 Patents throughout her career. Here's her IBM profile page: [https://www.ibm.com/ibm/ideasfromibm/us/ibm\_fellows/2014/rhonda\_childress.html](https://www.ibm.com/ibm/ideasfromibm/us/ibm_fellows/2014/rhonda_childress.html)",1531916779.0
w-g,That is a 1082-page PDF with a Creative Commons license!   THANK YOU for sharing your hard work!,1531939934.0
Hodorgasm,I read a book on assembly back in 2003 and forgot most of it.  I'm definitely interested to read this to refresh my understanding as there are times I'd like to use [Compiler Explorer](https://godbolt.org/) to test certain assumptions about bottlenecks and optimizations in the assembly output of C++ code.,1531958822.0
Lee_Ogre_Growl,For the love of God no one read this thing. ,1531919322.0
ReadTheArticleBitch,"**How Will Data Science Evolve Over The Next Decade?**




How do you think data science will change over the next 10 years? originally appeared on Quora: the place to gain and share knowledge, empowering people to learn from others and better understand the world.


Answer by Eric Mayefsky, Head of Data Science at Quora, on Quora:


There are many aspects to data science and being a data scientist that are constantly changing and will change over the next ten years. I'm personally going to limit this answer to the career fundamentals of data science.


I think by far the biggest advancement along these lines you'll see in the future are clearer, deeper career paths within data science. A big issue right now is that while more and more companies are committing more heavily to data-driven product development and their data teams, there are very few examples of folks deep in their careers who are pure data scientists, particularly when you exclude people in management. And that just makes it a lot harder for folks who are passionate about data science and happy with their current roles to have the confidence they should have that those roles and that career progression will be there. I'd also add that I've found data science to be a field where you can add a ton of value relatively early in your career, in part because the field is in its early days, and in part because the nature of the work is exploring the corners of what your product does and can do that no one else has looked at. But I think for some this also adds to the feeling of how much more can I grow in this field? when the impact is so big so quickly. The senior folks we've been able to bring on have added a ton of value, and I think everyone on the team can see that, so they know getting to senior-level impact as a data scientist is possible, but I think this will be easier when the distribution of career stages evens out a little bit. 


To be clear, I also think it's wonderful that data science prepares you well for a variety of ways to add value in tech, and I also think you're seeing the lines blur between data science and product management and other related roles at a lot of companies. (At Quora, for example, roughly half the product management team has data science in their background, and our Head of Product used to be the Head of Data Science, though I think we're a bit of an outlier in how data-driven our product processes are.) But I think the commitment I and other data science leaders make to career paths within data science as its own discipline are also very real, and it'll be nice for folks to have more role models and belief in that reality as time passes.


The other big item is that I think you'll see diversification in titles. Widespread adoption of “Data Scientist” as a job title I think has been helpful in bringing the field to prominence and shining a light on the value there is in hiring smart people in a first-class function to think about your product from the data perspective. But it's also pretty obnoxious from a clarity and efficiency perspective because the phrase means so many different things. You have


all being called “Data Scientist” right now. Part of the challenge of companies who do make a big investment in data science as a function is that it's hard to distinguish yourself from companies who think of data science as a 'back office' thing—and I don't really expect that to go away—but there's also a ton of horizontal diversity all with this one title, where different people just want to do different things, and so hiring processes are pretty inefficient, and in the worst case, you have folks not only interviewing at but actually joining companies who don't have the role they actually want. Whereas at this point, if you look at engineering for example, most companies can pretty clearly distinguish an infrastructure engineer from a product engineer at the very beginning of the hiring process (i.e. on their careers page).


This isn't something I expect to happen overnight—you don't want to put your team at risk by giving them strange titles that can hurt their professional growth, so there can be this tendency toward uniformity—but I think as more companies with prominent data teams grow in size, you'll see them try more different things to help make the hiring matching process more efficient, and some of them will stick and proliferate, and you'll eventually see a diversity of job titles that more closely map to the work and expertise. A slightly different take we're currently exploring is to try and find data science candidates with specific product expertise and signal the demand for that work directly up front in the hiring process.


This question originally appeared on Quora - the place to gain and share knowledge, empowering people to learn from others and better understand the world. You can follow Quora on Twitter, Facebook, and Google+. More questions:

✌ cool people read before commenting ✌",1531838926.0
ToastedWonder,I think [teach yourself cs](https://teachyourselfcs.com/) has a pretty good list. I don't know if I'd say they're must reads though.,1531848498.0
titibuga,"It is hard to list important books for ANY computer scientist. The field is too broad to say with confidence that certain books are must-read without it being about near-basic level stuff. The algorithms book of CLRS is an example, it does not contain only basic stuff, but it is considered the bread and butter of algorithmic stuff of any CS student. Actually, the first link you posted does a good job about this, because it divides the books in a heap of different categories. But I think it is pretty hard to talk about the top 3 books without being biased towards my area of preference, for example.

I am not able to point you to links, but I guess that you may find good book lists by looking for things like ""Important machine learning books"", ""Important TCS books"", ""Important AI books"", etc.

",1531843201.0
brechindave,"You could try this list. Seems to be a bit of crossover.

http://www.listmuse.com/best-books-computer-top-10.php",1531857624.0
Liz_Me,"Programming Pearls, Bently",1531891607.0
pseudocrypt,"SICP, Godel Escher Bach, AIMA

Those are three of the best CS books I've ever read. SICP alone is responsible for helping me nurture my initial interest in computerized problem solving. 

",1531848345.0
Zorbathecat,Nice,1531847126.0
yopladas,"More advanced books for a third year undergrad

For concept: introduction theory of computation by Michael sipser

For practice: computer organization and design by David Patterson",1531864890.0
takanator,"I am glad you showed me that list, on the beginning, I thought it is going to be next enormous list of books, no one ever going to read but it's pretty short and straightforward.

Looks like a great goal to read them!",1531899338.0
cp5184,"Well, the ""bible"" of CS would probably be the art of computer science series...

I wouldn't say they're must read though.",1531843794.0
giller_,Hey everyone spreading the word about good books is always helpful for people learning cs. Add my snapchat [***cse.life***](https://cse.life) and send me book recommendations in chat so I can repost to spread knowledge! ,1532011139.0
salgat,"There is none. No doubt books help and I read a couple every year, but the best resources for learning are online. Even something as simple as looking up design patterns on Martin Fowler's site for example.",1531896367.0
,[deleted],1531873035.0
fmresearchnovak,"We don't really have enough information to answer.  What is the difference between these two courses?  Based on the titles, both courses seem impossibly broad.",1531932229.0
csp256,"For learning more about the Kalman filter I suggest reading [Probabilistic Robotics](https://docs.ufpr.br/~danielsantos/ProbabilisticRobotics.pdf). If memory serves chapter 3 covers the Kalman filter, while the somewhat more useful *extended* Kalman filter is introduced in chapter 7. The rest of the book is pretty damn good too!

For a more hands on approach [Student Dave's video tutorial series with source code](https://www.youtube.com/watch?v=FkCT_LV9Syk) strikes a rare balance between casual and insightful. It might actually be where I recommend you start. ",1531841531.0
magnificentbop,"Amazing, thanks for sharing this.  You are absolutely right that many explanations are a bit opaque.",1531823375.0
bgeron,Is this a data science thing?,1531823079.0
TheApadayo,Another similar technique that I think is a bit easier to understand is a particle filter. It does almost the exact same thing as a Kalman filter but can represent multiple different mean estimations because of the number of simulated particles is higher than the single mean of a Kalman filter. Basically you simulate a lot of particles for each state and calculate the probability of each particle existing at each step and re-sample new particles for the next step based on that probability.,1531866221.0
tending,I don't understand his use of the velocity and acceleration equations from physics. Are you supposed to always use those regardless of your data? Or is that just typically what's used in robotics? Like how would you apply this to say stock prices?,1531835681.0
TheRodsterz,I definitely need more books that I have no time to read!,1531788241.0
pkrumins,My book is in there. So go get it!,1531801073.0
Science_Podcast,"Link to the article:
https://arxiv.org/abs/1806.05129",1531765181.0
veggietrooper,"Man, the lengths these round-earthers will go to to propogate their lies. Sheeple! /s",1531772897.0
fmresearchnovak,"This question is really subjective.  It depends a lot on the person and the language(s) known and the new language.

So, I will give you a very very rough estimate that has a variance of like 12 months.  First, I assume you are fluent in an arbitrary programming language with three years of experience programming in that language on a daily basis.  And, I assume you have never learned any other programming languages.  Starting with those assumptions, I would say you could learn any new language in a few weeks.  Anything from 1 week to 3 months would seem reasonable to me.

&nbsp;

You could learn to write hello world in under an hour (probably under 10 minutes).  You could write a very basic program in a day (like sort a list of random numbers).  It would then take weeks to learn the various tools, and habits of that language.  You could write something complicated / hard after maybe a month of practice.

&nbsp;

Some languages are harder to switch between (for example C -> LISP would be a hard switch, but Java -> python would probably feel pretty easy).  And some people learn faster than others.  And, it's not easy to say at what point of knowledge / skill can you say you ""learned"" the language.  For a lot of languages you'll probably never learn every feature / tool that language offers.",1531759231.0
khedoros,"Are you already experienced in some other language/languages? If you've got previous experience, picking up a new language that's somewhat similar can be a matter of a couple weeks to a month, at least for a basic level. As much as the language, you'd also want to become comfortable with whichever programming libraries are commonly used in your field; learning a large library can be harder than picking up the base language, sometimes. If you're learning programming itself at the same time, or if you don't have experience with many languages already, you'll be slowed down a bit, but if your timeline is a couple years, that should be enough to become proficient in a few languages (just don't pick, say, C++ and Haskell as your two).

A comment on C and C++: C is almost, but not quite, a proper subset of C++. C++ is almost like 3 different languages. ""C with Classes"" or ""C with IOStreams"" is one: Basically, code is written in a very C-like style, using minimal features from C++. I might call the second ""C++98"" or ""C++03"". It's ""proper"" C++, but in a style that would've been commonly used 15-20 years ago, and ignoring the changes made in 2011. It's object-oriented (sometimes excessively using those features), and even templated. ""Modern C++"" (the third style) has shifted more toward heavy use of type deduction, composition of generally-applicable algorithms instead of purpose-built code in each OOP class, lambda functions, and increased use of smart pointers for automatic memory management. Suffice to say, C++ is a big language, and can be written in ~~styles~~ dialects that are just about mutually unintelligible.",1531759845.0
computistxyz,"It's an incredibly difficult thing to measure for sure because, at the end of the day, people learn at very different rates in very different ways, obviously. However, a point worthy of note is the very fact that an experienced programmer/computer scientist is generally much more able to learn new languages/technologies quickly compared to a beginner, analogously to the whole ""school/university teaches you how to learn"" concept. A Bachelor's computer science course takes 3 years, during which a student may learn a number of languages, to the extent they will be able to pick up a new language very effectively afterwards. I'd say it's a fair prediction to make that spending time learning C/C++/Python over 3 years will give you a *much, much* easier time learning *X* new language when the time comes.",1531762731.0
east_lisp_junk,"> So I might have 1-2 years to learn a language or two that my top picks for Phd programs are using and I can tell them I know.

I doubt learning any particular language is likely to affect your prospects much at admissions time. Your application is probably going to get accepted or rejected based on much bigger factors than whether the particular language/libraries/codebase a group works with is on the (probably long) list of things you'll have to get up to speed on before you can contribute much.",1531767731.0
aerojoe23,"Because you're asking this question and didn't say other wise, I'm assuming you don't really know any programing languages. The good news is that once you know how to program switching languages doesn't seem to be a big deal. Switching from object oriented programing to functional programing would be a bigger leap than say C++ to C#. 

From what I've heard Python is used a lot in research, so mastering that should be a great option. But really if you have an organized approach to problem solving, you'll probably end up learning to programing as it becomes a tool to solve a problem. If you really feel like you need to get a head of it, try finding small problems to solve by using Python. Even if the your school work just wants you to solve them by hand. On the other hand, you maybe better off focusing on school work you're actually graded on for the time being.   
",1531767103.0
bnelo12,"Integer overflow has been around much longer than 1939. Before modern computers there were mechanical calculators which would have definitely had integer overflow problems. IBM has been developing mechanical calculating devices since the 19th century.

I believe what they are discussing here is some sort of philosophy on whether or not there is a difference in learning all integers or learning a technique to generate all integers.",1531684148.0
agumonkey,similarly ada lovelace complains about mutable state and variable assignment,1531686452.0
DumChikiDum, I couldn't understand this. Can someone post a simplified version of it? And also what is that weird looking symbol? I haven't come across that before. ,1531714357.0
ice109,more about sig figs than integer overflow - sig figs is a way to right down large numbers - or small numbers - integer overflow is not.,1531681994.0
,Think of things that might be useful for yourself and build them.,1531651751.0
kdnbfkm,Can the TA's help you?,1531657664.0
gct,"If you were a math major, you might enjoy working on some of the problems on [project euler](https://projecteuler.net/), as they're programming, but focused on mathematics.  Unfortunately in programming there's just no substitute for writing as much code as you can and making every mistake there is.",1531660753.0
Kibouo,"When I hear these stories I realize I'm lucky to have a good school.

Try to find something you like. Then make something around it. Don't think too hard. Once you start you won't be able to stop. ",1531657895.0
arcadja__168,"Hey,

Comp Sci grad here.

I’ve totally been where you’ve been, and I think the thing that really helped me was eschewing the often sun-par university materials and signing up to an online learning site with a focus on programming. Pluralsight is my favourite, but I also can’t recommend Team Treehouse enough for getting starting with a particular language.

Using these gave me the confidence and step-by-step type teaching to help me get a foothold in the subject and get stuff done.

Also when I started pestering the uni PHD advisers (if you have those), that was a massive help, for some reason most people don’t bother utilising them - but they’re mostly just bored and love to sit down and explain stuff.

My other tip would be persist in questioning, if you don’t understand I think the difficult thing is to repeatedly push for a different explanation.

EDIT: Removed some flexing.",1531652153.0
,[deleted],1531652328.0
Kel-nage,"If a value is nonsensical, what meaning would it have for it to be positive or negative? For example, 0/0 is just as nonsensical as -0/0 - neither value is positive or negative.

Technically speaking, the IEEE 754 representation of floating point numbers allows the sign bit of NaN to be 0 or 1 - but it’s value is irrelevant as all comparison operations on NaN values return false by definition.",1531580923.0
mrexodia,"The 80-bit floating point standard used by the x86 processors does have multiple representations of NaN (the sign bit can be either set or not set), however during computations there is no difference. See https://en.m.wikipedia.org/wiki/Extended_precision for more information.",1531590936.0
bart2019,"No, because NaN > 0 is false, and NaN  < 0 is false. Oh, NaN == 0 is also false.",1531587547.0
12GallonPoopBucket,I'll just leave this here: [What Every Computer Scientist Should Know About Floating-Point Arithmetic by David Goldberg](http://www.itu.dk/~sestoft/bachelor/IEEE754_article.pdf).,1531617579.0
mingp,"Think of NaN as, ""the computation broke and we can't assume anything about it"". In that light, it doesn't make sense to ask about the sign.",1531590551.0
KevinOllie,IEEE floating point defines it.  It exists,1531621718.0
green_meklar,"In terms of the bit encoding, theoretically NaN can still have either a positive sign bit or a negative sign bit. However, the behavior of NaN for floating-point arithmetic should never take this into account. This is part of the floating-point specification and non-negotiable; it would be a hardware fault for the hardware not to behave this way. (As far as I know the hardware is free to assign whatever value it wants to the sign bit of an NaN value. You should consider the results of extracting the sign bit of an NaN value to be undefined behavior.)",1531626021.0
feedayeen,You can perform bitwise operations to identify the individual bits states depending on the language. The sign bit is a don't care value by spec so chip designers can do whatever they want at that level as long as comparisons to return false. ,1531593487.0
IntolerableBalboa,"It doesn't represent any value that could be + or -.  And even if it did, you wouldn't be able to do anything with that.

> NaN in of itself cannot be (+) or (-); rather the value that it represents can be (+) or (-)

I think you must have been high when you thought that up.",1531624746.0
The_JSQuareD,"NaNs do have a sign bit, and their value can be examined, for example with [signbit](https://en.cppreference.com/w/cpp/numeric/math/signbit) in C/C++. However, I don't believe any semantics are defined for the sign bit in NaNs, so their value is meaningless in practice (an implementation could set it arbitrarily). ",1531629741.0
stochastaclysm,No,1531641479.0
_ACompulsiveLiar_,This is like asking if NaN can be red or blue. ,1531620340.0
DonaldPShimoda,"Edit: this is all wrong, so... ignore me.

5/0

-5/0

These can be represented as +NaN and -NaN respectively, and if I recall correctly some languages do it. However, they primarily do it to propagate the sign (since the signed bit is not considered part of the numeric part of the type). On a mathematical level, it's totally meaningless as far as I can tell.",1531580473.0
sailorcire,It's a tri-state value and your chip designer may have made an executive decision on weather or not to set the negative flag or not.,1531585396.0
ScientificMeth0d,"> I'm human. Are you.

Oh fug",1531547481.0
hut_hut_what_what,Things took a turn at the end ,1531532611.0
Godot17,"If your AI gets this trippy over the Bee Movie, I really wonder what it outputs if you feed it the script of Neon Genesis Evangelion. 

I'm kidding, please don't.",1531543619.0
InoriSky,"You kill you! 

Bye.",1531537217.0
church-turing,"> I have busted, boys!

( ͡° ͜ʖ ͡°)",1531548473.0
BenjiSponge,"What I love about this is how clear it makes it that all the ""AI generated scripts"" that are posted to meme pages every day are clearly fabricated.

This is the first one I've seen that seems plausible, though I'm definitely confused by some punctuation decisions. ",1531538768.0
thottius,This is absolutely incredible ,1531540642.0
Grynn23,>A bee's a trap? ,1531534151.0
TripleDigit,Get this thing a budget and send it to the screens in time for awards season! I smell Oscar gold!!,1531541966.0
Urban_II,USE THE LIFETIME. FREE THE BEES.,1531579922.0
proof-of-w0rk,"“I am. Flower. Blood pollen.”

— Barry, a little metal bee",1531589067.0
bo0mb0om,Can you elobarate what you did? Did you train on the script first and then sample from the model?,1531547289.0
orkan_127,Still better than some rap lyrics out there,1531556873.0
TheAwdacityOfSoap,Still better than Twilight.,1531546531.0
Sabmo,"be the bee, bee!

Thinking bee!

Honestly this started off so inspirational, I felt like the world was my oyster. Not sure how I feel having got to the end",1531602122.0
MongooseLupe,I’d love to see your bot and play with it! Is it something you created on your own or collaborated with others on?,1531550839.0
big_brotherx101,The eerieness of this makes me feel like I'm reading an SCP's log on a rogue AI...,1531557734.0
thatwhitegirltwerk,Open it. Smell it.,1531576194.0
lukaomg,Yeah. That’s a Kenny....,1531585978.0
moschles,"snap snap snap snap

screaming",1531613045.0
SamuraiBeanDog,Open it. Smell it.,1531625143.0
itsaworkalt,">As a consequence, we show that Kerenidis and Prakash's quantum machine learning (QML) algorithm, one of the strongest candidates for provably exponential speedups in QML, does not in fact give an exponential speedup over classical algorithms.

That's the real finding. Paper is way too dense and way too outside my field for me to pickup much though.",1531516559.0
FUZxxl,"> I need working code

then write it yourself! It's a good exercise and shouldn't take too long.",1531492520.0
rosulek,"FYI, in the worst case there can be an exponential number of shortest paths.",1531497214.0
rebz1952,"Well i can help you with that
I mean i wrote that code once in python 
But the problem here is still ‘unclear’ there aren’t ‘all’ shortest paths and if you refer to duplicate distance then a little bit of ‘conflict management in code might help’ you can save that in dynamical list that extends on its own but there is an alternative if the number difference between path is so less then you can call on a counter and approximate the difference 
",1531708354.0
zetsubou-tan,"So you’re looking for multiple paths?

The naive approach I can think of would be just be to use dijikstra to find the length of the shortest path then do BFS and check all paths of that length and return the paths that reached the goal. Storing all those different paths seems like it could get a bit messy though.

I’ll add more stuff to this if I can think of something more clever.",1531469784.0
RomanRiesen,"Do you need it to use dijkstra? There are other algorithms that might be more suited if you need all shortest paths. 

Floyd-warshall or johnsons (for (very) sparse graphs)",1531542664.0
Findlaech,"My gf is doing her internship in post-quantum cryptography, and looking at the state of the algorithms that were submitted to the NIST, she says that if quantum computers were to be mass-produced tomorrow, we would all be shitting bricks.

¯\\\_(ツ)\_/¯",1531466014.0
UncleMeat11,"> As far as I know current cryptographic algos/hash etc are safe from any threats of quantum computers currently in existence and even symmetric cyphers are easy to protect against QT by just doubling the key size.

There are no productionized quantum resistant public key crypto algorithms at the moment. There is a lot of active research here and proposals to NIST but at the moment nothing is concrete. Symmetric crypto can be defended against quantum computing as you say, but without public key crypto you use a tremendous amount of real world utility. 

That said, the progress towards productionized quantum computers is also slow and it does not seem likely that widespread use of quantum machines will exist before we have crypto solutions.",1531503484.0
quantum_dog,"I don't think so. Quantum computers can efficiently solve the hidden subgroup problem for arbitrary abelian groups and some non-abelian groups. This is not something people thought was possible for classical computers. Also, keep in mind the actual number of people who have ever worked on quantum algorithms. It is much less than the number of people who are currently working on classical algorithms. My personal opinion is that we have not yet discovered the *true* potential of quantum computing. I'm not saying that quantum computers can solve NP-Complete problems or something harder but that they can potentially solve problems we care about which might even be problems outside NP and even PH.",1531495204.0
ahelwer,"Scott Aaronson doesn't believe quantum computers will be a serious threat to RSA within the next decade, see [this question on his HN AMA](https://news.ycombinator.com/item?id=17425597):

>  Do you believe there is now a quantum Moore's Law in place? I've seen graphs showing quantum chips from Google, IBM, and Intel plotted on a log scale that are suggestive. (These exclude adiabatic quantum computers which are different beasts.)

> If there is do you think we're perhaps less than 10 years from QC capable of breaking common number theory based asymmetric cryptographic algorithms like RSA or elliptic curve for at least lower key strengths? That's what these graphs suggest.

> (I know breaking crypto is not by any stretch the only or the most valuable thing you can do with QC but it's the one that gets the most press and it's relevant to my current work.)

Scott's reply:

> I think it's too early in the field, and there's too much basic research still to be done, to talk usefully about a ""Moore's Law."" For godsakes, we're not even sure yet whether superconducting qubits or trapped ions or something else (or a hybrid) will be the way forward!

> Yes, you can make plots of the number of qubits, coherence times, etc. as a function of year -- and if you listen to talks by John Martinis, Chris Monroe, or the other leading experimentalists, you'll often see such plots. But at the very least, you need to look at both dimensions (qubits and coherence time) -- not just at ""number of qubits,"" which will be severely misleading! And even if you do, there are very few data points to use for extrapolation, since it's really only within the last ~6-7 years that people have even gotten qubits to work well in isolation, let alone scaling them up. So it's really hard to extrapolate.

> Like, I'm hopeful that within the next decade, we'll have systems with a few hundred qubits that will be good enough to do some useful tasks that are classically intractable (such as quantum simulation), though they certainly won't be threatening public-key crypto yet. But I'm not sure even about that. And I'd prefer to see what happens with this before speculating about the timescale for the next step, of building a full universal QC (the kind that would break our existing public-key cryptosystems)!

Someone else in that subthread did back-of-the-envelope calculations showing *even if* the number of qbits available in a quantum computer doubled every year from here on out, it would be over 15 years before we have enough qbits to successfully attack modern RSA key-sizes with Shor's algorithm.

There's also the commonly-voiced possibility of some super-advanced quantum computer hidden in the NSA's basement, about which [Scott is very skeptical](https://news.ycombinator.com/item?id=17425549).",1531510875.0
Foggerty,"[Not according to this?](https://www.youtube.com/watch?v=OWJCfOvochA)  (Skip to the last person being interviewed if you just want to know about the ""all the securitys will bee pwned!!!1!!"" bit.",1531471902.0
spinwizard69,Isn't it a bit early to be categorically declaring what quantum computer could or couldn't do?    At best the research is in it infancy.    Given that I wouldn't be surprised to find out that the NSA already has a Quantum computer running decrypting tools.,1531516531.0
trichotillofobia,How's that not illegal?,1531676265.0
ctchalk,"After 3 grueling days of detective work, I found the paper.

https://scholar.harvard.edu/files/ericbalkanski/files/the-adaptive-complexity-of-maximizing-a-submodular-function.pdf

While of course the article sounds very sensationalist at first glance, I guess the paper does have good results. Tl;dr, they give a tight upper and lower bound of squiggly-Theta(log n) parallel time for a particular kind of optimization problem; the best previously known algorithm required at least n parallel time, so the ""exponentially faster"" headline makes sense.

But these people posting links to papers and articles and not participating in the discussion or giving a synopsis is pretty annoying.",1531455835.0
GreyMX,How different is this from a Monte Carlo algorithm? Seems like a stretch to say that it's a fundamentally new kind of algorithm.,1531452505.0
svick,That is one horrendous headline.,1531503762.0
rlopu,"It sounds like they're just randomly selecting a subset from the input and going from there... So they don't actually touch all of the data... Especially for the movie suggestion one, the result might be acceptable but is it completely fair?",1531455768.0
shaggorama,"> so-called optimization problems

jfc this is unreadable garbage.",1531498806.0
kdnbfkm,"It would probably be over my head, but does the new technique have a name to lookup? The non-technical descrption reminds you of Alan Kay talking about swarm intelligence doing word-wrap though.",1531452851.0
manias,Is this just beam search? Or is there more to it?,1531487445.0
TheBlackCat13,How does this avoiding getting trapped in local maxima?,1531505473.0
jprider63,Is this the same as [this recently discovered algorithm](https://www.scottaaronson.com/blog/?p=3880)? It's interesting how different authors come up with similar results at the same time. ,1531499059.0
,[deleted],1531453935.0
lazylearner,"I for one, shall welcome our new robot overlords. ",1531475116.0
giller_,"Hey everyone! After searching for a cool computer science snapchat and not finding one, I created a snapchat [***cse.life***](https://cse.life). I encourage people to send me anything computer science related so I can repost it. The purpose of this snapchat is to share what it's like to live and love the cs life. Happy hacking!",1532010597.0
613style,"When you study the theory of computer science -- complexity, automated theorem proving, graphics, machine learning, etc -- you're studying math.  But often the day to day work of programmers isn't very math-based.  

So the question, ""Is it math?"" depends on how you engage with it.  But the question, ""Is it anything other than math?"" has an easy answer: no. ",1531433941.0
aelsilmaredh,"According to Wittgenstein, Mathematics is a branch of Logic (ie, Logic CONTAINS Mathematics within its framework).  All of Mathematics has been rigorously defined and systematized through Symbolic Logic

So, my opinion is that Computer Science is another branch of Logic, rather than a branch of Mathematics.  Keep in mind that at the processor level, all ""mathematical"" operations are performed using circuit implementations of basic logic operations (AND, OR, NOT, etc.).  A huge part of programming and designing systems is designing the logic behind the program.  Does it do what it was designed to do?  Does it do this as efficiently as possible?  Is the code structured logically enough that a different programmer can read and understand it?  ",1531456193.0
Fluffy_ribbit,"Computer science is a branch of mathematics. But that mathematics has practical applications, i.e. engineering. And that engineering has aesthetic aspects, i.e. design. ",1531432373.0
estebanagc,"This question consists of two different questions since Computer Since and Programming are not the same thing. Computer Science does include programming, but most of the programming used in the industry is not directly related to Computer Science but to Software Engineering, which is an Engineering and as such just applies *some* scientific methods (ie calling a quicksort with a built in library or choosing which data structure you will suit better for your purpose).

While software developers rarely use math other than aritmethic or boolean logic as far I know there are other fields of programming in the industry that actually use more advanced math, like a Machine Learning Engineer or Data Scientist.

As for Computer Science being Math, I think that the relationship between CS and Math is like Physics and Meteorology, it started just a branch of the academical field but it became so specialized that many Universities and Research Institutions have a different department for each one.",1531456881.0
SOberhoff,Some people (like me) understand Computer Science to be Complexity Theory and the like which makes it just a subfield of math. Though there are plenty of others who's first thoughts are databases and websites etc. It's hard to say who's right and who's wrong here. It's a matter of perspective.,1531432812.0
,nice spam,1531433745.0
east_lisp_junk,"A good place to start is [Propositions as Types](http://homepages.inf.ed.ac.uk/wadler/papers/propositions-as-types/propositions-as-types.pdf). This lays out the basic ideas for using a typed language as a logic. You can get a more thorough look at it in volume 1 of [Software Foundations](https://softwarefoundations.cis.upenn.edu), but I don't remember how well SF connects to the sort of automated reasoning techniques used with SMT solvers. Part of the cause of this balkanization is building tools and algorithms based on different logics. You'd probably gain some broader understanding of that design space by working with a few different logics for a while -- see what's easy/hard/impossible to express, what the search space for proofs looks like, etc. Actual decision procedures for particular theories tend not to work like syntactic-level proof search though. Looking at axioms and trying to come up with rewrites they justify isn't workable as a general strategy, so constraint-solving algorithms typically reason about the structures that logic terms talk about rather than the terms themselves.",1531411039.0
ahelwer,"Presenter here! Thanks for posting this. Here's the talk blurb:

This talk discards hand-wavy pop-science metaphors and answers a simple question: from a computer science perspective, how can a quantum computer outperform a classical computer? Attendees will learn the following:

* Representing computation with basic linear algebra (matrices and vectors)

* The computational workings of qbits, superposition, and quantum logic gates

* Solving the Deutsch oracle problem: the simplest problem where a quantum computer outperforms classical methods

* Bonus topics: quantum entanglement and teleportation

The talk concludes with a live demonstration of quantum entanglement on a real-world quantum computer, and a demo of the Deutsch oracle problem implemented in Q# with the Microsoft Quantum Development Kit. This talk assumes no prerequisite knowledge, although comfort with basic linear algebra (matrices, vectors, matrix multiplication) will ease understanding.

Slides are here: https://speakerdeck.com/ahelwer/quantum-computing-for-comput...

I'd love to answer any questions! I made this talk to cover all the stuff which gave me the most trouble as I learned quantum computing. I believe this material is easily within the grasp of all software engineers. If you're looking for a textbook instead of a video, I strongly recommend the book (also called) Quantum Computing for Computer Scientists by Noson S. Yanofsky.

There are also two technical errors I should mention so the spirit of Scott Aaronson ceases haunting me:

1) Early in the presentation, I said the gate quantum computation model and quantum annealing might be equivalent. This is incorrect on several levels; you can read more here: https://cstheory.stackexchange.com/questions/17703/quantum-annealing-vs-adiabatic-quantum-computation

2) I claimed that all quantum operators are their own inverses; while this is true of all operators in the presentation, it is not true in general.",1531413880.0
dhjdhj,Thanks for this. Now to brush up on decades of not having to use linear algebra. ,1531389171.0
Bromskloss,"The captions seems to be autogenerated, even though the usual warning for that sort of thing isn't there.",1531428371.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1531387014.0
sarahmeowz,Next: how artificial intelligence can cause disasters.,1531349254.0
geishabird,GEOSTORRRRMMMMM!!!!!!,1531330898.0
SnowceanJay,"This is not really new. For instance, this was used 15 years ago in some regions of France and has worked pretty well.

https://www.researchgate.net/profile/Jean-Pierre_George/publication/228831842_Real-time_simulation_for_flood_forecast_an_adaptive_multi-agent_system_staff/links/568e6aaa08ae78cc0515e4a4.pdf",1531380419.0
absolutedogy,I'm interested in the results but to jaded to install something like this even though you asked nicely unlike big companies (cough Facebook) I hope you get the data you need though!,1531323018.0
heyandy889,"come visit us over is r/cscareerquestions.

I have never heard of a BA in Computer Science...

What are your career goals? Do you want to be a prof, a software dev, an artist, work on a video game or Pixar movie, ? The most useful advice will take into account what you are hoping to do.",1531320158.0
Muppetmeister,"What exactly is creative coding?
",1531322868.0
nerga,"That bfa is basically going to be for if you want to go itto graphics designer. If you want to be a software engineer, stick with the computer science. This is for people to do mock ups, maybe design logos, do some ui design work, etc. You might be able to get a developer job if you do more coding on the side.

I think it could be a good minor to the computer science degree.",1531326767.0
hamtaroismyhomie,"Some areas you could consider going into is computer graphics, visualization, and augmented reality.

Personally, I don't think getting two degrees is worth it in terms of time or money.  You can just learn the skills in another degree on your own time.  If you are a sufficient self-teacher, then the only value from a degree is that it makes you more likely to be interviewed and/or hired.  The time spent getting that extra degree could be instead spent on much more relevant self-learning directed towards your interests and hireable skills.",1531412152.0
fmresearchnovak,"Getting two degrees is better than getting one.  

The combination is EXCELLENT in my opinion.  Besides the obvious game design, you could also work as a software engineer, graphic designer, 3D modeling and animation / simulation professional, or even end up in some VFX studio.

Seriously, that's a good combination in my opinion.  Why NOT get the BFA as well?",1531327985.0
Sack_of_Fuzzy_Dice,"Don't do too much stuff, too fast, and especially not all at the same time.",1531294415.0
AU_yzy0050,Do the basic stuffs slowly and solidly. ,1531328435.0
rosulek,"> why is it impossible for a computer to make that determination in another way?

It sounds like you have encountered the following theorem:

> L is recognizable (semidecidable) iff L is enumerable

And it sounds like you are thinking about the direction of the proof which goes ""if you have an enumerator for L then you can design a TM that recognizes L"". That's all fine, but it sounds like you are interpreting this step as ""the TMs you get from this theorem are the *only TMs that exist*,"" and that's just not true. Of course there are other TMs that do things better/differently/more efficiently than the special kind of TMs you get from this proof.

An analogy: C programs and assembly programs have the same power (in terms of problems they can decide). You prove one direction by writing a compiler that compiles C into assembly. But not every valid assembly program is a possible output of the compiler. For instance, the compiler always generates assembly programs that use a certain convention of manipulating the call stack, etc etc.",1531274597.0
flebron,"There's two sides of the iff:

1) If L is recursively enumerable, then it's semidecidable:

If you give me a string s, I'll forever run a loop that checks, on iteration i, if the enumerator for L outputs your string after step i of running it. If it does, I return true.

This clearly terminates if s is in L, since the enumerator will eventually (at some iteration) output s, at which point I return true. That's all that's needed for semidecidability.

2) If it's semidecidable, then it's enumerable.

Suppose I have a decision procedure P that tells me if a given string is in L or not. If it is, it terminates. If not, it does not terminate.

My algorithm will keep a list X of tuples <s, M> of strings and verifier machines it's running. My algorithm will loop forever, at each step doing:

* For each <s, M> in X where M is not finished, advance the simulation of M one step. If M finishes, output s.
* Grab the largest string among all the first components of X (it's sorted ascending, but that's just a detail). Increment it. That is, give me the next largest lexicographical string. Call this new string s'. Push <s', M_{P, s'}> into X, where M\_\{P, s'\} is the machine that runs your verifier P on input s'.

Now if s is any string in L, at some point it will be added into X as the first component of some tuple, and at some point the simulation of M\_\{P, s\} has to finish (because you told me P was a verifier for L, and s is in L). Thus at some point the simulation of the second component of the tuple will end, and the first component, s, will be output. Thus my algorithm enumerates L. It does not terminate.",1531301493.0
GNULinuxProgrammer,"This is a very good example of how people misunderstand what ""algorithm"" means for computer scientists. Say, when a computer scientist claims they found a polynomial algorithm to a problem, what they're actually claiming is that that problem can be efficiently solved. What they don't claim -- usually -- is that that algorithm has a certain special status with respect to that problem (again, it's entirely ok to prove that in a certain setting but  Similarly, if you give an algorithm to prove that something is decidable/semidecidable the actual claim of the proof is that the *problem* has a particular invariant, not that the *algorithm* has this invariant. This is important, because I see a lot of people misinterpreting this in places like reddit. In particular, it the algorithm can be ""slow"" or ""inefficient"" (whatever they informally mean) in ""real hardware"" or you can find ""better"" algorithms doing the ""same thing"".

I think it's very useful to think algorithms in various ways. One way is to think of them purely as constructive proofs. In this case a constructive proof of X being semidecidable would claim just that, without any claim about the proof itself. And then depending on the model of computation you're dealing with there will be other sorts of ways to analyze the algorithm such as asymptotically etc... But it's important to understand the difference between existence and analysis.",1531278793.0
rosulek,"> I don’t see N in the list.

If S is infinite then the enumeration never ""stops"". How do you know when to stop and finally give up (conclude that N is not in the list)?",1531274353.0
robotix_dev,"I just picked up [Designing Data Intensive Applications by Martin Kleppmann](https://www.amazon.com/Designing-Data-Intensive-Applications-Reliable-Maintainable/dp/1449373321/ref=mp_s_a_1_1?ie=UTF8&qid=1530426592&sr=8-1&pi=AC_SX236_SY340_QL65&keywords=designing+data+intensive+applications+by+martin+kleppmann&dpPl=1&dpID=51hOYx8BxXL&ref=plSrch).

I can’t vouch for it yet because I just got it a few days ago, but it was recommended here in another post and it has great reviews.",1531272803.0
SOberhoff,Perhaps [Patterns of Enterprise Application Architecture](https://www.amazon.com/Patterns-Enterprise-Application-Architecture-Martin/dp/0321127420)?,1531320873.0
Kiloku,"> use color to communicate information

make sure the colors are colorblind friendly. ",1531307999.0
Geckoboard,"Sourced from: [https://www.geckoboard.com/learn/data-literacy/data-visualization-tips/](https://www.geckoboard.com/learn/data-literacy/data-visualization-tips/)

* **Free wall poster and available through website**
* Written by Aspasia Daskalopoulou
* Illustrated by Eve Lloyd Knight",1531257107.0
maq0r,Both. If you're a SWE and want to break into infosec go work developing security software. You'll learn a LOT and also make more money than other SWEs. ,1531266628.0
ComradeBeaver,"What do you mean on **cyber security engineer?** As for me, I have been picking software engineer. My thoughts are software engineer is more wider route then others. Also, you can write security software, hacking, develop new algorithms as a software engineer.",1531259628.0
xShadowProclamationx,"software engineering as well as web dev is flooded. there is a huge demand for cyber security analysts specifically ones that actually know what they are doing. 

but it all comes down to what you want / like / passionate about. 

ups and downs with both. 

",1531259553.0
AsymptoticPerplexity,"I see what you’re getting at, but a problem with measuring intelligence like this is that for example schizophrenics become the most intelligent group of people simply because their behavior is difficult to simulate.",1531251436.0
evil_burrito,"I will describe how we use these tools first and then why.

We use tools from Atlassian: Bamboo covers both of these areas: CI/CD. Bamboo is equivalent to Jenkins in this context. Every time a change is made in the source control system, Bamboo launches a build that includes all the unit tests. This build includes the concept of success or failure, ie, the build ""passed"" or ""failed"". The build is also exactly how you would produce a build candidate for release, same scripts, same everything.

If the build fails, the tool allows you to view why it failed, i.e., what changed, who checked it in, etc.

If the build succeeds, theoretically, you have a package that has passed all its initial tests and is built in your standard way. Each product from the build tool is potentially a release candidate, if you wanted to use it as such. It would then go on to normal release testing, which is more extensive than regular automated testing.

So, with the build running every day/every checkin, however you want to set it up, every time it completes successfully, you have a potential release candidate. You know it gets built the same way every time and is tested the same way every time. This decreases the potential for process mistakes in preparing final packages.

Suppose you have a release you want to get out. It wants one more feature, which is underway by developer A and 2 bugs that are being fixed by developers B and C. Nominally, we just need to wait until these changes are reviewed and committed. The next build that contains these changes will be our release candidate.

Developer B finishes his changes, gets them reviewed, and commits them. Developer A finishes her feature, also gets it reviewed and commits them at more or less the same time.

Their changes get wrapped into a new build by the CI system, which starts when it detects changes to the repository. This build succeeds, which means build #12345 could be a release candidate, except you're missing changes by developer C.

The next day, developer C finishes his changes, review, commit. Build #12346 now will contain all the changes you want. Alas, the build fails because some of the automated tests fail. Since the only difference between build #12346 and #12345 are the changes by C, it's probably these changes that caused the problem. We all take turns walking by C's desk and tsking/tutting depending on your cultural background.

C gets his shit together and commits another change. Build #12347 contains A's changes, B's changes, and C's changes plus C's fix. This  build succeeds, which means you can take those packages and promote them to release candidate #1.

RC1 goes to QA for more testing, specifically oriented towards approving a package for release. This level of testing is typically not done on packages that aren't promoted to RC. That is, if you're not about to release something, don't bother doing the final testing.

QA might find some more issues. Rinse and repeat the above process.

The key takeaway is that the only bits that are ever released come from the CI system, no exceptions. This guarantees that the build process is reliable.",1531248516.0
Neebur,Wrong sub. Meant to post this to cscareers. Sorry. ,1531239222.0
mercurycc,But they are going to learn from the humans. How is that for Earth?,1531255166.0
pfannkuchen_gesicht,Those recommender engines almost never seem to work properly. ,1531309736.0
lrem,"In this purely abstract question, the events in which data is lost are:

- disk 1 and 2 down,
- disk 3 and 4 down,
- all disks down.

I'll leave crunching the ""numbers"" as an exercise to the reader.

The question becomes interesting, to a computer science lover, if you take into account data recovery and then failure correlation due to hardware batching, workload correlation and increased workload due to recovery itself.",1531143772.0
paul_miner,"RAID 10 is a stripe (RAID 0) over two sets of mirrors  (RAID 1). The stripe fails if any member of the stripe fails, and a mirror fails if both members of the mirror fail. So the probability of a mirror failing is m = p^2 so then you'd need to calculate the probability of either or both members of a stripe failing, where one member may fail with probability *m*. There are three ways this can happen, which you could compute and sum, or you could calculate the probability of it not failing and take its complement. The probability of both stripe members not failing would be (1 - m)^2

I think that's enough information you can work it out from there. Draw possibility trees to help visualize what could happen.

EDIT: I should add, that in this context, I mean *fail* as in *suffer data loss*. Really, any disk failure will put an array in a failure state in which at the very least performance will be degraded as the array is rebuilt. ",1531154884.0
xTouny,"**Take Charge.** Do not wait until someone hands you a problem or explain you a solution, Follow your passion, Follow your thirst of devising a problem and working it out by yourself.

**Problem Solver.** Challenge yourself and push your limits to the max, Even after solving the problem, try to think of a more efficient solution, Try to soar the difficulty of the problem.

**Understanding.** Learning computer science fundamentals like algorithms and data structure enhances the quality of your code, so it is a good practice to read about them in addition to other concepts like object oriented programming. i.e Do not write code while you do not understand it, this is a common mistake and this is what distinguishes high-tier software engineers.

**Useful Books/Websites:** [Jumping into C++ by Allain](http://www.mediafire.com/file/707se6bcl16z3uv/Jumping%20into%20C++%20by%20Allain.pdf)  \-  [The Self Taught Programmer](http://www.mediafire.com/file/wu5233435wd4om2/The%20Self-Taught%20Programmer%2C%20The%20Definitive%20Guide%20to%20Programming%20Professionally%20by%20Cory%20Althoff.pdf)  \-  [Beginning C++ Through Game Programming ](http://www.mediafire.com/file/2oqk44jbs89y0cj/Beginning%20C++%20Through%20Game%20Programming%20by%20Michael%20Dawson.pdf)(Have Fun)  -  [Code Forces](https://codeforces.com)  \-  [Codingame](https://www.codingame.com/)

**Advice:** Take the chance of exploring other domains of Computer Science since it is not confined to solely coding. [Computer Science: An Overview](https://drive.google.com/open?id=1Da-zkq8ln9AlJ9-Ie3velWYPMkfRUCDt) is a good reference for that purpose.",1531111926.0
fmresearchnovak,"It's hard to make money when you have a small amount of experience.  But, if you're self-motivated you can always do some freelance web development.  It's honestly not hard at all compared to AP CS.

Also, check out this: https://projecteuler.net/  It doesn't pay, but it's fun! :D",1531328220.0
personalmountains,"The title has been editorialized, it's ""6 Reasons Why New Programmers Suck at Programming"". And since I hate clickbaiting, here are apparently the six reasons:

- Wanting to know everything at once: Master one first, before moving to the next.
- Just reading and watching: Reading and writing code are two different things.
- Relying only on tutorials: Instead of trying to follow tutorials, pick out a project you love and imitate it.
- Not interacting: One should really make use of social media.
- Not practicing enough: Try to write at least a few lines of code from time to time.
- Not testing yourself: . Get quizzes, look at a few questions and try to answer them.
",1531074736.0
Dr_Proteus,"Wrong sub for this. But it does remind me of a line from a math textbook I used, ""Mathematics is not a spectator sport."" ",1531074979.0
vim2meta,no u,1531092169.0
nude-fox,Find the lab on campus that does computer vision stuff and ask them what to do. ,1531073257.0
jatatcdc,"If you're first year, you're most likely just going to be working on the same foundations as everyone else in your major, so for now, you'll probably just want to focus on getting that stuff as solid as you can, so you know what you're doing down the line.

If you're already beyond that thanks to APs or something similar, you'll probably want to take an AI/ML course, and a linear algebra course when those open up to you. Computer Vision is a bit specialized, so you're university may not offer classes or paths directly applicable to it for undergrads, if that's the case you might want to look at grad programs or schools that could offer you that.",1531075016.0
wolfpack_charlie,"Focus on the basics of programming and computer science for now. Machine Learning will build on top of that. 

If you can't wait to get started, however, you should pick up python. It's a relatively easy language, and there are a ton of ML/data science oriented libraries. 
",1531074908.0
MarsLanded,To experiment with a good library you can try OpenCV. https://opencv.org,1531080784.0
Grenian,Try to really understand linear algebra and further math courses throughout the first years. You will need it a LOT in this field.,1531087431.0
sashik18,"A lot of the research in computer vision is being done through electrical engineering departments. Try to learn about digital signal processing because that is the basics. This course Im taking right now  is also pretty good at providing some good background on motion estimation/identification. 

[https://www.coursera.org/learn/digital/home/welcome](https://www.coursera.org/learn/digital/home/welcome)",1531077495.0
SlightlyCyborg,"Get a webcam, study OpenCV, study convNets and DeepLearning. Go in that order.

[Here](https://www.youtube.com/watch?v=x31c9FYypOs) is a pushup counter I made and [here](https://www.youtube.com/watch?v=6Bo2ZN8dh9o) is how it works. You can try to implement it in your favorite language (with OpenCV bindings). My source is [here](https://github.com/SlightlyCyborg/daemon/blob/master/src/daemon/visual_cortex/pushup_counter.clj)",1531137194.0
mr_meeesix,I've started off with Computer Vision: Algorithms and Applications by Richard Szeliski,1531074930.0
spyhi,"CS senior here, took two machine learning classes. I would say load up on applied math, especially linear algebra, and probably calculus with constraints (Lagrangians, etc). Other useful classes would be just plain statistics (which most ML basically boils down to) and maybe physics, which I noticed helped people reason about the vector math. All of these come together in ML, so you’ll need them all eventually.

Someone else mentioned signals processing, which is also a good option for specifically computer vision, since that’s the kind of data you’d be manipulating. I haven’t taken it personally so I can’t say how accurate it is that it’d be easy to pick up.

TL;DR more and more types of math",1531108486.0
Law-tons,"Also what minors would you recommend for a cs major
",1531071816.0
Floatinatme,Find a mentor working in this field at a PhD level. They have seen students come and go and know what it takes to make it.,1531106530.0
Sathwik_Matsa,"You are a CS undergrad, So most foundational topics will be taught to you! Don't neglect, you should be really good at them to understand topics that are built on top of it. Later comes your specialization interests. Why I'm saying this is , back when I was in sophomore years I used to neglect academics because I thought it was too rookie or can easily be catched up and spent time on my personal interests (of course in comp sci) but in later years I really found it hard to cope with the semester topics. Don't be like that. Spend time on increasing in-depth knowledge and proper understanding of the topics taught at college before you spare time for personal interests.

If you are confident with your academics, you can look at Sentdex tutorials on [OpenCV](https://www.youtube.com/playlist?list=PLQVvvaa0QuDdttJXlLtAJxJetJcqmqlQq) to get you started.",1531126186.0
itah,"     in computer vision
Read a book about linear algebra. Its the basis for all the things.",1531131494.0
Screye,"In the long run, here are things that I think CV people should be decent at. Think of it as a graduation goal.

Get good at C++ and Python. You need to especially be comfortable with writing code for gpus, vectorized code. 

Linear algebra. Linear algebra. Did I say linear algebra. As a started do Gilbert strang's course on MIT OCW. It is challenging , but you will thank be later. 100%

Do not ignore classical Vision. It may seem as though everyone is hopping on the ML train, but classical methods work and are still extensively used in the industry. Also, hand engineered features seem to be making a comeback of sorts as a part of ML pipelines.

Of course you will eventually hop onto the ML/Neural networks train yourself, but that's for later.

TL;DR : Make sure your standard coding fundamentals are solid and do linear algebra.

Immediate recommendation : Do Strang",1531140609.0
EmperorOfCanada,"Ignore the butthurt academics who think this is too sophisticated for a 1st year. 

OpenCV is an easy place to catch up in an hour or with what got you a PhD 8 years ago.
",1531102628.0
cp5184,"Find a way to jump to masters degree type classes?  Maybe try to plow through your core classes early on?  Try to find compressed/3 year degree programs or whatever?

https://www.google.com/search?source=hp&q=computer+vision+degree",1531074537.0
ruizdurazo," Perhaps look into going through the material and lectures for Stanford's CS229n. Made by rockstars in the field: Andrej Karpathy (Stanford, OpenAI, Tesla) and Fei-Fei Li.

[http://cs231n.stanford.edu/](http://cs231n.stanford.edu/)

[https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)

Not sure it qualifies as first year material though. The site states the prerequisites: math, stats, ml, coding.",1531105669.0
QSCFE,"[ Getting Started in Computer Vision Research](https://sites.google.com/site/mostafasibrahim/research/articles/how-to-start) (This links is awesome)   
Contents

    1 Top Conferences and Journals
    2 Top Authors
    3 Top Groups
    4 Blogs
    5 Industry Labs & Startups
    6 How to Start in Research
        6.1 Using a textbook/course
        6.2 From Papers
        6.3 From Code
    7 Machine Learning
    8 Some  papers
    9 Acquiring Experience
    10 Material
        10.1 Online Videos & Talks
        10.2 Courses
        10.3 Computer Vision
        10.4 Computer Vision & Image Processing Coding
        10.5 Human Vision
        10.6 Others
    11 Exciting Applications
    12 Exciting Algorithm
    13 Others
        13.1 Jobs
        13.2 Datasets
        13.3 Software
        13.4 Deadlines
        13.5 Helpful sites
        13.6 Ad-hocks
        13.7 Links  

Courses

[Intro to Computer Vision](http://vision.stanford.edu/teaching/cs223b/syllabus.html) (Stanford; Prof Fei-Fei Li) Fairly standard CV course.

[Computer Vision](http://luthuli.cs.uiuc.edu/%7Edaf/courses/CS5432009/index.html) (UIUC; Prof Forsyth) Fairly standard CV course.

 [Learning-based Methods in Vision](http://www.cs.cmu.edu/%7Eefros/courses/LBMV09/) (CMU; Prof Alexei Efros) I learned a lot about texture (texton) recognition and some state of the art methods using fancy ML techniques.

 [Grounding Object Recognition and Scene Understanding](http://people.csail.mit.edu/torralba/courses/6.870_2011f/6.870.grounding.html) (CMU; Prof Antonio Torralba) This is an ongoing class focusing on higher level vision. The first lecture looks promising, but I’m not exactly sure what the rest of the class will be like.  

Machine Vision MIT [Course](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-801-machine-vision-fall-2004/index.htm)

Advances in Computer Vision MIT [Course](http://6.869.csail.mit.edu/fa12/)  

#even more cousre from *awesome-computer-vision*
#### Computer Vision
 * [EENG 512 / CSCI 512 - Computer Vision](http://inside.mines.edu/~whoff/courses/EENG512/) - William Hoff (Colorado School of Mines)
 * [Visual Object and Activity Recognition](https://sites.google.com/site/ucbcs29443/) - Alexei A. Efros and Trevor Darrell (UC Berkeley)
 * [Computer Vision](http://courses.cs.washington.edu/courses/cse455/12wi/) - Steve Seitz (University of Washington)
 * Visual Recognition [Spring 2016](http://vision.cs.utexas.edu/381V-spring2016/), [Fall 2016](http://vision.cs.utexas.edu/381V-fall2016/) - Kristen Grauman (UT Austin)
 * [Language and Vision](http://www.tamaraberg.com/teaching/Spring_15/) - Tamara Berg (UNC Chapel Hill)
 * [Convolutional Neural Networks for Visual Recognition](http://vision.stanford.edu/teaching/cs231n/) - Fei-Fei Li and Andrej Karpathy (Stanford University)
 * [Computer Vision](http://cs.nyu.edu/~fergus/teaching/vision/index.html) - Rob Fergus (NYU)
 * [Computer Vision](https://courses.engr.illinois.edu/cs543/sp2015/) - Derek Hoiem (UIUC)
 * [Computer Vision: Foundations and Applications](http://vision.stanford.edu/teaching/cs131_fall1415/index.html) - Kalanit Grill-Spector and Fei-Fei Li (Stanford University)
 * [High-Level Vision: Behaviors, Neurons and Computational Models](http://vision.stanford.edu/teaching/cs431_spring1314/) - Fei-Fei Li (Stanford University)
 * [Advances in Computer Vision](http://6.869.csail.mit.edu/fa15/) - Antonio Torralba and Bill Freeman (MIT)
 * [Computer Vision](http://www.vision.rwth-aachen.de/course/11/) - Bastian Leibe (RWTH Aachen University)
 * [Computer Vision 2](http://www.vision.rwth-aachen.de/course/9/) - Bastian Leibe (RWTH Aachen University)
 * [Computer Vision](http://klewel.com/conferences/epfl-computer-vision/) Pascal Fua (EPFL):
 * [Computer Vision 1](http://cvlab-dresden.de/courses/computer-vision-1/) Carsten Rother (TU Dresden):
 * [Computer Vision 2](http://cvlab-dresden.de/courses/CV2/) Carsten Rother (TU Dresden):
 * [Multiple View Geometry](https://youtu.be/RDkwklFGMfo?list=PLTBdjV_4f-EJn6udZ34tht9EVIW7lbeo4) Daniel Cremers (TU Munich):  

...  
 
https://github.com/jbhuang0604/awesome-computer-vision/blob/master/README.md  [This is most read]  
[Stanford’s cs131 introductory course](http://vision.stanford.edu/teaching/cs131_fall1617/index.html)  
- What after an introductory course?  
+ [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/)
",1531135704.0
Zophike1,"> Could anybody recommend a learning path and/or highly recommended introductory(!) materials for a first year CS student with an interest in computer vision?

For the math you will need it will be Linear Algebra(with proofs) , Multivariate Calculus(with proofs) , and finally Differential/Riemann Geometry(with proofs) ",1531154319.0
RedDeathofOsiris,"Computer Vision sucks don't do it. 

You're also a first year thus you probably have no idea what you may like. I didn't even know I was going to be a CS major my first year. Best to try a bunch of different things.",1531105742.0
TKAAZ,Version 1 is overly poorly written IMO. I think there's a middleground...,1531050010.0
coding_all_night,"It does a lot of processing that it doesn’t need to do when the IP is known

edit - just realised it is showing a different question / piece of code on each page view so this answer relates to the “long version” . Also after doing thousands of code reviews at work over a heap of years I can tell you now that the one line version is easier to understand - you can pretty much just cut the survey short now and quote me in the paper. ",1531043858.0
woojoo666,"I notice that the survey changes every time you refresh, but version 1 and version 2 never swap (as in, the longer version is always first). I think this is going to skew results a ton, because after understanding the 1st version, obviously the 2nd was a breeze to understand. So the 2nd version naturally ""felt"" more readable, even if it might not have been

Edit: a word",1531053990.0
nziring,"I didn't really like either of the examples.   #2 was efficient, and would be clear to somebody familiar with the problem space and the codebase, but might be tricky for somebody new to the area.   #1 was inefficient and verbose.",1531060102.0
renrutal,"I'm not a fan of long code, but I prefer the longer version. It's very self-explanatory, and you can extract the rules pretty easily.

Also the longer version would make it more likely for the maintainer to ask questions about the meaning of some rules, like ""what an unknown ip mean?"". Is is just an ip that has never appeared before, or there is a longer history to look up, and it only takes the last month in question, or a certain frequency?

The short version just makes you look at it like it is, with not hidden meanings.",1531065891.0
dhjdhj,What I think this function does is to demonstrate the lack of ability of whoever wrote this to write quality code. This function also demonstrates the disadvantages of not requiring strong types. 🤪,1531060699.0
Nikrom89,"Your main domain ([cerebralab.com](https://cerebralab.com)) is marked as unsafe to google on my browser and actually gives a 502 BAD GATEWAY. Don't know if it's just me, just reporting in!",1531048560.0
johnminadeo,"Long time dev but not a python guy, good luck with your endeavor!",1531050935.0
acroback,Eye bleach.,1531063630.0
woojoo666,did you ever publish the results of your study?,1532591845.0
zorokenshin,I don’t know anything about coding and I still took the test. Lol good luck.,1531042734.0
RedDeathofOsiris,I'm just looking at this now. Did it change? Previous comments don't seem to make sense. I thought I had this figured out but from my understanding the last test case should return true... I didn't even look at the test cases when I wrote up what I thought the function did. In it's current form though I don't think this will work since 'mean' isn't a built in function anyway - you have to add 'from statistics import \*' to the top of the file for this code to even compile. #Confused,1531108250.0
Shvingy,"Knowing only the basics of CPP/java/asm and not any abstract library things I answered.  It seemed very straightforward with the language of the function arguments, so I hope my answer wasn't too off-base.",1531107496.0
yackob03,Keybase does some of this. ,1530998612.0
CreativeCoconut,Nextcloud is planning to implement something like this but only for file-sharing. At the moment it is not usable yet and has a major flaw but till the end of the year that should be fixed. Maybe keep an eye out for a blog post or something when they announce that it is production ready.,1530998096.0
Zophike1,Does Slack have any plugins that achieve what OP is describing ?,1531154471.0
hnerixh,"For documents, perhaps syncthing, depending on how high the threat level is. Otherwise, a really simple and safe solution is an encrypted NAS (I suggest FreeNAS), then just run sshfs on all clients. Either way, backups, *backups*, **backups**.

Communication is, in my opinion, a separate problem, where for example Signal could work.",1531011335.0
Neker,IBM Notes & Domino,1531023027.0
AdorableButterscotch,What exactly is this? I’m a compsci noob. ,1531005465.0
Shaadowmaaster,"Very cool. The default update time is a bit high BTW, my phone is fine at 1 unless you seriously up the amount of boxes. ",1530969025.0
sfultong,"Nice, I made something similar when I was playing around to learn elm. It wasn't as pretty though.",1530982637.0
HimDaemon,"Nice simulation :)

Do you happen to work with cellular automata?",1530995672.0
GayMakeAndModel,"I love CA!  I used this to create a wave CA:

http://www.mtnmath.com/whatrh/node66.html

Just choose how new “drops” are created to start  a new,  interfering wave.  On a somewhat related note, you should check out the Cellular Automata Interpretation of Quantum Mechanics.  I know this sounds crazy, but the interpretation is from Gerard ‘t Hooft.  Hooft is a very credible source for physics research.

https://arxiv.org/abs/1405.1548

Edit: corrected a link",1531011059.0
pololoid,"I haven't done a detailed analysis of the algorithm but a few suggestions and comments.

You've used an example to explain your algorithm. Can you partially formalize this to a general case as an actual algorithm? That should help you analyze its time complexity better. 

From my understanding of the algorithm, this seems like a back tracking based approach. While this might give good and fast solutions under certain cases, it has a worst case complexity which is exponential (and thus not polynomial). 

I am not certain about my preliminary analysis because it is hard to analyze it from a single example, and thus I'd recommend writing it as an actual algorithm in pseudocode. 

I must appreciate the empirical analysis that you've done, but for an algorithm to be polynomial, you need to prove its worst case via an actual analysis across all possible inputs instead of only some input. Hope that makes sense.",1530900553.0
olliej,"I feel that I saw someone posting a variation of this somewhere else recently, and receiving similar responses: namely if you say you have a polynomial algorithm that solves a problem then it has to be polynomial over all inputs.

Also I am unclear why you’ve claimed polynomial over n-1, as any problem of N nodes could trivally be converted to N+1 by addition of a new node, and then solved via your N-1 solution (this is why constant terms are generally dropped from complexity statements)",1530928993.0
xdert,This would mean that Hamiltonian cycle is in P and thus P=NP. ,1530896618.0
m--w,"Hey, just as others have said, there is a misunderstanding of computational complexity here. I hope to just provide an example to show you where your error is. To start, we have proven the the lower-bound on the sorting problem -- in the general case -- is O(n log n). However, I can provide the following algorithm:

    def sort(numbers):
        n = len(numbers) # Notice how this the size of the input, thus 'n'
        for i in range(n-1):
            if(numbers[i] > numbers[i+1]):
                swap(i, i+1, numbers)
        return numbers       

Okay, so it is clear that this method will always return after 'n' steps, and is therefore has linear time complexity (i.e. O(n)) however, this algorithm is not 'correct' in the classical sense. That is, I can provide many cases in which this algorithm does sort my inputs, for example \[1, 2, 3, 4\] or \[1, 3, 2, 4\], but it has many uncovered corner cases. Namely, \[1,2,3,1\] because the last index is not checked, or \[3,4,1,5\] because the algorithm does not solve for out-of-order numbers that are separated by more than one position in the array. 

In your problem, you solve an explicit case in linear time, or constant time, but the issue is in order to be a 'correct' solution, it must satisfy ALL inputs that anyone could supply.

Hope this helps!",1531063345.0
csteacher,"C++ standards like C++03, C++11, and C++14 refer to the elements that make up the *language*.  These elements are mostly things like keywords and tokens, such as if, while, &&, etc., and the libraries, such as iostream, iomanip, etc.

Standards are like a *version* for the language, meaning that if you are using C++03 standard the features are different than if you are using C++11.  Sometimes things are added or removed.  Let's take a look at iomanip: http://www.cplusplus.com/reference/iomanip/

Note at the bottom you can see that some things have C++11 next to them -- that means they were added in the C++11 standard.  If you use an older standard like C++03 then those elements are not guaranteed to be there.  

The ultimate goal is that if every compiler implements standards then programs that compile with standard X on one compiler will compile on any other compiler/architecture combination that implements standard X as well.  What the executable code *looks like* may vary wildly between architectures -- so compiling on one system and then taking that executable to another system is *not* guaranteed to work.

In the title you also asked ""which to use"" -- that's up to you but note that newer standards like C++17 may not be available on all the compilers you have/use/must use.  When I teach C++ I make sure that my students understand the concepts of standards and force them to compile their programs with the C++11 standard in part because a lot changed with that version and that what our textbook uses (although it fails to give appropriate treatment to the WHY of standards).  It's important to understand that only older standards may be available if you are working on older systems and that you may need to familiarize yourself with the specifics of what is and what isn't part of that standard.",1530896257.0
khedoros,"You have to have an appropriate C++ runtime library installed as well, which is why a lot of programs will install a package like ""microsoft visual c++ runtime library"" (with some version number attached) when you install some programs. That's only indirectly related to the actual C++ standard that your program uses, though. It's more directly linked to the exact version of the compiler that you used and the libraries that are included with it.",1530893563.0
tulip_bro,"Are there any books or resources treating operating systems in terms of theoretical computers science, and/ or mathematics?",1531366323.0
DTH97,Learn how automated resume readers work. LinkedIn uses a similar algorithm for their job search. Try to write your LinkedIn in a way that makes you look good to these algorithms.,1530819069.0
Kooshi_Govno,"I get contacted by recruiters every week or two.  I'm not sure if that's a lot, but these are some things I've done to improve my profile:

1. Connect to *everyone*, people you knew in high school, people you've worked with, recruiters that contact you, that guy you met at that party that time. The more connections you have, the easier you are to find.
1. Fill out everything.  Get your profile to ""all star"" status
1. Get a professional headshot
1. Put some real time and effort into crafting your profile and wording things well. Wording should be concise and professional. Get someone who's a bit further in their career read over it for feedback.",1530823068.0
VeviserPrime,/r/cscareerquestions,1530820572.0
insomagent,People don't know anything about you other than what you write on your page/your resume.  Also leave off what doesn't impress.  All you need to worry about is blowing their minds,1530815972.0
experts_never_lie,"[commentary, not assistance]

As someone 20+ years in, I find it interesting that what's suggested here is pretty much the opposite of what I do.  Particularly the ""connect to everyone who asks"" vibe is definitely *not* something I do.  I only connect with people I've worked with *and* I respect.  I ignore multi-company recruiters and the one-company recruiters get as far as me reading and then ignoring their messages.  In theory, one might interest me, but it's not likely.

It makes me wonder if there's a point in your career where you should pivot from ""connected"" to ""selective"".  I have no data to back this up; just raising the topic.

I am ***not*** proposing this as a strategy for new grads.  I wouldn't know what you need.  What I am noticing is that the tactics of first-good-job people seem completely different from my I-won't-consider-a-job-unless-it-interests me state.  (and ""interests me"" typically means ""a lot of equity in a new industry"")

At least I'm not fighting with the new grads for most jobs…

Best of luck, and I would say you should probably take advantage of these tools (which didn't even exist when I came up).
",1530872961.0
philipwhiuk,Memory Bounded Breadth First Search works better than Depth First search. Finding a good heuristic is harder than you might think.,1530879132.0
uselesspieceofdung2,"I've gotten almost 90&#37; of my opportunities through LinkedIn. It requires time to build up and tweak your profile. I found writing articles on Linkedin during my internship, and sharing them with fellow colleagues particularly effective for gaining traction. As you're still in school, tapping on your school mates to share would be the easiest way. You can easily scale it up by sharing your content with your professors. ",1530888525.0
flaming_sousa,"Yeah, the ""4 seconds"" mark means nothing.

The big thing with computation in the theoretical landscape is that time doesn't matter, the number of computations does. Algorithms that solve NP Complete problems can take extremely little time in the real world  when the parameters passed in are small. (it's when the problem size gets big that they take a long time)

On top of that, not knowing the algorithm that you're using means we couldn't give you a good answer in the first place. 

Thirdly, if you restrict the kinds of things that are passed to the algorithm, you are effectively working on a different problem. Im not entirely sure what you mean by Hamiltonian graph, but the HAM Path / Cycle general refers to an arbitrary graph, not any specific kind.",1530799549.0
_--__,"I would like to add one more point to the very good answers from /u/flaming_sousa  and /u/amp180, P & NP are about **decision problems** - these are problems where the answer is either ""yes"" or ""no"" (e.g. ""Does this graph have a Hamiltonian cycle?"").  A problem like actually finding a Hamitonian path/cycle is known as a **search problem**.  On the face of it, search problems could easily be significantly more difficult than decision problems as you only have to answer yes or no for the latter, and there might be some neat ""trick"" that can guarantee a solution without having to find it (see e.g. *Eulerian* paths/cycles).  In some cases, you can use a solution to the decision problem to find an efficient solution for the search problem  (e.g. you can find a 3-colouring by repeatedly querying a 3-colourability decision algorithm to make sure you are correctly building up a colouring).

Also, there is an analogue of P vs NP for search problems, however, as the others have both highlighted, you are looking at a search problem within a class of graphs with a **guarantee** that a Hamiltonian path exists - and that puts it in a completely different class of problems altogether (for similar concepts, it is worth checking out the complexity class [PPA](https://complexityzoo.uwaterloo.ca/Complexity_Zoo:P#ppa).)",1530807878.0
amp180,"It sounds like you are sidestepping the NP nature of the problem by using extra information. There are problems which take NP time to find a solution, but only P time to check the solution. The reason for this is the amount of information available.

Eg. For factorizing numbers, you need to check every number less than the root of the number to see if it's a factor, then all the numbers less than the root of your number divided by the factor you find, and so on. Traversing this tree of possible factors takes exponential time. However once you've found the factors checking them to see if their product is equal to the number being factorized can be done in linear time. 

In your subset of the problem, you start out ""knowing"" the size of the cycle you are looking for, and that you will find a node on the cycle at most 1 node away. With this extra information you could design an algorithm that could find a possible node on the cycle and check if the cycle exists in P time, but checking all possible combinations and permutations of this to find any cycle would take you back into NP time.",1530802256.0
CorrSurfer,"Ok, so to rephrase, you have an algorithm that takes:

- a graph with n nodes that is known to have a Hamiltonian cycle 

and computes:

- a cycle of length n-1 if and only if it exists. 

Furthermore, the algorithm has a computation time that is polynomial in n?",1530801092.0
Workaphobia,"If you have an algorithm that gives you a Hamilton cycle of size n - 1 for any graph of size n, then it is easy to convert this into an algorithm for giving you a cycle of size n: simply preprocess the graph to add one disconnected node and run the original algorithm on the new graph. The answer gives you a Hamilton cycle in the original graph. Preprocessing the graph is a small step (depends on the representation but certainly polynomial) so if the original algorithm was polynomial so is the new one.

This would indeed imply that P=NP. Therefore, it should be taken as extremely strong evidence that there is something wrong with your algorithm. Either it doesn't give the correct result for all possible graphs, or it takes more than polynomial time in the worst case.

Running your algorithm on a computer does not prove its correctness or complexity. Only analysis can do that.


",1530816039.0
orangejake,"It's also worth mentioning that P vs NP is about *worst case* preformance. Testing an algorithm on ""random ish"" inputs doesn't show much really. A good example of this is the efficiency of SAT Solvers on *random* inputs (like what you'd want to use in the real world), while certain ""specific"" inputs (that arise from reductions) are still incredibly slow.

Often we solve NP Hard problems via heuristics, and if you're doing that and they work *most* of the time, that's very different than them working *all* of the time.",1530817560.0
vinaych,"For all who are asking the algorithm, below is the link:
https://www.academia.edu/36989974/A_POLYNOMIAL_TIME_ALGORITHM_TO_FIND_A_CYCLE_OF_N-1_VERTICES_IN_AN_UNDIRECTED_GRAPH",1530859885.0
FrodofromHarryPotter,I think I saw this before. What does A level project mean?,1530789621.0
RickSagan,"Can I ask the purpose of the survey?

I just made it.",1530796856.0
xAdakis,"It depends on the project, your project structure, and how lean you want to be. 

For relatively simple projects, all you really need are the source files- `.c`, `.cpp`, and `.h` -that make up the project, along with instructions for building the application. 

On Linux, a `Makefile` is the standard, with tools like automake simplifying the process. 

On Windows, a Visual Studio project/solution is the standard, with Cygwin being an option if you want to cross-compile for linux. 

In my projects, I'll usually separate the source from whichever build system I am using through the directory structure. For example, I'll have:

- `bin` - for compiled binaries and data files

- `inc` - for project header (.h) files

- `lib` - for compiled static libraries

- `source` for project implementation (.c / .cpp) files.


Then, I'll have additional folders for the build systems.

- `_GCC` - for the GNU Compiler Collection (GCC)
- `_VS` - for Visual Studio Projects / Solutions
- `_RPM` - for building RedHat Packages
- `_DEB` - for building Debian Packages

Both RPM and Debian usually use the Makefiles under `_GCC`, but with scripts to modify the project structure to be compatible with those systems.

As a further example, using Visual Studio, I would add the `inc` folder to my project's include directories, but I would not set the output to the `bin` directory. I would add a post-build script to copy the relevant files to the `bin` directory on successful build. (This prevents my test system from breaking completely on failed builds.) 

As for Git, just google for ""<IDE or build system> gitignore"". . .and you'll get results like this for [Visual Studio](https://github.com/github/gitignore/blob/master/VisualStudio.gitignore), which will ignore the type of files that you don't need in a repository. Just save the file as a `.gitignore` in the root folder of your project and git won't see the ignored files.

Once you commit any changes you've made, you could run `git clean -x -d -f` to remove all ignore files from your project directory. 
",1530794792.0
RomSteady,"Your project file is roughly equivalent to a makefile. 

Your solution file is there to organize multiple projects. ",1530763358.0
csteacher,"Basically everything you wrote is correct, although note that the cluster size doesn't have to be 4KB.  It varies (by default) with the size of the volume and, beyond that, you can change it yourself.  

See more details here: https://support.microsoft.com/en-us/help/140365/default-cluster-size-for-ntfs-fat-and-exfat
",1530731783.0
paul_miner,"File size can be less than 4kb, however, the minimum amount of space occupied by a file would be 4kb (unless the file is zero bytes, I don't remember if a file is required to allocate at least one cluster).",1530934140.0
QuincyQueue,"The cluster size here 4kb is chosen as a compromise between how much memory can be addressed by the FAT and the amount of wasted space we create in the clusters (known as internal fragmentation). Cluster size also affects the size of tabular data structures which track the use status of memory clusters.

It's not a special number other than it's what was chosen as most desirable as a compromise between factors like this. But yes, if the size of a page is 4kb and you ask the OS for 2kb, you get 4kb because you've made that the minimum unit of allocation in your memory management scheme.",1531487534.0
willisjs,Nothing comprehensive like the Feynman lectures. First thing that comes to mind is Knuth's The Art of Computer Programming but it's not a close analogue.,1530718082.0
satin_glitches,"The MIT SICP lectures come to mind.

https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-001-structure-and-interpretation-of-computer-programs-spring-2005/video-lectures/",1530722217.0
clumma,"The Feynman lectures on CS are pretty good...

https://www.amazon.com/Feynman-Lectures-Computation-Richard-P/dp/0201489910/",1530742525.0
csteacher,"In terms of videos I'm not too aware of anything.  In terms of books, it's hard to say.  I can think of lots of books for other subjects that offer something to the more advanced, while still being accessible to an armchair follower.  A Brief History of Time by Hawking, The Code Book by Singh, and Gödel, Escher, Bach by Hofstadter (he's a massive asshole in real life, by the way!).

In CS I can only think of texts that are incredibly well known, but most likely best for those who are experienced, such as:

* The C Programming Language by Kernighan & Ritchie 
* The TeXbook by Knuth
* The Art of Computer Programming by Knuth
* Compilers by Aho, Lam, Sethi, and Ullman
* Introduction to Algorithms by Cormen, Leiserson, Rivest, and Stein

I think all of the above have been out for 25+ years, with updated versions, and are still commonly used and/or referenced.  There are also lots of seminal papers such as A Mathematical Theory of Communication by Shannon.

It may be helpful to check out the list here: https://en.wikipedia.org/wiki/List_of_important_publications_in_computer_science if you want to know more!
",1530730416.0
rmc0d3r,I don’t know about the theoretical computer science part but for understanding how computers work at a very deep level you can have a look at “Code” by Petzold. Computers will never seem like magic black boxes once you read this.,1530720117.0
xTouny,"Download [Link](http://www.mediafire.com/?ix66gnu85n3mc)

* Quantum Computing Since Democritus by Scott Aaronson
* Algorithmic Adventures, From Knowledge to Magic by Hromkovic
* The Golden Ticket by Lance Fortnow
* The Master Algorithm by Pedro Domingos
* In Pursuit of a Travelling Sales Man by William Cook
* Probably Approximately Correct by Leslie Valiant
* Algorithms to Live by, The computer science of human decisions by Christian and Griffiths
* Life 3.0 Being Human in The Age of Artificial Intelligence by Max Tegmark
* How to Create a Mind, The Secret of Human Thought Revealed by Ray Kurzweil",1530767731.0
AaronKClark,"Clean Code, anything by Uncle Bob.",1530729759.0
sniddunc,"I would get the 256gb if I were you. Some schools are picky and say that you ""need"" to use Windows so go bigger than 128 to ensure you have enough room for that if it comes to it.

Also, be sure to get one without the touchbar. It just doesn't compare to a physical escape button while using editor shortcuts, for example.

Also, keep an eye out on Apple's refurbished store. I've purchased many computers from there and have never had any problems. The discounts are pretty significant, too.",1530716577.0
yakattak,Yes.,1530714475.0
sayubuntu,"A lot of universities have a deal with google for unlimited google cloud storage. I have the same MacBook and it’s fine. I spent a hundred bucks on an external ssd when I started working on deep learning projects, other then that 128 has been fine. Just use the web version of software rather then put then on your machine.",1530717464.0
AcidDaddi,You’ll be fine. ,1530718012.0
sparcxs,"I’m a heavy use developer, many docker images, 3 VMs, 3 Xcode versions, and probably 20 large git repo clones. I have 512G and use in the high 300s without actively managing my disk space. I think 256G wold be perfectly livable unless you work on multiple very large projects. Note that videos and photos are far more disk intensive than dev work, so if you plan on don’t photography or editing movies in addition to dev work, all bets are off. Other than that, an awesome dev machine, touch bar excluded.",1530727136.0
dumstick,I have a TB version. It’s great. It handles very well with all the software you need for development. Better than most windows pcs. Even complex cli tools or openCV will install seemlessly with a Mac. Only hitch is raw performance in data science tasks. It’s poorly performing with big data. I’m not sure if all laptops are this bad at big data tasks but for me this was a noticeable hitch. ,1530729387.0
heyimjordan,I bought a 256 and I haven't used anywhere close to 128 after two and a half years. You'll definitely be alright with 128!,1530714956.0
ofr8601,I would get a Windows computer... Scrap windows and install Linux mint.,1530720655.0
cruise02,"The book for the course is available in the [Readings](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-fall-2010/readings/) section, so that's a plus. I'd definitely read that online (or print out the PDFs) and buy a copy of Rosen's *Discrete Math*. It's one of the best text books I've ever read.",1530714662.0
rodrigomlp,Susana Epp's Discrete Mathematics,1530715697.0
DTH97,"I don't think this quite on beginner level, because it requires being familiar and comfortable with the basics of set theory and proof-based mathematics, but *A first course in abstract algebra* by Fraleigh gives a good understanding of most of how we get the results in a typical discrete math book. If you pair it with a chapter or two on graph theory it should cover all the big concepts. The main problem with using the book for this purpose is that it doesn't provide algorithmic solutions. It is strictly a mathematical foundations book.",1530718154.0
Pour_Louis,"SICP is not for beginners per say, but if you do the exercises your math will improve greatly. There is a very strong focus on algorithmic thinking and mathematics from square one. https://mitpress.mit.edu/sites/default/files/sicp/index.html

Calculus and Linear Algebra are required to do anything whatsoever in science. Here a great book on all that is math:  https://www.amazon.com/Mathematics-Content-Methods-Meaning-Volumes/dp/0486409163 ",1530719072.0
Fricken_Oatmeal,"This book is used to teach the Discrete Mathematics course for first year computer science students at the University of Illinois. Having taken the class and studied the text, I can say it's nothing short of excellent. Best of all, it's free. 

http://mfleck.cs.illinois.edu/building-blocks/",1530748341.0
tetroxid,"Do you speak German? If so I could share two excellent scripts, one on discrete mathematics and one on linear algebra",1530714319.0
rdrop-exit,"Concrete Mathematics - A Foundation for Computer Science, Donald Knuth et al.",1530775371.0
FUZxxl,"Try *Concrete Mathematics* by Graham, Patashnik, Knuth.",1530716901.0
xTouny,"First. Which branch/school of computer science are you pursuing? For greater details on CS branches, I recommend you [Wikipedia's Page](https://en.wikipedia.org/wiki/Computer_science#Areas_of_computer_science), [Computer Science: An Overview](https://drive.google.com/file/d/1Da-zkq8ln9AlJ9-Ie3velWYPMkfRUCDt/view?usp=sharing) and [Three Paradigms in Computer Science](https://www.researchgate.net/publication/220636751_Three_Paradigms_of_Computer_Science). I believe every school of CS has its own math mind set, Clearly, A programmer/software engineer shall not study algorithms in the same way as a theorist, The former's concern is on applying while the latter's concern is on proving. After that, inquire about mathematical preliminaries from someone specialized in the field you are pursuing.

Second. You might need specialized books for every field of Discrete Mathematics like [Graph Theory](https://archive.org/details/GraphTheoryTextbooks) in order to read the concepts more deeply and do more practice.

Third. A worth to mention textbooks. The Discrete Math Workbook: A Companion Manual for Practical Study by Sergei Kurgalin, Sergei Borzunov, 978-3319926445. 2000 Solved Problems in Discrete Mathematics by Seymour Lipschutz, 978-0070380318. Discrete Mathematics with Graph Theory with Discrete Math Workbook: Interactive Exercises by Edgar G. Goodaire, Michael M. Parmenter, 978-0132245883",1530767287.0
berimbolo7,"As others have said, Discrete Mathematics by Epp is great! I'm going through the book right now and it's excellent. There are a lot of excercises  at the end of every sections which helped me drill the concepts into my brain.",1530761777.0
,[deleted],1530721444.0
loosethink,"You might want to consider Godel, Escher, Bach: an Eternal Golden Braid, by Douglas Hofstadter.    (Perhaps read after the class.)  
For me, it tied together a lot of the theoretical aspects of my Computer Science degree as well as explaining Godel's Incompleteness Theorem and while I am not sure it intended to do this, but it helped me understand the underpinnings of much of mathematics through analogies.  (Hofstadter has other books on analogical reasoning, so it might have been an intended consequence.)  This book has helped me over the years in my career.",1530745823.0
PinkFloyded,"Not maths directly but really recommend this, https://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319",1530715320.0
homeomorfismo,"Theory of computation: Formal languages, Automata and Complexity, by Brookshear is a pretty good book. I haven't read more books about theory of computation, but this book let me introduce to DFA and NFA. ",1530725429.0
Nerdlinger,"Enh, generally speaking, the language doesn’t matter for a class like this (something like APL, erlang or, to a lesser extent, Haskell) could make some of the assignments trickier, but you can really do work with all of the concepts in any language. 

What would be easiest would be the language you’re already most comfortable withn though that might not be the most worthwhile.",1530671126.0
drvd,"Everything is simpler if not done in Java (and C++). IMHO your code will be _much_ shorter and much more focus on the algorithm if done in Python.

Skiena: ""Algorithm Design Manual"" 2008 uses C which gives nice short, understandable algorithms.

Also: Java is ugly for small stuff. You _need_ an IDE (you cannot write Java with Notepad). You need to build it or have your IDE build it. There is no REPL.

(Personally I'd go with Go (golang.org) as it discourages overly cleverness (like python one-liners) provides higher order functions and closures like Python, reads very well, doesn't need much infrastructure and overhead and compile times are so fast that I do not need a REPL.)",1530703463.0
onetwosex,"This website might be of some interest, even if it covers only the basics in data structures & algorithms using Python: [http://interactivepython.org/runestone/static/pythonds/index.html](http://interactivepython.org/runestone/static/pythonds/index.html)

Now, Sedgewick uses Java in his online Algorithms & Data Structures course @ Coursera, which shows that there is no golden rule as to what language one should use.

So, I suppose I'd go with the language that I'd feel more comfortable.",1530689089.0
hextree,"I would definitely pick Python. Time spent coding will be much lower, code will be easier to write, and more readable. If the goal is to understand the algorithms, rather than stuff like software design, OOP, etc, then I think Python will be best for this.",1530708540.0
hoyfkd,A reddit redesign that isn't a gigantic fuck you to users!,1530637699.0
,[deleted],1530616414.0
fullouterjoin,"Elementary school students should be taught logic and programming starting as soon as they would like. It isn't the outcome of ""knowing programming"" that is the predominate goal, but using the computer as an educational exploratory tool. [Seymour Papert](https://en.wikipedia.org/wiki/Seymour_Papert) wrote extensively about his [Constructivist Education Theory](https://en.wikipedia.org/wiki/Constructivism_(philosophy_of_education\)), [co-created Logo](https://en.wikipedia.org/wiki/Logo_(programming_language\)) and his book [Mindstorms](https://en.wikipedia.org/wiki/Mindstorms_(book\)) had its title famously used (with permission) by Lego for their [programmable blocks](https://en.wikipedia.org/wiki/Lego_Mindstorms).",1530627670.0
kdnbfkm,"For non-technical programs? No. Making such courses available would be nice... But that would be a State's  or even school district issue.

I re-read your question and you changed my mind. I welcome our new tech overlords replacing all those damn humanities requirements we had to take in school. Here here!",1530614875.0
SL4M_DunkN,"I think high school should teach more discrete math and less calculus, because discrete is a better foundation for code and frankly more people's lives. It would also be cool if scheme was taught intermitently throughout high school math courses; it's easy to set up and makes for a good foundation. That said, it's less about the programming and more about giving exposure to that way of thinking about problem solving. Someone who can solve problems with excel tables and someone who can solve problems with scheme scripts are equally well educated, IMO ",1530616033.0
abdelhakeem,"Well, I believe the course which will be helpful to almost all majors is one teaching the principles of computational thinking and some basic data structures and algorithms (think of csunplugged.org)",1530619973.0
Silbern_,"No. Not at all. Most people will never be in a case where programming will be useful to them. Since the tech world moves quite quickly, it's a skill that wouldn't be useful later in their lives either. I would support perhaps a basic *logic* class, with programming maybe featuring as a sub element, because then at least the logic is abstract and not tied to a particular form. 

However, honestly, I'd much rather schools teach the basics of how computers and other tech works. Given how important computers are in modern society, and how often they interact and use online services like twitter or their email service, learning how to troubleshoot their computer or why they shouldn't click yes on every popup would be a much better idea. It's ridiculous how many people still don't know these basic skills.",1530619347.0
csteacher,"I'm a CS teacher and I'd love for everyone to learn programming but I think there are bigger fish to fry.  In the past I've taught a non-CS major course called Introduction to Computers that would probably be useful.  Course topics included things like the components of a computer, how the Internet works, what various file formats are, how to effectively use search engines, and some hands on work with common applications (Word, Excel, etc.).

Computer literacy is important, and while programming helps, it is not necessary.

I think as time goes on you'll start to see a shift in the same way that we've seen with math.  Some schools have Cal 1 and 2 in their high school.  These aren't usually required but, maybe in the future, they will be.  In the same way, some high schools have programming courses but they are usually not required.  In the future they may be required, but a better idea would be to have a computer literacy course before a mandatory programming course.  When I graduated high school 20 years ago I did have to take a keyboarding course and also, independent of any classes, demonstrate proficiency with a word processor.

PS The push of [code.org](https://code.org) in grade schools is not effective in teaching people how to program or how programming works.",1530637760.0
xAptive,"Everyone should at least be exposed to it. The #1 thing I hear when I tell people I'm a programmer is ""I could never do that"" or variations like ""I'm not smart enough for that"". It's ridiculous.

Exposure to simple programming would show people it's not magic and doesn't require an extraordinary person.

Honestly, most beneficial to people might be a class that talks about the world wide web, discussing things like networking, dns, html, css, JavaScript, browsers, etc.

These are things everyone needs to have some understanding of to make good decisions. (Should I use that new cloudflare dns, or stick with Comcast?)

A lack of intelligent consumers hurts all of us. So even if people don't want to make a career out of it, such a course could still be very beneficial.",1530622151.0
DoubleDual63,"I agree, if its mandatory to make people to read up on their history, it should be mandatory to make people read up on their future.",1530621439.0
Hexorg,I think there should be a problem solving class in general.,1530629132.0
mghoffmann,"I'd rather have my high-demand skills kept in low supply, thanks.",1530659881.0
jhaluska,"Everything you teach, you have to find time for and that means you have to drop something else.  So no, I don't believe it should be mandatory.  Sure I think a computer literacy course is necessary and probably could be a small programming lesson in it, but I don't believe coding should prioritized.  I also don't believe the average public education teacher could teach it.

I'd rather see more time spent teaching typing growing up instead of hand writing.  I would have preferred less time spent on music and had computer coding be considered an art alternative.",1530617175.0
,"I believe it should be mandatory (like maths and geography) an introductory class to computers. There they would teach the basics like Word and Excel, using the internet (Browsers, e-mail, Wikipedia, Google), what *is* programming, what's a processor, RAM and HD, basics of security and privacy online... Then they could offer programming to those interested. There are things everyone should know, like how to use a computer, and there are things everyone should have the opportunity to learn, like programming.",1530625114.0
pico_12,"I believe the essentials of coding, just as basic maths, is skill everyone should  learn.",1530613891.0
Kclique,"Things that should be mandatory but often aren't: civics,  logic, rhetorical argument,  introduction to philosophy,  linear algebra,  personal finance,  discrete math,  intro cs. You obviously can't fit all of those.  But they all have serious merits. ",1530622226.0
ProgramTheWorld,"Coding and programming is highly specialized, so in my opinion, no. Learning the fundamentals of how to use a computer however, definitely.",1530623179.0
vidro3,"if not programming per se then at least how the Internet works, how computers work. stuff like that. ",1530628383.0
,"What do you mean with “basic principles of coding”? Learning a programming language? Or learning how they work? Or learning how to solve problems? 


I think nowadays it’s an important skill and we shouldn’t focus so much on whether students will need it later in life or not. School should give a broad general knowledge and coding/ computers became a huge part of our daily life and society. 


The problem is that most of those classes in school are a huge shit show (source: am german and most people how get their a-levels have had a computer science class for at least a year). People got super discouraged because all they had to do was move a damn LEGO figure around with Java. And control traffic lights. Then we were taught which different kinds of file systems for images exist and in the end we had to make a PowerPoint presentation about a country we chose. 


Most students were just annoyed by the class and the few who weren’t knew computers ans how to code better than the teacher. It was embarrassing. ",1530629134.0
LongUsername,"We had a class in middle school on computer basics. First half was ""here's a mouse"" and then Microsoft Works (it was the 90's). Second half was BASIC. 

There were a lot of the kids who couldn't wrap their heads around programming. Even the teacher didn't understand a lot of basic concepts beyond what he taught. I'm not a believer in the ""everyone should code"" but we need to make the option and support available to those who do earlier.

This reminds me: I need to start diving into Scratch so I can teach my almost 7yo son.",1530630002.0
juleschris7,"I have two reasons that I believe it should be mandatory to take a computer science course in school.

1. It was mandatory to take art classes in my grade school years and in college. I have always been bad at it and have never liked it, if I had not been forced to take it I might not be as positive as I am about how much I dislike creating art myself. My first computer science course was my senior year of college and I loved it. I really feel cheated that I didn't have the opportunity to take a comp sci class earlier in my life. It is important for people to be introduced to a range of topics so that they know their likes and dislikes.
2. I work in a research lab managing databases that were set up by others who had never taken a programming course in their lives. I really wish they had taken just one. Even learning how to name variables using a systematic approach would have saved us so much headache with the databases. Even though they created most of the databases without programming the knowledge would have been really helpful in how the data are organized and updated. It is clear to me now that the logic of computers is not actually logical to everyone. That being said if it weren't for how badly they screwed up the databases I may not have the job I have now to fix them :P",1530633717.0
JNCressey,"It could possibly be integrated with maths class. For every maths topic, as well as solving problems by hand, students could code their understanding of the method into a computer program.",1530638286.0
khedoros,"Sure. Right alongside car maintenance, home economics, personal finances, health class, and other life skills. Just enough to ingrain in people's minds that computers aren't magic. Maybe a semester course. Then, more in-depth classes for people who want them (like I took in 10th grade).",1530638614.0
brett_riverboat,Basic computer usage should be required but actual computer science/programming is a little overboard. Those that are interested or excel in the basics should have an opportunity to learn computer science though.,1530642928.0
FoundationFiasco,"I think Computational Thinking skills are much more important then just programming itself. I'm a computer science teacher, and most of my Key Stage 3 student, 11-13, are never going to program anything. However, they will all use computers to assist them in their lives, so it's important that they understand how to approach a problem in such a way that a computer can assist in solving it, such as using Abstraction to help create a simulation, or use Decomposition to break a problem down. It's something that we're trying to implement more into our lessons next year. ",1530643074.0
MirrorLake,"Having kids design a home or business budget using Excel sometime in junior high or high school might have a huge impact on financial literacy. Just looking at the amount of debt in the US shows how abysmal our financial educations have been. 

Programming should be made available, but maybe not required. I hear many people lamenting as adults that they never even had the choice to take programming in high school.",1530645215.0
babuloseo,"don't make them mandatory, but there should be an extensive offering since Grade 9 to Grade 12. like why did I have to learn databases in Uni, when middleschooler/higherschooler me could have picked it up? I did mess around with it at that age, and a course would have significantly boosted my understand of it.",1530649482.0
spinwizard69,"Total waste of time!!!!   Seriously you have students that can't even grasp algebra.    

Now that being said tech courses (not just programming) should be available to students who are qualified and interested.   This is perhaps where American education falls down the most as it is in the later mid grade and high school years that students should be exploring some of their interests.

In any event mandatory classes beyond the 3 basics, really are of no use because it forces students into things they have no interest in.   If anything it leaves many students with a very bad attitude with respect to their years in school. ",1530652441.0
teawreckshero,"It's not programming for operating a computer that I want, but the hands-on logical problem solving skills. I think kids struggling with basic algebra would understand better if they just learned how to do it in python (or some other high level language). A lot of kids don't understand that the goal of math class is to arrive at the **correct** answer, not just the same answer their teacher arrived at. There are rigid, formal, simple rules, and if you get that, the whole math subject becomes trivial.

But a big part of the problem is, often teachers don't even understand what math is! Math is the worst taught formal language in history. Some kids get to high school and can barely write syntactically correct math expressions. And once they graduate, they dump everything, never truly getting the hang of it. Then they go out into the world and complain that they weren't taught how to do taxes or how to budget properly. What they mean is, they didn't understand the point of their maths classes.

Algebra in school should be a matter of teaching the grammar and understanding what it means. Once you've got that down, everything else related to math is trivial. And if they don't already know a programming language by then, they'll already understand the concept.",1530666147.0
supertopher,"When I want to school, for CS, the first class I ever took I will never forget the first thing the professor said, something like, ""Computer science is the design and implementation of algorithms."" and mentioned they will not be teaching us coding/programming. That we were to use programming as a means to implement our programs and algorithms. 

To learn actual programming and coding we were given the option of taking optional zero credit labs where the teaching assistants would help us learn the program. At this school, we had to use C. I went to one of the labs once to see what it was like. They used Linux, Slackware to be exact, and all the computers were running Afterstep and I believe the editor was Emacs. Not a bad choice. 


I personally used Red Hat and Window Maker at the time, but I had respect for their decision. 

Anyway, I will never forget it and I like that approach. ",1530675080.0
saralt,"No.

People already complain about learning fractions.",1530624963.0
sir_sri,"Pretty much everyone getting any sort of degree will need to write some level of programming logic or code.  Rather than being software developers, it's going to be using subject matter expertise (whatever that is) to do some programming logic, whether that is in excel spreadsheets, document management, data gathering or something else.   All of that will depend on programming logic.  I'm a computer scientist, but I'm a professor, and the professor side of my job involves a lot of programming related stuff, that even the professors in Drama and English need to do - implementing logic for grades, managing files and documents from students, adding and removing student info from databases.   Former students in everything from forest and turtle management to human resources to psychology need some level of computer programming skills to do their actual jobs.  If your job is  tree or turtle preservation you need a database of all the trees or turtles you've checked, and that's a custom(ish) tool.  HR is trying to search/sort/rank profiles they can find, psych people are trying to manage databases of patients and drugs and to find info on new ones etc.  

I'm not sure we are at the point where the languages available for teaching computing are straightforward enough to be all that helpful to that range of problems.  But we're getting there.  More and more tools are going to be made available for more and more disciplines that let them encode job specific logic, or manage subject matter specific databases without a full time software developer hanging around.  

I think the role of computing is going to look a lot like game development, it starts out with computer scientists and engineers doing most of the work (think 1970's video games), and moves into the engineering team building tools that the subject matter people use to build something (present day).  

So I come down firmly on the side of (eventually?) needing programming in highschool.  Just like math, reading, writing, etc.  Some of it is just being an informed citizen, some of it is that you're going to need some level of proficiency in all of these things, regardless of what you actually do for a living.  The guy who welded the wheel back onto my dad's lawnmower needs to keep a database of transactions so he knows about warranties and who his customers are, and he needs basic computing skills to manage things like photos of all of the work he's done. ",1530627707.0
mdillenbeck,"No. Is much rather see a focus on home ec classes - tech people how to evaluate financial decisions like getting a credit card or loan, how to cook, how to balance a checkbook, how to do basic home repairs, etc.

For computer classes, I'd rather see usage emphasizes. Decisions on things like what a VPN is, how to do simple upgrades, how Trojans and viruses work, why posting to the Internet is free, how companies make money off ""anonymized"" data, what hardware numbers actually mean, and so on. Programing itself isn't very useful.  Consider this part of information aged home ec.

Programing? That should be an elective - but if rather see actual computer science taught (not coding but algorithm selection and implementation).",1530638412.0
flekkzo,"No. Teach them statistics and logic instead. Far better life skills.

Also stuff like basic cryptography, security, networking, etc would be good. From a practical point of view, not a theoretical.

Coding isn't terribly useful outside of actually writing software. Doesn't give any insight into practical use of computing devices for your average person.",1530631553.0
mendrique2,"as a functional programming fan, coding classes should not be mandatory. ba dum tshhhh.",1530617197.0
lordScrub,"I dont think this would be a good idea, especially with the prevelance of online classes and lecture content that students can take for free.

As I went to high-school in the US, I left not having been taught  how to file taxes, and how to save and invest my future income.

I feel like high-schools should mandate the ""life skills"" development of their students instead of creating manditory programming classes

I am not against programming classes in high schools, I just done think that they should be manditory.",1530617593.0
oridb,"No. There are so many more important things to make mandatory, like logic and critical thinking, or statistics.

Given limited class time, I would want those in the curriculum first.",1530634618.0
Halifaxing,Basic data structures could be taught at a highschool level that would be fun compared to other classes and open the mind to how things work without touching code. Basic tree structures and manipulations.,1530615190.0
BrightLord_Wyn,"Mandatory, no, but all high schools should be offering coding classes as electives.",1530630936.0
ParanoidDrone,"Straight up coding? Eh, not really. Unless you intend to go down that career path, I don't see much point to a mandatory Python or Java or whatever class in high school.

A more generalized course about what computers actually do and some of the underlying *ideas* that programmers use in coding, like filtering and conditionals and boolean logic? Sure, that sounds like a decent idea.",1530632181.0
xAdakis,"Yes and no. 

Everyone can benefit from knowing how to use a computer and proper operating practices. For example, they should be able to use all of Microsoft's or Google's suite of productivity applications: mail, calendars, documents, spreadsheets, etc. It is just basic knowledge that if everyone in the workplace used them properly, thing would go so much smoother.

However, I don't believe that everyone should know complex algorithms and how to write full C/C++ programs. 

I would say that many should have a basic understanding of the command line and simple scripting. There is too much time wasted on doing things manually, when a simple script can be used to automate most of the process. Unfortunately, most of the time it isn't worth sending a request for these scripts, because by the time someone received the request, the job could've been done manually. . .but that doesn't consider the time that could be saved in the future.",1530632509.0
Algamath,"Nah I don’t think so. I see it the same as whether or not there should be mandatory mechanic classes. You don’t need to know how an automobile works to drive one. The same goes with software. 

I’ve heard the internet explained as “tubes”. “There are tubes underground and they are filled with the internet.” This is a gross oversimplification, but it gets the point across. For non-technical people, it never needs to be more complicated than that. These tend to be the people who struggle with algebra. Forcing them to code would not help them.",1530633907.0
GNULinuxProgrammer,"Programming, no. Basic computer science, yes. I think algorithmic thinking, being able to deal with abstraction, procedures, generalization etc is a very important skill.",1530634862.0
twistedgrrrl23,Maybe just make that a more widespread elective. I think it should at least be available in more schools in more places.,1530635536.0
woodi22,"I think it should be a *highly encouraged* elective but it should not be required. I am about to graduate college with a BS in Computer Science. I love coding! But I grew up in a small town where the majority of my high school friends are welders/plumbers/farmers/trade-focused employed. Forcing these good people to learn code would, from my observation, have an adverse impact. Many struggled to get through high school because of their frustration with ""useless knowledge"" (like reading Shakespeare or completing the square). Requiring a coding class would have resulted in them cutting class and/or dropping out. ",1530639362.0
spacemoses,"It should be an elective and handled like a trade skill, like taking electronics or woodworking.",1530641955.0
Thatyahoo,"No.

I can't change the oil in my car, or replace an outlet in my house, but I can make a bitchin website.

And that's fine. There's nothing wrong with letting professionals handle their profession. ",1530643615.0
conundri,"Not necessarily coding, but the ability to identify and put in order the steps needed to perform a task or accomplish a goal is a skill that more people need to have.  ",1530643781.0
S1cK94,"It depends if you want to educate people to be the next generation of human beings or to be the next generation of cogs for the machine.

In the latter case yes, they totally should teach programming",1530644284.0
ohchan,"Since you’re all discussing on it, mind linking a crash course of basic coding which will help in excel based/ data entry work? I can’t see how it works for other people 🤔",1530644479.0
rrkpp,"I don't understand why people think this way. Its like saying ""wow, our society is really moving towards cars being important. Should everyone be a certified mechanic?"" No. Not everyone needs to code. Most don't need to code. Most never will, and when they do find that rare instance where they need some code, there's no shortage of people who DO code to happily do it for them. ",1530647567.0
Sparklingcobweb,"I think it should be a requirement if you are planning to head into STEM, economics, or education in one of those fields. In my education system we chose specialised subjects at the beginning of our third-last year of schooling. The coding we did during those three years was quite basic apart from two big projects. We didn't learn about algorithm classes and other theory, it was all about solving given problems. My only regret is that we worked in Delphi. I think Python would be a much better intro language (and actually might be the only language you need to know if you are heading into physics or microbiology, for instance).",1530656035.0
Stevied1991,My high school didn't even offer a coding class.,1530657350.0
onfire9123,"Folks denying that students should be doing *anything* are morons. What's a kid gonna do with its time that's more beneficial than trying out everything? Teaching kids computer science and having them practice a little bit with writing code will sharpen their problem solving skills and have them be at least a tiny bit familiar with the tech that sustains their lives. We live on computers now. It's silly to be absolutely in the dark about how it works.

I spent so much time in grade school repeating the same shit bored out of my mind. There's plenty of room within the standard curriculum to fit CS in with the rest of the humanities and math.",1530657807.0
untraiined,"I think youre asking the wrong sub. 

Everyone here will say yes/maybe.

We need to ask other professionals if it really is that necessary. ",1530660451.0
olliej,In America I’d be happy if they just started by requiring actual science in education rather than nonsense homeschooling.,1530664182.0
kyune,"The root of compsci is program solving, critical thinking, and some math.  People should be taught those things regardless of whether or not they want to deal in a technology-based field.  ",1530677776.0
Nosaj333,No school should be mandatory. Problem solved. ,1530679186.0
tjsr,"No.  Absolutely no.  It has no use to most people.  If anything let's start with the horrible term ""coding"" - what is that?  At best, we might want to explain to people some high-level basics of computer architectures.  But if you're arguing to make a highly specific and technical concept mandatory and then want to go about labelling it as ""coding""?  Oh boy.

The only ""coding"" people need to know about is the Dewey-Decimal system.",1530679849.0
rwp140,"yes, because you'll be quickly surprised how under standing code language structure and organization is helpful, let alone some basic coding. Nothing fancy just enough to understand the basics of how code works, and to be able to use its logic structures else where in life. No real advance knowledge. 

But I say that as one of the few who had programming classes available to them in high school",1530681195.0
rippingbongs,"Knowing what goes on behind software would at the very least be interesting for everyone, as we all use it. That said most students will go on to never use it so I dont know that this is a great idea. Like I took woods class, I ain't built shit since. ",1530681870.0
titanicx,"Nope. It will turn off a certain amount that may take to it later, and the vast majority of people will never use it and it will be wasted time. ",1530684767.0
Lanko,"Dude.  

Today I watched my CEO struggle for 20 minutes to come up with a new password that windows didn't dismiss for being ""too easy to guess""  I'd settle for kids coming out of high school with an understanding of how computers work. 
",1530689139.0
ReasonerJ,"For me it wasn't even an option, I think it should start there.",1530689605.0
acroback,"You do realize that coding is a glorified problem solving using software and hardware as tools?

Concentrate on Math, Arts, painting, physics, sociology etc, imo. 

Hnnnnnnnnngh...",1530692992.0
hoyfkd,I think HVAC should be mandatory.  ,1530696864.0
,"I don't think it should. For math, stats, engineering, etc. it should be, but I don't see how English majors will benefit from it.",1530743477.0
lordalch,"I would allow it as an option for a ""foreign language"" class.

 Choose one: Spanish, French, or ~~Parseltongue~~ Python.",1530619373.0
g051051,"Yes, just like basic financial management, cooking, car repair, and other ""life"" skills.",1530616653.0
vernes1978,"Yes.  
Even if just to introduce students into some of the basic concepts.",1530619910.0
elcric_krej,"No, coding classes are available for anyone and everyone free, online.

Forcing people to learn is pointless, it does not work.",1530621343.0
julianCP,"NO. But basic logic and ""algorithmic understanding"", yes DEFINITELY.      
Students should be able to write down a ""program"" in words.     ",1530621586.0
JimmyTwoTaps,"I agree with u/pico_12 If we are talking in terms of it being a requirement, then at a minimum students should learn basics such as variables, data types, data structures, and some basic algorithms. 

Languages like Ruby and Python are easy enough on newcomers that these concepts could be picked up with a few weeks, assuming students have zero knowledge. 

I think it could be interesting for Highscools to offer some higher difficulty computer science classes in place of math classes. This way you have a more situational/problem based approach to subjects like trig/cal/algebra. It would makes these classes go from “do this equation because math” to “here is how we can apply this equation/concept to an algorithm”. 

Going beyond high school, I don’t think it would hurt for colleges to offer CS driven classes. Me being a more hands on learner it is difficult for me to see how some math concepts apply to programming in a practical sense. ",1530617771.0
dwkeith,"Yes, CA and TX have made the first step by making it mandatory to offer CS in High School. I have been volunteering through TEALS to help them achieve that.

When you look at new tech, companies like Emerald Cloud Lab (where I work) are offering new ways of doing hard science in chemistry and biology that requires a base knowledge of computer science. The scientists that understand both will be able to outperform those that don’t. 

The same scenario is playing out in many industries, even as basic programming is getting easier, those with more advanced knowledge will get ahead.",1530621679.0
spacegirlmcmillan,"Yes of course they should. We teach math, science, history and language arts. People need to learn to code.",1530625001.0
DLabz,Nothing should be mandatory. Or prohibited.,1530629614.0
RR321,"Science should be taught, so computer science basics, etc. with a practical side like programming, sure, but the point is education with the aim of having a critical view of computers at large. 

So school should definitely not be teaching a single specific technology and making everyone a .NET monkey, iOS zombie or Chrome donkey... All of this usually happen under some b.s. grant from a (pretending to be) benevolent corporation enslaving a future generation of consumers instead of citizens.",1530635079.0
ice_planet_hoth_boss,Yes,1530636496.0
tennisguy1234567,"Yes. 

If I have to sit through ""health ed"" classes where our teachers instruct us to shampoo our hair first before washing our bodies, I should have to sit through something more valuable like a coding class. ",1530643182.0
EmperorOfCanada,"NO NO NO NO NO!!!!!

Coding is ... well programming.

Development is an art. 

At best they will ""teach"" programming. Seeing that if you are a developer you probably aren't a teacher the best they can hope for in schools are people who have a basic idea of what underlays things like the compiler. 

Thus the classes will largely involve memorization and regurgitation of programming terms and concepts. Outdated ones probably.

Not only will this turn most people off it may very well turn off future developers. I can just see someone who just gets it being told that since they didn't' memorize today's 10 coding terms that they won't make it in the ""real world"". Even worse is that people who think that memorizing all the programming patterns is going to make them a great programmer might delude themselves into going into development; this is where bad micromanagers are born.

If the goal is to promote basic literacy amongst the general population then don't teach coding. Teach some interesting tools for visual recognition or something. 

For future developers, just leave them with the resources and mentorship to do some development. Robotics, coding challenges, etc. Something more satisfying. Something that tickles the reward centers.
",1530662834.0
FUZxxl,"During lectures, take notes on paper.  Writing down what the lecturer says is an extremely effective way of memorisation.  It is important to take notes on paper as typing notes on your computer has proven to be way less effective.

In theory classes, make sure to get a good grasp of the definitions.  It is not important that you are able to remember all the details of proofs or the exact way algorithms work, but it's vital to have an intuition for the general idea and for how all the pieces fit together so you can work out the details on your own if you need to.

Make sure to get a lot of programming practice.  There is no other way to get good at programming.",1530613947.0
NeedsMoreWiFi,"I'm graduating in a few weeks, some how managed to pass with the highest mark possible, I'll post a few key points I wish I had done differently.

1st year, I was extremely motivated. Top of my class, but turns out the marks didn't count towards my final grade, became a bit discouraged.

2nd year, motivation gradually dropped, still achieved a top mark in the second year. Attendance slipped though.

3rd year. (Motivation == null), attendance < 20&#37;. Almost failed my dissertation, however spent the last 1.5 months non stop in the library and managed to bring it all back. I do not recommend doing this.

Keep on top of your work load.

Balance your social life, this is key to avoiding burn out.

Have fun.

Tackle your assignments gradually from when you first get them, they'll become a lot less daunting if you chip away even 30 minutes a day on them over the course of months.

Found a unit you enjoy? Use it as a base for further learning, don't stick to the core syllabus. It'll help you going forward.

Join a computing society if your uni offers one, or any society for that matter. Socializing is a key part to uni.",1530621981.0
tirolr,"Don't believe rumors like 'this exam will be the hardest one in 1. semester', since (a) this might fool you to put unnecessary effort into the corresponding course, and (b) it will most likely be wrong. Also, talk to students in higher semesters about which courses you can/should prepone (i.e. take in 1. semester even though the university 'recommends' taking them later) because they are (a) rather easy, and/or (b) require no/limited previous knowledge. This will further give you more time for you thesis later on, or even allow you to take some master courses at the end of you bachelor (while still being enrolled as bachelor).

I started with preparing & revising most lectures/practical sessions. As this (rather time consuming) strategy seemed to work out, I tried pursuing it since.

Note that this is just my opinion/personal experience. Also, I'm currently pursuing my bachelors degree (4th sem) at a German university.",1530614810.0
zagbag,"If things get rough: go talk to someone about it, preferably with a professional and in person. ",1530617337.0
sudip_bhandari,"During your course you might fall into a trap that many fall into. Don't be too skeptical about why you have to study one particular subject. All of those boring theoretical subjects, rigorous mathematical subjects are going to turn out very useful in the future. And believe me this is the only time you will get this dedicated time-resource to learn those stuffs. Don't think like, I am going to become a web developer: why should I learn matrix inversion or eigen vectors. (Heavily used in ML, I didn't know back in college)",1530624839.0
kdnbfkm,"Join a study group. Perhaps with whoever hangs out in the school's tutoring center. Maybe accompany classmates to the computer lab. However you do it school will be an easier place to find others compared to self-learning.

Take advantage of office hours, whether with the head instructor or TA's.",1530633364.0
Kris--,"There's some excellent advice in these comments.

If you've been out of education for a while, or are historically weak academically, I'd encourage you to refresh yourself on the basics. Such as essay writing, study skills/revision skills and eventually how to write a dissertation. Many of the comments talk about getting a lot of practice programming, which can only help. However, university is generally an academic practice. So you need to be able to pass exams, get good grades on essays and get a good grade for your dissertation - which is typically weighted higher than individual modules. There's some good books on each of those called Student Essentials, its all quite obvious, but of the type you need to be reminded.

Taking online courses on the same subjects as are covered in modules as you are taught can vastly increase the.. breadth of learning. You only have so much time in front of a lecturer at university. By the end of modules you'll simply know more about the subject, get better grades and have more usable knowledge. Programming skills is great, but backed up by strong theoretical foundation, you'll find debugging easier and faster, and make better programs.",1530650671.0
xAdakis,"Pick a side project and work on it whenever you are not doing school work. This can be a website, a simple game, a mobile app, or a desktop application for tracking the inventory of your pantry. 

Lectures and labs will teach you the fundamentals and give you a good base of theoretical knowledge. . . an integer is usually represented as 32-bits, this algorithm is better than this one under these conditions, this is how a compiler turns your C++ code into machine code, etc. . . however, it doesn't mean much unless you can use it in a practical manner. 

A side project gives you the opportunity to explore the practicality of coding. . .it goes beyond algorithms and equations and into different systems and libraries and how to connect them together to produce something you can actually use. 

The resulting product will most likely be far from perfect. . .your game may only get 5 frames per second, it'll take 10 minutes to compile a simple list of items, or it may be missing some key functionality. . .but this is how you learn. 

It's a wonderful feeling when you go to a lecture, the professor introduces a new algorithm, and then you think ""Hey, I can use this in my project."". . .you'll be more interested in the material, pay more attention, and be able to use the algorithms like a pro.",1530660279.0
lawnm0werMan,Get a firm understanding of Object-Oriented Programming methodologies outside of just Java asap. It’s all about the bigger picture. ,1530690310.0
frabjous156,"I can only recommend doing alot of programming, it’s really the main thing that makes you better.",1530614483.0
remy_porter,">  Is there compression?

There is an absolute shitton of compression.

So, let's see how well we can do with an ELI5 version.

The end goal of video data is to make pixels turn colors at certain times. This means, *in the end*, video data *represents* a series of RGB values for every pixel. But that's a lot of data- at HD resolution a single frame would be over 6 megabytes (assuming 24-bits per pixel). So yes, we compress it.

Broadly speaking, compression takes two forms: over space, and over time. Over space is something akin to PNG or JPG compression- knowing that there are patterns in the image data, we can throw away some pixel data and reconstruct it later. This may be lossless (PNG) or lossy (JPG). There's also compression over time- instead of sending data for every frame, I can send a single keyframe and *then* simply send a set of instructions which specify how the frame should change over time- the differences between frames, essentially.

But video isn't all you have in a video file. You also have audio, and audio has its own rules about compression. And worse- the audio needs to be synced to the video. You'll also have many channels of audio- like stereo or surround sound.

With all this in mind, though, we can start talking about what's in a video file. First, you have one or more channels of video data, and zero or more channels of audio data. To turn raw video/audio data into a compressed format, we are going to use something called an *encoder*- there are *many* encoders. MP3 is one audio encoder. h264 is one video encoder. Don't confuse these with files, just yet- while the output of an MP3 encoder may be put into an MP3 file, ignore *files* for right now.

Each of these encoders also has a matching decoder, which we'll use when we want to decompress the video. Together, we refer to these as ""codecs"" (enCOder, DECoder).

This doesn't solve the problem of syncing the audio and video channels though, so we're going to rely on something called a ""container format"". A container format is a way of structuring encoded video/audio streams into a synced format. AVI is a common container format.

Once we have all the data assembled, we can then dump it out to a file. Which file? Well, it depends on the container and the streams inside of it. And there are loads of options here.

> how could I begin to process the visual information

And all that together answers that question: you need to open the container and decode the contents using the appropriate codecs. And if that sounds incredibly complicated, you're absolutely right. Because of this, there are loads of libraries that streamline this, like FFMPEG, which basically abstracts out all the details and can decode pretty much anything, and hand you back the frame data as arrays of RGB data. From there, it's up to you how to do it.

I should also add, whether video data can be streamed or not depends on the codec used to compress it. Some compression formats stream quite well, others stream quite poorly (or not at all).",1530560232.0
mrjast,"Video compression is *very* complex. We've got decades of algorithm developments stacked into today's and tomorrow's standards (H.264, VP9, Ogg Theora, VC-1, AV1 etc.) and encoders/decoders. Here are a few basics of what's going on, including keywords you can look up.

* Virtually all codecs transform pixels into a different colour space, usually Y'CbCr (sometimes incorrectly called YUV though that's slightly different), which is basically luminance/luma (brightness) and two chroma values, for blue and red difference. Look up YUV or YCbCr to see examples.
* Because the eye is a lot more sensitive to details in the luma channel, codecs usually do *subsampling*, which roughly means that for a group of pixels you store less chroma data than luma data.
* The two big areas of video compression are intra-frame compression (which is about compressing a single frame on its own, like JPEG) and inter-frame prediction in which data from one frame, or even several is used to encode another frame more efficiently. To dumb it down a little, a P-frame (P = predicted) encodes just the ""difference"" to the previous frame. To calculate that difference, the encoder compares the two, trying to account for camera shifts and moving parts and such (""motion compensation""). The resulting frame has information about which parts of the base frame have moved where (frames are broken down into macroblocks which, in turn, may be broken down into smaller blocks of different sizes), and then only the difference to that transformed frame is saved on top of that. For more fun, there are B-frames, too (B = bidirectional); B-frames use are predicted from an earlier *and* a later frame. Frames that do not use inter-frame prediction are called I-frames (I = inter) or keyframes. Since you can't directly decode P- and B-frames, you need to seek to the nearest previous I-frame and decode mostly everything in between if you want to get at a specific frame.
* The basic approach to most intra-frame compression schemes is called DCT, applied on small blocks of the image one by one.
* After all of that is done, the resulting data is further compressed using a scheme based on bitstream prediction, e.g. arithmetic coding. The basic idea here is that you feed a prediction engine with your stream, let it predict which bit is going to come next, and store a sequence of more likely bits more efficiently than a sequence of less likely bits, as determined by the predictor.

That's just the basics, there are a lot more things that go into making a good encoder, such as psychovisual modelling and different profiles and heuristics of how much work various of the optimization parts of the encoder do. For instance, motion compensation is very computationally expensive; essentially the frames needs to be shifted in a bunch of different ways in turn to find which best describes the motion. Trying more patterns takes longer and there's an effect of diminishing returns. Finally, psychovisual modelling is the true magic in all of this. The best H.264 video encoder I've ever looked at, x264, blows all of the easily available alternatives out of the water.

Regarding file types, there are two sides to a video file: the video encoding standard used (e.g. H.264, also know as MPEG-4 AVC), and the container format used (e.g. the MPEG-4 container or the Matroska container). The two are largely independent; you can put pretty much any type of video stream in an MPEG-4 (.mp4, .m4v) or Matroska (.mkv) container. More information about container formats: [https://en.wikipedia.org/wiki/Comparison\_of\_video\_container\_formats](https://en.wikipedia.org/wiki/Comparison_of_video_container_formats)

That said, H.264 and the MPEG-4 container format tend to dominate the market these days.

All that said, while decoding the *container format* might be doable, you'd have to be quite insane to write your own video decoder these days. Check out the source code for FFmpeg, the swiss army chainsaw of video encoding and decoding; it's *huge*, and that's without counting all the third-party libraries it supports for encoding various standards.",1530563447.0
xAdakis,"So, a real ELI5. . . 

Video is stored in **frames**, which is played back at a specified **framerate**. (for the NTSC standard, this is approximately 29.97 frames per second) 

Audio is stored in **samples**, which is played back at a specified **samplerate**. (For the NTSC standard, this is 44,100 Hz or samples per second) 

The contents of **frames** and **samples** depends on the codec being used to encode the data. In an uncompressed format, a frame may be a single still image, a single frame of film. For audio, it would be a point on a wave form. You play these back at the above-mentioned rates and you get a video and audio. Each frame/sample usually has a timestamp on it to keep them in sync.

Obviously, this data can be and usually is compressed and simplified. For example, a video codec may eliminate data from frames by only storing the pixels that changed from one frame to the next. A key frame may be added every second- or some other arbitrary time span -to allow a player to scrub through a video without having to read the entire file. 

This is a deep rabbit hole filled with aging technology, so be warned. I wouldn't try to implement a codec by hand until you understand how to use the following:

[DirectShow](https://msdn.microsoft.com/en-us/library/windows/desktop/dd375454(v=vs.85).aspx) is still used in many applications for working with audio and video on Windows; however, it is extremely difficult to find quality Filters (codecs) that work with it that don't cost an arm and a leg.

[FFMPEG] is the go to tool for encoding and decoding video as it has support for several encoder and decoders with hardware acceleration. You can download the ""dev"" linking option when downloading for Windows to get the header and library object files. 



",1530572484.0
ricodued,"[Here is an interesting little overview of H.264 compression](https://sidbala.com/h-264-is-magic/)

And [here is a comment from /r/programming elaborating on that article and clarifying some things](https://www.reddit.com/r/programming/comments/6m9imx/h264_is_magic/dk01o6s/?utm_content=permalink&utm_medium=front&utm_source=reddit&utm_name=programming)",1530566205.0
ML-newb,This may help : https://sidbala.com/h-264-is-magic/,1530575070.0
csteacher,"Others have given you more technical, and correct, descriptions of compression, frames, etc.  To give you the shortened version, basically you can encode a single image (frame) of a video with a variety of compression schemes (examine gif, bmp, jpg, to learn more).  Videos frequently use 15, 30, or 60 frames per second, so you are being shown 15, 30, or 60 individual images per second but, because they are shown so fast, it looks like a video.  From a practical perspective note that videos typically have some continuity.  An image at 5 seconds is probably not substantially different from an image at 5.1 seconds.  So, we typically don't need to store the entire image but, instead, some data describing how the previous frame at 5 seconds is different from 5.1 seconds, which is a smaller amount of data.

Lastly, I do want to address the fact that you are talking about streaming.  There are tons of protocols, codecs, etc. but note that streaming is generally not substantially different from regular video files that you just watch, and don't stream over the internet.  So if you are interested in learning more, look up the more general topics of how videos (avi, mp4, mkv, mpg, qt, etc.) work.",1530575326.0
invalid_dictorian,"Here's a short list of CompSci and ECE topics to look up, from basic to advanced. But basically if you want to learn this topic, it helps to start with the fundamentals. The math details can be difficult, but don't get discouraged.

Some of these are course names too, so find it in your college's catalog. Others are very broad topics.

- Signals & Systems
- Digital Signal Processing
- Image Processing
- Pattern Recognition
- Computational Photography
- Computer Vision

Basically, the video compression technology of today utilizes all the research and the algorithms developed in all these areas.

Now back to streaming videos or just videos files. There are several aspects to it. First is how to deliver the files from one computer to another. Video container formats addresses this issue, when you see `.mp4`, `.avi`, `.mkv`, `.mov`, or `.ts`, those are video container formats, which defines what bits goes where. Some are more suitable for streaming than others.

https://en.wikipedia.org/wiki/Comparison_of_video_container_formats

Now the actual content stored in the container can use different compression technologies. Those are called codecs. H.264, H.265 (aka HEVC) are some of the most popular formats today.

https://en.wikipedia.org/wiki/Comparison_of_video_codecs",1530596562.0
CosmicCouchPotato,"Id say any topic that consists of making tools instead of using them is a microcosm. Like creating an operating system, making your own programming language. That being just the ""programming side"" of CS, another microcosm would be a lot of the areas of math that only a few are working on improving.",1530559364.0
,"Literally all of them are built on CS fundamentals.

Simpler things like web dev are made to be used without necessarily knowing the ""microcosms"". The people who write languages don't only write them for other people who write languages. They write them to be used.",1530548035.0
musicin3d,"Based on my experience as a web developer, I would expect games to be much closer to any sort of ""microcosm"". Everything from physics to UX (basically applied psychology) plus a lot of art make up a typical AAA game.",1530547258.0
abrasivestepfather,"I don't have any microcosms to add but I just wanted to encourage you in working with compilers. I have found that understanding and building one has been very helpful with my computer science understanding in general. Also if you haven't or don't know the Go compiler is in its standard library, so its a great tool for playing with a compiler.

https://golang.org/src/go/

edit: I linked the source code, where is the package's documentation:

https://golang.org/pkg/go/",1530557155.0
PM_ME_UR_OBSIDIAN,"Compilers, OSes, network stacks, distributed systems, ...",1530571269.0
jhanschoo,"Information retrieval.

The NLP side may expose one to grammars as well as language models. Quick retrieval and storage concerns the more engineering-esque problems. Weighing relevance is related to information theory and statistical methods.",1530584227.0
geekynoob3,Book and author's name? ,1530600271.0
fnybny,"1. Graph/ lattice theory for writing useful static analyses.
2. Grammars and automate theory for the front-end work.
3. Parsing theory.

These are all pretty fundamental.",1530553607.0
subignition,Only a third of this page is visible on mobile because of persistent ads. What the fuck.,1530526655.0
trobertson,"`f(m,n) = m modulo n`, looks like. The beginning stuff just asserts that `m` and `n` are natural numbers, and that `n` isn't 0.",1530524955.0
StEvUgnIn,"It's being said on the top: ""I can tell you...""",1530569092.0
StEvUgnIn,Why so much haters? ;-( ,1530540040.0
Fernicia,"I think the 'solution' is wrong.

I started by noticing the pattern in the number pairs. It is always x-y where x is greater than y. It let me start with a given value for x (7) and count how many times that appeared in the lists. If it was less than x-1, I knew the missing pair belonged to that value for x.

This got me down to x=4, because only two values for x appear, where y=1 and y=2. So the missing value is x=4 and y=3, i.e. '4-3'.

The OP ignores this ordering and says the answer is D, rather than B.",1530487255.0
PM_ME_IRL,This is about as interesting and relevant to compsci as a wordsearch puzzle.,1530488373.0
mrexodia,"> I summed all the numbers in each row, which gives 35, 31, 29, 28, 38
> Then for the answers 34, 35, 35, 38 for which 34 is the only result we didn’t see yet. Therefore I think it’s A.

Derp, op deleted the original post...

https://i.imgur.com/dcV1Ws5.png ",1530489694.0
hextree,"> the fastest sorting algorithm you’ve never heard of

Timsort is a well-known sorting method. It seems a little click-baity, and presumptuous to assume that someone reading an article on sorting algorithms would have never heard of Timsort.

> built for the real world — not constructed in academia.

> that is efficient for real-world data and not created in an academic laboratory.

... what? If I've read that right, it seems a bit condescending. Timsort is merely a modification of methods that were already created and heavily researched in academia. And regardless, why is it being implied that if it was constructed in academia it wouldn't be useful in the real world? The vast majority of stuff being used 'in the real world' efficiently is directly from academia.

> Timsort’s sorting time is the same as Mergesort, which is faster than most of the other sorts you might know.

All serious comparison sorts are O(n log n) average case, and integer sorts can be faster. So, no, I would say it is equal or slower to most other sorts I know.
",1530473807.0
Investisseur,Plenty of people know Timsort. Anyone who uses Python and cares about the underlying implementation certainly has,1530460866.0
,[deleted],1530472859.0
ProgramTheWorld,Tldr: it does an additional binary search so that it can move an entire block when it’s doing merge sort.,1530457029.0
shuerpiola,"Super sensationalistic. Mergesort is O(n log n) but tends to perform poorly on smaller data sets. Insertion sort is good on smaller data sets but scales horrendously. Timsort is derived from the two.

Even if youve never heard of Timsort, this shouldnt be anything mindblowing to anyone who paid attention in programming 2.",1530498088.0
kdnbfkm,"I heard about Timsort in regards to a lawsuit between Google and Oracle, having something to do with Java or something... But never looked up the details.",1530461258.0
Revrak,Given the title I tought it was referring to a new version of tim sort...which is pretty well known ,1530479791.0
Shed412,I thought this was a joke related to Hello Internet. Slightly disappointed.,1530470090.0
elcric_krej,"As far as I know, the gnu std still uses Introsort and the few benchmarks I can find show std::sort performing better in some cases than timsort. Clang uses something close to TimSort and the rust std just goes with mergesort (the argument being that, due to it being easier to optimize by the compiler and write without unsafe blocks, and usually just as fast irl, it's the superior choice for a std).

There's not ""best"" or ""fastest"" sorting algorithm, it's always a trade-off between speed, memory consumption, performance in various cases depending on the existing order and size, time volatility as a function or initial ordering, ease of optimization by the compiler and the CPU and probably many other factors. I'd wager that that even on the same dataset, ""the best"" sorting method is usually dependent on the specific arch and even on the exact hardware configuration (e.g if Asort is 1.01 faster than Bsort with an L1 of 256, it could be that Bsort is faster than Asort with an L1 of 512 considering the rest of the hardware specs stay the same... and they almost never do). And that doesn't even get into scalability when using multiple threads.

",1530480055.0
bart2019,"I wouldn't call it an algorithm. It's more a ""look at the data and then choose one of the other algorithms""approach.",1530505295.0
RexPowerColt69,"Never heard of this sort, thanks.",1530557709.0
leftofzen,"But we have heard of it, because it's reposted here many times a year.",1530485935.0
misingnoglic,"Great article! One of your images has a ""9"" which it then disappeared in the follow-up picture. The one where you're reversing runs that are decreasing.",1530463566.0
emperor000,That's assuming free will exists in the first place. Which it doesn't.,1530550205.0
Meguli,No. ,1530508214.0
hardwaregeek,"This is gonna sound bonkers, but I'd argue that Haskell is a good way to learn a lot of math. Haskell is a tough sell if you're coming from prior experience in programming languages such as Python or Java, but if you're approaching programming like math, then Haskell is probably the best analogue. A lot of the reasoning of Haskell, such recursing down a list, is very comparable to concepts in discrete mathematics and algebra. It's hard to explain exactly why exactly Haskell is a good combination with math without going into a long discussion of imperative vs functional programming (ignore those words), but basically Haskell focuses on items, their characteristics and what they do, while other languages focus on items and telling them what to do. If this sounds interesting, [check](http://haskellbook.com/) it [out](http://learnyouahaskell.com/)!

If Haskell is a bit too much, Scheme is another interesting language for math. There's a very classic book called the Structure and Interpretation of Computer Programs which discusses various mathematical exercises you can do in Scheme, such as solving polynomials using Newton's method, computing square roots, etc. The core idea of Scheme is very simple and it's a really easy language to just pick up and learn. It's even linked [in the sidebar](https://mitpress.mit.edu/sicp/full-text/book/book.html)!

Finally, you could use a more mainstream language like Python, Java, etc. And if you just wanted to learn programming, I'd suggest starting with one of those. But programming and math aren't actually that related. There's nothing particularly mathematical about Python, especially in a web application or a simple command line tool. That being said, Python, Java and JavaScript are all great first languages. In the end, there's no one way to learn to code, simply try a way and see what sticks!
",1530418633.0
blahblahbla39,Probably python,1530414442.0
BrunchWithBubbles,Haskell. It’s one of the few languages where functions are actual functions in the mathematical sense.,1530436322.0
Nokthar,"Python first, purely for the simplicity of the language. It is a great first language to familiarise yourself with coding. Although I personally think the best choice from a mathematical stand point would be a functional language such as Haskell. Purely functional languages are much closer to doing 'maths' then their counterparts.",1530417431.0
sn0rrlax,"So I know that the common thought from people that don't actually program is ""comp sci and math have strong ties"" and they do technically but not in the way that people think. I have a bachelors in computer science with a minor in math. Math didn't help me program until I started working on machine learning near the end of my 4th year. So I'd argue: learn math, but not directly for the purpose of aiding in the study of programming (especially high level languages). In terms of languages to learn, python and java are both great, and if you want more info on the differences or what to pick first I'd be happy to go more in depth as I'm sure most others here would",1530421490.0
underscore_frosty,"Haskell would be my top pick for this endeavor. It is very much a mathematician's language and computationally models a lot of ideas from mathematics very well. It is also a language that has very deep connections to some fundamental fields in mathematics, most notably mathematical logic, category theory, and type theory. Further, Haskell is being increasingly used by mathematicians to solve or work on problems in those same fields. So, in a sense it could probably help you understand a lot about mathematics from a very fundamental view.

However, Haskell's syntax can be very daunting to beginners, especially those coming from say a Java or C background because the paradigm is completely different (Haskell is a purely functional language, Java, C, and many other languages are imperative languages). Basically, in a functional language like Haskell you tell the computer what it should compute, but not how to compute it. In imperative languages you tell the computer how it should compute something rather than what it should compute. It's like saying ""compute the square root of 144"" versus ""follow these steps to compute square roots.""

However, if your intent is to do mathematics rather than learn mathematics, i.e. you just want to use code to solve problems arising in class or textbooks then a language/CAS (computer algebra system) like Wolfram Mathematica or MATLAB would be better suited for the task (free, open-source alternatives to these, if you care about that, would be SageMath and Octave, respectively). These languages have a lot of built in functionality for handling mathematical computations. You can do stuff from basic algebra and calculus all the way up to the more abstract stuff like working with groups and rings. These languages are used extensively in computational mathematics as well as in more applied fields like engineering, data science, and the physical sciences.

For your last thing about computational neuroscience and the likes, in these fields you'll be working with a lot of data which you will invariably need to calculate some statistics on. A language that's used pretty frequently for this among scientists is R. I don't know much about R so I can't really speak to it. Additionally, Python is another language that is frequently used in fields like data science and machine learning to manipulate and reason on large amounts of data. A significant advantage of Python is that it is an incredibly flexible language with a very easy to understand syntax. A disadvantage though is that Python is significantly slower than many other languages (Python is interpreted whereas many other languages are compiled). Very often, if you have no programming experience this is the language a lot of CS departments will start with before moving on to Java or C/C++. I have to agree with this and state that if you are just starting out with programming, take a look at Python.

Best of luck!",1530429415.0
vladmir_zeus1,"`Python` would be apt for your case, but there's also `Java`. I'd say give both of them a try, and then you decide for yourself. ",1530428363.0
nanonan,"I'm going against the grain here. Learn C. It's not specialised for math in any way, it is simply that it was ubiquitous and is still very popular, giving you access to decades of knowledge.",1530432850.0
the_shape89,"If you want to learn math, learn math. If coding problems helped as much as you seem to be thinking it will, then programming would be a staple of all math degrees. It's not. This strikes me as similar to learning French to help you learn English, since English has French words in it. 

But if you want to learn both because you really, really want programming skills, then go ahead. I foresee that most of the coding you'll do will be visualisation, since a lot of the math you'll be learning will be solved analytically rather than numerically. I think that the most efficient way to do things would be to just to learn math and programming concurrently rather than trying to mesh them together when you're at a stage where this isn't going to be natural or easy. ",1530429281.0
jx4713,"If you are choosing from most mainstream programming languages, in my opinion, it does not really matter. They say that Haskell is a more mathematically oriented programming language, but I did not necessarily feel as though this was true in my limited experience of Haskell. Take that with a pinch of salt, however. 

In my humble opinion, I would suggest that C++ would be a good place to start for one who wishes to develop a strong foundation in programming and also computer science. Learning to program well in C++ will surely expose you to a multitude of low-level and high-level features alike. You will gain experience of working in C and in assembly, you will need to know how the compilation process works: object files, executables, linking, loading, and basic computer architecture. This baggage is absolutely necessary in order to become proficient in c++, unlike many other languages such as Python, Haskell, Java (to an extent), etc... There really is no hiding. C++ also provides a vast array of standard containers which will expose you to algorithms and data structures. You will also see a variety of programming paradigms, for example, when you encounter C code, object oriented C++ code, or modern 'functional' C++11.

That said, I will tell you what will help you: competitive programming. The one thing I wish I had been exposed to from a young age. If you enjoy this, you will absorb everything you need to know about elementary algorithms and data structures. Along the way, you will hone your programming and implementation skills and develop strong problem solving skills. ",1530445623.0
combinatorylogic,"Start with a CAS. Mathematica, if you can afford it or can tolerate running it on a Raspberry Pi, or Maxima, or Axiom. For learning purposes they're pretty much equivalent.",1530527632.0
lifeisrecursive,"Julia. its used for a lot for numerical computing and heavily math centric. if you're looking for something that ties well into math concepts, that will be a good companion language.

lisp and scheme is also is built around math syntax and is pretty simplistic. so you could start there. lisp is a weird one though. but it has a lot of AI applications.",1530417015.0
juuular,Scheme.,1530422205.0
pseudocrypt,"Scheme, definitely.",1530424036.0
ktaylora,R and Octave -- especially for probability and statistical programing. ,1530424639.0
TheRubiksGamer,I would say python due to its simplicity,1530416799.0
feralwhippet,"the relevant question is ""pure math"" or ""applied math"". given op's description I would tend to think they are talking more about applied math, in which case Haskell may not be too helpful. If you have a mathematical bent and are interested in pure math then yes, Haskell (or Idris or Agda or any number of other functional languages with a close relationship to category theory) is the way to go. If you are going to stick to calculus/real analysis/etc... then I would go with Python, because it is very beginner friendly and NumPy/SciPy offers some great supporting libraries for various sorts of calculations.",1530442657.0
upandrunning,"Also you might consider something like jupyter notebook. It allows you to create structured documents containing code blocks that you can invoke whenever you want, and supports kernels for several languages. It's a great tool for learning and then for later reference.",1530443433.0
SippieCup,Scheme or lisp.,1530455823.0
phoresy,Lisp,1530456714.0
mashleys,"Something to consider is first start with a more math based language (I know a lot of people have suggested Haskell) for the purpose of better understanding math.  But if you really want to learn coding too, you should really learn Python and/or Java once you get the basics of the math based language down.  I would strongly recommend for the purposes you have explained (neuroscience and psychology) that once you get a good foundation in math laid down you should learn Machine Learning (I would recommend using R because there’s great open source libraries)",1530459677.0
MyRobotDidIt,"Of the commonly used languages, I would recommend C.",1530462280.0
crabbone,"Well, unless you are exceptionally lucky, and your teaching staff are knowledgeable about programming (we are talking about a fraction of a percent here), the language they will expect you to write your code in will be Matlab.  Doesn't matter that it is a proprietary language (although there's Octave, your teaching stuff will not know how to run Octave code in case there is a even a smallest, easily fixed difference between the two).  Doesn't matter that it is one of the worst designs for the language based on all the wrong assumptions (eg. that scientists will not need to write full programs and therefore you don't need any tools for that).

So, for practical reasons, just ask your professor / TA about the language they will want you to submit your assignments in, and learn that.

Most popular programming languages today are only marginally better than Matlab (badly designed, badly implemented).  We just happened to live in this special historical episode when demand for programming grows very quickly, so that humanity as a whole cannot keep up with it.  Like anything new and attractive it sprouted a lot of charlatans, people wanting to make a buck on the deficit, general confusion and mysticism.  Right now, there isn't any objective metric by which you could measure and compare languages, nor their usefulness for any particular task: the best we have to go on is the expert's opinion.  But, once you have >99% of ""experts"" with less than 5 years of experience...",1530598930.0
pico_12,"I'd suggest a high level language, preferably Python, yes the abstruction helps avoid a lot of baggage associated with low level languages like C and it's likes,but the learning curve is much less steeper. Personally I code in at least 5 languages but think in python
My first language.",1530620057.0
MsJenX,Spanish,1530422949.0
darkwing42,matlab because arrays start at 1,1530418036.0
StEvUgnIn,Go Scala. It's java inspired and it become a standard language for theoretical computation. ,1530524183.0
3rw4n,"Unrelated note:

Caution, at some point an example message is ""FINNISHISALLGREEKTOGERMANS"". 

My brain being a joker parsed that as ""Finish all  greeks or germans"". Hopefully this disclaimer will spare you a few seconds of unnecessary outrage and confusion.",1530394660.0
grawfin,Thank you for sharing this!!!,1530463523.0
corruptbytes,"took his class, p smart dude. not the best speaker",1530307585.0
Geologist2010,I had an older introduction to computation and programming using python I would've liked to donate. ,1530289147.0
QSCFE,">my crappy eyes stare at that damn screen enough as it is  

Probably eye strain from computer screen made you dislike eBooks but did you tried Amazon’s Kindle.",1530532306.0
Alignant,"Sicp and Htdp are free.

Do you need anything else?",1530306031.0
g051051,Having trouble how?  Explain what you're trying and your reasoning.,1530292382.0
krtkgla,"I think a = 1/2 will make it fall in case 3 of Master Theorem.
a = 1/2 makes f(n) = n

log3{2} is about 0.6

So, f(n) = omega( n^(log3{2}) )
&
2.f(n/3) <= (1-€)f(n) holds true for 0 <=€ <= 0.4

This will make your T(n) = theta(n)
",1530296444.0
_solidus,"I know it's probably difficult since the Master Theorem is confusing at first, since you haven't said what you've tried and what confuses you I can just offer you this little presentation by a professor from my university: http://cse.unl.edu/~choueiry/S06-235/files/MasterTheorem.pdf

I've used that presentation a lot whenever I had to work with the Master Theorem.",1530311206.0
tulip_bro,"## Microcosms in Theoretical CS/ Computer Science
I read an intro to a book for compilers, and in it the author made a great note that compilers is a _microcosm_ of CS. E.g., to write one, you'll need to understand:

1. Graph/ lattice theory for writing useful static analyses.
2. Grammars and automate theory for the front-end work.
3. Parsing theory.

It's beautiful in the sense that it combines so many areas of math and CS to produce such a prevalent and integral tool in the software world.

### Other microcosms?
What are some other areas that produce common and core tools in the software industry that can be viewed as a _microcosm_ of CS ideas? Some possible examples off the top of my head (although I'm out of my depths) I'd love to here people's thoughts on are operating systems, blockchain, databases, and maybe even web development.
 If this is popular enough I may even create a thread.",1530466614.0
NomNom150,What is the point of stable sorts? Like why use merge when you can just use heap.,1530268321.0
Obamendes,"Hi guys. Do you know of any conference that allows virtual presentation, as it's often expensive for me to travel abroad?

The few ones I've found have already closed submission or are not indexed by any major publisher (IEEE, Elsevier, ACM, Springer, etc...)

EDIT: With a Computer Vision/Image Processing scope",1530535865.0
greg_d128,"What? No assembly? PL/SQL that was nearly impossible to debug? 

Damn. I am old. At least I missed punch cards, but have definitely heard stories. 
",1530215196.0
flekkzo,"https://en.wikipedia.org/wiki/Malbolge

*drops mic*",1530215382.0
sailorcire,"If I don't get free bubble wrap and plates to smash, then I'm not joining",1530213659.0
Lj101,"/u/wilhelmtell is correct in some ways, you're taking a community and making a subcommunity from it. This usually always results in a pretty dead subreddit, it's a better idea to just post your health stuff to the other main subreddits. Look at any city subreddit that has a subcommunity, they're never active since you're just splitting the audience further.",1530260379.0
rotharius,"Nice initiative, perhaps you can collab with OSMI? See: https://osmihelp.org/",1530264917.0
motleyblondie,"This is absolutely a great idea. Thank you for creating this sub! 

There are far too many burnt out IT folks who need this. ",1530277302.0
realFoobanana,"I look forward to seeing how this goes (I just subscribed), and hope it is successful for you and your other subscribers! :) ",1530234839.0
Beefin,"Love it, just subbed. ",1530223924.0
Beastrik,r/nootropics,1530295571.0
,"To overcome a problem one must first understand it's cause..
simply : your reasons to eat unhealthily are much stronger than your reasons to eat healthily..
we all want comfort and ""happiness"".. eating unhealthily is easier therefore more comfortable for you..
let's pretend you switch EVERY FOOD in your house with healthier food, then your reason to eat healthily would be a little stronger, most likely not strong enough, and it's up to you to make it less great to eat unhealthily..",1530309721.0
Alucard_the_sinner,"Curious, just yesterday r/DevMind was created, with the intention of creating a subreddit dedicated to menthal health on programmers. Perhaps both subs could join in one",1530292614.0
Sparklingcobweb,What is... health?,1530303709.0
thinkerjuice,It already says everyone else is also welcome,1530258116.0
,Deleted?,1530212128.0
VinnR,"    @article{Keshav:2007:RP:1273445.1273458,
     author = {Keshav, S.},
     title = {How to Read a Paper},
     journal = {SIGCOMM Comput. Commun. Rev.},
     issue_date = {July 2007},
     volume = {37},
     number = {3},
     month = jul,
     year = {2007},
     issn = {0146-4833},
     pages = {83--84},
     numpages = {2},
     url = {http://doi.acm.org/10.1145/1273445.1273458},
     doi = {10.1145/1273445.1273458},
     acmid = {1273458},
     publisher = {ACM},
     address = {New York, NY, USA},
     keywords = {hints, paper, reading},
    }

Edit: Gotta get those Bibtex references",1530140847.0
averma7,A paper on how to read a paper. GENIUS!,1530136938.0
hsfrey,"Most of the first pass for me involves finding the places where idiosyncratic abbreviations are defined, and marking them in red, so I can find them quickly when they're referenced pages later.",1530148139.0
Truth_Seeker55,I wish I had read this paper at the beginning of my undergraduate study. Such a great approach.,1530140007.0
PsychoticallyAmiable,Is there a paper on writing a paper?,1530148501.0
GuyOnTheInterweb,"OK, I've skimmed through the ""How to read a paper"" paper, now I will have to read it again.",1530181936.0
krishamehta,"I had written a short review on this paper a while back

“1. How To Read a Paper.” https://medium.com/@krishamehta/1-how-to-read-a-paper-a8e8417301f6

",1530189418.0
anherali,Funny thing is when I opened it I was like I gotta read all this but it passed and I've read it. Thank you ,1530135381.0
dwhite21787,I expected this to be a broadsheet folding problem.,1530200109.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1530105026.0
__-_-_-_-_-_-_-_-_-,"Google conducts Code Jam every year just to give $10,000 to [tourist](https://en.wikipedia.org/wiki/Gennady_Korotkevich). He is the reigning champion for the past four years.",1530110170.0
dasdull,"For competitive programming in general I can really recommend the book Competitive Programming 3 by Halim & Halim. I became a much better competitive programmer by working through that book. It has tons of great code snippets and also serves as a catalogue for UVA online judge problems with hints on how to approach them.

> Ideally speaking, Python is the best language though there are a few things that can be done better in Java. 

Python might be fine for Google Code Jam because of less strict time limits but the go to language for competitive programming is and will always be C++. This is important because if you want to prepare for Code Jam by doing competitions on other sites such as Codeforces, you will often be forced to use C++ for raw speed. I think it's a good idea to get proficient in C++ anyway because speed will never hurt you in programming competitions.



",1530117328.0
jzoller0,Don’t hold it during my girlfriend’s Birthday!,1530111117.0
ImaginationGeek,"Blown to Bits
by Abelson

CODE: the Hidden Language of Computer Hardware and Software
by Petzold",1530126833.0
xTouny,"* Algorithms to Live by, The computer science of human decisions by Christian and Griffiths 
* Life 3.0 Being Human in The Age of Artificial Intelligence by Max Tegmark
* How to Create a Mind, The Secret of Human Thought Revealed by Ray Kurzweil",1530185201.0
geoffbezos,Principles of programming languages,1530291409.0
OneDickToRuleThemAll,"One of my favorites is The Art of Computer Programming is a by Donald Knuth

And of course my favorite subfield of AI: Natural Language Processing. I'll include


Speech and Language Processing by Daniel Jurafsky and James H. Martin


Foundations of Statistical Natural Language Processing by Christopher Manning
",1530295461.0
lifeisrecursive,How To Design Programs. Available for free,1530516417.0
whiteorb,"W3schools is not considered the best source. It’s remained behind spec and riddled with inaccurate info for many years.

MDN is the best resource for HTML, CSS, and JS and is used as Mozilla’s core source and maintained as such.

If you reference W3schools on Stack Exchange, with senior devs, or in an interview you’ll likely get corrected or scolded.",1530077395.0
Tacticus,Well for html and css Mozillas documentation is vastly better than W3Schools. https://developer.mozilla.org/en-US/docs/Web,1530076163.0
naisooleobeanis,The linux man pages,1530494025.0
xTouny,[C++ Resource Network](https://www.cplusplus.com/),1530090875.0
,"u/flexibeast kinda late comment, but i loved the article. do you know of any similar ones? i've been trying to learn more about neural nets and this article was a great start ",1530328422.0
zokier,"APL and Iverson's essay [""Notation as a Tool of Thought""](
http://www.jsoftware.com/papers/tot.htm) comes to mind.

And then there is of course Lojban and variations of it that seem also relevant. Browsing Wikipedia bit further leads to this promising article https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning

This is and has been interesting topic academically, but I don't think it has really borne any useful fruit yet.",1530102004.0
Meguli,"Please dear someone, point us to all sorts of good reading on this topic. ",1530077140.0
bzBetty,https://medium.com/iotforall/the-difference-between-artificial-intelligence-machine-learning-and-deep-learning-3aa67bff5991,1530094783.0
WhackAMoleE,"I read the article and did not see any meaningful distinction between fake versus real AI. Having followed the AI hype from the 1960's to the present, it all looks like fake AI to me. We have very clever techniques for data mining, but no idea how to go about building artificial general intelligence. The article doesn't even bother to make the distinction. ",1530052288.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1530023576.0
chinmay289,"Count me in. I'd love to watch something like this. Such videos would need to help viewers in improving the intuition of these concepts. Like 3Blue1Brown did with his Linear Algebra series, among his other video series. Just being able to map these ideas to a piece of popular system in the real world, software/social/otherwise,  would go a long way with students.",1530024194.0
corruptbytes,"Honestly, there are so many YouTube series, it's more important what you will do to separate yourself

what I love from YouTube videos: 

1. Little to no code, if any code, should be ONLY pseudocode.

2. Beautiful diagrams with clear transitions e.g. an explanation of BFS should definitely include animations of the entire traversal

3. Clear, concise audio

Personally, I really love the MIT videos, but if it had 2 instead of the professor on the board, would be nicer IMO.",1530066534.0
xTouny,"Could you show students the beauty of theoretical computer science and what distinguishes it from other sciences including pure mathematics? How could the fundamental study of problems and their classification awe and inspire scientists to devote their whole lives pursuing improbable succeeding theories? How could such an abstract subject be enjoyable and fun?

It is rare to find books alike ""A Brief History of Time"" or ""The Selfish Gene"" in Theoretical Computer Science, So, For me, This is exactly the gap needed to be fulfilled and showed to science enthusiasts, and even the general public.

It is worth to mention popular science books like ""Quantum Computing Since Democritus by Scott Aaronson"", ""The Golden Ticket by Lance Fortnow"" and ""Algorithmic Adventures from knowledge to magic by Hromkovic"" found [here](http://www.mediafire.com/?ix66gnu85n3mc). A Formal Salient Book to be added to the list is [The Nature of Computation by Moore and Mertens](http://www.mediafire.com/file/krudh7e6ffardmb/The%20Nature%20of%20Computation%20by%20Cristopher%20Moore%2C%20Stephan%20Mertens.pdf).",1530039100.0
Khiv_,"I would be interested in these topics. There are a few graph theory and discrete math videos out there but they aren't that good. I have watched MIT's mathematics for computer science but those videos are just recorded lectures.

 I would prefer shorter videos with visualizations and analogies that made the topic more palpable, like 3b1b. ",1530057062.0
Winstonin2,I would watch ,1530029157.0
LeCountDeMoney,All in!,1530036043.0
Crazypete3,"Yeah I'd watch it, there are tons of video on how to program but just not enough on theoretical concepts and other things like these. ",1530042142.0
tulip_bro,"Absolutely! I think TCS is missing mainstream exposure to some of the ideas we may find _fascinating_. It wasn't until I learned about concepts like the Halting Problem, or Curry-Howard, until my love of CS was sparked at a deeper level.

I think the Computerphile youtube is doing a great job right now, but the more the merrier.",1530067505.0
,[deleted],1530033826.0
uncleXjemima,"This was my least favorite class I’ve ever taken, so for that reason I’m out ",1530043216.0
MusicologicalHater,I’d watch! I’m just beginning my journey of  a computer science degree and i think it would be helpful for me,1530048794.0
rotharius,"Just do it. Seriously, start making videos. Begin small. Once you get the hang of it expand. It does not matter if it has been done before; there is an audience for knowledge presented in your own way.",1530055283.0
CottonCaandy,Sounds like a great idea :),1530063417.0
TheDarkLord_22,"They already exist - 
**MIT LECTURES BY MR. ERIC** ",1530085337.0
Zulban,">it may seem like I'm throwing my hat into an already crowded pool.

Yes. Sorry but I can't join in on the blind enthusiasm of this thread. Intro compsci is probably the most saturated course ever for free online content. Good content too. People here saying ""I would watch"" are lying to you, straight up.

My question to you is... which intro CS courses have you watched some of, and what are your problems with them?

If you want to make content, make something special and unique. Promote your best skills which you're most passionate about. Don't do the thousandth video on data structures.",1530067477.0
simondvt,Absolutely yes!,1530042924.0
angelol90,I would,1530047573.0
Analog-Digital,Yes please!!,1530049330.0
quant271,Yes,1530051322.0
fangfufu,count me in as well.,1530053760.0
rizx7,Yes please! ,1530055055.0
Cocomorph,"I was thinking of doing this exact thing myself, so you have my vote.",1530055816.0
LearnerPermit,have you considered seeing if you can join the khan academy program?,1530057359.0
franku-san,Very into the idea,1530057740.0
saffa51917,Me me me,1530058668.0
Rolexx,I’d watch too,1530061580.0
Hellbasedgod,This is something I've been wanting for a long time!,1530064046.0
cntrlnmbr1,I'm interested. ,1530065581.0
yagya_senixx,me me me me me,1530066366.0
,I,1530066816.0
TheExplorativeBadger,"I think this would be a great idea. Nothing better than teaching to learn the material in even more depth yourself! I would recommend taking a particular care in planning the progression of these videos. A seamless ""how to get started"" video on all things programming fundamentals will go a long way, but nothing worse that a confusing sequence of ""how to"" videos. Good luck! Send a link when you produce the first.",1530068401.0
Jared1109,100% would watch,1530071015.0
sarcasmasaservice,"I would appreciate this very, very much.

!RemindMe 2 days. ",1530084469.0
Vampyrez,"Would add Chomsky hierarchy, semantics, abstract interpretation, functional programming (I know this one is a bit different) to list of potential topics",1530092372.0
zielu,I would love to see something like this. Please inform us if it will kick off. ,1530101774.0
Zophike1,"> Discrete math + applications (sets, relations, etc.)
> Probability theory + applications (probabilistic method, Lovasz local lemma, etc.)
> Graph theory
> Finite automata
> Computability + intro complexity theory

One thing I would like to see if possible is something like [this](https://plforums.org/) but for the wider TCS community, also I would like to  a series dedicated to the following topics:

- Quantum Information Theory 
   - [Classical and Quantum Computation By Alexei Yu. Kitaev, Alexander Shen, Mikhail N. Vyalyi, M. N. Vyalyi](https://books.google.com/books?id=08vZYhafYEAC&printsec=frontcover&dq=mathematical+quantum+information+theory&hl=en&sa=X&ved=0ahUKEwjIj8Wo2PbbAhVp64MKHchaBN0Q6AEIZTAJ#v=onepage&q=mathematical%20quantum%20information%20theory&f=false)
   - [From Classical to Quantum Shannon Theory by Mark M. Wilde](https://arxiv.org/pdf/1106.1445.pdf)
   - [Post-Quantum Cryptography
edited by Daniel J. Bernstein, Johannes Buchmann, Erik Dahmen](https://books.google.com/books?id=VB598lO47NAC&printsec=frontcover&dq=arxiv+quantum+cryptography+book&hl=en&sa=X&ved=0ahUKEwiQwriU4_bbAhUH5IMKHdW-Cl4Q6AEISDAF#v=onepage&q&f=false)

- Mathematical Logic:
    - [Mathematical Logic: Foundations for Information Science
By Wei Li](https://books.google.com/books?id=_ZQ9BQAAQBAJ&printsec=frontcover&dq=Mathematical+logic+foundations+for+information+science&hl=en&sa=X&ved=0ahUKEwip-NmavPbbAhUC94MKHfI5BuEQ6AEILDAA#v=onepage&q=Mathematical%20logic%20foundations%20for%20information%20science&f=false) 
   - [Logic for Computer Scientists
By Uwe Schöning](https://books.google.com/books?id=7KxgQjJaL78C&printsec=frontcover&source=gbs_atb#v=onepage&q&f=false) 

- Algebra
 - [Basic Abstract Algebra: For Graduate Students and Advanced Undergraduates
Robert B. Ash](https://books.google.com/books?id=5m7CAgAAQBAJ&dq=Abstract+algebra+category+theory&source=gbs_navlinks_s)
 - [Linear Algebra in Action: Second Edition by Harry Dym](https://books.google.com/books?id=TRKDAgAAQBAJ&printsec=frontcover&source=gbs_atb#v=onepage&q&f=false)

 - [A Graduate Course in Applied Cryptography](https://crypto.stanford.edu/~dabo/cryptobook/BonehShoup_0_4.pdf)

- Programming Language Theory 
 - [Principles of Program Analysis By Flemming Nielson, Hanne R. Nielson, Chris Hankin](https://books.google.com/books?id=RLjt0xSj8DcC&printsec=frontcover&dq=Principles+of+Program+Analysis&hl=en&sa=X&ved=0ahUKEwjF49rw0fbbAhXl34MKHS3ABOYQ6AEILDAA#v=onepage&q=Principles%20of%20Program%20Analysis&f=false)
 - [Semantics with Applications: An Appetizer
By Hanne Riis Nielson, Flemming Nielson](https://books.google.com/books?id=oPi0yERDUeYC&printsec=frontcover&dq=Semantics+with+Applications:+An+Appetizer&hl=en&sa=X&ved=0ahUKEwjehNCE0fbbAhVL2oMKHSmyCJ0Q6AEIKTAA#v=onepage&q=Semantics%20with%20Applications%3A%20An%20Appetizer&f=false)

 - [Practical Foundations for Programming Languages by Robert Harper](https://books.google.com/books?id=9sshAwAAQBAJ&dq=Type+theory&source=gbs_navlinks_s)",1530198394.0
bartturner,"Would love it but even better a podcast even though sometimes tough without visual.

Drives me nuts how little there is with podcasts today and a hassle to make YT videos audio.",1530386542.0
AmatureProgrammer,"Its been 4 months. Curious if you decided to make a channel? 



",1541193324.0
bailey36217,do they charge money to teach it in a college? ...then yes,1530068933.0
theofficialdeavmi,Harry Porter.,1530087905.0
,Pls,1530051513.0
wookiecontrol,No one ever,1530074725.0
Kel-nage,"I'm not aware of any R-to-JavaScript conversion implementation, especially with regards to R's wide plotting functionality.

If you want to do it on the cheap, the best way would be to generate the plots on a server in R, and display them on the website. If you want to do it as your Dev has suggested in JS, you could take a look at [d3.js](https://d3js.org/) and the wide ecosystem of libraries around it - but that's going to take a lot of work/money to implement properly. I'm not sure that your Dev has justified that cost though!

P.S. This kind of question isn't really suitable for /r/compsci - I'd have said it would better to ask it on /r/programming or /r/javascript - but I doubt they'll answer that differently to me.",1530019039.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/programming] [The JSON datatype in SQL can totally make your life easier when it comes to certain scenarios. I wrote a blog post about it and would love to find out your views about it. • r\/compsci](https://www.reddit.com/r/programming/comments/8tzohm/the_json_datatype_in_sql_can_totally_make_your/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1530016999.0
MaunaLoona,I don't see the difference between an EAV table and storing it in JSON. You have the key/value pairs either way. I just don't see whatever it is I'm supposed to be seeing when looking at the image of EAV table compared to its JSON representation. ,1530025066.0
ProgramTheWorld,"If you don’t like EAV tables and prefer to store everything as key value pairs, maybe you shouldn’t use a relational database at all and just go for NoSQL. A huge concept in traditional relational database is that all values  are primitive values such that one “cell” should not contain multiple pieces of data. This violates one of the most fundamental principle in the entire traditional model, and doesn’t offer any benefits over modern NoSQL databases.",1530016600.0
MaunaLoona,This means the process of evolution is doing computation on geological time scales.,1530015206.0
MaunaLoona,"This is why I don't think technological singularity likely to happen. Technological advances hide behind computational complexity. If the computational complexity needed for further technological advancement is higher than the gain from previous advancement, technological advancement will take an exponentially longer and longer amount of time, even if we have ""computers building computers building computers.""",1530015159.0
,"Can someone please tl;dr? I’ve written evolutionary algorithms before, but the jargon has me confused (I’m not a cpu scientist)",1530019604.0
PsychoticallyAmiable,Seems to me evolution sidesteps this with sociology ,1530034759.0
DeluxeMarbles,Thinkpad W530,1529968214.0
norse_dog,"Six core samsung chromebook plus. I seriously would recommend before considering anything else - it's fantastic for web browsing, battery life and keyboard, dirt cheap, runs linux and any compute load that will require more power should run in a cloud instance anyway. ",1529968420.0
maranathaman,Lenovo Yoga 720. ,1529969233.0
Stanian,"I got a Dell XPS 15 9560, really happy with it so far. I run it dual-booted with windows and ubuntu for entertainment and work respectively. The GPU helps a lot with machine learning work/experiments.",1529963819.0
mexgirlmindy,"HP notebook 15-ay103dx

2.5 GHz i5
8 GB ram
1 TB

I bought it on sell from Best Buy right before I started college. ",1529963897.0
xanderdantes,"MacBook Pro 15” w/ Touchbar, Radeon 550, 256 GB",1529964723.0
krtkgla,Looks like r/explainlikeimthree tbh,1530011942.0
Free_Math_Tutoring,"> As a single computer has only eight CPUs

Uhm... oddly specific.

Weird article. Written in almost Wikipedia Simple English™ and giving an overview so broad I can't imagine anyone who get's any use from it.",1529955332.0
KondaxDesign,"This doesn't really seem specific to Google at all, but a very high level explanation of map reduce. The only reference to Google is that they distribute it over multiple systems.",1529962337.0
davidlowellbyrne,"Seriously tho, I don't know why map reduce boggled my mind. This really made it clear.  Thank you. ",1529966119.0
CreativeCoconut,"It is as much an art as programming is. But is programming an art?
UI Design is just a field you can study and work in as much as programming and engineering and the rest of it is. The reality is, you cannot do it without training. 

I mean, it is also kind of insulting to all the people whose job consists of designing stuff. I don't mean that you are insulting them, just that the way a lot of programmers of look at this part of application design, kind of is. 

Look at it the other way around. Imagine someone is a really good at taking pictures and wants something like Instagram. He kinda knows a bit of Python but is not proficient. What would you tell him if he asked you ""how can I make a social network for photographers""?. 
The two answers are, hire someone who knows how to do it or dive into the field more and study. You don't need a degree to make good design, only to make it great, but without any education in the field you won't get far by just trying stuff out randomly. There is a reason why you can study design and people spend their lives researching design ideas.  
At least look for a simple course on UX design or get a book which covers UI design. 

As an analogy, how hard can it be to design a door? Well have [look](https://www.youtube.com/watch?v=yY96hTb8WgI&feature=youtu.be) ;-)",1529953578.0
chaotic_david,"""How do I create the perfect front end?""   

Hire an interaction designer to design it for you.",1529950919.0
wejdan912,"I think the best way is to understand your user, Who will want to use my app? How will they use it? When will they use it? Should they be able to use it with just one free hand? Is my target market older or younger?  If older, does this mean I should make elements larger to make them easier on those with poorer vision?  Dig down into usability questions from the beginning.  ",1529950993.0
NsfwOlive,yea,1529995008.0
CorrSurfer,"Everything in programming and also in computer science is part (1) art, part (2) craft, and part (3) science.

Learning an art is difficult, so you normally focus on learning (2) and (3) and hope that with spending enough time, you develop a feeling for the art part as well. ",1530005020.0
Slukaj,"1) This is /r/compsci, not /r/programming. It took me about five minutes to realize you're talking about Auto Hot-Key. 

2) This is /r/compsci, not /r/programming. The sub specializes in theoretical computer science and conceptual problems, not specific programming questions. Wanna know how to sort a deck of cards as fast as possible? We can help.

3) Why are you trying to do this? ",1529948551.0
Mechakoopa,"Off topic here, but I'll start you off by saying that you have no context in which to run AHK until the user has been logged in. Maybe rethink why this needs to be changed in a way that requires user input prior to login. You don't really want to be giving anyone access to machine resources or doing anything that requires elevated permissions like changing IP addresses prior to authenticating.",1529948497.0
AThousandTimesThis,"You have to create the GUI using Visual Basic, not AHK. Be careful, though: [methods exist](https://www.youtube.com/watch?v=hkDD03yeLnU) to determine the original IP.",1529948583.0
Meguli,"To me, if you want to see topics discussed in that book in more detail, the book Computation Structures is very good. It is not a sequel though. ",1529953283.0
JeffSolin,"I don’t know the answer, but I’ve been teaching this curriculum for over a decade and I love it. I’ve supplemented a lot, and modified a bit for honors HS students, but I love it. I dream of someone rebuilding the entire platform as a web app or even native OS versions. Although the current apps work pretty well, there are frustrating issues and usability quirks. An update, especially one that could run in a browser, would greatly impact accessibility, equity and exposure. I realize it might be hard to sandbox something like that in a browser, but it seems possible if taken on by the right people. 

I also wish the people that posted their solutions on GitHub would take them down. ",1529987781.0
rodrigomlp,"There is a great book, pretty standard in CS curriculums, called ""Computer System's: a programmer's perspective"".  In my opinion it was an amazing follow up to Nand2Tetris since there is a lot of overlap but you also get to see the topics in  greater depth, learn C pretty well and also get your foot in more advanced topics such as OS and Networking. All lectures are also available online at [http://www.cs.cmu.edu/\~./213/schedule.html](http://www.cs.cmu.edu/~./213/schedule.html)",1530004549.0
nighthawk648,"What is your goal here? 

Do you want to create physical machines to facilitate the creation of a quantum sim or quantum computer?

Or do you want to learn the engineering maths to try And formulate some postulates that can give way into how quantum information and mechanics truly work and reconcile with relativity? 

Or lastly, use developed machines to implement quantum mechanics and field theory to compute data on a software and abstract level? Tied to above but closer to SWE then science. Just trying to understand. ",1529933863.0
Zophike1,/u/flexibeast can you ELIU plz :>),1529981370.0
RatherPleasent,"Checkout out chegg, you'll probably have to pay a hot $14 a month if you really want to keep using it.

Or course hero if you feel like reading other student's syllabuses.",1529894122.0
PastyPilgrim,"It would take 3 seconds. We have those divisions of stages because they often have dependencies on a prior stage. Like, you can't decode an instruction that you don't have and you can't execute an instruction that you haven't decoded. Though there's all kinds of techniques, special hardware, etc. to mitigate some of those things.

The clock speed is how long it takes for all of the ""dust"" to settle and you can't guarantee that your results are available until the clock has ticked, which means you can't run stages with dependencies.

That said, modern computers aren't serial like this. We have things like [pipelining](https://i.imgur.com/Plfa7kp.jpg) because, for instructions that don't conflict (e.g. use the same register), you can execute different stages at the same time. So, for example, you can send $R1 and $R2 to the ALU during execute while, at the same time, you're fetching another instruction to execute.

When pipelining is being used (and not blocking during conflicts) you can do *a* Fetch, Decode, and Execute stage in one clock cycle, it's just that they won't be F,D,X cycles for the same instruction.",1529875588.0
experts_never_lie,"""hert"" is a [deer](https://en.wiktionary.org/wiki/hert), like ""hart"", and not a singular form of [Heinrich Rudolf Hertz](https://en.wikipedia.org/wiki/Hertz).

",1529877191.0
phire,"It really depends on the architecture.

On an older CPU like the Zilog Z80 (found in some early 8 bit micros, like the TRS-80 and the ZX Spectrum), instructions take between 4 and 23 cycles to complete, depending on the complexity of the instructions which works out to 4-23 seconds at 1hz. 

It's contemporary the MOS 6502 (found in the NES, C64 and Apple 2) has a more optimised design and can complete it's fastest instructions in 3 cycles, with it's longest instructions taking 8 cycles to complete.  
Computers with the 6502 were faster at most operations than computers with an z80 running at the same clock speed.

When you get to pipelined CPUs like the 5 stage MIPS I, every cycle it would execute the fetch for instruction N, the decode for instruction N+1, the execute for instruction N+2, the memory access for instruction N+3 and the writeback for instruction N+4.... all in parallel.  
At 1Hz, It would take 5 seconds for a single instruction to execute, but once it was going, every second one instruction would enter the pipeline and one instruction would finish. This makes it average one instruction every second. 

Though, the MIPS can't run and full speed all the time. If an instruction accesses memory that's not in the L1 cache, then the whole pipeline is paused for 10-20 seconds while this memory access takes place. Other things like exceptions and branch miss-predictions can cause the pipeline to flush and start over at the first stage.

Later on, you get superscalar CPUs like the Intel Pentium, where the pipeline can peek at speeds of two instructions per cycle. If instructions N and N+1 meet certain conditions (like N+1 doesn't depend on instruction N and instruction N+1 is one of the more simpler instructions), then they both go through the execute stage in the same cycle.

At 1Hz, the Pentium can peek at speeds of two instructions per second.

Finally you get to modern out-of-order CPUs. Intel Skylake actually operates on a sliding window of upto 224 instructions. Every cycle, upto 5 new instructions can be added to this sliding window. The CPU identifies dependencies between instructions. Every cycle, it locates upto 8 instruction which have their dependencies met and executes them (these instructions will not be in program flow order, it just fines 8 random instructions from it's 224 instruction window that are ready and executes them, ""out of order"").
Finally, it checks the status the upto 4 instructions at the end of it's sliding window and ""retires"" them. This forces any side effects to be applied in the correct order.

A Skylake CPU clocked at 1Hz can peek at speeds of 8 instructions per second in the middle of its pipeline, but other parts of it's pipeline limit it to averages more like 3.5-4 instructions per cycle over good code.
For code that misses cache and accesses main memory, you might see delays of 100+ seconds.


",1529922112.0
spacegirlmcmillan,"Hertz is not plural, it's someone's name. It's always Hz no matter how many there are.",1529900564.0
dryadzero,"There is no correct answer to this question. The concept of a cpu has nothing to do with implementation. Modern CPUs would likely pipeline these kinds of operations, so maybe one second? maybe this is the only instruction between two branches, and it takes as many cycles as the the pipeline (why not 200 seconds for my nonspecified cpu?).  Maybe you're on an older cpu that has non pipelined instructions that take multiple cycles.

What if the instruction isn't in the instruction cache? maybe it takes 2 minutes to get it from memory. Maybe the virtual memory address maps to disk space, how long does it take to get it off the disk? How about if the instruction is on a CDROM that's dirty, and the CD reader has to read it multiple times to read it correctly?

If the question is, ""on x86, how long is the instruction pipeline?"", then the answer depends on the architecture.

[https://softwareengineering.stackexchange.com/questions/210818/how-long-is-a-typical-modern-microprocessor-pipeline](https://softwareengineering.stackexchange.com/questions/210818/how-long-is-a-typical-modern-microprocessor-pipeline)

Edit: as a followup, consider the 6502 processor, found in the Atari, NES, Apple 2, etc.

The fetch-decode-execute varies wildly based on the instruction. Some instructions are 2 cycles, some are up to 6, some lengths even vary depending on the address being referenced. [http://www.6502.org/tutorials/6502opcodes.html](http://www.6502.org/tutorials/6502opcodes.html)",1529910053.0
bart2019,"It would run in n seconds, where n is the number of clock cycles the instruction needs.

In modern architectures, different phases of the execution pipeline for multiple instructions will likely run in parallel, so you can't just add them up.",1529923886.0
Madsy9,"With a clock speed of 1 hz, the A CPU would spend one second for every pipeline stage. This assumes a superscalar CPU. So with an 11-stage pipeline, the CPU would spend 11 seconds on each instruction from the first fetch stage to the last writeback stage. This 'roundtrip' time is called latency.

But this is the general idea and of course it's more complicated than that. You can have pipeline stalls and whatnot. And while the latency increases with the number of pipeline stages, throughput increases up to a certain point. And throughput is really what we are after when designing fast CPUs.",1529898062.0
moschles,"It would be far slower than that.   The hert is just the clock speed.  The ""Fetch""  would depend on the speed of the RAM and the QPI, and there are cache considerations.",1529874693.0
taejo,You CAN use Rice's Theorem here.,1529862269.0
zetsubou-tan,You should probably ask this in r/Math or somewhere else where the logicians gather,1529862226.0
TwistedStack,"In general, I'd say nothing is hard to learn if you learn how to learn. It just becomes a matter of time and experience.

I hate how other people would categorize skills into ""difficulty"" levels when I was growing up. It just adds an unnecessary psychological barrier to just spending time to learn new skills.",1529855790.0
BoobDetective,"Programming is the easiest part of Computer Science. You will definitely be able learn that. Regarding ""how hard"" something is, it is relative. Instead of worrying about hardness, just do it, and see if you enjoy it.",1529855309.0
remy_porter,"> How hard is it to learn?

I'm going to ignore the specifics of your question and reply to your title: it's very hard.

Learning is hard. Learning anything- programming, juggling, an instrument, how to be open and honest with those close to you, how to write an essay- *is hard*. It takes discipline, dilligence, dedication, and probably a number of other words which don't start with ""d-"".

The question you're really asking is: can I learn programming (and perhaps broader aspects of computer science)? The simple answer is: yes, of course you can. You just have to put in the work. As Bob Ross was fond of saying: ""Talent is just a pursued interest"". Pursue your interest, and one of two things will happen: you'll get *more* interested, pursue it harder, and get more ""talented"" as a result, or you'll exhaust your interest. Turns out, whatever you were pursuing isn't for you. And that's okay! That's great! You still learned some stuff- even if you learned nothing beyond, ""no, I don't really like that,"" that's still valuable.

Learning is hard. You will fail. In fact, failure is the only way to learn. You don't learn very much at all from your successes.",1529876594.0
midwayfair,"As most people have said, programming is only one tiny part of CS, and it really is sometimes the easiest part (though sometimes implementing really simple ideas can be challenging -- I can think of a few algorithms that I could probably explain even to a child in minutes that took me several hours to program correctly).

>What are some skills you think I need to make it easier on me?

No one really seems to be answering this, so I'll give it a shot.

>I'm a beginner and I'm scared that I'm just not smart enough to learn programming.

Can you give instructions to someone that they can follow? Then you know the basics. Computers are dumber than people in most ways, so most of computer programming is figuring out how to give really precise instructions. On the other hand, most people think they can give good instructions, and then they go to program a computer and find out that they aren't as good at it as they thought.

One skill you can use for this is to plan ahead with something called whiteboarding (because, well, it usually involves a whiteboard and markers. You want to be able to erase easily). Write out your instructions, then follow them literally. Pick a really simple task. Be as objective and simple-minded as possible. This is worth practicing even for people who already have done a lot of programming. You'll not just practice the execution of programs but you'll start thinking algorithmically, which will be immensely helpful when you start studying algorithms.

>In general how hard is it to learn?

Some people pick it up very easily. Some people only get it through great struggle. Some people seem to be unable to shake themselves into the mindset required. I read a theory once that sometimes it comes down to how accepting of absurdity people are -- that you're willing to just follow a rule and see where it leads, which is also a pure math ""skill,"" and not coincidentally some people never seem to be able to shake themselves into the mindset required for writing proofs in math.

Another skill you'll want is some level of mathematical maturity. Most things in computer science come down to math, and to me at least it's super interesting math. Types of algebra you don't learn in high school (linear algebra and modern algebra), graph theory (a wonderfully intuitive and visual type of math that you can get a lot out of with [nothing beyond basic algebra](https://www.amazon.com/Introduction-Graph-Theory-Dover-Mathematics/dp/0486678709)), and discrete math (thinking of it as the study of things you can count will be fine) all give a bunch of tools that make programming seem easier.

A final skill I would say is valuable is tenacity, or stubbornness. How willing are you to hammer away at a problem without looking up the answer? Are you willing to spend eight hours working on what someone told you is a ""simple"" problem without even asking for a hint? I once took close to a year to fix a memory leak issue in an assignment for CS50 (which you should check out, it's an awesome course) before finally asking for help. It turned out to be that I forgot to check for one type of character that could appear in a word. I fixed it by adding about four characters to a file that was a couple hundred lines of code. I didn't solve it on my own, but along the way, I learned nearly everything I think I could have possibly learned about every part of that code and developed a bunch of ways to improve troubleshooting. I just missed a real-world piece of information.

Like most things in life, how smart you are is less important than how much effort you're willing to dedicate.",1529863451.0
baryluk,"The MOST important skill is experimentation and not being scared of failure. It is computer, you are not going to break anything if things go wrong. PS. Software developer for 20 years.",1529875857.0
afiefh,"Programming can be as simple or as complex as you make it. You could simply write out the logical steps to carry out a task with little to no abstraction. This works, but becomes hard to maintain and debug after a while, at this point you'll reach for abstractions like design pattern and oop. As long as you remember that these are tools in your arsenal for the purpose of more concisely expressing the logical steps you will be fine.

A CS degree will focus very little time on programming. I had only 3 programming courses in my 8 semesters degree. Most people I know finished their degree knowing how to program but definitely not being good at it (including me obviously). That's because the purpose of CS is to teach you the science of computing (hence the name) not how to translate that science into code. An analogy I once heard is a physicist trying to build a bridge, sure he knows all the relevant physics and can figure out the different parts, but without a couple of years in the bridge building to get acquainted with best practices, material issues due to factory behaviors, worker sloppiness...etc he will be very inefficient at it.",1529902116.0
kiw1berd,As long as you like to problem solve and are good at googling (which sounds silly but is an actual skill) you’ll have no problem learning! Just remember it’s okay to look things up and bugs/code issues are just part of the process. ,1529862930.0
Rancerle,"I consider myself a beginner, and I'm writing code for IBM at a summer internship!  This is a field where the basics will get you pretty damn far if you learn how to apply them when all you've got is an assignment, a function library, and a blank C file.

Keep learning, study like you mean it, and hustle hard getting your resume out there when you feel ready to begin applying to jobs.  

Also, developing a wide range of language proficiencies will help a ton as well, and be plenty of fun when you can start using your UX lessons to design your resume loaded up with projects that highlight your abilities with crypto.",1529902518.0
SargeantBubbles,"Nobody is smart enough to learn programming. Every programmer you know at one point was too dumb to learn to program, and did it anyway. My main advice for you is to code a lot, and make actual things with your code (make a 2D scroller game as a project, instead of just writing down some code that looks the same but really does nothing). Next part is that you WILL fuck up, a lot. Don’t get discouraged. Every genius programmer you’ve ever known is only a genius programmer because they rolled with the punches. Next part is learn from your mistakes quickly. You will inevitably make mistakes, but learning from those mistakes will keep you from making them 10 more times. God speed, and remember that the internet is your friend.",1529912193.0
GNULinuxProgrammer,"I mean 50 years ago this question would have merit but nowadays we programming languages specifically optimized for ease of programming, so learning programming is usually pretty easy. As other answers said programming is by far the simplest and tiniest part of CS.",1529937329.0
theFiggofTruth,"I believe ""logic"" is what you need to be a good programmer. What I mean by that is, the ability to take a design and break it down into logical steps for the computer to follow. It ends up being like a puzzle that the computer is gonna solve.

The more you can interconnect the steps, the higher tier the logic. So the faster the computer solves the puzzle.

I know programmers who know all the syntax etc etc but they can't program anything without a tutorial or a google tab open.

To develop logic it's just practice. There's nothing to be afraid of other than giving up.",1529865843.0
etherealyn,"Not hard at all. All you need to do is to like the subject.

Some tips that will make your life easier (in the long term):

1) learn how to use command line interface 

2) learn how to debug

3) learn how to debug code without a debugger, i.e. in your mind (use paper and pen if the code is too complex)

4) during your studies try not to copy and paste important parts, but read carefully and write code from scratch. This alone will dramatically increase your coding skills. If that does not help, it means that either the code is awful OR you're not experienced enough to understand it.

Of course you need to look up language features if you don't remember them 

5) focus on algorithms and data structures, in a nut shell, computer science is all about information transformation

6) be pragmatic ",1529875088.0
khedoros,"> What are some skills you think I need to make it easier on me? 

Determination, patience, and willingness to be wrong 100 times in a row before you get it right, and maybe only understanding *why* it was right much later.  


The biggest chunk of work is learning useful patterns for basic things, so that you can think higher-level about how to put the pieces together.",1529880606.0
Askee123,If you’re interested in it you’ll be motivated enough to push through your weaknesses. It’s all about how much work you put into it,1529891019.0
shuerpiola,"> I'm just not smart enough to learn programming

Pssh. Programming is easy as hell.

Here is [EdX] (https://www.edx.org/course/programming-basics) Programming 1 course. Its how I got started and what gave me my first steps into earning my Computer Science degree. Give it a shot; you have nothing to lose.",1529904423.0
kdnbfkm,If you are learning on your own sometimes it will be gradual and sometimes punctuated equilibrium when something finally clicks.,1529915585.0
wilber-guy,"Take a big long look at the world, and realize that every single situation in it has an algorithmic rhythm, that once you began to see. All of programming becomes easier. Also discrete math helps a ton! Never give up, realize there is no such thing as ""smart"" and keep using the internet as your learning resource. ",1529869663.0
dmcdem,"Get an old dell optiplex, load ubuntu on it, start learning, screw it up, reload ubuntu! ",1529869666.0
teawreckshero,"> Computer science is no more about computers than astronomy is about telescopes.

""Programming"" is just a means of getting a machine to carry out the mathematical operations you want it to. At the end of the day, CS is math, not operating a computer. If you can think mathematically, then you can think programmatically. It's all just logic.",1529875958.0
zezozio,"How far have we travelled, eh!

This is foundational work that we know take so much for granted that most of today's youngsters have forgotten that our elders had to fight tooth and nails to get rid of the goto...",1529855039.0
Triple_Elation,Chapter IV: Dealing with the Velociraptors,1529857339.0
combinatorylogic,"Another thing to read on `goto` is http://www.literateprogramming.com/adventure.pdf

",1529920307.0
andrewcooke,i like the event indicators.  i wonder why they didn't catch on?  too specific / narrow i guess?,1529868324.0
TomSwirly,"I completely disagree with where you are at.  Students who do the last step naturally instead of a pointless recursion call are showing their intuitive understanding of how it works.

More, this is a natural optimization if you were actually implementing this algorithm, and one you see in a lot of code - where when you get down to a fairly small number of elements, they directly sort them, rather than have another recursion call.

I remember being a bright high school student doing math and (early) computers.  It drove me nuts having to dumb down my work for some of the teachers - for example, even though I relentlessly showed my work, I still got dinged by a teacher for skipping the step `1x = x` (and this was some years into algebra, too).

You want to teach kids to be _good_ at the material and make it their own.  Forcing them to regurgitate memorized material and being intolerant of minor _correct_ variations is not doing them a favor.  It means they waste a considerable amount of brainpower, not trying to get the _right_ answer, but to get the specific answer that you are expecting.",1529843017.0
TomvdZ,"> Is it wrong?

What do you mean by *wrong*? Did they parrot the thing you told them in class? No. Is it a reasonable way of doing mergesort? Yes.

The only reasonable argument I can think of for counting this as wrong is that it increases the number of base cases the algorithm has to consider. The following algorithm is incorrect:

    MergeSort1(Array A, left, right)
        if(right - left == 2) magically sort A[left ... right -1] and return
        MergeSort1(A, left, (left + right) / 2)
        MergeSort1(A, (left + right) / 2, right)
        [do the merge step]

because it doesn't work when the length of the array isn't a power of two. A possible fix would be

    MergeSort2(Array A, left, right)
        if(right - left == 1) return (array of length 1 is already sorted)
        if(right - left == 2) magically sort A[left ... right -1] and return
        MergeSort2(A, left, (left + right) / 2)
        MergeSort2(A, (left + right) / 2, right)
        [do the merge step]

and *this* would correspond to the algorithm the students demonstrated on your test (actually, you can't distinguish whether they used this algorithm or the former incorrect one but that's because your test happened to use an array of length 16). However, this algorithm can be simplified to the following:

    MergeSort3(Array A, left, right)
        if(right - left == 1) return (array of length 1 is already sorted)
        MergeSort3(A, left, (left + right) / 2)
        MergeSort3(A, (left + right) / 2, right)
        [do the merge step]

and you could argue this is ""more correct"" because it is simpler and simpler is better (for some notion of ""better""). In particular, simpler is better because the simpler your code, the fewer mistakes you can make in it.

However, if you care about this distinction, you should explicitly teach it. But it is more about a general principle of algorithm design (try to keep the number of special cases low) than it is about knowing ""THE CORRECT"" version of mergesort (and the principles of divide and conquer are much more important than petty details like what you consider your base case).

Note that wikipedia defines divide and conquer as:

> ... recursively breaking down a problem into two or more sub-problems of the same or related type, until these become simple enough to be solved directly.

and it is indeed very simple to sort a list of length 2 (it takes just a single comparison and a swap). The ""algorithm"" that your students demonstrated fits the definition of divide and conquer.",1529831216.0
Cowlegend,"You said that a lot of students stopped at 2 and then sorted by magic before continuing to merge as normal. I just wanted to say that I would hope if your students are learning how to do mergesort, then they should all know how to sort a list of two numbers, and it seems very strange you would want them to explain how they did something so trivial in their answer (tests have limited time!).

The main idea with mergesort is using divide and conquer to solve a problem, so I think going down to a size of 2 shows they understand how it works (the remainder is just an implementation detail), but it might still be worth explaining in class how usually it is taught as being split down to lists of size 1. I would give them full marks for this answer.",1529843313.0
bremby,"Going all the way down to lists of length 1 is a detail. As long as they understand the divide and conquer part of longer lists, then it's fine. In actual implementations you could see mergesort stopping at lists of some length (e.g. 16 values) and then using a different alg to sort those. The reason is that the second alg might be able to use registers and caches more efficiently, thus running faster.",1529829127.0
dantuba,"In practice, a merge sort implementation would only go down to some small size, then resort to some **other** sorting algorithm (maybe insertion sort) for the small lists, before merging back up to the entire sorted list.

The key here is that there has to be some other sorting algorithm involved, i.e., the ""magic"" you are referring to in your question. Granted, the way to sort a list of length 2 is pretty trivial!

The only way to involve no other logic than merging and splitting, is to recurse all the way down to lists of length 1 (or 0). So I would say if your students only recurse down to length-two lists, they're not *wrong*, but they have failed to specify something about how exactly the algorithm is working.

Of course, you are the teacher and know the context and your students best, and grading is just a carrot/stick mechanism to encourage further learning, so do whatever you feel is best. I think this entire discussion would be a great topic to bring up in your class!",1529836991.0
SirClueless,"I think this is a natural thing for students to do, especially if they're not intimately familiar with recursive algorithms yet. Stopping at a base case of size 1 is a natural thing to do while implementing this algorithm on a computer, because it results in fewer edge cases and you'll have to handle odd-sized lists anyways. Stopping at a base case of size 2 is a natural thing for a human to do because who would bother with a sorting algorithm when you have a list of size 2 and it's obvious what to do.

I don't think there's any resources out there teaching the wrong thing, it's just that the natural human way of thinking about this leads to this magical ""sort the list of size 2 the obvious way."" From a pedagogical standpoint, you might want to change the example list from size 16 to size 15 or something, so that it's clear students need to do something about strange-sized lists and teaches them that leaf nodes of size 1 are natural and inevitable and can be handled with merge sort without any sort of magic.",1529845021.0
hextree,"It is a completely valid way to do it. As long as the base case is constant sized, it is a correct application of the divide and conquer paradigm (which doesn't specify how small the base case has to be, it just ought to be constant size). And the runtime complexity is equivalent.

The problem of course, is they might forget to also handle the base case of size 1; this base case can still happen if you are splitting a list of odd size. So if they forget to handle this then they are of course wrong.",1529845720.0
ioquatix,"Consider the base case. Does the algorithm the student propose work with empty list? list with one item?

While there are many variants of merge sort, a pure merge sort must have a base case of size 0 or 1. If you tried to merge two lists with more than one item which is not in order, the result will be incorrect.

I would give some marks for understanding divide and conquer, but I'd also retain some marks for students who understand base case correctly.",1529841643.0
baryluk,"Going down to two is ok. In practice most algorithms stops around 20, and switches to insertion or selection sort apart anyway. Yes formally you should go down to 1 and merge from there. But merging two number is equivalent to sorting in place really. Sure, going down to one when actually implementing in paeudocode is better, because it makes algorithm simpler and makes it more robust to edge conditions i.e. uneen lists. The 99% of tricking and concept is in merging bigger lists.",1529837719.0
cruise02,"""...then magically sorting this list...""

This is the part that bothers me a little, and a lot of the answers aren't addressing. If they're not showing *how* they do that sort (with a simple compare and swap), then I think it's fine to take a point off. If they do show it, then they should get full credit.",1529852333.0
McChubby007,There are plenty of visualisations online showing this sort.,1529828281.0
maladat,">Suppose I have a CNF problem with set V variables
>with at least 1 clause
>with each clause having at most 3 literals

Good so far... you have described 3SAT, which is NP-Complete.

>with each clause have distinct variables

Here's the problem. This is a restricted case of 3SAT, and is no longer NP-Complete. In fact, it is solvable in linear time, and reduction from a general 3SAT problem may take exponential time.

>Suppose I have a DNF problem with set V variables
>with at least 1 clause
>with each clause having at most 3 literals

Again, good so far, at least assuming this problem is Co-NP-Complete (off the top of my head, I can't remember, but it doesn't matter, because...)

>with each clause have distinct variables

Same problem here - this is a restricted case of 3DNF-UNSAT that again, is no longer Co-NP-Complete (if, in fact, it was to begin with - not sure) and can be solved in linear time. 


What you have done here is shown a reduction between two easy P problems. Because P is a subset of (NP intersect Co-NP), you can, of course, describe one as an NP problem and the other as a Co-NP problem, but to prove NP = Co-NP, you need to show a reduction using an NP-Complete or Co-NP-Complete problem. 

My recollection is that there's a proof that NP is a subset of Co-NP if and only if Co-NP is a subset of NP - in other words, either they're equal or neither is a subset of the other - but I may be misremembering.",1529783731.0
timmyotc,No.,1529783205.0
Woken12245,Can someone tldr,1529784126.0
BuxOrbiter,"Hi, I took some time to read over both implementations.

It’s excellent that you’re asking these sorts of questions, a lot of other people would just move on.

The quick answer is that your implementation performs up to twice as many comparisons in the worst case. The other implementation takes advantage a key property of bubble sort: After every ith pass through the array the i last elements are guaranteed to be in the correct position. Therefore, after i passes the optimal algorithm only needs to sort the array starting from 0 up to n - i elements.

Precisely, the other algorithm does (n-1)+ (n - 2) + ... + 1 = n * (n - 1) / 2 comparisons. Whereas yours may perform up to n*(n-1) comparisons. 

Take for example the array: 3, 2, 1


Their algorithm:

    3 > 2 swap
    3 > 1 swap
    2 > 1 swap
    Halt

Your algorithm:

    2 < 3 swap
    1 < 3 swap
    1 < 2 swap
    3 < 2 do nothing
    2 < 1 do nothing
    3 < 2 do nothing
    Halt
",1529861244.0
timmyotc,"Take the case 
5, 2, 3, 1

Your algorithm will switch 5 all the to the end of the list, then start from the beginning of the list to continue searching for smallest elements, instead of working its way through the list.",1529777718.0
,[deleted],1529778277.0
GuiltyAsChargee,"There are two optimizations in bubblesort.

1. After every iteration, the last element is guaranteed to be in the right places. Therefore , you don't need to check it.

2. Break out of the loop if there is no element in the wrong order. I.e if the list is sorted, break.

Here is my C++ code:

Bool check = false;
Int k=1;
Do{
Check = false;
For(int i = 0; i < n- k ; i ++)
{
  If (v[I] > v[I+1]) swap
  Check = true;
}
K++;
While(check == true)",1530222275.0
aboak,"It's because your version only compares each adjacent position in n, as opposed to comparing each position to EVERY other position after it in the list. This means on each iteration of your loop, you make only one comparison instead of comparing a given index to every index after it.",1529777712.0
quasiperson,"You can just delete the first if statement all together. If `n==0`, i starts out at 0, then `i<n` will evaluate to `0<0` which is false. The contents of the for loop will never be executed and result will be returned, which will be 0.",1529763611.0
315Lukas,"> switch, but that doesn't count

A for-loop contains an if-statement, why does that one count then?",1529765032.0
LagrangePt,"In general, you can replace any if statement with some simple math.

Instead of

    if (boolFunc())
        return func1();
    else
        return func2();

You can do

    int foo = (int)boolFunc();
    return func1() * foo + func2() * (1 - foo);

It's not usually a good idea in most cases, since it executes both sides of the loop, but it does get rid of the if statement.  I've only ever seen it done in glsl, where branches are often more extensive than just doing both calculations.
",1529765446.0
ezubaric,"The OP specified ""iterative"", but one can of course write it using generating functions:

`>>> from math import sqrt`  
`>>> f = sqrt(5)`  
`>>> def fib(n):`  
`...     return int((((1 + f)/2)**n - ((1 - f)/2)**n)/f)`  
`...`  
`>>> fib(5)`  
`5`  
`>>> fib(10)`  
`55`  
`>>> fib(16)`  
`987`",1529771757.0
jmite,"It turns out you can write Fibonacci without any if-statements, using  Lambda Calculus and Church-numerals. Functions and function calls are all you ever need!",1529779991.0
poundSound,Have you tested it? if `n==0` then `i==n` so the for loop won't execute thus returning 0. You shouldn't need the if.,1529763675.0
umop_aplsdn,"> Self-posts and Q&A threads are welcome, but we prefer high quality posts focused directly on graduate level CS material. We discourage most posts about introductory material, how to study CS, or about careers. For those topics, please consider one of the subreddits in the sidebar instead.

>> *but we prefer high quality posts focused directly on graduate level CS material. We discourage most posts about introductory material*

>>> **high quality posts**
>>>> **graduate level CS material**",1529794718.0
Chiralmaera,"I would add a minor scoping change:

    private static int iterativeFibonacci(int n) 
    {
        int result = 0;

        for (int i = 0, prev = 0, prevPrev = 1; i < n; i++) 
        {
            result = prev + prevPrev;
            prevPrev = prev;
            prev = result;
        }

        return result;
    }",1529801253.0
JGordonOfficial,"You can use the solved recurrence relation for the nth one, (phi^n - (1-phi)^n)/sqrt(5).",1529771485.0
petecasso0619,"Here is a C++ recursive solution with no 'if' checks. The Fibonacci values are computed at compile time, NOT at run time due to the way the template instantiation mechanism works. This isn't what I'd call good code. If you try to instantiate the fib template with a negative value, your compiler may run out of resources (try it and see).  

```C++
#include <iostream>

template<int N>
constexpr int fib() {
	return fib<N-1>()+fib<N-2>();
}

template<>
constexpr int fib<0>() { return 0; }

template<>
constexpr int fib<1>() { return 1; }

int main() {
	constexpr int fib_n[] = {
        fib<0>(),fib<1>(),fib<2>(),fib<3>(),fib<4>(),fib<5>(),
        fib<6>(),fib<7>(),fib<8>(),fib<9>(),fib<10>(),fib<11>()
    };
    int i=0;
    for (auto v : fib_n) {
        std::cout << ""i= "" << i << "" Fibonacci value = "" << v << std::endl;
        ++i;
    }
}```
",1529797263.0
The_JSQuareD,"Are you familiar with the matrix power method to calculate Fibonacci numbers? That's my favorite method. It has complexity O(log n), and doesn't suffer from the floating point rounding problems of the closed form method. It's also much more general, as this method can be used to solve any recurrence relation of a similar form.

There's a bunch of different solutions implemented [here](https://www.geeksforgeeks.org/program-for-nth-fibonacci-number/) ",1529779413.0
TomvdZ,"> Of course you could use a switch, but that doesn't count.

You could replace if with for. Does *that* count?",1529763644.0
Call_Me_Your_Daddy,"The way you wrote the for-loop, you could, but you wouldn’t get the intended answer. You could remove the if, check if i <= n in the for-loop, but your result = prev + 1, so it would break on the first iteration, but your result would be 1.",1529763707.0
colorado777,Do ternary operators count? ,1529764465.0
Kah-Neth,Now compute them without using a loop,1529776842.0
kdnbfkm,"It is possible to improve efficiency by not doing a branch, doing both alternatives and letting God sort it out later, but only in special cases...

Even when these things are *possible* if the concept is alternative than somehow has to be included (i.e. exploiting absolute value, or bit operations, or compliment subtraction or indexing within range or integer division thresholds or whatever).

The runtime of a branch has to be slower than doing the other ways too and it obscures the logic also... So this doesn't come up all the time. But it can.

hehe Not fast but:

    /* N -> { 0, 1 }; */
    N = ??? // 0 <= N <= (MAXINT-1), actually negative N -> { 0 } also
    tmp = (N + 1) / 2; N = tmp;
    tmp = (N + 1) / 2; N = tmp;
    tmp = (N + 1) / 2; N = tmp; // repeat log2(MAXINT) times...

So... to test `x = (a < b) ? y : z;` you could maybe:

    int diff = (b - a);
    int tmp = (diff + 1) / 2; diff = tmp; // repeat log2 times...
    int x = diff*y + (1-diff)*z; // thanks to /u/LagrangePt

Okay just one more! `int eq = 1 - gt(a-b) - gt(b-a);` Now all you need is `void* goto_labels[2];` and you're Turing complete and less confusing than BrainF*ck!",1529787714.0
wolverine_23,Use while loop,1529800368.0
,You and every first year university student out there.,1529804062.0
funk_monk,"Are we counting loops as if-statements in disguise or not?

If yes then your code contains two if-statements. If not then replace any if statements with loops that conditionally cycle once and you have a zero if-statement piece of code.",1529810884.0
FUZxxl,"Yes.  You can use Binet's formula:

    static int fib(int n) {
        double phi1 = 1.618033988749894903; // 1 + Math.sqrt(5) / 2
        double phi2 = -0.618033988749894903; // 1 - Math.sqrt(5) / 2
        double scale = 0.447213595499957928 // 1 / Math.sqrt(5)

        return (scale * (Math.pow(phi1, n) + Math.pow(phi2, n)));
    }

I'm not exactly sure about the syntax, haven't done Java in a long time.  The logic is correct though.",1529837060.0
,"If you're serious about machine learning, take calc 3. If you just want to take the one class but plan to work in a largely unrelated area, I wouldn't worry about it. Departments are usually realistic about which prerequisites are necessary, so the ML class will probably be pitched at a lower level than normal to accommodate the gaps in the students' maths background. ",1529729490.0
Andy_Reds,"If you want to go into HCI, taking Calc 3 is not worth it. You can still apply ML without knowing what is happening under the hood. However, if you decide to study Machine Learning seriously, say, as a PhD topic, Calc 3 is absolutely essential.",1529756817.0
minimim,"Completely true, and always important to think about when designing languages (if you're into that).

It gets *very* interesting when one gets into expressiveness, irregular forms or context.
",1529717371.0
varno2,One thing worth noting is that the similarity between modern programming languages and natural languages is by design. Much of the design of programming languages comes from the work of chompsky (among other things a linguist) and his studies of language acquisition in the 60s and 70s.,1529755622.0
trichotillofobia,"I say bollocks: once you're fluent, higher levels of expression no longer come from deeper understanding of the language, but from better concepts that apply to other languages as well. Programming languages also don't have pragmatics in the linguistic sense. Furthermore, natural language doesn't fit in a syntax tree very well. You'll need all kinds of links to make sense of many constraints. And a natural language has a huge vocabulary, in contrast to the intentional poverty of a programming language.

Linguistics tries to understand the system behind natural languages, but programming languages have been designed. The system is known.",1529748006.0
codeveloper,"Nice! 
I find the syntax tree very unintuitive though. The Genetic Programming approach seems to be simpler to understand:

Leaf nodes are terminals eg. Variables, constants, functions

Other nodes are operations eg. NOT, +, *, while

[Example (Wikipedia)](https://upload.wikimedia.org/wikipedia/commons/7/77/Genetic_Program_Tree.png)",1529746776.0
mycall,"> “The ant ran over the car.” .... The statement is syntactically correct but semantically wrong – we can’t conceive a meaning from it.

OP doesn't have my parking spot.",1529765442.0
dwhite21787,"Two remarks:

Please make 100 “fo”

You want to represent a concept of an amount, such as “fifteen” but you’re merely replacing the 1111 symbols with “ay-fo-tu-un” .  If anything, hexadecimal has a head start on your scheme, at least in IT. You’ve got to get people to stop conceptualizing Arabic amounts if you want this to catch on, and that’s a steep hill to climb. ",1529666532.0
virbinarus,"There's a system called location arithmetic, which is similar but not phonetic. 

https://en.wikipedia.org/wiki/Location_arithmetic

This is a nice idea for how to say it though",1529699903.0
dwhite21787,"here, go nuts

https://xlinux.nist.gov/dads/?",1529670743.0
een-ze-nood,"For my data structures and advanced algorithms class, our big project was to create a B-tree. It was a lot of work. I would suggest looking into those just in case (B-tree and B+ tree are different btw). 

That’s awesome that you’re giving yourself a head start! It’ll pay off in the long run. CompSci is very time consuming, so every bit of work you do ahead of time will help. ",1529667201.0
krtkgla,"Would be really helpful if you start practicing problems/concepts based on dynamic programming paradigm. They are often the most meaty set of problems in a coding interview. 

Also, just brush up on STL (if you haven't) so you can save time while writing code.  Goodluck!",1530012954.0
KatsuCurryCutlet,"Shouldn't it be slightly more accurate to add the word ""efficiently"" since this is with regards to complexity rather than computability?",1529655484.0
Wurstinator,"This is all really misleading, the article makes it seem like BQP contains problems that are not recursive. In fact, the result is that BQP contains a problem that is not part of the polynomial hierarchy.",1529655540.0
dualmindblade,"It sounds like they only proved the separation relative to an oracle, am I missing something?",1529674651.0
Acandis,"Reading the comments here I'm left to wonder: is it known if there are (classically) uncomputable problems that a quantum computer could compute?

The article was worded in such a way that it seemed to suggest this new paper was answering this question, but I understand the difference now.",1529697334.0
celerym,Ah the problem of how to get more quantum computing research funded.,1529719719.0
rhysbans,[https://arxiv.org/abs/1411.5729](https://arxiv.org/abs/1411.5729),1529720251.0
monorex97,did they even read the first chapter of mike and  ike?,1529693906.0
jayerp,The Faro Plague,1529665897.0
DevFRus,"I have a question for the mods (or others in the know). Are links to wordpress blogs auto-spammed, while links to medium blogs aren't? If so, why is this policy in place? There are a few good programming medium blogs, but also a lot of trash. While on wordpress, there are a lot of good theory and scientist blogs (and also a lot of trash). See for example the catalog of blogs on cstheory: [What CS blogs should everyone read?](https://cstheory.stackexchange.com/q/22191/1037) Although maybe that is a bit dated, now.",1529641312.0
ringraham,"How do I get better at determining time complexity? We covered it in my data structures class, but I really struggled with it. ",1529954554.0
,"The Vogelstein brothers at Johns Hopkins do a great deal along these lines. There's Leslie Valiant at Harvard, Surya Ganguli and Dan Yamins at Stanford, Josh Tenenbaum and Tomaso Poggio at MIT, and a whole host of people in the Computational Neural Systems groups at CalTech and CMU. Might be worth taking a look at some of their recent papers and perusing the citations. ",1529627484.0
anemonk,"Not sure about general sources for papers, but you may find work from these guys interesting:
https://scholar.google.com/citations?user=JNXWWkIAAAAJ&hl=en
https://scholar.google.com/citations?user=nSZG-vcAAAAJ&hl=en
https://scholar.google.com/citations?user=67aYFTQAAAAJ&hl=en

",1529639785.0
Andy_Reds,"I know that the Blue Brain team are working on applying computational topology to neuroscience, if you want to look into that. 2 examples to get you started is ""Cliques of Neurons Bound into Cavities Provide a Missing Link between Structure and Function"", published in the Frontiers of Computational Neuroscience journal, and ""Supporting Information: A topological representation of branching neuronal morphologies"". 
",1529656683.0
olliej,"This is nonsense - it is basically using the same arguments as used by perpetual motion machines.

The basic argument is that some operations are ""reversible"", and from there makes the exciting leap that if it is reversible then if you reverse the operation you get all the energy back. The example they use (seriously) is pushing a ball up a ramp and then down the other side. They try to hide the loss of energy by saying that it may have to run slowly, which is the same argument people use when they claim perpetual motion of flywheels.

They go so far as to say a ""perfect fredrik gate"", which i think should be translated into sensical english as ""idealised fredrik gate"".",1529614690.0
fanmanutd11,This is really cool. Prolog is the main language I use at my job. It is very different from any other languages I have used. ,1529592406.0
thecodeboss,"Hi all, OP here. I recently gave a talk at a local user group about Declarative Programming with Prolog, and I decided to blog about it as well. Declarative programming is a huge paradigm shift for those of us who are used to imperative languages like Java, C, C#, Ruby, Python - and probably most of the languages you use. Instead of you telling your program explicitly what you want it to do, you just define what result you want, and you let the program figure out the best way to arrive at that result! Prolog is a perfect example of a declarative language that is simple enough to understand what’s going on, but complex enough to where you can do some real-world things with it (it’s often used in natural language processing, actually). If that sounds interesting, I encourage you to check it out . This is a 3 part blog series, with the remaining 2 parts being released 1 month apart each.",1529587425.0
rincewind123,I strongly recommend this book for prolog https://www.amazon.com/Programming-Artificial-Intelligence-International-Computer/dp/0321417461,1529611106.0
EasyMedium,"This is awesome. It's over a decade since I did anything in Prolog, but I still have fond memories of the language. Great write-up, I look forward to the following blog posts! :) ",1529591464.0
semicc,"One reason I enjoy using a declarative rules engine like Drools is for the benefits to approach you see with Prolog. If you want to deploy declarative, Java based, rule evaluation APIs in a more approachable environment then I’d check it out. ",1529603083.0
nhays89,"Got a little confused with your input on the second example..you mentioned X = food brand, but you have ""ice_cream"" as X. Is it normal in prolog to create facts of the same type, but have totally different associations?",1529618030.0
Lucretia9,"Had. “Programming in logic” course at uni. Was”taught” by the head of dept who had a rep for hating undergrads, was Italian with a thick accent, barely understandable. Only barely managed the final project, don’t think many passed including me. First book I got rid of out of uni, next to the FP book, similar experience with the FP course.",1529627352.0
hsfrey,"How difficult would it be to implement prolog-like programming in, say, perl?

Prolog looks so magical merely because all the loops and searches that run it don't show on the surface.",1529628710.0
Singularity42,It seems to me like modern graph databases might be used to solve some of the same use cases,1529657210.0
nebbly,"I don't really see the point of this. I guess he's saying integration tests specifically can be mitigated with monitoring. OK, I mean, sure. But testing (especially unit testing) still results in faster development, so what's the point?",1529571436.0
frost-circle,Because manually running and checking 100 services on every single change is way better?,1529572593.0
LongUsername,"Why did you change the title to an inflammatory one for Reddit?

Gathering runtime metrics for sanity checks is a good practice. I've implemented such systems before and it can help field support figure out that something was wrong with a system before the support call comes in.

But as someone who worked in safety critical operations you would have been laughed out of the room (and sent for retraining/possibly fired) for suggesting pushing code without it going through a full V&V cycle.

Also, according to software safety theory there is no such thing as a ""random"" software failure. All software faults are Systematic.",1529581268.0
420Phase_It_Up,"This is a horrible idea. Unit testing, among other automated testing, is the most surefire way to ensure that your code is reliable and stable. I see developers defending not testing their code all the time and it usually boils down to either laziness, incompetence, or both.
",1529580492.0
combinatorylogic,"That's exceptionally stupid. Rare error conditions are rare, unless you test for regressions vs. known past cases.",1529579400.0
TheYellowCromulon,I wonder how good the quality remains when running this over and over again on the same video. Infinite slow mo,1529573857.0
audiodoct3r,This is why updates exist. Because something is never truly done until it dies...,1529571758.0
Ultralite_Beam,I got started by following project based tutorials on YouTube and then forking GitHub repos and building on top of them. ,1529536945.0
jutct,"Get Android Development Studio. They have tutorials and stuff right on the website. For crossplatform, take a look at Flutter. Seems to be pretty cool.",1529534419.0
ulrenx,"I initially learnt app dev for Android (Android Studios) via youtube videos, and started off with building the usual email/password login system since you'll learn how to set up & store/fetch data from your database too. 

I think [this link ](https://medium.com/mindorks/how-to-learn-android-development-f33dd6dba40d) from medium did a great job at explaining what to look out for when learning android app dev. 

Alternatives for learning app dev without android studios to a certain extent would be React Native. I'm not too sure about Xamarin as I personally have never used Xamarin before. Codes written in React Native is cross-platform deployable. Since you're starting out, use React Native + Expo, it will help you out q a bit as compared to Native Codes. 

You can drop me a message if you need help at any point in time! I'm sorry if my answer was messy, but feel free to clarify w me anytime :) ",1529563407.0
hydrauliquesimple,"exercising, the most you exercise, the most you become better, its like when you go to the gym, you don't expect to have a fantastic body by just going to the gym and take some selfies..",1529603452.0
,[deleted],1529534593.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1529529526.0
saulr,Yes - it's an Enterprise-only feature in Windows 10: [https://docs.microsoft.com/en-us/windows-hardware/customize/enterprise/shell-launcher](https://docs.microsoft.com/en-us/windows-hardware/customize/enterprise/shell-launcher),1529517277.0
StorsJT,"This is a super long read which I haven't finished but looks really interesting.
> Chasing elegance instead of simplicity. 

This is definitely something I am guilty of when writing a 16 method nested Linq query in C#.",1529566633.0
SOberhoff,">  Abdicating responsibility for data usually through automatic memory  management and/or virtual machines. Indicative of an insecurity in ones  own understanding of information representation and likely algebra. 

Wait, what? So if I program on the JVM I'm less likely to be able to recite the quadratic formula? What a bizarre claim.

Also, is this some kind of backhanded brag?

> I'm sorry that so many technologists seem well described as prostrating,  click bate worshiping, sycophants who couldn’t program FizzBang let  alone a 60+ fps, audio enhanced, texture free, physics sandbox used for  creating the most amazing user experiences. 

(""click bate"" is the author's spelling)",1529585370.0
computer_activist,It doesn't work that way. It's about the structure of problems not about unitary processor cycles. e.g. A problem of size n can take n^2 steps to solve. It doesn't matter how fast you can do a step but it will take n^2 steps. Similarly NP problems are problems that have a structure solvable by a non-destermintic turing machine taking n^c( where c is a constant) steps. and P is the same except for deterministic machines. It doesn't matter how fast they move it's about the patterns. The speed ups will always be relative so problems solvable with n^2 will always be much slower than n steps no matter how fast the computer moves.,1529457242.0
_--__,"P (and NP) aren't about speed, they are about **scalability**. An algorithm is in P if, when you increase the size of the input, the running time increases polynomially (e.g. if you double the input size, the running time takes, say, four times longer).  ""Exponential algorithms"" (which isn't NP) are ones where the running time doubles with only a constant increase in the input size (e.g. if you increase the input size by 3 the running time will double).  NP algorithms/problems are algorithms/problems for non-deterministic computers (very roughly, computers with unlimited parallelism) which scale polynomially. ",1529458433.0
TomvdZ,"For the reasons laid out by others it doesn't matter, but processing power will not continue to increase exponentially.",1529473297.0
gnupluswindows,"The programs do what they're designed to do. Will the people who create them ever want to create programs that help you realize your goals and ambitions rather than entice you to click things? Maybe, but that's more of an economics question than a computer science one, isn't it?",1529440847.0
ReverseEngineered,"Google Plus Music recommends to me other bands and new albums that are similar to what I already listen to. On the one hand, they are likely being paid for that advertising and it keeps me listening to their service longer. On the other hand, I want to find new and interesting music, so I'm happy to have those suggestions. Is it working for me or is it taking advantage of me?",1529455674.0
nawal86,"i think the only way is to run them on our own compute resources. potentially as a distributed cooperative system - e.g. share the load on characterising stuff, but individuals get to change the parameters near the output and recompute the recommendations. ",1529480470.0
anotherdonald,"These are the corporations brought us such great feats of humanity as the slave trade. I don't know if that should really make us feel more comfortable.

For the rest: what an insufferable piece.",1529504944.0
DevFRus,"This presentation is also [available as a YouTube video](https://www.youtube.com/watch?v=RmIgJ64z6Y4) but you have to have a high tolerance for lip smacks to watch it. Overall, though, I found this to be a very thought provoking talk.",1529404740.0
TombKrax,[https://svn.torproject.org/svn/projects/design-paper/tor-design.pdf](https://svn.torproject.org/svn/projects/design-paper/tor-design.pdf),1529408631.0
CompSocChris,">It is called onion routing because onions have layers and this networking protocol also has layers.

Like a parfait?",1529409901.0
NakedOldGuy,"First, your traffic enters the TOR entrance node.  Then it gets routed to a random redirect node operated by the NSA.  Repeat that a few times, as configured.  Then the traffic goes through a TOR exit node operated by the NSA and it passes through the plain ol' internet to your source for dark web porn/violence/guns/drugs/fraud.",1529409929.0
itsaworkalt,"This was a very excellent article, thank you for posting it!

Edit: I can see why this seems like spam or something but I seriously just enjoyed the article. I knew of Tor but had not idea how it worked so it was nice to get a top level rundown.",1529414613.0
jordanosman,"So just to be clear, the kid from Harvard got in trouble because they found the original node right cuz that's the one that has the senders info? How did they find the first node? Was it not a guard node?",1529427612.0
oantolin,I was hoping for a homological algebra article. This was interesting too.,1529411002.0
Spacekiwi,"False, both keys are the same (hence symmetric) and should both be kept private.",1529383321.0
khedoros,"It's already been answered, but: Hint: What does ""symmetric"" mean, and why would you have separate public+private keys?",1529383500.0
johanvts,"It is really the result of a lot of work, if you want to cite something I would take the quote from Turings 1939 PhD thesis, if you want to read about it , ""the universal computer"" by Martin Davis p.146-149.",1529383657.0
almart,[Is this what you are looking for?](https://www.cs.virginia.edu/~robins/Turing_Paper_1936.pdf),1529387771.0
Cocomorph,Are the references [here](https://en.wikipedia.org/wiki/Church%E2%80%93Turing_thesis#History) not what you're looking for?,1529383676.0
cuntinuum,I think you forgot to write the body of your post,1529353885.0
anamorphism,"if you're using mysql 5.6, sure.",1529356812.0
suffixaufnahme,Try /r/cscareerquestions,1529359770.0
BumbuuFanboy,"Making an interpreter is much, much easier than making a compiler. Just to give an idea of the difference in difficulty, me and a single partner once had to write an interpreter for a language with high level features like records, exceptions, and first order functions over the course of like two weeks, while implementing the basic functionality of a compiler for a language with only integers, booleans and arrays took a team of four people the majority of a semester to implement.

How you operate on an AST is heavily dependent on how that AST is constructed and what kinds of programming paradigms you are using. Your AST will likely be a very recursive data structure (expressions may be binary operations on other expressions, or perhaps a function name and a list of input expressions, etc.) and therefore thinking recursively about your AST is key to operating on it. 

The straightforward way to do this in an object oriented framework is to have each type of node correspond to an interface with the methods you would need. For instance the interface Expression may have a method evaluate that takes in a context that maps variable names to their current value and return some kind of Value interface. This is a bit of a rigid approach that would make it a bit more difficult to extend your AST with more functionality. For a more general approach in an OO framework, you may want to look into the visitor design pattern. That would allow you to create a new visitor for each new functionality and all of the new code would be localized to new visitor code rather than spread across your whole AST. The visitor design pattern is a royal pain in the ass to work with sometimes though.

If you are working in a language with pattern matching, like OCaml, Erlang, Rust etc., then you could accomplish the same things with functions that fold across the AST. This has the benefit, in my opinion, of being much more readable but can make it somewhat more difficult to add new nodes to your AST. ",1529350406.0
gct,"You have to define evaluation semantics for your language.  If it's a pure language functional language (like haskell) all you really need is function application.  You recursively apply that to the AST until you have your result.  If it's an impure language (we'll say Scheme to go to the other end of the spectrum), you have to define some notion of how to look up variable bindings (their environment) and all that.   Execution semantics can be very complicated, just look at the spec for C++.",1529358627.0
combinatorylogic,"Once you have an AST, you just [rewrite](https://en.wikipedia.org/wiki/Rewriting#Term_rewriting_systems) it all the way down to something you can easily execute (machine code, some virtual machine bytecode, source code in another language, etc.).",1529394245.0
miki151,"Once you have your AST, you create a function such as `interpret(Node node)` or `compile(Node node)` for every possible node in your tree. Then call it for the top node.

It's probably easier to interpret than compile. I'll give you a small example.

    // this class stores values of all current variables at a given moment during interpreting
    class Context {
        setValue(string name, object value) {
            // probably use some kind of map or dictionary to store the value
        }
        object evaluateExpression(expression) {
             // returns the value of the expression based on the current values of variables that occur in it
        }
        ...
    }

    // if you run into something like 'x = 5', this function will assign 5 to x in your context
    interpret(VariableAssignment assignment, Context context) {
        object value = context.evaluateExpression(assignment.expression);
        context.setValue(assignment.variableName, value);
    }

    // interprets a standard for loop like 'for (x = 0; x < 5; x=x + 1)'
    interpret(ForLoop loop, Context context) {
        interpret(loop.initialAssignment);
        while (1) {
            boolean condition = context.evaluateExpression(loop.condition);
            if (condition == false)
                break;
            interpret(loop.body);
            interpret(loop.stepAssigment);
        }
    }

",1529348489.0
kdnbfkm,"An AST is used to structure code-elements and analyze the semantics. It need not be used to run code directly, but it could be... (or perhaps transformed into a flow graph or bytecode or some other form) The result would probably resemble direct-interpretation of Lisp S-expressions (but have semantics of your language).",1529373210.0
iwantashinyunicorn,"Don't. The OU ""CS"" degree is not worth the paper it's printed on. The OU does many good degrees, but CS is not one of them.",1529341382.0
MrEvilPHD,"They're missing some critical information. Recalculating the hash wouldn't require you to recalculate every other hash, unless the previous hash is \*part\* of the new one. Can someone clear that up for me?

Also, I've heard a lot of hype around Block Chains, but wouldn't GDPR create a LOT more difficulties, making them have very few, if any, uses?",1529370971.0
duhace,"i can do it in less words:

it's dumb",1529362185.0
amself,"Use \[here](https://www.paypal.me/pools/c/83f5W4qZXG) instead of
[here](paypal.me/pools/c/83f5W4qZXG) ",1529325369.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1529294521.0
,"Aside from being an information sponge and learning everything you can, treat the internship like a temp-to-hire position. If you do well enough, you have the chance of being offered a job after graduation.  
  
If you treat your experience as only a training exercise or graduation requirement without displaying growth and the ability to learn efficiently, you may not show enough potential to your superiors to be offered a position. Work well with others, be quick to pick up processes, and show initiative by contributing well.   
  
Also consider building a network within the organization, learn all of your colleagues' names and establish them as contacts (assuming they like you and do not mind exchanging contact information with you). They can be highly valuable to you in the future.  
  
Otherwise have fun and ask the right questions.",1529274331.0
arunnerslife,"1. Network. Get lunch with people in and outside of your department. Setup meetings with people even if its just to get to know them and find out what their role is

2. Learn as much about the company as you can. The idea of an internship is to get a return offer. The more you know the better. Is it a place you want to work after college? Or is it just a really good way to gain experience. 

The work you are doing now is indicative of what you would be doing after college. Is it something you enjoy? If you have time try to work on multiple projects or at least learn about them. See if you can attend meetings to just learn about what else is going on at the company. ",1529273534.0
GNULinuxProgrammer,"Basically if you do well, you'll get an offer once you graduate. I've never seen anyone not getting offer from a company they interned in. So, try to do your best, don't slack off.",1529286627.0
MasterBob,"I would learn it in your free time. You could even doing it on the clock if your boss values that kind of improvement.

> **Why learn computer science?**

>There are 2 types of software engineer: those who understand computer science well enough to do challenging, innovative work, and those who just get by because they’re familiar with a few high level tools.

> Both call themselves software engineers, and both tend to earn similar salaries in their early careers. But Type 1 engineers grow in to more fulfilling and well-remunerated work over time, whether that’s valuable commercial work or breakthrough open-source projects, technical leadership or high-quality individual contributions.

> Type 1 engineers find ways to learn computer science in depth, whether through conventional means or by relentlessly learning throughout their careers. Type 2 engineers typically stay at the surface, learning specific tools and technologies rather than their underlying foundations, only picking up new skills when the winds of technical fashion change.

> Currently, the number of people entering the industry is rapidly increasing, while the number of CS grads is essentially static. This oversupply of Type 2 engineers is starting to reduce their employment opportunities and keep them out of the industry’s more fulfilling work. Whether you’re striving to become a Type 1 engineer or simply looking for more job security, learning computer science is the only reliable path.

- https://teachyourselfcs.com/",1529246543.0
sniddunc,"If it actually interests you, then I'd go for it. While you may not learn much (or anything) on the Software Engineering side of things, the degree could save you later in life. I know a great software engineer that is in his 50s. He has a great resume with a ton of experience, but nobody is hiring him. He believes that it's because there are more entry/mid level positions than there are advanced positions, and he thinks that he's getting overshadowed by people in a similar position that have degrees.

Also, you don't necessary need to do computer science itself. If you have any options for simply ""Fundamentals of computer Science"" then that'll do and could save you a lot of time.

However, it you go into computer science then don't expect to learn much about software engineering. It's mostly theory and math. Learn backend by yourself. If you get good enough, you could even apply for full stack positions. If not, it'll still be very useful to know what the back end engineers you work with go through. Understanding both sides helps work on both sides.

If you can do it, get the degree. It'll help you later on.",1529245421.0
poodleface,"As a person who worked in a different industry (35mm film projectionist!) for ten years before returning, I can tell you that finishing my degree was really valuable (for me). 

When I did my undergraduate at Georgia Tech, I pursued it full-time while working part-time. I was able to integrate myself well socially, meeting people who were also interested in pushing beyond the bare requirements of the curriculum. That sort of connection has proved to be invaluable, and has opened doors down the road. 

When you are facing a challenging problem, there's a lot of value in being able to speak with other people who are in the same stage of understanding that you are in. When you speak only with those who already know the answers to your questions, they will tend to go directly to the solution, especially if their time is divided with work (I'm thinking here of your back end team). Honestly, explaining concepts to others taught me the most (which is  why I pursued TA / instruction opportunities).

I would say that if you are not taking advantage of these aspects of school, it does not hold as much value. You can definitely do this part-time (this is how I finished my Master's), but it is more difficult.

Another aspect to consider is that course materials have to be geared to a wide range of competencies, even when pre-requisites are established. You will find that in many advanced courses that you will not be explicitly taught the things you are the most interested in learning. I would recommend finding courses that have more open-ended projects that will allow you to put the theory into practice. In undergrad, there are fewer opportunities to do this than you would like. I ended up joining a campus development group to give me more chances to actually build things.

In your introductory courses, recall Daniel-san in The Karate Kid having to wash cars and paint fences before he could learn karate. You'll be way ahead of the early stuff with your existing experience. You'll want to consciously use this time as an opportunity to challenge assumptions and break bad habits (while not picking up new ones). Be aware that some things you know may be simplified to aid initial understanding for other students. There was always an advanced student in classes like that who would periodically challenge the instructor during lecture when he would teach something that was not an industry standard. Don't be this person, it's not a good look.

There is nothing to prevent you from picking up side-projects and using that as an excuse to learn a new technology. It accomplishes the same goal as a class project, but you have to have the fortitude to push forward through failure points, structure your own time without an external schedule, and have people you can talk to that can point you in the right direction when your reach exceeds your grasp. I was not one of these people (I think I am now)! Your existing experience may be enough to follow this path, as others in this thread are advocating.

I don't know what you should do specifically, but this was my experience.",1529254824.0
Marty_McFlay,"Where I live there is a tech school that offers a locally respected 2 year software engineering associates degree, and the school has full accreditation  in the evenings for something like $12k.  You might want to look into something like that.",1529260807.0
GayMakeAndModel,"It blows my mind that I haven’t seen a single comment that referred to ‘industry’ as retail, legal, or healthcare for example.  If you get into an industry where actual subject matter expertise is important, you’re golden.  I think of programming as less of an industry and more of a supporting skill in any given industry.  That being said, it’s easier to land a job where the employer teaches you the domain when you have a degree.",1529286573.0
flekkzo,"Backend stuff isn't that hard:) Worst thing I've dealt in my 19 year career (Software Engineering degree) is by far CSS (terrible and illogical). I'm more of an everything developer.

For your own sake, to progress and see beyond the surface, I recommend learning more. Things you will not learn from coding and (often ill adviced) tutorials. How you learn is up to you, but depending on where you live straight up school will cost you more than it's worth.

So why learn? Why is the keyword here. You want to know why things are the way they are. This means learning about things you will more than likely never practically do. I recommend learning about algorithms, compilers and language design, databases, operating systems, networking, cryptography, software design, memory management, etc.

All that will help you understand how things work and will make you a better developer that can grow and continue your career. There's a lot of cookie cutter, look it up on stack overflow, developers around today and you will stand out from them by learning these things. Just don't try to jump into all subjects at once. :)

Good luck!",1529244686.0
dnlmrtnz,"I am a front end dev, had a diploma which I got over 1 year of part time study. Had the same desire to learn more and go into back end, data structures, design patterns... enrolled in a degree online and after almost 4 years (2 yeas left) I can tell you it is worth it. You’ll learn amazing things you can’t get from free or online sources... I study part time while I apply everything I learn at work. I definitely recommend you get started!",1529280193.0
stvaccount,Yes,1529280878.0
Jaeemsuh,"Yes you should learn these things. If I were you I'd stick to data structures, algorithms, a new language, and probably some linux/networking. However, I would not spend a lot of money paying to learn them. Start with night classes at a community college if you need structure, otherwise there are more than online tutorials/books at the public library. Good job planning ahead, React is hot right now, but when it isn't you don't want to known as just the React guy.",1529286059.0
mattpkobus,"Highly advise ""UMUC""",1529297476.0
careago_,Applied Science is better ,1529300083.0
BoobDetective,">  I'd love to pick up more of the theory & 'science' behind computers. 

Yup, go for CS. If you want to do it, do it.",1529307221.0
giladbau,"As someone who is on the hiring side, I would definitely recommend you get an academic degree. It is usually required if you want to get far, as it is a simple way for companies to select candidates for interviews, even before they arrive to me for review for my teams.

That said, I would choose a degree you think would help you get deeper into what you care about. If you want to focus on the algorithmic side of things, go for CS. However, if you want to get the best of both worlds: going deeper into algorithms AND architecture and design patterns, you may want to consider software engineering. In many universities there is a distinction, and you should choose what’s right for you.",1529249835.0
shoshy566,"From what I understand a lot of companies don't necessarily require a CS degree to get the job as long as you have experience and something to demonstrate that. I know that when I interviewed for some CS positions (I'm a computer engineering major by the way) they had me take a test to demonstrate my coding abilities anyways. However, I will say that actually taking CS classes goes through a lot of theory that you probably won't be able to go through just from tutorials alone. 
Personally, I'd get the degree however I think it's really a matter of preference. You'll probably find that it's a lot easier to get a job with a CS degree but if it'll take 6 years to complete it, you might also find that it's better to just build up a really strong portfolio and apply like mad.",1529243307.0
kdnbfkm,"Start by learning a little C, and some sort of server side scripting that a System Administrator might use.",1529252863.0
,That’s a good read. “Not practicing enough” is the biggest reason for me to sucking at it. I know coding is my passion and I love this stuff to the death but I do also know that I am not like those “crazy” coders who sit and code for hours in a day. ,1529223111.0
Gavcradd,"This can be distilled into one tip really - do it. Pick a small-ish project  (whether that be a game, sonething web based, etc) and dive in.When you get stuck, read up and learn but DO NOT copy and paste code. When you're done, repeat with another project. 

Yes, your first few will be full of holes. Yes you will get stuck. But at some point you will look back on an earlier  project and think ""man, that would have been so much better/easier to have done that way"". That's the route to success.

I wrote my first line of code in the early 1980s. I could write you a (simple) program in about 15 different languages. Almost 40 years later I'm still learning. Every day is a learning day! Currently, I'm struggling to fit frameworks into my dinosaur approach to coding. ",1529225766.0
qwertyasdf12345,"You may want to check out scapi: [https://scapi.readthedocs.io/en/latest/](https://scapi.readthedocs.io/en/latest/)

Instead of worrying about the implementation details and nailing all of that, you can just use OT / Garbled circuits via the API.",1529227392.0
ldpreload,/r/crypto might also be a good place to ask.,1529228364.0
rosulek,"Are you struggling with the actual implementation of the crypto, or converting your computation to a Boolean circuit? There are many implementations of garbled circuits, for example JustGarble.",1529242479.0
dagit,I have not personally implemented them but my co-workers have. What have you tried and what didn't make sense? Maybe I can get them to look at this thread and help out.,1529652650.0
atram010,"Start [here](https://teachyourselfcs.com/).  Also try ""An Invitation to Computer Science"" By Schneider and Gersting.",1529212490.0
agaskell,"A good book for absolute beginners is [CODE by Charles Petzold](https://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319/). If you are open to MOOCs the [CS50 course](https://www.edx.org/course/cs50s-introduction-computer-science-harvardx-cs50x) is good.

You might also find this github repo useful: https://github.com/ossu/computer-science",1529212782.0
repsilat,"It really depends on what you want to know. For theoretical CS, I love Sipser as a nice, gentle introduction to the essentials. Better than Wikipedia, and I say that as someone who hates textbooks and loves to learn from Wikipedia. It gets to an advanced undergraduate level on complexity and computability.

For programming or algorithms I don't have good recommendations, I mostly learned that stuff from the internet and in the classroom sorry.",1529217293.0
StorsJT,"It really depends on your personal background. 

If you have a background in maths or some kind of applied maths (like economics/statistics) then your [average computer science textbook](https://en.wikipedia.org/wiki/Introduction_to_Algorithms) would suit you well. It teaches you the kind of thinking you need for Computer Science in formal language.

More into engineering and tinkering with tools? Learning to program will give you a great intuition into lots of Computer Science fundamentals. The language/environment doesn't matter too much, but [I'd suggest C#](https://en.wikipedia.org/wiki/C_Sharp_(programming_language)) since it's easy to get started quickly with all the tools you need (links to tutorials are in the article). As you learn a language more deeply you inevitably tread on lots of computer science topics such as algorithm complexity and computer architecture.",1529222080.0
barsoap,"For CS as such I'd start with discrete maths, in particular [these lectures](https://www.youtube.com/playlist?list=PLqW6TsBi5033igAJYRkuVuEzOju8ZYTgq). Then have a good, deep, look at algorithms and data structures, after that follow your guts.

For coding wizardry [SICP](https://mitpress.mit.edu/sites/default/files/sicp/full-text/book/book-Z-H-4.html#%_toc_start), for code engineering and beginners [HTDP](http://www.htdp.org/2018-01-06/Book/).",1529242074.0
alwaysonesmaller,r/learnprogramming has a great FAQ in their sidebar with recommendations for books and other resources,1529187546.0
wandercamp,"[http://4chan-science.wikia.com/wiki/Computer\_Science\_and\_Engineering](http://4chan-science.wikia.com/wiki/Computer_Science_and_Engineering)  


Ignore the fact its from 4chan, its by far the best self study guide I've seen",1529237748.0
Count___Duckula,"Download Python, Ruby and C#.

Think of a random project and work on churning it out in at least one of the first two then in the third.

It doesn't have to be difficult or even un-trodden, I'd even go as far find something in already done in a different language.

Computer science isn't something worth tackling by theory for awhile and even then, programming is more than enough.

Note: Just my thoughts, few of us actually do real computer science or at least meaningful advancements.",1529216239.0
sublinus,Get enrolled in compsci at some university or do online courses and hear lectures on youtube. Ask questions that might come up here.,1529224852.0
niceperson33,I’d recommend a computer,1529193131.0
kdnbfkm,You could do worse than this. http://www.inr.ac.ru/~info21/ADen/ The language isn't widely used so translate to whatever you use.,1529206015.0
Skootr4538,"I think you can learn ‘coding’ or building apps in a high level language like python, java, javascript especially building apps for iPhone, android or html5 mobile apps. I would also learn how to do database design and improve your maths so that you can take an algorithms, data structures and computability course. ",1529218107.0
alwaysonesmaller,/r/learnprogramming or /r/edX is a better place to ask,1529185318.0
jayvbe,"No experience with CS50, but consider these 60 lectures from Stanford for self-study:

https://www.youtube.com/watch?v=NcZ2cu7gc-A&list=PLnfg8b9vdpLn9exZweTJx44CII1bYczuk",1529289844.0
exstaticj,"It is a good introductory course. I went through it several years ago and enjoyed it. You WILL learn something. It is fun, good paced, and free. Enjoy.",1529203469.0
xTouny,"For me, I assess those online courses as low-valued. I believe grasping a concept and acquiring a problem solving skill is obtained through picking up actual problems and working them out by yourself, Push your limits to the max by creating your own problems and thinking of the concepts you are dealing with, Not by sitting on your arm chair watching someone else lecturing.",1530093961.0
osoese,This is really cool! thanks for posting.,1529212178.0
eternusvia,"I have used PETSc extensively for sparse linear systems. Seems like ScaLAPACK is similar but for dense systems. They have a user guide here http://www.netlib.org/scalapack/slug/

Why do you want to know how it maps to the processors? Usually the point of these libraries is that the software handles it for you.

EDIT: I see now that ScaLAPACK requires a more hands on approach. But it looks like they talk about how to do the distribution in chapter 4: http://www.netlib.org/scalapack/slug/node68.html#chap2dbsd",1529170330.0
yyzjertl,"How large exactly are your matrices, and where are they currently stored?",1529172068.0
amself,"> companionship 

Paging u/fuckswithducks",1529167182.0
Razor_Storm,Reminds me of clippy,1529184279.0
StorsJT,Quack.,1529183918.0
Catalyst93,"You would probably need to have a large amount of labelled data.  What I mean by labelled data is that you have a set of images where the organelles have been identified by a human.  I think this sort of approach could be feasible, however there might be other non-ML approaches as well.  Hopefully someone who knows more about image processing responds.",1529114985.0
XG_West,"From past experience, use of existing high performing CNN architectures (Resnet50 etc) work well if you’re willing to put in the effort to create a decent training set. Consider boosting your training by using image augmentation.

OpenCV will take you a long way if you don’t want to go through deep learning and are able to exploit geometric shapes, edges etc",1529131003.0
east_lisp_junk,"There are lots of regression/curve-fitting programs and libraries out there, but making something *useful* from that still requires at least a bit of human insight. No matter how high-DPI your drawing is, it will consist of finitely many samples, and it will be perfectly-matchable by a high-degree polynomial. That polynomial doesn't offer insight about behavior outside the region you drew. If you draw how the curve looks when x ∈ [0,1], any possible result at x=1.1 will still fit with some high-degree polynomial that fits perfectly with what you drew. So instead it's up to you to figure out what broad class of function ought to describe your curve.",1529100875.0
solomonxie,"Just happened to have the same issue when trying to figure out the complete formula of Forgetting Curve. 
Really do hope there is one can do the reversed work.",1529098247.0
Equa1,"This is a fun way!

https://js.tensorflow.org/tutorials/fit-curve.html",1529346969.0
Resquid,No,1529096261.0
kujiranoai2,"Would my friend be able to use this to classify his extensive collection of pornography - e.g. into blondes, brunettes etc - do you think it would work for that? 

This is the type of project that would really motivate him. I’m guessing.",1529104816.0
bartturner,Love Google shares this type of stuff.,1529100068.0
Kel-nage,Sounds like you might want a [bloom filter](https://en.m.wikipedia.org/wiki/Bloom_filter) to do the initial test. Then you can either accept the false positives or do a slower scan over the data to check for certain.,1529079232.0
kdnbfkm,If the ordering doesn't matter just sort them and only keep the first of a streak.,1529079550.0
indiebryan,The ads are getting stronger ,1529083243.0
_ACompulsiveLiar_,"Software engineering will be the last thing to be automated. Proper software development is a creative process that really requires the full critical thinking abilities our brains can throw out. Going from requirement B to an actual product can't just be done by training a neural network on existing B->product or anything even close to that.

If this scenario occurs, we will have to have created pretty much a human brain, or something so close that we have way more issues to worry about (like automating literally everything in the world, technological singularity, ethics of AI, etc.) than you losing your job.",1529036910.0
minno,"Writing specifications in enough detail that a computer can understand them is generally referred to as ""programming"".",1529047919.0
zokier,"Psst, I'll let you in on a secret: vast majority of software engineering has already been automated.

Just look at the history of computing. The promises of COBOL and SQL, the expert systems that followed,  even Excel in all its glory.


With todays tools people (and not just engineers) can do stuff with few buttons what would have been a whole project 50 years ago, nevermind the wizardry that sw engineers can do so easily nowdays.

",1529044527.0
tmlbl,"The whole goal of modern application libraries is to create as much abstraction as possible so that developers only have to write ""business logic"", which is basically what you describe. The problem will not be people telling AI what to build, it will be them using tools that template and generate code so all that they have to do is describe that feature. That future is not some far-off dystopian vision - it's modern SaaS development. ",1529040330.0
khedoros,"Step 0: The engineering team specifies requirement B at the level of detail necessary to properly constrain the starting and ending conditions for the program.  


The change you're describing means that things we consider big projects now become smaller, and things that we'd consider too difficult now would become feasible. We'll be at that place sometime when we understand more about how human creativity works and how we write programs, or when we find a way to create novel programs without what we previously would've thought of as creativity (kind of like the shifting definition of AI over the years).",1529043286.0
chrisname,"It will happen, but probably not for a few decades. [Recurrent neural networks can generate syntactically valid code](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), but it's meaningless and doesn't compile. Since NNs are probabilistic there will always be an error rate, but I suppose the same is true of biological brains.",1529053527.0
combinatorylogic,"Software engineering, just like any other engineering, is essentially just a constraint optimisation problem. You have a set of constraints and must find a *good enough* solution that satisfies all the given constraints, nothing else. Finding this solution in an infinite multi-dimensional morphological box involves a lot of non-trivial search heuristics, but each and every one of them is automatable in theory.

So, yes, your vision is perfectly possible. The only thing here is actually formulating the constraints - I would not bet on natural language processing ever being useful for such things. Engineering will be reduced to writing down those formal constraints, but you'll still require a formal language to do so and you'll have to learn how to do it.",1529075965.0
crono731,I'm sure simple programs and functions could be that way but I'm not sure it would scale up. would we be able to have the computer learn good software engineering principles especially with debate always going on about what is good? and I feel like when it would have to test itself the halting problem could come in somewhere ,1529035254.0
TomSwirly,"I remember walking down a flight of stairs right before I graduated from university, discussing the same question.

That was in the early 80s.

My theory - I will be long retired before that really happens.",1529065330.0
Revan_The_IV,"Even if ai could write all the code and everything, I’m sure people would still want that “human touch”, at least on the front end. I’m not worried about it",1529075681.0
_georgesim_,"Machine learning is not even close to resembling the type of intelligence humans have, much less something as specialized as software engineering.",1529526390.0
ultrasalubrious,"No one write the program that does that, ok? Seriously.",1530047473.0
hn_sctb,"> Do you think machine learning will ultimately replace software folk?

No. But with better tools and AI, it'll mean fewer software engineers can do more in less time and as a result we'll need less software engineers as time goes on. 

What happened to IT will happen to dev. What used to require 100 IT professional 20 years is now managed by 10. 

The power of scaling, tooling and AI applies to devs as much as it does to IT and other professionals.",1529077535.0
pfannkuchen_gesicht,"yes, if you want multiple clients working simultaneuosly with your server, you need multi-threading. If it doesn't really matter you can also just make use of the backlog queue and work on your clients requests consecutively. Depends on what you want to do.

You don't necessarly need a new thread for every client, you can just use a thread pool and queue requests up for them. So essentially you have a fixed number of threads that are waiting for elements in a queue.",1529044260.0
fakehalo,"I'm not familiar with Winsock/windows for this, but you can use select() to monitor activity on multiple (non-blocking) sockets/fds without threading on linux/*nix.  It's a common approach for daemons.",1529082924.0
zokier,"The alternative for thread per client is various forms of async programming. Check for example the ""iocp"" and ""overlap"" samples here: https://msdn.microsoft.com/en-us/library/windows/desktop/ms738545(v=vs.85).aspx",1529076962.0
HeadAche2012,"There is a backlog, so multiple clients get queued up ",1532535875.0
mellowbrickroad, awesome! thanks,1528985269.0
RikityRakity,This is something I’ve been wanting to do for a while. I’ll try to get up to speed. Don’t think I’m going to get through the first chapter in 2 days lol,1528988130.0
TheUltraTurd,just what i was looking for!,1528988490.0
zealousDiscreetShrug,"A bit on the side, if you want to learn Quantum *Physics* instead, I find this sequence of books most helpful for self-study:  https://metapuffer.wordpress.com/2018/05/09/self-study-book-path-quantum-mechanics/",1529022114.0
,You post your MAL profile and a few hours later MAL goes down. :hyperthonking:,1528991178.0
RexPowerColt69,Thanks,1528997581.0
Lucretia9,Does  r/QuantumComputing exist already?,1529000691.0
lazzy_8,Ike is short for Issac?  Mind blown!  How did I never figure this out?,1529036016.0
tboneplayer,Subscribed! Looks awesome!,1529036209.0
FinxterCom,Thanks for the effort! Why not use a more descriptive name for your subreddit? So that random people will be able to find it... ,1529175593.0
wookiecontrol,No,1529078890.0
xplot,but why?,1528989369.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1528940334.0
jhaluska,"Pick the simplest idea that you think you can split into 5 relatively equal work loads.  Add complexity as you complete things.  If doing ML do some cool analysis of how it works, or compare competing solutions.

You want the work to be the easy part, as you're going to find out getting everybody together, tracking progress, etc is going to be the majority of the work.",1528953996.0
xShadowProclamationx,what areas of comp sci are you guys interested in???,1528928281.0
ntopower2,"A meta programming project of automatic object generation. It could work either with heuretics and some basic syntax as input or you could enhance it by adding some nn layers and feed it with actual projects containing objects and some description for them. I guess you should start by object recognition in the language code and then train your model to understand object structure. I am not sure if this idea is already done zillions of times, I found [that post](https://stackoverflow.com/questions/11959513/python-automatic-object-generation-plus-naming) which has some similarities but it is overall simpler.That project is scalable, even if you don't succeed in training a model efficiently you can simplify your object scope and present a working solution and it has the ability to grow even more by ie adding more languages support.  
PS I am not an expert in ML or meta programming, please don't roast me :P",1528930207.0
brystephor,"Create a service that analyzes a companies stock prices and based on fluctuations, predicts whether or not in the next month the stock will rise. If you really wanna make it more advance, categorize it by industry. Make it so you can enter any ticker symbol and it'll push out an answer. Use trends from other companies in the same industry and see if there's any correlation. Or compare a ticker to the s&p 500 and see how much it correlates. Lots ya can do with this one. Plus you'll probably learn a lot about stock trends and if accurate enough, consider using it as a tool for yourself later on.",1528934757.0
maladat,"I'm a CS PhD student. At my university, the way a lot of seniors and non-thesis master's grad students come up with project topics is basically the same way that MS and PhD students come up with research topics:

Step 1: Choose a professor you think you would get along well with whose research area is interesting to you.

Step 2: Approach the professor, introduce yourself if you don't have an existing relationship (for example, from taking one of the professor's classes), and explain that you're interested in the professor's research area and are trying to find a project topic.

Most of the professors I interact with are delighted to have conversations like this and keep a mental list of project ideas long enough to completely overwhelm you.",1529004992.0
gwern,"Wouldn't this fall under Turing machines with limited size tapes? Lack of space will heavily restrict what functions can be computed, and how efficiently (since you have space-time tradeoffs so you can to some extent make up for limited memory by doing tons of extra computation). It also sounds a bit like multi-tape models but those have about the same computational power and complexity classes AFAIK.",1528916368.0
gabriel-et-al,"I didn't read your full post yet (can't do it right now) but you seem to be talking about [type-0 grammars in Chomsky hierarchy](https://en.wikipedia.org/wiki/Unrestricted_grammar).

Also, you said the term ""reality"" is problematic, which I agree; how about ""context""?",1528921832.0
ponycomplete,Might wanna look at [Partially Observable Markov Decision Processes](https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process).,1528935862.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/slatestarcodex] [Attention Machines - Is this a branch of complexity theory? - Crosspost from r\/compsci](https://www.reddit.com/r/slatestarcodex/comments/8qujbd/attention_machines_is_this_a_branch_of_complexity/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1528912982.0
eigenman,Constrained Optimization.,1528941670.0
exarpy,"Ripple has committed over $50 million in funding, subject matter expertise and technical resources to UBRI’s first wave of university partners —17 prestigious institutions around the world. These schools will determine their own research topics and priority areas of focus, while also partnering with Ripple to:

* Collaborate on research and technical development that will stimulate widespread understanding and innovation in blockchain.
* Create new curriculum to meet high student demand for learning about blockchain, cryptocurrency and other FinTech topics.
* Stimulate ideas and dialog among students, faculty, technologists and business leaders on topics of shared interest.

UBRI’s complete list of partners is as follows:

* [Australian National University College of the Law](https://law.anu.edu.au/)
* [CITP at Princeton](https://www.princeton.edu/)
* [CSAIL at MIT](https://www.csail.mit.edu/)
* [Delft University of Technology (Netherlands)](https://www.tudelft.nl/en/)
* [Fundação Getulio Vargas (Brazil)](https://portal.fgv.br/)
* [Haas School of Business, University of California, Berkeley](https://www.berkeley.edu/)
* [IIT Bombay](http://www.iitb.ac.in/)
* [International Institute of Information Technology, Hyderabad (IIIT\-H)](https://www.iiit.ac.in/)
* [Korea University](https://www.korea.edu/)
* [McCombs School of Business, UT\-Austin](https://www.utexas.edu/)
* [The University of North Carolina at Chapel Hill](https://www.unc.edu/)
* [The University of Pennsylvania](https://www.upenn.edu/)
* [UCL (University College London)](https://www.ucl.ac.uk/)
* [University of Luxembourg](https://wwwen.uni.lu/)
* [University of Nicosia (Cyprus)](https://www.unic.ac.cy/)
* [University of Oregon](https://www.uoregon.edu/)
* [University of Waterloo](https://uwaterloo.ca/)",1528906590.0
jmercouris,"I'm tired of hearing about hypechain, can we lay it to rest?",1528910468.0
jtmont8,"It looks like a joke now but I'm fairly convinced that in the not too distant future the most influential social media accounts will be run by some kind of AI.
",1528905126.0
moraceae,"You should be able to do the projects locally, though you might not have the full suite of tests they use for grading. There's also the Spring version of 15-440 at CMU, which focuses on filesystems instead of networking.",1528853737.0
davidlowellbyrne,I really dig this presentation. What is the tool your using to create these? Do you do all the graphics your self from scratch? ,1528843882.0
PythonGod123,"Nice article on graphs. I enjoyed reading this. Link the other comment said, the pictures are very nice. Keep up this good work !!!",1528852184.0
gabriel-et-al,"The python.org website has a pretty simple design guide for implementing graphs, which uses the adjacent list approach.

https://www.python.org/doc/essays/graphs/",1528853599.0
tldrtfm,"I liked it, good work!

One small comment: On the adjacency matrix part, in the quoted statements about time complexities, I think you mixed up adjacency matrix and list. That is, you wrote list, when I think it should be matrix. ",1528923823.0
agumonkey,also on https://www.reddit.com/r/squiggol/,1528837356.0
agumonkey,Companion article (same authors) http://www.brics.dk/RS/03/13/BRICS-RS-03-13.pdf,1528837548.0
taauji,"I am learning trees for the first time. Sorry if this is a very noob question, but doesn't every two nodes should have the same predecessor, because any two siblings would always have the same parent",1528794415.0
bgeron,/r/askcomputerscience,1528801440.0
programmaton,"All good answers. A minor footnote: in practice, good implementations of binary trees use the same data type to represent all nodes, including the root node, say, a `struct node_t`. However, the abstract binary tree data structure has no parent for the root node. This is ironed out in practice by making the parent link point to self or to some other special value. Similarly, left child of leftmost node and right child of rightmost node might be made to point to root, aiding in the implementation of various other operations (traversals, removals or insertions, etc.).",1528808334.0
deevus,Can that even be considered a binary tree? ,1528797818.0
DogmaticAmbivalence,"I'd be interested

Compression is such a wide field though, can you be more specific?",1528802330.0
combinatorylogic,And how is it even related to computer science? It's a brief and opinionated hiatory of some part of computing. Not mentioning anything of importance from the actual *computer science*. ,1528793382.0
Thousdays,"but.... Al Khawarizmi wrote the firt ""Algorithm"", his name in greek is algoritmi. This graphic is ever so flawed but not very useless",1528824716.0
hundredPercent-beef,"If you want to keep spamming the same Medium blogger everywhere, could you at least pick one with good content? New and interesting ideas? Stuff that's on-topic for the subs you post in?",1528781246.0
nightcracker,"I wouldn't call `O(max(S))` memory usage exactly 'extremely efficient', especially when you expect a *sparse* data set.

Much better memory usage would be to use a plain old hash set.

And never underestimate linear search. If you have < 100 elements it will beat out almost anything. If order doesn't matter (like here), a great optimization trick is to, after scanning, swap the found element at index i with the element at index i/2. Assuming locality of reference (which happens really often), this greatly improves scanning performance as commonly searched elements bubble to the top.",1528745546.0
yyzjertl,"This post makes it seem like you invented sparse sets when in fact you did not (e.g. by saying things like ""I introduce"" and by not citing the original paper). Please don't do this.",1528743976.0
paul_miner,"I think there's a minor mistake in the explanation of the second condition of has():

>If the index suggested by S[x] is less than the size n, it cannot exist.

I think it should read ""*not* less than"".",1528746659.0
surelyourejoking888,Very interesting post. I like seeing content about 'obscure' methods (or at least things that have fallen well out of the spotlight). A lot of performance improvements can arise from rediscovering old solutions. ,1528767367.0
celerym,This is super useful for some stuff I'm working on. Thank you!,1528784943.0
nevabyte,Very nice.,1528743441.0
unlocal,"If all you want to do is ‘store integers’, this is a terrible approach. The only apparent value lies in the preservation of the order of insertion (and rapid traversal of the contents of the set in this order).

It’s trivially defeated for most normal definitions of ‘extremely efficient’ that don’t need this property by a well-selected trie.

",1528795997.0
baryluk,This is extremely inefficient. Nothing new. Two data structures with main data and index are very old technique.,1528826361.0
QuantumFTL,"I agree with many of the other commenter here, this is an interesting and very simple data structure, but the article title is incredibly misleading and most of it should be rewritten to address our objections.

It's a useful data structure in its niche, but unless you define what you mean by ""efficient"" (especially in the context of storage, which is usually about space efficiency) the fundamental assertion of the article is confusing and inaccurate. ",1528923313.0
basyt,check out [this](https://web.archive.org/web/20101102120728/http://measuringmeasures.com/blog/2010/3/12/learning-about-machine-learning-2nd-ed.html) bad boy. its on my to read list for almost a year now. i'm gonna start it one of these days. i promise.,1528733480.0
kdnbfkm,"Whether the article is true or not right off the bat it looks like BS...

What's this?
>Data system support for the Internet of Things

F' YOU!!",1528696970.0
OptionsReprimanded,TLDR ?,1528700538.0
MakeTheBrainHappy,What do people think about this article? ,1528679355.0
bremby,What do you mean by those terms? ,1528665803.0
ntopower2,"Well almost every book regarding algorithms usually demonstrates direct applications to cryptography like RSA or share key crypto. A pure number theory book probably won't cover these topics accordingly tho.  

The basis of a crypto scheme is a reversible function that obscures a message to sth unreadable. Usually those functions work on a fancy domain like a Galois field so it's only natural for a skilled cryptographer to be proficient in algebra and number theory.",1528645679.0
StefanOrvarSigmundss,They stare into quantum crystal balls and tell the future.,1528645226.0
hughk,"Priority scheduling was not new with the AGC. There were many real time computers at the time such as the IBM 1800 or the PDP 8. Round robin was only used for time sharing and you wouldn't want to use it to control a steel plant or an assembly line. There were smaller systems being built for planes and missiles but they had problems ltrying to do too much and still be able to fly.

The real achievement was getting the size down and the power consumption reasonable. This also entailed shrinking the code, hence the need for hand optimised assembler.

There was another computer which actually flew the Saturn V in the so-called instrument ring which sat at the top of the third stage. This provided telemetry, gyroscopes, an analogue flight computer and the so-called Launch Vehicle Digital Computer. The LVDC was a beast built by IBM handling the work of getting the payload into orbit. The Apollo Guidance computers had to be much smaller as they only had the engines of the service module and lunar module to lift them.",1528658469.0
alwaysmadd, Cool. I hope all those pages are numbered. ,1528640398.0
pbaum,This is a great historical perspective on the origin of these concepts. Will be sharing the article with my high school CS students.,1528650401.0
Fidodo,"This is pretty long so I'll have to read it later.

*Adds to list of hundreds of interesting things I'll read ""later""",1528657308.0
teteban79,"Nice article, thanks! Coincidentally, I had the chance to listen to Ms. Hamilton herself speak about these issues a few days ago. Here is the [video](https://www.youtube.com/watch?v=ZbVOF0Uk5lU) of that talk \- I am sure you will enjoy it",1528655758.0
maweki,https://www.reddit.com/r/whatisthisthing/,1528574418.0
Nerdlinger,/r/geology/,1528580327.0
SteeleDynamics,DFS?,1528567805.0
VorpalAuroch,"*algorithm, singular",1528580431.0
Breathing-Life,Is there any chance you can link the algorithm code?,1528582809.0
redaaa99,"For anybody who wanna see the full project demo for the other algorithms:
The demo: https://www.youtube.com/watch?v=EqSFCL1dZJ0
The code : https://github.com/redaaa99/A-Maze-me",1528596948.0
kati256,"Well, I'm so copying this just making the maze bigger, I feel this would be great for explaining to my friends how the different algorithms work. (We literally have an exam on this in like two weeks lmao)",1528579775.0
gabriel-et-al,"Hey nice one! I've coded a naive maze generator in javascript, here's the link:

https://ihavenonickname.github.io/maze-generator/

It starts when the page loads. Pretty useless but I had some fun coding it.",1528603292.0
gifv-bot,"[GIFV link](https://i.imgur.com/xI2R0XH.gifv)

---

_^I ^am ^a ^bot. ^[FAQ](https://www.reddit.com/r/livven/wiki/gifv-bot) ^// ^[code](https://github.com/Livven/GifvBot)_",1528567107.0
delarhi,https://bost.ocks.org/mike/algorithms/,1528599049.0
mbprothero,That’s awesome,1528677127.0
realfrankaroo,"In general, it's really a matter of convenience and practicality. Having a customisable and effective interface (i.e. file system, configuration organisation, command line tools, etc.) is fundamental to carry out most of automations, and UNIX like systems make it very easy to do so. Bash, Perl, Maven, Node, Docker, Gradle, SBT, pretty much all compilers in existence have all fantastic command line interfaces built around UNIX like systems and you get a lot done with them. So, in general and if you are not working in Microsoft shop, Linux is the best choice possible.

However, programming is just a portion of the job. You have other people in the company to talk to, you need to share documents, you need to go in meetings and not everyone is a nerd. There are certain products like video\-conference tools and, let's be blunt here, MS Office that simply do not work well on Linux.

With macOS you kind of get the best of both words: when you are doing your actual job you have all the goodness of UNIX (even if not everything that Linux would provide for) and when you need to do all the unfunny stuff you still have decent tools at your disposal. At the end of the day, spending days try to get Cisco WebEx working on Linux is not really an efficient way of using your time.

In conclusion: if it depended on me, I would use Linux everywhere, but considering I am working in a wider company with other people and with existing tools, macOS is the second best thing. Plus, the company is paying for the laptop :P",1528557539.0
realeyes_realize_,"The OS philosophies are entirely different. Windows has a design philosophy to be more idiot-proof so there are a lot of excessive GUIs and what not.     
Where as on a *nix machine, you have complete control over every detail of your machine (this is less so on an Apple, but it's significantly more than Windows). You can compile from kernel and have fine control over every shell, app and utility on that machine if you want.     
The system architecture on Windows is also a lot more messy than on a *nix, where the kernel, hardware, and user levels are clearly defined as well as having a very modular user interface.    
For programming in particular, most languages are built for *nix or -like technologies. Also, the more you understand *nix, the easier it is to understand certain conventions and language that just don't translate well in a Windows environment.    
That being said, certain tech stacks work better on Windows than on a nix system, like .NET. ",1528557690.0
nerdshark,"Unix isn't just one single operating system. It is a whole family, a lineage of operating systems ultimately derived from AT&T Unix. It has since been codified into a family of standards (called the [Single UNIX Specification](https://en.wikipedia.org/wiki/Single_UNIX_Specification)) for compliant operating systems to follow. These include: specific C headers that must be available; kernel and userspace APIs; utilities; and the shell. 

The benefit of the Specification depends on your use case: for users, it's that you can pretty easily adapt to any Unix-compliant operating system, since they will all be very similar in the most important ways. For developers, it's that code you write will be very portable between Unixes, with modifications generally only necessary when you use platform-specific functionality. ",1528557692.0
Revrak,"I think it’s a matter of preference. In my experience mac cli programs don’t really have the same flags as the linux version and since things need to run in linux making things run on Mac doesn’t solve anything . I personally use linux and windows with cygwin and I rarely have to deal with any issues with cygwin

Also afaik osx was not based on unix but something else. I forgot if it’s based on darwin or freebsd or some other project ive never used)",1528558513.0
bartturner,"I very much did and use to use a Mac.  But purchased a Pixel Book which now has GNU/Linux with developer channel.  Since buying have not even turned on my Mac.

OS X was close but still NOT GNU/Linux.  With GNU/Linux on my laptop I can use the same containers I use in the cloud on my laptop which is ideal.

Have a problem with a container just take a snapshot and debut on your laptop.  It is the ideal setup for me and then on top get great security and then the cherry on top is having Android.",1528633245.0
RexPowerColt69,"I would like to get a mac as a dev box, but with those the price tags, I just can't justify it.",1528910151.0
pulsar512b,"I'm a novice, but Mac and Linux usually come with some variety of programming tools (Python and Java come to mind).",1528564900.0
mcandre,"bash is less shit than command prompt, but mac does not make it exactly convenient to manage buildbots. Stick with Linux, kid.",1528556176.0
SerSanchus,"Ever tried to open a window in macos using only C++? Impossible if you don't use objc and a closed API. A complete nightmare.
Stay with Linux. So much fun.",1528556490.0
ee-z,"It's really cool! Liked the design and the dinamic. Sadly, I only got 27 points. I felt like the timer goes a bit fast though.",1528524703.0
rdmdota,"This is nice. But I feel the timer is too strict. Also I think there should be at least two modes: high score and practice. High score should have a timer and should punish mistakes. Practice should do neither. Making one mistake later on drops you back to the ""easy"" boolean expressions and you have to grind through them.",1528531667.0
SophiMiller,"I made it back in 2014. Now i couldn't get pass 20 points myself haha. When I have time, I will try to work on a mobile app version.",1528532985.0
GeneralFailure0,This is pretty fun. I clicked through to try it out and ended up playing about 5 rounds. Only thing I'd add would be an indicator of my high score so far.,1528533013.0
alive_or_,Its pretty fun!,1528524212.0
,Great game! A suggestion: Add 1-3 lives instead of failing at the first wrong click. That would feel more motivating I think.,1528541395.0
AIforce,this is cool!,1528530074.0
Muha95,Quite fun! Do you plan on open sourcing or already have a repo somewhere?,1528540963.0
oMOOoCOWo,"I really like this! It could be the CS person in me, but this is better than most mobile / mini games I try.",1528544517.0
seal2434,*launches game* wtf this is so easy what even is the point *10 minutes later* GODDAMN IT IT SAID PURPLE BUT NOT 2! (Really cool 😂),1528555779.0
Matt-ayo,"It's a really cool game, I love how simple yet challenging it is. ",1528587068.0
jhaluska,That's such a simple and fun way to teach the programming concept.,1528530752.0
omegides,Great game!,1528533652.0
balazsdavid987,"Simple yet challenging, I love it!",1528535200.0
mrexodia,Cool! You should make an app out of it.,1528536241.0
pier4r,Nice introduction game. A game based on logic (and not only) that I like a lot is gladiabots. ,1528542101.0
papa420,inb4 nintendo dcma for sound effects ,1528554014.0
statscsfanatic21,"Wow, really enjoyed the game. Mind sharing how / what you used to code the game from scratch?",1528558481.0
atkulp,"Fun, and a good way to teach Boolean expressions!",1528560267.0
Dunedain96,This game would be perfect for mobile! ,1528561485.0
Zophike1,Where's the source code for the game ? I'd like to build and extend upon it :>).,1528561500.0
Breathing-Life,That’s awesome,1528564526.0
AppleSmoker,It's great! Love the Super Mario World sound fx,1528565330.0
ItzWarty,It's really fun! I'd play lots more if there wasn't a timer or restart on loss.,1528570375.0
jmdugan,"great!

ramp up the timer, a bit more slowly (meaning much more time on levels 8-15)",1528570432.0
olfeiyxanshuzl,This is great!,1528572119.0
targus3000,It's wonderful. I enjoyed it and shared it with my friends too!,1528578454.0
Epistechne,"Personal best is level 26, that timer is stressful. 
EDIT: New PB lvl 34",1528582079.0
Breathing-Life,"Awesome game, got 88 
[Proof](https://ibb.co/c3tioT) ",1528582339.0
misingnoglic,Very cool idea! I'll have to bookmark it for when I have a student that struggles with boolean logic!,1528585942.0
MrZMath,Fun.,1528591938.0
DONT_WORRY_ITLL_FIT,I love the anxiety of the timer! Powerful learning tool.,1528596532.0
nicocappa,If you like this you might like the game Not Not on mobile,1528600291.0
Billythecrazedgoat,sounds nice,1528607972.0
mrnate91,Great stuff,1528624093.0
Lee_Ogre_Growl,Broken on mobile. Only able to click on two boxes and then the game freezes. ,1528549268.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1528514812.0
gct,Beware of thinking you know everything because you're almost always wrong.  With a degree you'll always jump the line in front of the person that doesn't.,1528508526.0
Commander-TeeJ,"Go and test out of the classes till you can't, the sad reality is employers look for degrees. Also you have much more classes in college than programming languages. ",1528505656.0
brystephor,"College isn't about the classes. It's about the opportunities. There are many people here to collaborate with. There are many resources I wouldn't have access to other wise. There are many people who know WAY more then me. I get things I wouldn't be able to unless I was a college student. If you go to college and do nothing but go to class and get decent grades, you're screwing yourself.

If you do know as much as you're making it sound like, then classes will be no problem and you'll be able to spend all your other time making cool projects and doing really neat stuff, find a professor you like and impress the hell out of them.",1528513667.0
chocol4tebubble5,"Computer science is far more than knowing languages, and the number of languages you ""know"" is a very poor measure of your ability. Literally anyone can learn the basic syntax of a whole bunch of languages without understanding any underlying concepts - it's not what makes a great computer scientist and it would be exactly the kind of thing you would learn if you stooped to the humble levels of a computer science degree.

You've been learning since 7th grade? Congratulations, you are completely normal. Many very successful people started a lot earlier, many started much later.

The attitude that you have nothing left to learn is very damaging, and I would suggest dropping it as soon as you can because I assure you that will never ever be the case. I'm guessing from your age, what you really want is a pat on the back and a ""Congratulations, here's your award for being the only 14 year old on the planet to write code!"". Well, tough. You're not the first, you're not the last. What you can do is learn some humility and accept you still have a lot to learn.

I had quick look through your post history; drop the red pill crap right now. You have no idea what you are talking about.

On that note, since you are fluent I suppose you would find it trivial to tell me the output of this program, *without* Googling or compiling it? If you haven't guessed, there's a catch. Extra points for spotting it.

    #include <stdio.h>
    
    int* f() {
        int i = 5;
        return &i;
    }
    
    void g() {
        int j = 42;
        j++;
    }

    int main() {
        int* x = f();
        g();
        printf(""x = %d\n"", *x);
        g();
        printf(""x = %d\n"", *x);
        return 0;
    }",1528534979.0
eternusvia,"As Commander-Teej said, you need a degree to open most doors. Depending on the college you attend, you will still have many opportunities to explore advanced classes and do research.

When you apply to colleges, make sure to talk with faculty from the computer science department and show off your skills. This will prove to them that you are ready for advanced classes and they will help streamline stuff for you.",1528505928.0
vriemeister,"Its actually a huge power sink and not a trivial part of processor design. Here's an overview that will give you an idea what to search terms you can use to find more  

https://www.electronicdesign.com/products/what-s-difference-between-cts-multisource-cts-and-clock-mesh
",1528498068.0
combinatorylogic,The thing you're looking for is called a [*clock tree*](http://www.mouser.com/pdfdocs/clock-tree-101-timing-basics.pdf). ,1528501349.0
jhartikainen,They send current constantly but the voltage oscillates and the difference between the high and low is what counts as a clock cycle,1528497518.0
enum,https://en.wikipedia.org/wiki/Crystal_oscillator,1528497523.0
admiralrockzo,"Same as flipping a light switch on and off. The voltage changes each time the switch toggles. Granted, you can't do this too fast. To speed it up you need to look at some physical laws, most importantly that for a certain current the rate of change of voltage is inversely proportional to  capacitance. Capacitance is proportional to area, so to speed up a circuit just make it smaller.",1528501883.0
maranathaman,Crash Course Computer Science have a great video explaining it: [https://www.youtube.com/watch?v=FZGugFqdr60](https://www.youtube.com/watch?v=FZGugFqdr60),1528502310.0
hornetkeeper,"Ok thanks, that was my question",1528501947.0
sarkie,"Today i learnt.

Thanks op",1528499267.0
faintedremix009,"Here you go :)

https://youtu.be/bCVT1BtlZn0",1528521346.0
Gavcradd,"Feel like I'm missing something here - my answer was going to be that the CPU contains a quartz crystal that oscillates at the required frequency, as described in this link :https://en.wikipedia.org/wiki/Crystal_oscillator

However, you've all gone off on a very different track so either my understanding is out of date OR I've misunderstood the question...",1528561462.0
Enlightenment777,"https://www.amazon.com/USB-Complete-Developers-Guide-Guides/dp/1931448280/

https://www.amazon.com/Serial-Port-Complete-Virtual-Embedded/dp/193144806X

https://www.amazon.com/Parallel-Port-Complete-Publisher-Lakeview/dp/B004U1B7MK
",1528479067.0
werediver,"Is the subject application an off-line application (you're not mentioning any web-backend)?

If so, the operating system handles user authentication for you. Are you sure you have a good user case for another layer?",1528472225.0
ivnshvrk,"https://en.wikipedia.org/wiki/Mathematical_universe_hypothesis

https://en.wikipedia.org/wiki/Super-recursive_algorithm

https://en.wikipedia.org/wiki/U-bit

https://en.wikipedia.org/wiki/Constructor_theory

Use these approaches to define the set of formally describable or constructively computable universes or constructive theories of everything. 

Now simulate the entire universe.",1528461299.0
atk93,"https://uva.onlinejudge.org/

Uva has a massive collection of programming problems. You need to solve them with a good enough algorithm that they can be solve inside the time and memory constraints. ",1528470934.0
cp5184,"This sounds a little like a homework question.

In the beginning, you'd have a wall of dials, you'd dial in your program, flip the on switch, then a few hours later you'd get a few digits, say, six or 8 digits.

Different things were probably tried, but then we developed punch cards, other than that little changed.

Then we developed batch processing.  Let's skip batch processing on punch cards and say we've moved to solid state memory.  Batch processing is mostly running one process to completion.  No multiprocessing.

Here we're seeing the beginnings of programmable operating systems.

The first operating system may have been developed in 1951 or earlier.

I don't remember the exact delineations.  With multi processing, operating systems were able to hold more than one process in their memory.  With multi tasking, operating systems were able to swap between processes.

As the elders tell us, before unix, was multicis.  This was an early, popular time-sharing operating system.  It's been a long time, at a glance it seems like time sharing is a term used for a very basic form of un protected multi tasking multi processing that allowed a primitive form of multi-user computing.

Multicis had many drawbacks and probably wasn't supported by many platforms.

With unix, many standard forms of abstractions were created, device files, command line scripting/interpreting, and many of the common utilities we know today.

This was bell labs UNIX.  It was a commercially copyrighted operating system.

BSD was released in 1977 with a bsd license, in many ways it's similar to unix but with a different license.

The elders tell us that in the age of darkness, CP/M was the popular home operating system.  CP/M became displaced by many flavors of DOS.  Quick and Dirty DOS was bought by microsoft and renamed Microsoft Dirty operating system, Apple had their own operating system.

The elders tell us that although many people who didn't use computers before hard disks, disk operating systems were developed for computers that had floppy disks, but no hard disks.  Most of them mimicked CP/M and provided simple tools like BASIC interpreters and such.

Significantly, apple would develop the LisaOS, a sophisticated GUI in 1982 that offered protected memory, a feature windows wouldn't offer until windows 3.0 in 1990.  Apple would release macintosh OS in 1984, and throughout most of it's life it would be technically superior to microsoft's offerings.

Windows was first released in 1985.

Linux was released in 1991, it probably eventually was given a gpl copyright, then that changed to gplv2

Unix came first.  BSD was an offshoot of UNIX.

Macintosh was independent.

The MacOS and, by extension, LisaOS used licensed patents of XEROX PARC's, the ALTO, famous for the ""mother of all demos"", which paved the way for the modern GUI.

Apple paid to license their patents.

Here's the rub...  there's a chance if, say, microsoft wrote a program for LisaOS, there's a chance microsoft could then be sued for violating XEROX/Apple's patents.

So microsoft asked for and got a license to use apple's patents to develop programs for Macintosh OS.

Microsoft turned around and used that patent agreement to write Microsoft Windows.

Apple sued microsoft.  The courts ruled that due to the exact language of the license between apple and microsoft, windows was covered under that initial license.

Linux was based on MINIX which iirc was based mostly on UNIX.",1528453898.0
kdnbfkm,"https://www.discoverbsd.com/2013/03/history-of-bsd-begginings.html?m=1 https://www.levenez.com/unix/ https://www.freebsd.org/doc/en/articles/explaining-bsd/what-a-real-unix.html

You should read the [Unix Haters Handbook](http://wiki.c2.com/?TheUnixHatersHandbook) for perspective.",1528459649.0
heyandy889,">Basically,  Right from the beginning, I need someone to explain the entire journey,  licences, inter connections between different OS, etc. in a layman  simple and straight language.

This would fill a textbook, my friend. ;\-) Take a look at [https://en.wikipedia.org/wiki/History\_of\_operating\_systems](https://en.wikipedia.org/wiki/History_of_operating_systems).",1528469269.0
everything-narrative,You might read [In The Beginning Was the Command Line](http://cristal.inria.fr/~weis/info/commandline.html),1528470981.0
RickSagan,[This](https://i.imgur.com/RgkVBgR.png) might help you... ,1528473524.0
agumonkey,"Don't forget that win/mac were personal computer OSes first. An OS is """"""just the glue"""""" that holds electronic devices together around a CPU. There are a lot of other kinds of OSes.",1528486050.0
bartturner,"Would change ""Linux"" to ""GNU/Linux"".   Linux is NOT a OS but a kernel.  If change ""Apple Mac"" to OS X then all the others are operating systems.

You also really need to include CPM as it more the root of things.   Well it and Unix.

Linux is a kernel and what Android, Tizen, Tivo, ChromeOS, and what many other OSs use as a kernel.   What I think you are referring to is GNU/Linux as some will just call it ""Linux"".

",1528633385.0
HeSheMeWumbo387,"They’re pretty long, but I’ve found the [Synthesis Lectures on Computer Architecture](https://www.morganclaypool.com/toc/cac/1/1)  to be excellent resources on several topics (in particular, I’ve made a lot of use of the ones on GPUs and coherence/consistency). You should be able to access them for free from a university IP address, depending on your school.",1528460679.0
phynics,"I don’t know of a lessons learned article, but Intel developer manuals are as in-depth as it gets. 

https://software.intel.com/en-us/articles/intel-sdm",1528451594.0
sd_glokta,"To my shame, I've never heard of the Curry\-Howard Isomorphism. Could someone give it an ELI5 treatment?",1528426768.0
venmoney,Thought I was on r/nba for a second.,1528421927.0
figure--it--out,"Why does it seem like there’s such a big jump from learning how to program to actually being able to program anything useful/good?

I’ve known ‘how to program’ (i.e. knowing the basics and syntax of multiple languages and being able to make tiny little games and stuff like that) for years but I don’t know, it still feels like I couldn’t ever make anything worth using. ",1528420762.0
SOberhoff,If anybody can give input on [this](https://stackoverflow.com/questions/50736147/how-do-operating-systems-determine-where-to-direct-device-input) I'd appreciate it.,1528452775.0
Toast_Hatter,"I've been using these to revise for the A-Level **OCR** exams coming up really soon here in the UK, I thought I should share them somewhere just in case anyone may find them useful in any way. I will be adding to them up until the final exam, at the moment it mainly features the basics and a few other things I personally struggled with remembering.

Plus even if these cards aren't useful for you, I'm sure there will be plenty of others available on the site

If there are any issues or if you want specific cards added please message me through reddit.",1528393473.0
lm2s,"The CPU has memory of its own, in form of registers and in form of caches. When it needs a certain value that is neither on the registers nor on the caches, it fetches it from the RAM.

The processing always happen on the CPU, the RAM has no processing, it only holds values.",1528394408.0
dclawrence1978,"Old school answer but should still mostly be true:

Registers are not part of RAM and will reside in a different physical location. A CPU should be able to read and write from the RAM, but that’s about it. The registers act as dedicated inputs to whatever instruction is being used, like numbers to add or an address to jump to. They’re similar to parameters in a function, as opposed to variables (RAM), and in fact compilers will (or at least used to) sometimes pass arguments to a function via a register since it’s slightly more efficient than using the stack (which is most likely in RAM). Registers aren’t addressed like memory, either. You access specific registers by name (edit: more accurately by ID).

Caveat: I haven’t had to program at that level in a long time. I’m sure there are all kinds of exceptions these days due to hardware optimization. Modern OS’s don’t like you messing with that stuff either so unless you’re writing drivers, what you can do ends up being fairly limited.",1528402731.0
combinatorylogic,"What? DRAM cell is a transistor + capacitor. The advantage is high density, and this is where advantages end, it's a capricious thing that need to be constantly refreshed. Registers are much bigger, but they're based on flip-flops - i.e., transistors only, no capacitors.",1528446275.0
NovaX,"I dual majored in CS and CE, and crammed in a masters of CE. The university had separate CS, CE, and EE degrees.

EE is very math and physics based, obviously focused on hardware applications. There are required classes on both digital and analog logic. Some required or elective courses have no relations to computer s. For example thermodynamics and power systems. DSP and wireless communication classes were pretty popular in my day.

CE is a branch of EE that focused on computers. There are still required classes that are unrelated as a baseline, such as analog logic and thermodynamics, which is where I did worse (as not how I think). There were some baseline CS classes as well, such as algorithms and operating systems. There is more of a focus on logic design (microprocessors, VLSI, etc) and embedded systems. For example, in one class our labs was to build hardware (like networking), write mini-OS, and use interrupts to interact with the raw machine. Classes are very design focused and labs are a lot of pain debugging your work.

CS is focused on the software side, with a smaller baseline in the fundamental math and hardware. The classes focus on a much broader set of topics such as algorithms, operating systems, graphics, networking, AI. There is more opportunity to customize to your interests, it felt easier, and more fun. There is also much more opportunity job wise.

The flaw in CS is that it does not teach good design, as code is never revisited after an assignment and is cheap to iterate on. In the real world code lives on, so a good architecture can have a huge impact. For EE/CE the initial costs are very high, so more thought it needed and there is an emphasis on learning from research. CS gave me the knowledge do be a programmer, while CE molded me into an engineer. So I found both valuable, with CS being financially better and CE more personal growth.",1528391697.0
the_PC_account,Short version. CS = programming. EE = circuits. CE = both.,1528406352.0
Bluffz2,"I did comp e. It’s kinda like a lot of the other engineering degrees with physics and calc 1-3 + diff eq’s. We also had to take some CS subjects like algorithms and data structures and OOP. Our main focus was understanding how a computer works from scratch, so we used a lot of HDLs, OS design, networking, etc. 

Our projects ranged from building a pipelined processor from scratch using VHDL and an FPGA, to building a web server from scratch in C, to general programming in Java. We also did network configuration. I took electives in penetration testing and algorithm optimization, but there are people who went even deeper into EE.",1528393718.0
,"MajorPrep made a video on it : https://www.youtube.com/watch?v=xDNB8BoV0KU

(would recommend most of his videos, he knows what he's talking about)",1528423795.0
MrBoof100,"I’m a computer science major in college and I can say comp sci is mostly the programming aspect. I believe computer engineering to be designing computers, building computers, etc. I think electrical engineering can be used to optimize the wiring and power consumption but also can be anything electrical, not just computers. Hope my input helps but I am just a college student. I highly recommend comp sci if you are good at problem solving and logic",1528387113.0
vomitHatSteve,"I studied comp. sci.

That major tends to be fairly broad. It covers a little bit of every component of a computer (how circuitry works, what goes into a CPU, how networking protocols work, how data is store in a computer, various models for representing that data and their effect on performance, software development methodologies, project management, and any number of programming languages).

Ultimately, it is meant to make you a *software engineer*. i.e. you can write code, and you can learn a new programming language quickly. But the real intent is that you write good, maintainable code that will run efficiently and can be fixed years later.

Electrical engineering is going to be much lower\-level, where you spend a lot more time on how circuits work. And as an electrical engineer, you'll be more likely to be designing *physical things*. You'll design circuits, which will then be built in the real world rather than focusing primarily on software.

I'm not super\-familiar with computer engineering, but I suspect it would fall somewhere in the middle. It looks like it might be essentially an EE degree with a focus on computers, which means you'll end up with more time spent learning algorithms and data structures.",1528388579.0
BrightLord_Wyn,"Comp sci. and comp. eng. both have a lot of cross over depending on the individual school curriculum. Comp. eng. is more math intensive. Elec. Eng. is a ton of math, and has very little to do with computers although you will of course be using them a lot.

Comp. Sci. traditionally is more theory of computing. Comp. Eng. is traditionally more application of computing technology.",1528389707.0
merimus,"Computer science is about programming computers and understanding algorithms.  You need to understand vaguely how computers work, but not necessarily be able to design a chip.  


Computer Engineering is about understanding how computers work at a very low level.  These are the folks who can design chips.  They also know some programming.  


Electrical Engineering is about building circuits.  resistors, inductors, capacitors etc.  


A vague (and someone wrong way of putting this is).  


An electrical engineers build the motherboard

A computer engineer designs the chip

A computer scientists writes the code which runs on the chip.  
",1528387580.0
cthulu0,"EE doesn't focus on computers. EE is a very broad field. Here are thing EE's do that are only tangentially related to computers or in don't relate at all:

-semiconductor physics/material science

-electromagnetic and power distribution

-control theory

-vlsi design of non-computer chips

-dsp

Very ELI5:

CS = math (related to idealized computers)

EE = engineering (related to electricity but not necessarily computers)

CE = engineering directly related to actual non-idealized computer",1528487384.0
bartturner,"IMO, if have the math skills then go Computer Science.   Otherwise Computer Engineering.

Here is a podcast that is focused on having a Computer Scientist and Computer engineering that touches on the differences.

https://www.programmingthrowdown.com/

They also have one episode dedicated to the difference.

Computer Scientist will also make more money on average.



",1528633832.0
the_PC_account,Short version. CS = programming. EE = circuits. CE = both.,1528406365.0
the_PC_account,Short version. CS = programming. EE = circuits. CE = both.,1528406372.0
token_white-guy,I have the 7th edition by James Kurose and Keith Ross if its helpful...,1528380794.0
LearnerPermit,Wow mind blown! He taught at the community college I went to in the early 90s. He always wore a nice camel hair suit.,1528399548.0
Bluffz2,"I have the international version, will that help?",1528380249.0
mnestorov,"I have one. I'm at work right now and the copy is at home, is it urgent?",1528375384.0
Maverick524,I think I may have it somewhere. When I get home I’ll look if no one else ya answered the OP. ,1528406391.0
cthulu0,"Well supervised deep learning is really high dimensional curve fitting.

But supervised deep learning is a subset of deep learning. And deep learning is a subset of machine learning. And machine learning is a subset of AI. So machine learning and AI in general are not curve fitting.

",1528487602.0
Inaccurate-,"[Code by Charles Petzold](https://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319) sounds like it is exactly what you're looking for. It reads more along the lines of a novel, starting at the very beginning (electricity, binary, etc) working up through all the parts of a basic computer (including digital logic, assembly, etc). It's as good as (or better in my opinion) than any undergraduate book on OS/Computer Architecture you'll find. And a much more enjoyable read.",1528316452.0
libelecsBlackWolf,Hennessy and Patterson's *Computer Architecture: A Quantitative Approach* is relatively an easy read and covers a broad number of computer architecture subjects like the ones you mention.,1528313887.0
chmaruni,[Ben Eater](https://youtu.be/HyznrdDSSGM) has a great playlist of 44 videos where he builds a whole breadboard computer. I love his series.,1528313872.0
starnakel,nand2tetris.org => free course about building a simple cpu from logic gates.,1528313178.0
PsychoI3oy,https://www.youtube.com/user/Computerphile,1528315081.0
teteban79,Andrew Tanenbaum's books definitely,1528316170.0
AcidDaddi,Computer Systems: A Programmer’s Perspective,1528346302.0
combinatorylogic,http://www.projectoberon.com/,1528361265.0
bdf369,"Clive Maxfield's books ""How Computers do Math"" and ""Bebop to the Boolean Boogie"" are fun introductions",1528392275.0
TheEdgesOfThePoptart,"Hey man!
As someone that had 2 internships at major companies, and got full-time offers from each. I would say that getting the most out of your internship is a mixture of networking (really getting to know your team on a personal level), and showing interest and progress work wise. I personally think that you should have designated days that you plan on going in and working the full day, but then give yourself a few days where you're purely work from home. The days that your in the office, really make an effort to talk to people, see what they're working on, and ask if you can learn from them. Then, on your work from home days, get your work done and read up on some of the other projects/technologies people are working on.",1528312769.0
BW40cle,"Your plan should be okay but definitely try to be physically there as much as possible, my previous internship became almost complete remote work and it made it nearly impossible to learn new things or make an impression, whenever I was there all I got were surprised comments that I even still worked there at all.",1528312855.0
evil_burrito,I advise against. You want to be in the office as much as you can. Out of sight can be out of mind. You want people to know who you are and you want the chance to impress them. You also want to learn as much as you can about the craft but also about how professional software development works.,1528313162.0
WarWizard,I'd definitely advise against being remote. It is going to be critical to be around so you can get help when you need it. You also don't want them to forget about you!,1528336478.0
roopieepoopiee,"Communication! I have interned remotely as well and just have had a full time offer after about a year. Email your statuses on projects, utilize screen shares, call people when you need clarification. It goes a long way.",1528317668.0
seasa_,"If it's a name brand company, and you're delivering (ideally, impressive) results, you should be fine. Definitely take advantage of the networking and building connections, but you don't have to live at the office for that.",1528316089.0
ConsenSys_Socialite,Working remote is a lot easier now with Zoom. It’s still nice to go in 2-3x a week too. See what works best for you and always communicate ,1528334098.0
kennethjor,"I would advise you to go to the office as much as possible if you have the opportunity, at least in the beginning. Get to know your colleagues. Seeing as it's an internship, you're probably looking to learn from them as well. Being in the office helps with that.

If you do work from home, discipline and communication is key. If someone doesn't hear from you all day it's easy for them to dismiss you as slacking off. And to be honest, our monkey brains are very distractable. If they know you, a rapour has been established, and they trust you, this becomes easier.

I've worked from home exclusively for nearly a decade now and I can tell you it's not for everyone. It's very easy to feel lonely and not part of the team, which is *your* problem and something *you* have to work to fix, hence why communication is key. If you have any specific questions about working from home, I'll be happy to answer. In broard strokes I'd say:

* Communicate with your team a lot - this is the third time I mention this, it's that important. Daily meetings to ""touch base"", chat channels like Slack, that sort of stuff.
* Have a dedicated work space, preferably a room that is just your office. Your bedroom or the kitchen table can also work, but this space has to be distraction free. This applies to both other activities you normally do in this room (playing games, eating, watching TV) and other people.
* Be firm with other people in your household. Just because you're in the house doesn't mean you have free time to deal with all sorts of random shit. Explain it to them. Many people don't understand that a 5 minute distraction can easily cause you 30 minutes of lost productivity. (Although this also applies to an office environment)
* And obviously you need good, reliable, and fast internet.

If any one of the above are a problem, you will fail, and I'd say you should drop the whole idea of working from home in the first place.",1528339982.0
Work_Account89,"Between internships and full-time jobs, I try and be in the office as much as possible. Even after 5 years physically being there has a huge advantage of being on the phone etc. Getting to discuss issues with people and work them out if needed. Also helps to get to know people which is great during an internship",1528357195.0
chocol4tebubble5,"In an ideal world, I'd go for it. However, internships are incredibly valuable and considering you want to work full time remote work might be a bit of a risky move. It's great if you are an established developer at a company with a many years of experience under your belt, but might make it difficult to learn and to be recognised as a valuable team member if new.",1528314666.0
Leblancwork,"I work as an engineer for a large teach company, do not ""Work from home"". It was become an unofficial way to allow employees to take care of household tasks laundry, chores etc. I would be the first one in the door and last one out this will be noticed. It stink however I would suck it up for the first 6 months and earn that right once you get the full time offer. ",1528376945.0
punkbastardo,make sure you switch off your idle notifications!,1528323978.0
singdawg,Suck it up and go full time. ,1528384675.0
hackingdreams,The time of a person who is capable of breaking said proof is worth *significantly* more than you're offering.,1528311095.0
plipplopplupplap,Why don't you just submit a paper to a nice conference ?,1528308441.0
sparcxs,"First, very nice write up, I look forward to rereading it and fully grokking your proposal. However, doesn’t « eventual » consistency violate ACID? The other thing that concerns me is the reliance on clocks. Transaction IDs are unique, timestamps are not, especially with distributed systems. When I think of an insanely busy SQL systems I think of stock exchanges where billions of transactions have to be correct. The odds of having a transaction occur at the same nano second is not insignifiant given a week containing 20 billion transactions. We’re looking at 160K transactions during any given second of the trading day or 160 transactions per millisecond.  It’s possible we’d have the exact same time stamp for 2 transactions no matter how granular we go. The other problem is clock drift is pretty significant when we get into milliseconds, let alone nanoseconds. That’s some food for thought, not sure if you’ll address that in another paper. Interesting stuff, keep it up!",1528312586.0
plgeek,"You should explain in the document how this system does not violate 

[https://en.wikipedia.org/wiki/CAP\_theorem](https://en.wikipedia.org/wiki/CAP_theorem) 

In the process you will discover the hole in you reasoning. A quick read suggests the clock you rely on is not partition tolerant. ",1528575628.0
teteban79,">how do I recognise a problem can be solved via max flow algorithms?

This is (in general) a weird question to ask. It sounds like asking ""how do I recognise mechanical issues that can be solved with a screwdriver?"". You would find that you can use your screwdriver in many imaginative ways to solve the issue, sometimes in very oblique ways such as banging a nail with the head of the screwdriver, a task more fitting to a hammer.

In the same way, many problems can be transformed in such a way that they *can* be solved with max flow algorithms, although it may not be the best, or the most natural way. In fact, the problem you mention can indeed be solved by a max flow, although it feels more natural to state it as a matching problem.

Of course, some problems do lend themselves to be modelled most naturally as max flow problems. How do we recognise these problems? I do not have a straight answer for this. In my experience, I would say practice and exposure. The more you work on problems, the more you see the pattern emerge as you begin to model it; you develop a feel for it. For max flow problems, if you can rewrite your problem as something like ""maximally assign X supply to Y demand"", then you likely have something that can be solved via max flows.

>Secondly and most importantly, how do I construct the flow graph to visualize the problem

Are you asking how you go from an informal, natural language description, to a formal model? Definitely practice, recognising details that can be abstracted away, recognising pitfalls that will make implementation difficult/inefficient, etc.",1528272939.0
dewayneroyj,Too many to list so check out this great article explaining the top 9: [Top Ethical Issues in AI](https://www.weforum.org/agenda/2016/10/top-10-ethical-issues-in-artificial-intelligence/).,1528273865.0
imtheone-,"The biggest issue is the possible consolidation of power between the government and major tech companies to build personal models monitoring all user activity. From conversations at home, to in the car, to web browsing activity at home. This personal model being sold and merchanted for advertising, prediction, and surveillance purpose.

Also, whether there should be more stringent laws on data collection. As of right now, most users agree to give away their data on smartphones which ends up being used for ML/AI purposes. This is not a conscious choice and therefore should be brought to attention if we want to talk about ethics.

Important ethical issues.",1528305844.0
khanh93,Oxford professor Nick Bostrom has a good [book](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/1501227742) on the topic.,1528310462.0
drvd,"Can/should  a sentient AI be forced to ""donate"" (here presumably as a copy) an ""organ"" (here e.g. some neuronal net) to other AIs? All details around such a donation (anonymity of donor/receiver, compensation, etc). ",1528360373.0
LukeB3,I’m interested as well!,1528295835.0
east_lisp_junk,I would be skeptical about feedback quality for a writing course offered in that format. You aren't going to get much actual professional attention on your writing if the instructor's attention has to be shared across a thousand students.,1528401527.0
Umbrall,"Because anyone can verify the proof of work themselves, the only way byzantine agents can gain control is just by controlling 50% of the servers.",1528268885.0
drvd,"It is not ""preventing"" such a failure in the sense of ""won't happen"". It just makes this type of failure either very costly or very unlikely.",1528360469.0
SepticPeptides,"The proof\-of\-work chain is a solution to the Byzantine Generals' Problem.  I'll   
try to rephrase it in that context.  


A number of Byzantine Generals each have a computer and want to attack the   
King's wi\-fi by brute forcing the password, which they've learned is a certain   
number of characters in length.  Once they stimulate the network to generate a   
packet, they must crack the password within a limited time to break in and   
erase the logs, otherwise they will be discovered and get in trouble.  They   
only have enough CPU power to crack it fast enough if a majority of them attack   
at the same time.  


They don't particularly care when the attack will be, just that they all agree.   
 It has been decided that anyone who feels like it will announce a time, and   
whatever time is heard first will be the official attack time.  The problem is   
that the network is not instantaneous, and if two generals announce different   
attack times at close to the same time, some may hear one first and others hear   
the other first.  


They use a proof\-of\-work chain to solve the problem.  Once each general   
receives whatever attack time he hears first, he sets his computer to solve an   
extremely difficult proof\-of\-work problem that includes the attack time in its   
hash.  The proof\-of\-work is so difficult, it's expected to take 10 minutes of   
them all working at once before one of them finds a solution.  Once one of the   
generals finds a proof\-of\-work, he broadcasts it to the network, and everyone   
changes their current proof\-of\-work computation to include that proof\-of\-work   
in the hash they're working on.  If anyone was working on a different attack   
time, they switch to this one, because its proof\-of\-work chain is now longer.  


After two hours, one attack time should be hashed by a chain of 12   
proofs\-of\-work.  Every general, just by verifying the difficulty of the   
proof\-of\-work chain, can estimate how much parallel CPU power per hour was   
expended on it and see that it must have required the majority of the computers   
to produce that much proof\-of\-work in the allotted time.  They had to all have   
seen it because the proof\-of\-work is proof that they worked on it.  If the CPU   
power exhibited by the proof\-of\-work chain is sufficient to crack the password,   
they can safely attack at the agreed time.  


The proof\-of\-work chain is how all the synchronisation, distributed database   
and global view problems you've asked about are solved. Resource: [https://bitcointalk.org/index.php?topic=8809.0](https://bitcointalk.org/index.php?topic=8809.0)",1528835971.0
jmite,"Why are you interested in the crossover between the two fields in particular?

My understanding is that they're very different. Bioinformatics tends to deal with sequences of discrete data, whereas computational geometry is usually dealing with the real numbers.

If there were a crossover, my guess would be in a link through machine learning somehow. Like, maybe there's a geometric clustering algorithm that is useful in bioinformatics?",1528241287.0
acousticpants,"protein conformation and protein folding may be of interest to you 
",1528259819.0
nickybu,"This seems pretty ~~small~~ interesting. Finally finish university in a few days so might just have to  enroll! Any idea whether all lectures will be available at the beginning or if it is a paced course?

EDIT: not sure where 'small' came from",1528255323.0
EpicWinningRob,"Sorry for the potentially silly question, but is this course online? From the UK and I'd love to join in",1528274874.0
anjelswhat,Just signed up! ,1528256874.0
IPvIV,Just binge watched/read the whole first section. Really fun and accessible! :D,1528420356.0
HelpfuI,"I’m currently doing courses to stay ahead of my classes. I’m only just starting classes relevant to my major so I’m doing cs50 and mit’s intro course thing.

You can look up ossu on GitHub. Someone compiled a list of free resources that is supposedly comprehensive enough to replace a degree, not just supplement one. I plan on doing more and more of it each summer, so that I can crush my classes in the fall.",1528220460.0
YourUndoing,"as far as Calc or Physics, I would probably first point you to [khanacademy.org](https://www.khanacademy.org/). It's a pretty well respected general education site.  

For the CS side of things, focus on the basics first. Data Structures & Algorithms I & II is generally one of the gate-keeping courses in most CS programs. Typically if you can't get beyond those, you aren't going to be successful at upper level courses.  

There are some basic computing courses on Khan Academy as well, but for more in-depth stuff, maybe consider buying a Pluralsight license if you can afford it. A lot of great video courses there but obviously this isn't free so if you are indeed only looking at free courses, Harvard, Stanford, and some other high level institutions have free online courses for a number of lower level prep courses. It looks like Stanford houses theirs on Coursera so you are probably on the right track starting with that.",1528220547.0
,literally in the exact same boat dude. Good luck to you!,1528218450.0
slotech,"[The Mechanical Universe](https://www.youtube.com/playlist?list=PL8_xPU5epJddRABXqJ5h5G0dk-XGtA5cZ) is basically CalTech's Physics I & II courses, as developed into a video series back in 1985. It's sort of the granddaddy of Open Courseware. Don't worry, the subject hasn't changed all that much in the last 30 years; physics is still physics.  

If the pacing feels a little slow, you can always use the YouTube controls to notch it up to 1.25 or 1.5 speed.  The computer graphics were JPL's ground-breaking finest at the time, but have more of an 80s Phong Shading nostalgia to them these days.

I watched the hell out of these on cable, back when TLC was actually The **Learning** Channel.",1528227036.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1528161058.0
johnny_logic,"[Computer Science Distilled: Learn the Art of Solving Computational Problems](https://www.amazon.com/dp/0997316020) ($20.89 at time of posting) is a high-level conceptual overview of much of computer science, but it is pretty shallow. To get deeper while still being broad, you may want to read [Computer Science: An Interdisciplinary Approach](https://www.amazon.com/Computer-Science-Interdisciplinary-Robert-Sedgewick-ebook/dp/B07CGYG6YK) (cheating a bit, but it is $9.99 on Kindle). For a language-specific *tour de force* read [Structure and Interpretation of Computer Programs](https://www.amazon.com/dp/0262510871) ($47.09, or [free online](http://web.mit.edu/alexmv/6.037/sicp.pdf)). Finally, for a fun, episodic overview, see [The New Turing Omnibus: Sixty-Six Excursions in Computer Science](https://www.amazon.com/New-Turing-Omnibus-Sixty-Six-Excursions/dp/0805071660) ($29.08)  If you are referring to a specific theoretical sub-discipline (e.g. Theory of Computation), I might recommend something else. ",1528166075.0
jx4713,"Try this book: [Computer Science Illuminated!](https://www.amazon.co.uk/Computer-Science-Illuminated-Nell-Dale/dp/1449672841).

It is described as ""n engaging breadth-first overview of computer science principles and provides a solid foundation for those continuing their study in this dynamic and exciting discipline"".",1528627477.0
chaoticgeek,This is just a post of books with affiliate links. ,1528166988.0
dixieStates,"Please define the term ""Modern Programmer""",1528171433.0
easeypeaseyweasey,"https://fivebooks.com/best-books/programming-computer-science-ana-bell/
Here is 5 books recommended by Ana Bell MIT lecturer who did most of the lectures of 6.001 Intro to Computer Science on MIT's opencourseware.",1528181588.0
Zulban,"A great way to start is by writing documentation. Find some code that is a mystery to you, figure it out, and document it well for the next person. This is also a great way to learn how the project works so you can write code later. It's a harmless way to actually help, and learn how to submit code to that community.

Choose a project you like and use. Find a problem or feature that you'd actually like to solve/have \- then find people talking about that problem and feature and ask how to help.

I'd include a video I recommend too, but I can't remember what it was from.",1528136196.0
,"Long time open source contributor here. 

Start small. Fix bugs and other low hanging fruit. 

Be persistent in asking for feedback without being an ahole.  Reviewers are often busy, so make sure you have your PR or patch well formatted and documented 

Respond to reviews quickly, effectively and without ego. 

Pick a project that interests you like Apache commons math or similar libraries.   

Most established projects have contribution guidelines. Follow them. 

Write tests that show the problem and it’s fix. 

",1528191791.0
Alexsandr0x,"I have the same struggle and I found some ways to solve this.

- Talk to people that already help open-source projects of your interest, personally I solve this going to technology events where these people were talking about your projects.

- Start report Issues, is a great idea to report a bug through posting issues.

- Check if the project that you are interested to help have a good bug tracking on github, example is sklearn that have a tag for ""good first issue"".",1528136724.0
imstarlordman,"Piggy-backing on this question, I found a very small project on Github that I would like to fork. However, it seemed to be a very informal project. I want to know if I will have trouble if I fork it given that it never gave a license. Will I have to explicitly ask permission from the guy who coded it?",1528158082.0
bart2019,"How do you start? That's easy. Find an open source project that you like to use yourself. Sooner or later you'll  stumble over a bug. Try to fix it. Submit it to the project. You'll become more familiar with the source, and you'll find fixing bugs easier over time. And, before you know it, you'll be a regular contributor .",1528178923.0
darrenldl,"If you have a heavy math background, I'd recommend looking into functional programming(some popular ones are Clojure, OCaml, Haskell, Scala, Erlang), and possibly formal methods if logic happen to be your forte.

Practically any language will do, but I would suspect you enjoy functional programming more than other paradigms as you can translate most math ideas relatively intuitively into code.

Also remember you can choose to contribute to the open source community rather than just open source projects, namely by starting open source projects yourself.

Now I'm not sure which would look better on CV, but I don't see why either one is better than the other, and starting your own projects is usually easier to begin with. Do expect the very first few ones to not succeed as well as you'd like, however, but put them on your CV regardless.",1528194413.0
reepha,"In my opinion, your first attempt at contributing to an open source project should involve an open source application that *you* regularly use. Think of a minor feature or convenience that you would like to see and dive into the source code and try to implement it yourself. Even if you aren't able to figure it out, you'll learn a lot just by trying.",1528250638.0
kdnbfkm,"If you are doing this for resume reasons try applying for positions as a job, or unpaid internship. Like Google Summer of Code and others. This sort of thing is bad for the industry perhaps but you might as well go to open source job interviews and go the whole route with one of the bigger OSS foundations... :/ Try the GNU foundation! :p

I should have read your post better. Yes, it is absolutely normal to be useless. You can know enough to answer 80% of the questions people in forum/chat have and still be utterly useless to contribute in a technical way. :/ The other contributors to little projects know alot and even they are next to useless compared to the true expert the founded/currently leads a project!

Depending where you're coming from the uselessness can partly be from workflow issues and hold ups, like not having a GitHub login if the project is hosted on GH, or being unable to install certain SDK and a thousand other little things... And even then those junior contributors are 3-10x better at coding and debugging than you too. And the project lead puts them to shame. But there can be a thousand little issues to clear up before being loser level. :/",1528137436.0
Wulfnodh,"Honestly, padding your CV  with open source projects is probably the most difficult path you can choose, but if you are serious, learn C and then simply find a GNU project that interests you.",1528143962.0
flexibeast,"i'm curious \- when he's not dismissing type\-oriented programming as ""the dark path"", does Uncle Bob actually have some useful advice? ",1528121086.0
combinatorylogic,"Fuck it. The very first one - this bullshit ""clean code"" from that retarded self-styled ""guru"" Uncle Bob. Nobody should ever read this pathetic crap.",1528121549.0
WhackAMoleE,Zen and the Art of Motorcycle Maintenance.,1528134759.0
DevFRus,"I really liked this article and think they are advocating for a good idea. Journals contribute very little to computer science and chasing glam journals like Nature can be detrimental to computer science work. Does anybody know good CS papers that were published in glam mags? Most that I know are awful. 

We should avoid introducing the current broken academic publishing system into fields that have managed to avoid it. This is why I don't understand people in CS who want the field to move away from its current focus on conference; they are much better venues than glam mags, although of course they are also not perfect.

That being said, as a junior researcher in cstheory who works at the interface with biology (which relies completely on the current broken academic publishing system), I am not sure if I have the courage to boycott submitting my work to glam mags. Of course, blogging, posting to ArXiv/bioRxiv resolves part of the problem (i.e. access), but it doesn't address the structural problem of prestige-chasing & authority-chasing that fuels the publishing industry.",1528093390.0
Softillicus,"The current academic system heavily rewards publication quantity over quality (quantity is a more reliable way to build brand-name recognition).  As a result, prestige publications and ""glam mags"" play an outsized role how research is conducted.  This is true across every discipline, including CS and the sciences, and as a result, quality research is often buried beneath hundreds of pages of low-effort, low-quality papers that are just meant to appease publication quotas and other administrative nonsense.  Dismantling this broken system in favor of an open exchange would be _great_ for researchers.  The problem is that universities aren't run by researchers, they're run by bureaucrats.  Substantive forward motion by the sciences will require not just boycotting and dismantling this broken publication system but also reforming the culture that pushed us towards it in the first place, the culture that values name-recognition over actual results.",1528138989.0
OutrunPoptart,"> The public already pays taxes that fund our research. Why should people have to pay again to read the results?

Let me stop you right there. Let’s not kid ourselves and think that the lay public could even begin to gain a rudimentary understanding of just the abstract of any publication in *Nature.*",1528143063.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1528071337.0
FinxterCom,I would also recommend Andrew Ng's [Coursera Course](https://www.coursera.org/learn/machine-learning). He is the ML god.,1528048223.0
TechnoBureaucrat,"Is machine learning likely to ever become commonplace enough to be a universally useful skill to have, or is even expected by employers? It seems to be specialized enough nowadays that I don't see the purpose of a course like this unless it's literally just to scratch the curiosity itch.",1528054183.0
Phreakiedude,Is it worth it?,1528042163.0
pandeykartikey,You can also try course on fast.ai that is also nice.,1528092351.0
usulio,"Couple points -- first, n choose n/2 is very close to 2^n . You seemed to make a whole bunch of arithmetic steps to try to get around this, including using some approximations in one place (x! is about x^x ) and then claiming the same thing is an inequality later. Just use (n choose n/2) ~= 2^n and stop there. See more: https://en.wikipedia.org/wiki/Central_binomial_coefficient

Second, some NP-hard problems do have sub-exponential time solutions, by which I mean solutions faster than 2^n when the input is of size n. But these solutions aren't polynomial time. See more: https://cs.stackexchange.com/questions/9813/are-there-subexponential-time-algorithms-for-np-complete-problems",1527999199.0
GandhiNotGhandi,"Most authors use ""subexponential"" to mean something stronger than anything less than 2^n time. The most common definition is the intersection over all epsilon > 0 of 2^(n^epsilon) time. This means showing that for all epsilon > 0, there is an algorithm that runs in 2^(n^epsilon) time. Other authors define it as 2^(n*epsilon) time for all epsilon > 0, or equivalently 2^o(n) time.

See [Wikipedia](https://en.wikipedia.org/wiki/Time_complexity#Sub-exponential_time) and the [Complexity Zoo](https://complexityzoo.uwaterloo.ca/Complexity_Zoo:S#subexp) for definitions.",1528036143.0
TomvdZ,"> Does this mean there's a sub-exponential algorithm to an NP-hard problem?

I don't think it is, but even if it were, that wouldn't be anything special. Many NP-hard problems have subexponential algorithms. In fact, for any c>0, there exists an NP-complete problem that can be solved in 2^(n^c) time. You can create such problems by artificially padding the input of an existing NP-complete problem, but there are many ""natural"" NP-complete problems with subexponential running times. Many NP-complete problems are still NP-complete when restricted to planar graphs (e.g. vertex cover or independent set, not clique obviously) but these can be solved in 2^(sqrt n) time (where n is the number of vertices).

Having subexponential algorithms for NP-complete problems doesn't really conflict with our current understanding of P v.s. NP. ""Subexponential"" isn't the same as ""polynomial"".

And FYI, if you take the sum of all binomial coefficients (k choose n) for k=0,...,n you get exactly 2^(n).",1528008997.0
GNULinuxProgrammer,"> I'm told that n^n grows faster than n!.

Well ith factor of n! is n-i but ith factor of n^n is n and clearly n-i <= n  and both products have n factors; so n! <= n^n. In computer science it's not a good habit to take things for granted, you should be able to verify things people tell you.",1528040391.0
nealeyoung,N choose N/2 is about 2\^N/sqrt\(N\),1528033335.0
DevFRus,"You're already gotten answers to why this is not subexponential, but I just wanted to show you how to do step 7 to 8 without graphing calculator: ((n/2)^(n/2))^2 = (n/2)^n = n^n / 2^n

Now plug that into your original: n^n / (((n/2)^(n/2) )^2 ) = n^n / (n^n / 2^n ) = 2^n

Of course, the more important thing is to learn that subexponential doesn't mean one step less than 2^n but o(2^n^\epsilon ) for arbitrarily small but constant \epsilon (or sometimes, more restrictively as 2^o(n) ). Note that you haven't established either of these.

Edit: tried to fix all the silly mark-up mistakes, hopefully I got them all.",1528127974.0
NotInUse,"The language categories are both incomplete and with the overlap poorly specified.

For older people when computing was stupid expensive your access was defined by schools or companies so either your major or your job defined what you could access instead of the other way around.  Simple correlation of category and job won’t capture this.",1527995754.0
,[deleted],1527996887.0
Nerdlinger,I’d guess it correlates far more strongly with age. ,1527991933.0
,"""You need permission

This form can only be viewed by users in the owner's organization.

Try contacting the owner of the form if you think this is a mistake.""",1527988205.0
kushangaza,What about functional languages? How would you classify SML or Haskell?,1527989865.0
theZcuber,"1. Markup isn't a programming language. It's called that for a reason.

2. _Many_ languages fall into multiple categories.",1527998342.0
RichZK,"No

You will learn whatever language the company who hires you is using. The programming language you are most proficient in may correlate with the job you get. However, the way you think and your problem solving ability is what is mostly considered when an interviewer gives a yes/no to hiring you. ",1527988757.0
Lieeefe,Assembly. Started from the bottom now we here!,1527995952.0
coderag,"Yes it does. I work in [ASP.NET](https://ASP.NET) during office times. In my home, I work in Arduino and Raspberry Pi. The first programming language I learnt was QBASIC which helped me to learn C programming during my Bachelors. The C Programming I learnt helped me to learn C\+\+, Java, C#, JavaScript to name, but few. Therefore, the answer to your question is yes it does \-\- not directly but eventually.",1528008612.0
raghar,"Crappy websites in PHP -> small CLI C programs -> small desktop Java programs -> bigger desktop Java programs -> C++ desktop programs -> Scala backend programs

I'd say that the fact, that I my first language correlate to my current job (something web-related) is more of an accident.",1528029161.0
abathologist,What a weird list of language categories. What decided that breakdown?,1528034522.0
ldpreload,"First language was GW-BASIC. Then HTML, I think. Then Visual Basic (before .NET), then C++, then Java. Now I'm mostly Python and C professionally, Rust on the weekends.

_""It is practically impossible to teach good programming to students that have had a prior exposure to BASIC: as potential programmers they are mentally mutilated beyond hope of regeneration."" -Edsger Dijkstra_

I think he was wrong.",1527991896.0
IAteQuarters,You might run into the problem that a lot of people might have learned object-oriented first so you might need to adjust for that when you do your analysis!,1527992397.0
iKnowSwift,"Nope. My first three languages were Java, C++, then Python. Now I work with Swift everyday.",1527995015.0
thephotoman,"I started in calculator BASIC.  I couldn't make a job out of that. 

Picked up Python to do number crunching in college.  Then picked up C, C\+\+, Java, and C# for my CS degree.  Now I'm working in the code mines, slinging my AbstractSingletonProxyFactoryBeans around.",1527996537.0
ztherion,"Does TI-BASIC count as a first language, since I wrote simple 15-line programs in it? Or Game Maker, since I made a Galaga clone once? Or Python, since I screwed around with it as a preteen? Or Bash, since I learned how to use a command line as a teenager? Or Java, the first language I had any formal training in?

Is Python procedural, object-oriented, or functional? I've used all three depending on the context. Functional isn't even in the list.",1528005533.0
swxxii,Functional - Haskell,1528009250.0
starnakel,Logo. Teaching history now.,1528013557.0
Araneidae,"Think you should have added a question about age, it's going to affect the styles of languages people learnt first.",1528014257.0
InnerChutzpah,"First programming language that I did real work in was Perl 5. I am a consultant in ML/AI technologies. So,.....yes. Kind of.",1528016686.0
brettmjohnson,"My first programming language was Fortran 4 in 1975. Yes, it correlated to my eventual career, but I never used it again. ",1528024938.0
Capitan-Fracassa,Fortran 77 and one of the first programs was OCR for numbers only. Today R is the language for my professional needs.,1528031196.0
NPVT,FORTRAN BASIC Assembler!,1528031270.0
jna1310,"Newbie here.  Started with Swift as a means to make a little cash so I can finally quit the old 9-5 and dive into programming full time.  Future “career” will be entrepreneurial, working 80+ hours a week for myself instead of working for the man (“9 to 5” is a gross understatement in many cases).  Swift led me to web dev and ultimately JavaScript, based on my need for a simple web platform to accompany & enhance mobile apps.  Thinking of learning Python next, for building more complex/scalable apps in AWS.  Would love to hear if anyone else started with Swift, and what path you’ve taken from there.",1528031976.0
chadwickofwv,"Batch file scripting, so yes I still use it on occasion.",1528043268.0
arkrish,This will not make much sense unless you have a narrower scope of experience. Some people have been coding for 3+ decades when many languages were not yet created. ,1528043712.0
dbdemoss2,"If you’re a kid and know multiple languages, just shut up. This isn’t for you. ",1528032895.0
MacStylee,"\(FYI : No. 

First lang was Java and C. First job was Perl.\)",1527990665.0
BulbaBooty,"1. there are 110 opcodes, so you need enough bits to store all 110 - this is 7 (as 7 bits stores numbers from 0 to 127)

2. there are 8 registers - you need 3 bits to be able to specify a register (2^3 = 8)

3. the storage space address is everything left- 32-7-3=22

4. since there are 22 bits in the address of the storage space, the size of the space is 2^22.


5. this question is not specific to the described system. it's just asking what the largest integer you can store with 32 bits is - this is always 2^32 -1.",1527948497.0
TheApadayo,"i. You need to be able to represent 110 distinct values for each opcode so you need ceiling(log2(110)) = 7 bits because with 7 bits you can have 128 distinct values and you can’t use partial bits

ii. Same as above but with 8 registers which uses 3 bits with no leftovers

iii. You started with 32 bits and used 10. 32 - 10 = 22 bits remaining in the instruction for memory addresses

iv. This one is slightly ambiguous as it doesn’t specify if the processor is byte or word addressable. You can use 2^22 memory locations so if it’s byte addressable (which is the given answer) you can access 2^22 bytes or 4MB. If it’s word addressable (can only access memory locations aligned to 32 bits = 4 bytes) then it would be 16MB because each memory location refers to a distinct 4 byte block of memory. 

v. If you have a n bit word then the max value it can represent is 2^n - 1 because it’s can represent 2^n distinct values but 0 is one of them hence the -1",1527949146.0
iheartjunghwa,I just had this course as well. Go over assembly real quick and you’ll see these answers :D,1527948522.0
kdnbfkm,"If you were trying to squeeze those opcode and register bits together it won't work. 110 instructions * 8 registers = 880 inst,reg combinations and 2^9 is only 512, so it still takes 10 bits. No compression tricks there (unless at least some instructions didn't use registers... you *could* maybe add a flag to some instructions like ARM does for carry)",1527955319.0
,"No, they don't mean the same thing. The first sentence looks like this in propositional logic:

(1) ~(R v H)

And the second sentence looks like this:

(2) ~R v ~H 

By DeMorgan's laws, (1) is equivalent to ~R * ~H, while (2) is equivalent to ~(R * H).",1527898299.0
GayMakeAndModel,"Jan is rich or happy

Edit:  Jan is not rich and Jan is not happy.  The negation is what I stated

Edit:  The negation of that (original proposition) is Jan is poor and sad.",1527897014.0
throwawayplsremember,"In this case, Jan is an element of R’ and H’

R’ intersect H’ will be where Jan is located.

R’ intersect H’ = (R union H)’ <- DeMorgan’s law

Your second statement about Jan looks more like: (R’ union H’)

So, no. These two sentences does not mean the same thing.

Btw i used the prime notation to denote complement sets",1527975286.0
Sack_of_Fuzzy_Dice,"...interesting...I have no idea what it does.

But take my upvote regardless!",1527873181.0
JonNaco,"this looks neat. anyone mind to eli30?
",1527874067.0
GRelativist,Neat application.,1527895595.0
TaipanRex,Github page: https://github.com/TaipanRex/visgraph_simulator,1527864662.0
p_pistol,"This is (also) a navmesh.

https://en.m.wikipedia.org/wiki/Navigation_mesh",1527899058.0
jx4713,What do you mean by CTF?,1528137898.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1527835834.0
pemungkah,I remember reading the original article in Scientific American and using a Scrabble board to run simulations. (No computers for middle-school students!),1527828051.0
makaaz,Shouldn't medium be banned for being behind a paywall now?,1527833821.0
2lazy4forgotpassword,"I'm trying to get through MIT's distributed systems 6.284 (https://pdos.csail.mit.edu/6.824/schedule.html). I managed Lab 1 completely (with little hints from already solved solutions on github). But I'm not able to make the basic Raft implementation in Go for Lab 2. I don't want to cheat using github for that as well, I won't learn anything that way. 

I'm damn frustrated. I've spent >1 week trying to get something running, first few sessions I tried writing code, now I'm just up against a wall. How should I learn/implement Raft? I've seen this link http://thesecretlivesofdata.com/raft/ too, but implementing it according to spec is still difficult for me. Any tips?",1528014322.0
SuitableArk,"More than a million??? I have like, five. ",1527825514.0
wapxmas,"Are you getting PhD?
",1527778081.0
teteban79,"Can you clarify the question? I don't understand what you refer to as ""pumping lemma automaton"".  Maybe you have a different wording of the lemma in mind; I usually refer to it as that, for a language to be regular, any sufficiently long word must be able to be pumped on its middle section and still be a word of the language.  In your example this is easily satisfied as long as you pick a middle section that doesn't straddle the ab boundary.

By the way, the grammar you propose forces at least one 'a' and two 'b's, which is not exactly the a^(n)b^(m) language, even if under the notion of zero being out of naturals",1527767701.0
maladat,"The pumping lemma only works one direction.

If language L is regular, then it can be pumped.

That a language L can be pumped does NOT mean that it is regular.

https://en.wikipedia.org/wiki/Pumping_lemma_for_regular_languages#Converse_of_lemma_not_true

If you want to show a language is regular, then write a regular expression (a regular expression in the mathematical sense, not in the ""something a programming language calls a regular expression"" sense - in general, those regular expressions aren't actually regular expressions), deterministic finite automaton, or nondeterministic finite automaton that recognizes it. (In this case, a regular expression using the most basic definitions would be ""aa\*bb\*"".)

The DFA would be something like this:

 -->[q0]--- a ---->[q1]---- b ---->[[q2]]

With the addition (that I can't draw in non-monospaced ASCII) that q1 has a self-transition on ""a"" and q2 has a self-transition on ""b,"" and q2 is an accepting state.",1527793814.0
helpusobi_,Solved,1528549384.0
shaggorama,Holy shit that Timecube article,1527783610.0
VermillionAzure,This is the best in-joke that I've never known about.,1527820355.0
RagingDoug,"I dont know where you're going to school, but you should take both. ",1527746653.0
Meguli,"There are many good online ML courses but not many great computer architecture ones. If your prof is a good teacher, go with computer architecture. ML is a fad anyways.
",1527773150.0
remy_porter,"Oh, a wiseguy, hunh?",1527721966.0
Madsy9,"Like others have said, there might be some gatekeeping going around, but there is also a clash of cultures. People with academic backgrounds might expect everyone else to follow etiquette from academia, which discourages:

* Intellectually lazy questions, low-effort, not putting in effort yourself
* Asking about a topic way beyond your expertise
* Not respecting people's time
* Bad grammar, bad punctuation
* Asking people to do the job for you (such as asking for sourcecode or the whole solution to a homework problem)
* Plagiarism

The points above are often a part of social norm etiquette in academia and they work well there. You ask peers, professors and assistants for help, but you have put in the work yourself to make it easy for them to help you and you respect their time. But people outside of university are not hard-wired this way and so a clash of cultures happen, where people have different expectations.",1527729766.0
mcgrotts,"IDK, I've asked plenty of C# and the javascript questions and either got some decent feedback or no feedback.  Granted more often than not I would answer my own question.",1527718209.0
Voidsheep,"I've seen several posts like this, but they never contain a link to the actual question with rude, hostile or disrespectful answers, so it always leaves me wondering if the quality of the question was any better.

Anyone can contribute in SO and it's community-moderated, so it would be a miracle if someone didn't act bad in there every single day. You can't expect random people to always act nice, but it can still be a very valuable resource.

However, you've also gotta understand the site is under a constant flood of garbage questions, with absolutely no effort out into solving the given problem, it's evident by following the new questions pool for a bit. Questions that literally cannot be answered, because they provide no relevant context or code. Questions that are duplicates of each other and questions where someone didn't even bother to check the library documentation before asking.

For all that to be filtered out, the moderation has to be pretty fast and loose, with some good questions unfortunately taking collateral damage. But it's free and nobody gets paid to answer the questions, so it's pretty much take it or leave it. The best and *only* way to improve it is contributing yourself.

I think SO works best when you don't treat it like problem outsourcing system or personal assistant, but rather put in the effort to isolate the problem you can't solve and provide a proper example. Ideally something  you can actually write a test for, so someone can explain why it fails and how to make it pass.

Besides, when you isolate your problem into an example stripped from your original context, there's a good  chance you figure it out in the process.

It's easy to criticize free community-driven services and open-source projects, but they still tend be pretty damn good value for money and the people complaining rarely contribute to make it better.",1527748369.0
3lRey,A lot of the CS field is like this. Most think they're some wunderkind super genius because they learned how to use a computer. It doesn't help that those fields pay as much as they do in the states. The cash and high demand get these people thinking they're high society super brains. Don't think about it too much and just take it as generic human spirit. ,1527711131.0
balefrost,"So just to be clear, this is not the best sub for this topic. From the sidebar:

> Self-posts and Q&A threads are welcome, but we prefer high quality posts focused directly on graduate level CS material. 

SO doesn't exactly qualify as ""graduate level CS material"".

This would be a fine post for /r/programming, /r/askprogramming, /r/learnprogramming, and probably others.

---

SO takes a hard stance on duplicates. If you ask a question that duplicates one already asked, you should expect your question to be closed. But the person who closes it should also provide a link to the existing question, which will hopefully have a relevant answer.

It's a mistake to think of SO as a site to help beginners learn to program. It *can* be useful to beginners, but it's certainly not a discussion forum and doesn't really cater to the needs of beginners. ",1527726991.0
Deipnoseophist,"Really glad you posted this.
I asked a question there for the first time last week. I felt I spent a good amount of time researching my question, I phrased it well, showed examples of attempts at the problem and it definitely wasn’t a “help me with my homework” question. 
I was instantly down-voted into the negatives with snide comments.
There was one guy who was genuinely trying to be helpful but the rest of the users on there made me feel like an idiot. I genuinely thought I had done something wrong.
Happy to know it’s just a toxic place.",1527715531.0
BenjiSponge,"I'd like to posit that this is simply untrue and that you're experiencing confirmation bias. You'll see snobbishness and arrogance in any field.

I'm not sure why SO gets all the hate it does. Are there assholes on it? Sure. But I don't think I've ever gotten a net negative response on any question I've ever asked, or really ever seen, and I've been going on there for 6+ years, maybe even more so recently.

Maybe you're expecting too much from people donating their time willingly to help you?",1527714811.0
interputed,The smartest people are usually nice. Only the idiots have an inferiority complex.,1527732663.0
MyRobotDidIt,I think it's better on the amateur coding forums. ,1527730853.0
Raknarg,Some of us have incredibly large egos.,1527733350.0
Avir94,"I know exactly what you mean.. I asked a Perl question once and this gut literally told me ""You shouldn't be programming if you don't understand this."" amongst other a$$holy comments from him. I was quite #triggered after that.... ",1527743283.0
Yeamf93,"It's not just on Stack though. Most programmers I meet, especially ones that have more knowledge than most but are not yet experienced (when you are a master, you won't need to brag about anything - but when you're on your way to mastery : ugh ) : They seem to be arrogant and snobby. Just yesterday, one of them was checking out a code someone else had written, and I could hear him say things like, ""I know this code is wrong. 61 lines? I did it in 7 lines. This is so stupid. Where did you find this guy? This must have a bug."" Then he found some sort of issue with the code. ""I knew it. I know bad code when I see one. I love finding bugs in other people's code. It's shit. clearly had no idea what he was doing. I can do this in my sleep. It'll take me 1 minute."" 

The person who wrote the code is an intern who's just learning and was sitting right there!!!! (and the arrogant fuck knew it!) 

Funny thing is, a senior dev checked the code and said that it was fine (working as it should have) but that with experience, it would become a lot better. It was buggy, but generally, a very well done. 

I hate programmers. ",1527749079.0
nom-de-reddit,"Once upon a time there was a blog by Joel Spolsky called Joel on Software. Joel basically created the idea of “rockstar programmer”.

Joel was eventually one of the founders of Stack Overflow, which attracted many of his followers, who all viewed themselves as rockstars.

If you go there and ask a question, you are obviously not a rockstar, so you are scorned by those that see themselves as rockstars.

Normal people there just answer questions, but there are still rockstars that scorn them, too.",1527732276.0
,[deleted],1527710733.0
flaming_goldfish,"SO is notorious for being a toxic or hostile environment. There's a great article on it by April Wensel:

[https://medium.com/@Aprilw/suffering\-on\-stack\-overflow\-c46414a34a52](https://medium.com/@Aprilw/suffering-on-stack-overflow-c46414a34a52)",1527712870.0
__GG,"The site doesn't allow pleasantries, it is just question and answer. It gives it a very dry and unwelcoming feeling. That and poor social skills. Developers don't often have to think about how to foster good relationships. Basically, you have people trying to gain points on a site that requires dry human interaction. There are also a lot of rules around posting and the dryness means people can't really sugar coat the mistakes.",1527744345.0
samofny,Some reddit subs are like that.,1527735733.0
azvibe,"StackOverflow aren’t snobby and arrogant..  They have very limited time to answer questions and if 1) the question and answer exists elsewhere on the site or 2) if you’re just looking for the answer without even trying to look like you’re figuring it out for yourself, you’re going to get roasted alive.",1527730808.0
,Does anybody know a better website?,1527712926.0
blunderboy,"I have been active contributor to stackoverflow questions/answers. I also felt the same thing multiple times and even today after 4\-5 years, I sometimes ask a rookie question and it gets closed \- sometimes without justification or rude response.Recently it happened again last week but that's okay. We need to understand the rules of the garden and play it accordingly. But if you think from the contributor's perspective, it is actually right to do sometime. StackOverflow managed to grow such a huge community because the community and moderators kept this place neat and clean. A garden of beautiful questions and answers only and Every other wild plant \(or non\-flower plant\) is unrooted.

If community and mods are not strict, the following may happen \-

1. Imagine you search for a question and land on stackoverflow just fo find out the question was poorly asked and nobody replied because nobody understood the question. That rarely happens right.
2. Imagine you land up on SO for a question which does not have complete details so that is completely irrelevant for everyone. Will that be helpful ?

Rest I do agree some folks on SO have rude behaviour and probably that's because of their job position, degree or arrogance on their knowledge in a particular area.",1527747387.0
pecp3,'Why is everyone generalising?' ,1527749814.0
theZcuber,"If you follow the asking guidelines, your question will almost certainly not be downvoted immediately. If you disregard the guidelines and post without a minimal example and/or what you've tried, you're going to get downvoted.",1527727458.0
jkuhl_prog,"Stack Overflow, I've found, is a mixed bag.  Sometimes you get good help, sometimes you get condescending morons who get on your case because it's a ""duplicate"" of a post that was made in 2012 (lets just ignore the fact that said code is now obsolete for being 5 years old . . .)

I mean I get the point, they don't want to answer the same questions over and over and over again, but they go over the top with it.  I'm glad the devs at SO have recognized that there is a problem with how newcomers are often not well received, but I wonder what they'll actually do to address it.",1527727266.0
beyond-antares,"I've actually had a very good experience on stackoverflow and all it's sub sites. Even from the most basic questions such as setting up github all the way to bug fixing of IDE's and package extensions. 

Have you noticed a trend in arrogant advice? What topics are you generally asking questions for? ",1527752220.0
SlimeyJusticeWarrior,"Maybe they're not dicks. Maybe it's YOU who are a dick coming with your expectations to some community. Try putting more efforts into questions and not ask something stupid that can be googled in two seconds.

IF SO was so shitty, there'd be no people participating. ",1527764737.0
Microscopian,"I have got many great answers from Stack Overflow. If you think you're getting bad answers, report them to meta.stackoverflow.com; there's a link right on the question page.",1527726451.0
lofi91,"Also, from my experience so many people in CS can be placed on the Autistic spectrum. Not saying that CS guy/girl == Autistic, but the traits are certainly more prevalent in our field. Those traits crossed with the logical and deterministic subject nature can breed an approach that some may consider rude.

Others just like intellectually dick waving for the reasons stated below...",1527729194.0
not_porn_throwaway,"these are mostly people who have only ever been ""very good"" at this one thing, so they tend to give it a pretty central spot in the things that make up their person

stackoverflow also ended up in a spot that encourages this top dog behavior",1527731059.0
classhero,"So don't use it. What's the point of *your* flamebait post? You're not asking a question, you're looking for confirmation (which you'll get real easy, given that you're on reddit). ",1527739789.0
AaronKClark,It's the Dunning Kruger effect.,1527725001.0
alvahrow,YES thank you!!! ,1527728964.0
drvd,"Maybe this is a common misunderstanding. Votes on questions do not rate how relevant/hard/important the question is for the one asking the question but it rates how useful this question and a high quality answer probably will be for programmers in the years to come.

SO is trying to curate important, long lived questions with high quality answers.

A syntax error in line 367 or a build failure for v17.3.2.7 on OSWhatever might be troublesome for the poster and he/she might have done all the research but this is often not upvoted. Other places are much better to ask such things.

Some question on SO are ""rude"" to the ones offering their time (and/or their employers money) to answer the questions. You cannot get blockchain-XY to build on you machine now I should spend time to debug your build? You obviously never read the official library documentation or any halfway decent blog post about this library and you run into the deadly common problem explained in the official documentation, the project FAQ and every blog post but better ask on SO stealing someones times? That's rude too; even for a novice.
",1527744478.0
FoeHammer99099,"If you didn't think the people who frequent Stack Overflow are better programmers than you, why did you come to them with your programming question?  Obviously that's not license to be rude, but these people probably are better programmers than you.  

That's mostly due to experience. You're probably a high school/college student with a year or two of formal CS/programming education under your belt.  The people answering questions on Stack Overflow tend to have years of professional programming experience.  It gets really easy to forget that basic programming concepts aren't super obvious things that everyone knows.   

Stack Overflow also has rules about what kind of questions you should ask (Programming questions with specific answers, that are general enough to help others in the future), and when you should ask them (after attempting to solve the problem yourself, and looking for an answer on SO and elsewhere). Many people don't read those rules, and the people who spend time answering question on SO generally feel like those people are wasting their time.  

Furthermore, most people on SO aren't rude.  The majority who just ask & answer questions (Disclaimer: I answer questions on Stack Overflow, hopefully without being rude). If you do see someone being rude, you should flag that for a moderators attention, and raise the issue on Meta Stack Overflow if you feel like flagging isn't enough.  (That assumes you have the rep to do so, which is it's own problem).",1527733770.0
xworld,"Honestly, I always feel comfortable and treated respectfully in stackoverflow.",1527724494.0
manys,Side effect of both a lack of flagging and distributed moderation among those who choose to participate. That multiple mods have to agree in order to take consequential action only slows things down.,1527724412.0
Andy_Reds,Thanks! looks great.,1527716526.0
Malorby,Thanks \- I'll give it a try.,1527777630.0
OpiaInspiredKuebiko,are you trying to build an acronym generator or assign value for storage compression?,1527705720.0
TheTarquin,"What areas of academic security are you interested in studying?  Cryptography? Language Security? Static analysis and formal verification? Malware? Threat intel / analysis?

Security is a huge field, and the good academic programs tend to be pretty specialized around the interests of their main researchers.",1527685177.0
hyphendoodledocx,"Are there any barriers that prevent foreign students from entering this field in the US? Since this field of research needs some confidentiality, I am concerned about being rejected for not being a US citizen",1527687856.0
HomeTahnHero,"Look into CERT, a cybersecurity division at Carnegie Mellon’s Software Engineering Institute",1527692026.0
ezubaric,http://www.cyber.umd.edu/,1527705873.0
maq0r,John Hopkins has a great hands on program. Check it out,1527710170.0
Catalyst93,"Most U.S. graduate schools are very receptive to accepting undergrads from outside the university or outside the U.S. 

Once you've found some labs that you want to work in, be sure to contact the current students/professors expressing your interest in their research.  Some might just want you to apply to the school before reviewing any info, but some might be willing to talk more.",1527689007.0
Liz_Me,"Stanford has an excellent program in applied cryptography. 

https://crypto.stanford.edu/ ",1527700136.0
nziring,"The NSA Centers of Academic Excellence in Cyber-Defense has a special designation for graduate schools: CAE-R.   The list of designated institutions can be found here:
[CAE Institution List](https://www.iad.gov/NIETP/reports/cae_designated_institutions.cfm)
look for the schools with ""CAE-R"" in the second column.   Each institution name is a link to the main
cybersecurity lab or research institute at that school -- this can help you see which areas of cybersecurity
that school focuses on.

As /u/TheTarquin pointed out, which school is best for you will really depend on the area(s) you wish to study.  While a big school like CMU can allow you to pursue a PhD in a wide variety of cybersecurity areas, smaller schools are usually more specialized.

Pretty much any of the CAE-R designated schools will be open to accepting qualified students who did their undergraduate degree elsewhere.   Requirements vary widely, but many of them do require you to take the GRE.

Hope this helps.",1527722250.0
,[deleted],1527682366.0
cthulu0,"I think it would shift the needle slightly because the most of the same people who believe that P != NP  (which is most computer scientists) also believe that graph-isomorphism (and integer factoring)  belong to an NP-intermediary complexity class that is between  P  and NP complete,  so graph-isomorphism being NP-complete would certainly cause a few to modify their view.

But still only a very few.  

",1527701829.0
anotherdonald,You have a P time solution then?,1527681135.0
jx4713,"GI is not currently proven to be in P nor is it currently proven to be NP-hard. So, I suppose that there are some people who might believe that it is part of NPI (an NP intermediate class of problems which are neither in P nor NP-hard). Ladner's theorem tells us that if P != NP, then NPI is not empty. So I guess you can see for yourself that GI not being in NPI does not really tell us a lot. Even empirically, there are a whole load of other potentially NP intermediate languages (natural or otherwise). Who is to say that their complexity would also be settled? In fact, even if one were to show that NPI were empty (that is every problem in NP is either in P or NP-hard) then this would still not tell us that P != NP.",1528138545.0
PM_ME_IRL,"I don't know who this Rodney guy is, but tell him to stop ending Moore's law!",1527648168.0
Credulous7,"How much of our economy - tech for instance, has been based on the assumption of moore's law? Scary thought. ",1527678129.0
avidsoul,"So essentially, by hitting the maximum ratio of components printed in a circuit board. Brookes hypothesizes/deducts that we will enter a new era of innovations.

Thank you for the link /u/TebbaVonMathenstein, as it was most informative and mind boggling to the neophyte that I am.",1527666834.0
SlightlyCyborg,">Trying something new would most probably set things back a few years. So brave big scale experiments like the [Lisp Machine](https://en.wikipedia.org/wiki/Lisp_machine)📷 or [Connection Machine](https://en.wikipedia.org/wiki/Connection_Machine) which both grew out of the MIT Artificial Intelligence Lab \(and turned into at least three different companies\) and Japan’s [fifth generation computer](https://en.wikipedia.org/wiki/Fifth_generation_computer) project \(which played with two unconventional ideas, *data flow* and* logical inferenc*e\) all failed, as before long the Moore’s Law doubling conventional computers overtook the advanced capabilities of the new machines, and software could better emulate the new ideas.  


  
I am not surprised he is a lisper.",1527670382.0
combinatorylogic,Pity he did not mention *reconfigurable computing* as a way out of the dead end of stagnating general purpose architectures.,1527677003.0
NabiscoLobstrosity,"Moore's Law is becoming less relevant today for several reasons. First, a bigger portion of performance-focused computing depends on specialized architectures rather than outright speed, so the performance gain in that area will be driven more by better implementations than by size reductions.

Second, it didn't account for the internet or distributed computing at all. At any given moment, probably 99% of the transistors in the world are idle. Even if looked at on the scale of the full CPU, I'd bet that at any moment 90% of CPUs in the world are running at just 20% of their processing power or less. Meanwhile, we are finding ways to better offload processing tasks to the cloud, or share them between multiple devices. So transistor **utilization** is also an important measurement. 

Moore's Law has been used to quantify the way and pace that computing affects society. But the circuits don't do anything until we tell them to, so perhaps a better (but much harder to measure) metric is to track teraflops performed per capita. That is a more direct measure of the usage of computing power, and we'd probably see it increasing along with Moore's Law - except it will continue increasing after we pushed circuit manufacturing to it's limit. ",1528167002.0
talkstocats,"Total aside, but I can't comprehend how a smart person can write out something like ""1/100th"" and not catch it in editing. Fractions and ""ths"" are both fine, but you look stupid when you can't decide so you just mash them together and hope it kind of works. 

Also ""we actually seen"". He didn't even try to copy edit this. ",1527689880.0
Autism_Tylr_Schaffer,"Semantically, your title makes no sense. ",1527638269.0
IndependentBoof,"Because, like science in general, data science is modeled in empiricism. Data is highly-valued and indispensable to empirical analysis. Outside of the perspective that observations are real and can be empirically validated, there isn't much use for data, much less analytical approaches to interpreting it.
",1527651596.0
claytonraymond2004,Could you give some sort of context for what you are referring to? Sounds like something like using a certificate to authenticate a user but unsure. Never heard of this before.,1527633491.0
heroltz998,"To be honest, I have no idea what the number below the bold answer is supposed to be. Assuming it's a binary number, it's value in decimal would be 232, which makes no sense as the bold answer comes out as 229 in decimal. So somehow magically you add three to the bold answer to get the answer below it. Are you sure this is not an error in the training material?",1527603582.0
richardathome,"The tiny numbers appended to the end of the number indicate the base of the number, not the power. (It would be above the number if it meant a power Like this: 2A^16

It's saying ' Convert 2A in base 16 (hexadecimal) to binary (base 2)'

2A in base 16, is 42 in base 10 (decimal), is 00101010 in base 2 (binary)

BB in base 16, is 187 is base 10, is 10111011

Then add the 2 binary numbers together:

00101010 +

10111011

-----------

11100101


(last line of the sum is doing something else - are you sure that's the whole question? Do they mention 2's compliment anywhere?)",1527605218.0
thbb,"Meh, I am deeply skeptical about the whole notion of measuring the complexity of a problem as a function of the time to solve it given the size of its input.

Beyond quasi-linear time, we know very well that the entropy (or how we can approximate it) of the input matters much more than its actual size to provide results efficiently.

I don't pretend to have an alternative theory, specially considering that the Kolmogorov complexity of an arbitrary input string is not computable, but perhaps, in my retirement years, I'll spend some time looking for alternative approaches to measure computational complexity with a theory closer to how we actually tackle problems beyond polynomial time.

EDIT: by the way, if this rings a bell and someone knows of similar attempts, I'd enjoy reading about them. I have a few more or less related papers, but nothing close so far.",1527625566.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1527587780.0
_--__,"The difficulty with your argument is that it is not altogether clear what your ""decision problem"" is.  Ignoring for the moment that ""randomness"" does not play a part in the definition of P & NP, your machine is not actually 'deciding' anything based on the definitions I give below.

I've found the best way to think of a (decision) problem in NP is as follows.  A ""problem"" is a collection of **instances** (e.g. an instance could be ""a graph""); and these instances can be broken up into ""yes"" instances, and ""no"" instances (e.g. graphs with a Hamiltonian path are ""yes"" instances; graphs without a Hamiltonian path are ""no"" instances).  The **decision problem** is essentially ""Given an instance, is it a 'yes' instance?"" (e.g. ""Does this graph have a Hamiltonian path?"").  Now a (decision) problem is in **NP** if  ""yes"" instances can be ""easily verified as being a 'yes' instance"".  For example: a path in the graph (something which is ""easy"" to present) can easily be checked to be a Hamiltonian path: so the Hamiltonian path problem (""Does this graph have a Hamiltonian path?"") is in NP.
",1527599272.0
GandhiNotGhandi,"The way you've described it, your decision problem is not computed by a Turing machine. Really, this is an [oracle](https://en.wikipedia.org/wiki/Oracle_machine): a kind of black box that spits out YES or NO depending on what you feed into it. One can indeed use a similar argument to yours to show that with the addition of certain oracles, there are problems in NP that can't be solved in P.

The problem is that in the real world, there are no black boxes! Replacing oracles with Turing machines introduces the possibility of inspecting the code of the Turing machine to do something different.

Oracles are a commonly studied object in theoretical computer science. You could learn more about oracles by taking a class or reading an introductory textbook in the subject; I won't be able to do them justice here.",1527580629.0
gnupluswindows,"It's not clear what the problem you're presenting is. It looks like you're talking about two different problems and are getting them mixed up. 

If it's ""find the right number"", then it can be verified in polynomial time but not found in polynomial time, but it's not a decision problem, because it has to produce a string, not a yes/no answer. 

If it's ""check this number input by a person"", then it's a decision problem, but the computer isn't searching for a number, it's just checking the one number that was input, so it's in P.  ",1527604127.0
TomvdZ,"> NP since it's a decision problem

It's not a decision problem. Formally, a decision problem is a set of strings over some alphabet. Your goal is to decide whether a given string (over that alphabet) is part of that set. E.g., Hamiltonan Cycle, when viewed as a decision problem, is simply the set of all graphs that have a hamiltonian cycle (assuming some representation as a string over some alphabet). This whole randomness situation doesn't fit the framework of deciding whether a given string is part of a set.",1527585594.0
WetSound,"From the last line of the first paragraph of NP (complexity) - wikipage:
""More precisely, these proofs have to be verifiable by deterministic computations that can be performed in polynomial time.""",1527582866.0
fmresearchnovak,"I have some doubt in my own answer based on what the other commenters have said so far.  Having said that, I think your above reasoning is correct despite the fact that P !=NP has not yet been proven.

&nbsp;

P != NP is likely true, but unproven.  You have given a problem that falls into NP AND does not fall into P.  This is an example (maybe you could consider it evidence) that P !=NP.  If P != NP this problem raises no issues.  However, if P = NP then this problem should not exist.  Right?

&nbsp;

Actually to really say that P !=NP means saying there are problems that exist that are verifiable in polynomial time (NP), but are not solvable in polynomial time (P).  So we have a problem.  Can we say that there does not exist any algorithm to solve the problem that runs in P time?  It would appear to us that no such algorithm is possible for this problem.  But, what about other problems?  Can you really say P != NP for all problems?  Even for this problem, maybe there is an algorithm, but humans are not smart enough to understand it.  Just as a cat or dog cannot understand how a car engine works (but they know the car can move), there are probably algorithms that you cannot possibly comprehend.  IMO that's why P != NP is so hard to prove.  How can you possibly state something about all possible algorithms!?",1527601905.0
figure--it--out,I prefer the 3Blue1Brown video on how bitcoin works. It's a bit easier to understand and gets the important points across better while skimming over the unimportant stuff. Plus it's figures are a lot better and understandable. https://www.youtube.com/watch?v=bBC-nXj3Ng4,1527568895.0
inconspicuous_male,12:08 to skip the lesson on supply and demand and get to the technical part,1527566258.0
k-word,Is there a version without the shit soundtrack?,1527565200.0
robot_counselor,"I finally understand, I should have learned this ages ago",1527603532.0
6stringtheroy,Would appear problem solved. An,1527629478.0
Hakuna_Potato,great video!,1527561934.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1527538477.0
R-M-Pitt,"No maths but still lots of technical vocabulary. My grandma wouldn't understand (she probably would because she was a biologist and is pretty sharp, but you get the point). 

Try:

A Neural Network is a computer algorithm mainly used for image processing tasks, such as working out what objects are in a picture.

The structure of a neural network is based on the structure of the human brain. It is made up of ""neurons"" and ""links"". Information flows in one direction, out of one neuron, through a link and into the next neuron. A link has a ""weight"", which is how much it influences the neuron it is connected to. A higher weight means that a link (and the neuron it comes from) has more influence over the neuron that it is connected to.

A neural network has thousands of these neurons and links, and they are usually arranged into ""layers"", where each neuron in a layer receives information via a link from every neuron in the previous layer, and passes information over a link to every neuron in the following layer. 

For a neural network to work well at its task, the weights need to be set correctly. The weights are set in a process not unlike trial-and-error, called ""backpropagation"". A neural network is shown an example, the answer it gave is compared to the actual answer, and the weights of the links are adjusted in the right direction. This is repeated over and over until the neural network gets good enough.",1527537158.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1527512900.0
bob_7474,"Hi.
I would recommend reading the information provided by SDxCentral, it has interesting information about NFV Network Functions Virtualization:
https://www.sdxcentral.com/nfv/?c_action=num_ball
https://www.sdxcentral.com/nfv/definitions/whats-network-functions-virtualization-nfv/

Regards",1527526625.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1527506220.0
balefrost,"Notebook and pen? I don't understand, what do freecodecamp and HTML/CSS have to do with anything?",1527464201.0
firattheripper,"So you have h(n) = 3 + 6 + 9 .....n.
Lets take paranthesis of 3 so it becomes 3(1 + 2 + 3 .... n/3).Summation of a series from 1 to n is n*(n+1)/2.In your case,your series must be 3*[n/3*(n+1)/6] makes O(n^2 / 6) and it is simplified to O(n^2).
I hope it is true and helps you",1527455640.0
tadinhah,It's O(n^2 ),1527455005.0
harlekin-,"On a first glance I'd say you can rewrite your regular expression to 01(0+1)*, hence one equivalent NFA might check in two consecutive states if the word starts with 01 and afterwards loop in an accepting sink.",1527500365.0
pynick,Glushkov construction,1527511779.0
F1A,"I don't understand your notation.

All DFAs are NFAs however so you can just write out the transitions. Easy.",1527458276.0
vvillium,"https://www.amazon.com/Quantum-Computation-Information-10th-Anniversary/dp/1107002176

Best book hands down. This will bring you to the frontier of quantum computing. The book is also very approachable and meant for people trying to learn. It covers some linear algebra as well as physics in order to bring you up to speed.



Michael Nielson is an amazing educator and expert in the field. His you tube lecture course https://www.youtube.com/playlist?list=PL1826E60FD05B44E4 Quantum Computing for the Determined, is a short version of that book. He also has a free book online on Neural Networks that is probably the most referenced source on the matter. http://neuralnetworksanddeeplearning.com/index.html",1527462887.0
khamzah22,What's your background and credentials sir? ,1527447548.0
csp256,"I really enjoyed:

* Quantum computing a gentle introduction

It works great as a companion to Mike and Ike's classic.",1527466741.0
BubblingMonkey,"I'm interested in it, but I am new to Comp Sci in general. Is there anything I need to know beforehand to keep up to speed with the sub? Like anything different between here and there more like it",1527501862.0
Naznarreb,Yes and No,1527474884.0
AnnanFay,"> I know that this isn't exactly computer science

Hmm?",1527483023.0
CorrSurfer,"These algorithms are so simple, it doesn't really matter in which language you implement them (if I recall correctly, I once read a textbook that had a pseudo-code of ~6 lines for simulated annealing). So I don't think that are any statistics available about language distribution of their implementations.

If you crave for solving combinatorial NP-hard problems, then it may interest you that satisfiability solvers are often employed for this purpose, and they are commonly written in C or C++, as raw computation speed is crucial here. The programs encoding the problem you want to solve to SAT instances however are written in all kinds of languages.",1527450828.0
GayMakeAndModel,Functional languages tend to be preferred due to their flexible but well behaved type systems.,1527898119.0
Upstream-moonbeam,"Great post..super interesting and insightful...Its nice to confirm something’s I suspect. 
The pull to refresh,is definitely something that I found curious at times and now makes sense. Crazy world will be/are living in. ",1527435804.0
htuhola,"""Five Programming Languages That Is Must To Learn""

""There are hundreds of programming languages but which to learn is a major concern, because some fade its popularity with time. Moreover, new languages will keep coming according to the need of the generation. Still, there are some languages which are young, popular and powerful as they were a few years back and had that popularity. So followings are the languages which worth learning and every programmer must know.""

""1. JavaScript
Javascript or JS is a soul to modern webpages and web applications. It is so pervasive that every major browser supports it. It is easy to learn, implement  and most loved language by the programmers. With it animation, 2-D motion graphics or even a game can be made. Because of its wide popularity it is not going to fade away for the next few years.""

""2.SQL
SQL or Structured Query Language is not a programming language in itself but a language to handle the database for efficient retrieval, modification of data. Every application, software or website have some data in database which need to managed properly. To do that SQL is used, so there is no second thought that a programmer must have some knowledge about it and its underlying concepts. It is one of pillar on which today's software is standing.""

""3. Java
Java is a general-purpose programming language which means it can used for many different things like game development, for applications development, android development etc. Its a object oriented language that mean it visualize every real world entity as object having some attribute and action that can be performed on it. Moreover a code written in Java can be run on any machine, you do not have to recompile your code again and again for different systems, which is a plus point. There are thousands of library you can deploy for your projects. The syntax of Java is similar to C or C++ but highly abstract as it provides fewer low level routines as compared to later. Starting into Java may be hard but it will enhance your understanding about various underlying concepts. Its one of the most popular programming language for client-server web application and Sun Microsystem, Inc ( company who owns it) has announced that almost 1.5 million devices support Java and its still growing.""

""4.Python
Its a general purpose interpreted language, used majorly for automating the task and testing purposes. Its easy to learn and the code written in python is more readable, all thanks to its simple automatic indented syntax. Time taken to write a code is much less as compared in other programming languages like C or Java. You can use it for scripting, web application, desktop application( example Gimp), scientific computation, simulation, games. It is heavily used in areas like Data science and machine learning. Due to its simplicity it has successfully made its way into top programming languages of 2018.""

""5. PHP
Originally called Personal Home Page but now Hypertext Preprocessor is a server-side scripting language. It is used for web development as a backend programming language, i.e server runs your PHP code and sends the result back to your computer on which browser is running. There is no second thought that it works in conjunction with MySQL. Good News is that PHP dominates the internet with more than 82% of market share. Today, almost all the server supports it and its open source that means you do not have to pay anything for it. Moreover, because of its wide popularity you can easily find help to your every problem.""

""Some Last Words
I have not mentioned C or C++ as I assumed you have already learned these two primary language as they are mother or father of very essence of programming.""",1527507965.0
minno,"This looks like a variant of the [one-time pad](https://en.wikipedia.org/wiki/One-time_pad) cipher called a [stream cipher](https://en.wikipedia.org/wiki/Stream_cipher). Unless there's a vulnerability in the PRNG you're using (which there probably is, no point in Python using a cryptographically-secure one by default), then it's impossible to break. However, it's still vulnerable to modification if an attacker can guess what the plaintext is.",1527416683.0
AdrianWillaert,"I find the idea of using functions as key quite interesting. Upon a first look, I have the following thoughts towards the security of your cipher:

1. since the cipher is deterministic \(i.e. for a given message and key, the resulting cipher is always the same\), it cannot be [IND\-CPA](https://en.wikipedia.org/wiki/Ciphertext_indistinguishability) \(indistinguishable under chosen plaintexts\) secure. This means that an attacker, if given the ciphertext of one of two plaintexts, can tell with nontrivial probability to which plaintext the ciphertext belongs.
2. by chaining the s and r part of the key, the cipher basically calculates an individual permutation for each position of the plaintext. This permutation is the same for each application of the cipher, i.e. it stays the same for different messages.    
Consider a scenario where an attacker has access to a large number of plaintext\-ciphertext pairs. By calculating a frequency distribution over the character set the attacker can intelligently guess the used permutation, assuming the message is in english.
3. Looking at pt. 2, the cipher should be equally secure when completely omitting the shift part and directly calculating a random permutation for each position.
4. The following is more guessing than knowing, but by using relatively simple functions as key it might be possible to accidentally introduce dependencies between some positions in the ciphertext. This might allow an attacker to calculate the used functions by searching for such dependencies \(again assuming the attacker has access to many ciphertexts or even plaintext\-ciphertext pairs\).  
",1527431351.0
vicethal,"haven't even looked at the list of them, but I'll happily seed a measly 8GB. It's going on my torrent box that seeds a bunch of Linux ISOs.",1527445516.0
tim466,How fast are you guys downloading it? I only get 300kb/s.,1527448490.0
jake_morrison,"Erlang and it's newer cousin Elixir are functional for very practical reasons. Erlang was developed by Ericsson for building telephone switches.

Avoiding side effects makes it easier to build reliable systems. Avoiding shared state is important for concurrency. Immutable data structures are easier to debug and can be shared for efficiency. When a function only depends on its input parameters, then it can be restarted on failure by a supervisor.  If there is a crash in production, the crash log includes the function call and its inputs, allowing developers to exactly reproduce the problem. 

Unlike academic functional programming languages, Erlang has a very simple type system. It does, however, have some world class tools like QuickCheck to do static and dynamic analysis of the system. ",1527463638.0
Ravek,"Functional programming is useful anywhere. All modern programming languages anyone uses incorporate functional programming concepts like closures and higher-order functions nowadays and millions of developers use them every day. 

People tend not to use pure functional programming languages that much, because it's really inconvenient to be so restricted. That doesn't mean functional programming as a whole is restricted to academics though.

",1527410117.0
_--__,[Jane street](https://www.janestreet.com/technology/) are big proponents of FP.,1527410033.0
bigpigfoot,frontend state machine patterns use fp a lot.  fp is very useful when you want to control transformation steps of a particular object.  in frontend development that object is essentially your browser...,1527405401.0
trout_fucker,"Functional programming concepts play heavily into the design patterns used by React/Redux, which could arguably be the most wide spread use of functional programming in the industry.

As far as purely functional languages, Facebook has deployed Haskell to fight spam.

https://code.facebook.com/posts/745068642270222/fighting-spam-with-haskell/",1527405298.0
lutzh-reddit,"[Verizon OnCue](http://cufp.org/2014/timothy-perrett-functional-programming-at-verizon-oncue.html) is sometimes mentioned as a success story for FP in the industry.
",1527425298.0
Peter-Campora,I recommend reading why Discord uses Elixir https://blog.discordapp.com/scaling-elixir-f9b8e1e7c29b.,1527518054.0
,Some languages are built specifically for functional programming such as elixir. ,1527425110.0
gen-zero,"ugh -- functional makes a lot of sense, but, from what I've observed, functional tends to waste a lot of system resources. Anytime an object is modified, a new object is created in which the desired changes are merged into the new object. 

In terms of creating cleaning code bases, it makes sense, but in terms of optimization, I can't imagine that its more efficient. 

But, I'm just an intuitive cunt face.",1527410715.0
Tappedout0324,"Well NAND and NOR are universal gates which can be used to implement almost anything, we use so many just to make life easier that’s all.",1527388439.0
Umbrall,Because it's easier. You can't implement every operation with just and or or. You'd have to use something like xor and most of the time you'd be reimplementing those three gates anyway.,1527387248.0
Tittytickler,"Well, first off, those are the three basic logic gates, but there are more than that, like nand, xor, etc. Second, just think about it. All three of those are different. Why use two OR gates when you can use one AND gate? It is inefficient and doesn't make sense to have one type of logic gate, when there are different types of logic. Imo it is similar to asking why we still have trains AND cars.",1527387563.0
Rioghasarig,"I don't think computers only use 3 logic gates. They also use nand gates and xor gates and probably more gates. 

Since other people are already giving good answers, one thing I want you to ask yourself is why you think they should stick to one? Do you think you understand the function of a logic gate in a computer? I'm happy to explain if you don't, I just want to get a sense of how much you already know. ",1527388554.0
aioeu,"I would suggest it's probably a bad idea to count gates in this way.

Take the logical [""implies"" operator](https://en.wikipedia.org/wiki/Material_conditional), `X → Y` This has the truth table:

      X   Y  |  X → Y
    ---------+---------
      F   F  |    T
      F   T  |    T
      T   F  |    F
      T   T  |    T

`X → Y` is logically equivalent to `¬X ∨ Y`. Does that mean that if it were actually implemented in silicon the engineers would string together two distinct gates, a NOT gate and an OR gate? Or could it actually be implemented with fewer transistors directly, in which case it would be reasonable to call it just one gate?",1527402348.0
remy_porter,"To put it another way, I have two variables, X and Y. I want to know if one of X or Y is true\- I don't care which, I just want one. If I have an OR gate, that's easy: `X or Y`.

But what if I only had an AND gate? Is there a way to find out if X or Y is true with only AND operations?",1527388194.0
kdnbfkm,"There are many Boolean Logic gates but they can all be constructed via either NAND or NOR.

There also exist alternative logic design systems like Threshhold Logic or Majority Logic, but all the digital logic systems can construct functional units to emulate other kinds of gates.",1527391497.0
TheChrono,"I can't stand getting through this because it pauses every, damn, sentence. To interrupt the flow of knowledge to say ""BOY ISNT THAT SIMPLE?! YOU MAY NOT HAVE KNOWN THAT!""

You had me on board with the title and the intro. Now please just tell me what I need to know.",1527430292.0
maxakusu,"I admit I’m a little confused that you think your explanation of what a branch is, is in fact so simple. It’s a collection of commits that add up to the current state of that branch. You can branch off at any point as well.

Anyways, I just wanted to say that I completely disagree that merge is ever the correct thing to do in a collaborative environment. If you’re doing things right, there’s only two kinds of branches anyways, master and feature branches. Feature branches can be squashed and rebased off master (minimizing the issue of merge conflicts and making for easier management of the commit history) and merged to master from there without requiring further conflict resolution.",1527395099.0
nhays89,Well written! Thanks. Next article should be dealing with conflicts! That's another tricky git situation that people always get confused with.,1527404782.0
rotharius,"As someone who entered the field through a side door, what would be the best way to get familiar with the math and notation required to be able to read and understand this? Any recommended introductory reading?",1527360323.0
timmyotc,Please read the sidebar.  Submissions should pertain to graduate level subjects. ,1527328985.0
ImaginationGeek,"How much do you want to talk about the theory of educational psychology?  :)

Knowledge is constructed, not transmitted.  For your mind to construct its knowledge, it must connect new knowledge to existing knowledge.  Thus “prior knowledge” is very important, and bay be bigger and more subtle than just whether you’ve completed the prerequisite courses.

Knowledge is also broader and more subtle than just facts that you know.  It can also include things you can do, and even ways you are able to think.  Much of mastering CS is in learning the various relevant ways of thinking.

(Unfortunately it’s very difficult to measure “ways of thinking” since they are very hard to observe directly.  Usually we only get to see the outputs after “ways of thinking” have been applied.  This is one reason many interviewers like CS job candidates to think aloud when doing a technical interview - it reveals the thought processes.)

So other students may have come to the course with different prior knowledge (in very subtle or hard to observe ways).  That prior knowledge gives them more things in their minds to connect their new knowledge to, so they can potentially create more connections.

However, keep practicing.  Practice helps with the knowledge construction and with strengthening the mental connections that you are making.

Find alternate explanations, examples, etc.  Seeing the same material presented a different way may spark a connection that the previous explanation was unable to trigger.

Also, gaining more experience with the rest of CS will also eventually give you more things to connect algorithms knowledge to.  Unfortunately, that takes more time and diverse experiences than are probably feasible before this class is over... but it means there’s still hope in the long term.

Finally, please don’t get too hung up on “if I don’t master this, I’ll never be the best” thinking.  This isn’t an all-or-nothing career - there are plenty of good, respectable jobs in between CEO of a tech giant and serving fries.  Most people don’t get to be the best (by definitions of “most” and “best”), but people who aren’t “the best” can still be very, very good.  Just aim for doing something you’re excited about and you’ll have a happy career.  :)",1527321121.0
Laughingllama42,"I certainly feel like it takes time like it won't immediately click for everyone. I know for myself when taking algo I found that I would understand how to write the algo at times and yet not know how it worked. So for myself I would then just read up on it and do the usual, practice until at some moment it's as if a spark just happens and things end up all making sense. It's weird but when it comes to logic especially if you're starting out not everyone is wired to automatically think this way so yes practice but find practice methods that work for you. I know for myself I'm a very visual person drawing things out always helped. ",1527318375.0
mcvoid1,"Algorithms and data structures are generally the first classes that teach actual computer science, and it gets lots of undergrads by surprise because they thought they just had to cobble together code. CS is applied math, and math builds on itself. This is an indication that some foundations need remediation. Don't worry: you didn't hit your limit. You just need to catch up on something or another to give you the right context.",1527344964.0
Count___Duckula,"Edit: Post is on the way down so I'll cut it back to the unpopular bit:

Few of us have made significant contribution to computer science. It's not about inventing new cool stuff, it's about doing cool stuff with old stuff.
",1527318464.0
Akrenion,"How much math did you do beforehand?

Algorithms have a clear goal and you can formulate them in a clear fashion. Reading those is hard and so is wrapping your head around them.

Ask yourself what is the goal and how do it's actions help achieve that.",1527318377.0
,"Some algorithms by nature are hard to grasp. After all some were designed by very smart mathematics and computer scientists. Sometimes knowing the ins and outs of every algorithm is a waste of your time as long as you can implement it and understand it's limitations. Other times you may need to dissect it. My advice is try creating your own data structures and algorithms to help you solve problems. It's the best practice you can get. Don't use existing structures, just make your own as practise. 

",1527321032.0
CSMastermind,"I feel like you're asking a very practical question (can I pass algorithms) in a very vague way.

The short answer is that you are _almost_ undoubtedly smart enough to pass algorithms.  You may have missed some fundamental earlier concepts, and now it feels like you're hitting a 'wall.'

See Sal Khan's TED talk where he mentions 'swiss cheese' education.

And practice by itself won't be good enough.  Look into tutoring.  You need **deliberate** practice.  You need to identify exactly where the breakdown is in understanding and you work to fix it.

Now to answer the question in the title of this post: yes.  Some people are pretty incapable of learning algorithms.  People with mental disabilities are an obvious example.  IQ itself is mostly a predictor of how well you work with abstract knowledge so below a certain IQ (which does have a genetic component) it will probably be difficult for some people to understand algorithmic problems.

The good news is that if you're in college, then you're probably not in that group.",1527345929.0
Cocomorph,"Getting the hang of dynamic programming can be challenging. I know that was just an example, but keep plugging away.  
  
In general, you can do it. I assure you, you can do it. Your anxieties may make the whole process a bit slower, incidentally, but that would only be a catch-22 if you were racing your peers. You aren't racing, so don't sweat it.",1527327241.0
ReneDiscard,I think the jump from the first/second year courses into the typical data structures and algorithms class is a lot more difficult than people give credit for. ,1527326386.0
l_lecrup,"I think if you are capable of using reddit, you are capable of improving your understanding of anything. Maybe you could give us an example of a typical problem in your course, and talk us through your problem solving strategy. Sometimes one has a bright idea straight away, but usually some sort of strategic work is necessary. You break a problem down into constituent parts, you try to use a technique that worked for a similar problem, you try to solve a related problem with fewer constraints, or more constraints, to make the problem easier etc.",1527327613.0
trialofmiles,"I believe that all people can become decently proficient at application of a subset of known algorithms and perhaps the “invention” or new algorithms or variations.

The key, I think, is that many algorithms require develop sitting with them for a long time to develop a meaningful understanding/intuition of how they really work. Once you have that for some subset of algorithms, I feel like learning new algorithms gets easier. I suspect that for many people who find algorithmic thinking difficult, they haven’t put the time in.

I would also say, it helps if you focus on related domain areas or subsets of algorithms. I think it’s realistic to become very proficient in say a set of computer vision or audio signal processing algorithms. No one knows everything.",1527331810.0
48151_62342,No.,1527340922.0
mrexodia,DP = cache for problems you expect to solve again in the near future.,1527354167.0
pamplemouse,I cruised through CS until I hit PAC learning. Something about it never clicked and I realized there was a limit to my brain power. Quantum cryptography was also difficult. Nevertheless I managed to make a career despite my humbling mental handicaps. ,1527354793.0
Josuah,"There are some things I'm not good at. Things which I don't think I will ever be as good at as other people. And there's also a subset of those things for which I don't think I will be as good as the *majority* of other people.

To me, that's a reality that can't be avoided or worked around no matter what, any more than I could ever be as good as Michael Phelps at swimming (due to the actual physical characteristics of his body). If genetics plays a role in mental retardation, I don't see any reason it wouldn't play a role along the entire spectrum of mental functionality be it intelligence or empathy or whatever. Mental abilities and capabilities are a function of our physical components.

Still it is important to recognize that ones limits are not something that one can necessarily know without testing those limits.

There are some things I'm not as good at because I haven't focused on them as much and I don't necessarily want to because my interests or priorities lie elsewhere. There's nothing wrong with specialization in a world with so many options and so much information.

But there are other things that no matter how much time I seem to spend on, my progress is much slower than other people or I don't seem to really get in the same way. I imagine dyslexia might be a way to understand that: no matter what, the brain is built differently and won't process symbols the same way as people without dyslexia.

One thing I'd say is you don't have to be at the top, can recognize you won't be at the top, but still put in the effort to be at the top if it is something you want to prioritize. That's still a good thing. It's the only way to hit your limits and you will still end up better off than if you didn't even try.

I'm good at some things, and leverage those skills and that knowledge. For the things I'm not good at, I rely upon my peers and colleagues. It goes both ways. You can still be a very good computer scientist or software developer even if you aren't a master of algorithms.",1527355139.0
coshjollins,"I've found that if i'm not understanding something, then either I have a bad teacher or I am missing some preliminary knowledge, in which case I need to back up and relearn the basics better. ",1527356211.0
XwingMechanic,"One thing I tend to try is to overlearn. 

What I mean is that most classes (especially in engineering and CS) usually teach you the bare minimum based on the teacher’s preferences. But if you learn sufficiently differently from the teacher or have a sufficiently different background, then it’s going to be tough to be successful given only the teacher’s lectures. So what I do is go find new resources online or texts, including the underlying pure math. Mostly because I like the way mathematicians think: precise logical rigor tends to clear out all the b.s. handwaving. You might like the way some other people think though, so it’s up to you what resources to focus on. Having an idea for where things actually come from helps me a lot to put the lectures in context (and to know when the teacher is being overly handwavy). ",1527358118.0
BossOfTheGame,"I know this feel. I floundered in my algorithms classes. I never felt like I could find a solution without looking it up. In some respects I still feel this way, but it's gotten better, it just took awhile. I marginally passed the classes, but only after extensive help from the instructor. I actually went on to TA the algorithms class 3 times. The first time I still had no idea, the best à was able to do was grade by pattern matching against a rubric. I made lots of errors and probably overgraded several students to compensate for my own lack of understanding. However a year later I TAed again, and I still floundered, but I started to get it, by the end of that semester I started feeling comfortable. By the final time I TAed I really got it and understood the course material. Shortly after I graduated and now have a PhD. I think the reason it was so difficult for me is because I have more of an engineer's mind than a mathematician's. Algorithms are hard (some more than others), keep at it, it will start to make sense, it just can take a lot of time and effort. It's really worth it though. Algo gets really deep and interesting... almost philosophical at times. Hopefully this annecdote is encouraging.",1527361166.0
iBastard,After looking at a solution always try to think about a way of thinking with which you could have come up with the solution  by yourself. And if you're not doing algorithms it doesn't mean you shouldn't be doing any programming.,1527361444.0
drgmonkey,"This may seem strange, but one thing I realized really helped me with computer science and algorithms was getting clean and organized in my daily life. Organization is the goal of CS and optimization is a means of reaching that goal - usually through algorithms. So once you start thinking every day about how you organize your life, you can link those tasks to algorithms. What’s the most efficient way to clean my room? Am I using bubble sort?


I realize this won’t help everyone, but my point is that something outside of studying that you can think of algorithmically will help familiarize and contextualize algorithmic thinking. The more you keep it abstract and untethered to your own goals, the harder it is to think about.

Just my 2 cents :)",1527363385.0
My-third-eye-stinks,"Keep working, one day it will click for you. You might be missing a key connection and once you make it, you should find yourself better at algorithms. Somethings you can become really good at very easily, others take time, either way eventually you will make the connection you need. ",1527367413.0
Xonophilia,"It sounds like there's a flaw in one of your assumptions about the material. In which case, review the material and your CS/Math foundations.

You should always be actively engaging the material (ie. asking questions to yourself like: does this make sense with what I already know? where will this be useful? time/space complexity? alternative solutions? my opinions? pros/cons?). 

Don't concern yourself with others ""getting it"". They may already have prior experience before entering the class. Learning is not a race.",1527368237.0
tony475130,"I started college with the full on intent on getting a c.s. Degree because I like programming and computers in general. What I didn’t like and wasn’t good at was math and most other sciences like chemistry and bio. I nearly aced every comp sci class Ive taken with A’s(save for a sole B) but I’ve either failed or had to retake a lot of the extracurricular sciences and math classes that didn’t pertain to computer science like engendering physics and calculus. I would sit on my desk studying for hours on end and still only understand half pf what I studied. It was a pain in the ass having to work twice as hard as the rest of my class and still come out half confused on the material. Professors were either helpful or complete dicks in times of need. I feel your pain but I would definitely seek out help from other with better experience who can coach you through your programing classes. As much as I want to say just study more, I have to admit its better ask someone(like in your class maybe) who is nice enough to give pointer on what your not getting.",1527390001.0
chrisokasaki,"I've been teaching algorithms for years, and I'd like to offer some specific advice about Dynamic Programming, even though I realize that was an example.

First, realize that, when you're struggling, it is often very difficult to identify WHY you're struggling.  Explaining your reasoning to a teacher/TA/mentor can help because they may be able to identify that your actual difficulty is someplace other than where you thought it was.

For example, suppose that your hammering skills are weak \(maybe because you've always used a nail\-gun\), and somebody asks you to build a house using manual tools.  You might conclude that building a house is almost impossible, when the real problem is that you don't know how to use a hammer correctly.  Learn how to use a hammer \(and probably several other tools\) and you may find that building a house is much easier than you thought.

In this analogy, Dynamic Programming is the house, and the hammer is recursion.  Many students struggle with recursion, and then, when they hit DP, they think that DP is nearly impossible.  But, if you beef up your recursion skills, DP becomes significantly easier.  Despite the fact that the end product of DP usually contains no recursion, the relationships behind DP are heavily recursive.

Second, Dynamic Programming is a great example of a technique that's easier to tackle in two steps rather than all at once. Solve the problem twice: the first time using brute\-force recursion \(which will end up making the many of the same calls over and over again\), and then convert it to use dynamic programming instead.  The second version will no longer be recursive, but the logic that you use to fill in a particular spot in the table will be the same as the logic used in the recursive calls.  Once you get good at it, you may be able to do it all in one step, but that doesn't mean you have to do it that way from the beginning.",1527704567.0
NothingButJackal,2meirl4meirl,1527331206.0
nameless_pattern,https://en.m.wikipedia.org/wiki/How_to_Solve_It,1527348197.0
,Yep.  Some people just don’t think logically.  ,1527318155.0
alazyzombie,If it helps I recently got a decent dev job for next year and Im quite awful at dynamic programming questions lol... Shit is hard. Honestly this and graphs are the most difficult parts,1527328099.0
rodrigomlp,"Yes, IQ is a limiting factor in any intellectual endeavour.",1527321574.0
very_sneaky,"IIRC intelligence only affects the rate at which you can learn something. Once you’ve learnt it, my understanding is that it has very little effect on how well you execute that skills. So using this, can you get “good” at algorithms? Sure. Depending on your intelligence, however, it might take you some time to grasp. I’m not sure how well this applies to ad hoc problem solving though, such as in an interview situation.

Source: I watched a few Jordan Peterson lectures on IQ and intelligence testing",1527321979.0
The-Klein-Bottler,"Get good at algorithms? That’s a very broad question.

Specific kinds of algorithms already made (such as searching algorithms to graphics algorithms to calculus integral operations), or making your own algorithms?

An algorithm is just a concept. An algorithm is a sequence of steps. Cooking recipes are essentially algorithms. There are Rubix Cube algorithms. Algorithms for forcing my sister to sensitively torment my membrum virile.

What do you mean being “good” at them? Like applying existing ones into use for programming?

",1527318590.0
rathen45,"try monologuing algorithms in terms of everyday tasks. 

Wake up

get ready for school

Start coffee

shower 

for loop until clean

wash

rinse

end loop

   get dressed

eat

brush teeth

get school supplies

etc..",1527349203.0
mysleepyself,"Do you mean like an object language? Because that's usually the word I hear people say when they refer to the language being discussed in another language.

https://en.m.wikipedia.org/wiki/Object_language
https://en.m.wikipedia.org/wiki/Metalanguage

If you mean strictly in a programming context maybe you would be interested in metaprogrammimg and generic programming. 

https://en.m.wikipedia.org/wiki/Metaprogramming",1527326143.0
Cirrustratus,sorry for the grammar.,1527304018.0
nerdshark,"The difference is that different operating systems have different directory separators for paths. DOS and Windows traditionally use `\` as the separator, while Unix, Unix derivatives, and POSIX use `/`. Which separators a programming environment supports is, obviously, defined by the programming environment. Some support both, some support one or the other. It varies a lot.",1527288743.0
Sack_of_Fuzzy_Dice,"While i cant think of robots, have you thought of arduinos and raspberry pi's?",1527266980.0
Carpetfizz,"if you're into NodeBots  / programming robots with Javascript (Node.js), you can check out the [Johnny-Five Inventor's Kit](https://www.sparkfun.com/products/14604) ($124.95 USD)",1527491087.0
SweetOnionTea,You sound like you're on a great start! Honestly you might get bored with any freshman cs courses as most just teach basics. A couple of things I'd recommend is learning git if you already haven't. Also coding style guides. I believe Google has their style guides up for free.,1527257997.0
bamfdan,"No idea how this would work in america, but you seem to already understand atleast basic programming principles so your probably good there, but for myself in the UK a lot of CS first year was the study of algorithms and databases, learn mySQL maybe and just general DB terms.

Also look into algorithm design, which is basically what computer science is, the study of algorithms: [https://books.goalkicker.com/AlgorithmsBook/](https://books.goalkicker.com/AlgorithmsBook/)

Mainly basic sorting and searching methods. Also have a little look into general logic.

CS courses are often very little about programming, be aware of that. My modules in first year were;Intro to Java, Intro to databases, computer systems \(hardware, neumann architecture etc\),operating system concepts \( how does a pagefile work, how are running prgrams organised by the OS etc concepts not code\), human centric computing \( learning what makes a peice of software/website easy and attractive to use\),

algorithmic foundations \( that book \) and logic \([https://en.wikipedia.org/wiki/Logic\_in\_computer\_science](https://en.wikipedia.org/wiki/Logic_in_computer_science)\).

If what you wanna do is just code, then aim toward software development courses where possible.

Id say the main thing is read up on is things like how binary search, quick\-sort, merge\-sort work. and Read some books on logic for computer science. Its more logic than classic school maths.",1527260192.0
MandalaEDM,"I came into University with a strong background and graduated in the past few months. 

Honestly, I recommend getting into some hobby that will help define you outside of computer science and to not study this summer. 

At the end of the day, some preparation might help, but most of it will not effect the grander scheme of things. You are just contributing to burning yourself out without gaining much. 

The biggest lacking trait I see in coworkers and classmates is NEVER techinical. It's always either an inability to be social and communicate well in a professional environment or the ability to ""turn off"" and get interested in something not computer science.
",1527452045.0
da_borg,"It sounds like you're off to a good start with coding for sure. You're right to think about Math, since you have probably Calc I-III and discrete.

Your university probably has a standard book for all their Calc 1 classes. You could get that (search for a calc one syllabus or email a professor/dept head) and start reading it and doing the problems. When you get stuck, post to /r/learnmath or another math help forum. As you discover what your sticking points are, go back and brush up on them using either of the sites below. Most people get stuck on trigonometry or algebra.

If you can't find the starting book, Thomas Calculus is well spoken of. 

http://www.purplemath.com/modules/ordering.htm
https://www.khanacademy.org/
",1527261686.0
Meguli,"For beginning calculus, best lectures I know are Calculus Revisited videos from MIT. Those are old but God, they are amazing pedagogical achievement. ",1527271683.0
azvibe,Learn C and or C++.  I used to work at one of the top CS departments in the US and they used their CS 101 Class (Intro to C++) to weed out the coders from everyone else.  Once you get C or C++ down pat (including commenting code and debugging) that will serve as your foundation for all other coding moving forward.,1527731083.0
hextree,"You did a better job of explaining the intricacies of the hash map complexities than the previous article I saw posted in this sub (which incorrectly said it was only O(1), and caused a big debate in the comments). However, even with rehashing, insertion time is O(n) amortised.",1527253810.0
buckboostltd,Thanks :) ,1527260727.0
,[deleted],1527280978.0
,Thanks for sharing this!,1527289164.0
AaronKClark,"Thanks for this, Adrian!",1527304714.0
minimim,"I mean no offense, but it doesn't include anything a beginner actually needs.

It's all very interesting and a good primer on *implementing* those data structures, but that's not what a beginner would do, is it?

A beginner guide to data structures should tell them to grab an already implemented library and give interesting algorithms and examples about how one would use them.

Besides, from the perspective of a beginner using a modern language, what's the difference between a double linked list and an array anyway?

",1527278091.0
PathToTheLight,RemindMe! 6 hour,1527269711.0
tim466,"The runtime table says O(n) for deleting from a double linked list, should be O(1).",1527298035.0
longjaso,"Do: Ask questions, be willing, work hard.

Don't: Show off, disregard advice, badmouth your boss/coworkers

These seem like basic principles (they are) but you'd be surprised how many people in the working world break all of those on a daily basis.",1527214101.0
fxr,Find a haskeller. Follow.,1527230404.0
violenttango,"You're not going to get into the meat of the development for several days or weeks even, so just do your best to soak up their methodologies for getting work done.  Frequently even if you disagree on strategy you will be a more effective team member when you can meld well with their existing paradigms.",1527214202.0
BananaHockey,"So I am attending university soon and my school offers Comp Sci as a BS or a BA. I should get it as a BS, right?",1527655912.0
_cherryDocs,"I don't know how much schooling you have left, but the best way to find out if you want to do computer science is to do an internship. It doesn't have to beat a big company or far away or anything. If you find that you enjoy it, stick with CS. Personally, I did a software development internship when I wasn't entirely sure if I was going to stick with CS \(though I didnt have another option to go to\) and even though it wasn't what I want to do full time, it helped me figure out that I actually wanted to do cyber security.",1527203373.0
TechnoPeasantDennis,"Computer science is a pretty big field, so you can pursue aspects of the field that you find most enjoyable.  On the other hand, you do not want to spend your life doing something you dislike.  In order to answer your question, you should consider what in particular you enjoy about computer science. 

I don't know your background in computer science.  Especially if you are new, you might find that your computer science courses rekindle your excitement.  You might also find working with similarly-minded comp sci student fun and engaging.  Don't forget that it is possible to switch majors after you start.  If you find that computer science isn't for you, you can switch majors.

I would definitely recommend talking to the professors from each department since they can give more personalized advice than a stranger from the internet.  Also, depending on the rigor of the program and amount of general electives, you might be able to get a minor in the other field.  

I hope you find this helpful :)",1527204359.0
emef,"It's hard to understand what you're asking when you say ""my"" field. If you love it, then it's your field. If you're neutral and good at it, maybe it's your field. If you hate it, probably not your field. It's more of an experiential thing than a set of criteria. For me I've always been interested, spending my free time learning and building side projects and never really been burned out. Others will have different experiences and still think it was a good choice for them.",1527204589.0
atreayou,"Comp sci is different depending on your job and what you want to do. If you are passionate about one thing I would say get a comp sci degree and then go find that job (machine learning, research, websites idk). Stem degrees are great all around and can really help you stand out. ",1527202351.0
dman24752,"Do you like starting out your career with a job that will pay at least $70K? If so, you should choose CS.

I was in a similar situation. I was really interested in Political science, but ended up going into CS as a career. It depends on what you want to do, but from an advocacy/political standpoint, you're better off getting a job that makes a lot of money then using your free time (and money) to further social causes you believe in.",1527207455.0
brookhaven_dude,Try something related? Perhaps a course in statistics or operations research?,1527268072.0
fmresearchnovak,Why not double major and do both?,1527442573.0
kittytheexplorer,"Hi YaxchiOglan! If you are interested (on the same level) in both fields, then the next thing that you have to do is to dig deeper into your personality. Regarding your concern that IR has a very limited career options, you can still break into it.. If.. you're highly confident of your potential to be very good in this profession. This may not guarantee your success in getting a job but chances are high that you'll land a good one. On the other hand, CS has a lot of job opportunities as you can read from here [Reasons to Pursue an IT Career](https://www.infotechresume.com/it-career-advantages/); however, you always have to continue learning technology as it changes and improves from time to time. I know some CS graduates who find it hard to get a job, even those who already have an experience and plan to apply to other companies. One of the many reasons is they lack the initiative to continuously learn their craft.. to always hone their skills. If they do that, they will be able to compete with other job candidates. 

The other factor that you have to consider is your financial needs. You can make big bucks both in IR and CS. It will just depend on how expert you are, how courageous you are to beat the odds, and how unique you are in dealing with the matters of your profession. By the way, here is the link for a personality test: [16 Personalities](https://www.16personalities.com/)  I believe this can help you decide on what career you have to pursue. :)

&#x200B;",1536556743.0
olaeCh0thuiNiihu,"No, it has been proven.

If you want to change the definitions, sure.  If Turing machine means duck, halting means dead, and program means bullet, then yes, a program running on a Turing machine can be proven to halt.",1527190995.0
sadmafioso,"It is impossible to have an algorithm that ""detects all pathological algorithms"" ;)",1527190479.0
Rioghasarig,"> Wouldn’t a pathological algorithm be unable to actually execute, given that the recursive halting decision check would be unstable?

I'm not sure what you mean by this. There is a such thing as a Universal Turing Machine that can simulate literally any other turing machine. In this sense, at least, there is no such thing as a ""pathological algorithm"". It covers all cases. 

Have you looked up the actual proof for this? There's actually not that much to it but you'll probably have to learn a bit about Turing machines if you haven't already. ",1527191219.0
olliej,"* Wouldn’t a pathological algorithm be unable to actually execute, given that the recursive halting decision check would be unstable?

No, if the algorithm can execute on a Universal Turing Machine, then the halting program detector can run over it. There is no concept of a ""pathological"" program that you're describing -- that is why we say that it is a universal machine. Fundamentally there is no such thing as a ""pathological"" algorithm in the sense you're trying to describe.

* Would it still be possible to decide whether nonpathological algorithms generally terminate?

No, that's literally solving the halting problem. Remember the halting problem is specifically the general case of a universal machine. There are plenty of complex systems that can be solved, but they're 1) not Turing complete, and 2) generally the answer is ""yes"" because they are finite execution environments

* Couldn’t a halting decision algorithm detect pathological algorithms and treat them as unstably halting?

No, because detecting a ""pathological algorithm"" would require solving the halting problem.

I think you're having trouble because you're thinking of an algorithm that does something random, so it does occasionally halt, and occasionally not, through a non-deterministic mechanism that is not part of the machine state.

The problem with that concept is that that would mean part of your program is not actually running on the TM you're trying to verify on.

At a very high level, if you wanted to running your does_halt(fn) function over such an algorithm, you would need the randomness to also be part of the algorithm, specifically the randomness generator would have to be part of the program, and therefore deterministic, w.r.t to the initial algorithm state (which would now include a seed).

Take a function:

'''
function foo(x) {
    for (;random() > 0.5;) {}
    return true;
}
'''

For the purpose of your analysis the function would actually need to be represented as something like:

'''
function foo(starting_random_state, x) {
    random_state = starting_random_state;
    (random_value, random_state) = random(random_state)
    for (; random_state > 0.5; (random_value, random_state) = random(random_state)) {
    }
    return true;
}
'''

the random() function is necessarily deterministic w.r.t it's input state, and is itself modeled on the TM, along with the rest of the algorithm.

The does_halt() would need to verify for all initial states that eventually a value greater than 0.5 would come out. Now in this case it's logically trivial to prove that the function will terminate -- all you need to do is compute the full cycle of the RNG for every initial value. Of course, if the initial size of the RNG state is unbound, then there's an infinite number of seeds, and so that becomes infinitely long, and your does_halt() function no longer halts.",1527200757.0
VermillionAzure,"A huge hole in Turing machines is that they have unbounded memory. Real programs or subprograms often operate on bounded memory. I think there is a huge spectrum between context free and Turing complete programs that deserves to be explored. The halting problem is decidable for bounded memory machines... But it is considered intractable.

The halting problem also doesn't tell us about approximations to the halting problem. Static analysis makes heavy use of approximations to give a sketch on correctness, circumventing Rice's theorem by giving up completeness.

Turing was not wrong, but there are many ways that people try to circumvent the general case result by fixing a class of programs. ",1527207382.0
TarMil,"What do you mean by ""unable to actually execute""?",1527189278.0
anonrektard,What's a pathological algorithm?,1527194433.0
zilogz80cpu,https://teachyourselfcs.com,1527170592.0
jhp2000,"I'm someone who learns best by reading textbooks, and I've taught myself a lot of material out of textbooks over the years. I can give you a list of textbooks that I've enjoyed reading. These aren't necessarily the best for getting a thorough grounding in a field, or as a companion to a course, because the best textbooks are often opinionated, and trade a certain amount of comprehensiveness for a more engaging presentation of one particular point of view on a field. For example, the best compiler textbooks mostly come from a functional programming point of view (in my experience).

Textbooks I've liked:

* Lisp in Small Pieces, Queinnec
* Handbook of Practical Logic and Automated Reasoning, Harrison
* [The Implementation of Functional Programming Languages](https://www.microsoft.com/en-us/research/publication/the-implementation-of-functional-programming-languages/), Peyton-Jones - available free online
* Readings in Database Systems, Hellerstein and Stonebraker
* Algebra of Programming, Bird and de Moor
* Pearls of Functional Algorithm Design, Bird
* Purely Functional Data Structures, Okasaki",1527180048.0
LgDog,"Algorithm Design, Jon Kleinberg and Éva Tardos

is a book I recently come to know, it is great. Its proofs are better written than those in CLRS,  but its algorithms are more high-level, which I think is a good trade for someone who already know programming but want to improve in the formal/theoretics aspect.",1527184309.0
KatsuCurryCutlet,"1. Algorithms: CLRS 
2. Linear Algebra: Hoffman and Kunze
3. Theory of Computation: Michael Sipser's introduction to theory of Computation. (alternatively hopcroft and Ullman)
4. Discrete Math (or math for cs in general): concrete mathematics by knuth
5. Compiler design: I forgot the actual name but people refer to it as the dragon book
6. Control theory: process Dynamics modelling and control by W. Harmon Ray and Babatunde

Other noteworthy books: 

1. Structure and interpretation of computer programs

2. Algorithms design manual

3. The art of computer programming (there's 4 volumes right now imo it's not necessary I'm just putting it out there)

4. Combinatorial Optimization by papadimitriou

5. *Bonus* for engineering mathematics you can try erwin kreyszigs advanced engineering mathematics. Covers calculus linear algebra (and I can't remember what else it's been a while and my copy is a 3 hours drive away right now)  even has discrete optimization algorithms if I recall correctly, or at least max flow and it's algorithms. I definitely found this book fun considering the size and content

And i doubt you're doing theory but:
Computational complexity I use two books, one by Sanjeev Arora and Boaz barak, and another by papadimitriou.

Try to find a copy of the Boaz textbook if you're interested instead of the draft that's been circling around, the draft is riddled with mistakes


For the two toc books, one by sipser and the other by Ullman, my favourite was sipser because the concepts were explained very well. However I found ullman's take on a toc textbook was distinct enough and had offered a great alternative view so imo it warrants a read as well if you're interested

As for programming what my roadmap looked like was learning C from K&R then picking up back practices on python on my own before settling down in java with Google LOL",1527208949.0
bahulu,"http://www.cl.cam.ac.uk/teaching/1718/CST.pdf

Here you go. Cambridge has a list of the textbooks their undergraduate use sorted by subjects. However this list doesn't cover the mathematics course. ",1527201633.0
rodrigomlp,"Computer Systems: a Programmer's Perspective, Nand2Tetris and Elements of Programming Interviews in Python",1527211525.0
nimblerabit,"I learned mostly through reading textbooks in University, but not many of the books we were assigned stood out as being particularly great. Here's a few that I did enjoy:  

* [Artificial Intelligence: A Modern Approach](https://smile.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597?sa-no-redirect=1).
* [Computer Organization and Design: The Hardware/Software Interface](https://smile.amazon.com/Computer-Organization-Design-MIPS-Fifth/dp/0124077269?sa-no-redirect=1)
* [Algorithms (Sedgewick)](https://smile.amazon.com/Algorithms-Robert-Sedgewick-ebook/dp/B004P8J1NA?sa-no-redirect=1). I liked this much more than CLRS.

To balance things out, I'll also give a list of books I thought were awful:  

* [Discrete Mathematics with Graph Theory](https://smile.amazon.com/Discrete-Mathematics-Graph-Theory-3rd/dp/0131679953?sa-no-redirect=1)
* [Fundamental of Database Systems](https://smile.amazon.com/Fundamentals-Database-Systems-Ramez-Elmasri/dp/0133970779?sa-no-redirect=1)
* [Programming 32-bit Microcontrollers in C: Exploring the PIC32](https://smile.amazon.com/Programming-32-bit-Microcontrollers-Exploring-Technology/dp/0750687096?sa-no-redirect=1)
* [Software Engineering: A Practitioner's Approach](https://smile.amazon.com/Software-Engineering-Practitioners-Roger-Pressman/dp/0078022126?sa-no-redirect=1)
* [Essentials of Theoretical Computer Science](http://cse.ucdenver.edu/~cscialtman/toi/Essentials%20of%20Theoretical%20Computer%20Science.pdf)",1527221032.0
DaveVoyles,"* Learn C the hard way (or Python)
* Code: The Hidden Language of Computer Hardware and Software",1527174031.0
Meguli,"For linear algebra and calculus, I advise for Advanced Calculus class of Prof Shifrin, on YouTube. He follows very closely his book Multivariate Mathematics. Book may not be the best but with lecture vids, it is amazing to work through. ",1527226765.0
shaggorama,SICP,1527230645.0
NytronX,"Textbooks aren't the best way to learn any of those subjects. They are just supplements. In the case of CLRS, that textbook is by default the best one because of its encyclopedic nature, not because it's easy to read or well written. The amount of work needed to write a one as complete would take years. 

So basically, using the internet is your best textbook. MIT OCW is your friend.",1527174323.0
scatteredthroughtime,"> Algorithms, Data Structures, Compilers... Electronics, Computer Design/Architecture, Circuits... Electrical Machines

[*Code: The Hidden Language of Computer Hardware and Software* by Charles Petzold](https://en.wikipedia.org/wiki/Code:_The_Hidden_Language_of_Computer_Hardware_and_Software) addresses these in an accessible and comprehensible way.",1527178988.0
lism,"Anecdote I know but I've just finished my 4 year CS course having bought 3 textbooks across the whole course. I looked at each of them roughly 3 times and then they went in a box and I haven't used them since.

Honestly unless your course requires you to have a certain book, just learn from online resources. I know youtube tutorials don't seem very academic but for most people they are simply better for learning than a textbook. If you prefer block text then there are online resources for that too.

To be honest I don't understand why textbooks are still used in higher education.",1527172347.0
pmourfield,Rob Conery has a great book called The Imposters Handbook. http://bigmachine.io/products/the-imposters-handbook/ With a Volume 2 in the works,1527207524.0
Josuah,"* SCIP
* Compilers: Principles, Techniques, and Tools (Dragon Book)
* Operating Systems Concepts (Dinosaur Book)
* Applied Cryptography (Schneier)
* Art of Computer Programming (Knuth)
* Introduction to Algorithms (CLRS)
* Design Patterns (Gang of Four)
* Electrical Engineering: An Introduction (Schwarz & Oldham)

Those are some of the books I've saved on my shelf. There are others, but I think these are ones that particularly standout in my memory.",1527211735.0
RunThread,Google.com. But honestly that’s how I started and still using everyday to learn. You can sign up for coursera courses for free.,1527172545.0
ExtraSloppyyy,"Start studying areas of applied math(eg Applied probability, statistics(this is a biggie), data manipulation(get familiar with numpy and pandas) and get familiarity with ML(people seems to drool over ML these days).

Source: I’m an applied mathematician and comp scientist",1527162478.0
brookhaven_dude,https://www.ic.unicamp.br/~wainer/cursos/1s2013/ml/livro.pdf,1527268018.0
trout_fucker,"A BSc will greatly increase your chances, yes. ",1527161424.0
RockNabster,If coding is all you want then there are a lot of schools that offer it online for free. A real degree for free from the comfort of your couch. Look into it! :),1527161324.0
djdefekt,The [Verified Certificate](https://www.edx.org/verified-certificate) that EdX offers has apparently started to become accepted by some employers.,1527161825.0
_--__,"Assuming you are also requiring the tape alphabet to be {1} what you are describing is a [k-counter machine](https://en.wikipedia.org/wiki/Counter_machine) where k is the number of tapes you have available.  Critically, if you only have one tape, then the resulting machine has strictly less expressive power than a Turing Machine (the reachability (i.e. halting) problem is decidable for a start); but if you have two or more tapes then the machine does have the same expressive power as a Turing Machine.",1527166082.0
jockonarock94,"I just finished my data structure last sem with an A. You should know OOP well, be familiar with setter getters for the projects, and know your dynamic binding and polymorphism. Most of the data structure content is conceptual and theoretical so it helps a lot to thoroughly understand your lecture content.",1527156789.0
havokang,Would also like to know,1527156693.0
srikanth_16,Once you get comfortable with basic data structures i would suggest code up the Huffmann Coding in your primary language. Code up everything on your own it will teach you many concepts. Make sure your solution has the optimal time and space complexities.,1527158878.0
GayMakeAndModel,Know basic statistics.,1527898839.0
Dustin-,"Python for the same reasons people have already said, and C++ later on for things that Python doesn't teach you/doesn't have (memory pointers, static typing, etc). This is actually the order I learned in, and I can't imagine trying to learn C++ without some other programming experience as well.",1527129848.0
funkinaround,"I want to throw in a recommendation for [Racket](https://racket-lang.org). There are a number of different reasons why:

1. Racket is a language in the Lisp\-Scheme family and Scheme was the introductory language used at MIT
2. You have two great, free books to use as resources for learning Scheme/Racket: [How to Design Programs](http://htdp.org/2018-01-06/Book/) and [Structure and Interpretation of Computer Programs](https://mitpress.mit.edu/sites/default/files/sicp/full-text/book/book.html).
3. The language is sometimes referred to as a ""batteries included Lisp"" meaning the default installation comes with not only the base language, compiler, and runtime, but also includes libraries for GUI programming, charting/plotting, database drivers, web servers, etc. It even comes with an IDE \(DrRacket\) that will help you develop your programs by interactively letting you know when you have syntax errors or where symbols are defined. You will be able to write the same \[desktop\] programs in Racket that you use on a regular basis without too much extra complexity.
4. It will expose you to functional programming paradigms and object oriented paradigms, and if you choose to go further, even has Prolog\-style logic programming features. If you work through SICP, you will also build a register machine that will allow you to grow your understanding of how machine code is executed on your computer.
5. It has the best system for creating macros. This will allow you to write ""programmable programs"" and is a feature that is either sorely missing from or is quite gimped in all of the other languages mentioned here \(Python, C, C\+\+, Java, Go, etc.\). 
6. There is a syntactic uniformity whether you're just writing a normal program or processing HTML/XML/JSON documents. Some languages, like Scala, have XML built in, which is quite useful, but you'll then need to learn XML syntax on top of the Scala syntax for interacting with XML objects. Most other languages do not have these document/markup languages built in and rely on libraries to support their manipulation. Racket can nicely model documents in those languages as expressions that look like, feel like, and can be manipulated by all of the regular Racket expressions.
7. You can read more about why [here](https://www.quora.com/Why-should-I-learn-Lisp), [here](https://stackoverflow.com/questions/4724/why-should-i-learn-lisp), [here](http://www.gigamonkeys.com/book/introduction-why-lisp.html), and [here](https://www.quora.com/Why-is-the-Lisp-language-worth-learning). Those articles may make the case for Common Lisp instead of Racket, but the arguments in favor of Common Lisp apply to Racket as well. Also, they may not indicate that it's a good first language to learn, but all of the positives listed make it, in my mind, a great language to learn and why not have that be your first.
8. Since it is frequently used as a teaching language, the Racket community is used to answering questions from beginners. I think you'll be able to find support for any issues you encounter in whatever language you choose to learn, but it's nice that the Racket culture already has this.
9. Scheme, a language that Racket shares its history with, was first introduced in 1970 and is quite mature by now. You may have noticed that languages like Rust haven't been recommended, and that's likely because Rust is \(and other languages are\) still being developed. The Python, C, C\+\+, and Java recommendations are fine from the perspective of maturity, and Racket shares in this maturity because of Scheme.",1527153526.0
Andy54ewevee,"Spanish
",1527155656.0
philthechill,Python is the best beginner language and it's all around great.,1527127854.0
teawreckshero,"C and/or Python.

C is the go-to low level language. It's simple and doesn't have all the...""features"" that C++ has.

Python is, imo, the perfect high level language. You'll find you will start pseudocoding in something very close to python.

To practice either, I recommend working through problems on HackerRank.com",1527129941.0
NowImAllSet,"I am honestly pretty surprised none of the top comments are simply telling you that this is entirely subjective. As you can tell, you're going to get a different answer from every person. The honest truth is there's not a best language. Every one will have advantages and disadvantages, both in usage and in academia. I would think about what kind of programming interests you and go with the commonly used language. 

Think Android development is cool? Just want to get a job and be marketable? Learn Java first.

Excited about machine learning? Want to try out this whole ""programming"" thing, but not sure if it's for you? Try learning Python first.

Think web development is the future? Javascript might be a good first language to pick up.

Interested in embedded systems, or some of the more theoretical aspects of computing and algorithms? C or C++ would probably be the best break in point for that.

And with every language I mentioned, there are **strong** arguments on why that SHOULDN'T be the first language you learn. The important thing is to find something that captures your interest and causes you to stick with it. The learning curve is pretty sharp for some, so just set a goal and keep your eyes on it. Good luck! ",1527155300.0
een-ze-nood,"I would have to say java. This is based on my experiences and what I have learned. 

I went back to college at the age of 26 for computer science. I had no experience in programming whatsoever. The first language we learned was java. Java was primarily what I used for the first three years of schooling. I took one c++ course my second year, which was an object oriented language course, and a course on assembly as well. 

It wasn’t until my fourth year that I started using more languages other than java. This past semester I learned prolog, python, clojure, scala and c. 

Different languages serve different functions and are good at different things. That’s what I have learned through my computer science program. I think java has been a good base for learning overall and I feel like java  promotes better coding habits. 

I am sure people will disagree with me, and I’m sure they will have very valid arguments. This is just one perspective. Hope it helps a little!",1527128447.0
deltaSquee,"Haskell!

When you aren't used to ~~imperialist~~ imperative programming, Haskell is pretty easy to pick up.

Honestly, it probably depends on what area of compsci interests you most.",1527131012.0
FuzzNugs,"C is the first language you should learn, without a doubt. Then C++, then Python. In that order you will be building on things learned in each prior language. This path will get you the deepest understanding of “programming.” I think it’s important to learn how the languages are progressing as you learn them. Going from Python down will leave you unclear as to why each language can’t do what Python can do - “why do I need to specify a data type?” for example.",1527134363.0
tastycakeman,Prolog,1527140087.0
migafgarcia,"C, because you want to learn programming.
I would recommend Python if you wanted to make something quick.",1527145406.0
masta,"I'd say learn C. Learn pointers, and really learn them until you look back and realize that until that moment you didn't really know them, and then learn them more until that phenomena happen again. And, then go down to ASM and learn what pointers really are down deep. Then learn any weakly typed scripting language like python, or whatever. Appreciate your observations how much they suck, knowing what they are really doing down deep. Don't way to much time on weakly typed scripting languages, set yourself apart.... By avoiding the bazzar languages, go straight to the cathedral.",1527164952.0
IJzerbaard,"It's controversial, but I always recommend assembly (for any semi-reasonable ISA, maybe x64 if you want to run your code directly, maybe MIPS, whatever, not 16bit x86 in DOS though that shit is horrid).

Assembly is not easy to use for bigger programs (no built-in ways to manage complexity, it's all down to discipline), it's not easy to become fluent in (which I would not even suggest trying unless you like it) and not even easy to get going - if you're going that route, it will likely only be until you're tired of it and want to ""switch to something real"". But it does have some advantages as a starter, perhaps, depending on how you learn things.

- Once you get going, the learning curve is nice for a while. It's very incremental. After you know the fundamentals about the execution model, you can learn instructions one by one as needed. You can write non-trivial programs after learning just a handful of instructions and you'll never have to learn the whole list.
- There is no fancy syntax to learn. There's just almost no syntax.
- There are few Big Scary Concepts. Eg WTF are types? In assembly you can ignore this question/philosophical debate until you're ready for it, instead of being confronted with it immediately as you would in Java. Python is a bit gentler there but it's still an issue. There are pointers, but in assembly they are *less* mysterious than in languages that treat pointers more abstractly.
- At a fundamental level, assembly matches well with how people think about the basic steps of tasks, eg ""add this to that"" or ""if some condition, go back to step 3"", like instructions written on the back of a treasure map or like text adventures or like basic flow charts. I've yet to meet anyone who *starts out* thinking in terms of nested structured control flow, it's something you get used to later.
- It would give you some intuition for how higher level constructs can be implemented and what higher level concepts really mean. That means that when you switch to a higher level language, you won't be in a constant state of brain-freeze due to everything looking like total magic. For that's for a bottom-up learner, who gets frozen by not knowing how things work. For a top-down learner assembly is obviously a bad start.
- If you're ever going to work with a native language, you *will* be reading assembly code while debugging at some point, and you will be prepared for when that day comes.

If you start with assembly, feel absolutely free to drop it after a couple of months and maybe never look back. I really don't think it's critical to get good or even decent at it, just to have some exposure to it so that everything else makes more sense afterwards.",1527131377.0
TrainToClimb,"Learn C and everything else will come easily. That's how my university did it and I haven't had an issue switching between/learning other languages.


Edit:

A lot of people are saying Python. Python is easy to learn. Can you speak English? You can basically write Python. Python is my favorite language and what I work in most nowadays, but don't learn it first. You want to have the solid concrete understanding of programming so you can continue to learn without a lot of issues. C is the way to do that in my opinion!",1527140359.0
YourUndoing,"English, but I'm biased. Some people prefer Chinese or German.  

Oh, you mean programming language? I'm gonna give you an answer that many in here might not agree with. Learn as low a level language as you feel comfortable with first. Learn a language that makes you do a lot of the heavy lifting. Assembly would be awesome but in reality I think C is fine as a first language. If you learn how to do stuff like network programming or systems programming in a language like Python or Java, you miss many of the inner workings. Learn C first, get your hands really dirty. Then when you move up to a higher level language like Java, Python, C#, etc, you will have a better idea of how things work behind the scenes.",1527131887.0
green_meklar,"If you're serious about learning a language for comsci purposes, start with C.",1527147084.0
cyancynic,https://mooc.pharo.org,1527138203.0
Howtoeatpineapples,"Depends, Python is good for learning the very basics however if you want to dive right into making applications then I'd suggest Java or C#",1527140426.0
saumanahaii,"It's probably not terribly popular here, but I love coding in Javascript.  It can be really weird at times, but it is very easy to get going with and can be used as both an OOP and Functional language these days.  The library situation can be a bit insane but it's still a fun language to code in.  When you get further into the major it won't be as useful, depending on what the curriculum focuses on, but as a first language and one for quick work it's pretty awesome.",1527141194.0
Darksair,Learn Scheme and read the first two chapters of SICP.,1527184456.0
mikesilva11,"For me, python is the best language to start in learning programming.  It has a nice and readable syntax. Just an opinion. :)",1527464300.0
GNULinuxProgrammer,"In most top CS programs like Berkeley, MIT, Stanford they teach Python as a first language. [Here](https://cs61a.org/)'s Berkeley's introductory class.",1527128723.0
lobotomize_,"Python for sure, easy versatile and fun ",1527128343.0
d0ug,"Python—it’s a great language for wherever you decide to take your career. It’s got fantastic libraries for machine learning / math support. If you want to go more the web developer route, it’s got Django, which is a battle-tested web framework. It’s also great for one-off scripts for parsing files or directories, which opsec companies like for parsing massive amounts of logs (among other things)",1527129448.0
duckquackattack,"If you know what school you'd go to for their program, find out what they teach their beginning courses in, and start there. You will probably learn more than one, and once you learn one, learning others is much easier to pick up. In the beginning, at least for me, it was mostly about learning Object Oriented Programming \(happened to be in Java, but I mostly do C# now, which is very similar\). ",1527130361.0
code_donkey,"Take your pick of python/java/c++/c, you'll probably be introduced to them all as a comp sci major. Python has the simplest syntax of those 4.",1527131865.0
afrocluster,"Do you know any programmers or can you get access to one at least twice a month? If so, learn whatever they know and get them to teach/mentor you. 

Once you know one, learning a new one is just a time investment.

If you can't get access to a programmer then Python, Ruby, or JavaScript are your best options. Simple to install, easy to use interpreters, and they have a massive wealth of online resources to assist you with learning.",1527133338.0
RickDeveloper,"https://m.youtube.com/watch?v=poJfwre2PIs

He’s a great guy!",1527139927.0
Ravek,"It doesn’t matter, just don’t stop after just one or two.",1527141917.0
ElHermanoLoco,"IMO, either Python or Java (or C++ if you have a good teacher).

Python is great to learn primitives and control (variables, loops, conditionals, functions) as the syntax stays out of your way, importing libraries is super easy. But Python hides a lot of the more low level stuff, which is also very useful once you need to understand what's happening under the hood to debug, optimize, etc.

Java has a less accessible syntax, so there's more to get through to get the basics down, but learning it does a good job of teaching you what a computer is doing as the language handles less for you automatically. C++ is even further down that path, but it can go wrong if you don't have a good teacher (or you aren't more wired for that).

Good luck!",1527143779.0
Wurstinator,"I agree with Java or Python, though I'd prefer the former. Since others gave you plenty of reasons why you *should* pick a language, let me give you my opinion why some are worse.

*C* is rather old-school and arguably the most difficult of all those proposed. It works for the very basic stuff but you are missing out on things like OOP that is present almost everywhere in modern SWE.

*C++* has modern features but it can be very frustrating to use and learn. You often have to deal with cryptic error messages when you use code of others and sometimes even in your own code. Also, it is very difficult to find a good source to learn from (though this is true as well for experienced programmers learning the language), as a majority of tutorials and books teach terrible styles.

*Haskell* is rather obscure. It's definitely fun to learn for experienced programmers and I can recommend to everyone. That being said, I absolutely discourage using it as a first language as you can translate only a small number of concepts from Haskell to the usual languages.

*Python* is cool and easy to learn but it's flexibility brings some beginner's traps with it. Especially the lack of static typing. Now, I learned Ruby as a first language, which is very similar, and I switched to C++ fine in the end. However, I just want to note that my programs from earlier are horrendous and I am kind of glad that I lost the code.

*Java* is a bit harder than Python but brings static typing with it, forcing you to write better code from the start. In the end, I hate Java as a language for development but I am a fan of it as a beginners language. It has all modern concepts of languages like C++ and removes a lot of the more complicated features like pointers. ",1527146803.0
Arkitos,"C, then Python or whatever depending on your needs.

DEFINITELY check out Harvard's free online class CS50. It'll help you get started like nothing else!",1527151041.0
agumonkey,"pragmatically python:

- limits a lot of syntax struggles
- is easy to jump in because of repl and dynamic variables
- out of box nice set of structures: lists, dicts, sets with comprehensions
- some good stdlibs: collections, itertools
- lambdas (albeit limited but alright for first language)

then

- very trendy these days (mit, data science, numpy, pandas etc)
- dave beazley (and others) talks on metaprogramming
- generators

be warned though: this will not teach you abstraction, ""engineering"" or problem solving; lots of non programmer scientists end up writing python glue that can do magic (thanks to numpy, pandas) but is basically ..80s BASIC (aka linear list of statements with some conditionals and print output)

which leads me to the rest: I'm still from the lisp/fp school of thought so:

- HtDP v2 book (scheme based)
- SICP book (scheme based, same philosophy as HtDP but lot less arithmetic)

these will give you extremely valueable tour of the field which can avoid plateau that imperative programming (like python, naive c++) can yield.

",1527158956.0
pynick,"I can see why people think Python would be the best language for starters, but I think that its simplicity is also the weak spot as you do not learn what's really going on. If I had to choose again, I'd start with C\+\+. If you know that you should be fine with any imperative language.

\- A Python enthusiasts",1527160584.0
RSN_Bran,"I learned in the order of Python -> Java -> C/C++

Languages gradually remove the training wheels and force you to do more stuff yourself",1527161076.0
PerfectCreatures,"Assemblyyyyyyyyy! It a perfect language because it has low supply, high demand, and can be use for writing operating system! It more efficient than any language if it is written the right way! It can also be use for advance hacking and reverse engineering, too!",1527162248.0
FUZxxl,Go might be quite a productive language to learn for a beginner.  It's reasonably easy yet powerful and teaches you many things about good program design through its excellent standard library with its many examples and good documentation.  Read the book *The Go Programming Language* to learn how to Go.,1527162662.0
varanshukla,"C is the best and first choice always because of the logic and paradigms that it'll give you a better and deep understanding of how everything works, then later C++ and then any you wish.",1527163443.0
l_lecrup,"Almost everyone here is stating what order they themselves learned programming, and then retrospectively justifying it. I think this shows that there are many paths to understanding programming, and that we mostly just get exposed to different languages by chance.

Based on the above observation, I think the thing to do is pick a language with good learning materials that doesn't require picking up any secondary knowledge. If there is a course you can attend, that's great! Otherwise there are loads of online materials (and probably Python is the winner, though I am not 100% sure about that)

As for me, I learned C/C++ then Python, then a smattering of various things like C#,VBA,SQL. Right now I am learning Lisp as a bit of a hobby exercise. I think learning a low language first does force you to encounter some trickier concepts that definitely help you actually understand what is going on. Python is good to know the basics of because it is quick and useful.",1527164163.0
Jayspeer,I would say emacs-lisp. It greatly improved my vim experience,1527164516.0
SirWusel,"In my opinion/experience, the first language depends on the person. If you can handle writing relatively ""uninteresting"" software for a while without losing interest in programming, then learning something like C would definitely have its advantages, since you will be required to learn things more in\-depth.

If you want to jump into building cool things relatively quickly, then something like Python is the best choice. It handles a lot of things like eg memory management or references for you \(at least for the most part\) and allows you to focus more on building actual applications. For beginners, it's also easier to debug and play around with, compared to languages like C or Java. And Python is also still a very desired language, as far as I'm aware. But sooner or later you should still go back to a lower\-level language and learn about some of the underlying concepts.",1527165524.0
GhostInTheCode,"There is no perfect language to start with, but there are languages that are probably worth avoiding. That being said there are a few suggestions I could make:  


* If you value simplicity, Python.
* If you value verbosity and explicity, Java
* If you value low level understanding from the offset, C/C\+\+
* Javascript could be good for an internet\-focused beginning
* If you're really mad, and want to start pretty much from the lowest levels of computing... an  assembly language. You could go for x86 ASM, but I would probably recommend something a bit more friendly such as MIPS.
* If general creativity is your thing, you might want to look at languages such as processing.

But seeing as this is curiosity to decide a major... I'd definitely recommend Python. You don't need to get too into the deep end, you should be taught from the ground up anyways.. and it's reasonably quick to get interesting results from your programming. And even more than that, it's simple enough that you can focus on getting your programming structures and techniques right without worrying so much about syntax \- it reads pretty easily in most cases.",1527171610.0
Nodebunny,C,1527172965.0
eviltofu,Elixir.,1527173373.0
mcandre,Yes.,1527186988.0
savage_slurpie,Python because it's non-venemous,1527195492.0
elcric_krej,"Don't learn one, learn a few at the same time.",1527197513.0
forcenuggets,JavaScript.,1527133074.0
berryer,"even though it sucks as a language, because it sounds like you're in HS I'd recommend TI-Basic.  Programming works the best when you have concrete projects - code up some shortcuts and formulas for whatever math & science courses you're in now!  Alternatively, if you're into any games that have modding or botting communities, those can be an interesting place to learn - just learn whatever language their toolsets use.",1527130269.0
Oscujic,Scratch,1527131828.0
jkuhl_prog,"JavaScript or Python.  They're both easy to set up and easy to get going with.  You can learn basic programming concepts from both of them.

Then, when you move on to languages like C++, you can focus more on things like types and pointers and memory management and all the trickier details that JS and Python both gloss over (or avoid completely) rather than trying to figure out those concepts while also trying to figure out the basics.

The downside to JavaScript however is that generally you'll want to know CSS and HTML to go along with it.",1527136999.0
DE4THM4RK,"A lot of people have recommended Python and I will agree with them.  Python is what I consider an elegant programming language. Clean code. Easy to follow. Lots to learn in whichever field you take your career to. 

When I started college I did an introductory class with Python and then they started us on C++. While it was good to learn about pointers and memory management I leaned towards Python everytime I wanted to code for myself. For my Data Structures class I implemented everything I learned in Python just to get it in my head. 

It may not be perfect but it is definitely a good place to start. Plus the tutorials and libraries are plenty. ",1527139821.0
vaer-k,"I think python is a bad idea because it hides so many fundamental cs concepts from you. This is why it's so easy to write -- so much is abstracted away from you. I started with Python and when I moved to other languages I was surprised and confused by many common patterns and concerns, especially when it comes to fundamental data structures.

I would recommend instead starting with java. 

It's a great general purpose language with a huge platform and  ecosystem, long history, and lots of  documentation. It's not so down-to-the-metal as C or C++, so you don't have to worry about getting mired in details as a beginner, but it exposes you to many important concepts you might not otherwise know to consider.",1527147277.0
Van-Storm,"Python was my first, it’s really easy to learn and self-teach because it runs in its own program and the commands are simple.  Don’t know how much you could actually do with it though, it’s more for numbers.",1527127932.0
parthnuke,python is best language for beginners. There is course on edx from MIT. I learned from there. ,1527129667.0
17WANGC,Ruby on Rails,1527133809.0
pythonicus,"Learn FP first, don't let OOP rot your brain.",1527135369.0
Leobiosoul,English ,1527128768.0
bartonsmart,Python seems to be the consensus.,1527132153.0
gopnik_conscript,Python,1527138348.0
The_Sloth_Racer,My first language in college was C#. Do **NOT** do that. Avoid!,1527166655.0
ianwold,BASIC (old school) or Python (new school). C if you're feeling adventurous,1527134056.0
BruddaTurtle,Python my dude,1527137970.0
fmresearchnovak,"Programming is the act of planning or writing a program and a program in this context is a glorified sequence of instructions.  When you are using a program (photoshop, text editor, etc.) in general you are not creating a set of instructions, you are performing a set of instructions (which you are probably coming up with in your head on the fly).

&nbsp;

A GUI html editor is a bit of an edge case and in the computer science community I think there would be a lot of debate about whether or not it is considered programming.  Personally I think it is.  When designing a GUI you are providing instructions to the computer about how to display data.  However creating / editing a GUI is very simple compared to writing executable code.

&nbsp;

I think a helpful rule of thumb that helps see where the line is drawn is that programming creates some artifact that the computer can run again and again.  A GUI can be displayed (over and over) and a program can be run (over and over) but when you go into photoshop and edit an image, the steps of editing cannot be repeated unless you physically do them again.  So, you did not write a program for the computer when you did those steps.",1527083282.0
YourUndoing,"Programming is designing a set of instructions designed to solve some sort of problem, or accomplish some sort of goal. You can program by drawing symbols on a piece of paper or in the dirt, if you have some mechanism to execute those instructions.",1527093388.0
khedoros,"> What about a GUI html editor?

HTML can't encode programs; it isn't a programming language. It's a document encoding language. ""Coding"" has a wider definition to me than ""programming"" does. I'd say that you can ""code"" in HTML, but you can't ""program"" in HTML.

> Wouldn't any operation that changes the state of the machine be 'programming'?

No. By that definition, running someone else's program would be programming.

> Where is the line drawn?

A programming language needs a few things. It needs the ability to take input, provide output, change the computer's state, and to make decisions based on the computer's state, or the input that it's provided. Notice that I'm drawing the line based on what the language can do, not on what a particular program can do. After all, ""Hello World"" doesn't take input or react to anything.

> Must programming strictly be writing code?

Programming doesn't have to involve writing. There are graphical programming languages. But the program is still ultimately converted into a string of commands for the computer to run. Since it's often convenient to represent those commands as text, most programming languages are based around text.",1527095880.0
sparcxs,"Is it Turing complete, that’s the canonical differentiator. Can you write a Turing machine in Photoshop? No, then it’s not programming. If the behavior can be set to change based on input, it is likely programming. That still casts a very wide net. A much narrower definition would be, can the tool be used to bootstrap itself. Can you create a Photoshop clone with Photoshop. That’s a no. Unfortunately that ensnares interpreted languages like JavaScript which are certainly considered programming. I’d stick with the Turing Completeness test.",1527110686.0
derefr,"Pedantic point: you're maybe equivocating between two different meanings of the word ""program"" here.

In software engineering, ""program"" is a transitive verb whose object is software, where the definition is ""to create/maintain/develop said software.""

But there is an older usage, from digital logic, where ""program"" is a transitive verb whose object is a digital-logic device, where the definition is ""overwrite zeroes with ones on said device."" This is from where you get nouns like ""programmable timer"" or ""Electronically Erasable Programmable Read-Only Memory."" Even modern flash devices speak of ""Program/Erase cycles.""

In the older digital-logic sense, yes—anything you're doing on a computer will involve ""programming"" the computer's memory.

But this usage has nothing to do with what people mean when they speak of ""programming"" as in *developing a program*; and when you are in the jargon-domain of software engineering, you should really just ignore the existence of the *overwriting zeroes with ones* definition.

To not do so, would be like assuming that a particular greek symbol means Foo because it means Foo in your favorite branch of physics. Each scientific/engineering domain gets to define what these words/symbols mean for itself!",1527123055.0
mtaon,"Programming doesn't necessarily mean writing code. There are many example of [visual programming languages](https://en.wikipedia.org/wiki/Visual_programming_language).

Code is just one representation of the program. In a compiler this text is usually parsed into [abstract syntax tree](https://en.wikipedia.org/wiki/Abstract_syntax_tree). Visual programming languages like [Scratch](https://en.wikipedia.org/wiki/Scratch_\(programming_language\)) and your example of a GUI HTML editor are based on modifying the program as a tree. In a way this not too different from writing code: the only difference is that the user is not able to create programs with invalid syntax.",1527085099.0
WhackAMoleE,">  typing into a text editor

Is writing a grocery list the same as writing a novel?",1527093226.0
vangrif,">But technically could you consider something like image manipulation in photoshop... to be programming? 

Funny that you mention that. There is an esoteric programming language that uses images as it's source code:
http://www.dangermouse.net/esoteric/piet.html

Edit: grammar",1527118387.0
leftofzen,"The end result of 'programming' is to produce a set of instructions that a computer can understand and which perform some calculation for you. I have no idea where you are getting the ""technically"" from when you say technically image manipulation (etc) is programming, because you are wrong.

* Image manipulation - not programming. You are not writing any instructions, you are simply instructing a program to use existing code to achieve an effect
* Typing into a text editor - if you are going to use a compiler and turn this into a program then yes, that is programming. Simply saving random text into a file is not programming.
* Moving elements in a GUI HTML editor is programming. You are writing (with the help of GUI tools) a list of instructions for an HTML parser to run.

The line is drawn at a list of instructions. What about image manipulation corresponds to writing a list of instructions that the computer can understand and execute? Nothing (except perhaps your image manipulation software has a scripting capability, in which case using that would be programming to a degree). You invoking existing behaviour like ""Fill"" is not programming, you are actually performing the instructions, not writing them.",1527118473.0
,"You are looking in the wrong direction. Yes, there are many WYSIWYG stlyle programs and they could all be called programming in a sense. But the real knowledge that most programmers have that separates them for script kiddies and 'scrum masters' is software engineering. To even start coding, you need the theory of computation \- time complexity, algorithmic efficiency, proof, recursion, etc. You need to understand the componets and services and how they interact. Cohesion, coupling,  code quality metrics, patterns, architecture, etc etc etc. Coding is the basics. The absolute lowest basic level of what a programmer does. Don't look to simplify it.",1527109189.0
teawreckshero,"To answer a question with a question, what is ""code""? It seems you mean in some ""language"" or according to some ""grammar"", i.e. obeying some formal pattern. So I would say yes, but I would also so what you think of as code or language is far broader than how you currently think of it.

Edit: to answer your last question: the line, I believe, would be the chomsky hierarchy. If you are doing something that is not in accordance with a formal grammar, then you are not coding. But I'm betting more things in our universe are formalizable than we recognize.",1527126873.0
brookhaven_dude,"It's something similar to building a big house. 

There is the architect who does the design. The engineer who makes sure things are structurally sound. There are construction workers who actually do the digging, laying bricks, wood work etc. There is electrical and plumbing. Indirectly, places like home depot and lowes are involved in providing tools and infrastructure.

You will perform one of these roles. You could be a coder, architect, tool maker etc. It all depends on interest, experience, ability and money. ",1527268294.0
menahemendel,Most hackers don’t know how to write code,1527096438.0
Fricken_Oatmeal,"Here's lecture notes from a computer architecture class that uses MIPS:
https://wiki.illinois.edu/wiki/display/cs233sp18/Lectures",1527046053.0
jokeeffe007,"Regarding ""XOR Encryption"" ...  If you XOR a data byte with a key byte two times it will return to the original data.  Basically you need to XOR a series of bytes to ""encrypt"" the data and then you can XOR them again to get back to the original data bytes.",1527080940.0
Tappedout0324,"https://m.youtube.com/watch?v=eUH-mfp_5BM

https://m.youtube.com/watch?v=uLDhuw4lPyI

Used to watch this during my study sessions few years ago, not sure if they’re helpful but look around YouTube you’ll find something ",1527044974.0
kdnbfkm,If you have access to a MIPS compiler write a bunch of short programs and look at the assembly output (or disassemble output). Compiler code won't be exactly like human writen assembly but it might give you some ideas.,1527071307.0
kscroggz,Lmfao bro I’m legit in your class. I couldn’t agree with you more. CS-264 T-Th,1527098848.0
mara2701,"I like this youtube\-tutorial for MIPS: [https://www.youtube.com/watch?v=u5Foo6mmW0I&list=PL5b07qlmA3P6zUdDf\-o97ddfpvPFuNa5A](https://www.youtube.com/watch?v=u5Foo6mmW0I&list=PL5b07qlmA3P6zUdDf-o97ddfpvPFuNa5A)

In addition, you could read ""Digital Design and Computer Architecture"" by David Money Harris & Sarah L. Harris. The book covers a lot more subjects, but they provide a thorough introduction to the microarchitecture of the MIPS processor and the MIPS assembly language.

I feel for you: Our professor just handed us a long list of MIPS\-assembly commands without ANY further explanation...programming in assembly can be so much fun, but universities tend to destroy it. Keep going, you will like assembly in the end!",1527076435.0
mmcdermott011,"Just finished my architecture class where we touched on assembly (not specifically mips) 
If you’re looking for concepts as well as how work like xor,  we used the nand2tetris.org site ",1527051630.0
LearnerPermit,"I feel for you,  when I was a freshman, I took a class on 6800 assembly at a community college, because that's what the university I wanted to take expected you to know.

I ended up taking assembly again because that particular university only accept about a dozen CS major transfers a year. And my second choice preferred 8086. When I transferred to the university, upper division digital architecture was all mips focused.

I think honestly you just have to struggle through it enough to get a passing grade. The only important concepts you'll need to retain for most careers is the stack, how memory is stored in hardware the trivia of how floating point works. ",1527049896.0
lFailedTheTuringTest,"I have one stack specific projects for you to get started:

Generate  2 arrays A and B of specified size(input from user at run time) filled with random integers and display their dot product or element wise multiplication. This project involves simple pushing and popping from stack. You will need maintain stack pointer backups and store the size of arrays somewhere. The size of your stack pointer will be ""input size of array"" * 4(bytes) * 2. 

This project should take you roughly 4 hours to code and test from my approximation of your skills in MIPS.

Feel free to get back to me if you need help or need me to elaborate on the problem further. 

About the encryption stuff: XOR basically undos the XOR operation if you do it again with the same number. So C= A XOR B will be some integer. And then C XOR B will give you A back. Try generating random keys, store them in a static stack location or just one of the available registers. You will encrypt when you do XOR with a key, this will give you an encrypted message. You will decrypt when you XOR the encrypted message with the same key. If you do successive encryptions and then succesive decryptions you will stillget your original number back.

You can generate random numbers in assembly using a simple routine that implements the Tausworth algorithm.

One thing that has always helped me with assembly programming is imagining the registers as global variables.
",1527100248.0
stevenxdavis,"I don't know what program you're using to implement your programs, but I really liked [MARS](http://courses.missouristate.edu/KenVollmar/mars/) when I was learning MIPS assembly. It was much more helpful than what the teaching assistant had recommended.",1527125800.0
PTomCruiser1,Learning MIPS was the worst. You have my sympathy :(,1527099437.0
boilerDownHammerUp,"I was under the impression that synchronous logic meant that things happen on a clock signal, (for example, a register is synchronous) and asynchronous logic is happening always (a logic gate). Next state logic typically refers to a state machine, where the next state is typically decided using asynchronous logic, and the next state becomes the current state on the clock cycle (synchronous logic). If anyone understands this better please let me know, I could have it wrong!",1527043478.0
cthulu0,"
Here is the only think I can think of that can make sense of what your professor said:

You can make a  circuit that  externally behaves like a combinational circuit (its outputs are functions only of its current inputs, not past history) but  the internal implementation is sequential (due to efficiency concerns) and thus the internal implementation can be synchronous or asynchronous.
THIS ONLY WORKS IF THE INPUTS CHANGE SLOW ENOUGH.

E.g. Suppose you want to make a combinational circuit that adds N  multi-bit inputs.  The output is just the sum of the current  N inputs.

The brute force standard conventional combinational way is an adder tree. 

However suppose this circuit had an internally generated clock (so the clock is not visible to the outside world) and flip flops.  Then it could sequentially scan every input and do a running accumulator sum. Thus only 1 adder (instead of an adder tree) and 1 register is needed. 

As long as the inputs only change once every N clock cycles and the output is only looked at every N clock cycles,  this circuit looks purely combinational to the outside world because remember the clock is internally generated and doesn't leave the block.

That's the only think I can think of to explain your professor's strange statement.
",1527090368.0
lana_sciencee,I am quite confused? Can someone please explain this to me?,1527050643.0
Elekhyr,"It's called strong typing :

- https://arne-mertz.de/2016/11/stronger-types/
- https://www.fluentcpp.com/2016/12/08/strong-types-for-strong-interfaces/
- https://www.boost.org/doc/libs/1_37_0/boost/strong_typedef.hpp",1526997187.0
Kel-nage,"This form of typing is very common in the programming language Haskell, primarily because it makes it very easy to create new datatypes from existing ones (it doesn't need all the boiler plate that languages like Java or C# need when constructing new classes).

However, in my experience, the constraints applied in Haskell are much more relaxed than you've described in your comments (so for example, the fullname would just be a string). This is in part due to being lazy - but also because often for the kind of problem you've described, hard and fast rules about allowed values are either very complicated (such as the [regex for validating an email address](http://www.ex-parrot.com/~pdw/Mail-RFC822-Address.html)) or are not as well defined as you might think (such as described in the classic [falsehoods programmers believe about names](https://www.kalzumeus.com/2010/06/17/falsehoods-programmers-believe-about-names/) article).",1526999988.0
eegabooga,"Not sure if what you’re talking about has a specific name. But Domain Driven Design, domain modeling, and rich domain model (vs. anemic domain model) are some topics that touch on similar ideas. ",1526996935.0
dasFisch,"I used to have something like this on my resume, years ago. Didn't really phase anyone. Try not to mention the porn or anything, and really focus on the technology and work you did. 

Good luck!",1526959419.0
PC__LOAD__LETTER,You made an “entertainment media application” that utilized machine learning. You can speak volumes on technical detail without going into the dirty bits. Source: interviewed a bunch of people with this exact type of thing on their resume. No one cares if you don’t make it obvious.,1526963535.0
jnwatson,"A few months ago, this guy's resume was passed around. He had a link to his github account, so I figured I'd take a look.

The first entry on his github project list replaced the contents of a hard drive with randomly generated ASCII penises.

I was like, we MUST hire this guy. And we did. (It helped that he had a great resume).

No joke.",1526965693.0
usernamy,"All you really have to do is talk about what you actually did. No one will care unless you keep talking about porn. 

“Implemented framework x to provide a streaming service to x amount of people”

“Set up infrastructure using x that allowed x amount of connections with x data points”

Unless you’re saying “provided AI to get people to jack off quicker and more efficiently” or “used a neural network to figure out what style of hardcore fetishes lead to the biggest profits” you’re good ",1526963543.0
spongebob,As long as it didn't involve any PHP I'd say yes :-),1526972278.0
findandwrite,"I think this has been answered before with something to the effect of do it, use soft language and and their really interested that they press you for more info, they’ll be ok. 

Unless it’s like wierd scat porn. That’s never a good look",1526958022.0
rick2g,"“I created a Machine Learning stack integrating particularized visual partitioning and finely differentiated facial recognition methods combined with a wide range of unique biometric identification data points generated by both dynamic body motion pattern matching and static optical analysis to aid in the rapid fetching of specialized query specifications.”

“Porn Search Engine?”

“Porn Search Engine.”  ",1526993671.0
SharingWasCaring,"Thanks for all the responses,seems like talking about the code and not being explicit with the application is the consensus",1526969819.0
jimmahdean,You could ask in /r/cscareerquestions?,1526957901.0
MCGoddard,"My first foray into embedded development was a dimmer switch that turned up the Barry White whilst you turn down the lights... Wasn't on my CV, but was on my portfolio website. Came up at interview, think it might be why I have this job...",1526984164.0
thepunitsingh,"You can mention it in this way: worked on a machine learning project for classification of nsfw images from safe for work images.
Since nsfw includes a large range of things such as nudity, violence etc the interviewer then may or may not ask what kind of images were that. In case he/she asks tell him about pornographic images in soft language as they were image of not of legal age or they include nudity, avoid using porn or pornographic.",1526962396.0
YourUndoing,"Use non-descript language in terms of describing the industry you worked for. This is actually pretty common for those of us who have done consulting work with clients that have worked NDAs into the contract. The last consulting company I worked for had a database with client names and the industry descriptor we needed to use in our resumes for clients. We actually had a living resume that was kept up to date so that it could be provided to clients when a team was getting put together for a project. So for instance, if we had a specific health care company we worked for, it would just say Major Healthcare Organization. For something like a NSFW company it would be much more non-descript (we had a few of those as well, although not many, and most were relatively tame). And it would be more something like Online Video Delivery Platform or Entertainment Provider.",1526962748.0
kennethjor,"It really depends on the person reading your CV and you may want to alter your CV depending on where you're applying.

Personally I'd put it on there, but not directly mention the NSFW nature of it. Talk around it. As others have mentioned, the fact that it's NSFW is unlikely to be the heart of the project, the machine learning bit would be.

On another note, if someone doesn't want to hire you because of it, that's probably not a person you'd want to work with anyway?",1526989776.0
AaronKClark,"I used to help run a couple escort sites in the 90s. I put it on my resume to get my foot in the door early in my carreeer, then dropped it when I had more experience.",1526967287.0
elcric_krej,"It depends, it depends on the exact project and on the part of the world you are in.

Personally I had a kink-related website project on my profiles (angelist, linkedin, stack overflow and resume link) for about 1 year (I decided against renewing the domain and shut it down). I got plenty of offers during that period, including from a guy which was openly religious (Jehovah witness I think, or some other such evangelist sect), who I ended up working for.

But my experience is working with people from Western Europe, it may be that I could have really ticked off someone from the US bible belt or Arabia/Iran, so it's all very bound by context.

If you are in the Netherlands, feel free to have a scat fetish website or prostitution ad board on your resume, as long as it's well designed, I doubt many people will give a shit. If you are in Pakistan... maybe reconsider that. 

Obviously those are extreme examples, the problem really lies in the ""in between"" parts of the world.  That would require some intuition and knowledge of local culture on your part.

But think about it this way: If you think this project is ethical and you think it's interesting enough to fit a resume, would you want to work with people who are so sensitive as to be turned-off by the theme of it ? Would you get along with those people in the first place ?",1526995357.0
GNULinuxProgrammer,Focus on the algorithms/technology/engineering you plan to use and mention those.,1526969442.0
bastih01,"I went with ""Adult Entertainment"" to describe this kind of things.",1526976251.0
pure_x01,"If you don't offer to make deepfakes of the bosses wife you should be fine .



In all seriousness focus on the technology and not the domain. If the domain is important the refer to it as the ""adult industry"". A higher abstraction would be ""entertainment industry""",1526998612.0
markth_wi,"Having done something like this, don't focus on the topic. Focus on the work.

So the spiffy porn taste matching cross-reference routine you wrote is PRETTY easily tweaked to be a alternative to requested inventory cross-reference matching program, now isn't it. 

That is something you can pitch around all day long.",1527002725.0
locotxwork,"I would focus on the GOALS and OBJECTIVES of the project.   Were all objectives met on time? Did you go over budget?  What did you learn from the project and how can it be applied to help us save time or money.  I would mention it because it shows it doesnt' matter what the context is, if you know the objectives and the work, you can be mature enough to look past the content and focus on the work.  ",1527007535.0
sparr,"I did IT for a porn(ish) company. No one cares. It's one or two sentences of an interview, at most.",1527008526.0
cclites,"I worked for a company that delivered on-demand adult movies in hotels world-wide. Damn straight I added it to my resume, along with outlining my contributions to the project. 

Nobody cares about what you did - they care how you did it.",1527012166.0
Lendari,Consider leaving it off. If removing it significantly reduces your experience then keep it high level. Focus on what the project accomplished and why it was important in broad technical terms.,1526962291.0
rainbowsixtrash,Idk something about hacking finds out the truth.,1526938460.0
Aleksis_Boleslav,it's not a philosophical message. don't just look at the image. analyse it.,1526938550.0
jhp2000,"All of Leslie Lamport's papers are here: https://lamport.azurewebsites.net/pubs/pubs.html

He's a giant in distributed systems obviously, and also a really good technical writer.

The papers also have some brief comments attached, which helps to know which ones might be interesting, are especially important or groundbreaking, etc.",1526944704.0
j3pl,Haven't read much of this but it looks pretty good: [Distributed Systems for Fun and Profit](http://book.mixu.net/distsys/),1526950488.0
trobertson,"You've probably encountered all of these before, but they are absolutely required understanding for any work in industry distributed systems: [Lamport](https://en.wikipedia.org/wiki/Lamport_timestamps)/[Vector](https://en.wikipedia.org/wiki/Vector_clock) clocks for ordering events, [Paxos](https://en.wikipedia.org/wiki/Paxos_\(computer_science\)) (""legacy"" systems) and [Raft](https://en.wikipedia.org/wiki/Raft_\(computer_science\)) (recent and future systems) for distributed consensus.

You should, at the very least, read the papers for Lamport clocks and Raft.",1526951378.0
araeld,"I found this book to be one of the best introductions to the subject: [https://www.amazon.com/Introduction\-Reliable\-Secure\-Distributed\-Programming/dp/3642152597/ref=sr\_1\_1?ie=UTF8&qid=1526978779&sr=8\-1&keywords=reliable\+and\+secure\+distributed\+programming](https://www.amazon.com/Introduction-Reliable-Secure-Distributed-Programming/dp/3642152597/ref=sr_1_1?ie=UTF8&qid=1526978779&sr=8-1&keywords=reliable+and+secure+distributed+programming) . This book covers a lot of the main topics, such as broadcast, distributed shared memory and consensus, while also covering vector clocks, failure detectors, byzantine failures and so on, which are the building blocks for those complex algorithms. Although the book is very intensive in theory, it relies much more in intuition than in mathematical proof.

After understanding the foundation, you can seek technical papers on your own, such as Raft and Lamport's papers already mentioned here by other people.",1526979444.0
ryfm,this one is good https://amazon.com/Designing-Data-Intensive-Applications-Reliable-Maintainable/dp/1449373321/,1526963624.0
Doctor-Awesome,"You might already be beyond this, but I like the Lawrence Livermore National Labs tutorials (they have some MPI ones) - https://hpc.llnl.gov/training/tutorials#training_materials",1526935410.0
mattyw83,There's also distributed systems by Tanenbaum and Steen which they released for free last year: https://www.distributed-systems.net/index.php/books/distributed-systems-3rd-edition-2017/,1527058275.0
jkff,https://www.amazon.com/Elements-Distributed-Computing-Vijay-Garg/dp/0471036005 is a really good and fairly accessible book.,1526958364.0
Lucretia9,"Grab gnat with poly orb, check the Ada 2012 reference manual for annex E, it’s the only language I know of with direct support for distributed systems. 

Although I think the aim was to have heterogeneous small computers all talking to each other within the same application, like a plane.",1526984569.0
Ty1eRRR,"hey /u/chtuzpah . I am currently in the same situation as you were. Have you found a project for this topic? 

What you ended up with?",1541936828.0
libscott,I suspect its impossible tbh. ,1526987892.0
magnificentbop,"Have thread A use a counter, and only release the semaphore when it has done it's work 4 times.  Use a signal when it is done to tell thread B to fire.  Use a counter in B to do work twice.",1526996740.0
repsilat,"Looks like dynamic programming to me. Figure out how you'd solve it for some `n` and `k=1` (or `k=0`), and how you might extend a solution from `k` to `k+1`.

A hint: there are two dynamic programming ""variables"":

1. The ""time step"", and

2. Whether you have the potato at that time step.

The thing you store in the k×2 array/memo is the number of ways to get to that situation.",1526920392.0
emef,"Generally speaking, you want to have a good grasp of the fundamentals, so that when working with a new language/library/framework you mostly understand what's happening behind the scenes of the interface you're using. Taking that to your examples, if you were building a site using some web framework you understand basic HTTP mechanisms (request-response, GET, POST, cookies, SSL, etc), how requests are routed to handlers, how the ORM is mapping operations to SQL, etc. etc. Each library you use will do a lot of the same things just with a different interface, so you'll have a much easier time working with new tools if you know the fundamentals. Similarly with data science (specifically ML), there is a pretty standard workflow in terms of preparing data (extracting features from source data, handle missing/default values, normalize, etc), choosing evaluation metrics (accuracy, PRAUC, log loss, etc.), splitting your datasets into train/test/eval, tuning model hyperparameters, etc. A bunch of these steps are the same for different problems of the same class, often supervised learning. You don't need to learn ahead of time all the details of tensorflow, torch, scikit-learn, theano libraries.",1526917370.0
fmresearchnovak,"I think your mode of thinking is not absolutely correct.  You seem to think that some things you will have to learn ""as you go"" and other things are things you should just know (should have already learned) and that these two sets are separate.

&nbsp;

Actually, I think there are many many things in comp-sci and a limited number of things you can be fluent in on a given day / week.  So, you can learn to become fluent in only a few things at a time.  Trading them in and out as needed.

&nbsp; 

Being fluent is impressive to employers because it shows how fluent you are able to become.  However, it is unlikely that you are fluent in the precise thing a particular employer is looking for (if you do then you struck gold!).  

&nbsp;

So, to cover the last base, you should be able to learn new things quickly.  That way, if an employer says ""use abc framework"" you can say ""no problem!"" and then go teach yourself.  It's not that the framework was something you should have learned before.  Or that it falls into the ""learn as you go"" set.  It just happened that you have yet to become fluent in that!

&nbsp;

Showing that you can learn anything quickly and effectively is what is really impressive.  But, obviously, that takes time to demonstrate.  Does that make sense?

",1526930676.0
IAmDaBadMan,"Speak with your professor or academic advisor.  They will probably know someone in the same situation as you and can get the two of you together.  Also, see if your school has a computer science or programming club.  ",1526907310.0
jet_heller,"We had an organization at school for computer science people. We had an office at on the university grounds. Anyone could go there and talk to others in that field of study. Does your university have something similar? If not, do you have interest in starting such a group? ",1526912820.0
duckduckfuckfuck,try codeforces.com it has a very large and active community and the problems are pretty good.,1526918596.0
KillerBofSteel,"Is there a computer lab where you go to school? Like an area for CS majors to use conputers? If so, you may bump into people there that may be taking the same classes as you and in the same situation ",1526923823.0
fmresearchnovak,"Are there other first generation students in comp sci at your school?

&nbsp;

Also, you can use this community!  I'm sure that interview questions and support can be found here :)
",1526931851.0
The_Jeremy,find people during office hours; worked for me.,1526955537.0
ggchappell,"Very nice paper. I wish there were more publications like this: ""Here are some ideas that can improve presentations of a topic.""",1526934907.0
shahidiceprince,"[Operating Systems: Three Easy Pieces](http://pages.cs.wisc.edu/~remzi/OSTEP/)

We used this book in the CS450 Operating Systems master's course. It details a Unix-based OS called xv6 created at MIT. It's one of the best books to learn and understand the inner workings of an OS.

Oh, and it's free.

P.S: [xv6 source code on GitHub](https://github.com/mit-pdos/xv6-public) ",1526895027.0
sabas123,"If you want most of the essentials how memory works. I think this is the best guide you can find on the internet.

[Ulrich Drepper: What Every Programmer Should Know About Memory](https://www.akkadia.org/drepper/cpumemory.pdf)",1526892806.0
bartturner,"Wish I had a perfect source for you.   But what I found to be a great source is lwn.net

https://lwn.net/Kernel/Index/",1526900713.0
esderpp,My computer systems org class used a book called like computers a programmers perspective,1526958816.0
goxul,Computer Organisation by Hamacher et. al had a lot of these concepts which I used for my undergraduate studies.,1526976294.0
Lucretia9,Implement a kernel for real hardware.,1526987251.0
green_meklar,"Maybe there are environments you can set up that are rigged against A\* and make it way less efficient? These might take a ridiculous amount of computing power to test efficiently, though...",1526867288.0
klysm,"Given that A* is optimal in the sense that it looks at the minimum number of nodes possible for any admissible algorithm, so I would go as far as to claim a GA couldn’t actually beat A* at its own game. However, if we relax the constraints to be near optimal (clearly A* doesn’t really matter anymore), genetic algorithms [have been shown to be good.](https://pdfs.semanticscholar.org/849b/f9404a98d496b6ea107e6e7a72d31ca9ff51.pdf)

I think you’ll find your means of comparison are going to be a bit handwavy until you really pin down that research question. ",1526878739.0
Hook3d,"What are the use cases and what does the state space look like? 

I'm tired so this might not be 100% accurate; I'd enjoy clarification if I'm off-base. 

Intuitively I would say that a genetic algorithm can obtain better performance in certain cases because A* is complete like normal BFS, whereas genetic algorithms are afaik not complete (not guaranteed to converge to a global optimum). Perhaps if the candidate pool is kept sufficiently small after every selection step, it can improve on A* in situations with a lot of branching. 

For instance, consider a graph **V** which has an average branching factor of 10 and a vertex *u* which has ten outgoing edges to vertices *v*[0] to *v*[9] with edge weights of E[0...n] + heuristic(E[0...n]), and they are equal. Your A* algorithm will explore them all in whatever order they are encountered (since they all have equivalent priority). If you know that there are many optimal solutions in your particular problem / state space, you can tell your genetic algorithm to use the same heuristic but put a cap on the number of candidate solutions that can be added to your solution set, say 5. Then your genetic algorithm will explore half as many edges as the A* search. I think this would be mostly useful in dense graphs with many optimal or near-optimal solutions. 

I think?",1526877590.0
julesjacobs,"\>  Thus, I am struggling to come up with ways to create a meaningful comparison between the two algorithms.

Why do you want to reach a conclusion other than the conclusion that's obviously correct, namely that A\* beats GA whenever A\* is applicable? GA isn't even guaranteed to find the optimal path.",1526908439.0
evil_burrito,"If you're willing to rig the game, you would want to make the search space absurdly large and also sprinkle in some tricky min/max hillclimbing traps. GA would be unaffected by either of these problems whereas traditional search algorithms may get caught by either or both.",1526938411.0
,"A* is not always faster, it depends on the application. The travelling salesman problem is significantly faster using GA for example ",1526867456.0
bartturner,Nice,1526817453.0
GoblinsStoleMyHouse,10/10,1526835621.0
Catakryst,"It'd be worth describing how votes are done in the leader election process. When a node sends the RequestVote RPC, it also includes information about the latest entry in its log, and a follower will only vote positively if it's at least as up to date. Otherwise, it's possible that a committed change is overwritten (see the original paper for details) ",1526836426.0
voronaam,"What never ceases to amuse me in CS is how trivial ideas get published, named and even visualizations are built to explain them.

Great job on the visualization though. Thanks for sharing.",1526837129.0
keten,"I think one of the reasons causation is hard to model in machine learning algorithms is determining cause and effect is necessarily an interactive process. 

A lot of the time machine learning algorithms are not involved in the data collection process, rather they are fed big datasets and expected to find the pattern in them. The best you can do in this situation is find correlations. Determining cause and effect necessarily requires multiple stages; you need the scientific method.

There's a reason we don't do this now very often - it's really expensive! You essentially need to give your algorithms ""bodies"", or some way of interacting with their environment to run experiments. And what if the body they are given is insufficient for running the kinds of experiments necessary? Now you need to give them a way to communicate their needs to others. You need to make sure there are adequate safeguards so that the body doesn't cause harm or damage things in the real world. You need to maintain the body. A lot of the time it's probably just going to be cheaper to hire people to solve your problem.",1526788523.0
GayMakeAndModel,"To build truly intelligent machines, give them a rich environment.  Same as with humans.",1527899360.0
fmresearchnovak,"You're using github as a venue to host a very simple webpage which is simply a list of articles written by others.  You've curated the list (great job), and seem to be interested in others making carefully thought out suggestions for additions to the list.

&nbsp;

I think that the project is difficult to comment on because you've done so little.  The difficulty of recommending things is not really making recommendations, but earning the respect of others to value your recommendations.

&nbsp;

You should make something big and cool that requires you to write some code.  Maybe your article system could show hit-counts or measure the usefulness of each article based on user reviews.",1526932621.0
justinba1010,"I highly recommend looking past the money, really do this for the pleasure it will be a whole lot easier. Second it will be difficult if not impossible to get any deep understanding of all of those specializations. I say go through the normal curriculum, then specialize into something you like. Artificial intelligence has a lot of math, at least a lot of game theory and a good amount of discrete. Machine learning is a lot of statistics I believe. Cryptography is barely undergraduate number theory and other fields. Considering at least 3 of your specializations are deeply rooted in math, I recommend following some math curriculum.

Get Thomas Calculus\(or pirate it\), and go through Calc 1, 2, 3. Get a book on Discrete Mathematics, and a book on Linear Algebra and if you really want the introduction to crypto, get a good number theory book. I am currently a Math and CS major at South Carolina, so here's their roadmap for reference.  [https://cse.sc.edu/files/CScurriculum2016.pdf](https://cse.sc.edu/files/CScurriculum2016.pdf)

Good luck!

Edit: By ""barely undergraduate number theory"" I mean if you want to get past RSA and Diffie\-Hellman you're probably going to need some if not entirely graduate level understandings.",1526755715.0
GMU-CS,"Learning to program and learning computer science are two very different things. You can learn enough Python to get a job in a few weeks, conversely you could spend years learning everything there is to know about system architecture and operating systems. ",1526758335.0
fmresearchnovak,"Are you planning on taking courses at a specific college of university?  If so, meet with an instructor there and they can help lay out a flowchart / roadmap of what courses to take at that institution.

&nbsp;

If you're trying to take courses ad-hoc via online coursework and a local community college or something then I would suggest this:

* Data Structures (no need to start with Intro courses)
* Computer Organization / Architecture
* Algorithms

After that I think you will find that you can find courses that are specifically on the things you've listed (machine learning, operating systems, etc.) at various places and you should be pretty well prepared to take any of them.

&nbsp;

A couple of things you mentioned usually aren't offered as a traditional course.  For example, competitive programming (just sign up for a local competition! :D), and open source contribution (you'll have to figure it out on your own).  Also, as someone else mentioned, if you become an expert in even 1/2 of these it would be very impressive.  If you learn all or most of them well you will be above average in the CS world.  Good luck!!
&nbsp;",1526997336.0
eternusvia,IA ^ 3 takes as much space as IAAA.,1526769888.0
iwantashinyunicorn,"I had a paper there accepted last year, and then after acceptance they said they wanted it reduced from eight pages to four (effectively three because of the frontmatter and references) so we could keep copyright on it, and that we had about two weeks to do it. Nnnnnnope.",1526834406.0
aelaos,"I only knew him from his algorithm , but turns to be a CS Bob Ross talking about happy programs :) ",1526753276.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1526713689.0
_ACompulsiveLiar_,">Why did you decide to major in computer science?

I fucking love it.

>How has your life changed since you chose such a challenging field?

I started enjoying the challenges I found in school/work.

>Are there certain characteristics you believe are crucial for someone to have to become successful in this field? 

Hard work. Just put in the time and effort. A sprinkle of natural talent helps but nobody became a good programmer by just being smart.

>How is your social life

Tech jobs aren't really demanding on your time. They can be, but you don't have to settle as most aren't. Some people do find it rewarding to pour their soul into work, most probably like the easily achievable balance.

>is your time consumed with coding?

A good amount is, but because it's my passion. Even if it weren't my career I would do it as a hobby. Personal projects keep coding from getting mundane.

>I don't even know if I'll be able to change to a comp sci major this late

You can, but don't change just because you think it's an easy paycheck or a pretty STEM degree. You will not make it far if you aren't going to take it seriously. If you don't love it, you better have a really good work ethic.

If you do love it, and will put in the work, then I firmly believe it's almost always worth it to make the switch over. It's lucrative, secure, and rewarding.",1526706793.0
YeastyWingedGiglet,"> Why did you decide to major in computer science?

My high school AP Calculus teacher made me realize how much I love math. As a young kid, I spent a good amount of time on my computer playing games, making youtube videos, and other things. I had only done basics on code academy but just kinda decided that I wanted to go into Computer Science. I have yet to regret my decision now that I'm in my senior year. It's been awesome! 

> How has your life changed since you chose such a challenging field? 

I have become very methodical with the way I live my life. My biggest change has been my ability to critically think and solve problems. I've gotten immensely better at solving problems and not getting discouraged when a seemingly hard problem is put in front of me. The work is hard, sometimes ""bang your head against the keyboard and chuck monitor"" hard, but extremely rewarding :)
 
> Are there certain characteristics you believe are crucial for someone to have to become successful in this field?

The only thing you need is a solid work ethic. No matter how smart you are, if you don't put in the effort, you won't do well. ",1526709093.0
Koalchemy,">Why did you decide to major in computer science?

I've been programming since I was pretty young, my friend's older brother knew a lot and would always teach me new stuff to try on my own computer. It's also really fun, I think I would spend most my time programming if I could.

>How has your life changed since you chose such a challenging field?

Well for one, I feel like I'm always being challenged by my classes so it never gets boring. There always seems to be a reason to learn a new language or framework. Besides that, I've been able to automate a lot of things that I wouldn't have otherwise which makes life really convenient sometimes.

>Are there certain characteristics you believe are crucial for someone to have to become successful in this field? 

I think that computer science as a field attracts a lot of different people with a lot of different interests. You can be creative, logical, etc. and there will always be a place somewhere for you. I would note though that one thing all people in this field share is persistence. You either have to be a genius, or willing to slam your head against a wall for hours on end if you want to succeed. The problems you run into can be very obscure and time consuming to fix.

>How is your social life and is your time consumed with coding?

Social life is completely fine. You will probably have periods of time where it suffers but it isn't a constant thing. Right in the beginning when you're learning all of the basics/syntax/logic it might be time consuming, and you might have classes here and there that assign massive projects which can take up a lot of time. Besides that I would say it takes up as much time as any other field. It's my personal opinion though that the more personal time you invest into programming, the easier your classwork will be.

The last thing I would mention though is that CS is closer to math than it is to typical engineering courses. It's not a rule but, generally the more comfortable you are with math(algebra, pre-calc, linear algebra) the easier CS coursework becomes.",1526708522.0
khedoros,"> Why did you decide to major in computer science?

I've been deeply interested in computers since I was about 10. I wanted to know the intricacies of how they work, how they're designed, and how I can get them to do what I want.

> How has your life changed since you chose such a challenging field?

During college, I found a lot of the understanding that I was looking for and found out that the rabbit hole is indescribably vast. I think I'll note here: CS is a study of computers and how they can be used for logical manipulation and information flow. You learn some programming to help you do that, like an astronomy student learns the basics of using their instruments. There's an analogy ""computers are to computer science as telescopes are to astronomy"". They're useful, and even essential tools, but they aren't where the science happens.

> Are there certain characteristics you believe are crucial for someone to have to become successful in this field?

Which field? Actual computer science, or something related to software development? For software development, I'd say attention to detail, the ability to hold an understanding of a complicated logical structure in your head, and a willingness to deeply concentrate on an abstract problem for a long time.

For computer science, I might say the same thing, but at the end of your work you have a research paper that may or may not expand the human body of knowledge, and for software development, you have some code that can hopefully be integrated into a product, which may or may not be successful on the market.

> How is your social life and is your time consumed with coding?

I think the question implies an ugly stereotype. I've got about 15 friends from college who I still regularly keep in contact with, a decade after we all graduated. I met up with two of them tonight when my wife, son, and I went to a baseball game. There's something about work life that makes making friends after college 10x harder than it was *during* college.

Having your life consumed with anything is fairly unhealthy. In terms of personal projects (i.e. outside of work), spread the past 10 years I've probably spent a year's worth of evenings doing some kind of recreational programming. I've usually got other things that are more important.",1526714557.0
chromodynamics,"Because it is exciting. Figuring out the correct answer to a problem is exhilarating, which means my day job extremely enjoyable. As well as this I seem to have an almost innate understanding of computing concepts, they just come very naturally to me. It is not a field I would get into if you don’t love it, as it will be a chore to learn things if you aren’t deeply interested in it. It is very hard for some peopk to get into the mindset of computing, and the attrition rate in university courses is astronomical. So I say unless you truly love computing, maths and problem solving, stay away",1526724261.0
tevert,"> Why did you decide to major in computer science? 

I loved legos when I was a kid, then FIRST Lego League combined with a love of video games drew me into software. I was roughly 14 - I consider myself incredibly lucky to have discovered a true passion at such a young age. 

>Are there certain characteristics you believe are crucial for someone to have to become successful in this field?  

Curiosity. If something's broken, *why* is it broken. More importantly, if something works, *why* does it work? How would I approach problem XYZ?  Is there a better way to do this? How does this help my coworkers, my managers, my users? How could I make this better for my coworkers, my managers, my users? Any monkey can sling code. A good engineer is thoughtful and slings the *right* code. 

> How is your social life and is your time consumed with coding?

I would say unimpacted. Most of my friends are in the software field, just as I imagine most zoologists are friends with people in the animal field. We play games, hang out at bars, etc. In college we hung out even more. Work-life balance in the CS field can be legendarily bad, if you sign up with (and stay with) companies who treat you badly. 

Don't bother sticking around with companies who treat you badly, and you'll be fine. If you're being asked to pull more than 40 hours/week, regularly, then leave. ",1526707108.0
StruBoy,Logical thinking and problem solving skills are valuable. ,1526708095.0
nhum,"A decent math background is important in cs. People underestimate this, but the good programmers tend to be good at math.",1526712446.0
bamfdan,"I really feel like your asking the wrong questions. from the course your on to computer science is a big jump. Also coding is probably about 20% of what i do as a computer scientist there's a lot more than just coding. You really need an interest in computer science if you wanna get involved in this, its not just a programming course.

Its hard, its maths, its logic, its the study of algorithms. Find out the 101 comp sci algorithms text for your uni and read that to really get a grasp.

For example this is some basic study of algorithms : https://books.goalkicker.com/AlgorithmsBook/

One of the 8 modules i do a year, along side 2 c programming modules, 2 logic/maths modules, a databases module and a hardware based module on robotics.",1526712679.0
mircatmanner,Everyone’s comments pretty much answer all your questions about CS but the best advice I’d have for you is to get some experience in cs before fully committing a semester for the major. I’d say buy an intro course on udemy and see if you can motivate yourself and stay fairly interested through 50-60% of the course then go for the intro class at your university. Don’t worry if you find the material stale or boring(I wasn’t really interested until I got to data structures),1526722282.0
RippledBarbecue,"**Why did you decide to major in computer science?**

Loved games from an early age and got interested in how they worked (this was helped by drift0r's in depth stuff and woodysgamertag's stuff on minecraft and the old COD stuff) And then took CS as an A-level enjoyed the course and 2 years later I'm finishing my first year of my degree with all modules as a first so far :) 

**How has your life changed since you chose such a challenging field?**

Don't think I'm far enough in to it to answer properly but it does change how you think about things like I tend to apply algorithms to a lot of things in life subconsciously like graph theory whenever I'm walking places,thinking about sorting algorithms when looking for things etc.

**Are there certain characteristics you believe are crucial for someone to have to become successful in this field?**

Strong mathematical grounding (did a-level maths) helped me a LOT in a couple modules but hard work and patientce when you have buggy code goes a long way!

**How is your social life and is your time consumed with coding?**

Social life still the same as before,not all CS majors are recluses,sure a lot of us are but the stereotype isn't 100% I still find time to go out with friends etc but sometimes it can be really rewarding to stay in and finish something off you've been working on or to just play around with something new.",1526723611.0
Insomniac412,">Why did you decide to major in computer science?

I was kinda lost in school, and I didn't know what I wanted to do with my life, etc. I was a part time student in my mid 20s who worked 40+ hours a week. I was attracted to CS due to the demand in the job market, but quickly found out that it was what I should have been doing all along.

> How has it changed you life?

I found something that I was really good at, and something that really interested me, when I had a hard time finding meaning in other things. 

>Are there certain characteristics you believe are crucial for someone to have to become successful in this field? 


There are a few answers here, that talk about work ethic, and that's totally true, but personally I was able to develop a strong work ethic from pure curiosity. There are a lot of ""Eureka"" moments in CS, and those moments are like crack to me. 


>How is your social life and is your time consumed with coding?

Contrary to popular belief, software/cs/tech doesn't mean that you will be spending all of your time with strained eyes, pecking away at a keyboard. You'll spend a lot of your time communicating with others about what you're are working on, and talking about concepts a lot of the time. Sure there is a lot of coding, but even with that you're working with others, sometimes in a configuration called ""pair programming"" where it's like a pilot/navigator situation. On top of that there is a whole CS culture, that has a unique sense of humor, and community  excitement.

I say go for it, it was one of the best decisions I have ever made.",1526735601.0
brettmjohnson,"> Why did you decide to major in computer science?

I started out pre-med, but was not able to enroll in many of the required classes for my first two years. (Registration time-slots were by lottery.) In my third year, the registrar demanded that I declare a major, so I looked at where I had the most credits, so I took a B.S. in Mathematics. But most of my math classes were C.S., which was taught in the Mathematics department. I was good at it, enjoyed it, and it seemed easy to me.
  
  
> How has your life changed since you chose such a challenging field?

I earned money for university by mowing lawns, delivering newspapers, and stacking 100-pound sacks of flour on pallets. After graduating, I eventually landed a job at a minicomputer manufacturer writing operating systems and compilers. I have been a professional software developer for almost 40 years, earning enough to live very comfortably. I will retire this year.
  
  
> Are there certain characteristics you believe are crucial for someone to have to become successful in this field?

I currently work with several excellent software developers, but each of us has very different strengths. One has an excellent memory and keeps up with new developments in the industry more than the rest of us. He also has Design Patterns memorized down pat. One is practically silent, almost an enigma. But she is the most thorough person I have ever worked with. Every design consideration considered. No corner case left untested. Truly beautiful code. Always the smartest person in the room. One guy is the fastest programmer I have ever met, but he usually needs to rewrite the code several times. He can write it three times in the time it takes me to do it once. He can also maintain the whole system picture in his brain to more detail than the rest. One guy is the best explainer/teacher I have ever worked with. I can explain something in 1 or 2 different ways before I run out. Dave can keep switching angles until you eventually get it. My personal strengths are 3D visualization, abstraction, and visualizing concurrency. Problem decomposition is excellent with all of us. One skill that is almost universally required is good communication, especially writing, which is completely under appreciated in school.

> How is your social life and is your time consumed with coding?

If your time is consumed with coding, you burn out quickly (and I did). You learn to temper it. I met a wonderful woman and was married to her for 24 years before she succumbed to cancer. Our son is now married with two beautiful children. I go to concerts, movies, and regular winery events. I eat out (too) frequently, and have enough good friends IRL that I can count on both hands. I also play the guitar, badly.",1526719946.0
hellowonderfulworld,Wow thank you everyone for putting in the time to write all of these amazing responses. It's pretty much what I have expected but reading everyone's thoughts kind of grounded me and made me be more realistic about this topic. ,1526784130.0
Sack_of_Fuzzy_Dice,"To quote my professor,

>This question is left as an exercise to the student.",1526693996.0
HandshakeOfCO,"Don’t waste your time guys.

“Smart people want interesting problems, not cash.”

There, I just saved you a half hour and a headache from this “writer’s” grandiose style.",1526685202.0
babygrenade,"He seems to be using the terms ""software developer"" and ""computer scientist"" interchangeably here.",1526684478.0
a88smith,"At first, the tone of the piece came off as extremely pretentious to me. Maybe that's not fair, but I do appreciate the point of the piece.

 However, I do disagree with the idea that other artistic positions, in other fields, aren't treated as factory workers, banging out pieces of art. My understanding of a lot of concept artists and artists in general is that there are deadlines and promises to keep. When we think of artists like painters for example, we think of someone painting something they they came up with. A beautiful piece that they came up with from their imagination. But, there are many examples of painters and graphic artists who have to crank stuff out over and over. Think of animated television shows, or comic books, card games, etc. They are in artistic roles also being managed by non-artistic people. 

My only point is that it doesn't necessarily seem like a CS problem, but a human communication problem. The solution though, of encouraging artists to get in management roles, seems like an interesting idea that may worked.",1526685230.0
sanxchit,"TL;DR We computer scientists are so much smarter than everyone else, we don't have time for your boring problems.",1526685227.0
Nate75Sanders,"This guy is a kid.  He graduated in 2016.

https://www.linkedin.com/in/brandon-foo-b72a5b157/

I'll cut him some slack because he's young and inexperienced, and he's trying to do big things, but he's spouting drivel and he shouldn't be doing that.",1526687753.0
Hedoin,"The author sounds like a software engineer that read on the internet too much. It's almost as if he was talked up onto his high horse by others after unjustly styling himself a computer scientist, not quite knowing what it means to be one but simply going by what others are saying. Shame anyone can prick through this facade, especially those that know the difference between these two very different things.",1526687912.0
kdnbfkm,Not a developer. Not a scientist. Does not belong.,1526771257.0
PC__LOAD__LETTER,What a circlejerk.,1526686817.0
dempa,Tonedeaf,1526686881.0
Detox1337,"Give a weighted score to: 


1)Percentage of overlap of hashes of the company names in the experience list

2)Overlap of dates on the experience list

3)Order of hashes of company name in experience

4)Hash of first name

5)Hash of last name

6) Hash of certification names

7)Runs of matched sequences of short (1 char) checksums for each other word in the resume

Adjust the weights and threshold until false positives are insignificant. 

You might want to include a point value for the first letter of each of the names",1526679105.0
jaggxi,"how about compare information that cannot be easily duped, like email and phone number. ",1526894322.0
malibu_danube,"I've thought about this problem a lot and I think the first step I'd take would be to break resumes into sections and hash them. Most likely when someone updates their resume they will leave much of it the same. If they change too much it would be impossible to compare anyways. 


After breaking the resume into small enough chunks and hashing them I'd have to determine each section's weight. I'd probably want to do some presorts and mapping for names as well, but the main process would be comparing the sections to others. ",1526658108.0
curiousElf,"Is there a way to transfer ""logic"" across different language platforms? 

Context:
We have an offline first android application that has a lot of form fields and we end up having to deploy again and again because
clients keep asking for minor validation changes to fields.

Is there a way to transfer the field validations as ""configuration""?

We already have dynamic validation for the simpler stuff like numeric range, regex, length.

But what if we want something slightly more complex eg: validate if it is a credit card (with a luhn check). 
Is this even possible? ",1526634307.0
MatheusDe_,I just started pursuing my degree and I just want to get a jump start on learning programming languages which one in y’all opinions is the best to start with?,1526648483.0
IJzerbaard,"[Continued Fraction Arithmetic](https://perl.plover.com/classes/cftalk/INFO/gosper.txt)  by Bill Gosper. Though I've never been in a position to use it, I thought it was a really cool idea, really eye-opening that such a thing was even possible, and I really liked reading about it. There's a weird edge-case with perfect cancellation though.",1526592703.0
KevZero,"[A relational model of data for large shared data banks
(PDF)](https://cs.uwaterloo.ca/~david/cs848s14/codd-relational.pdf) by Edgar Codd",1526591560.0
TheOccasionalTachyon,"[Derivatives of Regular Expressions](https://dl.acm.org/citation.cfm?doid=321239.321249#.WwBs8WSz69Y.link), by Janusz Brzozowski.

It's not especially long - 14 or so pages - but it presents an incredibly elegant way of modeling regexes. Basically, it defines rules for taking the derivative of a regex, and, using those rules, checking for a match becomes equivalent to taking a series of partial derivatives.

It's one of the clearest times I can remember reading a paper and thinking ""Damn, that's beautiful"", and it's also (I think) the easiest way to implement a simple regex engine.",1526754997.0
pemungkah,"""[Drawing graphs with _dot_](https://www.ocf.berkeley.edu/~eek/index.html/tiny_examples/thinktank/src/gv1.7c/doc/dotguide.pdf)"" by Eleftherios Koutsofios and Stephen C. North. Still getting value out of that since the 1990's.",1526622902.0
SOberhoff,"[A personal view of average-case complexity](https://www.researchgate.net/publication/3653458_Personal_view_of_average-case_complexity) by Russel Impagliazzo

The title doesn't do the contents justice. In this paper Impagliazzo explores 5 possible worlds that we may be living in without yet knowing it.

 * **Arithmetica**: P = NP.
 * **Heuristica**: P ≠ NP but hard instances of problems outside P are very rare.
 * **Pessiland**: There are problems that are hard on average but it is also hard to generate *solved* instances of such problems (wrecking cryptography).
 * **Minicrypt**: One-way functions (ways to produce hard, solved instances) exist, but trap-door one-way functions do not. This permits cryptography, but not public-key cryptography.
 * **Cryptomania**: Trap-door functions also exist, giving cryptography the full apparatus. This is the current working assumption.

",1526673663.0
3xecve,What I would recommend is just implementing the data structures from scratch. In my opinion that’s the best way to learn them. ,1526583115.0
rjt_gakusei,"What are you interested in within CS? Maybe try making a game or a simulation, and use data structures to efficiently manage objects and interactions. 

Edit: in particular, having a GUI and making your program slow down so that you can see the algorithm work in real, observable time, can provide a huge motivation.",1526620688.0
mcdowellag,"Most hill-climbing algorithms don't satisfy axiom (1) - they increase until they reach a local optimum and then stop. Versions that carry on do so by accepting a move to a point with a less promising value, in the hope of finding a better value later on. Proofs that you will find a global optimum eventually tend to say that you will repeatedly find local optima in a manner random enough that the chance of wandering into the global optimum is non-zero, so the chances of doing this eventually converges towards one as the amount of time spent increases. Unfortunately with n variables it is generally possible to construct functions with nearly 2^n local optima.-",1526705648.0
espergrafs,"""Let O\(k\) = 1 if S\(k\) is optimum and 0 otherwise. O\(k\) determines a sequence of the form 0n1\*, i.e. a finite number of zeros followed by infinitely many ones \(because of monotonicity\).

The probability that O\(1\),O\(2\),...,O\(k\) has no ""1"" is less than \(1\-eps\)k which goes to 0 as k goes to infinity, so the probability that S\(k\) is an optimum goes to 1 as k goes to infinity.""

This is not true. Your monotonicity assumption says that S\(k\+1\) is greater than or equal to \[at least as good as\] S\(k\). Then it could be that S\(k\+n\) = ... =.S\(k\+1\) = S\(k\). In this case, the probability that S\(k\) is optimum would remain the same for all iterations greater than k.",1527033856.0
Pakul1729,Link to grab all of them in 1 shot: http://books.goalkicker.com/all.zip,1526582095.0
yaylindizzle,someone printed and bound all of stackoverflow??,1526578057.0
Pakul1729,"""Please feel free to share this PDF with anyone for free,
latest version of this book can be downloaded from:
https://goalkicker.com/AlgorithmsBook


This Algorithms Notes for Professionals book is compiled from Stack Overflow
Documentation, the content is written by the beautiful people at Stack Overflow.
Text content is released under Creative Commons BY-SA, see credits at the end
of this book whom contributed to the various chapters. Images may be copyright
of their respective owners unless otherwise specified
This is an unofficial free book created for educational purposes and is not
affiliated with official Algorithms group(s) or company(s) nor Stack Overflow. All
trademarks and registered trademarks are the property of their respective
company owners
The information presented in this book is not guaranteed to be correct nor
accurate, use at your own risk
Please send feedback and corrections to web@petercv.com.""


This is the only information provided by them at the beginning of PDF. I hope it clarify your question.",1526615190.0
ryati,looks neat. Are they good quality?,1526590535.0
VessoVit,"Do you take requests? One for iPython / jupyter notebook, numpy, pandas, as alternative for data science with R would be really appreciated ",1526595040.0
Vrog1,"That's really really cool. Thank you!
",1526578757.0
crekadoodle,This is awesome! Thanks \<3,1526582156.0
sethosayher,Surprisingly high quality from what I've perused!,1526584307.0
Vetii,I've never heard of these! Is it from some editing company? ,1526585883.0
Elaphoil,You're the best and I love you.,1526608239.0
redevil707,Love you bro ✌️✌️✌️,1526626439.0
MainMarvin,"Thank you, truly. ",1526587131.0
campnou911,Thank you!,1526589346.0
zevzev,Awesome!,1526972507.0
bartturner,Need one on Dart.,1526817555.0
fenster25,With online mode so that you can play with your friends from your terminal,1526569074.0
Howtoeatpineapples,Nice.,1526882017.0
_ntnn,"I'd be interested in following this to see where it is going.

However:  
1. The about page is a 404  
2. There is no RSS feed",1526554073.0
davidnagli,Great concept!! Love it,1526559409.0
yawkat,You might want to try /r/cscareerquestions or maybe a bio/chem subreddit (I've seen comp chem people on /r/chemistry before),1526504979.0
dkfgo,"I used to work on a computational chemistry lab at my uni, I was their IT guy if you will. I dont have any advice for you, but many of the researchers there were women. I worked closely with one in particular and as far as I could see she didnt look like she had any issues on being taken seriously by her peers (quite the contrary), and she wasnt like ""tough"" or anything like that, she was just a regular nice person overall.

Your mileage may vary, of course, but I think you should just go on and do whatever you like the most. Since you're poking around here I assume you already like the ""computational"" in ""computational chemistry"". I didnt go too far into their research, I mostly set up the environment, installed, configured and made small modifications to software they used, the simulations were really cool though.",1526509605.0
Prof-,"The careers are there. I decided against a master/PhD after I finished my molecular biology degree to pursue a second degree in computer science. If you’re interested go for it! Money is definitely better, one of the driving factors, also lab work can be a bit depressing at times. ",1527137464.0
sparcxs,"I know little about that particular sub-field, but it’s one of the focuses of quantum computing. Fascinating stuff, to me at least. I’d love to focus on quantum computing right now. Only advice I’d give is do what you love most, careers last too long to do something you don’t enjoy!",1526527860.0
unobservant_bot,"I am not a woman, please feel free to take my advice with that qualifier. I have worked in the industry and currently work in academia in the mid south and in my experience your fears are ungrounded. I can say that about 50% of my colleagues, and in both fields my direct supervisor (currently vice chancellor of research) and her supervisor are/have been women. I know that these are anecdotal experiences, but maybe that will allay your fear.

As to your second question, I am a bioinformaticist, specializing in both computer science and statistics, unfortunately while the pay for people with our skills is higher starting out, it has not been my experience that in academia the pay is not that much higher, especially when comparing to the pay difference found in industry.

I would be happy to answer other questions you might have.",1526576441.0
HungryMolecule,"If I stay in computational chemistry, I will be also doing quantum chemistry, because it is good established field in my lab and it is necessary to do in phd. 
But dont get people who are doing something computational (science,industry,IT) sometimes bored, because they “whole life” is in that computer? Dont they say sometimes to themselfs something like “I would like to do it experimental, I want to check this results in some experiments”? ",1526537844.0
tweettranscriberbot,"^The linked tweet was tweeted by [@BaraSec](https://twitter.com/BaraSec) on May 16, 2018 14:24:11 UTC (6 Retweets | 2 Favorites)

-------------------------------------------------

Note: would like to have an InfoSec career in an abroad country someday.



I have the following choices now:



1. Intern as sys/net admin/cloud computing, and not get paid for toooo long.



2. Intern in programming and start getting paid after 2-4 months max.



Retweet please!

-------------------------------------------------

^^• Beep boop I'm a bot • Find out more about me at /r/tweettranscriberbot/ •",1526487314.0
lizaard64,"According to the rules of entropy, and the way the human mind remembers things - The best way to create passwords are arguably to use sentences - Which is an argument against the stupidity of _""Add one special character, one capital letter, one number, and at least one of $%&#!""_ ...",1526460257.0
anjelswhat,"The title is misleading, the study was inconclusive. ",1526471085.0
ultramegaman479,Use keepass people. ,1526499306.0
wllmsaccnt,"For work stuff, I generate a random GUID and then store it in my keepass file.",1526472127.0
PM_ME_YOUR_MASS,Can confirm: I have really good passwords,1526464934.0
spinwizard69,"I look at passwords and the way IT departments manage them as a bit of a joke.    The industry needs a better way to validate users, biometrics works for local devices, I'm not sure what the answer is for remote services.",1526468894.0
YeastyWingedGiglet,Isn't a strong password one that is really long? More combinations = more time needed to bruteforce the password? So adding special characters like ?!#$ and having a smaller password is less secure than one that's like 32 characters but is an english sentence. ,1526476615.0
DutchmanDavid,"# HAVE I TOLD YOU ABOUT OUR LORD AND SAVIOUR, THE PASSWORD MANAGER?!

# IF YOU LIKE TO KEEP CONTROL, TRY [KEEPASS](https://keepass.info/)

# IF YOU LIKE EASE OF USE, TRY [LASTPASS](https://www.lastpass.com/) WHICH CAN EASILY BE INTEGRATED INTO YOUR BROWSER AND PHONE!

^(nuff said)

# EDIT: HERE'S AN EXAMPLE PASSWORD FROM KEEPASS: ¡\s³ÏüGãÄ~ç5Òí{£0Cn¹ï÷Q-.Âüoi2å>
# YOU THINK YOU CAN CRACK AND/OR GUESS THAT SHIT? GOOD FUCKING LUCK!
",1526501935.0
jonathancast,"Because ""good passwords"" are stupid.

Quick edit: use a password manager.  You should only have one password memorized, which you should never transmit across the Internet, in any form.",1526511261.0
cozyblanket111,[obligatory xkcd](https://www.xkcd.com/936/),1526513294.0
butsbutts,its fun memorizing my most used 20 char random passwords,1526514572.0
PathToTheLight,The best password is the password you forget and have to reset Everytime you wanna log in ,1526519733.0
hotwindy970420,My password....oh,1526462785.0
iLrkRddrt,"I am not surprised by this at all. Its not so much making a strong password is hard, its REMEMBERING and KNOWING what password goes to what Application.

Honestly, if Operating Systems included more cloud keychain options like Apple's Ecosystem, It would be a much easier problem to solve.",1526498880.0
BadMillenial,"I actually don't know any of my passwords. I have a system based on a few reminders, and a re\-usable string code based on physical movement. Ask me to tell you or write down any password? I can't. And no password is stored anywhere. It's just a few physical gestures and a reminder.",1526519998.0
feralwhippet,"Since when does getting better grades correlate with being smarter? Possibly, it correlates with being more conformist, less questioning, but otherwise I think the basic premise of the study is flawed.",1526506582.0
agumonkey,"Takes a lot of gymnastics to use a good password system on your own. I use a blend of keyboard choreography, trivial reindex and hash, all linked or not for mines. Not easy",1526476769.0
rl_dr,/r/NoShitSherlock,1526488637.0
thenlow,Guess im retarded,1526467964.0
Joald,I think that this is not the right sub for webdev. ,1526463428.0
dmariano24,Nope,1526437405.0
PM_ME_UR_OBSIDIAN,Just keep your tombstones around for arbitrarily long ¯\\\_(ツ)_/¯,1526434942.0
laoreet,"PSPACE is the set of problems with ""yes"" or ""no"" answer, so that there exists an algorithm and a polynomial *p* so that for all integers *n*, the algorithm solves all instances of length *n* bits requiring at most *p(n)* bits of storage.",1526396052.0
werothegreat,"It's with regards to this paper: https://www.scottaaronson.com/papers/ctc.pdf

From what I understand, they're arguing that time travel would allow a programmer to tell a computer to solve a problem, then go back in time to when the program was first started and give the programmer the answer, effectively giving an instantaneous answer.",1526400019.0
YaZko,"When trying to classify problems from a computational standpoint, we may consider two main metrics: how much time it will require, or how much space it will require.

PSPACE classifies problems that can be solved using relatively little space: less than a polynomial bound of the size of the input. 

Note that it says little on the time necessary: you could reuse several time each atom of space during the computation. In particular, as far as we know, it does not exclude problems intrinsically requiring an exponential amount of time to be solved.

Being more precise requires ""jargon"", since we need to define the notion of computation we consider, in order to define what we mean by solving a problem, time, and space.",1526399029.0
laoreet,"> exactly

> non-jargon

It could help if you told what parts of explanations you haven't understood.",1526395909.0
ssc,"I think that the video *[P vs. NP and the Computational Complexity Zoo](https://www.youtube.com/watch?v=YX40hbAHx3s)* does a great job explaining P vs. NP.  He mentions PSPACE [at 8:42 in the video](https://youtu.be/YX40hbAHx3s?t=8m42s), where he describes it as:

> the class of problems that can be solved given unlimited time, but using only a polynomial amount of space for memory.

I didn't yet follow the link to the paper, but it's probably apparent from this definition how it fits with what you said about it (""*time travel would allow a programmer to tell a computer to solve a problem, then go back in time to when the program was first started and give the programmer the answer, effectively giving an instantaneous answer.*"").",1526433509.0
KatsuCurryCutlet,"Okay I can't ELI5 this completely because we do need to go into small math terms but I'll do my best to rely as little on the math. Some details are going to be skipped over because imo the important thing is to get the notion of space complexity across (easier than time actually haha)

Let's say you were asked to compute something, so maybe I give you some numbers and I want you to help me determine whether they have a certain property, at the end of the day I want a yes/no answer. (to be fair there are a lot of other kinds of problems but without the background let's not waste time getting to know them) 

To aid you in your process I hand you some gridded paper and a pencil and eraser. So you can use it to help in your calculations. Let's say that I enforce the rule that whatever working you conceive of that is pertinent to the calculation must be written down, and also you write always within the squares in the grids, so one symbol per cell only. You can erase the cells and reuse them, but we will be measuring the maximum used at any point in time. So example if throughout the steps you used, 1, 2, 3, 4, 5, 6, 5, 5, 5, 4, 3, 2, 1 (at the first step only one cell has a symbol, and the 7th step only 5 cells have a symbol and so on) we consider you having used 6 cells. Though the exact number really doesn't matter I just need to get the point across that we aren't going to sum up the number of cells used in the calculation and that we are allowed to erase to rewrite them.

PSPACE, is a set of problems where the amount of space you take to arrive at the solution is some polynomial with respect to the problem size. That is to say, if the problem size was n, you might need say.. for example.. n^3 or n^100 cells to get it done. From the other portions of the comments I expect you to understand that this is already a polynomial. One thing to bear in mind is that we don't really care about the exact number of cells being used, but must rather just how many cells you need in relation to the problem size, I.e. how your resource of space scales with the problem size. I'll explain the motivation in the next paragraph.


There is another class called EXPSPACE, where it's similar but problems take exponential space rather than polynomial space. That is to say, something like 2^n, which to give you some idea, grows insanely quickly, as in as the problem size grows the number of cells you're going to need will increase at an insane rate. (If you want better insight, try going to desmos.com and plot a polynomial like x^9 or some large number versus 2^x, and you'll realize that the exponential function eventually overtakes the polynomial) in fact there is a proof that demonstrates, in plain English that eventually all polynomials can be ""overtaken"" by some exponential function.

So the notion behind PSPACE is that it gives you firstly a measure of how the space you need scales in relation to the problem size, and secondly there is some intuitive sense of ""difficulty"" in having to compute the task at hand, e.g. comparison to EXPSPACE, problems that require much more space relative to polynomial space as the problem size grows larger 

Hope this helps! Feel free to pm if I seem to have overlooked a detail to make this explanation easier for you",1526427525.0
bradfordmaster,"One thing I'd add that's often non-intuitive is to think about what the definition of a problem is here. Specifically, for it to make sense to talk about the _size_ of a problem being _n_, you need to define how the problem scales. Similarly, to define a bound on runtime, you need to define what ""one step"" is, or in this case, ""one memory slot"" for a space bound. This means that a problem like ""choosing the optimal move in chess"" doesn't really make sense here unless you have some way to ""scale up"" chess, otherwise it's simply constant time. Something like factoring numbers works because it gets harder as the number of digits grows.",1526455753.0
,[deleted],1526397169.0
dualmindblade,PSPACE is set the of all problems that are no harder than solving infinite chess.,1526401276.0
patronus17,"Imagine you wanted to solve all the problems of the world. You come to me and I give you a blackbox which has all the *potential* solutions to your problems. However, there are some problems that you and I don't know how to solve ourselves correctly, and can only make approximations about them. But, we do know how to validate a given solution and see if that solves our problem or not.

Now, if our solution checker can validate the *potential* solution given by the black box with a polynomial runtime, then that particular problem is said to be in PSPACE.",1526422372.0
AvPrime,"> I'm writing a paper in a non-comp-sci area (General Relativity) and the topic I'm writing about involves some computer science, and it's written in a lot of jargon, and it keeps talking about how a certain setup has a memory storage of ""PSPACE"", and I've tried looking up the term online, including Wikipedia, and none of it has been particularly helpful. So, what, exactly, is PSPACE? In non-jargon.

So, you're writing a paper about GR, but you're referencing PSPACE but you don't know what it is?

Sorry I'm not much help, but I don't understand your story. Are you trying to do research for a paper you're writing? Or are you telling us you have no idea what your paper is talking about?",1526400979.0
ranieuwe,"Not everything we do in that space is AI/ML focused. A lot of it is CS and software engineering to expose specific AI/ML functionality. Microsoft is always hiring so if you want join that movement, look at the career page or send an open application. 

Disclaimer: am Microsoft employee",1526334350.0
sailorcire,"Look at BCI and then maybe use a third party AI library or API like Google's or Alexa's.

Just a suggestion.",1526333985.0
MaximRouiller,"Since your background is in Electrical Engineering, have you seen Project Brainwave/Project Catapult?

Those got me super interested in what my employer is doing!",1526343045.0
prof1le,"It definitely felt like I knew nothing when I first started

A good note I got on my first internship: my manager told me ""we are all equally smart, we just have been expose to the subject matter longer"".

What you should really get out of school is really good foundational knowledge of CS and be able to learn quickly. I don't think anywhere really expects you to know their technology stack, unless they hired you specifically for that reason, which would not be the case coming out of school.

My advice: be patient and don't be afraid to ask questions. no one expects you to be perfect on day 1. Try google first, but if you still are stuck after 30 min. go to someone you think will know the answer, no one wants you wasting your time if they could get you moving again in 2-3 min. 

If you can, try to figure out the root of a problem, is there anything you can say ""If I knew X, then I could solve the problem"",  to help keep questions as concentrated as possible.

**TL;DR** ask lots of questions, and you will find the subject matter fits into what you've learned over time",1526320690.0
RunnyPlease,"You’ll learn much more actually coding professionally. The key things to keep in mind are to 

1. Communicate status honestly and frequently. 
2. Don’t just learn how to do something by copy and pasting from stack overflow. Lean best practices and hold yourself to writing quality code. 
3. As a new hire you won’t be judged on what you don’t know. You’ll be judged on how you write code and work within a project team. It’s okay to admit you will have to do research as a first step when you grab a task. If your project manager needs something done quickly they’ll give it to someone else. 
4. It’s perfectly fine to ask to do pair programming to get to know a new system. 
5. Time box a problem. If you are genuinely stuck for an hour start asking for help. 
6. Congratulations. You are on the cutting edge of human technological development. It’s okay to get frustrated when you are literally doing a task that didn’t even exist through 99% of human history. 

",1526321619.0
hjorthjort,"Hell yes. Part of the job is learning to learn. Ask questions. This can not be overstated. Here's my algorithm when I need to wade into a new codebase, or do a change somewhere I don't know shit about, or have to use a new technology:

* Try to figure it out for a bit. Start somewhere that seems like it might be important, open a bunch of tabs with all stuff I need to learn more about. Enjoy and explore. Write down questions.
* Let your coworkers know what you are doing. Bring it up when you are chatting. I can't count the number of times some off-hand remark I made led a co-worker to point me immediately in the right direction. Ask them if they know anything about the thing you are dealing with.
* Start trying to solve whatever problem you are working on. Hit walls, lots of them. Suffer a bit. Write down more questions.
* Grab a co-worker who seems free, or a supervisor, ask if they have a few minutes, and they can help you wrap your head around a few things.
* Be prepared to feel stupid. Part of the job, part of learning.
* If things don't make sense, be candid about it, say you don't understand.
* Summarize to show to you and the other person what your understanding is. They will hopefully correct you if you misunderstood.
* Keep trying, and keep asking.
* If you've been stuck a long time, see if you can get a hold of someone in the company who knows more about this stuff than you. Ask to grab a coffee with them.
* ... you get the idea. Ask someone, try things, ask someone else, and keep iterating.

The best part is: if you constantly feel like you don't know squat, it might be because you keep hitting challenges, not because you aren't improving. A lot of people in CS are miserable because they are under-stimulated, which is worse.

If you *really* want to gauge if you don't know more today than 2 years ago, find a new junior hire and hang out with them a bit, see if you can help them out in any way. Then you can get a good reminder of how much you knew coming in to the job.",1526325155.0
locotxwork," Go in with knowing you don't know everything and open your mind and eyes to learning about what you are programming and how it is important to the business.  Everyone forgets that programming isn't just coding, it's team work, working with other departments, learning new technologies, learning how what you create effects the business, etc...   This is what a CS Career is about.  If you want to just do coding, you'll find scripts and outsourcing is going to win that battle, but if you want a CS career, you are going to have to master other skills other than just the technical ones.",1526323736.0
chromaticgliss,"I've been at this for almost 4 years after having been a hobbyist for 8 before that, and I still feel like I know almost nothing.",1526323958.0
generic12345689,You will always be lost. Tech is constantly changing. Once you build up experience from various mistakes you’ll feel more comfortable not knowing but having confidence you’ll figure it out.,1526330495.0
zirus1701,Welcome to the job market! Fake it 'till you make it!,1526326773.0
hjorthjort,What is a coop job placement?,1526324634.0
khedoros,"> When you first started did you feel like you knew almost nothing.

Yes. Computer science teaches you...computer science. It's not necessarily the best for teaching you software development in a corporate environment, and the jump from theoretical to practical can be a big one.

> Because I just started my first coop job placement and I just finished my second year. But I am so lost and feel like I would be in the same spot if I didn't even go to school.

Ah. My first job was when I graduated, so I didn't feel quite *that* lost. School's very different from work though, and a lot of CS is building up a baseline of theoretical concepts.",1526329012.0
Spoogly,"Anyone reasonable is going to be hiring you specifically because you're a blank slate and they can carve their needs into you. Otherwise, they wouldn't hire young developers at all. A willingness to learn and losing that part of your ego that tells you not to ask questions is all you really need.",1526332413.0
TheAethereal,"12 years in. Still feel like I know nothing. CS advances so fast and is such a massive field. So not only are there a million things you don't know, but the things you do know are increasingly becoming antiquated.",1526332579.0
Rocketmn333,"Yuuup, approaching two years now and can finally say I'm starting to become competent, but still a lot to learn ",1526333115.0
Radiospank,"encouraging responses, I needed to hear this too.",1526341238.0
supershinythings,"Wait until you've been around for about 20 years.  When you lift your head up from whatever you've been working on and look around, you'll realize that you now know less than nothing, because whatever you studied has been superseded or automated out of existence.  What you're doing is being outsourced, and kids in school are currently studying things that didn't exist at the time you attended.  So you spend an inordinate amount of time studying, reading and trying to keep up, only for each new thing to change after a year or two.

After awhile, you start spending more time watching the stock market as your next steps rely on how much savings you have.  Eventually you'll either get laid off one last time and say fuck it, or you'll transition into a lower paying area that doesn't require a whole new set of skills every time you take a dump.",1526328452.0
lrem,"For computer science I would suggest starting with elementary logic followed with discrete mathematics, only then computational theory and algorithm design.

Cyber security is so far up that alley that if you care mostly about making stuff, you probably don't want to start with computer science. Following this avenue, the language to learn for making stuff is Python. The language to learn to understand software failure modes, which constitute the base of security, you have to learn C (don't confuse with C++, which brings huge improvements in productivity, but is no more relevant to your security knowledge than C and takes way more time to learn).",1526311603.0
mvl_tigs,Sounds like a machine learning problem.. would be helpful if you included what VRPPD stands for. ,1526315170.0
PM_ME_UR_GUNZ,"Not to take away from your blog but the original article is an incredible piece of writing: http://insecure.org/stf/smashstack.html

For my security class when we got to buffer overflows we were basically pointed at that article and told the lab was already up.",1526362925.0
Lsaction,"Article is written interesting and I've discovered two new commands in gdb for me, hence thanks for this.",1526293096.0
CyAScott,"I’ve never tried exploiting a buffer overflow bug, but I’m glad it does work just like I imagined. I guess the next blog will be over writing the return address to a return address of another call in the stack.",1526336885.0
neelaryan,"Hey everyone, I am one of the authors in this blogsite. u/SubZero0x9  is the author of this particular blogpost. He'll reply to your comments and queries. Thanks. :)",1526376321.0
ooa3603,"Have you checked  community colleges in the area?

Also, many Deans/Program Chairs are willing to substitute courses with ""near equivalent"" substitutes if you get an agreement with them before taking the class.",1526257966.0
MikeDawg,"Don't schools have some sort of rule to graduate like you must take X hours of coursework from here? (From memory, I seem to remember a past school having 20 hours required).",1526263135.0
,"Athabasca probably has it. 

Edit: http://www.athabascau.ca/syllabi/comp/comp308.php
",1526380176.0
sailorcire,"Yes, I suppose you could.",1526256189.0
evilgwyn,Yeah when you see them online press alt f4 quickly. If you do it right then they get kicked off. Obviously they will be trying to do it to you so if you're too slow you'll get kicked off instead,1526256098.0
iwantashinyunicorn,"Yeah, you're lying, and you'll probably get caught. The honest thing to say is ""Appeared at the workshop on stuff we found under the sofa (peer-reviewed, printed proceedings) at the Big Fancy A* Conference 2018"".",1526241145.0
ObscureAnalysis,"Best to say ""appears in workshop X and conference Y"". Workshops are always a lower bar than the conference (even if some workshops are harder to get into than some lower tier conferences). You'd be lying through omission, which is icky. ",1526241444.0
sadmafioso,"No. It was reviewed for publishing at the workshop, not at X. These things are extremely easy to find out.... and even if they weren't you are clearly trying to misrepresent your work.",1526281146.0
rosulek,"If the workshop has peer-reviewed published proceedings, then I don't see the problem. Not all venues with the word ""workshop"" in the title are workshops in the sense mentioned by the other commenters here ([WPES](https://cs.pitt.edu/wpes2017/) in my field comes to mind). Follow the lead of other people in your discipline that you respect.",1526261584.0
JaxSolace,Research needs PhDs. Period. Doesn’t matter the sub area. That the whole point of a PhD.  ,1526241409.0
josgraha,"Topics that would help to have PhD

\- natural language processing

\- malware / vulnerability analysis

\- programming language research

\- high performance and even quantum computing

\- machine learning

My assertion is that pretty much any topic that you can study to get a PhD will help in your professional \(non\-academic\) career.  That includes teaching, researching, and applications of things that require deep knowledge.  Perhaps even one of the best skills you will learn is how to communicate with academics which is invaluable given the vast proliferation of research papers in all areas of computer science research.  All of the big tech companies have research divisions that hire PhDs and those skills are often required in non\-tech industries as well.  Once there was an amazing math professor, Dr. Tom Leighton, at MIT who went on to found Akamai which is one of the leading Content Delivery Networks perhaps they even invented the industry itself.  People are far too dismissive of deep knowledge yet when it is required, there is no substitute.",1526241923.0
nevabyte,If you want a job you don’t need a PhD or masters. ,1526283952.0
ITwitchToo,"Sadly, I think a CS PhD will almost only help you if your topic is in machine learning.

The problem is that most CS *research* happens at universities, and there is a really limited supply of university positions.

In other subtopics you can find a lot of competent people without PhD degrees so the degree doesn't really help you get ahead of them. In fact, having a PhD can be ""harmful"" in certain cases, either because you've specialised too narrowly to the exclusion of a lot of other things that would be more interesting for the employer, or because you lost time pursuing your own research and writing papers when what the employer wants is more ""practical"" experience with existing/established technologies.

My impression is that machine learning is a new hot topic and it can be hard to break into for people without a mathematical/statistical background. Just look at how much Google and Facebook are doing (and using!) for example. Anything from image recognition, face recognition, analysing trends, extrapolating from data, recommendations, etc.",1526240070.0
PolySoup,This subreddit seems to be allergic to capitalizing sentences,1526272923.0
hobsonlane,"Find all of the solutions to the 10 average score problem, and then  sort those solutions by some metric for the similarity in age. If the ages are normally (or uniformly) distributed, then the groups in all your solutions will have similar ages already.  So the sort just makes the selected solution have a little more similar ages.

To see why, generate 10 random numbers within the age range of the class, calculate the average and then do that again a few times and calculate the standard deviation of the averages, it should be something like age_range_std / sqrt( 10 - 1 ) or age_range_std / 3 . So 3 times more similar than a random pair of ages. ",1526204180.0
FUZxxl,"Since the quiz scores are likely discrete and within a small range (say, integers between 0 and 100), a dynamic program seems to be the best general approach.",1526206684.0
tulip_bro,Did you mean to post this on /r/programmerhumor ?,1526181917.0
chaoticgeek,"All the best coders test directly in production, as they finish uploading changes over FTP. ",1526182236.0
Lusankya,Everybody has a testing environment. Some people are lucky enough enough to have a totally separate environment to run production in.,1526187091.0
J2Me,Got to be one of the worst websites I have ever had to use.,1526184115.0
nondescriptshadow,"AB testing is a form of testing in production. More like 

> Test in production only if necessary but this example doesn't cut it",1526183266.0
nerdshark,Did you look in the sidebar? This is not really an appropriate question for /r/compsci. The sidebar has links to better places to have this discussion.,1526170893.0
combuchan,I'm really appreciating udemy. ,1526170522.0
gnullify,"I agree that it's unintuitive. I can't speak on why the row/column convention is the way it is, but I can say that you get used to it pretty quickly if you take a linear algebra class or do anything with matrices.",1526170739.0
,It's based on [matrices](https://en.wikipedia.org/wiki/Matrix_\(mathematics\)).,1526169362.0
Adnotamentum,"I don't even understand what this question is asking. If you have a 2D grid, how else do you reference a point on that grid unless you use an x value and a y value?",1526170832.0
Roachmeister,"Usually x is left-right, so columns, while y is up-down, so it's rows.

If I understand your last question, you're asking when to name the parameters x/y vs row/column. I guess it just depends on the problem domain. Generally I would use x/y in a problem space involving graphics, or any time floating-point numbers are involved. I would use row/column for something involving tables of data.

Also, it doesn't make sense to talk about ""row -2"", so any time negative numbers are involved, x/y probably makes more sense. ",1526173944.0
violenttango,Why do you use 4-5 question marks? Nobody knows.,1526170252.0
khedoros,"In mathematical notation, we almost always use ""x and y"", and often in that particular order. It's probably due to the letters in the alphabet.

For why ""rows by columns"": it's how matrices are usually discussed. So, at least some languages follow that when choosing how array indices work for multi-dimensional arrays. I used to consider it harder than I do now; do it enough, and you get used to it. Or you write your own graphics toolkit, and store images ""sideways"" in memory.",1526195895.0
TarMil,"I think you can blame mathematicians for that. Using (abscissa, ordinate) for Cartesian systems and (row, column) for matrices are both quite old conventions.",1526207361.0
nevabyte,"Why not just use a function you define yourself...

get(x,y) return arr[x][y];

or

get(x,y) return arr[y][x];

That way you can have it however. ",1526212906.0
igglyplop,Uh oh... You'd better never have to use ncurses or it'll confuse the living hell out of you...,1526169465.0
Bat_002,"For you and anyone else interested in this topic -- here are a couple of good sources of information.

[Spectre](https://spectreattack.com/spectre.pdf)

[Meltdown](https://arxiv.org/pdf/1801.01207.pdf)

[Google Project Zero's Blog](https://googleprojectzero.blogspot.com/2018/01/reading-privileged-memory-with-side.html)

Spectre and Meltdown are two similar types of attacks that manipulate the way we access data on our machines, in order to access privileged information. Meltdown has been patched on most architectures whereas Spectre has not.

Meltdown is related to a fundamental principle of computing called out-of-order execution. It manipulates the fact that there is(was?) a lack of security in the way in which we switch between two processes running out of order. In other words, the principle of memory and address space isolation is being manipulated through out-of-order execution to read arbitrary kernel-memory locations. 

Transient instructions are the first building block of Meltdown because they are what create a vulnerable side channel that can be used to leak privileged memory. Meltdown utilizes transient instructions to create a sending part of a covert channel. A transient instruction is an instruction that is executed out-of-order. Meltdown manipulates a transient instruction by assembly code that accesses an address like the start of an array, and uses it as a buffer to leak a stream of data. The code is assumed to never be executed because the statement before the buffer is designed to raise an exception that is handled. The vulnerability lies in the exception handling and the return type of an assembly register when switching between two processes.

I don't know as much about Spectre, and I'm really just summarizing the paper on Meltdown. Hope this helps.",1526146867.0
marvsup,Reading this title I thought this was about supervillains.,1526156247.0
zergling_Lester,"Meltdown exploits an actual bug in intel CPU hardware memory protection. When you speculatively execute an instruction that reads memory that you don't have read access to, you can't raise a hardware exception immediately, because what if it turns out that the branch where you do that was not taken after all? So apparently Intel engineers decided to use the speculative execution machinery again, and simply mark all further instructions as doubly-speculatively executed (which is in itself absolutely normal, the same as you'd treat multiple conditional jumps in a row), expecting it to never matter because if the push comes to shove the exception gets raised and everything gets discarded. And, crucially, they didn't even bother to make the faulty instruction to store zero instead of the actual value it was not supposed to be able to access. Afterwards it was the usual Spectre side channel attack.

And Spectre (type 1) turns out to be a fact of life of all modern high performance CPUs. They were not, cannot and will not be designed to allow a program to protect its own memory from itself. That's what hardware memory protection between different processes is for (well, as long as it works). So it turns out that speculative execution by its nature can virtually bypass range checks and break logical invariants of the program and allows leaking this ""logically impossible to obtain"" data through memory cache timing side-channel. 

Note 1: There's also Spectre type 2 which allows reading kernel memory by training jump predictor (and then having the kernel speculatively execute ""logically impossible"" code, etc etc), which is also pretty much an Intel bug, which should be solved by clearing jump predictor's memory when switching to the kernel mode (I guess it can't be very inefficient?).

Note 2: Meltdown actually only half-broke hardware memory protection. The virtual memory machinery remains intact because you can't access what you can't even address. What was exploited was the fact that all big OSes mapped the entire kernel memory and entire physical memory into the virtual memory of every process, but disabled all unprivileged access. Consequently the fix was to stop doing that on all vulnerable processors, and take the performance hit.",1526235189.0
PM_ME_UR_GUNZ,">CALL will, by definition, push the return address onto the stack before *jumping to the target address*.

https://stackoverflow.com/questions/33685146/x86-does-call-instruction-always-push-the-address-pointed-by-eip-to-stack

",1526089707.0
gsg_,"If you read the paper a little more carefully, you can see that the author is taking a somewhat ill-advised shortcut:

> We can transform this into basic blocks, except that we take a small shortcut with the conditional branch by not following it with an explicit goto to save on space.

Once you add those, it's fine:

    pow(b, e):
        r <- 1
        goto loop(b, e)

    loop(b, e, r):
        if (e <= 0) goto done(r)
        goto next()

    next():
        r <- r * b
        e <- e - 1
        goto loop(b, e, r)

    done(r):
        return r
",1526100474.0
arashskater,\+,1526398027.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1526072750.0
humanityloses,Finish breakfast. It's the most important meal of the day.,1526041214.0
,"Dismiss it because I'm obviously not smart enough.

Then a couple of days later some loud smarty pants will shout out exactly this answer in the classroom. And I'll be cursing myself for being dumb coward as always.

Just kidding. Or not.",1526045230.0
pgboz,"Wouldn't it make more sense to donate to the [Turing Trust](https://turingtrust.co.uk)? It's a charity set up by Alan Turing's family (run by his great nephew, James Turing) to help bring computers and computer education to poor communities in Africa.",1526038149.0
m--w,Next Monday... the 23rd of June... am I missing something? I'm confused by your post,1526020466.0
profedtech,How do we know this isn’t some robot programmed to scam us out of our money? I wish there was a test of some kind to be able to tell the difference!,1526040075.0
Erwin_the_Cat,"Flowers don't do Turing any good, donate it to charity.",1526047950.0
k-word,Would probably help if you told people how to donate,1526044939.0
karl_das_llama,Do you have a link?,1526043761.0
upstreamsalmon,Just have the google home call a local florist.  ,1526047081.0
i__Reddit,He is a dead man for christ's sake.,1526059462.0
lavahot,It's Dr. Strange's birthday? Well why didn't you say so? I'll send off my best bouquet to Dr. Watson.,1526097100.0
The_Schwy,Would the Allies have won WWII without him? Are there any good documentaries on the question? Thank you!,1526172134.0
,[deleted],1526021365.0
SOberhoff,"What are some applications of martingales in computer science? I've been learning about martingales and I always try to relate anything I learn to something I already know, which is theoretical computer science. ",1526009623.0
reedpuzzled,"I’m confused about the “this” and “->” and       “this->”. From the videos I’ve seen people treat this-> just as a pointer, please explain and maybe something will click.",1526085667.0
Nerdlinger,"> What Happens When Artificial Intelligence and Blockchain Tech Collide?

18 new booths go up in the RSA Conference vendor display hall. ",1525987352.0
unnamedn00b,"I think someone said its a ""Mega buzzword"" and perhaps if you also throw in quantum computing into the mix then maybe a ""Super Mega Buzzword"". (...because ""buzzphrase"" just sounds weird)",1526190793.0
thelastofthelostboys,Nothing exciting.,1526045638.0
bewalsh,"moon lambos, probably",1525988117.0
lcc0612,"Pretty cool project :)

I'm no statistician so I won't directly answer to your points. Instead, I just have a pretty generic thing to point out. 

Just wanted to mention that views, likes and dislikes may not be the best indicator of a video's quality. Views are usually the result of advertising / SEO etc, and already-established channels are always going to have a lot of views regardless of content.

Likes and dislikes as well could be misleading, as these ratings may not necessarily reflect the overall quality of the video (eg. A good tutorial in which the host pronounces ""GIF"" wrong garners a whole lot more negativity than you'd expect - I say this from first hand experience xD)

So while these metrics give you an idea of the _popularity_ of the video, you may want to use some other metrics to assess its quality. ",1525997751.0
east_lisp_junk,"When it comes time to evaluate a possible ranking method, what's your ""ground truth"" for whether a video is good?",1526001358.0
nealeyoung,"In my experience most TCS profs in the U.S. have PhDs in Computer Science.  I'd suggest you take at least one PHD-level course in Algorithms, and one in Complexity Theory / Undecidability.  As for math, it depends on the area of TCS.  For algorithms: discrete math / combinatorics, probability.  Yes, it is possible to pick up the background without taking courses.  You might start by working though Sipser's book on complexity/computability theory, and one of the standard texts on algorithms.  ",1525972918.0
foreheadteeth,"About 50 years ago, all the computer science departments were started by mathematicians, or so I was told. As far as I know today CS profs are CS PhDs. I'm a math prof.

There are mathematicians who do computer-style mathematics. I'm a numerical analyst, I do a lot of functional analysis and scientific computing. More broadly around scientific computing I would include linear algebra, machine learning and optimization.

In abstract algebra, there's [computational group theory](https://www.ams.org/notices/199706/seress.pdf) but I don't know anything about it (although I think I know someone who does that for his research). There are similar things happening probably in number theory, but I don't know any of that either. I also heard of algebraic geometry/topology for [big data](https://www.sciencedirect.com/science/article/pii/S0167739X16301856) but I have no idea what that is.

If you are young and invulnerable and want to tackle P vs NP, there is geometric complexity theory. [More info here.](https://www.scottaaronson.com/papers/pnp.pdf)",1525988803.0
Peter-Campora,"I don't think that most theoretical computer scientists have a PhD in math, but there are certainly some that do. Also, when you ask about theoretical computer science, are you specifically referring to research involving  algorithms, complexity theory, or Automata (which some people callTheory A), or are you also including things like program semantics, verification, and type theory (Theory B)? I can't elaborate too much about the math used in Theory A, but I can for Theory B.",1525973109.0
oliverjohansson,"I’m biologist and I know many computer scientists working in clinical research without PhD and background in maths, rather in statistics and more toward big data. Many gained this experience out of school.",1525989645.0
jmite,"> Questions: Do most theoretical computer science professors have their Ph.D. in mathematics?

No. Those that do will have done research in highly related areas, such as Graph Theory, Combinatorics, Probability/Statistics, Category Theory, Number Theory/Cryptography, Proof Theory/Mathematical Foundations, Homotopy (Type) Theory, etc.

There are plenty of CS professors who have a background (i.e. undergrad) in Math, but usually to teach CS you transition before/when you start your PhD.

> I would like to learn about what Theoretical Computer Science is for academia and what background someone needs to become a theoretical computer scientist. 

TCS is a broad area, with many sub-areas:

* Algorithm development/Complexity theory: know discrete math and combinatorics, data structures, graph theory, formal language theory, automata theory, algorithm analysis, staticstics.
* Machine Learning theory: know data-structures, local search techniques, probability, statistics, linear algebra, vector calculus.
* Programming Languages theory: know induction *really* well, the Curry-howard isomorphism, proof theory, Lattice theory, functional programming basics, set theory, Lambda Calculus, formal logic. Bonus points for category theory, domain theory, homotopy theory, and abstract algebra.
* Homotopy type theory: know dependent types, Curry-Howard, category theory, homotopy theory.
* Formal methods: Graph theory, boolean logic, algorithm analysis, SAT/SMT solving, automata theory. 

Note that each of these areas has knowledge specific to it, that you'd be deficient in coming from a Math PhD. I.e. if you do a math PhD but want to research Programming Languages, you won't likely have studied System F, polymorphism, subtyping, program analysis, the Lambda Cube, etc. Similarly, going into complexity theory, Math won't have likely taught you much about automata or Turing Machines etc.
",1526057282.0
Count___Duckula,"For the love of god, start by learning to OOP coherently. Everything else while come naturally and your peers won't cherish the rare moments when you bit your tongue.

Coming from maths you'll have no issue getting into the literature, the best you can do is come up with something interesting then try to justify it with the literature and complain a bit about what existing research says.",1526000692.0
SailingKing,"Might not be relevant to you but I still would like to share since I only found out about it myself recently. https://www.acm.org/publications/digital-library 
It’s the biggest digital library in this field and might be free over specific university networks. For individuals it’s 200$ a year though",1525975870.0
Hjulle,I found the instructions for this exercise from a course at my university very useful. There are both tips on how to read research papers and on how to choose what to read. http://www.cse.chalmers.se/edu/year/2017/course/DAT315/Literature%20Survey.htm,1526154177.0
Lynx7,"Are you in University? Typically in upper level classes you'll get a good idea of some research areas at least at the topic level - doing actual research is very different. 

To get an idea of what people study, look up professors and their labs webpages and read through the paper titles/abstracts that they have up. 

If you're in University the best way to start some research is to approach a professor and talk to them. They might let you do a project with them. 

If you're not in University then I'd suggest thinking about what interests you and start bombarding google scholar with keywords. The domain of ""Computer Science"" is huge - arguably one of the largest domains as computers and the value they bring to us touch every other domain. You'll find computer science researchers working in physics labs, or with doctors, or with anthropologists, librarians, psychologists, linguistics.... etc. So really if you're interested in learning about the broader field of computer science then the best place to start is the wikipedia page. ",1525971090.0
,[deleted],1526000297.0
gnupluswindows,"NP-Complete and NP Hard are classes of *problems*, not classes of algorithms. It's not meaningful to say an algorithm is in one of those classes. ",1525960966.0
thbb,"You will want to read about [Kolmogorov complexity](https://en.wikipedia.org/wiki/Kolmogorov_complexity). 

The answer to the question you're trying to ask is: it's much harder than NP-complete, it's actually undecidable.",1525967430.0
combinatorylogic,"As hard as it gets - the ultimate compression algorithm would be a bruteforce search for an algorithm that generates your source data (i.e., finding its Kolmogorov complexity). ",1526037982.0
nealeyoung,"You might be interested in [Kolmogorov complexity](https://en.wikipedia.org/wiki/Kolmogorov_complexity).  Computing it is not just NP hard, [it is undecidable](https://en.wikipedia.org/wiki/Kolmogorov_complexity#Uncomputability_of_Kolmogorov_complexity).",1525967319.0
yurichev,Is there a need for such algorithms?,1525957068.0
barsoap,"Yes: Brute-forcing arithmetic decoders where the expressive power of the language they're implemented in is polynomial. Worst-case no string of length shorter than the input data results in the output data and you get NP. 

Noone is using that kind of thing, though, paq encoders are slow enough as it is... and if brute-forcing is fast enough your data doesn't need to be compressed in the first place. And if you want to squeeze ten million digits of pi into under 1kb you could just write the paq code to generate them by hand, no need to wait an eon for the computer to find that program.",1525960665.0
TomSwirly,Not a computer science question - try /r/AskProgramming ,1525955687.0
guiraldelli,"LLVM, at University of Illinois at Urbana-Champaign.",1525938692.0
TarMil,VLC from École Centrale.,1525988008.0
pinano,Kerberos - MIT,1525950341.0
nigwil,"Ingres (relational database management system) - University of California, Berkeley, prototype in 1974, still around today.
https://en.wikipedia.org/wiki/Ingres_(database)
",1525947645.0
kamasutra971,BOINC - Distributed Computing at UC Berkeley,1525935510.0
xapon,"Andrew FS and Coda - distributed file systems at CMU.

Never heard of them before studying here, but many professors consider it a really big deal.",1525945691.0
inexactbacktrace,Ceph was actually born from [Sage Weil's PhD thesis](https://ceph.com/wp-content/uploads/2016/08/weil-thesis.pdf),1525960477.0
catkage,Project Athena - MIT,1525978242.0
anor4k,The Lua programming language was created in Pontifical Catholic University of Rio de Janeiro in Brazil.,1526013083.0
abudnik,Apache Mesos - UC Berkeley,1525952394.0
chx_,HTCondor - University of Wisconsin–Madison,1525936334.0
Adaddr,CMUCL,1525967169.0
joergen7,Cuneiform at Humboldt university,1525973377.0
jmite,Scala from EPFL.,1526056472.0
roboduck,Google and Facebook,1525978076.0
xav0989,LDAP from the University of Michigan.,1526007396.0
NinjaDoggie46,STRAPS ,1526222971.0
Irkutsk2745,"Linux, GNU, Samba...",1525958827.0
ricodued,Weather Underground has its roots in the University of Michigan,1525996413.0
supermario182,"Warp pipe, Napster, facebook",1525940341.0
,"> #Drawbacks

> Judy arrays are extremely complicated. The smallest implementations are thousands of lines of code.

I don't know why, but I find this really funny.",1525957961.0
everything-narrative,"I studied them at one point. They are quite clever Trie-like lookup structures, at least conceptually.",1525934623.0
jnwatson,"Here's a decent performance study: http://www.nothings.org/computer/judy/

> If your data is strictly sequential; you should use a regular array. If your data is often sequential, or approximately sequential (e.g. an arithmetic sequence stepping by 64), Judy might be the best data structure to use. If you need to keep space to a minimum--you have a huge number of associative arrays, or you're only storing very small values, Judy is probably a good idea. If you need an sorted iterator, go with Judy. Otherwise, a hash table may be just as effective, possibly faster, and much simpler.
",1525953100.0
elcric_krej,"Wiki says they are similar in concept to radox trees, which are quite a well known and easy to understand & write DS (used for stuff like the Vector in Clojure and, I believe, Scala).

If you want to get into ""real world"" data structures, there are thousands of slight variations on fundamental DS and algos by insane people to achieve insane speed.

But I'd hardly call a granny map or a moody camel queue a ""generic"" data structure.",1525936984.0
Terr_,"Seems like somebody's algorithm that has aspects which are tuned based on the hardware architecture it's expected to run on, which is where the claimed performance benefits come from.

",1525934786.0
zem,"the felix language uses them: https://github.com/felix-lang/felix/tree/master/src/judy

from what i gather, they're extremely fiddly to work with.",1525937962.0
,[deleted],1525959282.0
calligraphic-io,"I've wanted to study the algorithm. I'm almost certain I came across Judy trees as an optional indexing option in PostgreSQL and was curious how it worked, but I can't find anything about it now. Github is using them to [improve garbage collection performance](https://blog.github.com/2013-05-01-hey-judy-don-t-make-it-bad/) in their main Ruby web application. And there's a [Python library](http://www.dalkescientific.com/Python/PyJudy.html) implementing Judy trees that's built on Doug Baskin's [C library implementing Judy arrrays](http://judy.sourceforge.net/). ",1525946876.0
naasking,The [Felix programming language](https://github.com/felix-lang/felix) uses them for its tracing GC.,1525960440.0
jnazario,"i've used them in the past, about ten years ago, mapping IPv4 addresses (as 32 bit ints) to data (a JSON string). super fast and efficient. ",1525976433.0
ggchappell,"Yes, lots of us have heard of them. But Judy Arrays are a little unfortunate. They are a highly specific implementation of a key-value store, with lots of complicated implementation details specified. Contrast this with things like hash tables, prefix trees, red-black trees, etc., which are relatively simple ideas that can be explained easily, and then implemented in any number of ways.

Add to that the fact that documentation on Judy Arrays is generally awful. When I looked into them, I quickly decided that wading through an ocean of interesting but poorly specified ideas was not worth my time.

As I said, this is unfortunate. There are probably some good ideas in there, but good luck figuring out what they are.",1525986231.0
green_meklar,I read [this page](http://judy.sourceforge.net/downloads/10minutes.htm) and it left me with more questions than answers. I'm not sure my mind is large enough to understand this stuff.,1525997914.0
diaoyoudao,"We're not gonna talk about Judy, in fact we're not gonna talk about Judy at all!",1525991210.0
miekle,"https://www.sciencedirect.com/science/article/pii/S0168927407000335

Any relation to this? Written by a Judith.",1525969785.0
Workaphobia,"Says it's a trie, so unless there's some detail about how it different from a trie I'm not sure what there is to discuss.",1525939620.0
theshoe92,never,1525933603.0
maraca-obama,Used her? I never met her!,1525946676.0
johanvts,This should be called the natural science way of baking. The scientific method has little to do with programming.,1525927680.0
bremby,"Look at definition 7.1 \(assuming you have the Introduction to Theory of Computation\). It defines time complexity as a function of the length of the input.

In your example, we cannot determine the complexity just from your 4 samples. We need to know, whether the computable function / the Turing machine takes a deterministic number of steps based on the length of the input. If you say that it always takes some number of steps that's always less than some `k`, then it the function is computable in O\(1\), i.e. a constant time. If there is a finite set of exceptions, like your case with 42, the complexity stays constant, as we can simply calculate all of those exceptions and since it is deterministic we can take the longest running time as that `k`. In case of a non\-deterministic TM, there exists an equivalent deterministic one as non\-deterministic TMs have the same expressive power as deterministic ones.",1525802682.0
hextree,"In your case, input size is the number of digits in the input. And, you can not determine complexity from observations, it would mean checking an infinite number of inputs. 

The process you follow is to construct a mathematical proof which proves the bound is attained. You could start by asking yourself, for a given input XXXXXX of size n, what values do I need to pick for the digits to make the output as bad as possible? This might help you come up with an upper bound, then you have to complete the argument by proving that bound is never exceeded for any other choices of the digits.",1525810566.0
BadMillenial,"This is a variation of the old ""liars/truthtellers world"" riddle, where people are always in pairs, one is a liar and the other is a truthteller, but you can't tell which... what single question do you ask to determine which door is safe, and which door leads to sudden death? 

Personally, I think the tiger is safer than a woman, but essentially, the riddle answer is to ask either person, ""Which door do you think the other person will say is safe?"" And then do the opposite, because no matter what, the answer will be the door of death. If you ask the truth teller, he will tell the truth and say the other person will lie and give the wrong answer. If you ask the liar, he will lie and say the truth teller will give you the wrong answer.

Hopefully this helps you conceptualize the problem enough where you can turn it into logical statements.

Edit: Also, making a truth table with all possibilities should help you solve it.",1525797773.0
PPewt,"This might be excessively pedantic, but saying that dictionary lookups are O(1) straddles the line between being questionable and being flat-out wrong/misleading depending on how your hash (edit: and table) is implemented and whether the programmer even knows whether their dictionary is a hash or a tree on the back end.",1525797198.0
IJzerbaard,"That 3-variable equation thing can also easily be done in quadratic time, so IMO that's not a great example. It's a valid example though, the presented algorithm is definitely cubic time.",1525787909.0
kc3w,Saying that findin duplicate elements in an array is *n^2* is questionable. Using a hashtable it is more in the realm of *n*,1525802982.0
Nimish,"Multiplication is not constant time in the number of digits of the input...if it were, that'd be a huge breakthrough. ",1525858198.0
pritnf,"    if(!max && max < n[i])

Why `!max`?",1525787668.0
hextree,Wouldn't the odd-or-even example be O(log n)? Because you may be computing all the digits of n. You could do n%2 in O(1) time if you make sure only to use the last digit of the input. ,1525788725.0
can_i_have,No 6 will blow your mind,1525811433.0
buckboostltd,Thanks for sharing :),1525819829.0
thinkscience,this is great but is there a software that can calculate the time complexity of a block of code ? ,1527281180.0
pinano,"No, MAC addresses are link-local. However, it is very easy to detect a NAT. One technique is noticing that the TTL is, on average, one lower than similar users’. https://forensicswiki.org/wiki/NAT_detection",1525778147.0
magnificentbop,"The MAC address will appear in the layer 2 frame that encloses the IP packet.  Have you verified that the router is actually using the MAC address you cloned to it?

Another thing to consider: there are 4 addresses in the frame header, which your router uses to direct traffic it receives for endpoints in your network.  If they peek at the frame and see that the receiver and destination addresses don't match, that likely means that a router is in the way.  [See here for some resources on frames.](https://networkengineering.stackexchange.com/questions/25100/four-layer-2-addresses-in-802-11-frame-header)",1525781393.0
magnificentbop,"Another option: connect to eduroam using a router that supports WISP, and then use a second router for your devices.",1525781703.0
barsoap,"Hmm. What I'd do is tear down the router, plug in the computer directly, then assign half a gazillion MAC addresses to your NIC. Just make them VMs, I'm sure you'll be able to think of a use-case.

Now that you're compliant to the letter, tell them that their auth method is inherently broken. And ask them whether they'd want you to use a virtual router instead of a virtual switch.

See the policy go up in flames... or at least that *should* happen if your campus IT even remotely knows what it's doing. 

If they want you to authenticate yourself, PPPoE would be a good idea as it's very standard and, well, works over plain Ethernet.",1525785970.0
SOberhoff,"Context *sensitive* languages? That's quite the ask. If I may ask, what are you up to?",1525758577.0
GNULinuxProgrammer,"Are you sure you're asking context sensitive? I'm not exactly sure but I don't think parsing context sensitive languages, in general, is a tractable problem. Usual way to deal with that is using ""lexer trick"" e.g. to parse Python (which is context sensitive thanks to indentation being context sensitive) you can parse it normally and send token INDENT whenever you see a new indentation (which will be decided by some other algorithm). There are other ways to deal with it, such as doing ""multiple passes"" (i.e. parsing bunch of times) or stuffing a type system inside your grammar etc.

There is bison for C/C++ but it's LALR. ANTLR is LL.",1525759766.0
Samrockswin,"Umm... is it cheating if I say you can use any programming language? For example, Haskell has the parsec package and Python has the parser package.",1525757684.0
combinatorylogic,Take a look at any PEG-based approach.,1525769402.0
LittleAliceFellDown,"I would start by looking here:

https://en.wikipedia.org/wiki/Comparison_of_parser_generators

and checking by language/if it provides a lexer or not.
There is also this resource, I don't know how helpful, but it is a good read:

https://wiki.php.net/rfc/context_sensitive_lexer

Worst case scenario, you might end up writing one on your own, which is not as terrifying as it sounds - maybe using a lexer generator?(https://en.wikipedia.org/wiki/Lex_%28software%29)
Anyways, good luck!",1525764064.0
dragonnyxx,"In general, I would say that it's best to avoid parser generators and roll your own parser using recursive descent.

While building a programming language, I started with ANTLR. After all, parsing sounds like an insanely complex problem and there are a ton of tools to build parsers for you - so I assumed that ""Gee, I'll just write my own parser!"" was madness on the order of ""Gee, I'll just skip using a compiler and turn this high-level language into machine code manually!"".

It turns out it's not. Recursive descent is an *incredibly* easy algorithm to implement by hand, to the point that you can write a parser for a language like C in perhaps a day, assuming you already have a formal grammar for it. It's also much easier to handle unusual cases in - 99% of your language is straightforward, but there's this one edge case which is tricky... this sort of thing happens in most languages, and recursive descent makes it very easy to deal with, while parser generators do not.

Building a parser by hand also gives you an incredibly good understanding of how a parser and grammar work at an intuitive level. The entire time I was using ANTLR I didn't really ""get"" it. I continually struggled to make sense of its errors, to understand why one construct worked while another didn't, to understand why extra lookahead was needed in this spot, etc. Once I started hand-rolling parsers, I *got* it and became much better at designing grammars.

Now, obviously there are some tradeoffs as well, but it brings enough advantages to the table that I think it's almost always the right choice. There's a reason why every major compiler I am aware of uses a hand-written recursive descent parser instead of a tool-generated one.",1525779718.0
smthamazing,"You may want to experiment with [peg.js](https://pegjs.org/online), though I'm not sure how well it handles context-sensitive langs.",1525777555.0
ThePillsburyPlougher,"How context sensitive are we talking here?

For C++, PEGTL can deal with a certain kinds of context sensitivity, but if you have something which is context sensitive from the lexing phase, then you're going to have to write your own parser.",1525785378.0
VermillionAzure,"**The context-sensitive languages are equivalent in power to Linear Bounded Automata (LBA) which is essentially Turing-machines with bounded input scaling with the length of the input linearly.** The set of languages accepted by LBA is equivalent to handling NLINSPACE, or non-deterministic Turing machines with linear space. Apparently it's not formally known whether PSPACE-complete problems lie outside of P or NP (but it probably is), at least according to Wikipedia.

In my experience, most programming languages' semantic correctness is context-sensitive, despite claiming that they have context-free grammars.

I'll give you my experience with Scheme. I wrote Scheme in C++ using a parsing technique modeled after Parsing Expression Grammars using C++11 lambda expressions. PEG is a context-free language parsing framework. Scheme is a context-sensitive grammar until you realize that `(quasiquote)` and all of the ""primitive forms"" rely on the enclosed scopes not re-defining keywords to be temporarily something else, as well as the values of the current operating environment. At some point, the power of the language environment comes into play, and additional computation is needed to clarify the semantic meaning of the current parsed expression. Evaluation then continues after this additional computation. Some of this ""extra-grammar"" evaluation can even be Turing-complete, in the case of C++ due to its template generics systems also doubling as a metaprogramming system. In this case, I augmented my parser with calls to the enclosed environment for verifying that certain keywords were indeed bound to the default primitive forms, and then proceeded to different methods of evaluation based on the context. Thus, the computation I used here is part of the context-sensitive part.

C's compilation and acceptance of programs relies not just on its context-sensitive grammar, but also its type system checks as well as its preprocessor. Because C compilers often use a symbol table, I've assumed that its also context-sensitive.

Python and Lua? Interpreted languages also hold a symbol table and check it for evaluation. So I think they're also context-sensitive.

If we consider context-sensitive languages' power lies in the fact that the evaluation or reduction of a current expression depends on its pre-context and post-context, we are essentially saying that we potentially need to consider the entire string being parsed to determine whether it lies in a language. So, any parser that is generated for the context-sensitive languages should be capable of using information from effectively the entire string to make a accept-reject decision.

By asking for a parser *generator*, you are asking for a program-generating program according to a fixed context-sensitive grammar as input to this program-generation program. If we consider this as a Futamura projection, any program that can generate this program should be generic over the context-sensitive language recognition programs so that it can be re-specialized to accept ***any*** context-sensitive language. This means any true parser generator program should be able to produce LBA-equivalent automata and also solve NLINSPACE decision problems. Based on my experience with SAT and P vs. NP, you may end up encountering PSPACE and maybe EXPSPACE due to most SAT algorithms being unable to break back into polynomial time (although I'm unsure because NLINSPACE), so the non-determinism might blow up your running time on a deterministic machine which most of us write real programs for.

Certainly, ANTLR is insufficient to handle this unless you make heavy use of ""additional computation"" outside of its LL parsing framework.

To my knowledge, you are asking for an arbitrary program generator to generate restricted memory program that scale linearly with input -- effectively a LBA generator. Such a program generator would be probably equivalent to creating limited Turing-complete programs out of a program specification, which sounds like a Holy Grail-type of situation. Unless you can find a sufficient metaprogramming framework that can flexibly generate LBA-equivalent programs, I an inclined to say that there currently exists no true context-sensitive parser generator.

- Restricting the language class may do you good. The minimalist grammar (MG) and tree-adjoining grammars (TAG) seem promising. MG has a O(n^6 ) parsing bound, and is sufficient to capture parts of natural language. Perhaps if this could be adapted for the computer science domain, this would be promising.

- Additional thoughts -- there may be a connection between how one might encode the specification for generating what are essentially bounded-memory Turing machines in LBA and algorithmic information theory as introduced by Gregory Chaitin. Unfortunately, program-size complexity is undecidable for the class of Turing machines, but it should be decidable for LBA because the halting problem is also decidable for LBA. However, it may be infeasible to calculate such a thing.

 

",1525900490.0
jmite,Monadic parser combinators are where it's at!,1526058592.0
crosberrys,"I've been slowing working on my degree for a long time now, and held down full and part time jobs the entire time.
 
* Practice time management. Make sure you spend a little time each day so you don't fall behind.

* Stagger difficult courses. This last semester I took Operating Systems and Algorithms and I could barely keep up.  My grades suffered for it.
* Hopefully you can find something flexible. Working less hours around midterms and finals will help a lot.

It isn't too bad. I do feel like I'm at a disadvantage compared to the students who rely on student loans. Your social life might take a dip too. ",1525750730.0
bigbadmangos,"Depends on the job. I go to a top 5 CS school and work part time as a software dev at my unis research park (where companies hire undergrads) and work anywhere between 6 - 12 hrs/week. My job is super chill and flexible, mainly cause my boss so it's not hard at all to juggle. But if my job was actually stressful and I had to consistently put in 15+ hrs/week I could see it being difficult.  ",1525750073.0
AaronKClark,"I went part time for eight years, while working full time. I have kids so I was taking one class per semester. Finally finished in 2016.",1525754095.0
lrem,"If you already are pursuing the degree, estimate how much time do you have free. Assume you can put half of that into paid work. Will you earn worthwhile money? I did, many don't.",1525765326.0
Caracharias,"I'm doing undergrad CS full-time and work at a computer lab helpdesk at my University 16 hours a week. I can study for the majority of the time I'm at work, which is great. However, some weeks it feels like all I did was wake up go to class go to work go to the gym sleep and repeat. It's definitely doable though.

I highly recommend working for your University, the scheduling is fantastic and it's generally pretty relaxed.",1525781133.0
therealhrj,I am working part-time (dev intern) and taking the minimum credits to be a full time grad student.   It is incredibly taxing at times and I get rediculously burnt out every once in a while.  That being said if you love it you love it and I do! My only advice would be to ask yourself if back to back 70 hour weeks would be too much for you.  If no do it!  You'll be proud of the work you'll put out! :),1525749985.0
mendicantbias343,"A proper computer science degree at a university with reasonable reputation? The reason for a decent reputation is because it affects the pace and depth of the degree.

It would be difficult during crunch periods. You could be in a situation where you're tasked to develop a piece of software over 2 or 3 months with a group which will be evaluated on how effectively you work with your group. It'd be fine if all you had to rely on was your own discipline and getting into a steady flow of work, but more often than not group members won't pull their weight, there will be a massive rush in the final week or two to finish off the code, and write the report and you'll have days where you simply don't sleep.

If you were to work, you'd want something flexible that can adjust to these periods, otherwise your grades will suffer and I'd say it's not worth it to walk away with a third class degree.",1525750205.0
javaHoosier,"My cs program is somewhat intense and I work part-time as an assistant instructor about 20 hours a week. Semester before this I was a bartender at a restaurant with the same amount of hours. It’s super time consuming, stressful and tiring but it’s doable. I always say that with a degree like this you can break your life up like this. You have hobbies, gym, relationship, social life, great grades, work. Pick 3. Maybe you can juggle more but you’re more likely to drop them the more you have in the air.",1525756188.0
Hoffman9134,"I would say it depends on the intensity of the program at your university. At my University, it’s reasonably intense, but not something like a top CS school. I am going to be a senior next year and have worked part time through 5/6 semesters of my degree. The only reason I took off one of these semesters (the most recent) was because I was dealing with some personal issues and wanted to make sure I still got the grades that I wanted. Also, it was the hardest courses of my degree. However, without those personal issues, I would have still worked while attending school and did just fine. But I will say it is a lot more difficult doing both and you will have less free time than you would probably like. However, I would say from my experience it is definitely doable as long as you stay on top of your work and stay dedicated. ",1525749778.0
pulsar512b,"It irks me that he promoted his company, but I am also new to the industry, but I feel that it will be cheap over quality.",1525738544.0
CyAScott,"I tend to believe that outsourced software development will go the same route as outsourced customer service. At first it will seem super cheap, but the language barrier will end up costing more than if you sourced locally. My only experience with this issue is when we work with a client’s outsourced software department it’s like trying to get blood from a stone when exchanging emails and phone calls with someone that barely knows English. The easily doubles the development time when ideas and concepts cannot be clearly communicated.",1525740440.0
hmaddocks,Software out sourcing has been going on for decades. Most large consultancies do it. It has negligible impact on smaller companies. ,1525741371.0
jpmrst,"Just pulled Grune and Jacobs off the shelf to make sure I was remembering these correctly:

 * They use ""directional"" for LL/LR, as opposed to ""non-directional"" for Unger/CYK/tabular parsing.
 * But LL and LR do not refer to specific algorithms at all --- LL is any parser which consumes input from left-to-right, and produces a leftmost derivation; LR is a parser which consumes input from left-to-right, and produces a rightmost derivation

Dick Grune and Ceriel J.H. Jacobs, ""Parsing Techniques: A Practical Guide,"" 2nd ed., Springer, 2008.  One of the authors distributes the PDF of the first edition IIRC.

(Edit for formatting)",1525749333.0
Lucretia9,"Noam Chomsky defined a hierarchy for the classifications of CFG’s, might help.",1525744264.0
washeduplol,Computer Science is a very broad subject. Within Comp Sci what are you looking for?,1525726273.0
jkff,"But why? A SAT solver is obviously enormous overkill for just doing BFS, and I'm missing what other motivation this could have.",1525706712.0
Hook3d,Does this mean that knowledge bases/expert systems are represented as graphs in real applications? I always assumed the KB would be stored as a hash table.,1525705470.0
x2mike2x,If you're interested in this check out OpenJML.  It uses SMT solvers in an attempt to prove the correctness of Java code.,1525732543.0
YaZko,"I do not know how it compares with Isabelle/HOL facilities, but real analysis is definitively doable in Coq. 

See notably [the Coquelicot library](http://coquelicot.saclay.inria.fr/), and Sylvie Boldo's work in general.",1525698578.0
foreheadteeth,Isabelle/HOL has a number of modules that prove [various things in analysis](https://isabelle.in.tum.de/dist/library/HOL/HOL-Analysis/index.html).,1525692540.0
carette,HOL-light has a couple of hundred proofs in real analysis already in its standard library.,1525693963.0
teteban79,"No tool can do this currently (and won't be able for as long as I can think of). All theorem provers I can think of right now can only deal with *arithmetic* theories, and even then you'd be hard pressed to find one that can deal with non-linear ones efficiently on a larger domain.

EDIT in my head I read SAT-solver rather than theorem prover... I do not have that much experience with provers, but [this](https://www.reddit.com/r/compsci/comments/8hmhpj/which_interactive_theorem_prover_for_real/dykwivf/?st=jgw6a9aq&sh=3ba8e0f9) comment has more info",1525692273.0
kicsikrumpli,"Well, fuck your popup!",1525632772.0
Catakryst,"For those that like this kind of language:

https://esolangs.org/wiki/Hello_world_program_in_esoteric_languages",1525630765.0
spacegirlmcmillan,At least it's white-space delimited.,1525628659.0
mcandre,Mobile ad cancer,1525633789.0
Lendari,I cracked up when the hello world example scrolled like 5 screens on my phone.,1525651601.0
yaylindizzle,This link is so terrible. Full screen pop up ad and the links are also ads!,1525644856.0
,"Surely this is for something like /r/programming, not /r/compsci.",1525628630.0
shoshy566,"Interviewer: What programming languages do you know?
Me: Java, Python, C, VHDL, Verilog, Pikachu language
Interviewer: ... I think we're done here. ",1525639985.0
psytrancedsquid,"That just does my head in. 
Next thing will be PokemonOS 
",1525681563.0
Putnam3145,Is this not just a [trivial brainfuck substitution?](https://esolangs.org/wiki/TrivialBrainfuckSubstitution),1525632438.0
ModernRonin,"Pfft. All the *cool kids* have been programming in [Doge#](https://github.com/returnString/DogeSharp) for years now.

Get on my level, Team Rocket!",1525639095.0
thosakwe,Glad to have won an Amazon gift card! Less excited about the mobile pop up ,1525655959.0
Sergiy-Horef,The best programming language I’ve ever seen. All my entire life I was looking for this😂,1525638606.0
Stenwalden,I feel like vomiting,1525637022.0
didgeridoome24,My culture is not your god damn programming language,1525706397.0
,[deleted],1525629171.0
PunsForHire,"Consider that Kruskal’s algorithm and Prim’s algorithm will both produce the unique MST if all edge weights are unique.  In the case that edge weights are not unique, more than one MST may exist. However, adding arbitrarily small weights to the edges such that each has a unique weight (for example, by giving each edge a numerical label and adding epsilon times the label of the edge weight to the edge) would force Kruskal and Prim to produce the unique MST for this.  This adding of arbitrarily small weights is equivalent to ordering the edges explored in some desired way.  Therefore, there is an ordering of edge searches such that both algorithms produce any specific MST of a graph.",1525567740.0
incompetentrobot,Do you want to find an MST or enumerate all MSTs?,1525565685.0
Catalyst93,"Yes, if all edge weights are unique, then there is a unique MST.  If some of the edge weights are not-unique, then we reduce to the previous case.  Let T be the edges of the MST you want to find, now take x > 0.  Construct a new graph G' by adding x to all edges not in T.  Now it is the case that T is the unique minimum spanning tree of G', and so Kruskal must find it.

One can think of adding small perturbations to the edge weights as setting up tie breaking rules.",1525568412.0
nealeyoung,"Yes.  Proving it is a standard exercise in learning greedy algorithms.  Here's a proof for Prim's algorithm.  

Let T be any MST of any connected edge-weighted graph G=(V, E).  Consider running Prim's algorithm on G.  Consider any iteration of the algorithm.  Let S be the current component (the connected component built so far by the algorithm).  Let M(S) contain those edges leaving S that have minimum weight (among edges leaving S).

__Claim__: _There exists at least one edge that is in T and in M(S)._

_Proof._  Let (u,w) be any edge in M(S).  In the case that (u, w) is in T, we are done, so assume otherwise.  Let P be the path in T from u to w.  Since u is in S but w is not (or vice versa) this path P has to contain an edge (x, y) where x is in S but y is not.  Let T' = {(u, w)} U T \ {(x, y)} be the spanning tree obtained by adding (u, w) to T and removing (x, y).  Then


weight(T') = weight(T) + weight(u, w) - weight(x, y).  (1)


Since T is a minimum-weight spanning tree, weight(T') >= weight(T), which with (1) implies that weight(x, y) <= weight(u, w).  Since (x, y) also leaves S, and (u, w) is a minimum-weight edge among edges leaving S, it follows that (x, y) is also in M(S).  Since (x, y) is in T, this proves the claim.  

Now, by the claim, in each iteration of the algorithm, among the edges leaving the current component S, there is one of minimum weight that is in T.  So, in each iteration, Prim's algorithm can choose an edge in T.  By doing so in all iterations, it will output T.  QED
",1525612951.0
sturdyplum,"Yeah any of the mst could end up being created by running kruskals, there is no mst that would not be able to be made with kruskals.",1525577191.0
Hook3d,"Kruskal's and Prim's algorithms are greedy algorithms, which means they will look for the locally optimal solution at every step without concern to the global optimum. (I believe) The ~~completeness and~~ optimality of these algorithms is a consequence of the definition of minimum spanning tree, but that's not the case with most greedy algorithms. (As someone else mentioned, uniqueness is guaranteed *iff* the edge weights are unique.)

By introducing the constraint that we want to generate all possible min span trees with non-unique edges, the problem is now an enumeration problem. Simple enumeration and greedy are opposite approaches.",1525567741.0
jeekiii,"MST means Minimum Spanning Tree, so I'm not too sure what you mean, but assuming you wanted to say ""are all graph's MST obtainable by kruskal and prim"" the answer is yes as long as they are connected (obviously).

More specifically:

**Prim and Kruskal's algorithms computes the MST of any connected edge-weighted graph**

You actually don't need these two algorithm to guarantee that, a greedy search could provide it, these algorithm are just much more efficient.

The underlying principle goes like this:
If you have a graph, and you do a cut anywhere in it, the crossing edge of minimum weight belongs to the MST.

For example if you have 1 connected graph containing 5 nodes ABCDE, you cut it anywhere, say you cut it between A and the rest, then the edge with the lowest weight that connects A to the rest of the graph belongs to the MST.

Proof: given a graph ""G"", a parition ""A"" and a second parition ""B"". Suppose the crossing edge between A and B with the minimum cost ""e"" isn't in the graph: the MST ""T"" will not contain ""e"", but will contain another edge ""e2"" that connects A and B. Now add ""e"" to the graph. ""e"" and ""e2"" both connect ""A"" and ""B"", yet ""e"" has a lower weight, therefore ""e"" should be in the MST and not ""e2"".

Prim's algorithm does this: start with a node, add the minimum crossing edge between that node and the rest of the graph, now you have a partition containing 2 nodes, add the minimum crossing edge between these two nodes and the rest of the graph, repeat until all node are in the MST.

Given that you only ever add nodes that are minimum crossing edge, you are always going to get the MST.

For kruskal: process the edge in order of their weight value. Add the lowest-weighted edge at each step that does not form a cycle with previously added edges. Since we are careful to not form cycles, we are always connecting two previously unconnected subset of nodes, and since it is the edge with the lowest weight in the entire graph, we know that it is a minimum crossing edge, therefore we always add minimum crossing edge and thus we always get the MST.

Source: Algorithm fourth edition by Robert Sedgewick and Kevin Wayne (obviously I paraphrased, apologies for the poor english but it's 2AM right now)",1525565536.0
SudoKudo,"No, because some may not be connected.",1525566080.0
Terr_,Link is broken or not public. Not sure what the problem description is.,1525488721.0
izzythemachine,"It seems like you're not totally clear on the properties of BFS vs DFS. 

For Breadth-First Search, you use a queue and add the children to the back of the queue. So you would look at all of the Nodes one move away from the beginning Node before moving to the Nodes two moves away. 

For Depth-First Search, you would use a stack, and add the child Nodes to the top of the stack. This would cause you to do a deep dive - one move, then two moves, etc. without checking other ""one move"" children first.

When you build the children, you don't need to populate their children for BFS, you can wait until you get to that point in the queue.

Hope this helps!",1525508672.0
Gavcradd,"I'm a UK Computer Science teacher. If anyone is looking at doing this, good on you, but I have one tip - be prepared to go in a very, very, VERY low level initially. I mean, whatever level you think is suitable and then halve it. Yes, there are some very bright students who lap up anything you give them but the vast majority know far less than you think.

There is an organisation in the UK called CAS (Computing at School) that has this exact issue - people in industry, programmers, network engineers, etc, all want to help but constantly provide ideas that only the top 1% of kids would be able to access. They forget that they were probably one of the brightest kids in their own class and have had years of experience in their field. I mean like wanting to look at efficiency of algorithms to kids who can't even use a mouse (years of pushing iPads does that unfortunately). You can get there, but it takes a while.",1525503668.0
JaiX1234,"Adding on to other commenters. It’s always the same experience for me, been teaching a program at a uni for over a year now.  

These kids are not ready for computer science and it doesn’t make sense anymore. These days I just go in there and talk to lost uninterested kids.  

Computer science in my opinion? Too complicated for the average kid.",1526447393.0
TheUltimateFuckUp,"Christ, Pearson has a MyTheoryofComputationLab now?!?..where does it end??",1525439512.0
jrtc27,"The first one seems correct due to the commutativity of + (as alternation).

The second one is wrong. Expand the bracket like you would for standard arithmetic, since concatenation distributes over alternation. That gives you ε+ε00+1ε+001+000 = ε+00+1+001+000. Now sort these lexicographically to get ε, 00, 000, 001, 1.",1525439895.0
random314,"One of my favorite question to ask when I interview is the lru cache. It involves multiple days, such as linked list and hash table. It's not terribly complex and makes for excellent discussion. I feel like it's one of those problems that I can actually work with the candidate to solve step by step. ",1525438907.0
Mixolyde,"Not sure if this is what you mean, but this is my favorite CS programming homework assignment: 
https://www.eecis.udel.edu/~decker/courses/681s07/prog1.html
I use this assignment to teach myself new programming languages (some of which are in my public Github repos). I usually use iteratively deepening depth-first search for the smaller problems, because that algorithm is simple to implement, but there are other options. Then I use A* for the hardest problema, because depth first won't solve in reasonable time.
Also, try codingame.com if you want to learn and practice A.I. techniques. There are a lot of great puzzles and games.",1525434484.0
dvirsky,"Text indexing using an inverted index, is just  joyful mix of clever data structures, algorithms, optimizations tricks, compression methods, etc.",1525449773.0
BoobDetective,"I enjoy Google Translate's realtime image translation. It involves AR, OCR, Neural Nets and more! It is fun to think about how they implemented each feature it has. It's just a potpourri of interesting modern algorithms.",1525428160.0
PaperClip44,I made a zombie game that uses graph search algorithms to chase the player.  Had a blast building that!,1525435378.0
FUZxxl,[Tarjan's algorithm](https://en.wikipedia.org/wiki/Tarjan%27s_strongly_connected_components_algorithm) for computing the condensation of graphs is pretty neat with respect to the data structures it uses.,1525443709.0
beeskness420,Christofide's algorithm is cool in that it seems like you just throw a bunch of graph algorithms at it and out pops a solution. ,1525456422.0
atasco,"https://en.m.wikipedia.org/wiki/Suffix_array , you can do fancy stuff with this in linear time. Was really impressed when I learned about Suffix Arrays.",1525461930.0
epostma,"There are some pretty interesting algorithms (though not necessarily data structures) involved in computing some quantities in so-called robust statistics (where outliers don't have much effect). For example, the repeated median estimator; least trimmed squares regression; computing Rousseeuw and Croux' quantities Qn and Rn.",1525489261.0
shahidiceprince,Some pretty cool answers here. Just leaving this here to come back later. Don't mind me.,1525490130.0
auriscope,Reduction from Least Common Ancestor to Range Minimum Query (and RMQ itself).,1525502397.0
Aatch,Anything thing that uses dataflow analysis is pretty cool. The way information about programs seems to just fall out of them is marvelous.,1525519963.0
wretcheddawn,"One thing I worked on recently was to find a list of all cycles as well as nodes not in a cycle in a graph.  Had a hard time finding an existing algorithm for this, so had to create it myself.",1525435869.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/askcomputerscience] [Hey reddit, what is your favorite cs problem that uses multiple data structures and algorithms to solve it?](https://www.reddit.com/r/AskComputerScience/comments/8gxvll/hey_reddit_what_is_your_favorite_cs_problem_that/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1525425161.0
remy_porter,"Or, instead of MIDI, look into OSC. On the hardware side, there are OSC libraries for the Arduino, so you can use the Arduino to control the instrument, and your Python code can send messages via OSC.",1525437777.0
nightcracker,Look into the wondrous world of MIDI.,1525427920.0
KatsuCurryCutlet,"One intuition behind the idea of recursive enumerability is that it is the set of inputs on which a Turing machine would eventually halt on. Whereas if the TM were provided some input that was not in the set, it's not guaranteed to halt. 

So lets say we were provided some total computable function f, the range of f can be made into a set of inputs on which a Turing machine as follows: On given some input x, keep trying f on 0, 1, 2, 3... and so on and accept if f on one of the inputs eventually returns x.

Since f is total, it will always return a value. and IF x was in the range of f, then the turing machine we have described would eventually terminate. IF x was not in the range, the search could go on forever and we would not know, since the TM would just continue on forever. So the set of all x on which this Turing Machine accepts, (i.e. the domain of this turing machine) is exactly the range of the total computable function f. and since we can express each TM as a recursively enumerable function, this set of x is the domain of a recursively enumerable function.

Hope this helps. Feel free to ask for a follow up in case my explanation wasn't helpful enough",1525405176.0
holomorphic,"I assume that you are defining recursively enumerable to mean ""the domain of a partial computable function"". Let g be the (partial) function defined by mapping n to the least x such that f(x) = n (if such an x exists). That is, we check if f(0) = n, if f(1) = n, if f(2) = n, etc. So n is in the range of f iff it is in the domain of g.",1525403727.0
Rioghasarig,"It would help if you defined those terms here. It sounds like it may be something simple, but I don't remember what those terms mean. ",1525402602.0
kuba_10,"Okay, finally found resources myself.

To anyone who stumbles upon this thread: you are probably looking for *hardware sizing* methods. Could not find this term anyhow.",1525383636.0
riksterinto,"In any implementation I have worked on the main factor is number of concurrent users.  We also look to non-functional requirements however these aren't necessarily dependent on hardware.  eg. Maintaining the desired response time can be achieved by changing workflow or app responsiveness.


Hardware is changing all the time and no two implementations are ever alike so there is no real general approach to this.  I wouldn't classify enterprise applications with given architecture as limited in scope.  You also need to consider other factors that impact hardware choice like geographic, regulatory, hours of operation, security, production support, training, etc.  

TLDR hardware sizing is usually determined by business and solution requirements, not Computer Science/Engineering principles.",1525556038.0
Syonyk,"Interesting bit of writing on why C shouldn't be considered a low level language.  Though, by the definitions used, assembly (for x86) shouldn't be considered low level either.

I'm not sure I agree with the conclusions about how we need to build new parallel processors, but the gap between the architectural specification and the actual micro-arch implementation is now certainly too large for anyone to reason about.",1525361764.0
Lucretia9,"When I learnt C is was a high level language, because it abstracted the machine away, sort of. But in relation to other languages such as Ada, Java, etc. It is a low level language in a way. But both definitions are wrong, it's actually a medium level language as it sits between assembly (the low level language) and the aforementioned high level languages. To think otherwise is just wrong.",1525879545.0
DSrcl,"People arguing whether it is correct for the author to say C is a low-level language are missing the point.

The programming model used by C no longer provides a sufficiently accurate abstraction for the underlying machines. Most programs run on multicore machines with vector units, but none of these features are captured nicely by C's programming model.

The point is not that C is not low-level -- it is if you use the conventional definition that it requires excessive attention to details that are irrelevant to *most* people. The point is that C is not the abstract machine that people designed it to be anymore.",1525413096.0
spinwizard69,By any rational measure C is a low level language.   To paint it any other way just leaves the author open for controversy.   It will make looking for work a lot harder too.,1525395337.0
Free_Math_Tutoring,"> Everyone is marked as a Transistor.

In real hardware, hardly any transistors have ""random"" (i.e. user-generated) output, most are defined by the output of other transistors. So this would get you nowhere.

If you had a specific hardware architecture and defined each person to be a bit in memory, then you might get some random results on occasion, such as printing a single digit/character or something, but nothing useful would ever happen with finite time and students.",1525348537.0
gnullify,"This is just an adaption of the infinite monkey theorem, so you'd need infinite monkeys (in your case children). ",1525355303.0
andrewpolidori,Why not start it and put a couple articles out there? A community is way more likely to be interested in something that exists. ,1525354121.0
thebyteman,Let's use machine learning to form an opinion on this.,1525354522.0
vladcalugaru,"Yes. Make it interactive. It took me a while to wrap my head around some algorithms, maybe some visual animations and plots will help others learn it faster. ",1525363076.0
magnificentbop,"I would read it, especially if it used universal vocabulary.  I've always found the domain-specific terminology to be distracting at best and opaque at worst.",1525365766.0
afineday2die,"I would be very interested in the real world applications bit. Almost all the reading I find about machine learning seems to be about research: new techniques, or a new SOTA benchmark for a dataset.

I want to know about how people use machine learning to solve problems relevant to their domain, regardless of whether or not it uses the ""cutting edge"" in deep learning tech.

The theory part is not that something that I'm that interested in, I feel like there are many decent courses to learn them.",1525505895.0
TombKrax,"""People like [likeable thing]"" is a tautology",1525357710.0
Derpstiny123,I'd like it a lot. ,1525349544.0
DigitalGeo,I'd support it,1525352739.0
manys,"I'd be more interested in a discussion of when (and whether) it has *ever* been successful at this in the past. That said, ""inform"" is a pretty squishy term here.",1525372129.0
Ohigetjokes,I'd love to be on a newsletter mailing list for this.,1525398441.0
CyAScott,"I would like to see a web site with guides, examples, and best practices for developing neural networks. However, the theory behind neural networks are still a big mystery. I know there was a theory about [“information bottleneck”](https://openreview.net/forum?id=ry_WPG-A-), but a lot of people don’t buy that idea. Neural networks are a lot like physics, we can explain in detail how the mechanics of the universe works, but we have no clue why they are that way.",1525403895.0
metalxythe,"So, [scholar.google.com](https://scholar.google.com) for those who are uninitiated?",1525411302.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1525335093.0
ash663,"https://www.amazon.com/Computer-Networking-Top-Down-Approach-7th/dp/0133594149

Loved reading this during my undergrad",1525296547.0
keten,Why would you have to implement it? Just implement evolution and the placebo effect arises by itself.,1525290131.0
kernel_picnic,How high are you?,1525310394.0
t_bptm,"There are two understandings of the simulation hypothesis. One involves a placebo, the other does not. ",1525289714.0
mcandre,Placebo implements itself in beings attracted to a sense of control in their lives.,1525289945.0
boozy_hippogrif,Who says the placebo effect is a feature?,1525764824.0
djingrain,"Some that come to mind off the top of my head:

Calculus, Linear Algebra, and Computer Architecture. There are more but i cant think of what they are called at the moment",1525284508.0
ooa3603,"Calculus, Physics and Linear Algebra are the most important ones missing.

Maybe take them at a near by community college.",1525287766.0
SL4M_DunkN,"Linear algebra, especially if you're looking at security from a crypto perspective",1525304224.0
WhackAMoleE,"> would appreciate pointers

That's a good start.",1525283279.0
simonsaman,"I couldn't recommend you more to read Structured Computer Organization by Andrew Tanenbaum, that could be a good start",1525282221.0
scutus,"You could build a CPU with VHDL and then run some assembly on your CPU...this is heavy hands-on, but you won't get more detailed.",1525282740.0
Semaphor,"Once you master pointers and have a year or two of solid development under your belt, look at Ulrich Drepper's paper entitled ""What every programmer needs to know about memory"". It gives a gentle introduction into how modern CPUs work under the hood. Because what you see as a C developer is castle different than what the CPU does. It's a fantastic rabbit hole. ",1525298906.0
pinano,*nand2tetris* and *Code*,1525308084.0
Carpetfizz,"[Organization and Design, RISC\-V Edition \- Patterson & Hennessy](https://www.amazon.com/Computer-Organization-Design-RISC-V-Architecture/dp/0128122757)

The authors of this book recently won a Turing Award.",1525373257.0
malcontentGeek,Computer Systems: A Programmer's Perspective. This is the textbook used by my arch class and I found it very helpful. It starts with very basic ideas and builds up to pipelined processors and also covers memory intensively. ,1525637434.0
Doberman123456,"My university's computer architecture course is based on the book 'Digital Design and Computer Architecture' if that helps :)

Here's a link if you want to give it a read: https://github.com/MikePaNtZ/uhcl-work/blob/master/comp%20arch/Digital%20Design%20and%20Computer%20Architecture%20(2nd%20Ed)(gnv64).pdf",1526068736.0
farnz,"At the time (late 70s/early 80s), octal was still commonly used in preference to hexadecimal, and we didn't yet have the dominance of power of 2 word sizes we do today (plenty of machines still with 9, 18 or 36 bit words). Remember that the DEC PDP-10 would still be a common machine at this point, for example.

Hex was therefore going to be unfamiliar to a significant subset of network admins; your choice is octal, or decimal. Octal would work, but gives you no compactness advantage over decimal, or dotted quad.",1525279307.0
proskillz,"Vint Cerf is still alive and I actually attended a speech he did a while back. He touched on why he chose dotted decimal, but I can't exactly remember. I'm sure you could reach out to him directly and get a definitive answer. vint@google.com",1525290494.0
GuyOnTheInterweb,"The initial Internet Protocol RFCs do not use the dotted representation.

[RFC791](https://tools.ietf.org/html/rfc791) does not use this notation, this seems to have evolved separately.

The dotted decimal notation seems to have been introduced in 1981 in [RFC790](https://tools.ietf.org/html/rfc790) to indicate subnet allocations (which by then could be smaller than 24 bit netmask) .

The previous [RFC770](https://tools.ietf.org/html/rfc770) only allocates based on the first 8 bits and did not use the by-now familiar `aa.bb.cc.dd` annotation.",1525272739.0
jet_heller,"I wasn't there when it was chosen, but I suspect it was done for memorization. Before DNS, you would to be able to remember IP addresses and remembering HEX is much harder than remembering 4 short decimal numbers. ",1525271544.0
giesse,"    $ ping 0x7F000001
    PING 0x7F000001 (127.0.0.1) 56(84) bytes of data.
    64 bytes from 127.0.0.1: icmp_seq=1 ttl=64 time=0.083 ms
    64 bytes from 127.0.0.1: icmp_seq=2 ttl=64 time=0.063 ms
    64 bytes from 127.0.0.1: icmp_seq=3 ttl=64 time=0.043 ms
    64 bytes from 127.0.0.1: icmp_seq=4 ttl=64 time=0.078 ms
    64 bytes from 127.0.0.1: icmp_seq=5 ttl=64 time=0.041 ms
    ^C
    --- 0x7F000001 ping statistics ---
    5 packets transmitted, 5 received, 0% packet loss, time 3999ms
    rtt min/avg/max/mdev = 0.041/0.061/0.083/0.019 ms
",1525307375.0
feedayeen,"ITT: People who have spent years studying computers arguing if it's easier for people to remember hex or decimals.

Not ITT: People who have never seen hex. ",1525336302.0
istarian,Readability perhaps. It's much easier to remember 255 than FF. Did they have DNS yet then? ,1525299633.0
Irkutsk2745,IP addresses are actually supposed to be read in binary. And it just happened that back then 8bit was popular ,1525335896.0
AlceniC,"Actually, it is just presentation of 4 bytes to humans. All the mask operations are performed bitwise, and other operations are performed on number of bits.

So in essence, there is nothing decimal about it.",1525278597.0
riksterinto,"Maybe to minimize memory impact.  8x4=32 < 16x4=64

RAM was real tight back then.


Also possible 8bit hardware was considered.

",1525277277.0
fried_green_baloney,"Last time I looked, it's easer to remember

> 8.8.8.8

than

> 134744072

Hex vs. Decimal?  No opinion on that.",1525279173.0
Drainedsoul,How is this related to computer science?,1525290383.0
rockymma,"I would push back on the whole ""Code not documented, and it doesn't work"" bit, but try not to be confrontational, showing a little humility and an aurora of disappointment in your work will really go along ways in this situation. I would also be honest and tell him that you heard he was running a research project and were trying to impress, and blundered in the process. 

9 times out of 10, if you show that you care and respect the teacher and his/her feedback, you'll get far better results than if you were to try and appeal / escalate / go to the dean about the situation. If you have a good track record in his classes, and in this class in particular then that'll also help you get to a better point in the situation. 

I'm on the fence about whether you should mention having to take the course again, I wouldn't mention it unless you have to or he brings it up first. That way it at least sounds more genuine, and not like you're begging for your life w/motive.",1525246980.0
deanveloper,"I think that he does bring up a good point in that when you're in IT, you definitely shouldn't step outside of your bounds. If you need to create 4 procedures - you create 4 procedures.

You can't really do much about the whole development environment thing. Did you start from a completely clean one before starting?

I don't exactly understand the whole MGMT/TECH documentation, although to be fair this doesn't mean much to management:

> Add schema data to a file for a specific table for a specfic set of columns.

Also, not sure if you had syntax highlighting while writing this, but looking at the gist without highlighting is kinda horrific. If he doesn't have it, I can see how he complains about it being hard to read.

I'd send an email back to the professor. Let him know that this puts you below the bar, and that it worked in your development environment. I'm not sure what you need to pass the class, but if you can show that it works, you may be able to recover it.",1525245673.0
Thagor,"OK I will give you the perspective of some one who teaches tech classes (In Europe though so student teacher culture may differ). It is always important to ask your teacher to discuss your grades if you do not understand how they came to be. So go to him and ask him to discuss your grade. Do not go in with a notion of ""I want to discuss the grade to get a better one!"" go in with the notion ""I don't understand this why is it this way?"". When he gave you his side of the story, tell him why you don't agree with this assessment. Go from here but it is paramount to get his side of the story first.

sneak edit: if it turns out he is right in his assessment ask him if you can put in some extra work to get a better grade. I mostly grant that to students who ask me for it. But it might differ at your school due to rules/bureaucracy/bad student ""management"" software.",1525250567.0
AustinLMayes,"*UPDATE:* 

> Austin,

>I see what happened.  I started with Assignment-10-Results.docx as I have done all semester and do with all other students.  Inside this starting document, I get the following:

>	The IMP_EXPT package contains all the needed methods to import and export data from any table. The export and import scripts will throw errors describing issues relating to data integrity and will provide instructions on how to fix them. Equality can’t be easily determined so we have to do that manually for now.
>	Unfortunately, due to limitations in SQL's reflectivity and dynamic abilities overall, dynamic record creation is not 
>   possible without a lot of hacky ugly code that is even harder to read.
>   Due to this, there are a few limitations as to the way data can be exported:
 >      - Primary keys must be in a sequenced number format. NOTE: There can be breaks in the sequences, and these are handled seamlessly.
 >      - Column serialization is done in a very inefficient way, and can be taxing if there are a lot of columns per row.

> This is the TECH documentation.  Notice there is no explanation of 10.sql, so you haven't given me a reason to use 10.sql.  I focused on Assignment-10-Results.docx which gave me very little.  Honestly, based on this, I concluded that there was an import/export package that was not of your making (again, no reference to 10.sql).  Now, in your emails you have given me a reason to go to 10.sql and grade it.  

> After further review, your grade changes to 190/200 because your documentation lacked clarity to give me guidance on your approach.

*TLDR;* I got 130 extra points.",1525275287.0
balefrost,"So when I was in school, I was taking a computer graphics class. There was a computer lab full of I believe Sun machines that were used by the TA to grade our projects. When I was working on one project, it was snowing outside and I didn't want to trek down to lab. So I SSHd in and told it to route X windows to the X server running on my machine in my dorm room. Finished the assignment and submitted it.

My grade was ""would not run"". It turns out that, while my dorm computer could do 24-bit color, the lab computers could only do 16-bit color. A simple change in the initialization fixed it, but MAN was it frustrating. I learned a lesson that day.

Is it possible that your environment is incompatible with your professor's environment?

> it gets errors stating “imp_expt.stringarray” must be declared

It *does* look like you're declaring it at the start of the file. Did the professor just miss some lines?

---

What four procedures was the professor expecting? Does your implementation in any way resemble what the assignment was asking for? It's one thing to go ""above and beyond""; it's another thing to solve a completely different problem.",1525263478.0
musing5,I would suggest divine intervention. ,1525226958.0
mayankj08,"For OS I would recommend: http://pages.cs.wisc.edu/~remzi/OSTEP/

This book is really short and to the point and you can complete most parts of it in three days.
",1525226947.0
kamasutra971,"Hey try the temple OS ... Awesome tutorials by Terry Davis!

Very wonderful man and a down to earth person",1525251372.0
cluesthecat,"Maybe Khan academy? A lot of OS stuff is learned by actually using said OS but networking can be learned through a lot of books. All in all, you’re not going to fully understand until you’ve gone hands on. ",1525227032.0
rohan32,"https://littleosbook.github.io/

and a lot of praying",1525266341.0
anuragroy11,"For Networks, I can recommend this free course by Google https://www.coursera.org/learn/computer-networking

",1525266527.0
GreyDalcenti,"For networks, i recently came across this book https://www.saylor.org/site/wp-content/uploads/2012/02/Computer-Networking-Principles-Bonaventure-1-30-31-OTC1.pdf , its small enough to read fast, and covers the basics.",1525267752.0
AddMeOnReddit,"I️ just finished a class over this at Uni and this helped a ton. It’s essentially a summarized version of the book we had, which was OS Concepts with Java. 

Here ya go: https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/",1525267790.0
zinsuddu,"You can get a quick orientation by studying the powerpoint slides that accompany the essential books.  For example, a good, standard reference on operating systems is Silbershatz, Operating System Essentials.  The slides can be found, e.g. [from Yale](http://codex.cs.yale.edu/avi/os-book/OSE2/slide-dir/index.html).  At least you will have seen an outline of the subject and encountered some mention of all of the important concepts.",1525320580.0
patsluth,If I remember correctly with lower status registers you are supposed to replace the original values they held when you are done with them. ,1525231150.0
claythearc,In ARM the low registers are volatile. A lot of system calls or C++ calls like printf will destroy the contents when used. So it’s good practice to not use anything below R3 for sure (and maybe higher. I can’t really remember the cutoff) with data that isn’t meant to disposed off. ,1525261857.0
_--__,"Hi,  
I have removed your post from /r/compsci - as per the sidebar we discourage posts of this nature.  I recommend posting to /r/askcomputerscience.",1525272974.0
Smallzfry,"Go to https://teachyourselfcs.com/ and check out some of the books they have listed there, those are generally some of the best to learn from (or so I've heard).  I can personally vouch for the networking book being good, but I've never touched any of the rest.  I think there was a post here either earlier today or yesterday that had a list of books to look at as well.",1525203060.0
_--__,"From the sidebar: 

>  A list of book recommendations from our community for various topics can be found [here](https://www.reddit.com/r/compsci/comments/40mq3q/what_are_the_canon_books_in_computer_science/).",1525272676.0
turning_tesseract,"For Algorithms and Data Structures, I would recommend the book by Goodrich and Tamassia. There are three versions of the book that you can choose from, depending upon which programming language you are most comfortable with -  [Java](https://www.amazon.com/Data-Structures-Algorithms-Michael-Goodrich/dp/1118771338/ref=sr_1_2?ie=UTF8&qid=1525557182&sr=8-2&keywords=goodrich+tamassia&dpID=61cFhkf7NCL&preST=_SX218_BO1,204,203,200_QL40_&dpSrc=srch), [Python](https://www.amazon.com/Structures-Algorithms-Python-Michael-Goodrich-ebook/dp/B00CTZ290I/ref=sr_1_3?ie=UTF8&qid=1525557182&sr=8-3&keywords=goodrich+tamassia), or [C++](https://www.amazon.com/Data-Structures-Algorithms-Michael-Goodrich/dp/0470383275/ref=sr_1_7?ie=UTF8&qid=1525557182&sr=8-7&keywords=goodrich+tamassia) .",1525557461.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1525201012.0
djimbob,"Is it normal to forget things that you haven't used in 10 years?  Yes, especially in detail (though you often remember core concepts and can relearn quickly -- e.g., don't know how to calculate integral of x log x or find eigenvectors/eigenalues of a matrix A, but know the integral is the (signed) area under the curve and eigenvectors (**v**) and eigenvalues (λ) satisfy A**v**= λ **v**).  

Please note if you relearn discrete math, calculus, etc you'll probably forget most of the details again in few years once you stop using it frequently.

Is it worth it to re-learn?  For your current career path of doing test automation/QA for a web dev shop, probably not.  But it could be worth it to know for other reasons, such as you found the math really interesting and want to continue your education or you want to get into 3-d computer graphics or data analysis or machine learning (and get confused with the linear algebra to understand something like PCA).  ",1525199299.0
Meguli,I suggest Advanced Calculus classes by Prof. Shifrin on YouTube. You will see linear algebra and calculus in new light.,1525196654.0
foreheadteeth,"Hi I'm a math prof and I often get this question. To the question ""is math necessary in life"": no, many people can't even do percentages. My grandmother, God bless her, stopped school in fifth grade and she did fine.

You need math only if you decide to do something hard. For example, if you decided to go into machine learning, a booming area of computer science, hoping to get a higher salary, you probably would need some statistics and linear algebra and more.

If you're planning to continue doing QA, or perhaps move into a management or sales position where you never program, then the math is less useful.",1525209716.0
,[deleted],1525195563.0
agumonkey,hackaday has many people doing that,1525205089.0
ObscureAnalysis,"RISC-V has been mentioned here, but seriously check out their website riscv.org. They have workshops and summits almost monthly, and they have a bunch of mailing lists. Should be up your alley as well. (Also check out their GitHub for a bunch of neat-o tool sets --> https://github.com/riscv)",1525207686.0
frezik,"If you don't find one, you might be in a good position to start one. With FPGAs becoming more affordable to hobbyists, this is set to become a viable niche.",1525196478.0
RemarkableShame,Maybe [LibreCores](https://www.librecores.org/) is what you’re looking for.,1525198012.0
zsaleeba,You might be interested in /r/cpudesign,1525220257.0
pastermil,https://hackaday.com/2018/04/24/first-lithographically-produced-home-made-ic-announced/,1525221896.0
ZipCPU,Have you tried out the #cpudev IRC forum on freenode?  There's a small group of individuals all dedicated to building their own CPU's there.,1525290528.0
sailorcire,"Look in r/electronics, they might be more helpful and also freenode \#\#electronics.

I know Julian Ilett has been making a home brew, but....well...he knowledge is variable to say the least. 

There are many home brew CPUs (including relay based ones ❤️) on YouTube and you might be able to find a community that way.

Also, look at [gigatron.io](http://gigatron.io).

Hope that gets you started on the right path. 

I'd also like to recommend H&P Computer Architecture as well as Deschamps et al. Synthesis of Arithmetic Circuits. That is, if you need reference material.",1525199226.0
mcandre,"Awesome! Are you aiming for supporting a subset of amd64, or just rolling your own ABI?",1525206258.0
_georgesim_,/u/zipcpu created the ZipCPU and is somewhat active on reddit from what I can tell. I would see what subreddits he visits or ask him directly.,1525288848.0
Microscopian,Typing your title into Google showed this on the first page: http://homebrewCpuRing.org,1525207830.0
,[deleted],1525197175.0
EggCess,"**edit**: Almost everything in this post is wrong, see answer below

Hmmmm ... I'll try to answer this, but am not exactly sure where you are coming from. Why do you think it matters whether the Group Communication System is using synchronous or asynchronous communication; where's your question coming from?

Active Replication, \(I'm used to calling it State Machine Replication\), requires a consensus protocol \(or rather, mechanism, blockchains could also work for example\) to totally order incoming requests to each replica. Whether this protocol works with asynchronous or synchronous channels is irrelevant to the question, as far as I can see. As long as the consensus mechanism guarantees the same total order of requests at every \(deterministically executing\) replica, two different modes of fault\-tolerance can be guaranteed:

* Crash\-stop failure model: 2\_f\_\+1 replicas, for a max. of *f*  faulty/crashedreplicas.
* Byzantine failure model: 3\_f\_\+1 replicas, for a max. of *f* faulty/malicious replicas.

Also, the ""reasonable time"" part of your question somehow also doesn't make sense to me. One major benefit of SMR is its capability to completely mask either crash\-stop or byzantine faults without the client noticing any downtime whatsoever. The thing about a\-/synchronous communication is strictly a problem of the GCS, which masks such problems completely from the conceptual problem of determining the quorum for the desired mode of fault\-tolerance.

However, I fear I misunderstand the question ... maybe you can specify where you are coming from and what led you to this question? :\)

*edit*: Also, Fischer, Lynch and Paterson proved in 1985 that it is impossible to do deterministic distributed consensus with asynchronous communication and faulty nodes \([https://groups.csail.mit.edu/tds/papers/Lynch/jacm85.pdf](https://groups.csail.mit.edu/tds/papers/Lynch/jacm85.pdf)\)

There are solutions that introduce randomness and non\-determinism as an attempt to work around this problem, but that leads to other problems. Current real systems use these and work around the problems created by the workaround, but in the end it's reaaally hard to get a proper byzantince fault\-tolerant SMR\-system right.",1525199870.0
eternusvia,Perhaps this would be useful: [Creating 3D visualizations of MRI data: A brief guide](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4648228/),1525180234.0
tfmoraes,If you have the MRI images you can use a software like InVesalius or itksnap to segment the region you want then generates a mesh from it. Something like this http://www.instructables.com/id/Edible-Chocolate-Brain-from-MRI-Scan/,1525181848.0
Triddy,"Going through the Stanford List, here are the books listed there that I have also used and thought positively of:

* **Algorithms:** Introduction to Algorithms    
* **Computer Networks:** Computer Networking (Nice name)   
* **Machine Learning:** Artificial Intelligence: A Modern Approach    
* **Programming:** C Programming Language

Of the list, the only one I have read and *didn't* like was *Operating System
Concepts*. It's not that it was terrible, per se, I just liked [Modern Operating Systems](https://www.amazon.ca/Modern-Operating-Systems-Andrew-Tanenbaum/dp/013359162X) a lot more. I used the third edition, but I can't imagine the 4th is any worse.",1525152691.0
tzjmetron,"For fuck's sake, just go look at OP's profile. He/she is clearly not interested in our replies. He's probably hired by people to spam such sites. Reprehensible to say the least.- the deception, not the content that is. Shame on you, OP.",1525201160.0
duskhat,The books under “Algorithms” are good and well-known,1525149475.0
fj333,Looking through the reviews on Amazon will yield orders of magnitude more answers than you'll ever get in a single thread here.,1525160350.0
Ilyps,"For AI/ML, the book recommendations at first glance seem very good. I'm personally familiar with *Pattern Recognition and Machine Learning* by Bishop, *Artificial Intelligence: A Modern Approach* by Russell&Norvig, and *Pattern Classification* by Duda/Hart/Stork and would recommend those.",1525167172.0
,"Ban him for vote manipulation and self promotion. He has two accounts minimum that he's using here, the other being /u/ThisToni who both relentlessly comment about this site exclusively.",1525207672.0
ThisToni,Seems like a good website!,1525148502.0
,[deleted],1525149282.0
jpco,"I used the Sipser computation theory book, though I don't remember much how good it was.

Operating Systems: Principles & Practice is definitely worthwhile, though - I still have it prominently on my bookshelf.",1525156191.0
HungryhungryUgolino,"I agree with /u/Tridday's thoughts on the books listed and their preference of MOS over OSC. 
The AI choices are both pretty standard. ML is also one of those things where if you don't understand a particular explanation you can easily look it up online to find other sources. If you don't have stats and linear algebra experience they will be difficult, but the mathematics can be learned while you run into unknown concepts or methods for solutions. ",1525175483.0
versaceblues,"Would this book https://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738/ref=mt_hardcover?_encoding=UTF8&me= 

Be good for someone with no knowlege of machine learning that wants to learn. ",1525201540.0
WhatIsAMachine,Awesome list,1525164797.0
angrypotato1,/u/opfeels /u/me,1525152840.0
zozozozozozoz,School,1525154921.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1525141168.0
lenswipe,">insertIntoBlog($title, $author, $isPublished, $publishDate)


If I'm understanding you correctly, that looks like a stored procedure. I'd try and avoid those if possible because they essentially put part of your application logic in the RDBMS.",1525136454.0
zanidor,"> I can give a developer a limited view of the database's internal structure

A DAO abstracts the details of the database away from the programmer. With a DAO, you should be able to swap out the backing database implementation and change nothing in the application except the internals of the DAO. 

So, I'd say if you're exposing details of the database structure, then that's good evidence what you're building is not well described as a ""DAO.""",1525141586.0
fzammetti,"A DAO is a chunk of code \(typically a class\) that abstracts away the details of the underlying storage mechanism.  So, using some simple Java, something like this would be a DAO:

`class MyDAO {`

`public void addBlogEntry(BlogEntry be) {`

`// Write code here to take the data from the BlogEntry object and execute SQL inserts from it`

`}`

`public BlogEntry getBlogEntry(int blogEntryID) {`

`// Write code here to execute a SQL select based on blogEntryID, build a BlogEntry from it, and return it`

`}`

`}`

The exact implementation of those methods are up to you, but critically, if you later decided that you want to, say, make a web service call to another server that stored the blog entries in flat files instead, something  considerably different than storing in an RDBMS in terms of code if nothing else, you could do that just by changing the implementation of these methods.  The code that uses this DAO wouldn't need to be touched.",1525147375.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/webdev] [Is this an example of a Data Access Object? If not, what do you call this?](https://www.reddit.com/r/webdev/comments/8g4iz5/is_this_an_example_of_a_data_access_object_if_not/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1525136362.0
donghit,"Why not just reference the papers on it?
[Scroll to bottom of page](https://github.com/savrus/hl)",1525138785.0
scaevolus,Look into contraction hierarchies. https://ad.informatik.uni-freiburg.de/projects/google-focused-research-award-next-generation-route-planning?set_language=en,1525178217.0
PythonGod123,Do you know any pathfinding algorithms for zombies?,1525134955.0
BatmantoshReturns,"Hey, I actually got to see class project in person that became the basis of this paper! How did you stumble upon it?",1525132993.0
DogArt4Me,"Can't read. Two ""and""s in the text in figure 5.",1526613882.0
,Nice. I just left a university with my bachelors in CS. My nightmares are becoming a reality as well! ,1525027791.0
Commander-TeeJ,"If you're school is anything like mine don't expect to learn programming in class, you'll learn concepts but the bulk of it is up to your passion.",1525030036.0
CarJamn,Congrats! Try and get ahead if u can!  Start learning new languages now so you're not caught off guard at school!,1525028724.0
,"Find out the language that your University will focus on. Start learning the basics now, and possibly some data structures and algorithms. Good luck! :D",1525033389.0
tjclccs,"One thing I’ve learned is to make a Github account, and to upload all of the successful projects I’ve completed at uni! This will be essential in job finding, as it shows experience on top of grades etc",1525043394.0
Spoogly,"Congrats! 

To follow a trend, here's some advice you didn't ask for: Get an internship as early as you can, or get engaged with local Dev communities that overlap with your interests. Your degree qualifies for for the dream job, but your network gets you it.

Also, along the same lines, no one is really going to teach you how to interview, or how to talk to people you don't know (in a crowd, on a stage, or just one to one). These are useful skills, though, so if you see interesting opportunities to do those things, even if they're not in a computer science setting, take it.
",1525043480.0
really_horribleCoder,"Make sure you live while study. If you don't, you'll get FAT and BALD lmao. (I'm on this path unfortunately but I'm boutta get the heck out soon as I get out of these dang finals)",1525107838.0
pulsar512b,Congrats!,1525027004.0
random314,Data structures! Pay extra attention to those and don't ever forget it. It's going to super important if you want to get decent jobs.,1525054048.0
Carpetfizz,have fun!,1525037958.0
amanster82,"Just wait till you're in school. The excitment quickly turns into stres. But the time you graduate, you're looking for a job but even more stressed to find a job with the right people. There tends to be a lot of dicks in the industry that are usually in charge.",1525617835.0
rimhof456,Congratulations and good luck,1525034071.0
marknobs,"As a CompE that's just about to graduate, my piece of advice is to remember how excited you feel right now, and let it drive you to do well. My first few years were scattered and not very focused, and it caused my GPA to drop. Trust me, you'll be much happier by the end of school if you remember the excitement you have now when things get rough and let it push you to do great. Good luck, and kick some ass!",1525054700.0
wrong_assumption,Congrats on picking Stanford. Good luck!!!,1525058130.0
J-SR-71,Awesome ,1525058380.0
misingnoglic,Congrats!,1525070395.0
,"Good luck man. I'm close to finish my first year in my college. Am 21 and it is not my first field nor my first college. All I can say is stressful path is waiting for you.Things are probably different where you live in. I am living in Turkey so my college is charge-free but lessons are hard as heck. First term was good. Had one B and one B+ as well. This term, my midterms had hit me so hard. Apparently it is my fault that I procrastinated more than enough. On the other hand, my personal issues were something that became an obstacle in front of me. Now, I am trying to fix all those bad marks. 2 assignments that I have to do and final exams that starting on 18 May. And also that summer classes are something that I am trying to avoid.Either way, good luck, try to stay stabile both physicaclly and mentally.You got this.",1525075077.0
newbstarr,"Whatever other bullshit you hear, grades matter when finding a position and they must come from a place with a recognised standard.
If you really want to get a leg up learn how to work to a standard and in a team. Showing those plus some experience even if its 3 too 6 months will be invaluable to you when you want to become a software engineer professionally.",1525080173.0
gabrielerzinger,Your nightmares are finally becoming a reality\*,1525032673.0
drink_with_me_to_day,S&M,1525050468.0
__-_-_-_-_-_-_-_-_-,Great. Google competitive programming. ,1525058010.0
blackharr,"If they're unreleased, that likely means that they're College Board protected, so posting them here wouldn't be allowed. If teachers have AP questions that aren't yet released, not even their students get to keep them.",1524883148.0
ullerrm,"[Fair cake cutting!](https://en.wikipedia.org/wiki/Fair_cake-cutting)  Cake in this case is a metaphor for splitting any sort of 1D or 2D resource that can be finely divided -- network bandwidth, advertising space, CPU time, disk spindles, tracts of land, etc. -- between N agents.

Any given agent will have different valuation functions for different parts of the ""cake"" (i.e. some may prefer chocolate or vanilla, some may prefer lots of frosting vs little frosting, large vs small pieces, etc.), and it may not be possible to get a complete, continuous value function across the entire cake for an agent.  And, of course, some of them may be greedy little shits who want the whole cake. :)

So, to actually cut up the cake fairly without that information, you have to treat the agents as oracles, and write a protocol that asks them to make a series of small decisions. Move them towards collectively splitting the cake in a fair manner, without trying to anticipate or simulate how the oracles will respond based on their previous inputs, and in a way that's resilient to inconsistent or antagonistic agents.

It's a surprisingly hard problem, especially in a certain variant of the problem known as envy-free cake cutting.  We had a big leap forward in 2016 when [Aziz and Mackenzie](https://arxiv.org/pdf/1604.03655.pdf) demonstrated a protocol for envy-free cutting with N agents that has an upper bound on runtime; prior to that, the best known protocol was guaranteed to create a fair division but could have unbounded runtime with antagonistic agents.",1524868430.0
fireatx,"the [couch problem](https://en.wikipedia.org/wiki/Moving_sofa_problem) is just funny. it's unsolved, and one of the few problems like this that we can experience directly in normal life. think about this the next time you have to move furniture around a corner...",1524871263.0
Freemanium,"I mean, there's always the good ol 'P ≠ NP' if you're looking for a bit of a challenge.",1524868366.0
bobrodsky,"When does a compressed representation aid generalization? 

If you look at [this paper and the comments](https://openreview.net/forum?id=ry_WPG-A-) you see two things. 

* They show a clear example where more compression coincides with more overfitting.

* Tishby's papers have claimed the opposite and empirically see a link between compression and reduction of overfitting. In the comments of the Saxe paper, Tishby makes an argument for why this should hold. Unfortunately, this argument has some holes in it which might be difficult to fill. ",1524869294.0
mynewpeppep69,"The L-system problem is actually a lot of fun to think about. When I first learned about L-systems in a computer graphics course I spent a while thinking about why some of the more seemingly obvious methods don't work, and I highly recommend it. It's a really hard problem and I'm excited to see progress.",1524870159.0
etano,The sign problem,1524867828.0
peckrob,"Cache invalidation and naming things.

Source: fought with both today.",1524878377.0
AustinCorgiBart,"How do we handle the ever-increasing number of students interested in Computer Science when we don't have anywhere near enough teachers, and we aren't able/willing to pay teachers something roughly equal to what industry can offer?",1524927389.0
tonygd,Isn’t there a delivery truck problem that’s intractable?,1524876984.0
EricIO,"Two pages I've bookmarked that hosts some open problems in computer science.

* [Rewriting Techniques and Applications](http://www.win.tue.nl/rtaloop/problems/summary.html)

* [Typed Lambda Calculi](http://tlca.di.unito.it/opltlca/)",1524922279.0
markth_wi,I was always a fan of the [Collatz Conjecture](https://www.youtube.com/watch?v=5mFpVDpKX70),1524926611.0
Hook3d,I think you're expecting a little much if you want people to share their unpublished research on open questions. ,1524867111.0
,[deleted],1524885470.0
__-_-_-_-_-_-_-_-_-,Help our poor Salesman!,1524908715.0
really_horribleCoder,"Given a random program from the set of all programs that are able to exist, determine if it will ever stop, or ""halt"".",1525108144.0
really_horribleCoder,"Here's a more high-level unsolved problem: Detect statistics of the source a certain smell is coming from, given all necessary sensors for detecting any particles. This has basically been done in computer vision, using visual features/descriptors/signatures/etc but similar principles apply when it comes to computer ""smelling"" lol.",1525108405.0
w0rk_r3dd1t,The complexity of matrix multiplication: https://en.wikipedia.org/wiki/Matrix_multiplication_algorithm,1525148989.0
Morwenn,"As far as I can tell, we don't know any comparison sorting algorithm that works in O(n log n) time on bidirectional iterators and use only O(1) extra memory:

* Sorting algorithms with those properties exist for random-access iterators
* Mergesort can use only O(log n) extra memory with lists, but it needs to exploit the structure of the list (while bidirectional iterators are agnostic of the data structure and can't use that), and it's still not O(1) extra memory
* Mergesort on bidirectional iterators can be tweaked to use in-place merging to avoid using extra memory, but then its running time grows up to O(n log²n)
* Melsort is said to be able to run on lists with O(log n) extra memory, but once again by reusing the specific structure of the list, and it's still not O(1) extra memory
* Insertion sort does use O(1) extra memory with bidirectional iterators but runs in O(n²)

I don't know any comparison sorting algorithm that works in O(n log n) time on bidirectional iterators with O(1) extra memory: sorting algorithms with such properties on random-access iterators (mostly heapsort and friends) need O(1) random-access to provide O(n log n) running time.",1525356664.0
ThePillsburyPlougher,Unsolved problems are usually pretty interesting.,1524875808.0
shani30,Check out this paper on [Sanskrit and Hyper-Turing Computing](https://github.com/shani30/robomo/raw/master/Samskrit%20Samasara.pdf),1524923967.0
Roachmeister,Women,1524873263.0
Hexorg,"Decompilation - given a compiled binary, output a high-level language source code that can be compiled and the resulting binary behaving the same way.",1524924218.0
eiusmod,The top of the stack is the lower addresses.,1524857309.0
jet_heller,You have the logical top of the stack confused with the real top of the address space. ,1524857748.0
cp5184,"As I understand it, there's basically a constantly incrementing program counter, PC.  It's a register that stores the location of the current instruction.  So, say, 1->2->3, then, say, you shift to a different stack starting at 11, so you set the PC register to 11. ",1524857749.0
mcandre,"Successive stack allocation proceeds downwards, negatively, by convention. I don’t know why.",1524861779.0
dvito,"They use a rails application as the core.  Each commerce site then has its own scoped called to things like products, customers, images, everything.

For the actual physical presentation site, it is a liquid templating framework combined with whatever settings the user has set in the database.  They additionally have a ton of fancy javascript things you can leverage for crazier front end features.",1524853540.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1524845582.0
Krypotn,"
> If the graph is unweighted, then I wouldn't exchange the edge weights, but rather go through all vertices of one graph (one by one) and check if there is a vertex with a matching number of edges in the second graph. If there is, I would ""cross it off"" (maybe by setting some bool array element equaling the index of that node equal to false) and then I'd proceed. No relabeling needed. Time complexity O(n2).

Finding isomorphisms is not just about the number of incident edges of a node. Two nodes that are adjacent before the relabeling are required to be adjacent after the relabeling as well. Your proposed algorithm does not make sure that this is true.

",1524829259.0
SEMW,"> If the graph is unweighted, then I wouldn't exchange the edge weights,  but rather go through all vertices of one graph (one by one) and check  if there is a vertex with a matching number of edges in the second  graph. If there is, I would ""cross it off"" (maybe by setting some bool array element equaling the index of that node equal to false) and then I'd proceed. No relabeling needed. Time complexity O(n2).

Even a version of this algorithm that ensures adjacency would still fail, because it'll give false negatives if it happens to make the 'wrong' choice -- it's greedy and it doesn't backtrack. Simple example: consider these two simple, unweighted, undirected graphs

    a-b-c-d-e
    f-g-h-i-j

They are clearly isomorphic.  But imagine your algorithm starts by identifying c with g, on the grounds that they both have two vertices. It will fail to find the isomorphism.",1524829826.0
drvd,"> Why do we relabel the vertices?

That is basically the isomorphism part: You label the vertices of the first graph with the vertex labels from the second graph so that the edges match too. The label here is used just as identity of the vertex.

Regarding the weights: No. Forget about that until you understand isomorphism of undirected graphs (which is hard enough).",1524830578.0
Workaphobia,"Tangent: Can you argue that there is a subfactorial algorithm by showing that it's in NP (non-deterministically choose all relabelings), and therefore has a polynomial reduction to an NP-complete problem that can be done in deterministic exponential time?",1524844810.0
FatIrishPharaoh,I’m failing Algebra one. Good luck.,1524858414.0
Darwinmate,"Doesn't answer your super specific question but can you use R?
https://www.rdocumentation.org/packages/igraph/versions/1.1.2/topics/girth",1524810246.0
djimbob,"This is incredibly un-enlightening article.  It basically says ""Why X?"" and replies because there's a theorem that says ""X"".  I'm not going to name the theorem, prove it, or even intuitively justify it.

It's roughly equivalent to:

>Why can you take any flat planar map and color it using four or fewer colors so that no adjacent regions share a color.

> Because there's a theorem that states:

>>A planar map is a set of pairwise disjoint subsets of the plane, called regions. A simple map is one whose regions are connected open sets. Two regions of a map are adjacent if their respective closures have a common point that is not a corner of the map. A point is a corner of a map if and only if it belongs to the closures of at least three regions. Theorem: The regions of any simple planar map can be colored with only four colors, in such a way that any two adjacent regions have different colors.

> If you want to learn more, look up graph theory and specifically graph coloring.",1524842306.0
moduIo,"If you have a linear function f = Wx + b and you define g = W'x + c then g(f(x)) = W(W'x + c) + b, which can be rewritten as Ux + d, ie. still linear in the parameters.  Quite clearly, this extends to arbitrary nesting of functions.

I'm not sure what the point of this article is, since everyone who has a basic knowledge of deep learning already knows this and for those who don't my example is much clearer to understand by even someone with only high school math.",1524972911.0
jayden8250,What are the “must-take” math courses in college that go hand in hand with CS?  I’m thinking about a CS major and a math minor. ,1524793137.0
Morwenn,"I worked on the the poplar sort algorithm, which is based on creating then sorting a data structure called a *poplar heap*. The algorithm was originally introduced by Coenraad Bron and Wim H. Hesselink in their paper *Smoothsort revisited*.

I had some fun with the algorithm over the latest years and apparently managed to turn the described poplar heap into an implicit data structure and documented the whole thing as well as the improvements. However I'm more of a developer than a computer scientist and I have few questions left unanswered about it, most notably I've got an issue left open about [whether the poplar heap construction method runs in O(n)](https://github.com/Morwenn/poplar-heap/issues/1), or even about whether the algorithm is actually right: even though it never failed any test, it is emprirically derived. I've seen my code used in at least another project, so having any peer review would be welcome before people start using it and witness helplessly crash.

I'm not sure whether it's the right place to ask, yet I would be glad if anyone could provide some help and look at it with the eyes of a computer scientist :)",1524843133.0
,[deleted],1524817157.0
jetlife95,"Undergraduate CS student here. Interested in an AI career, maybe as a researcher. Planning on graduate school, and my interest is in robotics. Any recommendations on how to land a job in AI?",1524801933.0
qaphqaesque,"Hi, I gave an answer / asked for clarification. 

I'm telling you here, because I want karma and not the fake internet points from stack exchange. ",1524719412.0
Workaphobia,"I take it ""has at least"" means ""must have at least""? I'm not familiar with these automata but if it's anything like the standard finite automata or Turing machines then it's trivial to add unused states until they have n! many.",1524710658.0
_--__,http://www.faculty.idc.ac.il/udiboker/automata/wat.html,1524738274.0
slashcom,"Yes, you need a corpus to learn a neural language model. Typical the bottom layer of the neural network is an embeddings layer, and it's extremely common to initialize this layer with pretrained embeddings like fastText vectors.",1524706201.0
incompetentrobot,"You can use a pre-trained model for most applications. You ""just"" need to query that model in a way to accomplish your task, in this case autocompletion. Sounds like you need some kind of optimization problem where you optimize over the possible values for the next word or phrase.",1524706618.0
humor9268,"What: If you think your text is very different than all the available models like google_news_word2vec, wiki_word2vec, etc, then you need to build (train) your own model. Though it is possible, it is worth making sure that you indeed need a new model. A question you should ask is what is the evaluation criteria that makes you think you need a new model? Remember to keep the evaluation criteria as objective as possible.

How: you need text corpus. This means actual data like sentences (may be from documents), and not only words. This is because word2vec, howsoever glory they have gotten, use CBoW or skip-grams behind the scene to reach a final space where similar words are close to each other. If you read the tensorflow word2vec, they explain it in a better way than me.

Practical_How: use gensim, you will need to call few api’s, and you will be done. Dont forget that preprocessing is left to you. In general, other than usual preprocessing, stemming helps a lot. ",1524718159.0
uakbar,"So here's the deal. You're gonna need text if you can't find an already trained model which does exactly what you want ( check if you can find autocomplete word-vec models with a window size of 3. DO NOT use the normal models which consider words both before and after the target within a window ).

In any case, training with your own data which is specific to your problem is highly recommended. You can use the already existing models ( even if it's not the exact model that you need, it'll help your new model to converge faster. Or even if it's a completely different model, say the normal word-vec model. Though you can't be sure that this will help your model to converge faster since the initial embeddings are completely unrelated to the problem at hand ).

All the libraries provide support for initialization of embeddings from previous models instead of random/ one-hot encodings. In my experience gensim is the easiest to work with.",1524723499.0
MKC_K,"Once the current battery limits get resolved, more variety of wearables will be available; so more micro OS that will manage all these networks",1524689268.0
WhackAMoleE,Turning off the computer and going outside.,1524794262.0
GoinRoundTheClock,"My bet is google's work with AI. Maybe this isn't a very specific ""thing"" but its where it would come from. ",1524681338.0
sparcxs,"I’d say right now, it’s the move to serverless. Things like AWS Lambda. The next step beyond microservices. There’s also some interesting movement in programming languages with the plateauing of Moores Law.",1524718309.0
DigitalGeo,"Deep learning A.I

Reading the news, it feels like the hottest topic

It learnt to beat players in the world in GO, which was considered improbable, last year

Then it beat the world's best chess engine, Stockfish, a few months ago, which was considered unbeatable

Now more recently it's beating Dota players in E\-sports

The next big thing for me would be deep learning for cyber security",1525041498.0
fireatx,web assembly,1524870546.0
crabbone,"I think we missed the next big thing in CS in the late 70s (look for Backus's lecture he gave when receiving Turing award).  Since then, it was more downhill than uphill.  There were some interesting findings, but most of mainstream CS is stale / stillborn, and this reflects in Business Tech.

Maybe hardware is more interesting, I really know nothing about it.

On the business front... well... what do I know... If I were optimistic about the future, I'd say ""digital government"", ""digital elementary school education"".  But I'm pessimistic, so I'll say: more lock-in proprietary garbage on IoT devices.  More and tighter control over consumers by media corporations.  Bigger and more pervasive digital monopolies.",1525272625.0
turning_tesseract,And the most exciting thing further down is that it will be Computing 2.0 with Quantum Computing.,1524692747.0
AU_yzy0050,"In the near future, probably, within 5 years, blockchain may dominate at every corner.  ",1524703790.0
redpilledcuck,"This is literally on Wikipedia.

Looks like the arc weights mean how many tokens are consumed if they are from state to transition, and how many tokens are given if they are from transition to state.

Before firing:
https://en.wikipedia.org/wiki/Petri_net#/media/File:Petri_Net_A.jpg

After firing:
https://en.wikipedia.org/wiki/Petri_net#/media/File:Petri_Net_B.jpg",1524656639.0
redpilledcuck,"The weighted arcs have weights on them, while the unweighted ones are unweighted.",1524656456.0
grenadesonfire2,"Once you have the lexemes/tokens you should be able to break down a complex structure down into the steps that would make an if/else set of instuctions in a vm.

For PL/0 or any other learning language they are usually on a stack based vm. You need to keep in mind what needs to be on the stack when. 

Take an if/else statement for example. You have vm ops that represent logical tests. The tokens that are generated from between the if and the then (assuming if expr then statement kind of structure) would be evaluated and placed on the stack before anything else. Then you would generate the if portion in vm op codes and then finally the else statement (if present). Sprinkle in some jumps here and there and you are done.

Each high level instruction will need to have a pattern to accomplish that. High level being for/(do)while and whatever else your instructor has chosen to include in PL/0(many remove parts that might be out of scope depending on your university, some go all the way to classes some stop at loops and such). 

For more help, I recommend reading nand2Tetris.",1524661132.0
Ultralite_Beam,"If you're interested in finding out what a typical CS lecture is like, I'd recommend checking out the MIT or Harvard open courseware lectures on YouTube. 

If you want to know what industry is like, it depends on the company and position but generally programming and architectural concepts are more applicable than CS theory

Also if you want something more digestible, check out the YouTube channel computerphile. It has a lot of really interesting CS topics presented in a fun way. 

Don't get discouraged if you don't know what the hell anyone is talking about. You can learn all that stuff. See if you like the types of problems CS is about and if it's something you can get excited about",1524625849.0
gammison,"In the purest sense, Computer Science is a branch of math, sometimes pure and sometimes applied. Central problems include algorithm analysis, computational complexity, computability theory, and many others. Often other branches of math can mix and mingle with CS in interesting ways. Primarily, I would say CS is about how we formalize and solve problems ( I know this sounds really broad, but it's a broad field). More applied, there are subfields like graphics, operating systems, hardware design, cryptography, programming languages, HCI, machine learning, formal language theory, AI, quantum computing, computational physics, biology, security, etc.  There are too many to name. Coding sometimes is just the end result of a lot of work considered CS. For example, The entirety of C++ exists as a massive book that must be translated into a compiler. Practically, the major can be as programming or theory heavy as you wish, outside of general required theory and math courses. Check out the computerphile and numberphile youtube channels, there are lots of academics contributing from their various subfields. Sorry I don't have one go to video for you, but look around, there's great content all over the place.",1524628160.0
jmite,"Check out the Computerphile videos on YouTube, particularly the more theory heavy ones. Numberphile will give you an idea too.",1524629664.0
HaikusfromBuddha,"If you feel indifferent about computers and dislike math then you'll probably really dislike programming. 

You'll be on computers for hours on end trying to fix issues and half of the time it will be trying to implement a math Algorithim. ",1524629079.0
GNULinuxProgrammer,"> I wouldn't say I'm passionate about math bc I suck at critically thinking and I'm better with just doing questions similar to examples like a monkey see monkey do kind of thing ya feel.

Unfortunately CS requires a lot of mathematical thinking once you leave basic concepts. One should not confuse programming with CS.",1524647698.0
shaggorama,"Back in the mid 80s before personal computers were really a thing, MIT's intro CS course was taught by the authors of a text known fondly as ""The Wizard Book"", aka [The Structure and Interpretation of Computer Programs](http://web.mit.edu/alexmv/6.037/sicp.pdf), or ""SICP"" for short. Their intro CS lectures are available online and although they're not taught using a language anyone would consider particularly useful today (a perl dialect called ""scheme""), they do an absolutely incredible job instilling a passion and enthusiasm for CS in their audience. And consider, the people they were speaking too had way, *way* less exposure to even the concept of a ""computer"" than you. 

I strongly recommend you check out the first lecture, I think it will help you understand of CS is something that might interest you. It might even deceptively instill an interest in you that wasn't already there ;)

[Lecture 1A | MIT 6.001 Structure and Interpretation, 1986](https://www.youtube.com/watch?v=2Op3QLzMgSY)",1524629307.0
nateforpresident,"Ignoring the music [this](https://www.youtube.com/watch?v=BeoCbJPuvSE) video is pretty cool. If you want to think about comparing the different sorts going on and other comparable problems, then CS might be for you!",1524631768.0
not-just-yeti,"One of the necessary skills to CS is understanding innately that every character matters, when writing a program.  Not noticing that ""gague"" gives a red underline suggests that this is a skill yet to be attained...?",1524629455.0
TheDukeofTao,"Hi Group,

Recently launched Mbrane a new combinatorial mobile app game of perfect information that uses a empty Sudoku as its game board. Novel game. We're developing a Game Tree and working on the AI, wanted to know if you had any ideas on how deep the Game Tree would go or any ideas on developing a stronger AI. Thanks for your help and would love to hear your thoughts on the game if you have time.

",1524623833.0
sparr,The team that proved there's no 16-clue single-solution sudoku puzzle came up with a lot of good results regarding deduplicating sudoku positions. You might seek information in their work.,1524629378.0
kc3w,Let's see how long this proof will stand.,1524607288.0
bxtk,"I haven't scored it yet since I'm on mobile, but this sure ticks a lot of boxes on the [crackpot index](http://math.ucr.edu/home/baez/crackpot.html).",1524608919.0
suspiciously_calm,"This is junk, right?",1524607247.0
ATownStomp,"This might be the most ambitious first mathematics paper ever written by a pianist/filmmaker.

Mark, you have to admit how sketchy this looks.",1524607417.0
PM_ME_UR_OBSIDIAN,"Two utterly unambiguous signals of crankery that any late undergraduate student should be able to spot:

1. ""ZFC is inconsistent"" is a claimed result, but it's not the core result, just a stepping stone towards it. Talk about burying the lede.
1. Tagging the paper ""artificial intelligence"" when it has absolutely nothing to do with AI.

Mark: maybe formalize this in Coq and then we'll talk.",1524608633.0
Wurstinator,"It would be nice if you could make this paper self-contained. Maybe all the terms you use are defined in Turing's paper but I don't think many people are keen to read a scientific publication from '36. Some things are not defined at all, like beta prime, others only vaguely, like description numbers and standard descriptions.",1524641522.0
heyandy889,"Is this like a few years ago when those Italian physicists discovered [neutrinos that travel faster than light](https://en.wikipedia.org/wiki/Faster-than-light_neutrino_anomaly)? And the discovery turned out to be erroneous, owing to an incorrect time reference (GPS)?",1524607798.0
p_pistol,Lmao,1524612204.0
_georgesim_,I know this is a joke post but please.... do we realllly need this posted again.,1524616135.0
khanh93,"I think my favorite quote from the abstract is the claim that it ""disproves the SPACE hierarchy theorem"". You can't call something a theorem at the same time that you claim it's false!",1524608364.0
StickyDaydreams,"In an intro to CS class you won't come across any high (or medium) level math. You'll be fine without it. Study hard, repetition is everything when you're just getting started.",1524594457.0
thesia,The first calculus I used in cs was my junior year algorithms class. Before that point you'll want some knowledge of log and sumation/product notation which can be picked up reatively quickly,1524594989.0
hartal87,Intro to Computer Science likely has no math beyond logic.   Your math background is unlikely to matter at all.,1524611485.0
metaphorm,"I think you should take a slightly different approach. Take core Math and Science (calculus 1-3, discrete math, statistics, and at least one of physics/chem/biology lab course) courses at community college to fulfill your major pre-requisites. Wait until you're at the senior college to take the actual CS courses. You can definitely get started on learning how to program while you're still in CC though. There's a ton of good resources on the web for intro programming. 

the main thing you should know is that Computer Science as a subject is a branch of applied mathematics and is a very academic research-oriented discipline. The actual practice of programming is largely independent of that and if your interest is in becoming a professional software developer, you should really consider enrolling at [the lambda school](https://lambdaschool.com/) instead of doing a full CS major. might save you a lot of time and money if you're not interested in the academic side of computer science. ",1524595801.0
Marsmell,"I took a very similar path to you. I did a year at community college and transferred to a university. I'm 2 semesters away from graduating. Every university is different, but my university's computer science degree required 2 semesters of calc. I did algebra and trig while I was at community college and took calc once I transferred. 
In my experience, introductory computer science should not be very math heavy, but I recommend finishing pre-calc before you transfer.

It's really hard to get information before you're accepted into a university but they will talk to you if you try hard enough. If you already know which university you're going to, call them and insist on speaking to an advisor and you can ask them if you're taking the right path.
Also you should be able to find the graduation requirements for the degree online. That should offer some guidance as to what types of classes you should take.",1524604352.0
cjrun,"Your first Algorithms and Data structures will use precalc concepts, but also Discrete Math. If your community college offers Discrete, definitely take that as soon as possible. ",1524611089.0
skiw,"If the class doesn't have Precalc as a prerequisite then you should be fine.  As /u/metaphorm said, Computer Science is really more about applying math to computing, so think about what you really want to do with your career.  That's not to say that you shouldn't pursue computer science, just know that it's about equally as much mathematics as it is programming.",1524596741.0
tylerbmx777,"Great choice! I doubt you'll have any problems taking intro courses without precalc right away. Most likely, they will teach you first about computer basics and transition that into the basics of programming (basic data types, functions, loops, general programming stuff). Math (in the precalc sense) doesn't get too heavy until you start studying algorithms and efficiency. 

/u/metaphorm did bring up a good point about the academic aspect of CS; however, that's the beauty of community college. It will give a chance to get a good taste of where the content is going to go without digging you into a financial hole that will take a decade to get out of.

As for tips, I'd recommend that you do some research into the different types of computer science related fields. Do you want to work on web development and more front end work? Or does the back-end nuts and bolts of how computers work interest you more? From there you can decide if computer science is the best fit or if another path under the general umbrella of IT is right for you. Also, if possible, see if you can get a chance to ask different types of people or professors as many questions as possible about what different paths entail. 

If you take anything away from this, just get your feet wet with programming. Download python or get an IDE like eclipse and learn some Java (maybe start with code academy to get the basics). Ask questions and figure out what it is you want to do in computer science and do NOT discount the lambda school and other such programs as mentioned before. Computer science and the world of IT are pretty unique in the sense that a degree from the best school is not your only option. (Also, I'm currently finishing my junior year of a Math/CS program so feel free to PM me if you have questions)  ",1524599750.0
complicatedorc,"Hey, I did something similar to this. I'm finishing my second semester of CC right now with plans to transfer. Talk to guidance counselors!! They know what you need to do.

But also no, you don't need the maths for an intro class. What would be more beneficial to you is being aware of how programming works. Check out Harvard cs50 and it will probably be tougherz and more informative than any of your CC classes. You'll definitely see connections between your the math and CS classes. (taking precalc/trig currently and I had to take a lower level class last semester and I'm still on track to take Calc 2 at CC.) ",1524602120.0
Pickle_Jr,My intro to compsci class didn't have that high of math. You should be fine but if you're worried talk to your advisor.,1524603495.0
Pyle_Driver,"Contrary to popular opinion, computer science is NOT all about coding languages. Math and Logic are fundamental concepts in CS. Take the time to get up to speed with math and logic. Khan academy is a good online tutor. ",1524607041.0
anthonylenjohn,"Most ""Hello World!"" Programs involve some heavy duty calculus. Switch to markup languages while you still can",1524618391.0
jmite,"Do they say what the class covers? If they're doing any BigO analysis you'll need logs and such from precalculus. But if it's just intro to code then you probably won't need that kind of math. Be prepared that you'll need lots of logical thinking, different from what you've likely encountered before in math.",1524618733.0
GMUsername,I don't think I've ever used any high level math until my junior year CS classes. CSC 200 though? Sounds like you're in the VCCS system. I guess you're either planning to transfer to Mason or VT?,1524618860.0
istarian,"I took calculus 1, 2, and 3 as part of my CS degree... Didn't use it a whole lot. Granted you do probably need it for algorithm time complexity stuff...",1524620123.0
ipsocket,"I never used my discrete math and calculus 1 skills in my higher cs classes. 

Its always good to know.",1524620133.0
sedatesnail,"Some other have already given you some good answers to your specific question. I would like to encourage you to contact the schools you would like to transfer into to confirm that they will accept that class (or any of your classes really) for credit towards a degree. 

Some universities are strict about what classes they will accept and even if they give your credit towards graduation, they may not accept a transfer class as a replacement for an otherwise comparable class. ",1524601742.0
dr_steve_bruel,"http://adrianmejia.com/blog/2018/04/05/big-o-notation-and-time-complexity-to-speed-up-your-algorithms/
The second part is returning 404 page not found ",1524594071.0
spinkelben,"Overall good introduction. You could maybe go into a bit more detail of why the `k`constant is ignored. Maybe a plot of an few linear functions next to an quadratic one, to illustrate that for different values of `k`all the linear functions are kinda the same compared to the quadratic function. 

I really like your table of runtimes. Gives a good perspective on why this matters :)",1524592812.0
,"Well, more importantly than anything else you'll need to pick something you're interested in doing. Otherwise there's no point",1524542570.0
NovelDame,"Can we get more detail? 

If it's a research project, theres always Stuxnet or APT28. ",1524545056.0
_--__,"Hi,

I have removed your post, as per the sidebar this type of post is better suited to other subs such as /r/askcomputerscience.",1524545072.0
n008man3,"I don't do it myself, but looking into debuggers and /r/ReverseEngineering are a good start. /r/asm may be able to help as well if you're having trouble with Assembly.

Oh, and a word of advice, this sub is where the more heady CS people hang. The laymen don't come around here too often. I'd stick with subs more geared towards the practical.",1524535049.0
_--__,"Hi,
I have removed your post, as per the sidebar this type of post is better suited to other subs such as /r/reverseengineering or /r/askcomputerscience.",1524545133.0
WafflHausDildoKiller,"reverse engineering is just staying at home in bed. rather than get up and go to work, and engineer, simply reverse the process. simple. you can paypal me with a reward, thanks",1524538990.0
MarsLanded,"When I took the class, at grad level, it was mostly about algorithms. It covered things such as Google’s BigTable and File System, self organizing systems, and other things like that. It was a very good class for me take considering so much is going to cloud based services. ",1524534856.0
steventhedev,"It depends. Is it an undergrad course? Graduate level? Practical? Theoretical? Mix? Who's teaching?

The material covered can be almost pure algorithms with proof based homework, or it can be a survey of existing systems and how to use them.

Which is more useful depends on you and what you want. If you're struggling in a computer architecture course, this material tends to be very different in style, but it strongly depends on the instructor.

You should ask upperclassmen or grad students who have taken the course. They will probably give you a fair review of the course. If there's a lecturer you respect and enjoy learning from, ask them.",1524542846.0
_--__,"Hi,
I have removed your post, as per the sidebar this sort of post is better suited to other subs such as /r/askcomputerscience or your local university's reddit.",1524545198.0
GoombaJoe,"I would recommend it, I did an independent study on this in college and it is a huge part of the future as we continue to throw more cores at issues.",1524535863.0
sailorcire,"I didn't think it was that hard...I taught myself cluster programming in C and had a class in Java RMI. 

Better question is weather or not you are any good at multithreaded programming.",1524540396.0
covercash2,"it will build on Algorithms. the idea is to study algorithms that work well in parallel. reading up on functional programming will help you understand ""functions without side effects"" which are good practice for concurrent programming. 

ask the professor to send you a syllabus and review some of the concepts and see how you feel.",1524544576.0
,[deleted],1524515247.0
Druid-of-Luhn,"You are best off looking in /r/learnprogramming, or even /r/programming or /r/coding. This sub is more about Computer Science as a field, of which programming is mostly just a tool.",1524520157.0
Lord_NShYH,"In practice, the key to writing clean code is to iteratively remove unnecessary complexity.  Don't be afraid to throw away code; regardless of the author.  After all, that's why we have source control.",1524455238.0
sloggo,"A rule of thumb that's helped me, aside from working to whatever style guide is most appropriate to your work or company, is focusing on reducing the number of if/s and loops in any given function. I believe this is formally referred to as cyclomatic complexity. The more ""forks"" in the path through a function, the harder it is to read. Keep the number down and everything starts looking much neater and less indented, and the cognitive load required to understand a single function stays relatively low.",1524456111.0
ColdDemon388,Code Complete by Steve McConnell is an excellent resource on the topic. ,1524458448.0
AverageTrombone,"""Clean Code"" is an excellent book on this subject by Robert ""Uncle Bob"" Martin.",1524455014.0
pikob,"Start writing unit tests. If you are having trouble doing so, it's pointing to a design problem. Refactor code until you can write unit tests for it. You can do it piece by piece by identifying bits of stand-alone functionality and separating it.

Unit test is on one hand a proof of correctness of code (for a specific input), on the other hand, a proof of well designed unit of code that knows why it exists and what it's supposed to do. It forces you to think about what your code really does and how it couples to the rest of the system.

When you start running into trouble at the IO boundaries, I recommend this talk: https://vimeo.com/80533536.",1524470821.0
cbarrick,Reading code from well known open source projects is a great way to get a feel for clean code. The standard library for your favorite language is a good place to start.,1524455432.0
onthelambda,"IMO the best way to learn is to find senior engineers who will give you code reviews, and code review others. You can read what’s online and what others have shared, and in time you’ll build up your own theory of it.

It’s a hard topic to pin down. There’s a reason so many people talk about it. And it’s all very hard to enforce.

I also think that using functional programming patterns helps a lot, as a lot of the complexity trying to be managed goes away. Of course things like knowing how to reasonably manage a complex series of transformations etc will come in time.

If you feel the senior engineers you work with aren’t getting you there, consider looking for an open source project that has engineers you respect and try contributing.",1524463293.0
brettmjohnson,"I have been a software developer for nearly 45 years, and I honestly have to say my most recent best influences have been [Google's style guides](https://github.com/google/styleguide) and their [test-driven development](https://testing.googleblog.com/2007/01/introducing-testing-on-toilet.html). Every 10 years, I learn something that tells me I know nothing about my profession, and ~~TTD~~ TDD was the latest.",1524470880.0
ddl_smurf,"- Never repeat yourself (DRY) - everything should only ever exist once in your code. It's very useful in itself, and the pursuit of this goal will get you to learn many techniques which offer decoupling, which is a natural ally of making problems smaller - a pretty good compass for ""clean"" code
- Learn other principles such as Rule of Three, SOLID, and [other folklore](https://en.wikipedia.org/wiki/Category:Computer_programming_folklore). Learn the names of design patterns, algorithms and data structures. Knowing the names will help you accelerate the rest of your learning.
- Read a lot of code by others, find some good projects to read, ultimately ""clean"" will always be subjective, so you'll have to get a feel for yourself. Great projects to read if you do C would be ruby MRI, redis or nginx. (These are my personal tastes, I'm sure other coders will disagree with them, and that's fine). Be distrustful of comments and documentation, but don't skip on them either (and please try to make yours trustworthy and succinct).
- Don't be afraid to rework/refactor things, the more you do it (linear), the less you'll need to (exponential).
- Don't use globals, or at least try very hard to use as few as possible.
- Keep functions and classes as short as possible, except for the names of things, keep those as descriptive as possible.
- A shit ton of practice both in reading and writing code. If you ever think you've practiced enough, reconsider that position.
",1524488244.0
abello966,"I think each language has its own preferred ways to write, what we call ""idiomatic"".  Some languages include a formal style guide and most of them contain important tips that serve for clean coding in whatever language

If you know Python, for example, check out PEP8, download some program for checking it automatically and use it regularly, and you'll internalize it for other languages too",1524497626.0
pretty-o-kay,"Funny this thread popped up on this day. I was at the bookstore and I looked at a book called ""Beautiful Code"" which had coding excerpts from a variety coding experts. They went into full detail about how their modules worked, what design decisions led to the code being shaped this way, and why those design decisions are good ones.

The fundamentals I got were (I didn't get through the whole book so bear with me):
* Approach your problems iteratively. Start with the main idea. Break it down into smaller subsections and figure out how to solve those. Develop a cohesive logical understanding of the problem.
* Make your code analogous to your design. Write data structures that represent your data, and algorithms that represent exactly how you're operating on it. The boundaries between your individual problems should be precisely the boundaries between classes, modules, functions, etc.
* Make your code self-explanatory. Not just to inform *what* it's doing, but also *why*. Make your intent clear. Comments should be the last case if your code can't be explained otherwise. The compiler will optimize most things away, so take liberties for readability.

And finally, not mentioned in the book (of what I read) but general guidelines:
* Don't use literal constants. Use variables for everything you can give a name to. Don't call it 3.14, call it Pi. Same with functions where applicable.
* Write with user interface in mind. Organize and name functions so that if some other user were to make use of your code they would be able to figure out how to make it work in their code without consulting documentation. Even if the only user is yourself, it will benefit you.

Lastly,
Do things because they make sense. Not because they're smart, or sophisticated, or 'fast', or because it's what someone else did. Good code is humble code.",1524464269.0
combinatorylogic,Take a look at the Literate Programming approach by D. Knuth. ,1524474208.0
wavy_lines,"Pursuit of ""clean code"" is a waste of time.

That said, check out Casey Muratori's essays on [Compression oriented programming](https://caseymuratori.com/blog_0015)",1524479683.0
fireatx,"look up some styleguides for the languages.

focus on DRY, write helper fns, and comment your code if you ever wanna revisit it",1524467462.0
Workaphobia,Refactor for simplicity without over-generalizing. Write tests. Use source control. Work with other programmers.,1524502234.0
,Code Simplicity by Max Kanat-Alexander has changed my code for the better.,1524507242.0
Marsmell,"I'm currently a third year Computer Science major and There's a lot of emphasis on good documentation, commenting, and naming variables in a way that it can make sense to someone who knows nothing about your code before reading it.

Mostly for clean code, my courses have emphasized Google's style guides and test driven development. Someone already commented about these with links. The biggest emphasis is to limit function length and complexity. We use a service called code climate which I believe is paid but there are free alternatives. It analysizes your code and warns you if any functions are longer than 25 lines or too complex.",1524512880.0
oridb,"I'd strongly recommend The Practice of Programming. It's about structuring code cleanly and readably in general. It's something I wish most programmers would read.

https://www.amazon.com/Practice-Programming-Addison-Wesley-Professional-Computing/dp/020161586X

It's full of great advice, and is a fairly light read.",1524520590.0
learntechspeak,"https://medium.com/mindorks/how-to-write-clean-code-lessons-learnt-from-the-clean-code-robert-c-martin-9ffc7aef870c

Also, there's a book named Clean Code which seems relevant. ",1524563561.0
t_bptm,"I try to write code that doesn't need comments to be able to be easily read. This doesn't mean don't write comments, but good naming using conventional constructs improves things. 

What I've found useful is to write something moderately tiny, like a 1000 line program, then come back to it a month or two later and refactor anything that doesn't make immediate sense. Do this a few times and you can develop an intuition for what sort of things are mistakes against simplicity. 

",1524584008.0
phat_sumo,"My main tip would be to keep a consistent style. I personally always use 2-space K&R with an 80-char limit, but it doesn't matter really, as long as you're absolutely anal about keeping that style. ",1524493811.0
vwibrasivat,"Take a course in Object Oriented Programming, ( OOP ).

Learn the Entourage of ""design patterns"".

Learn what ""anti-pattern"" is.

Familiarize with some ""code smells"".



",1524458828.0
cyancynic,The best way to get good at writing good code is to read lots of good code. ,1524499755.0
dan994,Clean Code by Robert Martin is the book for you,1524509278.0
Scuba_Von_Wolfgang,"Clean Code is a book written by Robert C Martin (a.k.a. uncle bob) and talks about all parts of what clean code means.... you dont have to write code just like he says, but boy does he know what he is talking about.",1524509882.0
biagginis,"In addition to Clean Code"", which has already been mentioned here, I would recommend ""The Practice of Programming"" by Pike and Kernighan.",1524480531.0
skulgnome,"As a beginner, the only things you can do are to study existing styles, and to conduct your own experiments.",1524496658.0
britishbanana,"Read books, and digest them. 
Clean code
the clean coder
Code complete
Design patterns
Anti patterns
",1524512441.0
Ijc-90,Clean code book,1524496499.0
TheDulcimer,"Testing randomness is difficult - if I recall correctly the second book in The Art of Computer Programming deals largely with randomness and might be of interest to you. When we are talking about random number generators though we specifically are trying to get two big properties:

1. We would like the distribution to be uniform across the range of numbers we are testing
2. We would like the probability of any number being produced to be as independent as possible in regards to previous numbers in the series.

If we have a nonrandom seed then we are not going to be able to perfectly generate random numbers (with enough observations and computation time we could predict what the next 'random' number would be), however, we can make it so that it requires a great deal of observations and/or a great deal of computation time.

Randomness is a big field and highly connected with number theory, cryptography, graph theory, and statistics - if its interesting to you I highly recommend TAoCP book two.",1524517269.0
hamtaroismyhomie,Can you be more specific?,1524497468.0
drWeetabix,Generally it means you can do the same computations that you can on a Turing machine.,1524438735.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1524426450.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1524415480.0
_--__,"Hi,
I have removed your post from /r/compsci - as per the sidebar this sort of post is better suited to other subreddits such as /r/askcomputerscience or your university's pages.",1524545286.0
callmederp,"What have you tried to get the results in the main window? It looks like you are just using a pop up message window, which will always be it's own window.",1524404948.0
michaeljhunter0,How can I get the results from the pop up to be intergraded into the main_window. I want to get rid of the pop up entirely and just have everything display on the main window with the user input for the home along with the results.  ,1524405273.0
michaeljhunter0,https://gist.github.com/michaeljhunter0/702256ec2575e4eba2e9d11f67a07893,1524405434.0
michaeljhunter0,"https://gist.github.com/michaeljhunter0/702256ec2575e4eba2e9d11f67a07893

I'm trying to get rid of the pop up and just have everything intergraded into the main_window.

Example 

Please enter home price [                           ]

The assessed value of the home is: $________

The property tax of the home is :      $________

[Calculate][Quit]",1524405620.0
uakbar,"I don't know of any specific libraries, but this should be pretty straightforward to do in native Python (+ numpy and matplotlib of course).

Just feed everything to your model and determine if the result is a TP, FP, TN or FN by comparing against your data set. Then just calculate the Precision = TP/TP+FP and Recall = TP/TP+FN. Do this for all possible model parameters, and voila! You'll have a precision-recall curve when you plot the two against each other.

This shouldn't take you more than an hour or so (unless I completely misunderstood what you are trying to do. In which case, just ignore my comment :P ).",1524401920.0
cookingmonster,http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html,1524420710.0
_--__,"This question really belongs in /r/askcomputerscience, which is why I have removed it.  However, I will give an answer, and you can choose to post there if you want other opinions.

You have a walk from vi to vj labelled by w.  Suppose we consider a (w-labelled) walk which uses the least number of &lambda;-transitions.  What is the longest such a walk could be?  Suppose there are more than &Lambda; &lambda;-transitions between two non-&lambda;-transitions.  But since &Lambda; is the total number of &lambda;-transitions available to us, this means that there must be at least one &lambda;-transition that we have done twice.  That is, we have completed an entire cycle using only &lambda;-transitions.  We will not change the label of the entire walk by removing this cycle and doing so will decrease the total number of &lambda;-transitions - contradicting our assumption that the walk used the least number of &lambda;-transitions.  Thus there are at most &Lambda; &lambda;-transitions between any two non-&lambda;-transitions - for accounting purposes, let us count them as occurring ""after"" a non-&lambda;-transition.  Adding in the (at most) &Lambda; &lambda;-transitions possible before the first labelled transition and the |w| non-&lambda;-transitions means that we have at most &Lambda; + (1+&Lambda;)|w| transitions in our walk.

As for your second question, what do you mean by the extended transition function?",1524541610.0
SOberhoff,"The book has been mocked repeatedly by various experts. Here's a quote from Scott Aaronson:
>“The impact of NKS on all the areas of computer science and physics I’m familiar with has been basically zero,” he says. “As far as I can tell, the main impact is that people now sometimes use the adjective ‘Wolframian’ to describe breathtaking claims for the trivial or well-known.”",1524369266.0
l_lecrup,"I read a review that said ""what's true ain't new and what's new ain't true.""

He has made genuine contributions to physics and computer science, and he is clearly not a stupid man. But it is quite telling that he opted not to submit research papers for peer review and have them published in traditional venues (for the most part) and instead self publish his work.",1524377932.0
pingpong,"IMO, the only interesting bit is where it asserts that rule 110 is Turing-complete. Wolfram does not get credit for that, though—It was a product of Matthew Cook's research while he worked for Wolfram Research (he proved it in a conference before the book came out, which Wolfram also got pissed about for taking credit).

Correction: Cook presented his research before ANKOS came out, not after.",1524370515.0
celerym,"The assumption that nature is discrete isn't particularly outlandish, I think the quote completely missed the mark. NKS makes lofty claims as to its significance but offers nothing particularly predictive or testable. The quote here is an attempt to psychoanalyse W, simple mud slinging. The example with the carpenter thinking the moon is made of wood applies to every physicist resting on the work of those before them, the techniques and patterns of thought they were taught.",1524376343.0
,"it should have been called a ""A Kind of science"". I don't think there's anything post 60's in it.",1524388945.0
sixtysecondsensation,"**tl;dr** NKS is bullshit in scholarly circles because it's not a scholarly book. That doesn't mean that it's full of lies (everything in the book is reproducible), or that it's not an interesting read for amateurs and hobbyists. Much of NKS's criticism seems to be due to the fact that NKS or Wolfram don't adhere to ""academic-style"" science.

NKS isn't a scholarly book. It's not supposed to be read by experts who are trying to glean something significant from it. It can be read by a layman. There's much embellishment, but it's all grounded in a solid basis of computation and discrete systems. It's BS in the same way that you would BS quantum physics research to your congressman - if every sentence doesn't start with ""This revolutionary thing..."", they won't pay attention because they're not quantum physicists. NKS would be useless to a computer science PhD, but is a worthwhile read for a hobbyist looking for something interesting to play with.

The bottom line is that discrete systems are useful. There's a reason scientists use them. Continuous and analytical solutions get *really* sucky as soon as there's any sort of complexity introduced. If you're trying to solve a multi-body problem, you plug that shit into a discrete model and get a solution that works well enough. Nature exhibits discrete behaviour frequently (a simple example is the ubiquity of the Fibonacci sequence, DNA, or patterns of crystallization). Saying that Wolfram is a fraud because he uses discrete models isn't an argument.

Yes, Wolfram is full of himself and he comes across as arrogant almost to the point of assholery, but considering his body of work and the fact that he's legitimately trying to make his passion known to the world, I'm willing to overlook his vices. Wolfram Research has done a godly amount of work. Mathematica is a bloody lifesaver in my research. NKS makes *qualitative* observations, some of which, while not very applicable, novel, or relevant, are fascinating to read about. I've done some work implementing such systems and it's fun to see how closely they mirror nature.

Not all scientists choose the road to academia, and it's silly to think that if one isn't trying to prove P == NP then one is a ""lapsed"" scientist. This is called ""gatekeeping"", and it's unhealthy.",1524406426.0
RumbuncTheRadiant,"Are the various cellular automata he demonstrates bullshit?

Obviously not, you can trivially replicate the basic steps with pen and pencil.

Are they sort fascinating? Yup.

Will they supplant say the equations of Quantum Electro Dynamics? No evidence for that as yet.

Could you calculate various scenarios handled by Physics partial differential equations of your choice using cellular automata. Very likely.

Will it be an advantage? Will it lead to deeper insights? I think the jury is still out, no signs it will, no proof it won't.

Is this stuff fun and intriguing.

Yup. Definitely.

Is Stephen Wolfram the Best Thing since Sliced bread. Nah. But he's having fun, doing stuff he finds fun and you might find fun, so what's your problem? Let him have fun and you can have fun too. 

Try not to be an academic sour puss and play a little.",1524440656.0
,"Read it as a creative exercise or a thought experiment. It's leisurely, and if you're in the right mood, you can take a bit of aesthetic inspiration from it (like GEB).",1524374729.0
Deadly_Mindbeam,Rule 30 is good for generating bits of pseudo-randomness in parallel hardware. I think that's the highest impact discovery.,1524399667.0
hilberteffect,Wolfram is a pompous hack. Nothing more.,1524435563.0
markth_wi,"Ok. I think it's very fair to say that While Mr. Wolfram is extremely smart, autodidact and by many measures a genius, that said, as far as his book, I found reading it was hard. 

It's VERY important to understand that if I recall correctly, he self published the thing with no editing - which is mistake number one, because it's not clear **any** editorial hand was present. So it's random and tangential (and not in a good way).

[Metamagical Themas](https://www.amazon.com/Metamagical-Themas-Questing-Essence-Pattern/dp/0465045669), is a massive contrast here. I read/reread the ever loving shit out of that book over the years. Hofstadter makes tangents and jokes and in-references every 5 pages for 900 pages, and almost every inch of it is awesome. It doesn't even age particularly well, but it's eminently readable.

It's less a disaster of intent and more a cautionary tale to other similarly smart people who don't think they need editors; Hint, if you don't think you need an editor, you most definitely do.

Here anything by Nassim Taleb or David Foster Wallace's [Infinite Jest](https://en.wikipedia.org/wiki/Infinite_Jest) jump to mind here, as being the guys most desperately in need being sat down and beaten with their books until they are a whole lot more approachable or just less painful to readers.

I happened to be studying genetic algorithms around the same time as this came out. Being a fan of Mathematica I figured what the hell and started reading. 

About 30 pages in I'm realizing that he REALLY needs an editor. 
About 300 pages in I'm struggling, he's covering some of the same material over and over again , worse he's making a series of unfounded conjectures...and trying to make it all stick together, by about 600 pages in , I decide to call it a day. 

I think that while it's possible that Mr. Wolfram's book could be edited and reduced down to about 150-200 pages if you wanted to. Perhaps **perhaps** if we're being charitable, break it into two books;

- A book containing the more rigorous work around genetic algorithms, automata and various structures and methods.  

- A second book on his various conjectures and speculations might actually be interesting in it's own right 

- SWEET JESUS  - hire an editing team, as they say the first thing you do in writing is kill your darlings, Mr. Wolfram has almost nothing but. 

By way of comparison and contrast, and If you're interested in the subject (although my recommendation is probably from the stone age), may I recommend Melanie Mitchell's [""An Introduction to Genetic Algorithms""](http://cognet.mit.edu/book/introduction-to-genetic-algorithms) which was/is MUCH better at communicating basic ideas such that with almost no code to speak of, Prof. Mitchell gets you in the position where you've got a sense of how you might tackle simple automata problems using GA's. No wild speculations or unsupported suppositions, just straight up build up to explaining a fascinating subject. I tend to think of it as a single-subject textbook in a way.

It's far more valuable to me than Wolfram's book, and while I may have opened it from time to time, after a while I realized I was doing a good amount of side-projects and would refer back to different ideas presented in Mitchell's book so now I have two copies of Michell's work; 

One with dog-ears and doodles/notes from little experiments I did and another copy that sits next to Wolfram - much less abused than my ""work"" copy of GA's.

**TL/DR;** - While it's a fascinating subject, Wolfram is victim to not having an editor tear his draft apart before it saw the light of day. ",1524451958.0
keten,"Well assuming the universe is discrete - does it even matter? If the discrete components are small enough then a continuous representation would likely yield a good approximation, and continuous problems tend to be easier to solve than discrete ones imo.",1524369447.0
HellAintHalfFull,"IMO it is a worthwhile read, deeply tainted by the author’s arrogance (including the book’s title). ",1524414367.0
umib0zu,"I'm sure you can get from everyone here that Wolfram's book is useless for academics. I'd recommend [Think Complexity](http://greenteapress.com/wp/think-complexity-2e/) if you want to read about ideas related to complexity theory. It's free, and not 1000 pages.",1524414385.0
kotrenn,Some of it works as a nice light encyclopedia of interesting automaton and other rule systems.  And exploring those worlds is where the book shines.  It then falls apart when tying it into the real world.,1524405567.0
fringe-otc,"I don’t know, it had a couple new things that are published in other papers and it’s a lot of fun.  

If you ignore the narcissism is it all that different from “The Fractal Geometry of Nature?” Would you say that book is bullshit?",1524409628.0
motionSymmetry,"do the work yourself.  if it's still available for free, download it.  remove everything that references or self-references the ""NKS"".  this will leave you with the middle of the book, a couple of chapters if i recall, and that has substance - just not here-look-this-is-the-universe substance.  tl;dr, the computational ideas are good for computational purposes, if you are a computational person.  do not read great philosophy into it

(dr who would not find this in the tardis manual, eg)",1524444134.0
ianwold,"Yes, it's BS. Wolfram is full of himself.",1524378017.0
Ronald_Reagan_AMA,Relevant https://xkcd.com/793/,1524412249.0
,[deleted],1524368907.0
eigenman,Yes.,1524372968.0
crabbone,"I've heard this as an anecdote, not sure how much of it is true.

Once when Lisp was not only relevant, but also used for trivial and practical tasks, there were a bunch of mathematicians / programmers who held themselves in high esteem because they worked on Macsyma, a symbolic math system.  I think it started at MIT, but there's Wikipedia for that kind of trivia.

Macsyma was developed in Common Lisp, I believe, or some close relative of.  At that time Wolfram was an undegrad student, who had only few years exposure to CS.  Perhaps even less than that.  Someone from the demigods of Macsyma recalled a conversation he had with Wolfram, where the later said something to the effect of ""Lisp, in principle, cannot beat C in terms of speed"".  The demigod felt insulted, and must have responded accordingly.

Anyways, Wolfram didn't know what he was talking about at the time, but his elitist and shortsighted environment made it even worse.  If you ask me, it's very easy to be pissed by academics, who are full of themselves, but are really nothing but a bunch of squabbling hypocritical nobodies.  So, no wonder he and his reviewers have very different takes on his book.

But, frankly, having read a bunch of papers and books published by respected members of academic community in equally respected journals...  I don't see why NKS should be deemed bullshit.  There's so much peer-reviewed bullshit of even lesser quality (in particular in CS) published every day, that the overall situation makes NKS a very decent book in comparison.",1524408839.0
vznvzn,"contrarian opinion, hope it wont be voted into oblivion for a well intentioned stab at/ glimmer of honesty. think he is a real life living legend genius, and few of the rabble would be able to truly appreciate his work, possibly ever in his own lifetime. there are multiple/ numerous reasons that wolfram is dead-on correct in his ideas about ""a new kind of science"". it just wasnt phrased in terminology that has arisen later. seen in retrospect & correct context, **paradigm shifting**

* data driven/ big data
* algorithmic
* about the boundaries with the computable vs uncomputable, etc...
* statistical analysis (aka _machine learning_)
* empirical computation, experimental analysis of complex systems
* digital physics
* complex systems
* _emergence_
* physics metaphors/ concepts outside of the field
* _etc!_

its just that these are decades-long trends/ revolutions and we are still in the early stages of several. the critics miss the forest for the trees. yes, it may be that he is not the leader of all these fields, but he correctly ascertained/ sensed the importance long before others. so, in a word, farseeing, **visionary**

alas, also, **easy to mock by the impatient/ shortsighted**. more recent findings on the general aligned trends introduced/ called out/ focused on in the book

https://vzn1.wordpress.com/2016/01/22/undecidability-the-ultimate-challenge/

https://vzn1.wordpress.com/2017/03/31/universe-as-a-simulation-hypothesis-holographic-principle-recent-developments-notes-minisurvey/

https://vzn1.wordpress.com/2015/06/01/tribute-celebration-of-the-algorithmic-lens/",1524429529.0
w0rk_r3dd1t,"Yes.  Yes, it is.",1524405571.0
floridawhiteguy,"Time is a tool for measuring rate of change, not a thing to be measured in and of itself. Time does not flow, nor is it reversible. Time is not a dimension.
",1524418957.0
StickyDaydreams,C++ but it shouldn't really matter,1524347528.0
rev_mojo,"Java and C++ are very closely related. C++ is the older system, but probably less used these days. Though both Java and C++ are still in commercial use. Both are great for learning imperative style programming, and the lessons learned are easily transferrable to dozens of other imperative languages. If you're learning C++ in a .Net environment, then you get the added benefit of learning the .Net ecosystem.",1524348230.0
sailorcire,"Get the CEH book or Security+ if you want a job

Want something deeper? Read papers in SigSac.

Also look at 2600 and Defcon",1524260422.0
Nerdlinger,"Oh man… the thing with this field is that it is sooooooo wide that there are so many different entry points to look at and different niches within it to do it as a career. I can put together a list of stuff you may want to start looking at, but I really recommend you start attending in-person events and meeting some of the people in the field. See what they’re doing, get an idea of what areas interest you, and, of course, talk with some people.

It looks like you’re in Southern California, so if you can swing it, you may want to start with something like the upcoming [Layer One conference](https://www.layerone.org/registration/). Hitting up nearby [BSides conferences](http://www.securitybsides.com/w/page/12194156/FrontPage) is also a good idea. There are also a few security meet ups per month in larger cities, you can learn about them at these conference or just dig a bit online.

Let me know if you have any questions.",1524261662.0
xShadowProclamationx,look at owasp’s site the top 10 are a great starting point. also cybrary for video courses (free). and when you want to attempt to do hands on got to vulnhub.com to download vm’s to practice with. ,1524316545.0
tyroneslothtrop,Did you spell check that subreddit name before you decided to pull the trigger?,1524262224.0
skelterjohn,r/programming,1524256664.0
WrongNk0ding,"True, Swing is pretty outdated and JFX won't be part of future JAVA Releases. I'm using Spring Boot a lot. Write the Server in JAVA and the GUI in HTML5/CSS/JS, connect em via REST.

https://spring.io/guides/gs/spring-boot/
",1524258970.0
n008man3,"Not in industry, but SceneBuilder is a good tool for creating decent looking JavaFX scenes.",1524260763.0
R4p354uc3,"One of the things that makes computer science so fascinating to me is very similar to the reason I love physics. 

Physics involves understanding how the universe works, going deep into the *how* and *why* of it all. And I believe the primary goal of physics is to develop and prove a theory of everything: the base set of laws upon which the entire structure and behavior of the universe operate. With each discovery, we're peeling off layers of abstraction to try to get to that core knowledge, after which there are no more layers to peel off. 

Computer science is fundamentally the opposite. It involves not ""how the universe works"" but how we work the universe. And rather than peeling off layers of abstraction, we *apply* layers of abstraction to create systems that operate under their own isolated sets of rules. When you look at it that way, we are creating our own universes. Some are very rudimentary: a program might define a ""universe"" whose lifetime occupies a few milliseconds and involves adding two input numbers and outputting the sum. Others are far more complex: simulations and even things like video games that might *actually define* entire worlds with landscapes and physics of their own. Eventually, we might be able to perfectly simulate a universe just like our own, using our own implementation of the theory of everything. And at that point it all comes full circle.

But underneath all of those man-made layers of abstraction are real world physics, and we use those physics to create these systems. Perhaps our own universe is running on some engineer's desktop that he accidentally left running over the weekend. 

So I see computer science and physics as two ends of a spectrum of understanding. I hope that if nothing, that gave you something to think about.",1524258467.0
,"I have three thoughts:

1) Complexity theory (Reductions, P and NP, SAT, ...) is IMHO the closest you will ever get to describing ""nature"" in computer science - and I find it utterly fascinating: No matter what approach, what cool shortcut you imagine and think through - you always hit the wall with problems in NP. You can bet money on it: Think of an well-etablished or esotric way of computationally solving a problem, and out of nowhere some complication will arise that causes you algorithm to run longer than polynomial time. Every. Time. That feels like nature to me. Similar to how one is unable to construct a perpetuum mobile.

In terms of what I think what you mean by ""beauty"" and ""nature"", I think complexity theory is a very approachable way.

2) You like underlying structures? Check out the [computational trinitarism](https://existentialtype.wordpress.com/2011/03/27/the-holy-trinity/) - logic, types and program are three manifestations of the same object.

3) You like even deeper underlying structures? Check out [Category theory for programmers](https://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/). CT is where programming touches dynamical systems and string theory. It is very abstract, and has very beautiful results.",1524255803.0
,"I guess it really just comes down to personal preference. I personally really did not like physics in the slightest, so maybe CS just isn't up your alley.

But to answer your question: I think one of the major factors of why computer science is fascinating to me is that there are *so many* things working together to create what we commonly know today. Like almost an overwhelming amount of different things happening. Let's say you want to connect to reddit. If we broke down every level of technology involved in actually doing something as simple as connecting to a website, it would be an absolutely massive list.

You need a web browser which is coded with languages a, b, and c. The code for the browser was compiled with compiler x, interpreted with parser i. Converted to assembly language by assembler y. For a specific processor z. Which has a cache of size d, and registers of size e, with 2 billion transistors working together. And when you run the browser, it creates a service/process on operating system f. Which then creates a process control block in memory for the process. And has to follow some noncontiguous memory allocation algorithm. And when the process is run, it needs to use temporal and spatial prediction in order to pull the correct segments into cache at the correct time. Then the processor needs to pull segments into registers and schedule execution of commands based on whatever algorithm it chooses to use, round robin, fcfs, sjf, priority, etc. 

And that was just a very simplistic breakdown of the process to just run the *web browser.* That's not even discussing the server hardware, website code, networking hardware, ports, DNS servers, networks, security, encryption, database, server-side algorithms, APIs, authentication, graphical user interface, operating system, kernel, local authentication, boot-up, I/O devices, etc. that all play a part in this seemingly simple connection to reddit.com

Computer Science is a very densely packed field with a seemingly infinite number of layers to literally anything you do on a computer. Some parts are really boring. Some parts are really interesting.
",1524255905.0
crabbone,"If you are looking for inspiration, the Christmas Tree lecture by Donald Knuth was the one I found inspirational. I'm not sure if that's the one: https://youtu.be/BxQw4CdxLr8 I think he did more than one.

As for beauty... I don't believe it is universal.  It requires that the observer understands the language.  When I was little, I hated all modern art indiscriminately.  When I grew up, I went to study in art academy.  Learning art helped me realize that Picasso was a marketing fraud, but Munch was real, that Kandinsky had a bunch of good ideas, but didn't express them well (his book is much better than his paintings), while Mondrian was very honest and real, but had no clue about what he was doing and so on.

I think that Mathematician's Apology is the book, that does justice to the ""A"" in BA / MA you can get from a university when you study math.  I cannot speak for the science-y side of computer science this isn't really the part that fascinates me.  But, speaking about the math-y part, I think that the proof of the fact that it is necessary for the comparison-based sorting to be at least O(n log n) by means of model-theoretic games is... well, maybe not as fascinating as a proof of infinitude of primes, but I still think it's beautiful.  This is not the way I learned the proof: https://www.cs.cmu.edu/~avrim/451f11/lectures/lect0913.pdf but it's the same idea.",1524257969.0
grandzooby,"One of my favorite books is Gary Flakes', The Computational Beauty of Nature:  https://mitpress.mit.edu/books/computational-beauty-nature.",1524262841.0
rolata,"I think you can explore some aspects of game theory, information theory with a good knowledge of computer science. These areas to me, are always very awe-inspiring. ",1524254884.0
sPOUStEe,"One thing to keep in mind is the natural vs man-made distinction. It seems like part of your awe at physics is the order and patterns in the randomness of nature. There are no parallels to this in CS. It's only what we designed it to be.

All that said, I'm continually amazed at the intelligence of people in the top end of the field, and with what computing enables you to do. Think about how much of your modern, comfortable existence is made possible / continually cheaper / better by humble 1's and 0's. That might be your closest substitute given the metaphysical constraints of the phenomena you're talking about.",1524256322.0
dontwaitesforme,"Read ""Godel, Escher, Bach""",1524254873.0
lostchicken,"https://www.amazon.com/Structure-Interpretation-Computer-Programs-Engineering/dp/0262510871

SICP, and Lisp/Scheme programming in general, got me to really appreciate computing on a mathematically elegant level. Read the first couple of chapters (http://web.mit.edu/alexmv/6.037/sicp.pdf, free PDF!) and you'll be hooked.",1524263107.0
Overtime_Lurker,"It does sound pretty boring the way you describe it. You're a physics major, appreciate the physics behind it. Pretty much everything in computer science comes down to instructions encoded with 0s and 1s running through a processor. You might be running a script, you might be running a virtual machine, compiling something, running an instance of a game world on a server...but in the end, it's all just voltages and flipped bits.

Try writing some physics simulations (I use OpenGL and HTML5 canvas elements but I'm sure there's higher level stuff that's better suited). It's crazy how simple it is to simulate things from the real world. All you need is a position, velocity, and acceleration, and you can look at orbital mechanics. To orbit an object around a point, just find the direction to the point with (center point - object position) / distance between them. Make that your acceleration and increment the velocity by it every frame, and increment the position by that velocity. Or make a bouncing ball with the same idea, but make the acceleration a constant (0,-0.1,0) for gravity. Then make some ""walls"" and ""floors"" by inverting the velocity every time the object's position + velocity is greater or less than some value to get collision. I was amazed how good the simulations looked with such simple mechanics behind them. The bouncing ball didn't even have a shadow and my brain was like ""yep, that's a bouncing ball."" For the orbit simulation, I didn't need cosine and sine values to rotate the object. It was just gravity constantly accelerating the object towards the center of the orbit, how it happens in real life. You might just increase your appreciation of physics through computer science, or at the very least power through it to explore and appreciate physics further.",1524257232.0
DashAnimal,"I know this is a pretty common recommendation and you've probably already heard of it 
 or even read it, but can I recommend the book [Code: The Hidden Language of Computer Hardware and Software](https://www.amazon.com/dp/B00JDMPOK2/ref=dp-kindle-redirect?_encoding=UTF8&btkr=1)? I think having a history of how we got to where we are today (written in an entertaining way) is a good starting point, even though it barely scratches the surface of computer science.",1524255854.0
xxkid123,"Hey, I'm a CS undergrad who's also doubling in Physics. 

Here are two things that I found interesting on the more software side. They don't have much to do with physics, but perhaps our mutual interests align:

* Sorting algorithms: our method of sorting things manually is pretty inefficient when applied to a computer. We use something like an insertion sort, where we find the lowest number in a group, write it down, then find the next lowest number, etc. 

* Threading + locks. This is slightly more similar to Physics, because you have to deal with the underlying details and the higher level what-are-you-trying-to-achieve. If you haven't before, I would read up on this subject. 

Other things: quantum computing, operating systems, computer system fundamentals, hardware, etc. In general I take more interest in lower level courses that explains how computers work, than straight programming courses. Algorithms sits in between, the puzzle solving aspect is fun but not as interesting as operating systems.",1524256659.0
Darkfeign,"The computer science you will do as an undergraduate will mostly be to supplement your physics by endowing you with computer literacy. It will of course be hugely beneficial for you to develop experiments and testing environments/simulations on a computer to be used in physics later on.

But computer science has its own areas of interest, some relate directly to problem solving, as others have mentioned e.g. the complexity classes like P, NP etc.

Then there's artificial intelligence, a huge portion of computer science now.",1524268424.0
combinatorylogic,"Read about the Algorithmic Information Theory (Kolmogorov, Chaitin). This stuff is more fundamental than anything you'll ever find in physics. I wandered into CS from my area - particle physics - exactly because I was looking for answers, for the most fundamental notation that would allow to explain how *any* theory in natural sciences is constructed. And if there is an answer to such a question, it is somewhere in CS. The very notion of computability is fundamental, powerful and beautiful. And it's beauty is in its ubiquitousness - so many unexpected things in the natural world turn out to be Turing-complete.",1524427467.0
AU_yzy0050,"In my mind, if you are fascinated more by physics than computer science, then go ahead and do more stuffs with physics. No need to know why you don't want to do CS. Personality differs for each person. It is wrong to doubt yourself only because CS is hot and everybody seems keen on it. It is more important to find your own interest. 

For my case, I was studying EE when I was a freshman, and I became bored with all those theoretical stuffs (actually maybe not theoretical) soon. Only by chance, I happened to have some connections with computer science. It was like I was suddenly sparked. CS is more like something can putting things together and make them into real product you really desire. 

It might be a good idea to change your goal to be a physicist at this moment. If you don't like it later on, it is fine for you to change your goal again. Finally, you're gonna find a thing that makes your life happy and meaningful. 
",1524258278.0
shponglespore,"I'm kind of wondering why you see it as a problem. Surely you're not beating yourself up over your lack of interest in microbiology or economics. IMHO you should focus on physics and treat computer science as a tool for doing physics. If you get to a point where you feel like a lack of computer science knowledge is holding you back, you'll be a lot more motivated to study it, and hopefully the problems you encounter will help guide you to an area of computer science that feels more relevant to your interests. And if that never happens, so what? It's not like anyone is gonna look down on you for being ""just"" a physicist.",1524278004.0
thebyteman,"1) Find a problem which you can solve with CS.
2) Automate a daily task in your life to free up more time indefinitely to invest in other things you love. 
Those are the two things that got me hooked on CS.",1524283700.0
ianwold,"I find CS interesting because I find the broader systems in math, computation, logic, etc to be interesting. Formal systems in general are really cool not because they're super deep but because they give us a great amount of power to express more abstract ideas. That might not be your cup of tea, but I've got a sort of an engineer's mind, I suppose. In fact, I actually have a higher liking if the subjects I find dry because that means I understand it better and I'm able to use it for something.

I kinda ranted for a bit but hopefully I said something that gives you some help! Best regards and good luck with it all.",1524286584.0
ssjskipp,"I've always enjoyed computer generated art, and things like shaders are an interesting way to think in parallel, with math, etc.

[I enjoy the book of shaders](https://thebookofshaders.com/00/)",1524286327.0
ryani,"Instead of answering you, since you're already in a CS major, let me give you a couple homework problems that you might find interesting, that might allow you to find the beauty for yourself.

1. Write a program that outputs its own text.  The program needs to be self-contained; opening an external file that contains the program text and printing it is not allowed.
2. Design a small programming language and write an interpreter for it in two languages.  The first language should be whatever language you are most comfortable using.  The second should be the new language itself.
3. (Bonus for 2): How many times can you run the interpreter inside itself before your computer runs out of memory?

Hints:

For 1: Try to solve this for a while, but if you absolutely give up, google ""quine"" and you'll find a bunch of solutions that usually all riff off of one idea.

For 2: If you can't come up with a language on your own, [a simple LISP](https://www.reddit.com/r/lisp/comments/1fj0qf/lisp_vs_haskell/caarko8/) is not too hard to implement, especially in a language like C#, Java, or Javascript that already has garbage collection, and can interpret itself pretty easily.  Or, if you want to go even more primitive, a stack-language like FORTH is very easy to implement in any language (including itself, if it's featureful enough!)",1524288368.0
fwwwn,"Lots of good answers already, but I thought I'd share mine too.

If your interest leans towards the theoretical side of computer science, then I highly recommend reading [Introduction to Computer Theory by Daniel I. A. Cohen](https://www.wiley.com/en-us/Introduction+to+Computer+Theory%2C+2nd+Edition-p-9780471137726). It's written in a way that's very easy to read, and one of the few texts during my studies that I thoroughly enjoyed. It covers everything from language theory to regular expressions to automata to Turing machines.

On the other hand, if you are interested in the practical side perhaps, then I also highly recommend [The Elements of Computing Systems by Noam Nisan and Shimon Schocken](https://mitpress.mit.edu/books/elements-computing-systems). It's the book behind the well known Nand2Tetris course, and will give you a basic but adequate overview of the entire hardware/software stack of modern computers.

Both of these texts will give you a good look at some aspects of computer science, and hopefully you'll find something that you appreciate.",1524294875.0
Vegerot,"I would recommend Stephen Wolfram's book ""A New Kind of Science"".  He is a theoretical physicist and a very famous software developer who wrote an incredibly profound book on the beauty of computation and the mysterious limits of it.

Very interesting, and as someone studying physics and computer science for the EXACT same reasons as you suggested, this helped bring out the mysterious aspects of computer science that helped me get interested in Physics in the first place.",1524296128.0
mumrah,"Go watch Ben Eater’s YouTube series on creating an 8-bit computer. It will really make you appreciate the elegance of the Von Neumann architecture from which modern computer architectures have evolved. 

This will make you start thinking about Turing machines, computability, minimal instruction sets, and all sorts of interesting topics.",1524318806.0
sepantaminu,"I think you have to first appreciate the underlying mathematical differences of physics and computer science. My interest in CS started when I began reading about lambda calculus and formal systems. These are technically topics in logic and foundations of mathematics, but they helped me a lot in getting into that theoretical sweet-spot where mathematics and CS meet. There’s where the “beauty” lies in my opinion, or at least some of the beauty ! ",1524324102.0
Detox1337,"There is a famous quote by Marshall McLuhan(as a riff on a Churchill quote):""We shape our tools and then the tools shape us"". Einstein wrote ""What does a fish know about the water in which he swims all his life?"" I think the external perspective of Media Science is critical in appreciating the intricate dance between humanity and computing. Understanding Media: The Extension of Man (McLuhan) is cool if you can think in different modalities. It tends to piss off and lose rigid thinkers. I like how in 1968 it talks about electronic newspapers you can configure to show you only the news you want and how that becomes destructive, tribalizing and narcissistic. Does that sound at all familiar?


He also wrote ""The Laws of Media"" which covers the same stuff a bit more systematically. Comp sci always seemed to me preoccupied with questions of ""can we"" and ""how do we"" which bore me. Media Science is more about ""what happened when we"" and ""what would happen if we"". I find it a much better tool for answering the question of ""What should we do?"" which is a far more interesting question.
",1524326646.0
IlllIlllI,"I was in a similar boat to you (Physics and CS), and yeah a lot of CS theory is boring as hell. You should look into Machine Learning (in particular, the statistical approaches/interpretations to it), it's what finally got me like stats, and a lot of the background is inspired by approaches from statistical physics. There are a number of good textbooks, such as Murphy's *Machine Learning: A probailistic perspective*, but they do sometimes have the problem of being a little out of date, given how fast the field is moving. Machine learning will also probably have good applications to Physics research, and is a pretty lucrative fallback option if nothing else. In particular, variational inference, autoencoders, and GANs are pretty fucking awesome.

Another pure CS thing that I think is pretty neat is functional programming, but I haven't found a great source on that. It's kind of a good intersection between math theory and real-world applicable CS. ",1524270913.0
w00ten,So I'm not a pure comp sci person. I'm a systems administrator with a reasonable background in computer science and I enjoy physics as a bit of an intellectual hobby. Maybe take a look at quantum computing. It's going to drastically change what and how my job is but it is the perfect bridge of computer science and physics.,1524268721.0
PathToTheLight,Learn Assembly,1524276852.0
xSquittles,I took linear algebra this semester and really enjoyed it and it was like the math behind the sets of data you’ve been learning about it,1524279812.0
ryzahhh,Watch scifi movies. Once you fully grasp the fact that you are building the future you’ll be even more excited about working. ,1524287607.0
perna,"You just should be a mathematician, dude.",1524287839.0
singham,"You are in a very apt position to become a Quantum Computer programmer. I would suggest you can get more into this emerging field right now.

Jarrod McClean: ""Quantum Computation for the Discovery of New Materials 
https://www.youtube.com/watch?v=w7398u8G588",1524302036.0
thecluelessguy90,CS is more about handling expenontial growth.,1524321474.0
WiggWamm,"One of the things that I like about computer science is that fact that you can do almost anything with the code. You can create simulations, ai to help you run things, scripts to run things, hack stuff, build programs that help people do almost anything. Have you done much coding or only therapy so far?",1524326973.0
UNisopod,Godel Escher Bach,1524332998.0
generic12345689,Do a little bit of game development and or just play around with a physics engine. The rest grows on you after you first experience things you want to do with the knowledge. ,1524256408.0
natgough,I think it's just because computer science doesn't go into as much fine detail as physics does because computers are so complicated on there deepest level ,1524256103.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1524239863.0
__-_-_-_-_-_-_-_-_-,"You cannot compare Ph.D. with BS, it's comparing apples and oranges.

MS, in the present-day scenario, is often taken up by newcomers from a different major or a CS graduate from a not-so-good university at a fairly good one. The primary goal, in both of these cases, is to increase one's employability (at a Big4/N in the second case).  Another emerging category consists of those who want to work in a specific field, for example, ML, Data Science, etc. An MS with specialization in that particular field boosts one's chances of landing a job in the desired field. MS is also sometimes used to increase one's chances of getting accepted to a better Ph.D. program. There are no substantial disadvantages to having an MS degree.

Ph.D., on the other hand, is a completely different territory. It is concerned with doing research. You need to be really determined to embark upon this arduous journey. It is surely not for the faint-hearted. If you want to be a developer, Ph.D. would be of little use. In most of the cases, Ph.D. graduates go on to have careers in academia. Better salary/compensation is hardly ever a goal in pursuing Ph.D. ",1524235408.0
MarsLanded,"PhD student and working professional here. I do want to add having the PhD does add weight to your voice in the professional world. My manager knows my dissertation is in software testing and has me writing the master test plan for the program. Also, to reach the highest level technical positions, one must show technical expertise through research, published papers, teaching classes, etc...

I would certainly do an MS but specialize in a particular field. Cyber security and data science are in high demand right now.",1524236980.0
zagbag,"A lot of Machine Learners and Artifical intelligences have PhDs, I think",1524239721.0
megamanxoxo,Depends what you wanna do with your career.. if you just gonna be a software dev just get the BS.,1524234377.0
Workaphobia,"You need a Ph.D to teach or do research. You can get it to do software engineering too, but it will only enhance your employability if you're also a decent programmer, which some researchers are not. It's probably not worth it if you know you just want to program, but it can help open some doors.",1524236966.0
michaelochurch,"I'm 34 years old and dropped out of a PhD to work on Wall Street. Let me give you my perspective: I wish I had gotten the PhD. If you're confident in your drive and commitment and know you'll pass in reasonable time– and you should figure that out in the first year or two– then it is worthwhile to get the degree if you have the ability, even if you don't end up in academia.

Was it a good call to work on Wall Street from 2006–08? Fuck yeah. That was a really good experience. But after 2008, I should have gone back into the academic fold when I had the chance. It was a mistake not to return (although I may have switched from math to CS). It's almost impossible after 30; you're just as smart and have a stronger work ethic, but admissions and finances are... not your friend when you're old. 

Also, if you're smart enough to get into a top 20 PhD program, you should know this: intelligence beyond IQ 130 is disabling on regular corporate work. It's not socially acceptable to talk about this, but it's a problem that the corporate world (including if not especially ""Big N"" software companies) has. You get bored. Now you know why there are so few truly intelligent people in executive positions. 

What does this mean? Well, a PhD doesn't guarantee anything. But, it gives you an advantage in project allocation. If you're in that 130+ IQ set, you absolutely need that advantage (or some advantage like it, at least) just to survive, because you'll bore out of your first or second corporate job once they give you ""regular"" work. 

Bachelor's degree is the baseline assumption. That said, you don't need a CS degree to work in software. You just need to be able to show that you can code. To get a regular coder job (which, again, may not be enough for you) doesn't require much formal education; that's why bootcamps are driving the wages for such jobs down. 

A Master's is a good way to show skill and relevance, and it's not as hard as the PhD to get while working, but it doesn't have a lot of cachet. It's worth getting for the educational value, but it may not pay off as much as you expect. You gain all of the general-purpose learning a PhD would have to get– the remaining component is the dissertation, which is highly specialized. This said, a top-5 MS (and tuition-paying Master's programs are less selective than PhD programs) can also be a pedigree reset and a mid-career relevance boost. 

The PhD itself (once you complete your coursework) has nothing to do with anything you'll do in industry. You're spending 3 years on basic research, which can be a lot of fun, but the degree's value in the corporate world has far more to do with pedigree whoring than direct application. It's an IQ test. 

Let me explain how pedigree works. There are smart people who don't have impressive pedigree, and everyone knows it. There are people smarter than me who dropped out of high school. Exceptions to the pedigree/intelligence correlation exist... but you have to prove that you're one of them. Now, I can prove in a 30-second elevator ride to any CEO, anywhere, that I'm one of the smartest people he'll meet in a blue minute. But, and this is key, I can't do that *and* also make him like me. The advantage of pedigree is that you're assumed to be smart– you don't have to take social risks to prove it– and can spend 100% of your social energies on making the person like you. In business, that is huge. ",1524240042.0
,"From what I've heard from my CS professors: Getting a PhD is a very financially poor decision. You will not become a famous researcher, you will not make tons more money, you will do it only because your passion is research in the field of CS. If you're looking to pursue a path towards more money, going to a PhD is not the route.

Another thing I've also heard: A lot of companies will outright reject you if you have a PhD. I can't remember the exact reasons, I assume it's something to do with expecting higher pay, likely having a higher turnover at a basic appdev job, etc. There's only a handful of companies around where I live that will actually hire someone with a PhD, otherwise you'll likely be working at a university.

Of course I'm not an expert on the topic so I don't have much else to contribute myself, but these are things I've heard directly from people who have gotten PhDs.



",1524237245.0
ColdPorridge,"For a successful career in tech, coding proficiency makes you eligible, a Bachelor's can give you a leg up, a Master's degree makes you competitive, and a PhD is a waste of time if you're only doing it for pay or competitive purposes.

Online masters are equivalent to in-residence for all purposes.

But really more than anything, your own proficiency and people skills is the biggest factor in will landing jobs and continuing a successful career.",1524237755.0
sic2pp,"I will try to answer quickly.

I got a BSc in the UK and about to finish a PhD also in the UK. I also interacted with a few master students over the years, and also marked their work.

BSc \-\-\> It will help you to build your foundations. You may enter the tech industry without a degree, but you may not have the same theoretical background, which will help you in making wise decisions.

MSc \-\-\> Not very useful, unless you go for a highly\-specialised course. No point in doing a standard CS master course.

MPhill \-\-\> Good if you are not certain about a PhD, but still interested in research.

PhD \-\-\> This is a degree on its own. You are alone most of the time and you really need to grow up as a researcher. Writing a 50k\+ words thesis is very hard too. Do a PhD only if you are determined to learn more on a particular field. You will need a PhD if you want to stay in academia, but it is not a must in the industry. Still there are companies that value a PhD \(i.e., better salary and position\).

Finally, a degree will not prepare you 100&#37; for the industry. You will always have to study and read to be up\-to\-date with tech.",1524235623.0
ibcooley,"Depends on the role or field of CS you want to pursue, I suppose.

Can you have a successful career with a BS in CS? Yes

Can an advanced degree advance your career? Maybe.",1524234371.0
schwarzfahrer,Can you share what you’ve tried so far? Hard to know without seeing some code.,1524220366.0
JNCressey,"A few things you could check could be:

* Making sure the program is actually sorting. Fish out the data some way other than animation and check the values are actually being sorted.

* Making sure the program is actually spitting out frames from the correct times in the algorithm and sourcing from the correct set of data.

* Making sure the program doesn't go through all the effort of creating a frame and then just over-writes the whole thing with blankness before drawing the frame. (bliting the background *after* blitting the content?)

* Making sure it is actually being drawn within the animation canvas and not off the sides.",1524256134.0
tontoto,"I tried creating a sorting viz in JavaScript and found it kind of hard to do (it did not seem to like drawing things inside recursion of quick sort for example) but made this https://cmdcolin.github.io/resort/qs.html

My workaround is I save the array state for each operation and render afterwards which is pretty memory intensive...
",1524286907.0
puplan,Was this generated by an artificial neural network?,1524220738.0
skeeto,"You've got the right background that I think you can do it. A few years
ago I mentored a coworker with a physics degree who was trying to
kickstart a software engineering career shift.

He started off knowing *very* little programming and had little practical
experience. If he had been a candidate for a software development
position, I would have quickly rejected him. After about 1.5 years of
coding on the job under my wing, he became a capable and reliable C and
(to a lesser degree) C++ programmer, including solid familiarity with
x86-64 assembly. He's continued to improve some since then, but that 1.5
year mark is the point where, had he been an applicant, I would have
wanted to hire him.

He didn't practice or learn outside the 9–5 job, which is why it took
1.5 years. If you're really committed to putting a lot of time into
learning and especially practicing with your own projects, I bet you
could do it in a year or so. Though, on the other hand, you presumably
won't have a mentor, so it might still take over a year.
",1524223483.0
jake_morrison,"The fundamental issue is getting enough skills in some specific area that someone will pay you to use them to solve problems. And then convincing them that you can actually do it. 

With your background, you are reasonably close to the right skill set, but are not the same as a CS major. Finding something that is close to hardware, e.g. embedded development, should be straightforward. On the other hand, you could start doing web development. AI is the hot area right now, so people are willing to take anyone who can help, and very few people actually have an academic AI background. So learning the Python stack is good. 

If you can find paying work, then great. Otherwise you can do some side projects that demonstrate your abilities, or work for e.g. a charity. Build a portfolio, and leverage it to get better work. For data science, download an interesting data set, do some analysis, publish it on a blog. 
",1524206647.0
mirhagk,"Make stuff! Honestly that's the best thing you can do. Read tutorials and books, but always be making stuff too. 

[Google Code Jam](https://code.google.com/codejam/) has a ton of problems on the site from previous year's competitions and to practice with. They are good projects to try out new algorithms you have been reading up about.

And once you get a solid base down it's a good idea to branch out and try other things. It's a lot easier to learn a 2nd language, it's even easier to learn your 10th language, especially when you've worked with many different types. For my 4th language I learned Haskell from the excellent [Learn you a Haskell for great good](http://learnyouahaskell.com/) and while I've never actually used Haskell to make anything substantial the skills I learned from that have definitely made me a much better programmer. 

Once you can make things then I suggest looking for a job. Learning best practices is very difficult to do on your own, it's much easier to see why you'd do something once you start working with a team. The two summers I took a co-op taught me **far** more than the rest of my school ever did. ",1524231228.0
IncrementalBehavior,Check out /r/cscareerquestions,1524203576.0
PureAsSunlight,"Read head first python 
Read data structure and algorithms by clrs

Don’t worry about learning frameworks that you can learn in a month or two.

PS: by read I mean cover to cover and implement all the examples.",1524199030.0
theluk246,Just start doing it. What you already are. You will be there in one year. 😉 Good Luck! ,1524209079.0
atred3,"You don't need a year. Just polish up your programming and algorithms/data structures for interviews, and learn about databases. You don't need much else, unless you're going into something like front end which requires learning specific technologies first.",1524272389.0
SkitsofRandom,"I currently have a question regarding where to start with a problem. For background on my current knowledge: I'm currently a first year CS student who came out of high school advanced in math, so at the end of this semester I'll be finishing up with some basic foundations in mathematical proofs and having completed all of calculus/linear algebra \(although with little exposure to algorithm design due to the structure of the program I'm in\). In my free time I've been completing simple problems on [Project Euler](https://projecteuler.net) for fun. A few days ago I decided to tackle one of the problems rated at 100&#37; difficulty, and it has really thrown me for a loop. A link to the problem in question: [#478](https://projecteuler.net/problem=478).

As a preface to what I'm about to ask, I'd like to say that I am ***absolutely not looking for an answer/solution***. I'd like to figure it out on my own. Now that I've said that, I'd like to ask my question for anyone willing to read the problem.

So, the two major issues of the problem as I'm approaching it are generating the set of vectors to operate on, M\(n\), then creating an algorithm which will classify them, E\(n\). For me the current roadblock is the former issue. With my current skills all I know that would allow me to create M\(n\) is to brute force find all combinations of \(a,b,c\). What's pretty obvious though is that there must be a better solution, because the problem wants you to use the set M\(10,000,000\), while brute\-forcing M\(n\) with n\>100 creates an absurdly enormous set. What I'd like to know is what sort of math/CS I need to read up on to figure this out, because currently I have no idea whatsoever as to how to tackle this problem differently \(although I suspect that there is probably a trick that would circumvent the need to generate M\(n\)\). Also I'm aware that this problem is probably super far and above out of my league, but even so I'm still curious as to what sort of knowledge I'd need to figure it out.",1524196802.0
lucinaiscool,"I'm getting interested in researches related to social and information networks \(e.g. [https://arxiv.org/list/cs.SI/recent](https://arxiv.org/list/cs.SI/recent)\), but I feel I'm lacking some background to fully understand the methods in these papers. 

Any recommendations on where I could start so that I will get better insights in this field? I was thinking maybe analysis and probability theory would be a good start.",1524233823.0
deckeresq,"Programming languages PhD student here. Try checking out the simply typed lambda calculus! That's a good place to start on this line of thinking (it sounds like you're interested in type theory).

You'll find that everything can be represented using basic data types and functions (after all, using a mapping function which returns 'a' when given 0, etc., is functionally equivalent to the pythonic dictionary you're describing).

A great place to start (imo) would be Software Foundations by Benjamin Pierce. It's all in Coq (maybe there's a lisp version?), but if type theory is your jam you may end up loving that. Note that it can be quite an adjustment if you've never done functional programming before, but the ultimate payoff is worth the time and investment (and SF walks you through most of the basics).

Feel free to respond here or PM me if you want to discuss anything further!",1524147952.0
raghar,"Products and co-products. In other words: Cartesian products and sum-types.

 * containers (lists, maps, sets, etc) - products
 * tuples, value classes - products
 * ADT - usually sum-types = co-products
 * subtyping - co-products

You can describe anything as either a product-type or co-product type. You could ask a product value for a property's value by its name/index. You could ask co-product type which of sum elements it is.",1524151639.0
,"You are describing data refinement, a well studied concept. 

* [Textbook](https://www.cambridge.org/core/books/data-refinement/6B4B62F7D6413BBA1C140A40EE0C23DE)

* [Wiki](https://en.wikipedia.org/wiki/Refinement_(computing\))

The encodings you describe are different representations of the same information. If you have corresponding operations on the data type, you can even prove that they behave in the same way. 

Also, refinement may also work in one direction, but not the other. E.g., a queue data type is a refinement of a list, but not vice versa. In your examples, both the array and set are implementations (refinements) of a dictionary. In other words the dictionary is an abstraction of both an array and a set. You can even set up a refinement relation and prove this property formally.",1524152023.0
etotheprimez,"What about a graph, or tree?",1524144332.0
sonaxaton,"What you're describing is a generalization of a function, which maps inputs to outputs. And functions can be defined in terms of sets, so really it all boils down to set theory!",1524172567.0
rr1pp3rr,"I'm no theorist, really just an engineer.  But I am quite sure that any data structure can be represented by an ordered associative array containing either other ordered associative arrays, primitive values, or references.

Most likely someone well versed in this type of information theory could totally blow this idea out of the water.
",1524147919.0
toombak,"Tangential it might sound, Christian Bök's *The Xenotext Experiment* in which comes to mind in this direction.",1524161666.0
cyancynic,"FWIW PHP (ok just stop it) has just one collection called array that also acts like a hash depending on what you use for keys. It’s perfectly adequate.

",1524201930.0
ggPeti,"Yes there is a theory, it is called [Algebraic Data Types](https://en.wikipedia.org/wiki/Algebraic_data_type). It concerns the construction of complex types out of basic ones.",1524225825.0
DrKriegerPhD,"I saw a very interesting language designed by students in my programming languages class where all data types are essentially a tree. The idea was that all data types can be rethought of as a tree, and (with some notable exceptions) I loved this idea ",1524157584.0
Yoghurt42,"JFYI, JavaScript Arrays behave like what you described:

    var arr = ['foo', 'bar', 'baz'];
    for (var i in arr) { console.log(i); } // 0 1 2
    arr[10] = 'hmm';
    console.log(arr); // Array [""foo"", ""bar"", ""baz"", 7 empty slots, ...]
    Array.prototype.foo = () => {};
    for (var i in arr) { console.log(i);} // 0 1 2 10 foo",1524158315.0
ttt72,"Category theory. Data types is just 0-parameter function types. AFAIK, [Cats](https://typelevel.org/cats/) is the only one which implemented category theory and matured enough for serious programming that runs on datacenters. 

Some may consider Haskell but it avoid success at all costs.",1524317195.0
Bromskloss,"Note that if something can be stored on disk, it can be encoded as a string of bytes. This is called [_serialisation_](https://en.wikipedia.org/wiki/Serialization).",1524148095.0
_--__,"Throw in some ""behaviour"" and you've just described an Object (as in [object-oriented programming](https://en.wikipedia.org/wiki/Object-oriented_programming)).  Of course, as others have pointed out, you can get far more abstract than this.",1524164615.0
skulgnome,"No, no theory as such. Interfaces are an engineering consideration.",1524226100.0
p_pistol,Binary memory?,1524151139.0
39452,F,1524127551.0
KevZero,"If you figure it out, let us know.",1524099724.0
Kopaka99559,"This class looks very close to the one I'm currently taking. The subject is extremely fascinating. Would recommend for anyone who wants to know how a computer works, even if you aren't a CS major.",1524071151.0
Randolpho,"Taking notes on github with markdown files...

That's an amazing idea. Wasn't ever possible when I was in school, but get off my lawn",1524087472.0
Wildhaunterr,Thank you for sharing!,1524051690.0
someLinuxGuy1984,This is cool. Thanks. ,1524058171.0
GreyDalcenti,"really good notes, thanks!",1524060224.0
nevabyte,This is cool. You are cool too. ,1524062110.0
bob_7474,"Excellent, thank you for sharing, regards.",1524098184.0
theofficialdeavmi,These arw excellent notes. Thanks for sharing.,1524120391.0
PythonGod123,Wow !!! This is a dream come true for me. I can use these to get a head start in my degree program. Did you go to Berkeley ?,1524069723.0
iwantashinyunicorn,"Oooh, this will be interesting. Maybe this time I'll get a reviewer who understands that empirical algorithmics is actually a thing, and who doesn't auto-reject any paper that doesn't have a twenty page appendix of proofs for ""not being difficult enough"".

(I may be a tad bitter about this. Yes, it was the B track.)",1524073646.0
FUZxxl,What's a PC in this context?,1523995372.0
Seneferu,"Interesting.
I see one problem.
Both committees know that there is an other committee.
This might affect their decision process, even if it is only subconsciously.
But, I have no idea how one could avoid this.",1524042264.0
ellipticaltable,"Other people have already answered how to go about solving this particula problem, so I wanted to answer your final question:
> How do you base computing off of this

TL;DR: layers of abstraction.

For example, we can prove the following

1.  A TM with arbitrary alphabet can be simulated by a TM with alphabet {0,1}.
1. A TM with *k* tapes can be simulated by a TM with 1 tape.

In fact, if we are careful about how these simulations are constructed, we can ensure that the resulting runtime is only polynomially worse than the original.

With such tools at our disposal, your doubling problem becomes easy. As in /u/deneth1's answer, we can freely add extra placeholder symbols, without worrying about whether they are in the TM's ""native"" alphabet. Simply apply the generic transform (1) at the end to get a solution using the original smaller alphabet.

But we can make it even simpler -- just use a 2nd tape! We now do a single pass over the input, writing 2 1s to the *second* tape for each 1 that we read on the *first* tape. And finally we apply generic transform (2) to reduce to a single-tape solution. *Voila*

Armed with these tools, we can keep building fancier and fancier abstractions and subroutines, until we finally end up with a Python interpreter. Sure, a simple O(n^(2)) Python program might take O(n^(200000)) steps when transformed all the way back down to a 2-symbol,1-tape Turing Machine, but that's still just a polynomial difference.

We can also use these transformations in the other direction. For example, suppose that I want to make a ""universal TM"" -- a TM that takes a description of a TM as input and simulates it. As above, I'll write my universal TM using lots of tapes and lots of symbols. But I *also* get to assume that the input TM is just 2-symbol,1-tape. That makes the simulation *much* easier.

So don't worry about making slick TMs that use minimal alphabets/states/steps. Those are fun brainteasers, but nothing more. The key intuition that you should internalize is that it is even *possible* to do at all. And for that purpose, a methodical construction with nice conceptual building blocks is a better solution.",1523980861.0
deneth1,"I think that this problem isn't too hard.
What I would start with is moving to the right, passing all 1s and after the last 1 place some sort of marker, for example #
And then you go back to the first 1, erase or replace by something else, go to the marker, find the first empty spot and then write your two 1s.
Let me know, if that works out for you :)",1523945012.0
2020Tokyo,"id do something like first mark all the 1s


1111 > xxxx

loop until no more x's 

{ go to the rightmost x

  turn it into a 1 and add a 1 after the rightmost 1 (the first blank)

  xxxx > xxx11 }",1523945503.0
SwordInALake,I'm not sure what you've learnt yet but you may find state diagrams a more intuitive way to view Turing machines. Instead of as a table you're viewing them as a graph and seeing how states link together. I tend to then draw coloured boxes around some of the states to highlight subroutines and label what task they're doing. ,1523955426.0
Workaphobia,"First of all, like someone said, don't write it out as a table, write it out as a diagram with nodes and arrows.

Second, don't think about states right away. Think about low-level goals like ""I want to move the tape head all the way to the right and put another symbol after that,"" or ""Now find the next 1 to the left I haven't touched so far, using an x marker to keep track of what I touched.""",1523980080.0
Arancaytar,"The number of times you have to work with actual TMs to compute anything is really small. For most purposes in proofs, you're going to be working with pseudocode algorithms.

That means that you need to understand how a TM works and what it can do (and how much time/space it needs, asymptotically), but you don't need to literally use them any more than you need to write algorithms in something like Brainfuck.

Note: TMs can use any number of tapes and remain equivalent. I don't know what model you're using in the example above, but assuming you're working with a read-only input and an output tape, this is trivial:

- Start in state A
- If in state A and reading a 1 from input, remain in  A and write 1 to output, moving both heads to the right.
- If in state A and reading a blank symbol on the input (marking the end) go to state B and move the input head left.
- If in state B and reading a 1 on the input, write a 1 and move left on input and right on output.
- If in state B and reading a blank symbol on the input, terminate.",1523980254.0
JDew4061,Wot,1523916774.0
sstults,"The assumption is that the test collection is representative of live data. During your experimentation you can't always go back and get more test data, so do what folks in the Machine Learning camp do and further divide your test data into some percentage of training data. You can re-sample that set from your whole test set to guard against ""over fitting"". If this is an ongoing project, hopefully you can also build in a re-sampling method to keep your test data from stagnating.
",1523900616.0
p_pistol,"You don't. That's why it's good to have as much data as possible.

You can also use cross validation methods, etc, but the entire notion of unseen data is that it is, well, unseen. ",1523897755.0
repsilat,"Maybe easier to start by explaining a simple partition around a value (rather than around a list element.) Like ""Sort this list into groups less than 10 and greater than 10.""

The general idea is that things below the low pointer are less than or equal to the sort value and things above the high pointer are greater than it.

You move both pointers ""inwards"" while that condition is true. That's easy enough, right? Those things are already properly partitioned. The hard part happens (wlog) when the lower pointer hits something bigger than the condition value. In that case there are two things that might be the case:

1. Nothing to the right is less than or equal to the partition value. In this case, we can decrement the high pointer right down to the low pointer and we're done.

2. Something to the right is lower than or equal to the partition value. In this case, we decrement the high pointer down to the rightmost such element, and now we have the low pointer pointing to something that should go in the right partition and the high pointer pointing to something that should go in the left partition. We swap them to put them into the correct partitions and carry on.

With pivot elements it's just the same except the partition value is chosen from the list.",1523910260.0
Terr_,"Just to be clear here, this is **not** about Quickselect (another algorithm with Hoare's name attached) but instead about a particular implementation of Quicksort, right?


",1523923737.0
Kimano,"I hadn't heard it called that so I had to go look it up, I've always heard it called quickselect.

It's the same premise as quicksort, and if you haven't ever heard of that one, you can go find some excellent online material on it, as understanding that will make quickselect significantly easier to understand.

Quickselect is just trying to find the nth smallest element in a list. It does that by employing the same pivot and and partition method as quicksort, and then recursing into the partition that it knows will have that nth position element in it, rather than both partitions like quicksort does.

For instance, if you have a list with 5 elements, and you pick a pivot that happens to be the middle, then you swap elements into the partitions, you'll end up with an unsorted partition that contains nth elements 1 and 2, the pivot that is nth element 3, and another unsorted partition that is nth elements 4 and 5.

It makes sense then that if you're looking for the 2nd nth element, there is no reason to recurse anywhere but the first partition, since your second element is guaranteed to be in there, even if you don't know which position specifically.

Repeat this process until you've found that nth position element.",1523902120.0
theblacklounge,Me too pls,1523887043.0
inephable,What is that?,1523894045.0
notfrmTeXas,seconded,1523894318.0
maladat,"I hadn't heard of this, but it looks interesting.

If anyone has extra invitations to hand out, may I have one, too, please?

Thanks.",1523904184.0
yawkat,"There's [parrt](https://github.com/parrt), the antlr guy.

But honestly, you don't need to be a great programmer to be a cs professor and most aren't.",1523821005.0
Captain_Flashheart,"Dude. You do not want to see my professor's code. 

",1523828099.0
Insanity-Cow,"https://github.com/pconrad

Professor Conrad teaches at my uni, and his focus is on CS education and good programming practices! Sounds like it'd be up your alley",1523826173.0
rickityrixkityrick,"At my school we use email for version control, like proper devs",1523835288.0
soegaard,"You can find several on this list: https://github.com/orgs/racket/people

",1523818359.0
secretlizardperson,"Not all good programmer are professors, and not all professors are good programmers :). Just look at respected open source projects, and try to figure out why certain things were done that way. Ask yourself if it was the best way to do it (it won't always be, but at the very least it will help you think about the technical decisions made).",1523830209.0
stochastaclysm,Programming and academic computer science are two separate endeavours. CS research is closer to maths.,1523822622.0
Lj101,For functional programming: https://github.com/pigworker,1523831476.0
IJzerbaard,Daniel Lemire: https://github.com/lemire,1523825496.0
programmerChilli,"Shoutout to one of Cornell's PL professors (one of my favorite professors too!), Adrian Sampson. He created and has been maintaining beetbox since 2009, which is completely unrelated to his research.
 https://github.com/sampsyo

Pretty cool imo.",1523822652.0
mynewpeppep69,"Here are three professors at my school that do research in PL Theory. I've actually been doing research with the second for the past year and am doing research this summer with the third. They're all incredibly nice people and love to bring on students for their projects.

https://github.com/sweirich
https://github.com/Zdancewic
https://github.com/bcpierce00",1523831816.0
VikeStep,"Erik Demaine of MIT is pretty active on github
https://github.com/edemaine",1523833234.0
johndbritton,"You might want to check out the GitHub Education Community, there are a lot of teachers active there who use GitHub in their CS courses.

https://education.github.community",1524158705.0
elcric_krej,"Doctor Martin motherfucking Odersky, creator of Scala, dot calculus and Java generics... Amongst other things.

https://github.com/odersky",1523832839.0
orangejake,"[Chris Peikert](https://github.com/cpeikert) at UMich Ann Arbor (cryptographer, has lattice crypto implementation stuff on there).",1523837099.0
ezubaric,"Here's our groups Github
https://github.com/Pinafore/",1523837974.0
Arancaytar,"This is just limited to my own short experience in academia but:

1. Professors (and most full-time academics) are busy with a crapton of things that aren't programming (teaching, lesson plans, research, administration, grants, etc), and generally have a ready supply of cheap labor for when they *do* need things programmed.
2. When they do for some reason produce code themselves, it's very likely a rush job for their own use, which means you're unlikely to find it in public, and it's not going to show good practices or documentation.
3. Computer scientists aren't necessarily that good at software engineering; it's a very different skill-set. (I made the switch myself.)

Knowing some fundamentals of CS *is* useful in designing good software, but the best way to acquire that is to find an online course in algorithms and data structures.

To make an imperfect analogy, imagine you heard that good architects need to know a lot about calculus. That doesn't mean you should become a better architect by looking for and studying buildings that were designed by calculus professors.",1523861608.0
sigma914,[John Regher](https://github.com/regehr) is fairly active,1523862587.0
MIJOTHY,"https://github.com/andrejbauer

Prof of computational maths, so maybe not exactly what you're after, but has some v cool stuff on there, most notably IMO the programming languages zoo.",1523869615.0
AndreaDNicole,"If you're into computer graphics, Wenzel Jakob and Alec Jacobson both have big open-source projects they're/were in charge of.",1523876991.0
mercere99,"I'm a Professor of Computer Science at MSU (Charles Ofria).  Several years back I realized that I was enjoying my job less because I was doing all of my research vicariously through my students and decided I was going to code every single day, even if only for 15 minutes.  I've been able to get a ton done for my own work this way and have enjoyed it greatly.  Here's my github page: https://github.com/mercere99
",1523895689.0
mapio,"I started programming well before becoming a CS prof https://github.com/mapio
",1523824575.0
Zavidovici,"Radford Neal implemented his own version of R ( https://github.com/radfordneal/pqR ). (Also, as a former research assistant at UofT, I've heard it through the grapevine that Hinton says Radford was his smartest student.)",1523828947.0
lanemik,"[https://github.com/BartMassey](https://github.com/BartMassey)

[https://github.com/apblack](https://github.com/apblack)",1523839563.0
eshaansharma,"Dr. Charles Severance --
Associate Professor of Comp. Sci. at University of Michigan and Lead Instructor of Courseras Python specialization.
https://github.com/csev",1523841210.0
sparr,"Wouter van Oortmerssen: https://github.com/aardappel

http://strlen.com/

His career has included a lot of non-academic work, but I think he qualifies due to his time at Guildhall (SMU).",1523843063.0
theZcuber,"From a former university, but [Andy Meneely](https://github.com/andymeneely/) actively maintains the squib library for ruby.",1523845753.0
angrypotato1,[https://github.com/guidotack](https://github.com/guidotack),1523846798.0
notathrowaway,https://github.com/jim-parry,1523883467.0
AaronKClark,My graduate CS prof is on there as [okrarm](https://github.com/okaram),1523892633.0
TheOsuConspiracy,"Martin Odersky created the Scala programming language, and he's super active on github.",1524084125.0
buoyantbird,There's Peter Norvig who is active in maintaining `aimacode` on Github (bunch of repos related to his book AI: a Modern Approach),1524138406.0
kibleh,Me,1523852554.0
wellreaddead,"They say, professors can write code by staring at the keyboard. I have never seen tho.",1523871163.0
,[removed],1523945590.0
clownshoesrock,"The Pentium architecture is maddening to be your first emulator.

Honestly the whole x86 line is a shit show.

Really do something more reasonable to start with, like mips, sparc, or arm..  I'd go mips, because there is an emulator called spim that you can test against.

Now as far as emulating for games..  ummm this is not a feasible project.  Getting a processor emulated in a year is going to be hard for a smart person that is into this sort of challenge.  To play a game you need to emulate a system..  So motherboard, bios, bus, and cards.  You need to be able to get MS-DOS to boot..  Which is may be the easiest OS to boot.  Crap... you need to emulate a disk controller and disk.

So, the answer is..  this has to be a labor of love.. and it will take some time, and you will learn an upsetting amount of stupid things done in the past.

You will want to learn to use debug in your sleep.

If you get this to remotely work, you will be able to land a job with few problems..  Really this is hard, and having the drive to push the difficult project though is a rare thing to find.  
  ",1523751638.0
fzammetti,"Possible, yes:

https://en.wikipedia.org/wiki/PCem

Many years ago I was lead on a project that developed the first working Commodore 64 emulator for the old Microsoft PocketPC platform.  That is obviously a much simpler bit of hardware to emulate, but I can tell you it wasn't easy for the team.  I think what you're proposing would be much more difficult.

However, I'm not sure I understand why you'd want to do it.  Modern CPUs are backwards-compatible with a Pentium, so you can still run all the same software you could back then, it's a question of dealing with the software layer, which obviously has changed considerably.",1523751767.0
LongUsername,May be better to spend your time working on improving DOSbox. At least before you commit to doing your own look at the DOSbox code to familiarise yourself with what is involved so you know what you're in for.,1523782771.0
Lendari,"Its called dosbox and it already exists. It emulates the machine yes... but also sound cards, cdrom drives, analog joysticks and all the old hardware you'd have a hard time with today.",1523751876.0
Carpetfizz,"my undergrad CS class implemented a RISC-V emulator in C which I thought was pretty educational. You may find the starter code and some instructions here:

[CS61C, Project 2-1](http://inst.eecs.berkeley.edu/~cs61c/sp18/projs/02-1/)

[CS61C, Project 2-2](http://inst.eecs.berkeley.edu/~cs61c/sp18/projs/02-2/)

RISC-V is designed to be minimal and doesn't have a whole lot of instructions. It's a pretty neat exercise to refer to a green card specification of an assembly language and come out with a working emulator. We were given a couple weeks to complete this. Best of all, it's not an archaic language like MIPS. With little effort you can expand this emulator to the entire RISC-V specification and run real programs designed to run on modern embedded systems!",1523780770.0
jet_heller,"Seriously, use the ones that are out there. Use your skills for something better. ",1523752472.0
PolySoup,I think I have an arm assembler I made laying around somewhere written in c that I can throw your way. I'd have to find it. ,1523761918.0
Orangy_Tang,You might want to try asking over at /r/emudev,1523781097.0
ProgramTheWorld,">	I have programming skills and basic knowledge of computer architecture

>	Is it possible for me to complete it within, say, a year

Good luck with that.",1523827264.0
,[deleted],1523763024.0
hamtaroismyhomie,"MIT's [Mathematics for Computer Science](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-spring-2015/). Free textbook is in .pdf version there.

The first unit (of the four) of the MIT course is equivalent to the Oxford course.",1523725137.0
gnarlymath,You want an equivalent book to an entire mathematics course?,1523724082.0
jcsf321,Also called descrete math and logic ,1523724328.0
dcel,"I took this course 2 years ago. The book is actually available online: http://www.cs.cmu.edu/~15819/zedbook.pdf

As the name suggests it's primarily about Z, a discrete language used in formal verification, and lot of the symbols and nomenclature differ from more general discrete mathematics.

Unless you are specifically looking to learn Z or use formal methods I would recommend some of the other books and/or courses suggested here.",1523749529.0
Vampyrez,"The textbooks listed for the undergraduate course in Discrete Mathematics (which probably contains much the same content) are primary: Ross & Wright Discrete Mathematics 5e, easy intro: Chetwynd & Diggle Discrete Mathematics, alternative: Grimaldi Discrete and Combinatorial Mathematics 5e",1523737063.0
Darkfeign,"The issue is, and I see it quite often, while you're interested in game development now, you may not be in 5 or 10 years. The industry is very demanding and unless you end up in a great studio, probably isn't that rewarding either. I feel like computer science as a bachelor's degree will give you the foundational knowledge you need to learn game development in your own time if you wish to continue to persue that, while giving you a well rounded education for the many, many other programming jobs available in your area that aren't game specific. 4 years is a long time to develop personally, and to therefore develop new preferences and career goals. At this age, go for solid foundations that keep does open.",1523698653.0
pulsar512b,What can you see yourself doing? ,1523674891.0
ughit,"Don’t get the AS degree. You’ll be disadvantaged when it comes to jobs, paid less than someone who has a degree, and have less chances for advancement. You should identify a transfer school ASAP and look on assist.org to see what transfers. Take those classes ONLY and GTFO. Also, make sure you get your math sequence out of the way. Math has tripped up many a CS hopeful. ",1523689458.0
HeyVanity15,"I have an AAS in computer science from a community college, and I am a software engineer for a well-known game studio. If you want to do actual game development, rather than tools development (like me), you will absolutely need a BS at the very least, and likely more specialization beyond that. My education significantly limits my career options and salary, and given the opportunity, I definitely would have gone for a BS from the start.",1523720288.0
hamtaroismyhomie,"> I’ve been told I can get a pretty good job with just an associates.

Who's told you this? AS for software development doesn't mean much. You might get something for a low wage, but it will be a slog, and likely won't be enough to live in the bay area. Plus

What are the reasons you don't want to return to your 4 year? Were you miserable? Have you considered transferring to a less competitive 4 year?

Just get the Bachelors. The 2 years are worth it.

",1523727999.0
exorxor,"As a student, studying in some EU country with much cheaper education would be the most intelligent choice for you, but I am going to guess that you will find some way to rationalize that away.

The closest thing to game-development you will see in university is https://en.wikipedia.org/wiki/Rendering_equation and it is the basis of every game engine. I am not saying that they all directly implement this, but all of them try to approximate this, some more directly than others.

Community college is worthless. Many American universities are also worthless.",1523719234.0
avaxzat,"This proof does not show an identity function does not exist. Rather, it shows that a *total* identity function does not exist, i.e. an identity function that is defined on all inputs. This is clearly true because there exist computable functions that do not return a value for every input (your Q function being a constructive example).",1523647984.0
neilmoore,"Another possibility, besides Id not existing, is that *Q* isn't actually a decision procedure.  Specifically, when given itself as input, it might fail to halt, rather than returning TRUE or FALSE.  Under the ""obvious"" implementation of *Id* \(which calls f\), this is the case, as Q\(Q\) would in fact lead to infinite recursion: Q\(Q\) calls Id\(Q,Q\), which calls Q\(Q\), which calls Id\(Q,Q\), ...",1523647305.0
jfredett,"One way to approach this is via types. I don't know what level of comfort you have with them, so I'll do a quick notation crash course:

    f : a -> b 

denotes a function `f`, which takes elements of type `a`, and maps them to elements of type `b`.

    g : a -> b -> c

denotes a function `g` which takes two parameters, one of type `a` and one of type `b` and outputs a value of type `c`. Note that we can also interpret `g` as a function which takes a value of type `a` and outputs a function, `g' : b -> c`. That is, this notation is right-associative.

Finally

    even? : Integer -> Bool

denotes a function `even?` which takes an integer, and returns a boolean (True/False) value.

Let's now examine your functions, first `Id`.

    Id : (a -> Bool) -> a -> Bool

That is, given a function `f : a -> Bool`, and an input of type `a`, this function returns a boolean. This is actually a special case of a more general function:

    Apply : (a -> b) -> a -> b

i.e., given a function `f' : a -> b` and an input of type `a`, return an item of type `b`.[1]

Now, lets look at `Q`. Obviously Q's return type is `Bool`, so

     Q :  ?? -> Bool

So far so good. To analyze the required type of it's argument, we can examine how it's used, it's only used in the first part of the branch, where we have:

     Id(f,f)

Since `f` is used in the first argument of `Id`, it must be of type `a -> Bool`; however, since it's also used in the second type, it's type must also be `a`. This follows just by chasing the types. So we have that:

     f : a
     f : a -> Bool

so we must find `a` such that `a === a -> Bool` (where `===` means 'has the same type as')`, 

But this is impossible. In fact it's a so-called 'infinite type'.

So what does this mean? It means `Id` is _perfectly_ reasonable to construct, `Id` isn't the problem -- `Q` is.

Your original proof is indeed wrong, but it's only because it's pointing the finger at the innocent party. `Q` is the function which 'cannot exist'. It's perfectly reasonable to write it, but it will never return (it will in fact just be an infinite loop in the first branch, since in order to evaluate `Q(Q)`, you must evaluate `Q(Q)`, the other side of the branch will never get used. Consider the execution trace:

    Q(Q) => If Id(Q,Q) => If Q(Q) => If (If Id(Q,Q)) => If(If Q(Q)) => ...

An infinite loop. This idea is a core part of the proof of the undecidability of the halting problem, so it's worth trying to understand this mechanism well. I find that type theory can be really handy with that, but this trace-function method can be useful too, YMMV.


[1] Fun fact, you can regard `apply` as a way of asserting _modus ponens_, i.e., assume `a => b` (read: ""A implies B""), and assume `a` is true, then we can conclude `b` is true. This has a logical signature like: `(a => b /\ a) |= b`, which could also be written `(a => b) /\ a => b`, which is equivalent to our statement (if you forgive a little associativity stuff).
    ",1523678552.0
dragonnyxx,"Id(f, f) is just f(f). When f = Q (as it does when you evaluate Q(Q)) that Id(f, f) call is equivalent to Q(Q).

So the first thing you do when evaluating Q(Q) is ""if Q(Q):"".

That results in infinite recursion, so you never actually get past that statement. Trying to reason about what happens after infinite recursion is equivalent to trying to reason about what happens after you divide by zero (""ex falso quodlibet"").",1523648310.0
Sniffnoy,"Congratulations, you've just rediscovered the undecideability of the halting problem! :)",1523663669.0
TomSwirly,"> Let us suppose there is an identity function Id(f, i) that takes as inputs f, a decision procedure that returns TRUE or FALSE, and an input i. Id(f, i) will return the value of f(i).

That is not an [identity function](https://en.wikipedia.org/wiki/Identity_function).",1523717861.0
green_meklar,"It seems that calling Q(Q) leads to infinite recursion. On the first call it would step into Id(Q,Q), which would in turn step into Q(Q), and so on.

You could try to get around this by arguing that Id(f,i) doesn't actually call f, it just maps f and i to output values. But this doesn't really work, because in order to set up Id's output map we'd have to resolve the infinite recursion anyway. Infinite recursion doesn't have an output value, so if we take the definition of Id for granted, we have to conclude that Q isn't a valid decision procedure- it has at least one input with no well-defined true or false output. With this assumption, Id can still be a valid decision procedure itself, it just doesn't have Q as one of its possible inputs.",1523693099.0
PM_ME_UR_OBSIDIAN,"FYI, this isn't an identity function, which would take one argument and return it. This is just function application restricted to boolean-valued functions.

This is precisely the kind of paradox that type theory was created to address. Its first function is to disallow types of the form Q(Q), except under very special circumstances.",1523706242.0
eiusmod,"I'm not sure if you're talking about mathematical functions or programs (Turing machines, algorithms, or whatever you want to call them). The difference is, the former doesn't need to be computable but the latter needs.

If you talk about functions, then this would indeed result in a contradiction. That's because you really cannot define a function Id to work on all functions including itself: It would be like [the set of all sets that don't contain itself](https://en.wikipedia.org/wiki/Russell%27s_paradox).

If you talk about programs, then Id(f, i) wouldn't mean anything if f doesn't halt with input i, because your definition of Id conveniently skipped the possibility that the program doesn't halt. In particular, if Id was a program that simulated running f with input i, any method trying to simulate what Q does with input Q would get into infinite recursion.

--

Now you could fix this by asking the following question: Does there exist a program Id such that:

- Id halts on all inputs
- If f halts on input i, Id(f,i) returns the result of f(i)
- If f doesn't halt, Id(f,i) is allowed to return anything

I'll leave this as an exercise for the reader.",1523648211.0
hzhou321,"This reminds me the ancient statement of ""I am lying"".",1523659707.0
mynewpeppep69,"Lots of good comments already but, another way to see that this function is much more powerful than the word identity lets on is to rephrase the problem this way. 

Let Answer(M, i) be the problem of deciding whether M halts on input i with the answer YES. 

Then it should be really clear how this is just rephrasing the halting problem. 
",1523665389.0
DSrcl,"There *is* something wrong here. You say 'f' either returns true or false, so I assume you are implying here that `f' always terminates.

You actually prove something different here. Your proof is a proof-by-contradiction that`Q' doesn't terminate when applied to itself!",1523746600.0
ProgramTheWorld,"The function exists, it just doesn’t halt.",1523655683.0
Nerdlinger,Why do you think something is clearly wrong with it?,1523647306.0
Terr_,"Note: I'm a non-mathematician lurker, and this reply is pretty metaphorical and fuzzy because I'm trying to approach it from a different intuitive angle which might help some other lurker.

> I have this “proof” thing

It sounds to me like the same problem as:

1. I have a task for you to complete.
 * It involves an assistant and an object.
 * Always tell me what color the assistant claims the object is.
2. Your assigned assistant is a trickster. 
 * Whenever you ask him what color an object is, first he asks *you* what *you* think it is.
 * His answer is always something different than what you said.

> What's wrong here?

There's nothing strictly impossible about that situation -- getting a smartass assistant could totally happen in the real world! What it actually proves is that I assigned you task which is *sometimes impossible.*

",1523651493.0
watsreddit,"I think it's kind of difficult to reason about this problem without having some notion of what `i` is, or rather, what the domain of `f` is. Is this all the information that was available in the prompt?",1523650407.0
SelfDistinction,"You can prove the above, under the assumption that *Q returns*. In fact, this procedure will, iirc, get into an infinite loop and cause a stackoverflow.",1523656448.0
dman24752,Just make Id always return true and you're done. Problem solved.,1523662033.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1523646045.0
singh19196,https://www.geeksforgeeks.org/shortest-path-unweighted-graph/,1523639237.0
PaperClip44,"Here's a link to a project I did for school.  It's a java zombie game where the zombies use Dijkstra's to find the player.  Some of the source code is a fucking mess, but I hope it might be useful for you.  Feel free to use it if you want.  The Graph and Vertex classes might be useful for you.  This [link](https://github.com/MasonFlint44/dijkstrasZombies/blob/8a1a9351a8a229b58c26d80f8604164592e2d7d7/DijkstrasZombies/src/graph/Graph.java#L156) will take you to my implementation of Dijkstra's.",1523640914.0
IJzerbaard,But VT-x does support long mode. ,1523631690.0
FUZxxl,There is.  Have you tried KVM on Linux or bhyve on FreeBSD?,1523632027.0
magnificentbop,"What about just picking a point on the map, computing distance to nearby units, and weighting that distance by unit strength?  After anbinary classification, you could run a simple line detection to draw the front line.

 Granted, this is not the most efficient approach, but I would expect that performance would suffer for the amount of units on the battlefield long before the algorithm slows down.",1523627688.0
Yeldaizmir,https://www.reddit.com/user/Yeldaizmir,1523611661.0
jmite,"What is a computable data type? Also, be careful, ""recursive"" in computability theory means something different from recursion in programming.

But, when it comes down to it, induction and conduction are really the main ways do define anything computational that's not finite.",1523603428.0
godofpumpkins,As opposed to coinductive? Can you elaborate a bit on what you’re getting at? Not all data types we deal with have an associated induction principle,1523593172.0
flexibeast,"Unless you're using ""data"" in the sense of ""the dual to codata"", then no, because inhabitants of codata types are formed via [coinduction](https://en.wikipedia.org/wiki/Coinduction).",1523593579.0
tailcalled,"The first question is what we mean by a data type. One answer we could imagine is that a value is a sequence of symbols, and a data type is a set of values. This is a very simplistic definition, but it works for some purposes.

Next, a computable data type could be defined as one where we can compute whether a given value has that type or not. It seems reasonable to define inductive data types as those that can be described by a context-free grammar. However, using that definition is more restrictive than the definition of ""recursive"" that can be shown to allow all computable functions.

If we use these definitions, then there are data types that cannot be modeled using inductive types, because there exists e.g. context-sensitive languages that aren't context-free. A standard example is the language where you have two types of parentheses, and each must be matched with its own kind but can exist in arbitrary locations compared to the other kind (i.e. allowing [{]} but not }{). This sort of data type might be useful for modelling formatted text.",1523616358.0
timmyotc,"High availability isn't really what TPB gets, because it goes down for **hours**. Instead, they have pretty well tested disaster recovery. It's very likely they distribute backups of the database to multiple locations around the world, along with multiple copies of the server. Stand up the server in varying countries that are known to not comply with foreign law enforcement. Rinse and repeat.",1523584477.0
emilvikstrom,"The original team behind The Pirate Bay hosted the servers themselves in their own data center. They have a hosting company called [PRQ](https://prq.se/). They acted as their own ISP peering directly with upstream ISPs through an open interchange (IX). To do this all you need is facilities, a fiber cable to the IX and know how to announce your routes with BGP.

They relied (and still relies) heavily on net neutrality: that other companies will treat their IP packets exactly the same as all other IP packets. The same goes for the domain name: ICANN [does not interfere](https://www.icann.org/resources/pages/copyright-2013-05-03-en) in copyright disputes. They did lose the .se domain name because the national registry have to comply with Swedish law.

The first major takedown was when the servers got seized. The police took all machines in the entire datacenter, no matter the customer. You solve this problem by installing new servers an re-route your IP addresses with BGP (or point your domain name to new IPs).

It turns out that the dataset behind The Pirate Bay is [not very large](https://torrentfreak.com/download-a-copy-of-the-pirate-bay-its-only-90-mb-120209/). Pretty easy to just sync it up to new machines as long as you have backups hidden somewhere.

Between 2010 and 2017 they broke out the tracker to its own service, [OpenBitTorrent](https://en.wikipedia.org/wiki/OpenBitTorrent). The tracker is what keeps track of who has what chunks of a torrent. That meant The Pirate Bay itself only hosted the metadata about the chunks that makes up a torrent, not where those chunks can be found. I am not sure how they solve this legal problem nowadays, but one guess is that they rely on [DHT](https://en.wikipedia.org/wiki/Distributed_hash_table) so that all you need is the addresses of a few other machines.

There is not much more to it, really. The hard part is to stay within the bounds of the law when so many of your users are doing something illegal, and possibly to keep your own identity hidden if you don't want to get in trouble (this was not a concern for the original team).

This is how The Pirate Bay have done it. It is important to note that The Pirate Bay is a political lobbying project (or was...). That is why they have been open with their identities, being transparent with how they host stuff and how they believe they are acting within the law. Note that they do not try to be an illegal website!

If *I* had an illegal web site I would do it completely differently. I would use Tor to hide my servers and my identity, which seems very effective. Remember Silk Road? It took a very long time for authorities to track it down, and they only succeeded because the admin was a dumbass posting on a web forum about similar activities. I would pay my hosting bills with Bitcoin from wallets that are only used for this purpose. Like The Pirate Bay I would keep backups of the dataset, possibly even physical copies hidden in the forest or something. My computers would be encrypted with very long passwords and I would use trusted VPN services (possibly also routed over Tor) for all Internet access regarding the project. I would also lead a separate normal life like a web developer who earns his money fair and square, complete with social media profiles and a Reddit account to show how normal I am.",1523616800.0
sw2de3fr4gt,Check out [Bulletproof hosting](https://en.wikipedia.org/wiki/Bulletproof_hosting) and [Fast Flux networks](https://en.wikipedia.org/wiki/Fast_flux),1523596663.0
tevert,"At one point I heard they were kicking around the idea of weather-balloon servers that just couldn't be tracked, but I'm guessing the cost of that wasn't worthwhile. 

I'd probably try to design it in a viral fashion. Sweep the web for unsecured Wordpress servers and whatnot, throw docker containers on them, and find a way to mesh network them to process requests. 

From a datastorage standpoint - this is actually one of the few cases where a blockchain would make some sense. Highly distributed, impossible to tamper with. I'd probably have a few real databases that actually field requests from the website (for the sake of speed), but a blockchain could provide a more permanent, reliable backing for that. ",1523586953.0
CyAScott,Tor supports the concept of a [hidden tor service.](https://www.makeuseof.com/tag/create-hidden-service-tor-site-set-anonymous-website-server/) It allows tor clients to connect to the server without the client being able determine any identifying information about the server.,1523599440.0
eek04,"I do (legal) high availability work.  It would really depend on what kind of functionality you have on your illegal website.  For instance, if I just needed to serve small amounts of mostly data, posting encrypted data to one of the blockchains would work for storage.  If I needed something with fast updates or substantial etc, I need more centralized servers.

For the latter, it seems to be common to have servers in several locations, with some of them being semi-hot standbys.  People also keep around multiple domains, and have a top level domain that redirect.

I've not seen any illegal website that really has high availability, though. For a website, I'd consider that to start at 99.99%, which translates to 52 minutes downtime per year - and usually you smear that downtime (so the site will be down for some users some of the time, but not for all users hardly ever.)",1523610659.0
Conroman16,"These days you can hide behind a service like cloudflare for the security and anonymity aspect, especially now that they’ve released Agro Tunnel.  Then for the actual web portion I imagine you could hide in a few cloud providers around the net and maintain a highly available IP+replication between the hosts.  If you can keep your traffic completely encrypted from your edge to cloudflare, theoretically you’d be undiscoverable unless they were compromised or decided to have a change of heart on privacy.  Some notorious torrent sites already do this, as I’d imagine other sites do as well

If you’re doing something super bad though you’d probably need to get behind TOR as a hidden service and be extremely careful about how your manage your infrastructure so as to not reveal your true identity.  Even then it’s still theoretically possible to track packets through TOR and other encrypted networks via exit entrance/exit points, etc. so if you’ve got a lot of traffic you’d need to figure out how to mask that.  I like to think of it as a bunch of identical armored cars traveling in a pack.  You don’t know whats in them but you can see them and can fairly easily discern where they came from and where they’re going with a reasonable amount of effort

I think this is a pretty interesting thought experiment.  I’d love to see what a competent CySec professional and equally competent enterprise architect could come up with on this topic",1523601793.0
digitalice,"I think users connect to a reverse proxy, and that reverse proxy connect to the core servers via VPN. I could be hosted anywhere. The hosting provider will just see encrypted VPN traffic. ",1523593527.0
sailorcire,"Would I? No.


But remember r/EmpireDidNothingWrong",1523582823.0
Prof-,What are your favourite websites to get CS news/learn about cool and innovative tech?,1523597479.0
bushbud_lover,"What can a self-taught employed SWE do to get research experience and rec letters for graduate school? 


Current plan is to work for google and be a SWE for a research team. Good/bad plan?


Background:

I have a 4.0 from Rutgers mechanical engineering, and a 164/169/4.5 in V/Q/W. I work for a government contractor as a SWE since August. I got rejected from Stanford/CMU/Princeton for a masters program.",1523993508.0
drifty-t-sleep,Take discreet mathematics and data structures before operating systems. ,1523571814.0
Prof-,I would take a data structures class early on and take more upper year data structure classes as you move on. A bulk of interviews will be testing your knowledge on them. I’m sure your school wouldn’t let you take a few classes without knowing your basic data structures first.,1523574752.0
magnificentbop,"Theory of computing, compilers, and computer architecture/organization are conspicuously absent.  Are they offered?",1523582219.0
zetsubou-tan,"“High-level programming” 
“C”",1523571519.0
HotSuccess,That seems like a pretty solid degree to me.,1523572531.0
brettmjohnson,You are taking algorithms and data structures too late. Definitely do it before any OS/compiler internals. Maybe even before OOP (C++).,1523651501.0
,What do the computer graphics courses entail? ,1523579819.0
sailorcire,Do your English and other gen eds first. They are boring.,1523574683.0
pulsar512b,"Why no need for Python, or Java, or any other half-decent programming language????",1523572536.0
harrywilde,"These are pretty basic concepts in terms of a journey through mathematics, I would disagree with the top comment and say that depending on your background it could offer a good base upon which to reason about some key concepts of computer science like algorithms etc.

In my opinion a lot of CS undergrads suffer for their lack of knowledge in maths. Although in this case you could probably read through the material in your spare time given it has already piqued your interest and get a similar benefit.",1523563920.0
thedude42,"I really gained a lot of insight from my intro to number theory class when I was doing my CS undergrad. It really opened me up to a lot of thought about what numbers were and why certain algorithms worked. I feel like a lot of people I have encountered in industry who haven’t been exposed to this stuff have a lack of approachability to certain topics, or even believe things that just outright wrong about computing, by not having some of this background.",1523566424.0
toastyoats,"I have studied quite a bit of number theory, and I do statistical programming now, but I'd argue there's not a whole lot of utility in learning number theory *for the purpose of computer programming*. Some concepts come up in programming which have roots in number theory like modulus and integer factorization, but the vast majority of number theory has no direct application to programming. 

That said, I love number theory. I think it's a lovely subject and quite fascinating. I think the fact that the Collatz conjecture or the Twin primes conjecture remain difficult even in the modern mathematical landscape, despite how simple they are to state, encourages a certain reverence for the complexity of the universe and abstract ideas. I love the big mystery of what's out there in the abstract world, and I think number theory is a great way to dive in. 

But I wouldn't ever recommend number theory for an aspiring computer scientist looking to sharpen their skills. 

However, a fun intersection I can recommend for such a person is to do [Project Euler](projecteuler.net) problems! They're usually elegant and simple to state problems with a number theoretical flavor. 

I hope you enjoy the class if you take it!",1523565646.0
bgard6977,"If you want to be an engineer, the useful stuff is understanding modulus math and primes, and how that fits into hashing algorithms, random number generators, and cryptography.

These are useful foundations. Or you could just read this and maybe write the implementation in your spare time:

http://www.i-programmer.info/babbages-bag/271-public-key-encryption.html

Also, check out The Cukoo's Egg and Cryptonomicon.",1523571780.0
GNULinuxProgrammer,"More math is always good and number theory is very fundamental. Also, number theory has a lot of applications on cryptography/computer security. As a general rule, if you're in a good CS curriculum you can take as much Math/Stats as you can without *any* hesitation because your curriculum will require you to take a lot of CS classes that'll teach you apply math (so, generally, you can't accidentally end up ""too theoretical""). This also includes CS and EE theory classes such as algorithms, theory of computation, optimization, linear algebra, probability etc...

I think going to a good university and just taking ""programming"" classes is a very unfortunate waste. Software engineering will be learned while you're working as a software engineer, so a few classes that prepare you will be enough. Math and applied math on the other hand is way harder to learn after college. And CS papers require a lot of math maturity. The most important thing after an undergrad degree is being a good ""assistant"", a good ""learner"" and without a certain amount of math maturity it's really hard to learn CS from papers, which will fundamentally limit your quality.",1523581503.0
Captain_Flashheart,"Oh, there are tons of jobs in that.. just like there are many jobs requiring the pumping lemma! (This is sarcasm, but hear me out)

It's not required no, and I have never heard about that coming up in a job interview, but it gives you a good basis for understanding later concepts. Think about the various protocols used today for wireless communication. You'll see applications in the security field, which gives you more job options in the end. ",1523565921.0
klowny,"None of it is useful for general programing and especially not for software engineering interviews. 

It's a nice intro if you want to be an academic in cryptography though.  ",1523563018.0
Prof-,These were taught in my discrete mathematics class. Do they offer that at your school?,1523569579.0
pastermil,"Generally in decent school, they'd cover all the math stuff you'd need.

Different story if you wanna get into something specific.

Example: I took Artificial Intelligence as elective. Linear Algebra and Statistic was required. However, the teacher ended up spending a week or two covering for the basic math stuff, which I deem unnecessary since I had taken these two.

If you're taking something like cryptography, this might be more useful. Other than that, there's really no harm in taking more math class (given enough resource, of course, which is always limited :D )",1523577363.0
DTH97,"If you are interested in it on the basis that is used in cryptography, also look at Abstract/Modern Algebra.",1523588717.0
Kapps,"It's not remotely practical for programming purposes. Statistics is definitely the smarter choice from a practicality perspective.

That being said, I found Number Theory to be a very interesting class and the only Math class I enjoyed.",1523600558.0
drvd,"> this would even be useful for general programming or even interviews

This should never be a question. If all you ever do is just because it helps in interviews you'll fail.",1523963375.0
michaelochurch,"I would say that you should take the course. It's interesting and if nothing else it's often used as a source of interview brainteasers. Cryptography uses it heavily. You're not going to encounter it doing web apps, but it's still worth knowing. ",1524238245.0
xiipaoc,"Number theory is *famously* completely useless.  Gauss called it the queen of mathematics or the jewel of mathematics or something like that, because its only purpose is to further our understanding of *itself*.

That said, solving number theory problems algorithmically is a good use of your time and effort.  If it interests you, do it.  If it doesn't interest you, then why?",1523584516.0
albenzo,"Almost certainly college. While it is definitely possible to teach yourself and become just as good there is a heavy survivorship bias. Most people who teach themselves will bounce off of it without actually learning much. 

College will also (ideally) give you more chances to receive good feedback and expose/force you to learn topics you otherwise would not have seen that may be important.",1523554816.0
Gavcradd,"Don't think of college as a golden bullet - if you just turn up, do the classes and sail through, you'll be nowhere near ready. College requires a hell of a lot of self-study. The only thing that college does is give you that shiny bit of paper that employers want.

The answer really is both - college for the recognition, but self-study to actually get good at it.",1523554861.0
hendawg98,Not having a degree makes it magnitudes harder to get anywhere. Most jobs require a degree to even get your foot in the door,1523559645.0
onfire9123,"Even Aristotle needed a teacher. Having a mentor or guide is always superior to going it alone. 

Think of it this way, who gets to a destination faster and avoids the pitfalls? The pioneers that don't know where they're even going, or the followers that use a map after it's made?",1523559973.0
hippomancy,"Is your goal to be a master programmer or a professional computer scientist? If you don’t understand the difference between the two, you probably should go to college and figure it out.

CS doesn’t actually have a lot to do with computers. It’s much more about the conceptual side of how algorithms/data structures work and why systems we use look the way they do. Programming, on the other hand, is a mechanical task that requires a great deal of problem solving skill, which college will help with, but you’ll mostly have to learn on your own.

If your goal is to get a job as a programmer, college isn’t as good as a lot of practice and a good portfolio. If you want to set yourself up to do more advanced work in an area like theory, data structures, operating systems, compilers, AI, etc. you have to go to college. If you don’t know, going to college is a better default, since it’s hard to learn those sorts of things later in life.",1523560448.0
FatherWeebles,I learned a lot through self-study for about 6 months.  School is a whole other animal.  It adds a ton of pressure that'll force you to learn more in short amount of time than self-studying ever could in my opinion.,1523561797.0
fiftyplusnineteen,"College.  Yes, you can teach yourself, but college provides a structure for you. You have teachers and ta’s to help answer your questions. You’re meeting people who are studying the same thing, and most importantly, the rigor of academia will be hard to emulate on your own.

Good luck! ",1523555262.0
,"Relevant xkcd and its sequels:

* [Code Quality](https://xkcd.com/1513/)
* [Code Quality 2](https://xkcd.com/1695/)
* [Code Quality 3](https://xkcd.com/1833/)",1523555998.0
WHallendy,"That depends on your application and definition of ""master"". Even college demands a good amount of self-study, but college puts all the tools and resources at your fingertips. You just have to be willing to commit financially.",1523554885.0
,"College + internships. College will teach you theory, but internships will teach you the practical skills to become a good developer. Self-study will help you get the internships, by the way.",1523555839.0
paulthewall3,Not doing your homework for you,1523543602.0
PuffPipe,"For the graph to be planar, you cannot have any intersecting lines (exepct at the endpoints of course). Do you?

Edit: I see that you have an idea of what you're doing so disregard my comment. Good luck!",1523543813.0
its_ya_boi_dazed,Downvoting bc homework ,1523544941.0
Decent_Compound,Please fuck off Captain Cuck,1523544984.0
HelloAnnyong,"tl;dr: one is an alias of the other, they can be used interchangeably.

also this does not seem like it belongs in /r/compsci at all",1523553504.0
buffdude1100,http://mooc.fi/courses/2013/programming-part-1/,1523509671.0
Captain_Price_777,"If you don't mind, I think in a CS subreddit, your post needs to be a little more technical. I couldn't really figure out from your entire post what exactly it is you'll be doing, or how you plan to do it. 

In your webpage, you said ""The plan will be for the device to do nearly all of the work by scanning a document and grabbing the critical information."" -  so you want to turn text into video? how do plan to incorporate user's preference into that? frankly, do you have any idea how absurdly hard it'd be to implement?",1523887738.0
Zulban,"One of the teams in the 2016 Darpa Cyber Grand Challenge released their project open source, and it's still active.

https://github.com/angr

https://github.com/angr/angr-doc/blob/master/HELPWANTED.md

https://www.cs.ucsb.edu/~vigna/publications/2016_SP_angrSoK.pdf

You don't need to be a master security expert to help. From their readme:

>We are always behind on documentation. We've created several tracking issues on github to understand what's still missing.

I recommend joining a community like that.
",1523491219.0
spacezoro,"Most of your early level CS classes will be pretty basic and security naive. It'll come in during your late 2nd or 3rd year. For study material, I recommend trying to get your network+ and security+ certs, then CCNA and Server+. 

Alot of places online have security challenges you can work on or offer virtual network labs for this kind of stuff. Look into maybe learning basic things like cracking WPA networks, doing packet analysis, exc. ",1523479794.0
CharlieWestAmn,"So it really depends what you’re defining at security. I’m in the Air Force in cyber security, that said, the large majority of cyber security doesn’t involve hacking because we’re primarily focused on defending the networks. The people who are on the offensive side, is strictly kept to the military bc of the nature of warfare in general. This is obviously when speaking to foreign adversaries. Other entities and Agencies do have contractors and agents that run similar programs however they can never lead actually “offensives”.
That said the I have a bachelors in cyber security in which the curriculum was literally 8 weeks courses studying material for CIH, CEH, CISSP and a handful of other common certifications. While I have a solid understanding of most computer operations, I’m not technically proficient. In my major, we kept to a focus on “security” and how it should be done in the cyber domain. I am not a help desk employee, in fact I’m the last person you would ever ask. We simply do risk assessments/categorization, and write, implement and enforce policy. 
Hacking and code, most DEFINITELY have their place in the security conversation, however it’s much smaller when it comes to the conversations that create change.",1523496904.0
xSwagaSaurusRex,"Start playing Capture The Flag (CTF)s, you can do them on your own to practice and read the write ups when you get stuck. They’re made as fun games and learning exercises and will give you a good taste of the breadth of the field. Best part is a lot of them run on their own servers so you can play them on any hardware you have. When you do this for a while you’ll naturally meet other people and can join a team.

Get on twitter and follow a lot of information security people, like those books you read; follow the authors and see what they talk about over time. 

Go to conferences , there are a lot of conferences everywhere for this thing and the people are generally nice. 

Getting certs and contributing to the open source tooling is one avenue, but IMO it’s more fruitful to internalize what you want to learn by being faced with hard challenges in the wild then going back to prove your knowledge / make your life easier with certs and tooling.

There are many entrances into the field and a lot of people who want to help out, all you gotta do is take it one step at a time, and meet people along the way. Definitely stay out of shady places though. ",1523500175.0
Redrick_Schu,"Getting your feet wet in cryptography is never a bad idea.  https://cryptopals.com/ is a popular set of crypto challenges that start simple and ramp up in complexity pretty quickly. 

https://ctf365.com offers free access to various servers to do some penetration testing. I've not played with this service too much, but it seems relevant to your interests.

Try some penetration testing, reverse engineering of protocols and binaries, and pretty much anything to do with the network stack. https://www.wireshark.org for looking through network traffic, and getting a good overview of how the internet sausage is made.",1523503239.0
tmarthal,"If you want, read the book ""Hacking, The Art of Exploitation"" - https://www.goodreads.com/book/show/61619.Hacking Your local library will have a copy, check it out there for free and read through it carefully. 

It's a good bridge between basic programming and advanced system techniques which are usually exploited. ",1523504201.0
kandovan,"Places like [Amazon Web Service](https://aws.amazon.com/free/?sc_channel=PS&sc_campaign=acquisition_US&sc_publisher=google&sc_medium=cloud_computing_b&sc_content=aws_core_p_control_q32016&sc_detail=amazon%20web%20services&sc_category=cloud_computing&sc_segment=188908164616&sc_matchtype=p&sc_country=US&s_kwcid=AL!4422!3!188908164616!p!!g!!amazon%20web%20services&ef_id=Wr6OqAAAAI-3ZXFX:20180411234331:s) and [cloud9](https://c9.io/) provide free VMs for you to use. 

You might also want to look at 

    https://www.reddit.com/r/netsec/
    https://www.reddit.com/r/netsecstudents/
    https://www.reddit.com/r/computerforensics/",1523490396.0
puppyStir-Fry,"If you can't figure out how to get there, how the hell are you going to protect the government? ",1523506068.0
Haunted8track,"Hack the Gibson, they’ll find you ",1523490525.0
SkoochMaGooch,"There are some free online challenges/ events and a program of summer camps (not sure if they’re free, but you have to qualify) through the US Cyber Challenge- a program geared specifically at getting cyber security talent into the Federal sector.

https://www.uscyberchallenge.org/

They have a social platform with a directory of different cyber activities and events called CyberCompEx.

https://www.cybercompex.org/

And there is a lot of work going on around NICE to get more cyber smart people into the Fed space.

https://www.nist.gov/itl/applied-cybersecurity/nice
https://nice-challenge.com/
https://www.sfs.opm.gov/

If you want to do cyber security, and are serious about doing it for the Fed (even just for a while, they don’t pay nearly as much as private), then there are some programs to help you work that exact career path.

[edit: links and details]",1523495202.0
PinkyWrinkle,The navy has a Cyber Security position. Cryptological Technician Networks it CTN for short. No college degree required ,1523498022.0
CrypticWriter,"Can you take data communications or application development classes? Those are very important things to understand in the security world. If possible, take those.

Before you can do network security, understanding networking fundamentals is of course very important.

If you can't take stuff at uni, I'd look into basic networking, shell scripting, traffic analysis with tcpdump/wireshark, IDS/IPS systems and maybe some socket programming to understand the basic functionality of client/server applications. 

Malware analysis is another area that is good to look into. For malware analysis you first need to learn the basics of assembly and then you can perform static analysis of malware using tools such as IDA or solving crackmes with OllyDbg. If you want to learn assembly check out the NASM assembler. There are some good tutorials online.

Also check out some cryptography. Know how to write and break basic substitution and transposition ciphers, and know how to set up stuff like encrypted email with a public/private key pair. Knowing stuff like digital signatures, digital certificates, hash functions and so on are all good too.

Best of luck! If you have any questions feel free to let me know",1523513823.0
mircatmanner,"Pretty much every comment covered everything you need to know. I’m also pretty much at the beginning for CS and doing a focus on Cyber Security. So mostly everyone talks about certification and if you’re not doing anything this summer (vacation, jobs, internships) study for those certs and maybe try to get one. Also when you’re going to be looking for internships, keep yourself open to both IT and CS. I was looking through some InfoSec jobs in my area and quite a few of them had preferred experience in IT.",1523514408.0
CharlieWestAmn,"It’s very interesting stuff, intelligence is becoming huge (its extremely difficult in the cyber domain, therefore that much more desirable) so focusing on cyber security and possibly getting experience in intelligence fields would make you a highly desirable hire for Canada or the US.
Intelligence in the kinetic battlefield is “relatively” simple whereas getting predictive intel is common for forces as large as our countries and their capabilities.  However predictive intel in cyber is extremely difficult and we’re still paving the road for it.",1523574807.0
rainaezn,It’s 7,1523489514.0
mendeza503,"Creator here! I am excited to share this AR application I made on snapchat's AR platform, and I wanted to see what people think, any bugs they find, and any doodles you all create!

Link to try the snapchat experience here: https://www.snapchat.com/unlock/?type=SNAPCODE&uuid=ecc379949d1341719936b365291b16d3&metadata=01",1523463014.0
iJubag,Noob question but how could I get this on my snapchat? ,1523464994.0
tooters_united,I really wanted to see you walk around the painting. What happens when you look at it from behind?,1523465002.0
AlphaOctopus,"This is really awesome!

Any tips for somebody looking to get into AR development?

How did you start and become proficient enough to make something like this?

Really inspiring, thanks for sharing ",1523469877.0
Deamonized,This is really cool! Is the platform open for anyone to develop on or do you have to have some sort of approval?,1523471426.0
YungBokChoy,Really awesome and a great idea. However it's very hard to switch brushes and colors. When I click a button it just ends up painting instead of selecting. I'm on a galaxy 8. ,1523476829.0
turne22m,"This is incredible, great stuff!",1523479693.0
Carpetfizz,are you using optical flow to determine the relative camera position at each timestep or is there some other technique? looks nice,1523484473.0
vacwm,Input is an optional parameter to pass data related to your problem. It may be additional constraints or data that says something about a[].,1523455069.0
sailorcire,"Write a multithreaded webserver that dishes out data from a doubly linked list.

That should be a good enough to get you back in the swing.",1523450324.0
DrFiveTheHiveMind,"Cracking the coding interview 6th Edition by Gayle McDowell is a pretty good code practice book with answers on Github to double check your work. Additionally, there are websites like Hacker Rank and Firecode that give you some practice. ",1523460802.0
aron4432,Good stuff -> http://www.shubhro.com/2018/01/20/brushing-up-os-c/,1523461878.0
teteban79,"You are right in that, if you have a finite set of words, there is definitely a FSA that accepts exactly that set. Your constructive proof is quite robust.

Alternatively, you can show that 

1) given two FSA A1 and A2, you can construct another one that accepts exactly the union of what A1 and A2 accept. This is trivial: just add a new initial state that nondeterministically advances to the initial states of A1 and A2. Here, you are saying that of two languages which are FSA-acceptable (let's go with the right terminology and say, two *regular* languages), then their union is also regular.

2) Any single word is a regular language and thus acceptable by an FSA. Basically a linear one accepting one letter at a time.

3) a finite set is just a finite union of 1-element sets

And you are done.

Regarding the second part of your question, any nondeterministic FSA can be transformed into a deterministic one (look into NFA to DFA). Their expressive power the same. However the size of the deterministic one can be exponentially larger than the nondeterministic one.

Now going back to the example of the set of coordinates that lie within Germany, that is effectively an *infinite* set (I am assuming arbitrary precision on coordinates), so your proof would not apply and there isn't, necessarily, an FSA that accepts those and only those (one could exist though, depending on exactly how the set looks like).
",1523448577.0
spacelibby,"There's the standard ""make an NFA and convert it to a fsa"", but you can make your proof formal too.

You can make a trie out if your set of words. This trie can be the transition function for your fsa. ",1523473823.0
ITwitchToo,"vigorous proof? As in ""proof by intimidation: proof by vigorous handwaving""? :-P",1523453164.0
,[deleted],1523448493.0
pm_me_great_stuff,"I guess you defined an/your alphabet to be finite? If so there are 2 proofs I can think of:

1. Your tree automaton can be constructed by induction over the length of your word. qed

2. If you've already proven that for every non deterministic FSA there is an equivalent deterministic one just construct the non deterministic one as you mentioned and use the theorem to get a deterministic one. qed

The second one surely will be shorter but both are so basic that I don't see a point in more simplification.",1524070468.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1523442824.0
Maeher,3-Coloring being NP-complete by definition means that 2-coloring can be reduced to it. Literally the definition of NP-completeness is that the problem is in NP and all NP problems can be reduced to it.,1523431665.0
GiuMaz,"Yes because 2-coloring is NP so you can reduce it to an NP-complete problem. The reduction is easy, you keep the same graph and add a new supernode with an edge to every node of the original graph.",1523431973.0
CyAScott,"I use to use Excel to create my truth tables. Excel has som functions for converting base 10 to base 2 and back again. Simplifying the results is going to be a nightmare. Truth takes are better for finite state machines.

There are a few designs for multipliers that are very popular that are pretty simple.",1523421263.0
green_meklar,"This script can generate the table for you:

    data:text/html,<script type=""text/javascript"">
    var bitcount=4;
    function td(a,s)
    {
     a.push(""<td>"");
     a.push(s);
     a.push(""</td>"");
    }
    function go()
    {
     var limit=1<<bitcount;
     var pbitcount=bitcount*2;
     var ta=[];
     ta.push(""<table>"");
     ta.push(""<tr><td colspan='""+bitcount+""'><b>Operand 1</b></td><td colspan='""+bitcount+""'><b>Operand 2</b></td><td colspan='""+pbitcount+""'><b>Output</b></td></tr>"");
     ta.push(""<tr>"");
     for(var bit0=bitcount-1;bit0>=0;bit0--)
     {
      td(ta,""bit ""+bit0);
     }
     for(var bit1=bitcount-1;bit1>=0;bit1--)
     {
      td(ta,""bit ""+bit1);
     }
     for(var bita=pbitcount-1;bita>=0;bita--)
     {
      td(ta,""bit ""+bita);
     }
     ta.push(""</tr>"");
     for(var op0=0;op0<limit;op0++)
     {
      for(var op1=0;op1<limit;op1++)
      {
       ta.push(""<tr>"");
       for(var bit0=bitcount-1;bit0>=0;bit0--)
       {
        td(ta,(op0>>bit0)&1);
       }
       for(var bit1=bitcount-1;bit1>=0;bit1--)
       {
        td(ta,(op1>>bit1)&1);
       }
       var ans=op0*op1;
       for(var bita=pbitcount-1;bita>=0;bita--)
       {
        td(ta,(ans>>bita)&1);
       }
       ta.push(""</tr>"");
      }
     }
     ta.push(""</table>"");
     document.getElementById(""output"").innerHTML=ta.join("""");
    }
    window.onload=go;
    </script>
    <style type=""text/css"">
    table {border-collapse:collapse;}
    table,tr,td {border:1px solid black;}
    td {padding:6px;white-space:nowrap; }
    </style>
    <a id=""output""></a>",1523428480.0
Caelestialis,"Thanks all for your help, I appreciate it.",1523479571.0
LongUsername,"This really sounds like a homework problem. 

It wouldn't really be a ""Truth table"" but more of a ""look-up table"" if I understand your description.",1523462124.0
Sharif_Of_Nottingham,"Write it out. Start from the ones place of both numbers and look at the truth table for multiplying just those bits. Then do the same for each other pair of bits, and then you can mix in adders for the necessary columns.",1523426335.0
Kudbettin,"Nah, you just stash them into a corner. ",1523418221.0
xhable,Paint by numbers on a phone app? weird.,1523349038.0
Ramin_HAL9001,This is generally why I don't like arcade games unless there is an element of timing and an element of randomness to it.,1523349517.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1523296964.0
ColdPorridge,"IMO these type of posts should be downvoted without a submission statement. It's lazy posting and can feel like spam. A link with no context in such a broad sub is not contributing.

That said, with proper submission statements to kick start discussion, these types of posts could be great.",1523305037.0
gammison,"You'd probably find more info on how Mac OS X differs from Linux in general. Hardware wise they're operating systems that behave in close to the same way, I'm not sure on differences in kernel implementations. As far as I know OSX is originally openBSD based however most of the kernel has been rewritten at this point. You can check out more here: http://en.wikipedia.org/wiki/BSD

Ubuntu like all Linux distributions uses the Linux kernel. Most programs written on Linux can be ported to OSX without as much difficulty as porting to Windows. It'd be interesting to look at differences between the different kernels. One big one is that OSX doesn't have /proc. They're both based on Unix (I don't think Linux is technically Unix but OS X is?) and follow it's design philosophies. Some big ones are the shell, chaining together small programs to do complex tasks, and having a sort of pseudo file system for things like interfacing with hardware. A user may not experience these as much as a developer.

Software wise both systems have features like automatic updates (Ubuntu, not Linux in general) and stores for verified software. Ubuntu however is more open by default with access to various repositories through a package manager called apt. OSX does not come with open source repositories and a package manager by default. There are also desktop enviroment/GUI differences. Right now, Ubuntu has switched to Gnome and OSX uses a system called Aqua. Gnome can easily be replaced while I'm not sure on the viability of replacing the desktop and window management systems on OSX. Ubuntu used to ship their own enviroment called unity but it is no longer used as of Ubuntu 17 release. You should look at application differences between gnome's defaults and aqua. Apple has certain parts of the user enviroment locked down. Both operating systems use a shell called bash which launches on startup, and behave mostly the same. Linux coreutils like grep and sed may be missing or perform differently since they are not compiled using the GNU compilers and specs Linux coreutils use. You can actually install GNU versions but I wouldn't recommend it. There are also differences in the ways each OS handles updates.

Security wise, look at the kernels. I don't think you can modify the kernel on OSX much beyond adding device drivers. Principles like restricting access to kernel memory and having virtual memory for tasks should be mostly the same. Security from a user perspective like malware I'm not very familiar with. I think OSX doesn't allow programs to be installed from just anywhere by default. The lack of trusted repos by default is also annoying but remedied by the use of homebrew.

Also, just a heads up,  this subreddit may not be the best place if you're looking more for software and ux differences.

",1523260191.0
,[deleted],1523244900.0
BlckHorizon,"I don’t see anywhere were it claims to be polynomial time? You can still write code that is optimized enough to solve NP-hard problems on relatively small instances, without contradicting the presumed hardness of the problem.",1523245103.0
helot,"Concorde is an exact solver. For NP-hard problems either you have an exact method that you try to engineer to run fast enough in practice, or you have a polytime algorithm that you tune to hopefully get good enough solutions in practice. In CS for some reason (obsession with easy to analyze worst-case performance guarantees?) the latter approach gets a lot more attention than the former approach.  Successful examples of the former include the simplex algorithm, and integer programming solvers ",1523250196.0
iwantashinyunicorn,"Just because it's NP-hard doesn't mean it's hard to solve in practice. There's a whole other area of empirical algorithmics concerned with how you solve NP-complete stuff exactly and quickly on instances that you actually care about. For many ""hard"" problems, it's extremely difficult to get exponential behaviour to occur in practice with a good solver, and when it does occur it's only on deeply weird instances that are unlikely to occur from an application. See, for example, [this recent paper](http://www.jair.org/papers/paper5768.html), or [one of the first papers on the subject](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.97.3555), or [the annual SAT competitions](http://www.satcompetition.org/).",1523283221.0
codeAligned,"NP-complete doesn't mean ""unsolvable"" it just means that no fast solution is known, usually this means it takes exponential time. But the definition of NP-complete  actually isn't ""the problem takes exponential time to solve"", because there are a few exceptions.

(Actually NP completeness is a pretty formal definition that means a lot of other things but generally equates to the problem is hard when used in industry)",1523297630.0
halfmasta,"One thing I don't see mentioned yet is that Concord is ridiculously optimized.  Their heuristics for solving a TSP are the result of decades of academic work.  I've used it a handful of times, and their first proposed solution is often optimal, even for graphs up to size 200- the traveling salesman sample library also has data about how it's solved problems on the order of several thousand nodes.",1523293318.0
Tiger_Testicles,"If you have any projects you made you should include it on your resume. Generally they will ask about what you did and make sure you know how it works. Interviewers generally don't care about the code itself, too much to look through. They love if an interviewee contributes to open source. Shows that you can work with a group, so I suggest getting comfortable with GitHub and looking through others codes even if it is to fix a spelling mistake. Then some questions specific to a language. ",1523211563.0
lism,"I personally don't think they will go too deep programming and code. Why would they do that? that's what you're going to learn right? They will probably ask you about your maths background, why you want to do the course, your previous academic achievements.

There's another answer here talking about showing them your projects. Yeah that's great if you've got them but don't worry if you don't, you're not trying to get a job yet so it's unrealistic to expect students to have a portfolio. 

You mentioned that you've done a Java course in the past. That's great and is going to be more than enough knowledge to convince the interviewer that you're a decent candidate.

All that I have said comes from me having done what you're thinking of doing right now at a UK uni. Since you used the word university and not college I'm assuming you're not in the states so you should have a similar experience.",1523217508.0
bajuwa,"What exactly is a ""mature student""?",1523211008.0
k110111,I had an interview last week they asked what are pillars of oop and how can you apply them to a printer individually. They said students often memorize definitions but don't know the concepts,1523214406.0
bobrodsky,"What level are you applying at? Undergraduate, masters, phd?

Computer science isn't programming. (Though it is possible to focus on this aspect within a CS program.) I interview potential phd students and I look more for mathematical ability than programming ability. It doesn't take very sophisticated coding to do many types of CS research. 

Also, I've seen many older students applying to CS Phd programs after some industry experience. So I don't think you'll stand out too much from that point of view, and certain types of experience are viewed quite positively.",1523220611.0
GayMakeAndModel,"As an entry level candidate, they are going to want to know about your work ethic, how much you’ve studied independently, how well you can learn new things independently, and whether you are a good fit for the team.

Being able to understand programming should be a given, so I won’t address that here.  If you don’t know something, don’t bullshit the interviewers.  It’s 
a waste of their time, and it shows that you are unable to come to grips with not knowing everything.  As an interviewer, I expect that there is at least one technical question I will ask that sends you to a Google.  So don’t feel defeated if you don’t know what a forward record is in SQL Server.  How you deal with not knowing the answer matters a lot.  Ask the interviewer for the solution and talk through it.

Edit: every interview is a learning opportunity ",1523220759.0
Sitras,"I'd highly recommend looking through https://www.amazon.com/Cracking-Coding-Interview-Programming-Questions/dp/0984782850/ref=dp_ob_title_bk when doing interview prep. It specifically has a section on Java and has questions related to many ""gotcha's"" and trivia questions related to java. In additional to technical skills it also has amazing prep material for soft skills and how to talk about the projects that you have worked on.",1523237616.0
Cleanumbrellashooter,"I haven't heard of the interview you are describing. But if it's about Java, I'd recommend knowing your basic data structures, and where each of them excel. Then of course your standard Java OOP stuff; inheritance, polymorphism, etc. Then beyond that I'd say maybe practice a few white board algorithm problems in case they decide to do some white board coding.",1523263781.0
Bizarre_Monkey,"I see elsewhere in the thread that you were considering applying in the UK so here's my experience. 

I'm currently a mature (late 30s) CS student at a decent UK University.

My high school grades were atrocious when I was a teenager. I wouldn't have gotten close to the entry requirements for my course.

I had an interview in which they discussed my reasons for wanting to study with them. I also had to write a brief essay to show my writing level and I had to demonstrate that my maths was at an appropriate level.

Most universities in the UK will assume no previous CS knowledge for an undergraduate CS degree and will start from a good maths foundation only. ",1523270074.0
rbobby,"I would expect the interview to be more ""soft""/""social"" questions. Things like:

Why do you want to come to this school?

How will you ensure that you complete your degree?

Describe a situation where you accepted mentoring from a younger person.

Describe a situation where you mentored other students?

What happened with your previous academic career and how will you prevent a repeat of it?

What will you do with your degree after graduation?

Why should we accept you over someone else?


Maybe not... but best be prepared.

Good luck!!",1523273907.0
BenjiSponge,"/u/malavv covered why O(n log n) is worse, but your question isn't clear on you mean when you suggest someone claims O(n log n) is ""bad"". To be clear, ""good"" and ""bad"" are subjective qualifiers, and a time complexity is neither without context. There's only ""better"" and ""worse"". O(n log n) would be fantastically efficient for some algorithms and confusingly awful for others. And for some algorithms, it might not have the lowest time complexity, but it might be the more efficient solution given the data sizes and data it's operating on.",1523193210.0
_georgesim_,"I know which chart you mean and it's a steaming pile of garbage. Not only is O(nlogn) *very good* for most problems, it's the *best possible* running time for certain problems, such as comparison sorts.",1523315128.0
SelfDistinction,"As usual, it depends. 

On the one hand, you have good algorithm running in O(n^2 2^n ) time. Yes, it's the DP solution for the Travelling Salesman Problem, and it's a huge improvement on the original O(n!) brute force search. 

On the other hand, you have bad algorithms running in O(n) time, for example a lookup. This can be done in O(log n) for finding the element closest to a number, or O(1) for finding the exact element and failing if it isn't included.

However, an interesting remark can be made about O(n log n) algorithms: in theory they're as efficient as O(n) algorithms on small data sets, and only a few factors slower on large datasets. 

In practice, x-log-n algorithms (binary search, heapsort) fuck up your cache, so even when n = 100 you can experience a factor 50 or more slowdown.

And as always, there's a difference between working 40 years in your life and working 120 years, even though n is only 1000.",1523201795.0
jet_heller,"The thing about big O notation is that the most important thing about it is that it's comparative. It's used for comparing algorithms and classes of problems, not for providing an absolute about any single thing.",1523199726.0
korthaj,"The Big O notation captures the growth of functions and deliberately hides any multiplicative constants. For most any practical purposes in CS, log n behaves like a constant function. In fact, if you measure the performance of an algorithm with time complexity Θ(n log n), you'd be hard pressed to distinguish the graph from a straight line using statistical methods. That's why it's highly misleading to claim that O(n log n) is much worse than O(n). In practice, they are pretty much indistinguishable. The actual constants involved are much more important.

I know it's futile to try to correct the internet when it's wrong, but let's give it a try. :)",1523193764.0
basyt,nlogn is better than n^2 but objectively worse than n. at least that is my understanding.,1523202344.0
malavv,"O(n) is strictly better than O(n log n). Therefore, it isn't better in small nor in very large samples.

ex. n = 1000
O(n) means the algorithm will run in a time proportional to the number of elements, here 1000 unit of time let's say. 
O(n log n) means the algorithm will run in a time proportional to the number of elements **times** the log number of elements, here 1000 * 3 or 3000 units of time.

You can argue that in a specific situation the O(n log n) algorithm has less constant cost, but this is beside the notation. You may also argue that 3000 is not much bigger than 1000, but still O(n) will always be better regardless of the size.",1523191881.0
panda_burgers,"It's feasible, but you need to define the constraints for the GA (runtime, fitness evaluations) to make a meaningful comparison.",1523171097.0
crubier,"Yes it is. And I have been doing that for the past few months. We had a problem were we had polyhedral obstacles in 3D space, and needed a way to go from point A to point B.

We used 4 or 5 different algorithms with the same interface:

 - a traditional grid-based Astar
  - pro: relatively fast, and the speed is tunable by adjusting grid size
  - con: paths are not optimal because moving along a grid
 - an Astar algorithm whose nodes are not arranged as a grid, but are actually vertices of the obstacles polyhedron. 
  - pro: get an optimal solution
  - con: can get really slow as each node has a large number of neighbors (large branching factor)
 - a pure GA solver
  - pro: can be fast, and speed is tunable
  - con: solutions can be suboptimal, result is not deterministic
 - an Astar working on a simplified problem followed by a GA optimizer working on the real problem
  - pro: fast
  - con: not much !

We were able to benchmark these different algorithms, with 100-fold difference in performance in some cases !

Also GA become really useful as compared to astar when you start adding constraints to your problem (timing constraints along the path or other) ",1523178470.0
lavahot,"Depends on what you're trying to do. If you want your GA to come up with some algorithm that is more optimal than A* in certain cases, that's probably possible. If you're going to compare the runtime of A* against the runtime of the generation of the GA, you're probably going to be disappointed.",1523172557.0
,"It's an interesting ""field"" if you want to call it that, *comparing GA's or other learned models to actual data structures*.

There's at least a good pile of research papers behind this, specifically I remember one last week that compared a NN to a hash function, with interesting results.

 So I'd recommend checking those out, seems to be in the same vein.",1523212083.0
BenjiSponge,"Definitely not the right place to ask this, if you mean time dilation due to the effects of general relativity.

1. What's wrong with /r/explainlikeimfive?
2. https://simple.wikipedia.org/wiki/Time_dilation
2. https://www.reddit.com/r/askscience/",1523157164.0
Oriumpor,How about a direct link?  Not gonna run your random javascript clicksynergy...,1523149989.0
pulsar512b,"I find it very useful as a CS student. If I don't know how, say, to convert an ArrayList into an Array (i actually did that once as part of a group project) I search google, get 1 result that is useful, and it is stack overflow. ",1523137355.0
IamConor21,I have mixed feelings on it. For simple questions that have already been answered they can be useful to find the method you forgot. And especially with how poor oracles java documentation is its a life saver. But asking questions can be a nightmare. ,1523160992.0
Microscopian,"I find SO extremely useful if you write a good question. I know the rules have changed since it started, but this was largely to stop the epidemic of argumentative posts and ""do my homework""s.

If you're getting an error you have to post enough info for someone to replicate the problem or they can't help you. If you get flagged as a duplicate, read the answers there; chances are it really is a duplicate.

Votes are from other users, not mods, so if you're getting downvotes maybe the question isn't really as good as you think it is. Post a link to your question so we can see.",1523147061.0
gvs311,"I've had Information Retrieval Systems this semester. One of the methods is indexing by domain experts. Another method is indexing unique keywords. After that you have a number of methods to decide whether a document is relevant or not (Boolean method)or how relevant it is(Vector model). 
For dealing with multiple signals there are methods like Boolean Query to deal with those signals as well. You can make the documents which are associated with premium payers rank higher within your algorithm. 
If it helps, Modern Information Retrieval by Yates and Neto is a very good book.  
 ",1523175695.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1523121740.0
Tandrial,"This is a pretty good intro: https://cs.stackexchange.com/questions/2155/how-to-read-typing-rules

",1523116607.0
DonaldPShimoda,"I’m on mobile so I’ll try to keep this short.

The top line of each rule is just a name.

Normally you see gamma used instead of theta, but it represents the “current environment”. I will use G as a shorthand.

Let’s look at T-Plus. It says: “If in the environment G the expression `e1` has type ‘Int’ and expression `e2` has type ‘Int’, then in that same environment G the expression `e1 + e2` will have type ‘Int’.”

So it’s just a listing of the various type rules, and how they are derived. The ones up top are simplest and you move downwards. The part above each line represents the prerequisites, so to speak, and the part below the line shows the results. You can have one part below the line, and for each sub-expression in that part you must have a corresponding part above the line to justify their type.

Expressions which do not match these rules (like if you tried to add a Bool with an Int) are not well-typed, and should likely throw an error or something.

The small Ts are taus, and they represent “some type”. It’s a type variable. So an If-expression evaluates to the same type as its two branches evaluate to (and they must be of the same type).

I think that’s most of it. I learned this from a class in operational semantics where we followed the book “Semantic Engineering with PLT Redex”. I imagine these things would also be learnable from eg TAPL or something, though I’ve not looked into it much yet.",1523116757.0
sogregersen,"As I'm the author of the set of typing rules, feel free to send me an email or swing by my office, if you need any help. I'll be more than happy to explain - I'm even getting paid for it! You'll definitly know which TA I am by glancing at my nickname.",1523130771.0
cgibbard,[Types and Programming Languages](https://www.asc.ohio-state.edu/pollard.4/type/books/pierce-tpl.pdf) by Benjamin Pierce is a good book to start with if you want more than the descriptions given already.,1523127568.0
ryani,"So you probably have seen some people throw around the term 'syntax-directed' in this thread.

This means that there is only typing rule applicable in any situation.  For example, look at T-IF; it's the only rule listed here that can possibly apply to an expression of the form `if (e1) e2 else e3`.  So if you want to prove that expression is well-typed, you only need to look at that particular rule, there's no 'search to figure out which rules could apply'.

In order to prove the conclusion `Theta |- if (e1) e2 else e3 : tau`, you need to do 6 things:

1. Figure out what environment `Theta` you are trying to apply the rule in (this is probably an argument to your typechecker)
2. Figure out what type `tau` you expect this expression to be.  This could be the return value of your typechecker, or inputs into some algorithm which makes sure types match properly, or it could be an input to your function if you know what type each expression needs to have ahead of time.  The answer here depends on the language you are compiling.
3. Prove `T |- e1 : Bool`.  This will depend on the syntactic structure of `e1`, and will probably be a recursive call into the typechecker.
4. Prove `T |- e2 : tau2` for some type `tau2`.
5. Prove `T |- e3 : tau3` for some type `tau3`.
6. Prove that `tau2`, `tau3` and `tau` are equal.",1523143315.0
ProgramTheWorld,"Reading this brings back memories from college :’)
",1523122511.0
yawkat,"These are typing rules for expressions. The top part of each rule is the ""conditions"", the bottom part is the ""conclusion"". For example if x_1 : Int (as in, x_1 is an expression of type int) and x_2 : Int, then x_1 + x_2 : Int also holds.

(on mobile here, so no symbols) the big O is the ""context"". The T-on-its-side means that from the context (the lhs) you can get the type on rhs.

This collection of rules can be used to prove the type of an expression in fitch calculus for example.",1523116943.0
zokier,"This Guy Steele talk helped me understand the notation better:

[It's Time for a New Old Language](https://youtu.be/7HKbjYqqPPQ)",1523121587.0
agumonkey,"Also guy steele has been studying the use of similar metanotation in papers:

https://www.youtube.com/watch?v=dCuZkaaou0Q

it's not 100% consistent, so this talk might help you avoid confusion at times (or it may be a burden, which I hope not)
",1523142094.0
thesnowmancometh,"C'mon man this was literally going to be my project this weekend. ;D

Hell maybe I'll submit a PR.",1523072256.0
Axman6,"No FingerTree (a.k.a Haskell’s Data.Sequence)? One of the few functional data structures to be discovered since Okasaki’s work and gives you some nice asymptotics: O(1)* append and prepend, log n concatenation, index and IIRC insert and delete. It’s basically the structure you get if you pick up a binary tree by the furthest left and right elements, let everything drop down and fuse the paths to those elements. Really cool structure for building queues among many other things. ",1523101143.0
PM_ME_UR_OBSIDIAN,"The part where this is using `Arc` throughout makes me skeptical of the performance claims. Should I be?

Has anybody tried to use [hazard pointers](https://en.m.wikipedia.org/wiki/Hazard_pointer) in Rust? (Though IIRC those solve a slightly different problem.)",1523104844.0
,[deleted],1523072074.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1523051611.0
spinwizard69,"This is all well and good for high performance systems but AI hardware is need on the SOC that ship in our PC's and mobile devices.   We are starting to see some specialized hardware in the mobile realm but neither AMD nor Intel seem to have taken an interest in AI specific hardware.    For wide spread adoption of these sorts of technologies you really need dedicated high performance hardware that is available across the board even in an Intel APU with their shitty GPU's.   In the long run you really don't want to be burdening an integrated GPU with your AI workload.

This is why I suspect the rumors of an ARM based PC processor from Apple are true.   Apple is going their own way not so much for the ARM core but rather to build in the AI supporting hardware they will need in the future.   AI hardware quickly become critical to new products.",1523066198.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1523026702.0
SelfDistinction,"> This blog is a fine example of common GDPR myths. Had the author continued to read article 6(1) of the GDPR, he would have found that there are other grounds for processing of personal data than consent. No less than five, even, of which the legitimate interest of the data processor applies rather well to the examples given by him. Which kind of takes away the whole point of this blog.",1523051137.0
ketzu,"> I am not a lawyer and nothing qualifies me to talk about the GDPR.

Most important statement in this. They give no reason why their “analysis“ should be worth anything.",1523028711.0
shendude,Man totally read that as CD Project Red. ,1523048242.0
clownshoesrock,"It can be done..  it's just adds a bunch of complexity and places for things to fail as you get to ""thinner boundaries"".  and both operating systems would have to have some cooperation going on..  That leads to a bunch of bloat...

Imagine having two kitchen managers at a restaurant, one speaks French, the other German.  Each needs to learn enough of the other language to leave notes.. The rest of the staff needs to know enough to be directed by either.  And the kitchen managers have totally different cooking styles/menu's  So how does the staff change it up when they're about to plate an order?

**tl;dr This costs far more chaos than it would ever provide in utility.**  ",1523030534.0
Free_Math_Tutoring,"> I am not talking about VM I am talking about a thinner boundary between operating systems such that they can switch back and forth.

How would that behave? What exactly are you imagining, what would this lesser separation accomplish?

In general, it's possible to imagine loading one OS first (this is unavodiable), then having that load another one and removing itself, while the second has those same capabilities to load the first OS again. However, this would remove their ability to communicate which parts of RAM are used, so they would likely tear trough each others used memory, crashing all programs and services not saved as the current state on disk. You could save the _entire_ state to disk, but then you move into a territory far, far less efficient than VMs.",1523019242.0
ImaginationGeek,"One of the main things an OS does is to manage the hardware resources on the computer.  If you have two managers of the same resources, who’s really in charge?

Dual booting essentially lets two OSes take turns being in charge, but they cannot be in charge at the same time.

Virtual machines work because the hypervisor (or VMM) is really in charge, not the OSes.  It gives each guest OS the illusion that it has a hardware machine all to itself, so the OS gets to be in charge of those virtual resources that are given to it by the hypervisor.

If you want to divide up the resources and have each OS be in charge of a portion of them, there are two ways to do it.  If you give each OS a portion of every resource to be in charge of... well, that’s just another way of describing virtual machines.

If you want to divide along different types of resources (e.g., you manage memory and I’ll manage disk I/O...  then those are really just subsystems if the same OS.

If you want to mic & match or customize those subsystems (say you like the memory management of Windows but the I/O systems of Linux) then that is what extensible, modular OSes are for.  “Microkernels” are the most famous type of extensible, modular OSes, but other techniques for doing it also exist.

Granted, all the popular commercial OSes today are non-extensible (for the most part) “monolithic” kernels...  but the reasons for this are not (primarily) technological.  Extensible OSes have been built; they just didn’t catch on commercially.

If you wanted to take two different full OSes and divide up the resources between them, without a hypervisor to mediate, there is very little that could do that the above techniques don’t.  Meanwhile, both OSes would have to be purpose built to do that, intrinsically ties them to each other, prone to having bugs (in parts of the OS where you really don’t want any bugs!), and hard to find & fix bugs....  unless you’re very careful about modularity and good interfaces - in which case it starts to look more and more like one modular OS than two truly distinct OSes.
",1523044865.0
surface_book,You mean like Windows subsystems?,1523031231.0
redattack34,"I don't think it's impossible, it's just that no-one has built it (it would require custom hardware or co-operating OS's or both, and that takes work to develop). Why bother? VM's already accomplish the same things.",1523020360.0
rockfan705,"If you're taking a CS midterm and don't know how to solve the most significant question on the exam, maybe you haven't studied enough and/or don't deserve to pass the exam? I can't help with cheating.",1522982161.0
UnicornStrength,"If any of you decide to help me by writing this, please write a comment first here to let me know someone is going to help me out. If someone decides to help out I can then focus on the other questions on the exam that are also worth a lot of points, till then I am going to struggle trying to figure this out. Thank you guys.",1522981382.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1522972124.0
sailorcire,"Who you know and list as a co-author, previous publication history, novelty, and simplicity.",1523021921.0
Dubwize,Not exactly impact factor but google scholar metrics can give you a quick overlook : https://scholar.google.fr/citations?view_op=top_venues&hl=en&vq=eng_theoreticalcomputerscience,1523123154.0
destiny_functional,"

> I've received great feedback from physicists, but I've yet to run the ideas by anyone from the CS / math community. 

""Great"" feedback? ok... I think people were putting question marks behind most of your claims and calling out obvious flaws in them (like minimum energy of a particle being hbar * 1s - Something which is just obviously incorrect if only for relying on an arbitrary unit like 1s).

Before rederiving SR from ""computer theory"".. do you even have a solid grasp of SR? Your comments suggest differently and your understanding of QM also seems to be shaky. 

 Reading your posts it gives of heavy crackpot vibes (while lacking basic knowledge of physics). ",1523639329.0
thehamslammer,"Full disclosure: I have not yet read the full paper. Also, I’m only a bachelor’s degree level scholar (CS major, math+physics minor). But, one detail that stood out to me was using assumptions 1.1-1.3 to imply something in the body of each assumption. To me, it makes more sense to state your assumptions, and then in a following subsection discuss their implications. Discussing implications adjacent to definitions gives a circular feel to your assumptions. It took me a couple re-readings to know for sure that your assumptions weren’t circular in nature. That could be my relative inexperience in reading academic papers in comparison to professors/grad students, but I still think it’s a valid critique of the general clarity of the basis of your argument. I’ll keep reading in the coming days if you think my input could still be helpful! :)",1522992957.0
hubbahubbawubba,"Granted I've only skimmed the paper, but I find myself questioning your assumptions. For one thing, assumption 1.2 is incorrect to the best of my knowledge. QFT has rather conclusively shown that a quantum system with even a finite number of particles can evolve through an infinite set of intermediary states. In addition, assumption 1.3 appears to violate Heisenberg uncertainty. 

It's entirely possible I'm misunderstanding you, however, and I'm completely open to being corrected. 

All this said, you're not alone in your intuition that there may be links between discrete mathematics and quantum physics. Gerard t'Hooft has been working in recent years on modeling such systems using cellular automata, a variant of the finite automata used in the theory of computation. ",1523033759.0
sketchydavid,"You don't get an inherent speed advantage from quantum computing in all cases - quantum computers don't work just by trying all solutions in parallel and then somehow outputting the correct answer ([this comic](https://www.smbc-comics.com/comic/the-talk-3) is one of the best basic explanations I've seen). Instead, there are a few specific kinds of problems where a [quantum algorithm is known to be beneficial](https://en.wikipedia.org/wiki/Quantum_algorithm). You *could* potentially use something like Grover's algorithm for chess, [apparently](http://iopscience.iop.org/article/10.1088/2058-7058/11/3/31/pdf):

> But Grover's algorithm would really come into its own in algorithmic searches - that is searches of lists that are generated by a computer program but are not stored in memory. For instance, a quantum computer playing chess could use the algorithm to investigate a trillion possible continuations from a given position in about the same number of steps as a classical computer would need to investigate a million. Despite the greater scope for speeding up classical chess programs using ""tree pruning"" techniques, Grover's algorithm is still likely to provide a very significant improvement.",1522960315.0
tetroxid,"> Painless software development

AAHAHAHAHAHAHAHAHAHAHAHAHAHAHA

hahahahahaha

haa
haha

*sobs*",1522934787.0
TomCADK,"I agree with everything except the design patterns. Don’t get me wrong, I strongly support design patterns. They allow us to communicate intentions without getting into the weeds. But an over engineered solution, with plenty of indirection, and perhaps a misapplied design pattern is worse in my opinion.",1522928805.0
remy_porter,"Honestly, I thought the advice was going to be to start drinking at work.",1522930781.0
Wolfsdale,"Replace 'cron jobs' with simply getting a general idea of how to work with a Linux server. And you don't need cron anymore, you can use systemd to launch your thing on a timer which makes it much easier to disable and enable something (and see log output).",1522946090.0
BrightLord_Wyn,">like the majority of the programmers out there… I hate Windows…

No most programmers use Windows or OSX. I can't take anything else seriously after such a claim. CRON Jobs, seriously, how about a real scheduling system instead of some outdated system that makes Window Task Manager look like heaven?",1522957769.0
SSJ-DRAGADOS,My life a mug,1522964945.0
tevert,"I somewhat disagree with the comments. https://twitter.com/markrichardssa/status/857756221285302272

99% of the time, you can make your code clearer by improving your variable names and function names, or extracting chunks of code into clearly named private functions. Functions aren't just useful for code reuse - they add clarity! ",1522956843.0
mykevelli,Sorry I’m not familiar with the java ecosystem but conceptually what you want is to deserialize the JSON into an actually object. Then you’ll just access the properties in that object like anything else. In .NET there’s a common package called JSON.NET that handles serialization and deserialization really well. ,1522891563.0
skeeto,"[Ever wonder how Bitcoin (and other cryptocurrencies\) actually work?](https://www.youtube.com/watch?v=bBC-nXj3Ng4)
",1522885837.0
,[deleted],1522866636.0
GFandango,Plot twist: we are in someone else's minecraft :D,1523213358.0
Veedrac,"This is a very commonly used idea in theories of complexity and computation. It's how, for instance, we prove that systems are Turing complete.

> I'm more interested in the computer science related answers to *why* this was to fail if you tried. And also, it would be very interesting to know how lossy this process is, if it has any practical application, and anything else that's relevant.

That's pretty easy as questions go. A computer-in-computer can't be better than the outermost computer because the outermost computer can *choose* to solve whatever problem it's given with an inner computer. You might say I'm dodging the question, since that would allow for the counterintuitive idea that computer-in-computer solvers can be faster than other approaches in some scenarios, but JIT compilers for virtual machines are exactly that scenario! For the sort of problems JIT compilers are built for, it's *faster* to emulate a virtual instruction set on top of your real one because it allows you to make certain optimisations that hardware doesn't natively expose.
",1522867165.0
nonvelty,"Like others, I think you should reason in terms of turing machines. 

Once you accept that any and all computations can be represented by the simple basic operations of a turing machine, you can consider a very simple use case (say, calculating 1+1 in binary) and show that performing that task of an inner turing machine will require the outer machine to perform at least the same amount of operations to output that result. From then on, you can use mathematical induction to prove that this will remain true even as you go towards an infinitely complex computer.

Basically, by reusing the properties of turing machines (proven to cover any kind of computation, see Church–Turing thesis) and induction (proven to stay true towards infinity), you can build on that to convince yourself that there can be no calculation done in an inner machine that would be faster than doing it in the outer machine in the first place, no matter how advanced or clever the inside machine would be. I don't really need to consider those ""wait, what if"" complex scenarios or simulations, because the question can be answered at the simplest root of the problem and apply for *any* current or future machine.

At least that should hold within the realm of traditional ""computing"". Based on what you wrote I think you meant a simulation running on a traditional computer, but if you consider something that would perform work that is not otherwise ""computable"", you may want to to dig a bit deeper. Maybe look at https://en.wikipedia.org/wiki/Church–Turing_thesis#Philosophical_implications, https://en.wikipedia.org/wiki/Hypercomputation and https://en.wikipedia.org/wiki/Computability_theory, but that soon goes outside what I studied in my degree.",1522898218.0
hzhou321,"Computer inside computer, as others have pointed out, is trivial. We have it already, which is simply virtualization. However, what you meant, but you may not have realized is, can we have the simulation to create a program to run on the simulated computer, that is more powerful than the program that ran the simulation itself. That is a valid question.

As you will soon to learn, all Turing complete machines are equivalent, so cannot be more or less powerful, simulated or not.

By powerful, you really are thinking about artificial intelligence: can we program an AI that in turn create more and better than we -- the AI creator -- can ever create? Simply, can we create an AI that is more intelligent than us? That is intreaguing. I would say yes as I see nothing fundamentally prevents it. But you have to wait or work on it for any surer answer.",1522885515.0
CyAScott,"It took me a while to figure out programming and CS have almost nothing to with each other. I did have 3 required classes where we did some mild programming, but everything else was analysis, proofs, data structures, and lots, lots, lots of math.",1522885399.0
ogsenseig,"Currently enrolled in one of the basic CS 116 class at my local CC, all what my professor does each week is send us you tube links for different functions and methods. Very frustrating. ",1522862950.0
,"Something to keep in mind, Intro CS is taken by a lot of people in other majors but are picking up an elective. Some Bachelor of Science majors are also required to have one CS elective to graduate. This means someone in Biology with zero interest in CS may have to take a CS course. This helps to explain a lot of why there is a high failure rate ",1522867639.0
JayVaught,Make a how to guide for debugging. I am a TA for the weed our course at my university and most people fail or drop because they don’t know how to set breakpoints and monitor locals/data members in even the most simple application.,1522898555.0
MaxNid,Pointers were difficult for me starting in CS ,1522904025.0
EclipsingBinaryBoi,"My intro CS course was in Ada and the only reason I failed was because the prof was a very tough grader and wouldn't accept programs unless they worked 100% without any compiler error or warnings. Dunno how this might help you, but here it is anywaylol",1522874983.0
nerga,Asking for a name seems very unnecessary.,1522929443.0
sturdyplum,"Hacker rank has a wide range of problems some of them are very easy some of them very difficult. I don't know which 730 challenges will be solved but i don't necessarily agree that it's the best way to go. The issue is that at that rate you have to solve multiple problems a day, which is not an issue if the are all easy. But if you want real improvement you should see problems that are not challenging. Sometime a problem can take a week to solve. You will get more it if solving Jess share problems than a ton of easy ones.",1522859775.0
,"Anything goes. If doing those kinds of programming exercises amuse you and they get you to code every day. Go for it.
Continuity and enjoyment gets you a long way.",1522863534.0
elcric_krej,"> ""I see kids all the time in my classes who can see an algorithm and just rip straight through it ... my lack of speed is severely holding me back from reaching the level where I want to be""

Ah yes, as a programmer in a very busy startups, I've often had this problem: I've had to re-write partition or quicksort and it took me like a whole 15 minutes. If only I could reduce that to 5 minute per basic algorithm I'm sure our earnings would skyrocket.",1522859923.0
hextree,"If your goal is to get good at competitive algorithms programming, yes. If your goal is to acquire general coding skills, then this isn't going to help much.",1523462411.0
UncleMeat11,"Static analysis.

Security is a favorite application for abstract interpretation, symbolic execution, model checking, and more. These are all incredibly deep fields from a theoretical front, have a lot of space for practical algorithmic optimizations, and have clear value for security.

Oakland, ndss, ccs, and usenix are the big security conferences. Popl, pldi, and cav are all great conferences from a static analysis angle.",1522856708.0
orangejake,"You might want to look into Elaine Shi's work. She has both crypto and security interests.

Broad topics she works in are blockchain stuff and ORAM. I'll write a short blurb about ORAM as it's probably less familiar.

Imagine you have a company that stores your data, and you want to access it. You might think ""hm, I'll encrypt the data to keep it private"", which is a good first step. The issue is that even how you *access* the data can give away a lot of information (for example, binary search is pretty easy to recognize, or maybe you access one file 100x more than all the others) about the data. ORAM (Oblivious RAM) is the name for techniques to hide any access patterns that come across from protocol execution.

Current research involves creating ORAM versions of various algorithms/data structures (I believe). ORAM is usually considered to be a research topic in cryptography, but is definitely an algorithms-heavy topic.",1522855613.0
Neker,one word : cryptography,1522858641.0
Catalyst93,There's the burgeoning field of differentially private algorithms.,1522856654.0
gct,Look up Raluca Ada Popa and homomorphic encryption.  There's many PhDs to be had and possibly fortunes to be made there.  Read her CV is you really want to feel discouraged =D,1522870628.0
devsdb,"I am into applied crypto, building cryptosystems for example. It offers a good mix of algorithms, security oriented thinking, and because it is engineering, you also get the pleasure of building something interesting with code.",1522858080.0
shaggorama,Machine learning applications to cyber security.,1522910533.0
Pseudofailure,"I'm not sure the actual name for a field, but I'd consider looking at applications for heuristic-based threat detection. This would be things like trying to identify malware based on patterns, rather than matching existing samples; or software like Snort for identifying potentially malicious network traffic.

If you prefer a field more related to theory/research, I'd just say study cryptography and try to explore the infosec community to find a need that interests you. ",1522856730.0
jrmxrf,"Every single one. That's the optimistic theory at least ;)

You may enjoy the rabbit hole of cryptocurrencies. Example keyword: zk-SNARKs",1522874540.0
QSCFE,"same question but instead of algorithms I will add machine learning (security + ML).  
I'm more interesting in vulnerability research.",1522887419.0
07dosa,Let’s not forget that big data is a key element in actual workplaces. Real-time traffic analysis really pushes the limits of big data and AI.,1522912037.0
,"differential privacy / statistical disclosure control 

(e.g. https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf ) ",1522923099.0
MyTribeCalledQuest,You could potentially check out formal verification of algorithms used in blockchain contracts.,1522854339.0
DSrcl,Formal methods.,1522874837.0
Jenna1_,/r/compsci is a brilliant community! thanks everyone for the suggestions and I hope others find the discussion here interesting as well!,1522883335.0
kupo1729,"You might also be interested in the ""differential privacy"" literature. ",1522888387.0
t_bptm,"I'm not sure what the correct name of it is, but machine learning on encrypted data is quite interesting. ",1522990271.0
mcnnowak,"Have a quick google/Wikipedia binge on penetration  testing and fuzzing, might be interesting.",1522859274.0
Nerdlinger,"Well, it may depend on what he means by all of those vague words like “almost as good as” but his claim is probably wrong. There is a group of problems whose optimization problems are known to be hard to approximate unless P = NP. Set Cover is the classic example of this.   ",1522849982.0
plgeek,There is this result which says the approximations are as hard as NP. http://en.wikipedia.org/wiki/PCP_theorem?wprov=sfla1,1522852900.0
,[deleted],1522860969.0
redpilledcuck,"People here are confusing formal worst-case performance guarantees for approximation algorithms with what works well in practice. In practice, (meta)heuristics such as simulated annealing can give solutions very close to the optimum within a reasonable time. Sometimes there is no formal guarantee for solution quality, but empirically we find that they work. Not every case is a ""worst case"". ",1522858718.0
,[deleted],1522862429.0
IJzerbaard,"Technically there are none, because NP-complete problems are by definition decision problems and there is nothing to approximate (the result is either true or false, there is not getting ""close""). Of course, there is usually an ""obvious variant"" that is an NP-hard optimization problem, to which approximation is applicable.

Anyway, Max Clique and Chromatic Number are famously NP-hard to approximate even with a polynomial factor n^1-ϵ for all ϵ>0. They're also very hard in practice.",1522850856.0
Brian,"In any strict sense, this isn't true, and isn't really too meaningful in that NPC problems are by definition *decision problems* - you're asking something like ""Can all cities be visited travelling less than 5000 miles: yes or no"" - what does an ""approximation"" mean for that?

But what he may be getting at is that many actual *instances* of NP-C problems can actually be solved pretty easily.  The distinction is concerned with the *worst case* - ie whether there's *some* problem I can give you that your algorithm will need to spend more than polytime to solve, but this doesn't neccessarily mean that a randomly chosen instance will be hard to solve.  You could have an NPC problem for which 99% of the *instances* of that problem of a given size might be solvable with some crappy O(n^2) time without any contradiction.

And in fact, it turns out that with most NPC problems, random instances are actually very often easy to solve.  This is why NPC problems generally aren't used as the basis for cryptography - instead we use stuff like factoring, which are weaker in a theoretical sense (in that it could potentially be in P even if P != NP) - because creating a random ""hard"" instance is much easier than guaranteeing that an instance of an NPC problem isn't actually much much weaker than the worst case.",1522862708.0
l_lecrup,"The moral of what he is trying to say is broadly true: the problems that come up in real life are often (or perhaps even usually) easy to solve or approximate, even if they are instances of problems that are in general hard in a classical sense. 

As others have noted, it is not really meaningful to ask for an approximation of a decision problem. And indeed, there are problems for which a ""good"" approximation algorithm implies P=NP.",1522859462.0
FUZxxl,"That's not true.  Many interesting NP-hard problems cannot be approximated better than by a constant factor.  For example, the very important *vertex cover* problem falls in this class called [APX](https://en.wikipedia.org/wiki/APX).

Similarly, there are many interesting NP-hard problems where an approximation doesn't make a lot of sense.  Take for example SAT or ILP.",1522853467.0
nealeyoung,"I'd say no.  Try finding an ""approximate"" solution to factoring, or breaking an RSA key.  These problems are in NP, so they reduce to any NP-complete problem, so could be solved ""in practice"" if any NP-complete problem could be solved ""in practice.""  These are just a few examples.  There are all kinds of problems that arise in practice that are in NP, but that nobody knows how to solve effectively even ""in practice"".  

Also note that companies pay highly skilled people a lot of money to try to find, and solve, those variants of the problems that they care about that can be effectively solved.    And it is certainly not a given that that is possible for any problem you might care about.",1522861048.0
heyandy889,_Fast Fourier Transform Or Bust_,1522886715.0
clownshoesrock,"He's just wrong.

Yes many optimization style problems have good heuristics, Travelling Salesman is one.

But lots of them don't, as this isn't hand-grenades, and approximates are useless.

Integer Factoring is a reasonable example.
",1522889471.0
kupo1729,"There are always easy instances of NP-complete problems.  Sometimes all the problems you want to solve in the real-world fall into that set of easy instances.

Think of SAT for example.  If a problem is super super super constrained, it's easy; there's very few options per variable, we just check those, we're done.  If the problem is totally unconstrained, also super easy; just set everything arbitrarily and we're done.

If this is a topic that interests you, there's a whole area associated with solving NP-complete problems quickly in AI.  Planning, SAT solvers, and CSP solvers might all be of interest to youl ",1522890290.0
SOberhoff,"You might want to take a look at [Impagliazzo's Five Worlds](https://www.researchgate.net/publication/3653458_Personal_view_of_average-case_complexity). In particular your question boils down to whether we live in ""Heuristica"".",1522903953.0
yossarian_23,"I suspect the speaker OP references meant something along the lines of ""by claiming problems of type P are NP\-complete, we are *by definition* simultaneously making the claim that we have knowledge of some heuristic which we assess renders P/NP practically moot for problems of this type""",1524428638.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1522844892.0
GNULinuxProgrammer,"You'll compare Windows, OSX or iOS with some other (i.e. except those who are in the former list) UNIX-like operating system.",1522838956.0
foreheadteeth,"I'm a math prof and I teach the numerical linear algebra at my university, but I also like convex optimization.

The neat thing about numerical linear algebra is that we can do so much. We can compute all the eigenvalues of a 10,000 by 10,000 matrix. We can solve a linear problem with 1 billion unknowns. However, whether you actually learn these things depends on how strong the course is.

The big breakthrough of convex optimization was no doubt Nesterov's work on self-concordant calculus, which roughly shows that convex optimization problems can be solved approximately in polynomial time. There's a huge range of applications for this.

For AI and ML I would expect numerical linear algebra is more important, but outside of AI and ML I think convex optimization is quite exciting too.

Edit: Actually on second thought, the work on stochastic gradient descent is mainly in convex optimization and that's a huge area of current research so optimization is a great choice too. There was a very recent result on accelerated SGD that's a pretty big breakthrough. SGD is the main tool for training neural networks.
",1522837045.0
0b_0101_001_1010,"The ""Increasing Rust's Reach 2018"" program partners people from groups and backgrounds that are underrepresented in the tech community with Rust's team members for ~3 months to work ~4-5 hours weekly on a Rust project. As a ""thank you"" for participating in the program you get a fully paid conference ticket, travel, and accomodations for a Rust Conference of your choice.

The application phase is open till the 20th of April, recipients are announced on April 30th, and the program runs from May 15th to August 17th. 

The program aims to make Rust a more inclusive, approachable, and impactful project, while supporting the applicant personal goals and it was very successful last year. If you are interested or know someone who might be please spread the word. Check out the [web-site](http://reach.rust-lang.org/) and if you have any questions please ask in this [thread]( https://users.rust-lang.org/t/increasing-rusts-reach-2018/16607/2) or by mail at `reach@rust-lang.org` .",1522831962.0
bdrwr,"Just completed my physics undergrad. The only programming language you need in that major is python. Compsci covers a few others but I don’t know much about that. 

It would give you a huge edge to get familiar with using numpy, pandas, and matplotlib (python packages). More generally, get good with arrays and matrices cause you’re gonna do a shit ton of data science. ",1522807276.0
mynewpeppep69,"Minors don't mean much, but if you REALLY want one I suggest math as well.",1522792163.0
EdgeLord421SoClose,"Honestly, I would do a minor in something like math. It's good to have a minor in something that is RELATED, but also shows some diversity. ",1522791215.0
Carpetfizz,"just take more CS classes in the time you would have spent doing a minor. chances are there will be some interesting classes (related to theory, math, probability) that aren’t in the trodden path your fellow students take.",1522801434.0
Scuba_Von_Wolfgang,Like one bad ass yassnippet,1522789132.0
CyAScott,Pretty cool. More general purpose than .net/js fiddle. Usually I put this stuff in a github wiki page for my repos.,1522794902.0
embluk,Hopefully should be useful for people learning to code during a CS course or even for those who are advanced :D,1522778900.0
OverlordGearbox,"If possible you may look into linking with a github account.

Another feature may be exporting to common snippet tools like yasnippet. (or have it's own.)

These features would send it over the top for me.",1522883946.0
kibleh,Perhaps. Perhaps not. ,1522779053.0
briancodes,">To save you looking it up on Wikipedia, its definition there is that a class should only have one responsibility, which is further defined by Martin as ‘one reason to change’.

It's not really the case. His definition refined down to a module should be responsible only to a single actor, a class could do more than one job but it should only need to change due to the changed requirements of a single external actor.

The employee example had methods all related to an employee but the different methods within the class could change due to requirements from HR, or requirements from Management etc and this is why they violated the principle.
",1522793855.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1522733859.0
east_lisp_junk,"If you fix the input size, ""polynomial time"" is kind of a meaningless accomplishment.",1522721152.0
twsmith,"A 9x9 Sudoku can indeed be solved in O(1) time. It's the general case of an n^(2)xn^(2) grid that is NP-complete.
",1522721095.0
Ippikiryu,How long would it take to create that giant hash table?,1522720786.0
neusbal,You can use SAT-solvers to solve them pretty quick,1522738293.0
angrypotato1,"I mean hashtables are a bad data structure in general for worst case, as it's O(N)",1522722038.0
Sir_not_sir,"Did you forget ""free""?

Contact the publisher or look on Amazon.",1522712485.0
,"OP, the ""other side"" of the social justice debate is hanging out in tech, mostly because political stuff doesn't earn any rep in tech (except by accident, and even then, only briefly).

I think you're better off interning at a tech company. Even easier: call and ask about an ""informational interview,"" which will allow you to see the whole process and chat with employees one on one. 

If this is just an assignment in a social sciences class, you can string together an A+ argument from government labor studies around the world. If you only care about the opinions of African American women, you'd be better off going directly to subreddits that cator to that group, then asking who among them works in tech.",1522679725.0
NowImAllSet,"I think you have good intentions, but that this subreddit isn't really the appropriate place to advertise your study. Since you have funding, why not use a dedicated market research site? ",1522681967.0
TotallyExaggerating,"> As a note to everyone else who isn't an African American woman:  I think you're sexist and racist.

- OP",1522679307.0
cuntinuum,"As someone who works in data engineering, I find this article to be grossly and willfully ignorant. Anyone who's worked with real data knows it's far from binary and clear cut. It's messy, ill-defined, and requires deep domain understanding to cut to the heart of it. Requirements and requests come from humans and require much in the way of interpretation, elaboration, acumen, and lateral thinking. And for much of it, the goal is to provide a tool to help human decisionmaking and positive human experiences. 

You have to wonder if the author also has a dismal view of physics, which despite its more legitimate classification as science in the traditional sense was almost responsible for a global nuclear winter. It's almost as if software is a medium and a tool which can be used for great good (medical research, fighting crime, providing historically unprecedented access to education, among many others) or great evil (ransomware, identity theft, human trafficking, red rooms, and other illegal and immoral activity) and it isn't inherently good or evil. ",1522644160.0
elcric_krej,"Cute introduction that has my hopes up.amd quickly degenerated into incoherent babel about some sort of pet constpuracy theory.

No idea why this article go as upvoted as it did...",1522648507.0
SOberhoff,"While I welcome critical opinions of our field I feel like this person is primarily looking for a fight. 

>Because it is devoted to the creation of systems that limit choice, “computer science” is something more pernicious than a non-science—it is an outright enemy of scientific reasoning.

[...]

>Computer science education inevitably promotes authoritarianism. In the best-case scenario, graduates of these programs will go on to toil as “code monkeys” in the most despotic corners of capitalism—weapons manufacturing, robotics and AI, and finance. On a deeper level, they will absorb the innately authoritarian assumptions of the field. The binary worldview, with no tolerance for ambiguity, has created some disturbing mental excretions.

It strikes me as quite ironic that the author accuses computer science of promoting binary thinking when he himself is making sweeping statements, lacking all nuance, at every turn. Computer science education *inevitably* promotes authoritarianism? 

Also there's a paragraph where he laments apparently insufficient computer ethics training for students. I personally have never really understood this criticism. Computer ethics are human ethics. If you are doing unethical things with the computer your moral compass was already broken before. The computer has now simply put you into a position where this matters. So if graduates of computer science have questionable morals, so do presumably all graduates.  
Also I have very little faith that more ethics training will accomplish anything. It strikes me like attempts to reduce cheating by printing ""please don't cheat"" on the exam. As long as the opportunities and incentives are present people will keep doing it.",1522641627.0
keten,"Complete nonsense full of unsubstantiated technophobia. The weirdest thing is all the references to the military complex and authoritarianism and how the proliferation of computer science is some kind of profiteering scheme set forth by the man. 

Computer science may be a relatively new field that grew in popularity around the advent of the computer, but the fundamentals for it have been around since the creation of mathematics. Algorithms have been in use since antiquity, just look at the abacus. The only reason computer science has become a bona-fide field of study is before computers it was rather difficult to use algorithms in a practical manner, so people didn't care too much about them. Now it's easy to use them, so people care about them. That's it; There's no conspiracy.

As far as computers making people lazy and uncreative; what if they're only making people lazy and uncreative in respect to the things they don't care much about, and they entrust these kinds of tasks to computers so they can be productive and creative in respect to the things they do care about?",1522647797.0
hubbahubbawubba,"I assume English is not your native language? Either way, you should know that this density of forward-slashes is completely excessive and not at all pleasant to read. ",1522628517.0
riksterinto,"alright...which one of you pissed this guy off?

seriously though...somewhere before 'cognitive deference' ?WTF!? and the reduction of science to include only the 'natural' universe I realized the author spent less than a day researching this.",1522656492.0
tyler_the_compiler,"Athabasca University. I can confirm it's really decent. It's one of the very few distance learning universities that UofT, Waterloo,  et al will gladly take transfer credits from. In fact, if classes are full, but students need it for a prerequisite, profs from those other schools will suggest picking up an equivalent from Athabasca.

Source: alumni, and know a few people from more ""reputable"" schools who supplemented with AU

Edit: I should also mention that while it's based in Canada, they are internationally accredited, including the US.",1522587388.0
patchmo,Post University has a fantastic online program. Not sure what compsci courses are offered this summer but look into it early because it works by modules.,1522581516.0
vengentz,"I know this isn’t the answer you’re looking for, but you really should ask your advisor. Many schools have policies (or lists of pre-approved courses and schools) that will make obtaining the credit much easier",1522586591.0
scyth16,Umuc.edu,1522599783.0
,[deleted],1522599208.0
Chrispowell20,"University of Colorado offers an online program for all of their Comp Sci classes. You could contact the program director about getting in for a couple this summer if your school will accept the transfer credits. 

Program Details: [CU Comp Sci Post Bacc Program](https://www.colorado.edu/cs/csap-online) ",1522614588.0
Heliomantle,"Best solution is to take a class online with an actual uni, do an online program of some type, definitely stay away from for profit and only online colleges.",1522609617.0
ajh326,"Tufts I know is offering a few courses online over the summer https://engineering.tufts.edu/cs/courses-registration

(Apologies that the website is currently a little janky, its being redesigned)",1522625842.0
andrewfava,Western Governors University (WGU),1522592414.0
welshfargo,[Georgia Tech](https://pe.gatech.edu/georgia-tech-online) might let you take their master's courses.,1522608071.0
Spam_Detector_Bot,"*Beep boop*

I am a bot that sniffs out spammers, and this smells like spam.

At least 50.0% out of the 4 submissions from /u/frenchdic appear to be for Udemy affiliate links. 

Don't let spam take over Reddit! Throw it out!

*Bee bop*",1522565663.0
UmiKai,"Nice affiliate link lol

https://www.udemy.com/ (non-affiliate link still 95% off)",1522566988.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1522524679.0
zach_does_math,"Your algorithm basically checks every possible assignment to see if it does or does not satisfy the formula, except that where typically you might assume that nothing satisfies and increment the counter when you find a good assignment, you start by assuming everything satisfies and decrement when you find a contradiction.  This algorithm is certainly not in P.",1522523447.0
UncleMeat11,"No. 

A better way to ask this would be ""why is this approach wrong""? A three page paper isn't going to solve P=NP, especially if it does not address *any* of the known challenges in answering that question. 


Your paper is really hard to read. The matrices aren't formatted in a readable manner and the code blocks are badly indented into newlines. 

If I understand the broad strokes correctly, it looks like you have implemented a broken version of DPLL, where you try variable assignments and then see how this affects other legal variable assignments. This is not polynomial. Your loop goes over every assignment of variables possible per clause... but you don't consider backtracking. This means that your algorithm will produce incorrect answers.

Simply throw this at SAT benchmarks. You can empirically see the running time and incorrect outputs.
",1522523687.0
pulsar512b,"Reported for being a homework question. Not trying to be a dick, just helping out.    
-pulsar512b",1522459097.0
Swimmingpool518,"It's called offline-first. It's a spin-off of the previous ""mobile-first"" term.",1522528182.0
glass_ants,I think the terms you’re looking for are Single Page Applications or Progressive Web Applications. ,1522435695.0
Morgio_Zoroder,"I like to think Claude Shannon’s Master’s thesis where he proved that Boolean algebra could be used, not only to design better circuits, but that almost all mathematical functions could be executed using simple switches/relays. This was the foundation. Then he went ahead and wrote A Mathematical Theory of Communication where he proposes and proves that any “message” could be encoded in bits, a 1 or 0, on or off, true or false. Same Boolean states but as a firm transmitting, encoding, decoding, storing, and even compressing what we become to be known as data. The message being abstracted to mean almost anything. To him the meaning didn’t matter. He said, “ the semantic aspects of communication are irrelevant to the engineering problem.” I always liked this bit of cheek in an otherwise very dry paper. ",1522417231.0
jmite,The Curry Howard isomorphism is up there for me. The idea that logic and programming are just two sides of the same coin: types are propositions and programs are proofs.,1522420704.0
chx_,"That Turing machines are *meaningful*: the Church–Turing thesis. That general recursion, Turing machines and λ-calculus describes the same thing (this is surprising enough) which informally is just the things we find computable (now that is quite shocking).",1522416540.0
TKAAZ,"I love the Cook-Levin theorem, i.e. that boolean satisfiability is NP-complete.

""The proof shows that any problem in NP can be reduced in polynomial time (in fact, logarithmic space suffices) to an instance of the Boolean satisfiability problem. This means that if the Boolean satisfiability problem could be solved in polynomial time by a deterministic Turing machine, then all problems in NP could be solved in polynomial time, and so the complexity class NP would be equal to the complexity class P.""",1522440427.0
ChubbyDalmatian,Using 0 and 1 to represent everything.,1522416112.0
DonaldPShimoda,"Other people have written about my choices for *most* revolutionary (Shannon coding), but here's another very revolutionary idea:

That computers should present their information graphically.

Specifically, watch [Doug Engelbart's ""Mother of All Demos""](https://youtu.be/yJDv-zdhzMY), wherein practically all crucial aspects of the modern graphical interface were revealed to the public — in 1968. This is often regarded as the single most incredible demonstration of what computers might be able to do in the future (at the time it was presented). Some highlights:

- presented the mouse
- highlighted text
- ~~resized windows~~ had a window system
- manipulated text onscreen via WYSIWYG editing (unheard of)
- simultaneous text/graphics onscreen
- teleconferencing (via microwave transmission)
- collaborative editing (through the teleconference call or something)
- hyperlinking text",1522427144.0
everything-narrative,Hindley-Milner type inference deserves praise for its simplicity and practical usability. ,1522421638.0
qaphqaesque,"Someone already said my pick for *most* revolutionary idea. However, the idea of reductions are pretty damn cool.

By rewriting certain problems as other problems, we can build up classes of problems that are ""equivalent"" in terms of runtime. This is what lead to the statement of the P vs NP problem among a lot of other cool things.",1522426382.0
JukesOfHazzard96,"My professor likes to say, that virtual memory in operating systems, is **one of** the most important ideas in computer science. ",1522457574.0
mediumsize,"The idea of logic gates - using very simple binary switches to represent mathematics.  This created a layer of abstraction that made switching circuit theory possible.  It's the fundamental concept of digital computing. 

(NOT NOR NED-AND XNOR NAND OR XOR AND NEG-OR)",1522420829.0
elcric_krej,"All the crypto stuff  is quite cool and basically managed to spin off a new branch of math.

Parctically speaking, I think that some awesome idea lie around the usage of atomics and implementation of compiler&cpu optimizations for various memory ordering... But I can't think of any definitive example.",1522462064.0
dr1fter,Hmm... Von Neumann architecture?,1522415950.0
gct,Universality of turing machines,1522417862.0
boolhardhitta,Recursion ,1522420809.0
supershinythings,"Virtualization turned out to be pretty revolutionary.  It had existed in some forms for many decades, but only in the last 20 years were people able to run one or more modern readily-available OSes side by side on another modern readily-available OS.  Now timesharing compute, bandwidth and storage together are commodity products.

This is really a subset of general timesharing, a truly revolutionary concept that improved usability and simplified programs dramatically starting in the 60's.  The abstractions and services have evolved, but it's still fundamentally the same idea - multiple entities can share resources using encapsulations and deployment strategies which permit organizations to have access to compute resources without having to own the machine.

Then, I'd say the invention of compilers. Boy did that save a lot of work.  Today's cushy 4GL and 5GL programming languages run in a feather bed environment made possible by the compiler who did all the dirty work.  

RMS didn't invent the idea of the compiler but he did give his away for 'free' (as in freedom), which gave interested parties full access to a world that was previously confined to expensive black-box products.  He opened a door that will never shut.  Without Stallman we might not have had Torvalds and Linux, at least, not in the same way.  Torvalds would have had to write his own compiler and other tools first before attempting to write a kernel.  And all those other free tools sat on top of that kernel, suddenly giving it usability and traction.  Now ANYBODY can learn to use a computer, write code, and sell it, without having to license anything from a major corporation for big bucks.

So for me and my generation, these are the things that make up what I work with daily.",1522428861.0
manoj549t,I would vote for multi programming (process abstraction) and high-level compiled languages. ,1522416458.0
000xxx000,Caching or Indirection,1522428298.0
yawkat,Information theory is a big one.,1522441734.0
DreaminOfBananas,"I think Atanasoff is severely underrated.  He moved computation into the digital realm.  It's hard to imagine where we would be without his contribution.  

Turing machines and Shannon's information theory round out a pretty solid top 3 for me.",1522464870.0
kokobannana,Transistors ,1522446218.0
hardballs_,can’t believe nobody is writing this: hash tables,1522449376.0
chhakhapai,"The idea that you can tradeoff memory (structured in some way) for time and vice versa. Also, approximate algorithms.",1522481966.0
IPv8,The turing machine,1522415427.0
Nichololas,"More from software engineering, but: version control systems. ",1522425386.0
Valance23322,Multithreaded execution.  The ability to perform multiple operations simultaneously and synchronize information  between them massively expands the scope of what can be done.,1522425959.0
beeskness420,Linear programming and it's duality is pretty huge. ,1522459809.0
SeattleCoffeeRoast,"I remember in my first year in college we talked a little about Ada Lovelace. Paraphrasing my TA, ""Ada Lovelace believed that we could speak to computers and conversely computers can speak to us... in our own language.""

That left a strong impression on me. I often like more verbose languages because it reminds me of good books or poetry. There's an inherent beauty to what people write, and what comes out.",1523397854.0
Oxc0ffea,I think from the last 30 years: computation can arise in surprisingly simple systems like cellular automata.,1522422240.0
Nerdenator,The idea of mapping the state of a transistor to 0 or 1 to indicate off or on.,1522433566.0
bradrlaw,"Not so much revolutionary, but interesting paradox that computers can never truly guarantee they can be in sync with another computer.  


This is because you can never actually prove with certainty that a message sent was received and acknowledged.


https://en.m.wikipedia.org/wiki/Two_Generals%27_Problem",1522434118.0
Alltagsaregone3,"The dvd drive

[r/dadjokes](https://www.reddit.com/r/dadjokes/?st=JFEC83IT&sh=38bc1456) ",1522437941.0
,The realisation of DP and Network flows. Allowed us to turn supposedly impossible to complete problems to polynomials.,1522448477.0
bloodydick21,I think the guy who decided to put porn on computers wins hands down,1522421951.0
T0mmynat0r666,"If we talk of computer architecture, it has to be the idea of caching.",1522420781.0
bgaskin,"The idea of the Turing machine. 1936.

This is the definition of biggest revolution because it changed computer science from level-zero to level-whatever, an increase of infinity percent.

Next. :)

---
For the uninitiated, a Turing machine is a thought experiment of what is possible with a memory-state and a set of rules.

https://en.wikipedia.org/wiki/Turing_machine

http://history-computer.com/ModernComputer/thinkers/Turing.html

First described by Alan Turing in 1936.

It defines everything that computers can possibly do, and is interesting because it asserts the possibility of all kinds of computation and directly shows how this can be possible with even a ridiculously simple computer made of a pen and long (or infinite) strip of paper, reading and writing dots.

Real computers aren't *implemented* in this linear access way, but we still use this as a comparison model of what's possible (at least given infinite time and space in the case of increasingly complex problems with definite answers) for past, present, and future computers, including even abilities of the human brain as a computation machine.

At least if you set aside questions of souls and free will.",1522445688.0
MjrK,"The invariance theorem... every Turing complete language can have an interpreter for every other Turing complete language with some constant interpretation overhead which depends on the languages but invariant of the program being interpreted.

And Kolmogorov complexity... the size of the most compact representation of an object. I think it represents the inherent complexity of a string.",1522507609.0
agumonkey,"far from the 'most' but I like logic programming.

so abstract, so tiny, the idea of structural enumeration through predicates

similarly one of the most damning demo I've seen is the minikanren talk by felleisen and friedman where they encode lisp eval as a relation, and infer programs backward.",1522714799.0
Wispy-Willow,The lambda calculus. ,1522718912.0
EmbeddedEntropy,"I’ll give two:
Public key cryptography
Distributed ledger",1522416347.0
lxpnh98_2,"Not the most revolutionary in *all* of computer science, but I think the idea that the human brain is simply a biological machine was very revolutionary to the field of Artificial Intelligence. It's AI's central idea.

If that is true (which we know from biology and neurology), it means that, with enough power, we can build computers that think just like humans, or even better. We're still trying to figure out a lot of how humans think, and we don't currently have the power to make computers as 'smart' as humans, but perhaps we're getting there.",1522430057.0
twocatszerokarma,Ctrl alt delete,1522433363.0
dhjdhj,Reversible computation. ,1522435539.0
scatteredthroughtime,Controlled repetition/looping (via conditional jumps at the assembly level) because it’s the fundamental thing that differentiates modern computers from basic calculators.,1522458378.0
Pratik770,Please watch the Imitation Game. ,1522416170.0
MarthaPhucker,"It's probably about to happen, quantum computing. ",1522429933.0
EmbeddedEntropy,"I’ll throw out two more ideas that are revolutionary, but so far have had many false starts and major implementation problems:
Artificial intelligence
Quantum computing

Once we successfully implement and marry these two ideas, they’ll likely be the end of the originally generated human CS ideas. ",1522420898.0
gojukebox,"jQuery.





Just kidding, it's WordPress.",1522485344.0
HonestlyQuestionMark,Penicillin.,1522432935.0
uwabaki1120,Making people think you have privacy on Facebook. Then your shits sold off. ,1522429858.0
airforcefalco,Commenting code. ,1522453279.0
srbufi,You can’t trust anything.,1522492210.0
kaidon,Microservices. The amount of time and money people have spent forecefully using this pattern without any actual should inspire a new revolution of pragmatism to fix the messes we have made.,1522422241.0
picturethatt,Pointers,1522425033.0
Meguli,node.js,1522415858.0
joeroloff,Unplugging and rebooting.  ,1522429719.0
,[removed],1522416244.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1522406927.0
tulip_bro,"I've spent a few weeks researching Formal Methods. I have been down a rabbit hole of learning the theory behind it and it's applications, and I am continuing to do so because the relation between logic and programming fascinates me (I am working through books Calculus of Computation and Software Foundations).

I am having a little trouble finding new motivation however, since the majority formal methods application is in critical systems, but I work in fin-tech, e.g., if my software isn't formally spec'd or verified, no one will die, so the implementation trade-off doesn't seem to be worth it.

I stumbled upon a sub-area called lightweight formal methods (I believe Alloy Analyzer is one of the implementations of this) which seems to have more applicability towards my every day development. Can anyone recommend blogs, forums, etc, dedicated to this topic? ",1522941160.0
pulsar512b,"Hey, i am a student studying math and CS. Which should I study in higher education, and where should I go. I am at the Graduate level (master's, and doctorate). So, where should I go for a Master and PH.d",1522373537.0
PolarTimeSD,"In college, as part of the CS program, we had to take a Computing Ethics class. It was a good class, though poorly taught and attended. But as a working engineer and researcher, it astounds me how little my colleagues think of the ethical implication of their work. It's not 1980 anymore, we can't just remain ignorant about the potential consequences. ",1522353848.0
,"> Research papers that are likely to have a net negative impact present a trickier problem than grant proposals in the same situation. Should ACM KDD, AAAI, NIPS, ACM SIGCHI, etc. publish papers that reviewers and authors agree will do more harm than good, even when considering potential future research and policy? We believe the answer is ‘yes’, but with an important caveat: the research community should more actively evaluate its members on their ethical decision-making when it comes to research project selection. In doing so, we would simply be extending a well-established practice when it comes to ethics in the research process. If one has plagiarized or violated other ethical standards in the research process, it is appropriately difficult to acquire or keep a job, tenure or a promotion. The same should be true for someone who continuously chooses to conduct anti-social research, no matter how rigorous that research.

We can't not fund research that is deemed anti-social simply because it has certain implications, this is very different from lobotomy surgeries and other equivalent that had direct impact on people. In this case, absence of knowledge of what is possible, is a lot more anti social than absence of research. Why would any sane person risk their career and their life because we are afraid of what will come next? If we can't trust our researchers that will figure out how people might exploit others, how will we know how to protect ourselves from something of that nature?

With deep fakes for example, as another user suggested, cryptographic encryption of a key should be added such that an image remains verifyable as original. 

In similar albeit exaggerated manner,  Turing shouldn't have conducted further research on Turning machines because after 60 years people would be addicted to those machines, people would be left without jobs because many things got automated. The point I am trying to make and perhaps failing, is that we often can't look much further ahead and even if we actually do look ahead, at times, it's almost impossible to identify all  consequences, good or bad. Such consequences may seem negative, even if negative, so long they don't cause harm to humans, they should not be discontinued because then we are left in the darkness and the worst thing about darkness, is not knowing what's hiding in it.",1522399302.0
BaconWraith,That smooth scroll makes me feel a tad motion-sick,1522355693.0
jpfed,">Similarly, those working on generative models praise the quality of the audio and video (e.g. “deepfakes”) they generate

This is the one that just makes my jaw drop. How can people not understand how *terrible* deepfake technology is for the functioning of democracy?",1522354976.0
Drisku11,"Automation making menial jobs obsolete is the point. There's plenty of other work to be done that can't be automated right now (e.g. construction of high speed railways or giant border walls). Is it more ethical to lie to someone and let them do pointless work than it is to let them do something meaningful with their life? If so, we can always have one group dig ditches and another fill them in.

More generally, this idea that the development of knowledge itself--and not the methods used to get there--could be unethical seems awfully presumptuous and highly dubious to me.",1522562665.0
zhbrui,"This sounds like [subset sum](https://en.wikipedia.org/wiki/Subset_sum_problem), which is NP-complete. ",1522296048.0
bhrgunatha,"Whenever you want a sum using a subset of numbers, you should think about dynamic programming - [example SO question](https://stackoverflow.com/questions/4355955/subset-sum-algorithm) but I'm sure google can help you find something else. It won't *always* be suitable, but it's usually a good first step.",1522296506.0
HeraclitusZ,"This kind of study is called typically called programming languages (PL). 

The code generation stuff is called synthesis, and it is fairly popular right now. Look at any top university's PL group and you'll find synthesis. As an example, I know a lot of work is happening at Penn, Princeton, and Cornell revolving around synthesizing a domain specific sort of program called a lens, in particular bijective lenses. Another synthesis thing going on that I've seen at CMU, Penn, and Princeton has to do getting synthesis to happen from user specification more, whether it is type-directed, or users give input/output examples and machine learning does the rest. I think I also heard somewhere about synthesizing machine learning algorithms.

You'll also find some degree of code generation coming out of work on proof assistants like Nuprl, Coq, etc. that play off of the correspondence between programs and proofs. Basically, you prove that certain specs are satisfiable, and from that proof a program is derived. Tons of places use Coq specifically for verifying all their code. This kind of turning-logic-into-code comes from the Curry-Howard correspondence and drives a lot of logical type theory work all over the place.

The final place I can think of that you see code generation/replacement comes out of work on compilers, Optimizing user-written code for performance in the compiler automatically is pretty tricky, so the code transformations that go on get a lot of study. I think Andrew Appel wrote the goto textbook on compilers.

As far as domain specific languages go, there is a lot of work right now around making languages that enable better security/privacy (I can think of Steve Chong at Harvard off the top of my head), and also work across many universities on networking languages (look up, for instance, NetKAT and all its extensions; I've actually worked a lot on this). A few years ago there weren't really any networking languages that weren't super low level and ad hoc, so this is nowhere near dead.

In that same thread, Kleene algebras are really cool. They are an algebraic structure that do *so* many cool programming things. They do obvious things like describe regular expressions, but also non-obvious things like subsume all of Hoare logic when combined with a Boolean ring, prove correctness of code transformations, and form the basis of the DSL NetKAT. I think it's a useful thing to know if you want to make a DSL, since you get so much out of it.",1522302943.0
jmite,"Racket is a pretty big name in modern metaprogramming, I'd look into it.",1522301662.0
P0oltj3,I'm a bit biased on this topic but I would like to recommend [Rascal MPL](https://www.rascal-mpl.org). It's being developed by a small team in the Netherlands at the Center for Mathematics & Informatics (CWI).,1522305361.0
hzhou321,"I just posted this: 

https://www.reddit.com/r/programming/comments/87uij0/mydef_a_general_preprocessor/

And the manual:

http://hz2.org/mydef_manual.html",1522294481.0
geon,"A couple of interesting languages are Nim and Zig.

https://youtu.be/Z4oYSByyRak?t=1402",1522338028.0
hellajacked,"Working as a Ruby/RoR dev, DSL's play a big part there! You even use them unknowingly, such as writing RSpec tests and the like",1522340962.0
combinatorylogic,"One significant recent addition I can think of is [PEG](http://bford.info/packrat/) - it enables *extensible* parsing, making it possible to build really flexible syntax-level macros.

And, of course, metaprogramming is tightly dependant on everything else that happened in the compiler construction world - including SSA.  Turns out, SSA is also quite a powerful tool for metaprogramming - e.g., see my experiments on making a [generic SSA](https://github.com/combinatorylogic/mbase) available for arbitrary DSL construction.

Type systems are obviously closely related to eDSLs, and there were significant advances in this area.

As for the canonical sources - they're the same as for compiler construction in general.",1522367267.0
DSrcl,"Maybe you've seen this already, but take a look at Tiark Rompf's work on Lightweight Modular Staging.",1522435705.0
crabbone,">  it seems everything I find is from the 70's!

That's because that's when serious research in this area stopped.  Not to be misunderstood, this is a much needed and an important research, but the industry has its own incentives.  It appears that a lot of problems can be solved, albeit poorly, but not tremendously badly without or with limited meta-programming.  But, removing the ""meta-"" from programming, makes it easier to manage different aspects of the program.  Easier, as in easier to understand, not easier to write.

> Have you used anything like this in your work?

Well, I wrote some Common Lisp and Prolog...  programs that generate code are commonplace in these languages to the point that you cannot really do much without generating code.

When it comes to DSLs, I came to really hate it when people even use the word.  Every instance of when this particular feature of the product was advertised turned out to be a disaster.  The ones that deserve particular warm place in hell are Gradle and Jenkins Pipeline (both are variants of Groovy).  There's also Sandmail configuration language and a ton of other mini-configuration languages, ranging from more popular like XML and JSON to more obscure, but not less heinous, like HOCON or unnamed horror which is ALSA configuration language.

> Where can I learn more about this?

Unfortunately, I don't know about anything all-including.  I remember, however, that one of the more recent ICFP contests was dedicated to code generation (from high-level specification).  It was because it was sponsored by Microsoft, and, at the time, they were working on publishing some research in that area.  I will try to search for publications later, but I doubt there was much to write home about, it didn't make even a mild splash at the time, so hardly any breakthroughs.",1522561353.0
VermillionAzure,"**Metaprogramming** is a very broad field, IMO. It's everything from inference of code from specifications to macro systems and has its tendrils touch everything from programming language syntax and language structure to paradigm to type systems to automata theory. 

- The ""Schemers"" group (as I like to call them) that's distributed across many universities but focuses on Racket and the suite of language production tools around Racket has been working on metaprogramming for ages. I find great value in their research, but I also find it very limiting since many people get superficially turned off by Lisp itself (for whatever reason). Research from Racket has affected other languages like Rust and JavaScript -- specifically, Racket's module system and macro system are a point of interest with its deep-layering of evaluation stages and differentiation between compile-time/run-time/test-time/X-time stages. Personally, I think this research will come to a head once Rust gets started on their procedural macro system with arbitrary computation at compile-time -- I'm going to guess there will be many security issues and evaluation issues with their current system.

- Polymorphic malware and understand how viruses spread can also be a surprisingly ""metaprogramming-like"" topic -- code that is designed to spread and reproduce itself while also making incremental changes to make sure it is robust to hash-comparison and detection can be considered a metaprogramming topic.

- Perl 6 is getting considerable attention with regards to metaprogramming within their own community. Rakudo is supposed to have some sort of great support for metaprogramming and PL-stuff within it. I don't know too much about this but this may be a thing to explore.",1524184922.0
limita,"* Somehow standard resource on Domain Specific Languages in imperative world is Domain Specific Languages by Martin Fowler, but many people think it's too wordy and tries to make simple things complicated.

* Most of recent work I'm aware of is in using logical and functional programming languages for desiging some modelling language for AI. Most popular area is probably constraint satisfaction programming - look at Oz, Zinc, OPL, Comet, Mercury or Ciao. If you are not scared of Haskell, this paper gives a good example and has a very comprehensive ""related work"" section: http://homepages.inf.ed.ac.uk/wadler/papers/constraints/constraints.pdf

* I have some experience with [gsl](https://github.com/imatix/gsl) but only as user, never tried improving it :-)",1525096177.0
agumonkey,Smalltalk is a bit back into the light through Pharo. really lovely live metaprogramming environment,1522714867.0
eliot_and_charles,"Another brick in Lamport's ""why doesn't anyone but me care about state machines"" wall?",1522292869.0
leftofzen,"What even is this? What is he claiming? That comp sci people don't see the 'computing devices' in the list as being similar or the same abstraction? That's a pretty broad statement. While my first response wouldn't be ""yep they're all state machines"" I would say something along the lines of ""they all describe the same underlying computational process"". I really think most of my comp sci friends would say something similar, so I'm not sure where the author is getting all this hubris from.",1522301895.0
Count___Duckula,I might be missing something here but surely this is obvious? If I write in Lisp my train of thoughts is vastly different than from C.,1522347140.0
horsebees,"I would posit that the author is no different, that state machines themselves are influencing if not distorting their world view.

State machines, while useful in pedagogy, never paint the full picture, and have limitations in their use, even ignoring FSM vs Turing. They have no established way of communicating separation of concerns (I.e. functions/objects). They have no established dependency tracking, or change management. They ignore memory hierarchies, latency, transition rule determination time, or even simply serialization.  The proposals I’ve seen to bridge these gaps (e.g. the P language from Microsoft[1]) have not gotten enough attention IMHO.

1. [P: A programming language designed for asynchrony, fault-tolerance and uncertainty](https://www.microsoft.com/en-us/research/blog/p-programming-language-asynchrony/) ",1522315650.0
trill_winds,"I suspect you can go a level deeper in abstraction and just think of state machines as the span of any finite dimensional space.

A major strike against this representation, in my mind, is it removes the ability to reason about time in the way that is most intuitive to us - imperatively. A state machine is the closure of its states, kind of like how spacetime is to space. By losing generality, various representations of computation can more intuitively model the linear passage of time, reasoning that requires that we focus on paths and relations in a state machine.",1522343521.0
combinatorylogic,"He's wrong to call it a ""suffering"". 

Linguistic relativity is among the most powerful mental techniques available to humans, if you understand how it works you can apply it efficiently. 

And this is exactly why language-oriented programming is so powerful - by switching languages you're using to describe your problems you're switching the way you're thinking about them, allowing to escape from the otherwise hopeless local minima.",1522586057.0
HeraclitusZ,"Why don't you just put a nonterminal on the right side at the start that just generates junk, instead of trying to generate it from the middle later?",1522303158.0
kokobannana,How do they have 81 million hours? How many processors they have?,1522257279.0
alexthelyon,"Ah my two defining features, computer science and autoimmune disease, all in one post.",1522287431.0
floridawhiteguy,All models are wrong; some are useful.,1522287226.0
dnpmpentxe,Will they have to throw away the results due to [Titan returning wrong results in simulations](https://www.extremetech.com/extreme/266508-nvidias-titan-v-accused-returning-wrong-answers-simulations)?,1522256239.0
NowImAllSet,"I had a friend go there! They said it was absolutely awful, and probably the worst school they've ever been to. ",1522244842.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1522229822.0
HeraclitusZ,"Java Cup is a LALR(1) parser. If it can successfully generate a parser for a given grammar without getting shift/reduce or reduce/reduce conflicts, then it must have been given a grammar that is LALR(1). And if it gets conflicts and fails, then the grammar was not LALR(1).

It is harder to use Cup to reason about the *language*, however. If a grammar is LALR(1), then so is the language the grammar generates, but the reverse does not hold. There could be a different grammar that is LALR(1) and generates the same language.
",1522204513.0
no_detection,"Unless I'm missing something, it sounds like all you need to do is define your terminals, non-terminals and expressions, then see whether Java CUP is able to generate a parser.",1522204197.0
Commander-TeeJ,Computer science let's you gloat more,1522180936.0
amberlini95,"Software development is a lot more code oriented. Computer science has a good 18-21ish credit hours of code based classes, but it also examines the theory. Algorithms, Turing machines, Boolean algebra, computer architecture, and languages are just a few of the things that computer science examines much more deeply than software development. 

Which one is “better” depends on what you want to do. Computer scientists are often hired for not only coding jobs, but also engineering jobs because they have practice in critical thinking/problem solving (from the theory based classes). Software development majors usually get hired for the coding/actual software development. 

I guess the main difference is how much coding you want to do. Software development will probably have a couple of theory oriented courses, but it’ll be mostly code based. Vice versa with CS. ",1522176218.0
NowImAllSet,"In my opinion, it's like a mechanic vs. an automotive engineer. A mechanic can change your oil, diagnose and fix errors, install a turbo kit, etc. An automotive engineer can calculate and adjust for aerodynamics, thermal transfer, intake and exhaust dynamics, etc. As a blanket statement, an automotive engineer can probably walk into any Jiffy Lube and get a job as a mechanic, because the assumption is that if he knows the theoretical, technical stuff then he probably knows the maintenance and building stuff. Note that this is not necessarily the case, but it's the assumption. However, a mechanic cannot likely walk into a position fit for an automotive engineer and get the job.

The same relationship exists between a software developer and a computer science major. One is focused more on doing, the other is more on the theory and math. A computer scientist can likely walk into a software development shop and land a job, due to that same assumption. And, similarly, it might not always be true. But a software developer might not be able to walk into a heavily theoretical computer science position and get that job.

So, the end **TL;DR** is that the computer science major is probably the better one, since the software development degree might close some doors. However, if your end goal is simply to be a software developer, then the courses you take there will likely be more pertinent and leave you feeling more confident in a software developer position.

Source: am a software development major, working in a sea of computer science majors.",1522188286.0
5454a1,https://youtu.be/cSVDk-ugAQs,1522178818.0
Musique_NotEvenOnce,"Study computer science. If you study software development you'll simply learn different programming languages. These things are both domain specific and non-static. The principles behind the languages will be more valuable in the long term and give you more upward mobility and freedom in the work you choose to do. 
Plus modern technology interviews are essentially algorithms exams - understanding the concepts will be the bottleneck here, not implementation.",1522195037.0
Farsyte,"Analogy:

- software development is ""building telescopes""

- computer science is ""using them to study stars""
",1522193659.0
jubydoo,"How far along are you? Obviously, it depends on your school (mine, for example, didn't even have a separate ""software development"" major), but I'm guessing there's going to be a fair amount of overlap in requirements. If you're unsure, focus on those mutual courses. By the time you're forced to make a decision, you should have a better idea of the differences and which one you'd likely prefer.",1522194662.0
ISvengali,"I tend to be stronger on the SD side of things, planning big projects, seeing how to get from A to Z, things like that, and I strongly recommend a CS major.  

Youll have a stronger base on the nuts and bolts of things, and you can pickup the SD things when you actually start working.  

If you want to go _right_ (or quickly) into management, then make SD would be better.  Id still recommend CS though.  

How to build a compiler doesnt change much no matter where you are.  How to organize a project depends on the details of _every_ person in the group and what the stuff is being used for and what your target timeframe and market is and a whole host of other variables youll have to learn anyway on your own.  ",1522194699.0
snowman4415,"IMO it’s easier to learn software “development”  on your own or on the job with a solid CS background. This is not true the other way around. It’s a lot harder to get a grasp on theory while on the job, or at least more of a hassle because you typically aren’t getting paid to study theory where as you are getting paid to build good software. (Do CS)",1522199001.0
eponymous_wrecks,"As someone who hires software developers and who has developed software for 20 years, I would tend to lean towards hiring CS grads, as I haven’t found that universities are all that great at teaching the practicalities of real-world software development (e.g., you’re never dealing with a big enough code base for there to be any real design consequences, you’re almost always dealing with your own code, the assignments are well-defined, the code bases you deal with are almost always new).

But what the universities are good at is teaching the theoretical underpinnings of computing, the “laws of physics” you will be dealing with. Chances are you won’t be using or referring to them all the time or every day in your software practice, but it is an incredible asset to having them under your belt when you really need them.

Specifics of software development change all the time and are constantly changing, but the theoretical fundamentals haven’t changed much at all.

My best guess is that a CS degree will take you to more places than a Software Dev degree will.

Good luck!",1522208025.0
mcraealex16,At my school there is literally 1 difference.. we have the exact same course load with the exception of one physics. I wouldn't stress touch and if you feel like you made the wrong decision just switch after your first year im sure the courses will be similar enough,1522198887.0
DrApplePi,"You'll probably find that there's a lot of overlap.  
CS tends to require a bit more math, and the more challenging but way more interesting courses.  
My university required Computer Organization, Compilers and Theory of Computation.  Probably my 3 favorite classes.  ",1522201940.0
ChrisC1234,"Also remember, many people say that some degrees (software development, management information systems, computer information systems, etc.) are majors for people that couldn't cut it in Computer Science.  I've never heard of CS as a major for someone who couldn't cut it in something else.",1522204186.0
,[deleted],1522205566.0
nerga,"Are you in the US? I've never heard of a software development major. I've seen plenty of software engineering majors and it majors (which I still say go with cs). But I would be skeptical about a software development major, it sounds like something that a diploma Mill or boot camp would have.",1522206012.0
kokobannana,What's SW Dev major? Which institute offers such thing? ,1522236115.0
emdeka87,"So I just started studying CS. I’ve been a programming enthusiast since my teenager years and developed all kinds of projects ranging from server backends and mobile apps to custom game engines, IoT(Embedded) or reverse engineering. The majority of folks here recommend computer science, which is to be expected in /r/compsci. There is nothing wrong with that. In fact, it’s an excellent choice for some, but it might be too dry and theoretical for others. Expect things like formally proving the correctness of an algorithm and determining its asymptotic runtime, analyzing data structures like red-black trees, linked lists, stacks, queues, etc. Computer science is not about coding, so don’t start your study with this expectation. I made that mistake and it got me up to the point where I consider quitting CS entirely. In addition, my university seems to have pretty high standard s compared to other universities where I live. We worked through most of the CLRS book in the first semester, which is a pretty steep learning curve IMO.
There is no definitive answer to your question. Know what to except from CS (and what not) and you should probably not be disappointed. “Software developer” and computer scientists both have pretty good job expectations.  Many game studios accept CS students and software developer alike. In fact, I think the combination of theoretical computer scientists and software developers make up for an exciting and productive team environment; they compliment each other well.",1522237005.0
anamorphism,"i think the easiest description is that computer science is theory and software development is practical application.

there's going to be a good deal of overlap but one will start veering off into theoretical more general topics and the other will start veering off into specific applications.

if you want to program for a living, then software development is probably the better choice. if your school is decent, then this might actually prepare you a bit more for what coding for a living is actually like.

if you want to go into academia and do research or become a teacher, then computer science would be better. you'll have more options in general but may find it a bit more daunting to actually become a software engineer/developer for a living if that's your goal.

neither is really better than the other, they just have different goals.

probably best if you just look at the course lists for each major and see which course descriptions tickle your fancy more. wouldn't be surprised if some of the later required courses for one are elective options in the other. could also probably just double major without adding a great deal of courses to take.",1522196176.0
tevert,"* B.S. Cybersecurity and Information Assurance - I tell developers not to do stuff

* B.S. Data Management/Data Analytics - I query databases to make charts that are hopefully interesting

* B.S. Information Technology - I install Windows

* B.S. Cloud and Systems Administration - I deploy software onto a box, and then get called at 10PM when it stops working

* B.S. Network Operations and Security - I tell developers not to do stuff and then get called at 10PM when they do it anyway

* B.S. Software Development - I write code

* B.S. Health Information Management - I tell developers not to break HIPAA

* B.S. Business – Information Technology Management - I tell people to do stuff


Of course, most of these are quite interchangeable when applying for jobs and your degree really doesn't matter after a few years of job experience anyway. Moreover, switching degree mid-college is extremely common and not complicated if you get halfway and realize you don't like it. ",1522167867.0
anamorphism,"so, i'll try to elaborate on /u/tevert's response here.

> B.S. Cybersecurity and Information Assurance

focus on analyzing software and hardware setups to determine how 'secure' they are. you'll probably learn quite a bit about encryption and networking. you'll probably learn about the most common vulnerabilities out there like sql injection and the storing of sensitive information as plain text.

there aren't many jobs in this space as companies tends to not care about security until they're quite large. most likely you'd end up doing standard systems administration work or maybe land a gig at some security consultant group.

> B.S. Data Management/Data Analytics

databases and statistics. if your school is somewhat decent, you'll probably have a class or two about 'big data'.

probably learn sql, the normal forms, some administration stuff and so on.

jobs would either be more on the side of just being a database administrator or more on the side of doing data analytics (building dashboards for upper management out of the data you're collecting).

> B.S. Information Technology

catch-all. probably focused more on standard systems administration.

learning how to setup and maintain workstations. probably a bit about networking. maybe some scripting. probably learn about virtualization solutions.

jobs would be what people think of when you say 'IT guy/gal'.

> B.S. Cloud and Systems Administration

probably same as the one above but with more of a focus on virtualization and 'servers' rather than workstations.

hopefully you'd learn about containers and things like service discovery, auto-scaling, and so on.

this is a hot space right now, but your starting jobs would most likely be similar to a standard systems administrator role ('IT gal/guy').

> B.S. Network Operations and Security

you'll learn about networking. security stuff comes in the form of things like firewalls and learning about denial of service attacks and such. probably learn about SSL and different means of encrypted communication.

more standard 'IT person' stuff but you'll be working with routers and load balancers and such.

> B.S. Software Development

learn how to program.

various jobs doing software engineering/development/whatever you want to call it. most of the jobs are in web development these days.

> B.S. Health Information Management

similar to Data Management/Data Analytics but with a focus on the complexities of the health system.

probably more of a focus on privacy and such and probably more of a focus on programming.

jobs would be working with insurance companies or healthcare providers.

> B.S. Business – Information Technology Management

standard business stuff with a bit more of a focus on not being an idiot when it comes to computers.

you'll learn general IT stuff but at a high level just to understand the space. you'll be way more focused on the business part.

jobs? probably end up as a project manager or producer for some IT org.",1522176376.0
alkalinemusic,Where would software engineering fit into this? Im tryine to decide between that and software development for my degree path,1522170363.0
,[deleted],1522181995.0
INDEX45,"Not to be a jerk, but these degrees are basically rewards for showing up, and an implicit acknowledgement that the person couldn’t handle the tougher workload of a traditional CS or EE degree. 

Every one of these fields can be handled by a person with a normal CS degree, they are simply specializations of study or career path that everyone falls into. The reason degrees like these are created then is by basically stripping out all be difficult requirements (aka, what is the most valuable to actually learn) and padding it back out with fluff classes that are less technically demanding and so easier to pass. They are designed to segment people into job markets that are low pay, low opportunity, and high commoditization, and to make employers lives easier in the hiring process. They vocational degrees *at best*. 

Well, that’s the *optimistic* scenario. The *pessimistic* scenario is that they’re bullshit paper mills designed to scam you out of money with almost no return your investment of time or money. 

Don’t take my word for it, go look at the better schools, none of them will ever have degrees like this offered—at the very most they will offer specialization options as part of the optional degree requirements, but that’s all. The only place you see degrees like offered are low tier schools, which tells you all you need to know.

These things aren’t worth the paper they’re printed on. ",1522178508.0
,[deleted],1522179353.0
patchicat1941,"In spite of the superlatives , these are jobs that require an education ",1522185621.0
AsiansInc," • B.S. Cybersecurity and Information Assurance = Dont do, not many jobs

 • B.S. Data Management/Data Analytics = Create excel macros all day

 • B.S. Information Technology = Go watch ""I.T. Crowd"" on Netflix

 • B.S. Cloud and Systems Administration = Dont do, not many jobs

 • B.S. Network Operations and Security = Dont do, not many jobs

 • B.S. Software Development = Pick this, you could end up doing all of these with this degree

 • B.S. Health Information Management = Dont do, too much liability 

 • B.S. Business – Information Technology Management = Tell everyone what to do while getting paid more than you are worth",1522169399.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1522146224.0
3cpartsonline,high quality,1522146237.0
Carpetfizz,"generally university courses are laid out in a way that makes logical sense (kind of like a story, i guess). berkeley's course is entirely available online https://cs162.eecs.berkeley.edu/",1522131455.0
pastermil,"Not exactly a story, but Operating System: Three Easy Pieces provides naratives",1522125510.0
Darwinmate,"This is way too specific. For your own learning, I suggest you create the story you're seeking. A mind map or any sort of concept flow would help you a lot",1522124343.0
gvs311,Operating Systems Concepts is a pretty good book. Why don't you try it? ,1522144114.0
AdamGartner,"It’s basically the Lord of The Rings Quest though? Sauron is obviously the OS, Hobbits are processes, their quest is the code (program), their split and working towards the same goal are parallel running threads (concurrent for the viewer), the narrator/story is the CPU and its scheduler that operates in a round robin fashion (sometimes by other means) the information about their world is spread around virtual memory that has to be shared, sometimes accessible now, and some takes time to infer/remember due to read from hard drive, due to the process being quite big. ",1522134449.0
kakosf,"A bit old, but still useful: [""Modern Operating Systems"", Andrew S. Tanenbaum] (https://www.goodreads.com/book/show/166195.Modern_Operating_Systems). I bought it some years ago when I was doing an MSc in CS. As I said, it is a bit old but it is well written and the theory/fundamentals behind OS still hold.",1522148937.0
redditaddict07,"Thank you everyone for your inputs. I love reddit.
Thank you once again. :*
",1522159735.0
redditaddict07,"I also very much liked this https://in.udacity.com/course/introduction-to-operating-systems--ud923

Pros - Updated content + Very Interactive + Quizzes + Nice representation of concepts + Downloadable videos

Cons - Not any as of now.

Please check.",1522738904.0
metalxythe,Perhaps reading up on how UNIX came around can partially serve your purpose.,1522128797.0
balefrost,"So the lambda calculus is sort of the end result of simplifying algorithm definition until you reach the essence. The lambda calculus basically has two concepts: function abstraction and function application. That's it! But it turns out that you can encode anything you need in these two concepts. We can encode primitive values, like natural numbers and booleans. We can encode data structures like lists. The lambda calculus isn't necessarily obvious or natural, but it is simple. That simplicity makes certain kinds of reasoning easy (ish).

Another commenter talks about Church encoding of natural numbers, but I'm going to touch on Church encoding of booleans. When you think about booleans, you essentially have three things you need:

1. A constant to represent `true`
2. A constant to represent `false`
3. A way to produce different results depending on whether a value is `true` or `false`.

But here's the trick: we have to encode all three of those as functions, and those functions are only allowed to call other functions. So yes, I'm saying that `true` and `false` need to be functions, not constants.

To use perhaps more familiar notation first, we can write those in JavaScript like this:

    lambdaTrue :: function(ifTrue, ifFalse) { return ifTrue; }
    lambdaFalse :: function(ifTrue, ifFalse) { return ifFalse; }

The idea is that we make `true` and `false` behave behave differently from each other. We can call each one with two values, and they will return one or the other of the values.

Now we need a way to actually exercise them:

    lambdaIf :: function(condition, ifTrue, ifFalse) { return condition(ifTrue, ifFalse); }

This provides a way to combine an *unknown* boolean with the `ifTrue` and `ifFalse` values, and is essentially equivalent to the JavaScript ternary operator `?:`.

Now you can imagine building other boolean operators on top of that. For example, we could define `and` in JS as:

    function and(x, y) { return x ? (y ? true : false) : false; }

Or, we could simplify that to:

    function and(x, y) { return x ? y : false }

We can translate that to our JSLambda calculus like this:

    lambdaAnd :: function(x, y) { 
        return lambdaIf( x,
                         y,
                         lambdaFalse); 
    }

Whoops, we can't actually do that. The lambda calculus doesn't support the idea of global names (this is why I've been using `::` instead of `=` - I need a way to refer to things, but I can't rely on assignment). The only way to give some value a name is to pass it as an argument to a function; inside the function body, the parameter name will be bound to that value. So we could start with this:

    lambdaAnd :: (function(lambdaIf, lambdaFalse) {
        return function(x, y) {
            return lambdaIf( x,
                             y,
                             lambdaFalse
            );
        };
    })(
        function(condition, ifTrue, ifFalse) { return condition(ifTrue, ifFalse); }, //lambdaIf
        function(ifTrue, ifFalse) { return ifFalse; } //lambdaFalse
    )

We could then call that outermost lambda, substituting the parameters (`lambdaIf` and `lambdaFalse`) into the outer function's body:

    lambdaAnd :: function(x, y) {
        return (function(condition, ifTrue, ifFalse) { return condition(ifTrue, ifFalse); })(
            x,
            y,
            function(ifTrue, ifFalse) { return ifFalse; }   //lambdaFalse
        );
    }

Well actually, if you do one more iteration of substitution, we can simplify once more:

    lambdaAnd :: function(x, y) {
        return x( y, 
                  function(ifTrue, ifFalse) { return ifFalse; }   //lambdaFalse
                );
    }

But the lambda calculus uses different notation. Here are those JSLambda expressions rewritten in the lambda calculus notation:

    true     :: λx.λy.x
    false    :: λx.λy.y
    lambdaIf :: λc.λx.λy.c x y  (or λc.λx.λy.(c x y) if you want; parens only indicate precedence, 
                                 they don't have the same meaning as in LISP)
    and      :: λx.λy.x y λa.λb.b  (or λx.λy.(x y (λa.λb.b)) if you prefer)

The lambda calculus executes via a substitution model. When you apply arguments to a function, you instantiate the function body, replacing its parameters with the actual arguments that were passed in (like we were doing above). You have to be careful when doing this to avoid name collisions. But in the lambda calculus, names don't really matter. These two functions are the same function:

    λa.λb.a
    λx.λy.x",1522158123.0
adipisicing,[Aligator Eggs!](http://worrydream.com/AlligatorEggs/) This ELI5 game helped me understand alpha-conversion and beta-reduction.,1522126369.0
quiteamess,https://youtu.be/eis11j_iGMs,1522104498.0
EatMySnorts,"Judging by the answers in this thread, you're not the only one to not get it.

There is no *ELI5* version of lambda calculus. If all you need are the highlights and terminology, then wikipedia will get you there. Just stop reading when your eyes start crossing...

If you need to have an actual understanding, you're gonna have to work for it. ""An Introduction to Functional Programming Through Lambda Calculus"" is about as gentle an introduction to the topic as is possible, but even then, you'll only have a little more than the highlights and terminology portion of the subject.

The canonical text is ""The Lambda Calculus, Its Syntax and Semantics"", and it is **not** for the faint of heart. It takes a lot of time and effort to master that book.",1522148099.0
LejonR,"While giving a really, handwave-y definition, it's a semantically complete way of writing computer functions in mathematics

`
def cool_function(x) :
Return x**2 + 2
`

...Is Equivalent to...

LAMBDAx. x^2 + 2

Lambda calculus just does this in a way that avoids function names. Keep in mind, this should just frame your thinking about what lambda calculus is. Lambda calculus is a deep and complex topic, that converges and diverges from functional programming in many ways. ",1522102961.0
JustinsWorking,"I actually find my current project is a really easy example (mentally not practically) if you're a fan of Videogames.

Think of an RPG. You have a set of stats (this could represent a player, or an enemy for example) and you have gear, which is a function that takes a set of stats, and returns a set of stats.

so say the players variable is {hp: 10, str:1, attack: 1}

I have a function I've declared that takes stats, and returns the stats of somebody using a sword. We'll call this function EquipSword

In this case the function is: 

    f(stats) => return ({
      hp: stats.hp, 
      str: stats.str,
      atk: stats.atk + 2,
    })

So the function starts with a stat and returns a stat f(stats) -> stats

Next lets look at an ability, we can create a function that takes stats, and returns the resulting damage. We'll call it GetDamage

    f(stats) => return ({
      amount: stats.atk + stats.str + 1,
    })

This is now a function that takes a stat and returns damage f(stats) -> damage

Now to complete the loop we can create an ApplyDamage function

    f(stats, damage) => return({
      hp: stats.hp - damage.amount,
      str: stats.str,
      atk: stats.atk,
    })

This function is f(stats, damage) -> stats

So now imagine we have playerA, attacking player B

     ApplyDamage(PlayerB, GetDamage(PlayerA));

     or  for a more complex example if we want PlayerA to have a sword equipped

     ApplyDamage(PlayerB, GetDamage(EquipSword(PlayerA));

Hopefully you can see how you start to layer these functions, this is a more practical and fun example of Lambda calculus",1522113308.0
3rw4n,"Depending on the amount of energy you want to put into this: ""Introduction to Lambda Calculus"" by Henk Barendegt et al. is great ((http://www.cse.chalmers.se/research/group/logic/TypesSS05/Extra/geuvers.pdf).

Study the proofs and do the exercises and you will learn a ton, quickly. You can also read ""proposition as types"" by Philip Wadler (http://homepages.inf.ed.ac.uk/wadler/papers/propositions-as-types/propositions-as-types.pdf) and pick up the ""Introduction to the Theory of Computation"" book (https://www.amazon.com/Introduction-Theory-Computation-Michael-Sipser/dp/0534950973/)

Of course you don't need to read *all* of this to get a basic understanding of lambda calculus but if you want to understand for ""real"" so it sticks. 
",1522265983.0
Neker,"If ""it"" is computable, ""it"" can be expressed in the form of a [function](https://en.wikipedia.org/wiki/Function_\(mathematics\)). See Babbage, Gödle and Turing.",1522154827.0
Iroastu,"Literally so weird,never used in a professional setting, and just briefly had it mentioned in undergrad. Good luck. ",1522153692.0
RuttyRut,Play Half-Life 3 and you'll understand.,1522141010.0
chromaticgliss,"Lie and fake it until you do have experience. (I'm only kind of joking)

More importantly, don't let ""X number of years in Y"" requirements prevent you from applying. Often times there's a lot of disconnect between HR (who don't understand the jargon) and Tech Managers (who are required to give a list of requirements anyway). So those numbers are often irrelevant to the actual work, or are just an ""ideal"" candidate which they don't necessarily expect to get.",1522088229.0
no_detection,/r/cscareerquestions might have better answers.,1522087074.0
swimmer91,"A lot of those job descriptions are written by HR people who have no idea what the job actually requires.

At an old job, my co-worker found a job description posted online for our team.  He noticed some oddities and ended up discovering it was copied and pasted from a description for a similar position at a Fortune 500.

If you're interested in a job, just apply.  Study for the interview, go in with confidence, and ace it.

That said, if this is your first software job, don't be afraid to lower your standards just a bit.  It's perfectly okay to work somewhere for a year or two, gain experience, and move on.",1522088632.0
sayubuntu,"You get a degree, have a robust network in the field already that you can exploit, or outperform everyone with a degree by such a large margin people are forced to hire you.

",1522089107.0
ImaginationGeek,"I think others have done a good job of answering the direct question.  I just want to clarify one detail...  this may be unpopular to self-taught people, but it is true and important...

A programmer and a computer scientist are not the same thing.

You can self-teach to become a programmer, and indeed many people do.  But self-teaching to become a computer scientist is much harder.  I’m not saying it’s impossible, but I expect self-taught computer scientists who are on par with B.S. degree holders to be rare.

Now in practical terms for jobs...  if you want a programmer job then you just need to be a programmer. (And that is a *good* job, and pretty well paying.) What a computer science background opens up for you, besides grad school, is certain high-end jobs (developer at Google, for example) and jobs working in specialized areas (advanced work in machine learning, for example). It also provides breadth and the ability to work on different kinds of things in the future, instead of just knowing how to do one thing.

So there’s nothing wrong with being self-taught, but don’t confuse that by thinking a college degree doesn’t add anything at all.",1522092558.0
redditmat,"Most employers would ideally want someone with all the perfect experience and productivity dream - who is ready to volunteer. It's a stretch but that's how job market often works: you find hard working people and try to pay them the least. 

Have your projects and focus on the familiarity and experience with the technologies with which you want to work. Your projects (open source, blog, little packages, etc etc) count as experience. ",1522089930.0
gixxerjasen,"If you have no formal education and no experience, certifications are a good way to get your for in the door.  Many will argue that certifications don't carry much weight and they don't but they are many times more easily obtainable than education and experience.  Get through that first door and then prove yourself going forward and build that experience.

As for job descriptions, that's usually the ideal candidate and an ideal candidate will want more money than they are willing to pay.  Apply anyway and see what happens, you might be the one who ticks the most boxes of what they are looking for.",1522091369.0
skeeto,"Great pains are made to keep glibc backwards compatible, both with
respect to API and ABI. You can reliably and safely dynamically link old
programs against new versions of glibc without breakage. This is what
happens when you update glibc on your system.

Unfortunately the opposite isn't true — running a program compiled
against a new glibc but dynamically linked against an older glibc. The
new program may depend on new symbols that don't exist in the older
glibc. I imagine this is the problem you've come up against on occasion.
When I run into this problem, such as trying to run a newly built
program on an old machine, my usual solution is to statically link
against musl.
",1522086666.0
DannoHung,"Please see the ""Linux Libc"" section of this manpage: http://man7.org/linux/man-pages/man7/libc.7.html

Short version is: Someone forked and really fucked up the version numbers a long time ago. Normally versioned shared objects adhere to libwhatever.so.MAJOR.MINOR.PATCH

I'm gonna guess you're still a student, so pardon me if this sounds a bit condescending: The thing to keep in mind is that all of practical computing is just piles of someone coming up with what seems like a good idea at the time of how to do something, and then people slowly trying to fix that when they realize later on how it screwed up something important. Pray you mostly end up dealing with the things that more or less got fixed decently.",1522086646.0
NotInUse,"First off, linux is a kernel, not an OS so libc certainly doesn’t have to be the glibc implementation.  Second, I don’t think any major “distribution” ships with an unmodified kernel nor much else as it’s all a fragile buggy mess that requires endless patching.  Even taking the source of a “working” distribution and trying to build a copy of an existing distribution can be a brutal effort (look at the delays in CentOS 6 for example.)

Because of the way each distribution evolved its own patching patterns the versioning of the shared objects on a number of systems has no relationship to the versioning of the underlying software.  This means the same software verion of a library on two different distributions may have wildly different shared object version numbers.  What’s worse, an application built on a different distribution may *appear* to work for a time but because C doesn’t provide any signature information at the API or ABI level it may not be until something becomes unstable and crashes that you have any sign things aren’t working right.  This is (in part) why commercial software is released and supported only on specific versions of specific distributions.",1522132558.0
sparr,"To write the software? A modern app development team could produce something reasonably usable in a few thousand person-hours.

To get it certified so that anyone in the industry would trust it? Decades.",1522037163.0
atkulp,Consider contributing to [OpenEMR](https://www.open-emr.org) instead.  Writing one from scratch is simply not reasonable without considerable resources.,1522048119.0
,[deleted],1522036332.0
,"I work at a hospital that just switched over these softwares take years and years to develop, thosuands of employees, and millions of dollars to produce. And they all suck in the end.",1522042792.0
blastercaster,"Well some companies have been in the works of that for over 39 years, so I'd say longer than that",1522036400.0
TennGage,gauging*,1522036567.0
omniuni,"To make something simple, a few months of solid development for a small team. It won't be usable, though. 

That first release would need to be reviewed and tested by multiple doctors, and then would need to be integrated into their existing other systems. You're probably looking at getting something that at least a very small operation could use in maybe a little over a year. ",1522039189.0
,[deleted],1522074613.0
noideaman,"I wrote EMR software for a living for 5 years.  All of the processes, integrations, and laws are going to make it nearly impossible for a small outfit to get a solid EMR off the ground.  Epic, Cerner, Meditech, and to a lesser-extent McKesson, all have the mid-large range hospitals locked up.  Epic's presence in the medical office is growing (every one of my personal docs use Epic).

You're going to have an extremely hard time displacing them.",1522084709.0
reitnorF_,what are you trying to learn..?,1522021497.0
hamtaroismyhomie,"CS50 from Harvard. It comes with lots of supplementary materials and short videos, in case anything in the main lecture is confusing.  There's also lecture notes you can refer to.",1522022255.0
Michaelmrose,I recommend trying to learn with someone else maybe the group setting will help. Alternatively paying for tutoring or taking an intro to programming class. ,1522022117.0
daixso,Look for Free Code Camp groups who can help you. Also reading the suggested documentation will help a lot. Programming is tough. You have to change the way you think. You're going to have difficult times learning and easy times. It can be frustrating as hell. But if you keep working through it it does get easier over time. Good luck!,1522022469.0
Sniffnoy,"That depends -- in which variables?

Is ""n choose k"" polynomially bounded if k is fixed and n varies?  Yes, because in fact it *is* a polynomial in n when k is fixed.  It's equal to the polynomial n(n-1)...(n-k+1)/k!, so it's a little less than n^(k)/k! (which it's asymptotic to).

Is it polynomially bounded when n is fixed and k varies?  Well, obviously, as it's O(1) in that case.

Is it polynomially bounded when both n and k are allowed to vary?  No; 2n choose n is asymptotic to 4^(n)/sqrt(pi*n), i.e., it can be exponential.

From your original question it probably sounds like you meant the first, but if you're interested in the other cases, well, I've answered them too. :)",1522019445.0
SOberhoff,"n choose m is polynomially bounded if m is fixed. You can see this by going back to the definition of n choose m, multiplying and canceling as much as possible and just concentrating on the leading term, which will be something involving n^(m). However, if m somehow depends on n, for example if m = n/2, then you get n^(n/2) which is now exponential.  
So if you're looking for all subsets of size 4, that's polynomial. If you're looking for all subsets at least half as large as the entire set, that's exponential in the size of the set.",1522036614.0
Calandas,You seem to be looking for [this](https://en.wikipedia.org/wiki/Binomial_coefficient#Binomial_coefficients_as_polynomials).,1522016171.0
JDAshbrock,Just using the factorial expansion for n choose m we can see that nCm is O(n^m) if m is less than half of n. Otherwise it is O( n^k) where k is n-m. It is polynomial for fixed m and n but it is a neat combinatorial observation that if we let m vary over all possible values it is O(2^n),1522016263.0
timmyotc,"Please read the sidebar. 

> Self-posts and Q&A threads are welcome, but we prefer high quality posts focused directly on graduate level CS material. We discourage most posts about introductory material, how to study CS, or about careers. For those topics, please consider one of the subreddits in the sidebar instead.

Try /r/learnprogramming for this",1521998462.0
ReginaldIII,"It's just bad form, but your simplification should have {1,20} because the {1} in the original forces there to be at least one.",1521990021.0
lerunicorn,"""[]"" is an empty character class -- it won't match anything. Fortunately the ""\*"" allows there to be any number, including zero, of them in a match, so it won't cause the whole expression to fail, but it doesn't add anything either. ""[]\*"" can (and should) be removed without changing the result.",1521992538.0
timmyotc,"https://regexr.com/ is great to help you deconstruct a regular expression. A lot of regex is written by someone who picked it up the first time they needed it, so it's not always the neatest.",1521994312.0
dskmy117,"Do you know what flavor Regex this is? It looks like [] matches the empty character class (null) in some flavors, such as JavaScript.

https://regex101.com/ can be helpful for debugging regex. ",1521992707.0
No-More-Stars,"As said [above](https://www.reddit.com/r/compsci/comments/8715qe/anyone_good_with_regular_expressions/dw9donk/), the initial match is over-complicated.

> Also, not too sure what the []* means.

In JavaScript, `[]` is the empty character class, which will never match anything. 

I don't understand why it's there, it seems to perform no function as the [Kleene Star](https://chortle.ccsu.edu/FiniteAutomata/Section07/sect07_16.html) `*` means that it will always be ignored.

Simplification:

    P[A-Z0-9]{1,20}$

Note that you're limiting yourself to the ASCII character range with this regex, you may or may not want it to accept upper-case unicode letters.",1521993348.0
americk0,"Yeah I think that's just bad form. The ([A-Z0-9]{0,19}[A-Z0-9]{1}) will work but it's no didn't than ([A-Z0-9]{1,20}) and the []* should be invalid since [ specifies the start of a range but having ] immediately after means that it's one of the characters to match rather than closing out the range so I expect you'll get an error saying the range wasn't closed",1521992776.0
bart2019,">why would they use `([A-Z0-9]{0,19}[A-Z0-9]{1})`
rather than `[A-Z0-9]{0,20}`.

That should be `[A-Z0-9]{1,20}`.

Maybe that's your answer.",1522005900.0
armurray,"> Anyone good with regular expressions?

Having encountered regexs written by both my coworkers and my past self, I'm fairly confident the answer is no.",1521999561.0
HVAvenger,"Sometimes I come across a problem I think I can solve with regular expressions.


Then I have two problems.",1522000633.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1521927307.0
funciton,"If you've queried enough elements, you still put in O(n) work on generating the tree, you just don't see it immediately. Claiming lazy evaluation is O(1) is equivalent to saying 'I'll do it tomorrow' and then claiming you're done. ",1521989001.0
666_666,"Clearly this is not possible ""by normal means"" and you need something like randomness or laziness. If so, how about using global state:

    s = Node(0)

    def generate():
         global s
         s = Node(0, left=s)
         return s",1521922823.0
julesjacobs,"Here is a toy model:

    p = 1/2
    def foo():
       if rand(p):
          foo()
          foo()

The question is for what values of p will that program not terminate with positive probability?

Let p be the probability that we choose to end the tree at the current node (p = 1/2 in this code), and let x be the probability that a tree generated like that is finite. The tree is finite if we end it (probability p) or if we don't end it (probability 1-p) but both sub trees are finite (probability x^(2)). So we get x = p + (1-p)x^(2) so x = min(p/(1-p),1). So for p=1/2 the probability that the tree is finite is indeed 1, but if we pick p lower then the tree is infinite with positive probability.",1522051457.0
Admsugar,"site is unavailable
",1521994354.0
greenearth2,Link?,1521981926.0
metaphorm,"don't worry about staggering the interviews or anything like that. just apply, go interview, see what offers you get, and then make your decision. 

if you need more time to make a decision tell that to the company you have an offer from. they'll let you know how soon they need to know by. ",1521909761.0
jesussqueegee,"I would just try to apply roughly simultaneously and keep the interview process in sync - if anything, maybe apply to companies A and B slightly earlier.",1521908794.0
badlawnchair,"Check the interview tab on Glassdoor. User reviews can give you a good idea of the timeline you might expect for the entire interview process. 

In my experience you should be prepared to wait if you really want an interview with A or B, because some companies take a month or two to get back to you.

By the way it if you haven’t been through the technical interview process or if it’s been a long time, it might be wise to schedule an interview for a job you don’t plan on taking just for the practice.",1521909007.0
rosulek,"Here is a [visualization](http://rosulek.github.io/vamonos/demos/bellman-ford.html) that I made. You can draw your own graph and see the execution of BF on it.

I don't understand that graph because ""w"" is not a variable name in that algorithm. Maybe it should refer to D(v). The reason it changes each row is because in each iteration the variables u & v change which vertex in the graph they refer to.

BF algorithm is useful when there are negative edge weights. In the absence of those, you would probably use Dijkstra instead.",1521850282.0
dark_phantom,"You can reach a through b, you would be passing 2 edges, one of which has a weight of 2 and the second  has a weight of -2, hence the total distance is 2-2 = 0

As for b, there’s no shorter path than that of weight 2, thus d(w) for b is 2",1521850712.0
magnificentbop,"I think the first row is in error. I believe D(w) represents the known weight distance to the destination node at that point, so it should be 3.  That follows the general pattern of the algorithm.

I learned about the algorithm in a networks class; in early internet networks, it was used to decide the paths of layer 3 packets.  It has since been replaced by the link-state algorithm.",1521851165.0
tepkel,"Not really the right sub... (**sub**nautica, get it??) Compsci is about pretty high level theory and concepts. Think of it like theoretical physics vs engineering.

Regardless, this is probably not worth trying to set up until they release a precompiled alpha or beta.

GitHub is more or less a place to store a bunch of code you've written and want to collaborate on with others. This is a set of instructions on how to pull down a copy of that code into visual studio.  

Visual studio is a program used to write code. Most versions of this program cost a pretty significant mount of money. You might be able to get away with express here though... Not sure.  

Then there are more instructions on how to compile and get the code working with the game.

This is pretty much aimed at people who are actively developing this. Probably with the hope that more outside coders will join in and help. This is likely going to be super unstable, and not really worth it unless you are contributing and writing code. Just wait for a release.

Multiplayer subnautica though, huh? Sweet.",1521844149.0
Edg-R,"Every time I look at job application posting on LinkedIn (you can see stats), it shows that for an entry level position, there were like 30% applicants with a BS and the rest are people with masters. 

It blows my mind that I'm competing for an entry level position with people with a masters degree.",1521834044.0
lrem,"It's only catching up to Europe, where jobseeking advice is ""hide your PhD, it brands you as impractical and probably lazy""...",1521810387.0
bende511,"I’m about to graduate from my MSCS at UMass Amherst. I did it because I thought I wanted to pursue a PhD and it would make my application stronger (which it certainly has), but now I’m not so sure that is for me. In the meantime I’ve gotten to work on some cool research projects, I’ve lined up a much more interesting and high paying job than I had before, all on in-state tuition. Honestly, that last bit is key, as all told tuition will end up being under 30k for two years. 

",1521824313.0
ioquatix,I did a MSc in computer science because I loved what I was doing and I love learning new things. It had value to me and that will never decline.,1521877304.0
ice_planet_hoth_boss,"I got my MSCS from UChicago.  It's the epitome of the coursework-only degree described here.

I would describe the degree as basically a 2nd bachelors degree; maybe even less, since it was common knowledge that the undergrad version of any course (Algos, Networks, Systems, etc) was tougher than ours.  The program was/is absolutely a cash cow for UChicago as it tries to build up its CS dept - case-in-point, the intro systems course, for which you pay $6000, is Nand2Tetris.

I harbor regrets about spending so much time and money on the program.  But I try to bear in mind that (i) The economy forced a lot of mid-career people to learn new skills and pivot in their careers, and (ii) We are still figuring out the ""ideal"" way to give people the skills they need to pivot and succeed in a CS career.

The MSCS program wasn't perfect, but it offered a lot more depth than a coding bootcamp.  I work at a trading firm and we hire devs - I can't tell you how many candidates come in for a job that requires, say, C++ and a deep knowledge of algos, who know nothing more than front-end web development.

The right solution probably involves giving these MSCS degrees a different name.  For example, my undergrad school, Penn, offers a similar degree called ""MCIT"" - master of computer and info tech.",1521816456.0
RHC1,"Currently applying to Georgia Tech for a masters. Been in the industry for 2 years now. My main reason is because I want an edge in the work force and I actually look forward to learning higher level concepts that will build upon my BS in computer science. I didn’t get in yet but $7,000 for an MS and my employer will even pick up the tab; I would be a fool not to take advantage. Companies love MS grads and I only blame the private sector for driving the inflation of MS CS students.",1521834308.0
hjqusai,"There are like 500-600 students in my first semester MSCS course at USC.  It is definitely a cash cow.  On the other hand, it was the only program that accepted me, and I really really wanted to go back to school and learn CS (my undergrad is in math and econ).  So, honestly, I don't care if it's a cash cow or if my degree is ""devalued"".  I'm sorry that the interview process is a little bit more tedious for people who want to use the Degree as a filter, but IMO that's a stupid way to do things anyway.  It's pretty easy to tell when someone doesn't know what they're doing, degree or not. ",1521842776.0
carmichael561,"It isn't just an issue with CS. At Stanford, the MS in Statistics is also a joke. 

If you're doing a PhD in another department, all you have to do is take the classes and they'll give you the degree. The classes are useful and taught well, but it was way easier than anything I did as an undergraduate.",1521843613.0
exorxor,"Not all CS master degrees are worth the same, even from the same university from the same year. The courses you took, the things you have actually accomplished, that matters. 

I wish everyone the times of great education that I have received. Not every single one of your professors is going to be stellar, but if you have a few, their culture and way of thinking will permeate your thought process. 

I am not sure how to put an absolute number of the value of a recent CS master degree, but I know mine is worth its weight in francium (just taken because gold is too cheap).",1521911269.0
benium,"I feel like the value of a BS in CS has gone down too. Maybe it’s just my personal experience, but I haven’t been able to find a job because I have little to no work experience in my field. I feel like if I had gotten a job straight out of high school and started as an intern, I’d be more prepared than I am now having a degree. Maybe just me though. ",1521818320.0
Pour_Louis,"I have been thinking of taking a Post Grad Certificate in AI from Standford. I have a B.Sc in Mechanical Engineering. It would seem, according to this article, that a post graduate degree in CS is not particularly valuable. 

Would that make a Post Grad Certificate nearly worthless?

I'm trying to move out of the energy sector (O&G) into tech. 

Edit: https://scpd.stanford.edu/public/category/courseCategoryCertificateProfile.do?certificateId=1226717&method=load
",1521815906.0
prions,"I'm almost finished with my MSCS at a mid-tier state university, and I'd say in my situation it was worth it. There IS a lot of value in the degree if you have the drive to seek it out, but if you're just on the rails of your program you won't get your money's worth. 

I have an unrelated undergrad in engineering and worked professionally for ~2 years before going back. My impression of Bootcamps is that they're a waste of money and don't provide any value besides getting your first job. I wanted to learn as much fundamental CS knowledge as I could and not just beeline towards a webdev position in 3 months. 

I think a lot of the criticism comes from schools who tolerate low standards from the students. A significant portion of students in my classes are undecided about their career paths or international students. Both groups tend to skip classes and cheat on exams. Students like this don't learn anything significant and coast towards their degree expecting to get a job. Then they complain how their degree was a waste of money. 

Others say why not self study? Self studying is a big part of my time as a MSCS student already. If you're a student and don't do anything outside of class, you WILL NOT get your money's worth. A lot of people who push pure self study advocate that you can do it in your spare time. When you work 40+ hours a week and have other obligations, you don't have a lot of spare time and the priority of your studying is low. Also being a self taught student, you miss out on the mentorship and connections gained in grad school and ultimately lack any concrete proof that you did anything (a degree). Regardless of what people say online, employers DO value degrees. 

My outcomes have been pretty great so far. I've leveraged my 4.0 gpa, school connections and self study to really push myself ahead in job competitiveness. Last summer I had an internship at Nokia, and this summer I'm going to be doing a ML/Deep Learning internship at an industry leading startup in NYC. 
",1521831037.0
istarian,"This reads as a lot of rambling. There are clearly several different factors hinted at in the article. Namely what an MS degree is ""supposed to be"", where the ""value"" in a degree comes from (practical skills? possession of knowledge?
 ability to think?), what the MS degree at a particular school actually is.
  
Is it possible that the general ""value"" of any degree is at least partly subjective? In whose eyes is it *declining*?  
  
P.S.  
Obviously I have outside view since I have merely a BS.",1521922146.0
help_throwaway96,"as I said in another post, math and physics are sometimes even better prepared than computer scientists. CS content is much easier to pick up on our own, especially with the online resources. doing  a MS in machine learning will make you less qualified than doing a bs in math and reading about it online. it's very frustrating in my opinion. there is nothing unique about being a MS computer scientists that other with at least some experience in programming and math could not pick up easily. ",1521812974.0
fnybny,How are masters programs cash cows.  The university pays **you**.,1521820311.0
supershinythings,"If you're female you still need it or they won't give you the time of day.  

In over 25 years I have yet to meet a top level female senior engineer that didn't have at least a master's. Many had a Phd too. In contrast, I know dozens of guys doing extremely well without any formal degree. Like I've seen guys at the Fellow level - a rank they rarely if ever grant to women regardless of schooling - at most of the top companies I've worked at or with.

Nobody wants to believe that women can do this without a degree just as well as men, so they just never get that chance.

So really don't slack on the degree if you're female.  They won't let you in the door without it.  If you're a guy, well, it's less of a big deal than it used to be.",1521854865.0
,[deleted],1521835695.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1521785959.0
UncleAlanC,"I've been looking at how IPFS is implemented, and I don't understand how the merkledag is implemented. Can someone explain to me how it works and how it really makes the protocol so innovative? Code snippets preferable. Thanks!",1521771563.0
brettmjohnson,Can you elaborate? I don't really understand your question.,1521778694.0
,"Using excel.
In fact most testing software will document it for you and output a spreadsheet ",1521761153.0
bshechter,I see what you did there,1521756813.0
redditaccount1426,"As a student I can say the vast majority of plagiarism I’ve encountered amongst my peers has been in a last minute struggle to meet a deadline.  I’d wager that late policies may actually  substantially reduce the amount of plagiarism that occurs, if implemented correctly.

Edit: a lot of you are replying with things like “it’s the students fault we shouldn’t cater to them etc etc” and it seems to me you’re missing the point... if your goal in being a professor is a trial by fire where only the strongest survive your course, then sure, just punish blindly. But if your goal is to maximize the amount of students who walk away having learned the material, try a different approach than “if they miss the deadline fuck em.” Just my 2 cents.",1521752673.0
grandzooby,"I suspect part of the problem is trying to catch the cheating after the fact.  Setting up a process for doing the work can make it easier to prevent and detect.   For example, I've seen teachers set up a git repository where students have to check their code in as they work on it.  This makes it really easy to spot someone who suddenly submits a perfectly working piece of code at the last minute that's nothing like what they have been checking in all along.  If you only commit one solution on the due date, that teacher severely marks down the assignment... a key part of the assignment is regularly committing their work in-progress.

I teach a class in computer modeling & simulation.  In my first-week lecture, I show them all the sites I know of that already show implementations of models we'll cover (e.g. Rosettacode has many examples of estimating pi using Monte Carlo simulation).  I welcome them to refer to these in their projects.  But I also ask more than ""write a monte carlo simulation that estimates pi"" - I expect them to do a write-up of what they did, what was challenging, what they learned.  I also expect them to ""explore"" the problem, like how the solution changes with the number of iterations, and how they decided when to stop the simulation.  Of course, I can't use an auto-grader with this method, but my class sizes are small enough that grading isn't too difficult.",1521755695.0
YourFavoriteBandSux,"Here's what I've been doing for the past few years, and no one (except the cheaters!) seems really upset about it:

* I encourage students to talk, plan, and draw pictures together, while reminding them they must code by themselves
* I give an in-class on-paper quiz for each project, on the due date, that's worth 50% of the grade for that project

The quiz asks the sort of questions that, I hope, you wouldn't be able to answer unless you wrote the code.  So, that way, if you copied someone else's code, I don't have to try to detect it, because you aren't going to pass anyway.",1521767124.0
SudoKudo,"Currently in Computer Science 2. The university I attend always scares students to not share code. To even speak about your code is equivalent to saying ""his name"" in Harry Potter.

Most students hate this, as in real life, you will always have resources to help you. In my situation, I don't copy paste code, but I do talk with friends about how they started a problem or used a certain algorithm. If professors let students collaborate, discuss code with each other, and opened the classroom to be more inviting, there wouldn't be such a stigma around this issue.

You can never stop plagiarism. If people want to cheat, they'll cheat.",1521763390.0
NovelDame,"I'd also suggest an ethics course specifically tailored to CompSci. 

Explain why cheat sites masked as homework help are unethical, and explain how searching StackExchange to understand boolean operators is different from copy-pasting someone else's code that meets the requirements for that week's assignment. This helps to distinguish between academic expectations and corporate expectations. ",1521751612.0
elr0bert0,"Sharing technical design ideas is appropriate and should be encouraged, but sharing verbatim code is unacceptable. ",1521768371.0
lambinator1996,"Personally plagiarism will come back to bite them in the end even if they don't get caught because when the real life scenario comes along. They won't be able answer.

However in my university i was accused of plagiarism once because i copy and pasta'd 2 lines of code from the opencv documentation. This kinda stuff shouldn't be plagiarism because its the literal documentation but the copy and pasta of whole blocks should be regarded as plagiarism.",1521760648.0
tmarthal,"It helps to remember that if the students cannot do whiteboard implementation problems, they will also not be competing with the students that can. ",1521772410.0
NotInUse,"Decades ago when I was still in school half my data structure’s course was caught cheating which is most notable in that only a few assignments from 250 students actually ran (no one could make the stolen code run.)  The checker used at the time was pretty good at understanding common code constructs so you could rename everything and even make other cosmetic changes and it would still correctly identify commonalities in assignments handed in.

Unfortunately it doesn’t stop in college.  My last company had significant amounts of stolen code in their products and because the code stolen was ancient, riddled with bugs and no longer had the copyright notice it was never updated with the decades of security fixed of the original code.

The same goes for the “architects” at that company with no hardware, software or systems skills who didn’t have the ability to read or comprehend what they stole (by the time it’s an entire presentation with only the names of the original authors removed I say stole instead of plagiarize) and got shot down repeatedly for the ludicrous irrelevance to the problems that needed to be solved.  The executives they reported to couldn’t fire them as it would be clear they failed by promoting [other] such clueless people to their positions.

**tl;dr:** for many this is a lifelong problem, not an academic one and companies often reward the behavior.",1521772840.0
jgomo3,"Because you try too hard.

Let that happen and evolution will do the rest. Then the stupid kids who didn't actually learn will wish to go back in time and choose a different career when they realize they can't perform well in the real world.

The real learners are not concerned on what the rest of the class receive or not. What they want is to receive knowledge and abilities.",1521764463.0
Rollingrhino,"I like my current professor's approach, use whatever code you want, just have comments explaining what it does in detail",1521758246.0
thephotoman,"I feel that reducing toy assignments would help, and instead, actually require meaningful open source contributions would work well. 

I mean, CS101 is going to be toys, but let’s reduce its use after that. ",1521757840.0
corruptbytes,This is why some of my professors have either been: changing the assignments every semester and/or making students contribute to a current open source project.,1521772976.0
strobelight,"Just tell them that if the plagiarism detector comes back positive, they will receive no grade on that homework assignment.  Instead, their final exam grade will be used as their grade for that assignment.  Ultimately, the cheaters will pay the price at the end and the instructor doesn't have to use valuable time monitoring students' bad behavior.
",1521827557.0
ptitz,"Just switch focus on group assignments with randomized tasks. And give grades based on results, not on syntax. Voila, problem solved. At my school we just only had 2 week programming primers followed by  6 weeks team projects. At some point the approaches between teams diverged enough to make any plagiarism difficult. Plus there was often an element of competition to it, so many students got the drive to show off.

Who gives a shit about code plagiarism anyway? If a student can just plug a lib or re-write someone else's code, chances are they will be able to do the same when they start their working life...",1521831982.0
Admsugar,I am a software developer but also do code sample reviews for candidates. You would be astounded as to how much copy paste I encounter.,1521994397.0
istarian,"Honestly I always found the assumption that you were going to try and cheat offensive.  
  
At the end of the day functionally similar code is going to look similar too. Sure you could write some code comparison tool, but maybe the course grading shouldn't be so heavily weighted on coding projects?",1521778083.0
iamrob15,I think this is a problem with the education system and computer science is a part of the education system. I think there are ways around plagiarism such as more presentations and collaboration in the classroom. You can also setup a github to see who is collaborate what in group projects. It's pretty easy to call someone out on their BS during a presentation if they have no idea what it is doing or have a weak understanding. ,1521767303.0
bmckune,"I'm so confused - why is code copy and reuse plagiarism?  You teach the students to model and abstract through code reuse but yet you require them build their own code line by line.  

That is a confusing concept - understanding that we all need to learn the pieces that comprise coding; but seems counter productive.  IF they can't explain the code snippets then I might deduct points, but reuse is what we, as software engineers, are 'programmed' to do - excuse the pun.",1521815737.0
uni3993,You can't stop plagiarism as take-at-home assignments are black boxes you could never be sure that a student won't cheat and you can't catch every cheater a good plagiarizer can go unnoticed with little effort. The only way to stop plagiriasm is by having exams instead of take at home assignments. In assignments you could never be sure the student is the actual person who does the assignment.,1521767900.0
DasWood,"Solution:

Don't confront cheaters. Offer no clemency except those who come forward on their own. After your reviews are in. Clam them with an F. When they dispute it say you were forced to revise their course grade to F after indisputable evidence and that they've been reported to the department. Point out that according to institution rules they will probably be expelled and they are welcome to take the course again with a different professor at a different institution. 

If you want to play the game, at least do it well.

EDIT: You can even put that in the syllabus. If you regret cheating come forward. We'll work with you. If you cheated and don't they won't be confronted. They will be dealt with after the semester ends.",1521783105.0
BrothermanGrill,"I would recommend using THREE.js, although THREE.js is a library for 3D rendering you can also easily use it for 2D animation too. They have a camera specifically for use with 2D scenes: https://threejs.org/docs/#api/cameras/OrthographicCamera

Their library has great docs and there are lots of examples to learn from.",1521748304.0
Free_Math_Tutoring,"What do you mean by dark net? What do you mean by knot? 

Basic idea of creating a dark net:

1. Create a network
2. Don't make it accessible via HTTP-Requests to some DNS
3. Make it accessible in some other way

It seems to me your goal is very ill-defined.",1521742111.0
rPrankBro,Work and personal experience is far more valuable then multiple qualifications. I did a BS in CS just to be qualified but I'm still mostly self taught through personal projects and work. ,1521741543.0
QuincyQueue,"How much programming have you tried? If you haven't tried much, I'd say try more to see how you feel about it. If you like it a lot, CS might be for you. Otherwise I'd steer clear. ",1521866019.0
,"CS

Goodluck. IMO i dont think you should waste your time+money getting another BS degree though.",1521740597.0
jebuskid,"I am taking CS 537 at Wisconsin with Remzi this semester. He records all of his lectures, so those should help. Straight from the source!

http://pages.cs.wisc.edu/~remzi/Classes/537/Spring2018/Discussion/videos.html",1521734173.0
WonkySheepDevil,"Check out David Black-Schaffer and Mike Murphy on YouTube. I didn’t watch much of the latter, but looking back I think it would have been helpful. David has some great videos covering virtual memory which is arguably one of the hardest concepts out of that class. 

This is all still fresh in my mind since I took my final yesterday for OS, and we used OSTEP and xv6. Do you happen to be at Portland State?",1521728472.0
ithurtsus,Lots of devs smoke. I'm not sure if many do this weird talking thing on their smoke breaks though. Seems like it's 99% staring into a phone,1521699308.0
John_Titor_2001,I chew pouches because they're discreet and you don't have to go outside for a break. I used to smoke though.,1521754228.0
,[deleted],1521698214.0
Nerdlinger,"It falls into a lot of different categories CS, IT, math, engineering, psychology, etc. ",1521722239.0
dewayneroyj,"Cybersecurity is part of the broader field relating to Computer Science. You can read more about other people’s opinion [here](https://www.quora.com/I-want-to-work-in-cyber-security-should-I-major-in-computer-science-or-information-technology-with-a-focus-in-cyber-security?share=2f9b2624&srid=hjJzG).
",1521696377.0
MisquoteMosquito,My bsCE roommate is doing cybersec phd 🤯,1521699661.0
johntdowney,Depends on how deep you get into it.  There are surface level aspects in IT and in-depth aspects in CS or CE.,1521940958.0
ievic,C/C++,1521694322.0
xShadowProclamationx,it depends on the attack surface.,1521692269.0
crabbone,"If viruses = malware, then probably VBA, since MS Excel is the easiest target. Second would be JavaScript, but, who knows, maybe first. Again, since people do crazy things in their browsers. Plus there's MS Outlook, which likes to execute random MS extensions of HTML / CSS. I'm guessing that ActionScript was high on that list once, since Flash was ubiquitous, and it was also possible to embed Flash applets in PDF which would only affect Acrobat, but hey...",1521717045.0
SimonSezRUFF,bug bytes,1521694305.0
SpacePally,Most I’ve seen are written in Python,1521691446.0
IJzerbaard,"There is probably not a whole book about ""where does data go"". It's mostly straightforward, except local variables which are funny. Members are wherever the object is (trivially since the object *is* the collection of members, optionally plus extra vtable pointer and padding - so wherever the members are that's by definition where the object is). std::vector has a couple of members, including a *pointer to* the elements - normally the elements go in a heap-allocated array (but you might do funny things with custom allocators). This should be sufficient to answer the question.

Local variables are a bit complicated, a situation not at all helped by everyone and their dog posting online ""oh yea they go on the stack"". They *can* go on the stack, but a big part of the compilers job is making them *not* go on the stack - putting them on the stack is the back-up plan. For example by omitting them entirely (in trivial cases) or by using registers, [even for aggregates like vector](https://godbolt.org/g/E4DWsW) (though I'm not 100% happy with that code). It's also not as simple as putting a variable in a register for the entire duration of the function, that goes deeper than you likely wanted for now, but you can find out about this in compiler textbooks.",1521690092.0
Meguli,There is a relatively new book called Low-level Programming by Apress and it seems to go into nice details about these things. I am at fourth chapter and things are OK. Not always crystal clear but nice. ,1521694410.0
LongUsername,"Herb Sutter's Guru of the Week #9 is probably your most concise reference.

http://www.gotw.ca/gotw/009.htm",1521726759.0
iwantashinyunicorn,"How many numbers do you think you'll have to add together to be able to transmit a request to execute one instruction to another host? How long do you think the longest ""machine code command"" takes to run, and how long do you think it takes to transmit a signal over a network to a computer located one metre away?",1521678876.0
Peter-Campora,"I would start with Software Foundations before reading CPDT. Starting with those two should be plenty for now. You also might want to read some books on type systems, too.",1521903900.0
jwall013,"If you already have a surface-level understanding of several formal verification techniques, it may not be a bad idea to start looking for introductory textbooks and significant papers on each of the major ones (explicit and symbolic model checking, abstract refinement, proof systems like PVS, etc.).",1521906532.0
HomeTahnHero,I started out with Sipser in undergrad actually,1521684279.0
Zophike1,"> I've taken a deep interest in formal methods through one of my graduate level program analysis courses. We only touched briefly on the theory side, but now I want to go deeper since I find the theory and applications of it fascinating.

Oh interesting what class was this and what were the requirements, also I asked a similar question  of a less Sophisticated nature [here](https://www.reddit.com/r/compsci/comments/822szv/questions_about_formal_verification_in/). But you will get some immediate answers to your questions on #sel4 on freenode",1521687756.0
hippydippyhippy,Did you break their Bubble? ,1521668065.0
IJzerbaard,"Not really that neat or even a trick, that's the standard way to do it.",1521688119.0
BobFloss,"There's not really such a thing as replacing an entire process as far as I'm aware. I've never heard of Loki, but they're probably just trying to (incorrectly) simplify what is going on with that process. Looking at the babe, it's probably just injecting into other processes that are playing audio in order to apply effects to them.

Edit: just looked up Loki. Paste in the exact text it outputs or a screenshot of what it says. Also run a scan with Malwarebytes, Hitman Pro, and Norton Power Eraser to see what they have to say if you haven't already.",1521704742.0
leeabc13,?,1521660192.0
paul_miner,"Scheduled task, or remote admin if you're in a corporate environment?",1521658231.0
SOberhoff,"I didn't realize the book they wrote was so groundbreaking. I've considered reading it (at least partially) in the past. But ultimately engineering wasn't my cup of tea.  
Also isn't the scope of the Turing award a little large? I would've thought that the intellectual heirs of Turing would've been people like Edmonds, Levin, Håstad, Wigderson, Goldreich etc. ",1521659053.0
,"I should finish reading their Computer Organization and Design book...

Stopped somewhere in the middle.",1521663758.0
MCPtz,"Excellent book. Everyone has it, so they must be rich... right >_>?",1521674493.0
link23,"Wait, so Pi types are just type constructors? I always thought there was more to it than that...",1521595102.0
CodaFi,"Ah yes, the Σ Type: A “sum” that’s actually a *product* but (often) modeled by a *lambda*.",1521607730.0
redpilledcuck,What?,1521590924.0
cottonycloud,"My favorite ways of learning math is actually watching Youtube videos of 3Blue1Brown. Alternatively, if you're still in school, you can try looking for some fun-sounding math seminars, and more importantly, have a chat with your professors. ",1521575905.0
mynewpeppep69,"I felt similarly about calculus, but then I took some proof based courses and realized that the math I grew up learning wasn't really math, it was just applied math. If it makes you feel any better, calculus is the kind of thing that in the real world you look up as needed. If you really want to understand what's going on with calculus, take a real analysis course. You'll probably see this kind of math in your algorithms course, and it's more applicable to computer science in general. Maybe not as much software engineering, but as a CompSci major I assume you want to learn more than how to program best. 

To sum up, if you want to love math again, then look into proof based courses with reputable teachers. Otherwise, I don't really have advice for you. I think computational math is genuinely uninteresting and it sounds you might feel that way too after all of that time. Also, don't be so worried that you got a C, just focus on getting a wholistic education that allows you to develop your passions. ",1521574498.0
link23,"As someone else already said, I love the *understanding* that comes from math, not the formulas that I'm supposed to memorize. So calculus etc weren't super engaging for me. However, once I started doing proof based classes, I was hooked. Working a proof is similar to working a program in that you have to think carefully about edge cases and base cases, and construct your reasoning rigorously; so writing proofs comes naturally to a computer scientist. (In fact, there's a [theorem](https://en.wikipedia.org/wiki/Curry%E2%80%93Howard_correspondence) that says programs and proofs are really the same thing, seen from different perspectives; it's pretty cool.)

So, I'd recommend taking a proof based class (my favorite was abstract algebra, but I started with linear algebra), and seeing if that does anything for you.",1521582548.0
discoFalston,"I used to love math, then I hated it, now I love it again.

The problem I had when I stopped liking it was stress for how much work it was combined with my inability to see the application while I was taking the classes. It’s important to me to feel like I’m learning something I could use, and though I knew in my head it was useful, I didn’t feel it. Though, once I got into classes that used the math I learned I started loving it again.

Maybe that’s similar to your experience. When I was taking Calc, I did well because I treated it like exercising or working out, and I got competitive with myself. So it wasn’t enjoyable in the moment, but I felt good after the classes were over.

Last suggestion - check out Numberphile or Mathlogger or 1Brown3Blue on YouTube. All these channels explain concepts clearly and with enthusiasm. They helped me get back into it.",1521582652.0
tulip_bro,I picked up Coq ide while following the software foundations lectures and free book.,1521819261.0
checksum420,"A mind for numbers might be the nudge that you need, I can tell you that it was a game changer for me. I'd recommend that you relish in the practice of getting deep into the weeds, even if it doesn't make any sense. Drill enough and your model of mathematics will start to become more clear and frankly mind-blowing. Then it snow balls from there. Also numberphile is a great youtube channel to get you interested in some fascinating topics.",1522018125.0
mew_bot,Multi Agent systems,1521571753.0
RegExMaster,"Genetic algorithms/programming are quite interesting topics and are not difficult to get your head around. You can look into solving the n-queens problem with a genetic algorithm for instance.  

",1521574785.0
tevert,Never heard of it. Talk to your teacher or academic advisor. ,1521571408.0
DrApplePi,"My university requires it.  But it's hard to find anything about it.  There are apparently no books for the CS test on Amazon, even.
Searching on google, all the first results are for the website.  

They do have some practice problems. Probably not a lot on it's own, but it should be helpful in determining what you should be looking at.

https://www.ets.org/mft/about/content/computer_science",1521756597.0
BobbyChou,"There are 100 users in the bootstrap table that each should have a unique ID from 0 to 99 . Are there any libraries to do this ?
This is the link to my code: https://pastebin.com/9DPW68hi
And this is the result: http://campers-leaderboard-cmtran7393.c9users.io:8080/",1521519703.0
jeschner,"When I took an algorithms course at school we used Algorithm Design by Kleinberg & Tardos and I thought their explanation of asymptotic upper/lower/tight bounds was spot on. I still refer to it when I have questions or need to remind myself.

It would require you to find the book online or buy it yourself, but I’ve found that it’s worth the money. When I bought it I found an international version for ~$40 back in the day.

I’m sure there are other books that can explain it too, this is just the one I used.

Cheers


Edit: Found the book online: http://www.icsd.aegean.gr/kaporisa/index_files/Algorithm_Design.pdf. Page 31 of the pdf (36 of the book itself) is where the explanation for Ο, Θ, and Ω starts.",1521551153.0
holygoat,https://cathyatseneca.gitbooks.io/data-structures-and-algorithms/content/analysis/notations.html,1521572486.0
cullina,"If a sequence is not Ω(f(n)), it has a subsequence that is o(f(n)). If a sequence is not o(f(n)), it has a subsequence that is Ω(f(n)). If you interleave a sequence that is Ω(1) with a sequence that is o(1), the result has neither property.

1/ω(n) is an upper bound and ω(1/n) is a lower bound. If x ≥ y and f is a decreasing function, then f(x) ≤ f(y). You apply the same idea to exchange the order of decreasing functions and asymptotic bounds.",1521520247.0
,[deleted],1521525038.0
keksper,"Do you have a mathematical undergrad background at all? Specifically, linear algebra, discrete mathematics, and statistics?",1521549746.0
balefrost,Church is considered to be a founder of computer science.,1521516145.0
HeraclitusZ,"I think the premiss of your question is wrong. Church *is* considered a foundational figure of computer science. Turing just perhaps gets special distinction for providing a computational model that, as Godel pointed out, was obviously programmable.",1521516378.0
daerogami,https://www.reddit.com/r/compsci/comments/4vhxnk/why_is_turing_considered_the_father_of_computer/,1521516302.0
Nerdlinger,He is.,1521516175.0
thereticent,"Why do you continue to ask this question, when the answer is that he is? If you want people to educate you, you need to show evidence of being educable.",1521518099.0
PythonGod123,Just for a second I thought the question refered to the actual church... of the holy kind...,1521518565.0
GNULinuxProgrammer,"Imho it's really hard to trace at what point mathematics turned into CS, a lot of students of David Hilbert and Alonzo Church produced interesting results that could be considered foundational to CS. Take Emil Post's recursive functions, which is equivalent to Turing Machines and lambda calculus. I think it is more interesting of a question that why that generation of mathematicians started thinking about computational questions. I'd guess it's a natural extension of the foundational debates of late 19th and early 20th centuries.",1521520058.0
turkish_gold,"Honestly, I thought the Church–Turing thesis was the foundation of computer science. ",1521526219.0
panderingPenguin,He is by anyone who knows what they're talking about. Turing just has a more compelling story and a movie about him which is why he's more famous with the general public.,1521517247.0
jmite,"Church is the Grandfather of Computer Science. Just take a look at his list of supervised Grad students some day: Kleene, Rabin, Scott, Turing, Rosser, the list goes on.

",1521566899.0
nrlb,"I guess that would be computational fluid dynamics?  Though if the clay is cheap, might be easier reference common designs, build out those designs with some hypothesis, measure and records results, and test them under like conditions under which they will be evaluated.  Because if the CFD doesn't get good input (i.e. you'd have to test anyway), you'd get a garbage in garbage out scenario.  I'd see if the model matches your measurements under your constructed test if you were going to model at all.

Unless the project is specifically about using computational design to build boats, and not about just building a boat.

In any event, investing in a similar amount of material and replicating conditions of the test will probably be as equally important as any modeling you do.  Especially if you have a fixed time table to complete your project, because you'll spend a good amount of time learning how to model and not testing your designs.  Not that both aren't worthwhile endeavors, depends on your end goal.

This looked interesting after some googling:

https://www.simscale.com/blog/2016/11/boat-design-5-simulations/",1521486645.0
MondayMonkey1,"Bouyancy is just displacement * densitywater - weightclay right?  And if you have a fixed mass of clay, the second term is constant and the problem boils down to having enough displacement.  In other words, the boat floats if displacement * densitywater > weightclay.  Since water density and the weight of clay are constant, you can  derive a minimum displacement quite easily.  

Can you easily determine volume with your cad models? ",1521491132.0
mew_bot,"What models do u have? 
Cad? 
",1521483921.0
iambeingserious,Spam. ,1521450652.0
pineappleinferno,"I have taken courses through both EDX and Udemy. 

I found that with EDX, it is more directed towards the academic world. You will go into much more detail about whatever it is you are studying and you will learn more.

With Udemy, I found that the learning material is more directed towards the practically minded individual, meaning, someone who wants to use these skills for a business for example. 

I learned a lot of useful stuff from EDX, but had no idea how to use it for freelance development until I studied on Udemy. 

Also keep in mind that Udemy courses seem to be video lecture based. Some EDX courses are similar but the courses I took on EDX were text based (which I liked). Everyone learns differently.",1521470470.0
Meguli,SICP,1521453115.0
eveninghighlight,"you know, you should take online courses on Udemy",1521464136.0
not-just-yeti,"[How to Code: Systematic Program Design (Part I)](https://courses.edx.org/courses/course-v1:UBCx+SPD1x+2T2016/info) on edx.  Like the title suggests, it's an underlying approach to writing programs, and *not* just ""here's another language feature, here are three examples, now you go write a program that uses this feature"".  I'd finished my Ph.D. in comp sci, and even taught some programming courses; coming across this approach transformed my notion of what programming was (and, how to teach programming).

The videos are also on [a youtube channel](https://www.youtube.com/channel/UC7dEjIUwSxSNcW4PqNRQW8w).
The course is based on the intro at Univ. of British Columbia at Vancouver.
",1521483262.0
lanemik,"Find some math courses to take. Discrete structures is a good choice. Maybe get a copy of Coding the Matrix and/or Learning Math with Python. There is a lot of great lectures on Linear Algebra you could find, too.

Your CS degree will have lots of math that you probably haven’t been introduced to yet and not quite as much programming as you’re probably guessing. ",1521510193.0
eeeeeln,Leetcode,1521468368.0
me_alive,"These are good Coursera courses about Algorithms and data structures:

- https://www.coursera.org/learn/algorithms-part1

- https://www.coursera.org/learn/algorithms-part2

I'd also add smth about functional programming like this 

https://www.coursera.org/learn/progfun1 

And maybe cryptography. There are many coding tasks there:

- https://www.coursera.org/learn/crypto

- https://www.coursera.org/learn/crypto2
",1521480724.0
redpilledcuck,"Tim Rougharden's Intro to Algorithms & Data Structures.

Doesn't have anything to do with programming, CS is not programming.",1521498869.0
dawaeseeker,How about ccna cource a lil bit time towards networking ,1521492981.0
16bithustler,"Given that you've worked with Java, you may consider giving Android a go. There is a series of self-paced bits offered by Google that are more than enough to get you started on publishing a proper app to the Play store. Along the way, you can mess with Kotlin as there are some snazzy language features not present in Java natively.

Check out:

* [Fundamentals](https://google-developer-training.gitbooks.io/android-developer-fundamentals-course-practicals/content/en/)
* [Advanced bits](https://google-developer-training.gitbooks.io/android-developer-advanced-course-practicals/content/)",1521503054.0
seanieboii1105,Codecadamy - free courses for all languages (what I used to learn python) ,1521457563.0
Jegeru,"Colleges like Berkeley and MIT have recordings of past classes free for public use. The AI class I took on Berkeley's website even had instructions, homework assignments, notes, etc. It was just like being in an online class, except you couldn't ask questions and the actual class happened 3 or 4 years ago. ",1521487233.0
Inght,Everything you need https://github.com/ossu/computer-science,1521512326.0
,Bump,1521447438.0
Kann0r,"Training.Talkpythontraining.fm

Corrected link. ",1521447873.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1521400361.0
javaHoosier,"This is great. We are going over Decision Trees in my AI class. The implementation is more sophisticated than this, so it is confusing in certain places. Going over this gave me some aha moments.",1521449337.0
mayumer,Isn't the asker the guy who Google screwed over by patenting one of his ideas?,1521381979.0
kernelhacker,"What did you mean by ""but storing the n values without information about their order""? Bloom filters already don't store order?",1521384899.0
biduzido,Interesting... Definitely going to take a closer look later,1521384474.0
twbmsp,"Not sure about the relationship with bloom filters, which do not store any order information on the elements so I am wondering where this ln(n!) gain comes from. But I love the  trie entropy result you gave. =) I will have to think about it. ",1521416389.0
ssjskipp,"Wait, is this saying that when I know every element added to my set, it only takes 2.3... extra bits of information to add in, ""given an element, is it in my set?""",1521431022.0
jahewson,"Graph databases are often surprisingly simple. Here are some papers describing recent systems:

[Hexastore: Sextuple Indexing for Semantic Web Data Management](http://www.vldb.org/pvldb/1/1453965.pdf) 

[TAO: Facebook’s Distributed Data Store for the Social Graph](https://www.usenix.org/system/files/conference/atc13/atc13-bronson.pdf) 

[The Gremlin Graph Traversal Machine and Language](https://arxiv.org/pdf/1508.03843.pdf) 

And if you want a distributed system:

[Pregel: A System for Large-Scale Graph Processing](http://www.dcs.bbk.ac.uk/~dell/teaching/cc/paper/sigmod10/p135-malewicz.pdf) ",1521392841.0
cedg32,Neo4j has a number of tutorials on their site.,1521411706.0
vsync,It's rather about an object database but you might find the documentation for Rucksack interesting. Sadly the links I found seem to have rotted.,1521353165.0
,"The truth is that there's no magic bullet. There's this weird cargo cult mentality in which people think, ""Company X used management technique Y, and they made a great product. If we adopt their process, we're sure to succeed!""

I think the ideal approach is to hire smart and motivated people, put them on a worthy project, and get out of the way. The best management is that which manages least.",1521326821.0
balefrost,"This would be more appropriate in /r/programming than /r/compsci. 

I'm curious how ""waterfall"" looked for you. In by-the-book waterfall, you first go through a requirements gathering process. Once you're satisfied that you have all the requirements, you then move on to design. After you fully design the whole system, you then move on to implementation. Only when you've implemented everything can you then start to test. It's a very formal, very rigid, very slow process.

That's probably why ""waterfall"", when originally proposed, was meant to be an example of a broken process.

I'm guessing you weren't doing that. I'm guessing you were doing some planning up front, but otherwise just sort of burning through the work you needed to do. That sounds a lot like an agile methodology - in particular, it sounds like Kanban.

Agile doesn't demand that you spend half your week in meetings. Scrum doesn't even demand that. Standups should be 10 minutes each and sprint end/start meetings should come to only a couple of hours per sprint. Even if you're doing 1 week sprints, you shouldn't be spending more than three hours per week in process-related meetings. 

Developers have had to deal with too many meetings long before the current trend of agile software development processes. Meetings are insidious. People organize meetings because they ""seem like a good idea"", but unless the meeting has a clear purpose, it's probably a waste of time. Have you tried simply not going to some of the meetings? If you're worried about the optics of that, you could explain to your manager beforehand what you plan to do and why you plan to do it.

Agile methodologies require that everybody take an active role in shaping the process. It's no one person's job to decide what everybody else should be doing. As a team, you should all be deciding, together, how to work. It sounds like you don't feel like you have any power to fix it. Are you sure about that?

Many of the specific practices in Scrum are intended to be directly and primarily beneficial to the team. The point of the standups isn't so that you can report progress. It's not a status meeting. It's so that, as a team, you can decide how to tackle the day's work. The burndown chart isn't primarily meant to provide rollup to more senior managers - it's meant to ensure that everybody on the team has a shared view of the health of the sprint. I'd argue that the burndown shouldn't be broadcast to senior management. If they want to see it, they can walk down to the team's area to look at the one that's hanging on the wall. Heck, maybe they'll even take the time to talk to the team members to get a more holistic picture.

As a friend of mine points out, people love to say ""agile is crap; we should do X, Y, and Z instead"". But in actuality, the X, Y, and Z are the sorts of things that agile teams ought to be doing. ""We shouldn't have so many meetings."" Indeed, that's one of the points that Scrum was trying to address. ""We shouldn't have such a strong division of responsibilities."" Indeed, that's what Extreme Programming was trying to achieve with things like pair programming and ""travel light"". 

It sounds like your management has implemented a heavyweight process and called it agile. That seems like a misnomer to me. Have you considered that the problem isn't with the process, but with the managers?",1521338767.0
remy_porter,"In any domain, you are going to see fads, and they arise from one of the fundamental truths in life: nobody has a fucking clue what they're doing.

Nobody knows how to write software. Oh, you might have had a really clear idea in a specific case. You knew what to do, you wrote the software, and you're done. Nobody's going to write that exact piece of software again, so you're  off to the next bit… and you have no clue what you're doing. Anyone who is actively trying to advance their career is going to end up in situations where they have no clue. Even folks who just want to get a steady paycheck will end up in that position from time to time.

Nobody knows how to write software, but we all need to write software. Since we don't have a fucking clue, we turn and look at somebody who was successful. They succeeded, so they must know what they were doing, right? Well, no, of course not, but from the outside it certainly looks that way. If what they did worked, then that must be the way to write software. We'll do that. If enough people do that, we'll give it a name. Call it… XTREME PROGRAMMING (this was a real thing that coexisted with the early days of Agile before morphing into yet another Agile process). Once it has a name, it must be the right way to write software, so we'll do that.

And of course, it won't fucking work. It never fucking works. While it's breaking down, we'll take bits and pieces of our existing corporate culture and graft them back onto the name. Suddenly, Agile requires us to have a monthly ""Stakeholder Conference Luncheon"", because our organization has always had monthly Stakeholder Conference Luncheons, and since your project is using a new process and it's on fucking fire, you better believe you're having monthly SCLs. No! TWICE MONTHLY SCLs!

In the end, we're left with a few stark realities which we cannot escape: 

1) You are developing software wrong  
2) The process you're thinking of adopting is also wrong  
3) There is no good process  ",1521331379.0
beefsack,"Hating project management techniques is a universal constant of technical folk. Project management is a shit job and purely exists to balance expectations between stakeholders and the trenches.

Perhaps nostalgia is playing a factor here, but waterfall wasn't that different to what we have nowadays if you look at the sum of things. Because the feedback loop was longer, it was nicer to have longer stretches without interruption, but mistakes in design were equally amplified.",1521327769.0
PC__LOAD__LETTER,Honest question: do you believe that waterfall is the most effective development strategy for most software projects?,1521359154.0
VermillionAzure,"> and are comfortable seeing a dashboard of ""progress"" that they think represents reality?

Here's my thoughts on this:

If you think from a business and data science perspective, understanding how your team is doing on a very fine level and on an incremental basis is very valuable to them. That's why they're so metrics-focused.

They don't have your 10+ years of experience, so they cannot judge your productivity as if they were an expert in your profession. They are forced to gauge your success and ability by any type of data that they can get -- and I would guess that burndown charts fit this bill. What other choice, then, do they have that won't suck up a significant amount of their time besides starting fresh and putting in the years to learn programming because they graduated with a business degree from a great business school who maybe had 1-2 diversity CS classes?

**I think the problem is choosing good performance metrics.** Number of commits, lines of code, and even number of user stories completed are all soft metrics that are not intrinsically tied to the contribution of a single person to the success of a company. This is a very, very complex thing that is being oversimplified and viewpoints are being poisoned by imprecise and metrics whose results are convolved by other undesirable factors.

**Unfortunately, software engineering metrics are not in the place we want them to be yet.** To my knowledge, the fact we're still dealing with disgruntled responses due to this disharmony and loss of productivity means that there's definitely something wrong with *how* productivity and management tracking works. But can you blame them for trying to do so? They'd be perhaps too trusting as managers if they just took their engineers' words as granted -- imagine what engineers could do with that trust.

",1521332380.0
cupofchupachups,Horses for courses. I've seen waterfall work well if you know _exactly_ what you're building. I've observed agile really help a new product avoid going down a dead end by using a tight feedback loop and changing course quickly.,1521338696.0
,A lot of companies adopt agile or other methodologies when it really doesn't suit them. Agile doesn't work for all types of business models ,1521395039.0
,[deleted],1521405631.0
MandalaEDM,"I also agree that nostalgia could be a factor. I feel as if you have pointed out a good number of pitfalls associated with modern development techniques, but I do feel like modern techniques are best used to create modern software.

I work as a Dev Ops engineer in a cloud based industry and I find that modern, lean, micro architecture services once in operation feel very synergetic.

Waterfall works and has been used to make great products, but just in the same fashion that I wouldn't want to use a rigorous and structured language such as C++ for rapid prototyping I wouldn't want to use waterfall ideology in a new rapidly changing market segment.

Edit: Oh jeez, too much use of the word modern : )",1521330028.0
tevert,"Agile methodologies are extremely potent business tools that can make a company into a competitive powerhouse.... unless they fuck it up. Then it's destructive. 

It sounds like you're living in the latter world. I'm sorry you have to go through that. Other people such as myself have seen agile as more of a breath of fresh air though. ",1521335514.0
,[deleted],1521344759.0
_MrZeNoX,"1. Just program. Learning programming is 99.(9)% programing and 0.(1)% reading, watching etc.

2. Start doing small projects (shouldn't be longer than a week to do). No matter how simple they may be. From calculating powers and n-th number of the Fibonacci serie to ordering words in alphabetical order or making a calculator.

3. If you don't know something, google it. If you can't find it (and not that you're lazy and it just isn't the first link of your search) ask on stack exchange - it's a great community. Just be sure to keep your code clean and ask precise questions.

These are some tips that are on top of my mind now, if I think of anything else I'll just add it in. 

BTW I myself am still in the process of learning  and I'm no God of programming, so yeah. But... Hope I helped.",1521308423.0
Narbas,"Yes, strive to understand rather than remember. You can always Google specifics.

That being said nobody is going to give you a more precise answer if you don't put in the effort of writing out your question in more detail. On top of that if you'd checked the sidebar you would have seen there are subreddits better suited for asking this question on.",1521307958.0
ahbtg,Stack overflow,1521307958.0
BelucciMonica,programming is a craft,1521308203.0
be_cracked,No,1521286071.0
WArslett,"there is a difference between it being a recommendation and necessity. They are probably recommending Physics because Physics and Computer Science cover similar competencies: problem solving, understanding abstract concepts, working to an engineering process etc. Maths is often recommended in the same way.

Typically in your last 2 years of school in the UK you will be encouraged to pick four subjects that have some variation. I would argue that if you are doing Computer Science, Physics and Maths that you are focusing on a narrow range. I did Computing, Maths, Music and Literature. Ultimately it doesn't really matter that much, because it rarely has any bearing on what you can do at university anyway.",1521284370.0
Hello-Im-Random,"I don't think I've ever seen uni require physics for entry. Most of them just want maths at HL and that's it. Some want another scientific subject (CS/physics/chemistry, etc) but in this case you can choose. I got offers from unis without doing physics.",1521282103.0
Insane_Cheese,"You don’t need to do it, I know lots of people that didn’t. However physics can be quite useful for knowing good experimental practice and for the hardware side if the uni you go to has hardware.",1521287832.0
link23,"Yes. Induction is like recursion but backward, so it is very important for understanding and designing algorithms.

It's also an important proof technique, so I imagine it is a useful tool when proving the correctness of an algorithm (though I don't have experience in formal methods).",1521256273.0
pinano,"To answer the title: yes, understanding induction is necessary for designing and analyzing algorithms.

Don’t know anything about either of the books in the post body.",1521247839.0
TKAAZ,"Induction is an essential proof technique, and it is not that hard to get. I am not sure what you mean by ""mastering induction"", as the proofs highly depend on the use-case at hand. Just keep doing it and you will get it eventually...",1521314271.0
PM_ME_UR_OBSIDIAN,"Via the [Curry-Howard correspondence](https://en.wikipedia.org/wiki/Curry%E2%80%93Howard_correspondence), designing an algorithm is fundamentally equivalent to writing a (typically constructive) proof.

Go and ask /r/math if mastering induction is essential in becoming better at writing proofs.",1521306812.0
skulgnome,"Iterators, anyway.",1521316422.0
crabbone,"In different contexts, even if they are related to math / programming, induction may mean different things. As already pointed out by some, induction is a proof principle (one of the very cherished goals of some revisionists in mathematics is to derive induction as a theorem in their system, rather than to rely on it as a ""given"" method). In foundations of mathematics, it is a tool that allows proving claims about infinite things, using finite proofs, this is why it would be very useful to be able to derive it from some weaker system, like predicate logic or some extension of.

However, in logic, and in programming related to knowledge modeling / representation / etc. induction is also used to mean a reasoning process whereby you are able to draw probabilistic conclusions from true assumptions, although not universally quantified. For example: `all birds I've seen so far fly`, which allows you to probabilistically prove `the new bird flies`, but if the new bird happens to be a penguin... There are many more specific sub-divisions in inductive reasoning like prototype reasoning, statistical reasoning etc.  Are these useful for algorithms in general? - Not the way it is taught in academia. Academia relies on deduction in proofs almost entirely. But it is an interesting idea anyways :)",1521450424.0
gnullify,Python/Java seems like a good pairing if you want contrast.,1521211131.0
sailorcire,Get a new teacher if they are implying that HTML is a programming language.,1521216233.0
crabbone,"On this list, MySQL isn't really a language. The language MySQL database uses is called SQL, although MySQL made some modifications to it, the correct way to call it would be ""the MySQL favor of SQL"" or ""dialect"" etc.

With this in mind, your languages roughly fall into these groups: Java, C++, PHP, JavaScript, Python - they are all more or less Algol-like languages, you can define a source-to-source compiler for them, which will work for a large subset of all of them. Java and Python are somewhat unique in this group because they have notion of parallelism built into language, although Python gained this feature very late in its development, and it is a lot clumsier there than its counterpart in Java. Python is unique in that it has special construct for transferring control between functions before their execution terminates: `yield`. PHP is unique in that has special language constructs that make it possible to embed it inside other text files. C++ is unique in that it has two layers of templates that allow modification of its syntax. Python and JavaScript have destructuring as a language instrument. There's also some rudimentary destructuring in PHP. Python is now the only language in this group, which has all kinds of ""comprehensions"": list comprehension, dict comprehension etc.  Related: it has special syntax and treatment of generators. It also has richer array access syntax. JavaScript had many features that would've made it stand out more, but many of them were abandoned / only implemented by one of the vendors. For instance, it used to have E4X (in EcmaScript 4 standard draft). This would allow using literal XML and a special mini-language to work with XML embedded in JavaScript, but E4X never made it into XML standard family and was eventually discontinued by Mozilla, the only implementation I know of.

SQL, HTML and CSS are highly specialized languages that are not intended for performing computations in general. They are too different from the other group to have any non-superficial comparison. They also differ between themselves much more than the previous group. I think, you can call all of them ""data definition languages"" (this term used to be more popular at the time SQL was created, but fell out of general use some time after. At the time DDL were contrasted with ""data manipulation languages"", but since it only applies to SQL now, and it does both anyways, it sort of lost its meaning). Also, at the time, SQL was considered the ""fourth generation language"", but nobody uses this taxonomy anymore.",1521463235.0
feamcor,"You should try to compare among languages of different paradigms. For instance: English, Java, Haskell, Prolog, Forth.",1521876639.0
flekkzo,"If IKEA had named it it would be something like ""SORTERA SNABB"".",1521214003.0
JukkaSeriousTea,"Wow, this is really cool. I'm a sucker for interesting graphic design so these are up my alley. I imagine they would look good as posters as well...",1521211762.0
SoleSoulSeoul,"Sound a little better as ""kwick sårt"", no?",1521216504.0
diab0lus,"I'm guessing steps one through five are iterative, sorting progressively smaller subsets until step six is true?",1521249202.0
krawallopold,Upvoted because of Braunschweig (and because I like it),1521220505.0
yayapfool,This is actually amazing!,1521250294.0
pepitolander,"Pencil, die and ruler (it's a ruler right?) are 1x and the values to be sorted are nx. What does it mean and what is the point of showing it? I guessed that it indicates computational time but I don't understand what that ""ruler"" is for.",1521212020.0
Scum42,"Holy crap, that is awesome. Quick Sort was always one of the ones I learned about, but never actually implemented myself, so I wasn't familiar with how it works. I always assumed it was super complicated or something, but thanks to this, it seems really simple! This really is an excellent way to explain this algorithm to people who are unfamiliar... Or even non-programmers!",1521291646.0
lokomoko99764,"I'm currently in my first semester of second year and I'll answer what I can.

1. Yes, there is a fair bit of maths involved (I've taken one discrete maths unit so far, however the rest of the programming-specific units focus on algorithm design and more technical aspects of programming, at least in first year). I found game graphics design stuff to be heavier in that regard. I've never been the best guy when it comes to maths, however I received 95% for the entire discrete maths unit with a normal amount of effort. Keep in mind, I'm just at the point where the maths involved in programming itself is probably going to ramp up (by how much, I'm unsure)

2. So far, no. This may depend on your institution though. Writing code on paper is a challenging thing in itself, you'll have to practice that skill specifically if it's required

3. Well there are about 1 or 2 females in a cohort of like 50 people for reference. But apart from that, most people are pretty relaxed and don't mind chatting or making jokes about content, even with the lecturer/tutor at times",1521179253.0
nevabyte,"I am a comp sci grad from 2017 who is now working in the industry.

To answer your questions from my own personal experience;

1. It does involve a lot of math and logical reasoning. If you were wanting a head start try looking into discrete mathematics, automata and the predicate calculus. I found that the workload involving mathematics was a lot in year 1, I think to build an understanding of the mathematical theory of comp sci, a little in year two, and for me, there was no maths in year 3 as I chose modules which didn't have any in (there was for example a module in game theory which I could have chose).

2. This can vary. If you enjoy it then it will be somewhat easier than if you didn't, i.e. you will want to actively improve as opposed to doing it for the sake of passing an exam or project. Programming is a must, so maybe you could try and learn some high level languages and see how much you enjoy it and if you are able to pick it up easily. To add to this, the notion of picking up new skills fast will also apply to working in the industry as languages and technology are forever changing and advancing.

3. I found people are as sociable as you are willing to be to them. My lecturers were good and provided a good level of support, although this is University specific I suppose. Help is always there if you ask.

A bonus I found of doing a comp sci degree is that it involves very little writing, with the exception of your dissertation. So if you prefer not to write, that is essays and the like, then it is definitely a plus. 

Hope I helped in some way!",1521208081.0
Bobknows27,"If you dislike math, I would suggest avoiding CS. You'll definitely be required to take up to calc 3 and linear algebra and probably diff EQ and statistics. Even outside of the math classes, there's a lot of mathematical thinking in CS courses.

Difficulty will vary. Some people will breeze through, others will struggle. I'd say it's similar difficulty to a Math or Physics degree; there's definitely more struggle in enjoying it and staying motivated than hard concepts.

Environment is pretty similar to most STEM degrees, and will vary by school",1521186028.0
Loolzy,"2nd semester freshman here.


So far there is not that much math involved (other than the logic classes). People are somewhat sociable in my uni. Professors are pretty down to earth as well.


Difficult? Depends. It sure is busy. Beginner courses might be a breeze if you have prior experience, but the load will ramp up. Even if you're familiar with the language, assignments will still take you a while to complete. Be prepared to spend 10hrs per week on hw (maybe not the first semester).",1521186108.0
javaHoosier,"I went back to school after a 7 year break. I am at the end my second year. 

1. Depending on your school there can be a lot of math. My particular school I have had to take 4 math classes just as a part of my particular specialty in the major (of the specialties offered mine has the least required) . My algebra and trig were very rusty so it was a challenge. Calc 1, discrete structures, linear algebra, and stats. Several of my current courses use probability theory for the algorithms.

2. Yes, it can be difficult mainly because it is a lot of work. Learning syntax and how to write some code is the tip. Once you get passed that point then you have to understand how to solve problems with the tools you have. Sometimes solving the problem takes a lot of trial and error multiplied by severe frustration/self doubt. Then you have the stress of performing well in class.

3. Yes, it is social and the reason I went back to school. Despite what I said above. You are around like minded people who all want to learn. This is a great opportunity to make friends and get help from others when you don’t understand something.

This isn’t to scare you. It’s definitely doable and worth it. Just be aware it will require hard work and effort.",1521186686.0
HelloFromCali,"Graduating next semester...

1. Definitely will be math involved, I had to take Linear Algebra and up to Calc 2.
2. Difficulty honestly depends on your professors and your enthusiasm for the subject. If you practice on your own time then the course work will be a breeze. 
3. At my school people are very sociable, the CS club is very active and I have met a lot of friends in classes.",1521187300.0
ianwold,"Yes, there's a lot of math, but there's less math the ""lower"" your degree is. I got a BA instead of a BS and only needed Calc 1, 2, and Linear Algebra. Linear Algebra is actually a bit more easy for me. Discrete Math is also necessary, but I took that at the same time as Symbolic Logic (in Philosophy) and found it a breeze, but I also had a fantastic SymLog professor.

If you're unfamiliar with coding, it is a difficult degree. Even if you err on taking the easier classes, it can get a bit heavy. If you have been coding for a few years now, it should be a lot easier, especially as taking the introductory classes with double down on what you already know and get your mind into the academic swing if things.

As far as environment goes, some of my professors were really cool and some were super unsociable. Same went for the people. It's a mixed bag, but it definitely depends on your department and the size of your school. Bigger school equals more of both extremes.

Best of luck! Hope this is a bit helpful.",1521208460.0
jmite,"Yes lots of math, but a very different kind of math. Logic, graphs, and trees, instead of curves and calculus and such.

Yes, difficult, but very rewarding. 

Sociability depends on the group. People tend to be introverted, but not like the stereotype you see in movies.",1521223412.0
SargeantBubbles,"Third year CS student. I’ll answer what I can -

1. Math - yes, lots of it. That being said, a lot of math used for CS is stuff that doesn’t really seem “mathy”.  Lots of modular arithmetic and formal logic, which are kind of theoretical, rather than the typical “here’s the equation, solve for x”. In addition, for degree requirements, you’ll likely have to take up to Calc 3, and maybe ODE’s/stats. 

2. It is difficult; however, it is perhaps the most rewarding thing I’ve done in my life. If you’re the kind of person who enjoys building IKEA furniture, playing chess, stuff like that where you have to be creative, you’ll enjoy computer science. It’s all about learning a different way of thinking, and learning how to be clever. If you’re a quick learner, and willing to put in extra hours outside the classroom, you’ll be able to keep up well.

3. The people vary in my experience. A lot of people are really weird, as you may expect of CS students, and not very social. There are also arrogant kids, who think that solving a problem makes them Alan Turing (don’t be that kid). That being said, there are equally as many, if not more, great professors and super friendly people. Many professors will not care about your progress; find the professors who care about you, and latch onto them. They will be crucial in your upcoming years as a programmer for help and opportunities. 
In my experience, the friends I’ve made studying CS are some of my favorite and most genuine friends - it feels like some secret society that you’re a member of, saying stupid memes to each other about bugs that only a nerd would understand. 

All in all, do it. Take a 100 level class and see if you’re into it. If you are, stuck with it. It’s challenging in the best way, and you’ll thank yourself 20 years from now. ",1521239884.0
Arancaytar,"Can you post this in a readable format? Line breaks and indentation are only conserved if you indent every line by at least four spaces:

0

  2

    4
      6
        8",1521195761.0
Arancaytar,"Assuming that (5) is not part of the loop in (4), this relation isn't quite correct: 

You make exactly two recursive calls throughout the function, while your recurrence function would imply you make first 1 and then n/2 of them. I think that * should be a +.",1521235593.0
__horned_owl__,"Slightly off topic, sorry about that but:
does anyone know how to get the bounds of your original recurrence relation? (It's with non-constant coefficients and thus it presents interest in its solution alone) :) ",1521240526.0
,[deleted],1521245115.0
kernelhacker,Weren't they interested in concurrency? The solution proposed assumes that only one thread is working with the tree at a time.,1521209698.0
SummarizeDev,"Hi! We've released our [**natural language processing API**](https://www.summarizebot.com/summarization_business.html) and would be happy to hear your feedback!

Our REST API is a package of artificial intelligence and blockchain-powered solutions for analyzing and extracting various kinds of information from unstructured text data, videos and images.",1521289384.0
jonhy99,"I've been in the 2015 edition coming from Portugal.
It was the first edition on our country.
As far as I understood, the criteria for the candidates vary from country to country. Our criteria was something like:
- 4 students from 3 top universities chosen by Huawei Portugal.
- All from an telecommunications engineering background. In my case, I was finishing my bachelor. 
- 60% for your grades, 10% personal interview, 30% video presenting your self and why you should be on the program.

Despite this telecom criteria, we saw a LOT of foreign students with zero telecom brackground. Thus, I think it really depends on your country selection criteria.

If you have further questions, your welcome to ask :)",1521199264.0
80nd0,I'm not familiar with the program what is it? Also what country are you from? I'm curious how this program is implemented ,1521164531.0
zsaleeba,I know someone who did it. They got a sweet trip to China out of it.,1521175313.0
beatle42,Have you talked to your manager about updating your title to reflect your duties?  That's probably your best path if you want to change your title.,1521119033.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1521117324.0
gambs,"[Just read this, it's free and a good start if you don't know anything about AI](http://www.deeplearningbook.org/)",1521099850.0
tom808,"I have to agree with /u/ashdnazg.

I've tried to read several Packt books on various topics before. They were too low quality for me. Sections which seemed rushed or out of place coupled with mistakes made them too much of a challenge.",1521101360.0
tmfom,"No, because Packt. I've had too many problems with Packt titles with bad copyediting, wrong or weird code, and poor organization to recommend them.",1521117497.0
ashdnazg,"I haven't read any of the books there, but I know that packt (the publisher) is a bit notorious for low quality books. Supposedly some of their books are good, but buying one without getting clear recommendations is a bit of a gamble.",1521096853.0
IAmALinux,"For a $15 donation to the Electronic Freedom Foundation, why not try it?",1521144314.0
cyber-clown,"http://neuralnetworksanddeeplearning.com is another very good (and free) online book, that I'd recommend over the ones in the bundle.",1521104765.0
witekbb,"The ratings for these books seem to be ok but there are a lot of free resources online. :)

All data from Goodreads:

    ├── Pay $1 (about €0.81) or more!
    │  ├── Practical Game AI Programming by Micael Dagraca,
    │  │   Average rating of 0.00 from 0 reviews.
    │  ├── Statistics for Machine Learning by Pratap Dangeti,
    │  │   Average rating of 0.00 from 0 reviews.
    │  ├── Machine Learning for Developers by Rodolfo Bonnin,
    │  │   Average rating of 0.00 from 0 reviews.
    │  ├── Machine Learning Using C# Succinctly by James McCaffrey,
    │  │   Average rating of 3.50 from 4 reviews.
    │  ├── Implementing AI to Play Games was not found (will not be counted in total score).
    │  └── Three Months of Mapt Pro for $30 Coupon was not found (will not be counted in total score).
    ├── Pay $8 (about €6.50) or more to also unlock!
    │  ├── Deep Learning for Computer Vision by Rajalingappaa, shanmugamani,
    │  │   Average rating of 5.00 from 1 reviews.
    │  ├── Unreal Engine 4 AI Programming Essentials by Peter L Newton, Jie Feng,
    │  │   Average rating of 0.00 from 0 reviews.
    │  ├── Keras Deep Learning Projects was not found (will not be counted in total score).
    │  ├── Neural Network Programming with Java by Alan Souza, Fabio Soares,
    │  │   Average rating of 2.80 from 4 reviews.
    │  ├── Machine Learning: Algorithms and Applications by Mohssen Mohammed, Muhammad Badruddin Khan, Ejhab Bashier Mohammed Bashier,
    │  │   Average rating of 0.00 from 0 reviews.
    │  ├── Machine Learning for Opencv by Michael Beyeler,
    │  │   Average rating of 4.00 from 2 reviews.
    │  ├── Tensorflow Deep Learning Solutions for Images was not found (will not be counted in total score).
    │  └── Machine Learning with Go by Daniel Whitenack,
    │      Average rating of 0.00 from 0 reviews.
    └── Pay $15 (about €12.19) or more to also unlock!
       ├── Python Artificial Intelligence Projects for Beginners was not found (will not be counted in total score).
       ├── Building Machine Learning Systems with Python by Willi Richert, Luis Pedro Coelho,
       │   Average rating of 3.86 from 113 reviews.
       ├── Mastering Java Machine Learning by Dr Uday Kamath,
       │   Average rating of 4.00 from 2 reviews.
       ├── Artificial Intelligence with Python by Prateek Joshi,
       │   Average rating of 4.14 from 5 reviews.
       ├── Applied Artificial Intelligence: Neural networks and deep learning with Python and TensorFlow by Wolfgang Beer,
       │   Average rating of 3.21 from 14 reviews.
       ├── Deep learning with Python by Francois Chollet,
       │   Average rating of 4.68 from 59 reviews.
       ├── Deep Learning with Keras: Introduction to Deep Learning with Keras (2nd Edition) by Anthony Williams,
       │   Average rating of 1.75 from 4 reviews.
       ├── Hands-On Deep Learning with Tensorflow by Dan Van Boxel,
       │   Average rating of 3.00 from 1 reviews.
       ├── Machine Learning with Tensorflow 1.X by Quan Hua,
       │   Average rating of 0.00 from 0 reviews.
       ├── Machine Learning with R by Brett Lantz,
       │   Average rating of 4.21 from 80 reviews.
       ├── Mastering Machine Learning with Spark 2.x by Alex Tellez, Max Pumperla, Michal Malohlava,
       │   Average rating of 0.00 from 0 reviews.
       └── Deep Learning with R by Francois Chollet, J.J. Allaire,
           Average rating of 4.00 from 1 reviews.",1521107689.0
DeepBurner,"Don't think so, I find humble book bundles low quality usually, and as someone with a little bit of experience on the subject I can attest that there are better books",1521104742.0
hinomarucurrydisc,"If you're willing to be patient and lurk their website everyday, Packt also gives out different free ebooks everyday. I've seen some of the books in this bundle pop up on that list.

I grabbed a couple books for free and haven't gone through all of them yet, so that's been working for me. The downside is that you don't know what books will be offered for free and you necessarily won't get all of the books that are included in the bundle.",1521128383.0
JackChase24,"I know everyone is saying there are free resources out there and while I totally agree, if your okay with reading, $12 is a low price to pay for the extra references for the future. ",1521162441.0
16bithustler,"If you're interested in the Deep Learning trend, Coursera offers a [specialization](https://www.coursera.org/specializations/deep-learning) where you can opt to pay 50USD/month or audit the courses for free. The content is quite good.",1521503703.0
barburger,I would like to recommend Bishop: Pattern recognition and machine learning. This is the book a lot of universities use in their machine learning master's courses. ,1521136759.0
nightcracker,"An entirely different advice: books suck. They promote passive learning where you're nodding along. You'll also avoid common pitfalls, which is exactly what you don't want while learning: you want to know where the pits are, and no better way to do that than falling in when the training wheels are still on.

Find some AI/machine learning project you want to complete and do it. You'll hit roadblocks and figure them out yourself with your good friend google. There's an infinitude of resources freely available on the internet. Your first steps are probably slower this way, but you actually learn this way.",1521107256.0
ActualPoetry,You’re outputting the memory location of int * a each time the program executes. Memory is allocated for your variable from a different location each time the program is executed.,1521096935.0
CorrSurfer,"Note that code 1 has *undefined behavior* -- you are writing to the memory location where pointer ""a"" points the value of 100. Since pointer ""a"" has not been initialized, anything could happen.

The most likely scenario (if you are running the program on a moderately modern environment) is a segmentation fault, however.

Note that your program also does not compile as you spelled ""Void"" in a capitalized way. Also, C++ needs function prototypes and you did not include the ""#include"" lines.
",1521103160.0
IJzerbaard,"It's fine for an intro IMO. I wouldn't have used the wording ""optimal encoding for each character"" exactly, that's slightly more towards the side of oversimplification, after all Huffman codes only *approximate* the 0th order entropy limit (and sometimes accidentally achieve it, but in general that doesn't happen).

Since it's only an intro I won't complain too hard that stuff is ""missing"", but I think it's a pity that most people only know bit-by-bit decoding. That real-life applications of Huffman coding use length-limited codes is no coincidence, that simplifies table-based decoding.

BTW you write ""less bits"" but I'm pretty sure that should be ""fewer bits"", I'm not a grammar expert though.",1521062238.0
Trav_Cav,"It's just a basic intro, so I tried to keep it simple. Friendly feedback is appreciated so we can all learn together.",1521052007.0
costofanarchy,I think a good analogy for Huffman Coding is the game [20 questions](https://en.wikipedia.org/wiki/Twenty_Questions).,1521080973.0
TKAAZ,"Wrong subreddit, mate...",1521033392.0
TheoryPod,"Quantum computation could hold the key to being able to simulate matter exactly. This week we have an extended interview with Professor Alán Aspuru-Guzik, from Harvard University, to find out what a quantum computer is, how it works and how it can be used in chemistry.

The paper he talks about can be found [here!](https://pubs.acs.org/doi/abs/10.1021/acscentsci.7b00550)",1521026886.0
quietandproud,How much do you need to know to be able to enjoy this podcast?,1521035354.0
Head-in-the-Cloudz,Noob question: what’s to be gained from simulating chemistry/ matter?,1521037195.0
palsword,Your logo looks like it's giving the finger,1521099865.0
AlphaPrime90,"Guys this is a great series, informative and enlightening.  
Thanks for sharing",1521036805.0
JayPeaEm,"Nicely done. What other topics do you plan on covering? 

I have a few friends on Fellowship from Harvard doing protein synthesis and genetic engineering, a buddy working at NASA, I'm a Civil Engineer utilizing augmented reality on construction sites;  we may all not be experts but we're certainly knowledgable. If you need contacts let me know! We need more stuff like this",1521037554.0
_ntnn,"Thanks, do you have an RSS feed one can subscribe to?",1521058024.0
wviana,Does someone else recommend this course ? ,1521023023.0
eleitl,"Is the model open source? I don't see the model source nor trained results nor raw dataset cited in https://arxiv.org/pdf/1711.06402.pdf nor published on github.io

",1521039974.0
Deltron303o,Am I missing something or is this about computers deciding if ppl should live or die? ,1520991222.0
UnderwaterPenguin,The site looks great! Nice work OP,1521006448.0
_foobie,"Pretty good way to get free software development labor and directly profit from it, you scammy fucks. Eat shit.",1520961072.0
Matholomey,I have one. Not gonna share it :),1520984826.0
CorrSurfer,"The classical approach to do this requires that you distribute workload many times and only pay-out server-owners that solved many problems for you.

When doing so, you from time to time solve the problems that you give to a server yourself, and compare the result. If the result ever deviates, you know that the server is cheating.

If the majority of your NP-hard problems has no solution, cheating by the server becomes almost impossible to detect. To mitigate this, you inject, from time to time, problems into the stream of problems given to the server(s) that *have* solutions, and you verify that the server returns a correct solution.

You choose parameters to this process in a way such that you detect cheaters with a sufficiently high probability while not having to pay the servers too much for the added challenge problems.",1520936403.0
fobobar,"This sounds like [verifiable computing -problem](https://en.wikipedia.org/wiki/Verifiable_computing). As far as I have understood, there are ways how this can be achieved, though they are not yet that practical. For example, some approaches utilize fully homomorphic encryption, which alone is a tough problem in practice.",1520936382.0
celerym,"This is an interesting problem. One solution may be to make each program statement available to multiple hosts, but not the complete program. A kind of blind parallelisation and serialisation with some statements being repeatedly processed by different hosts, forming a network or graph. If any hosts forges a section of the result the network will diverge very quickly somewhere.

For fun, given enough hosts you could add noise or a stochastic component to the network communication and verify whether the results match the expected distribution.",1520935677.0
Jaxan0,"Another approach is interactive proofs (IP): https://en.wikipedia.org/wiki/IP_(complexity). In this setting, you would be the verifier, and the others provers. Then the provers try to compute something, and you can verify this by sending them (randomised) tests. This way, you can know they computed something, without them sending a full proof to you.",1520945774.0
hextree,"> I could send the script to solve the original NP complete program to a bunch of server-owners, and the first person to solve it gets paid.

What is your definition of 'solve' here? An NP-complete would be a decision problem, with a Yes or No answer. If the input lies in the language, then the answer is 'Yes' and they would be able to prove it to you with a polynomial-size proof. But, if the answer is No, they may not be able to provide any polynomial size proof that the answer is No, and you would may never get an answer to your problem.",1520969939.0
mdmacidmthcoke,"I may not understand the question but what is preventing you from just having the program send you an email that says ""ran"" to something similar?",1520926179.0
paul_miner," Not sure if this is applicable to your situation, but bitcoin mining pools deal with a similar problem. All the pool members should be looking for a nonce that results in an appropriately small hash value that any one computer is extremely unlikely to find, so they need to a way of knowing that an individual miner has actually expended effort searching. The pool does this by accepting nonces that result in small but not-small-enough hash values as evidence of computational effort, basically suboptimal solutions (in that the resulting hash value was not below the current bitcoin target, but still requires the same process of trying nonces and evaluating them).",1520969579.0
sturdyplum,Some special cases that might be interesting to bring up to your interviewer is that if the graph is a tree then it is always 2-colorable and if it is planar then it is always 4 colorable. ,1520980003.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1520878017.0
mynewpeppep69,"I think part of this confusion is how unrigorous the presentation of asymptotic analysis is in most contexts. Things like the fact that saying ""f(x) = O(g(x))"" actually means ""f(x) \in O(g(x))"" and that ""O(g(x))"" is actually the set with whatever your favorite definition of the asymptotic bound is. Compound this with the fact that presentations I've seen as you've mentioned will actually use these terms interchangeably, rather than explicitly saying things like ""the best case run time of f(x) is bounded below by Omega(g(x)) and bounded above by O(g(x)) so it is therefore bounded by Theta(g(x))."" 

Basically, I think you're not crazy and actually hinting at a big problem in computer science education, which is really hand wavy and inadequate justification for a lot of the mathematical tools used. My experience personally has been that when teachers are hand wavy like this, it really detracts from the understanding of the students. I don't think problems with rigour scaring students ever really has to do with the rigour itself but rather the presentation skills of the teacher. I've seen teachers teach very rigorous concepts as first time presentations for students without a strong math background in a way that facilitated understanding. It's just about understanding the vocabulary to use. ",1520881639.0
DonaldPShimoda,"You’re not going crazy, but I’d say you’re thinking about it wrong — or, at least, saying “they are properties for mathematical functions” doesn’t really provide a better way of thinking for someone who is confused.

Big-O **as used to analyze the worst-case runtime** answers the question “In the worst case, what is the longest our algorithm can take to run?” The full statement is necessary; you *cannot* rephrase this as “What is the longest our algorithm can take to run?” The “worst case” part is absolutely vital. (This is because we don’t actually apply Big-O to the raw algorithm, but rather to a version of the algorithm which produces the worst runtime.)

Big-W (pretend that’s Omega) **as used to analyze the best-case runtime** answers the question “In the best case, what is the fastest our algorithm can run?” You can see that this is different from the previous question in two ways: (1) best case vs worst case, and (2) lower bound on runtime vs upper bound.

But we can apply the analyses in other ways. **We can use Big-O for best-case analysis**, which answers the question “In the best case, what is the longest our algorithm can take to run?” Similarly, a Big-W analysis of worst-case runtime answers “In the worst case, what is the fastest our algorithm can run?”

From these rephrasings, we can see that **Big-O is used to analyze an upper bound on runtime and Big-W is for a lower bound**. Of course, we knew that already — but your friend didn’t. I think phrasing the questions this way is much more intuitive for beginners.

Edit: added emphasis for clarity.",1520878965.0
hextree,"You are correct, big Oh is nothing more that a symbol defining a class of real-valued functions which obey a certain mathematical property. Similarly for the other symbols. Essentially the 'less than or equals signs' of the compsci world.

Whilst it is true you *can* use Big Oh for worst case and Big Omega for best-case, nothing is stopping you from also saying ""The best case is O(1)"" or ""The worst case is Omega(n^2 )"" which are perfectly valid statements you could see in a theoretical CS research paper. In fact any statement ""The X case is Y(f(n))"", for any values of X, Y or f

As someone who used to do research in the field of complexity and algorithm design (and therefore had to be super pedantic about correct use of the notation at all times), I've certainly seen the confusion you are highlighting, and I can see why it arises; people have a tendency to want to describe how 'bad' the best case is, and how 'good' the worst case is, and forget the original point of the symbols.",1520905518.0
fQM8US4A,"You're completely right. Sometimes people at Harvard fuck up, just like everyone else.",1520881660.0
ismtrn,">I'm not the one going crazy, am I?

No, you have it completely right. You are also right in observing that this particular misunderstanding is a common one.

I agree with /u/mynewpeppep69 that it probably has to do with an idea which some people have that the less math you introduce the easier things become, even though in this case it just confuses everybody when you handwave the detail away and you get these misunderstandings.",1520892235.0
Edg-R,This is what I was taught in school as well.,1520895446.0
Arancaytar,"I learned the same thing at first, and only learned the mathematical definition (let alone the idea that asymptotic growth had a formal definition beyond O(n ^ count the nested loops)) much later.

Note that the best/worst case thing doesn't lead you *too* far astray from the actual lower/upper-bound definition. It's a bit oversimplified, but a worst/best case run time *should* yield such a bound if written correctly, and vice versa.

The thing here that is completely, terribly wrong is that a combined upper and lower bound means ""average case"". That doesn't even make sense - lots of functions have no such bound, such as f(n) = (1+(-1)^n ) * 2^n + n^2, which has a tight lower bound Omega(n^2 ) and a tight upper bound O(2^n ). ""Average"" isn't even a thing in asymptotic growth. If your algorithm has a degenerate case that takes exponential time, then it doesn't matter whether that's one in a thousand or one in a billion cases.

(Also annoying: The widespread abuse of the equals sign here. They're classes of functions; a function is a member of them rather than equal to them. If they were correctly introduced with set notation, they'd probably be a lot more intuitive.)",1520926487.0
not-just-yeti,"It sounds like one confusion is, what is meant by 'best-case'?  I'd rather say ""the best case *out of all inputs of size n*"", to make it clear.

Then it's easier to say ""out of insert-sort running on all lists of size n: I can prove to you that the best case is >= n [within a constant factor], and I can also prove the best case is <= n^2, but I think that part can be tightened.  Hence best-case is Ω(n), and O(n^2).""

It also makes average-case clearer: How can you average over *all* lists of length *n* -- there are so many of those!  So you have to think about ""all *permutations* are equally likely"".
",1520938552.0
tending,"I think it's incorrect to call it incorrect. Many algorithms books do the same, and there is no rule forcing consistency between disciplines. Computer scientists and number theorists don't even agree on the definition of natural numbers!",1520919599.0
StandardMilk,"You're definitely right. As I explained to people in my algorithms class, merge sort's best case time complexity is not properly defined as Ω(nlgn) and similarly merge sort's worst case time complexity is not properly defined as O(nlgn). All worst, average, and best cases are Θ(nlgn). A cool way of thinking about best case time analysis is saying that the in the best situation possible, the algorithm will, at worst, take O(x) time, for some unknown function x(t). Similarly, a way of thinking about worst case analysis is saying that in the worst situation possible, the algorithm, at worst, take O(x) time.",1520895218.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1520873491.0
not-just-yeti,"A couple of considerations, not often stressed in textbooks talking about grammars:

* Even if CFG doesn't capture all the legal strings we want, we still prefer them.
Consider any reasonable programming language: its grammar is almost always presented as CFG, but there are a lot of strings that accepts which still aren't syntactically legal:

  - For example, requiring a variable to be declared before you can use it.

  - a type system could be added at the grammar level, but it's MUCH easier to let the type-checker be a later processing-step.

Yes you could create a context-sensitive grammars which capture the above, but it'd be a nightmare in real-world code.

* Even with CFGs, we tend to divide them into two parts -- the derivations for individual tokens, and then the more-interesting components.  Then in compilers class, we consider lexing/tokenizing as a pre-processing step for parsing.  Conceptually it's one big-ass CFG, but in practice separating the two is extremely helpful.

* Finally, one advantage of syntactically limited languages is that they also come with guarantees/proofs of ""better behavior"".  (I think you understand this based on your question's phrasing, but it's something I had wondered about for a while --
 Why would we even care about using languages which we *know* have limitations?) E.g. regular languages can be parsed particularly easily, and regular-expressions(\*) have algorithmic bounds on matching-algorithms because the input is known to be 'simple'.

(\*) in particular, regular-expressions without perl's back-references.  Back references let you compute things like ""strings-of-Xs whose length is non-prime"" (!):   `^(XX+)(\1)+$`

(Related theory: [descriptive complexity](https://en.wikipedia.org/wiki/Descriptive_complexity_theory): knowing that a problem can be stated in a syntactically-restricted language can imply bounds on its computational complexity.  The [seminal result](https://en.wikipedia.org/wiki/Fagin%27s_theorem) is that a problem is in NP if, and only if, you can express it in first-order logic formula preceded by a single second-order ""∃"" block.)",1520873730.0
arnet95,"Seeing as how more languages are context-sensitive than context-free, in some cases you do not have a choice.",1520863622.0
WhackAMoleE,"First, answer me an easier question. How do you know your next door neighbor is sentient? When you solve that one, you're ready for the harder questions.",1520873343.0
CorrSurfer,"The topic has very little to do with CS and it mostly philosophical. Computers are devices that produce output from input according to predefined programs. Whether there is something like a ""free will"" depends on the definition of this concept, which is not a CS topic.

Having said that, perhaps you may want to look at ontologies for modeling emotions. Getting to know what ontologies are and how this relates to the field of artificial intelligence may help you with your essay, as ontologies highlight what AI actually does and how reasoning about emotions fits into the big picture.

I have no clue what you mean by ""5) Society of Mind""

",1520864126.0
DrKevinBuffardi,"You might already be onto it (since you've mentioned *Society of Mind*), but I recommend incorporating Marvin Minsky's discussion of this very topic in his magazine article [Why People Think Computers Can't](http://web.media.mit.edu/~minsky/papers/ComputersCantThink.txt)",1520897270.0
sparcxs,"Here’s my take on it; If we do create a computer with consciousness, it will likely be a quantum computer where superposition and entanglement  is relied on to mimic the chaos of organic thought processes. We don’t know what the root of consciousness is, so this is a pure shot in the dark, but it’s more plausible than a binary implementation of consciousness. I say that because if consciousness could be reduced to simple logic, we’d probably have figured it out by now.",1520881383.0
welshfargo,Roger Penrose [doesn't think so.](https://www.amazon.com/Emperors-New-Mind-Concerning-Computers/dp/0198784929/ref=sr_1_1?s=books&ie=UTF8&qid=1520950954&sr=1-1),1520951021.0
Kris--,"To answer your question regarding more technical subject matter for your essay, you might consider discussing neural networks. They have the perception among laypeople as being akin to human brains. This couldn't be further from the truth, they are a simplified model of individual neurons connected together, relying on statistical methods to derive insight and are only capable of performing very specific tasks - unlike the human brains ability of ""general intelligence"". This type of discussion could focus on the question when will computers be able to think or be conscious, if even some of most 'advanced' techniques in CS are merely veneers of intelligence.

You could expand this, or separately, write about the amount of computing power required to create thinking machines. Running a deep neural network for object recognition takes a reasonable amount of computing power. Human's do it at a glance. No doubt there's estimates based on computing power when the first AI machines might be created.

That's all I can think of at the moment.",1521703265.0
Misterbreadcrum,"I've been at a place that does commit messages like this for almost 2 years. I can't tell you how important it is to follow this guideline. You won't look back at 90% of your commits, but the 10% you do look at need to be descriptive. ",1520848505.0
dikiaap,"Too much redirect. Here's the main article: http://www.topaz.io/git-commit-message/

Edit: I also want to ask. How to add a line(<br>) in a commit like [this](https://github.com/torvalds/linux/commit/ac68b1b3b9c73e652dc7ce0585672e23c5a2dca4)?

    Link: http://lkml.kernel.org/r/20180224030046.24238-1-mcgrof@kernel.org
    Fixes: d9c6a72 (""kmod: add test driver to stress test the module loader"")
    Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: Luis R. Rodriguez <mcgrof@kernel.org>
    Acked-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>",1520857998.0
jonathancast,"Grr.  If you always want to complete the sentence ""If applied, this commit will"", then you want to use the *infinitive*, not the imperative!  Will is a modal; it takes an infinitive.  An imperative is technically finite.

(Pedantry, but if you're going to use latinate terms, you might as well get them right.)",1520859539.0
,"I was just thinking about this, thanks for sharing",1520847805.0
CSMastermind,Putting a link to a trello ticket in your commit message seems like an anti-patterb to me.,1520860040.0
Madamelic,"Even better, try to follow [Angular Git commit format](https://gist.github.com/stephenparish/9941e89d80e2bc58a153). 

It will make scanning your commits a lot easier and it is more descriptive.

Also a number of tools can build on top of and parse these git commits into Changelogs.",1520862349.0
umop_aplsdn,"sorry to be the ~~grammar~~ spelling nazi, but ~~imperitive~~ imperative*",1520850736.0
mayumer,"Read the sidebar, retard.",1520799058.0
BelucciMonica,/r/mysql ,1520784258.0
browner87,"I hate to be that guy, but Google has millions of results for this, and stack overflow. ",1520785899.0
rickityrixkityrick,"Depends what you're trying to do, you could use a frame work like Django that has an authentication package and handle it that way",1520784262.0
lrem,You're asking questions for which the first three years of a traditional computer science course on a Polish technical university give an overview of an answer...,1520778216.0
anaerobic_lifeform,"Your PC is made of electronics: a motherboard, memory, one or more processors, etc. Basically the processor executes codes and can interact with  devices (peripherals) according to established protocols, designed by smart people.

At the core of it, the processor is able to accept data that, when feed into the machine, make it behave in ways that are meaningful for a human observer. It looks like it shuffles data around but if you look carefully, you may notice that it performed an addition, for example. Take a look at a physical Turing machine: https://www.youtube.com/watch?v=vo8izCKHiF0.

A processor does not think, it only perform actions: transfer data from memory to register, add register with constant, put result back in memory, put a special value in a register, ... 
Typically you represent high-level concepts by encoding thems: numbers? encode them in base 2. Images? let's have a grid of color points, each color being encoded by three values (RGB). Characters? map character to numbers. A monster in a game? just a bunch of  properties grouped together (health, position, speed, images). High level languages help you define additional building blocks (types, classes) and manipulate values in an abstract ways (variables, instead of actual memory addresses, functions, etc). 

Using assembly and/or high-level languages, you can write more and more elaborate programs, like operating systems, which have special data to represent processes and can schedule and interleave the execution of different programs with the same machine. Nowadays you also have multiple processors that can execute code concurrently.
",1520780434.0
sturdyplum,"There is an O(n) solution to this problem that uses an eer tree (palindrome tree). There is another that uses manachers. Both of these data structures are quite cool.
",1520744286.0
scrumbly,There's an easier n^2 solution. Iterate over all possible centers of the substring (either a single character or the spot between two adjacent characters). Then just walk out in both directions for as long as the newly discovered characters match.,1520784301.0
No-More-Stars,"Add colour to the t/f matrix, center the text and capitalise/convert to Unicode. It's unnecessarily hard to read presently",1520761074.0
nothingtoseeherelol,"Wouldn't their ""brute Force"" approach be O(n^2), not O(n^3)?",1520785615.0
captcomp,"incorrect solution, you need to change the line substring = s[i:j] to substring = s[i:(j+1)] the solution given doesnt take into account the last character of the string. when run as is, aabcdcb returns cdc, but after the modification the correct output is given. ",1520798412.0
BenjiSponge,"If you want to be effective, ditch the weird labels and just lean what you need to know.

""Computer Science"" has plenty of practical uses. There are elements of ""Computer Programming"" that are completely impractical.

What do you want to do? Yours is honestly something of a bizarre question.

e.g. Do you want to get a job? Doing what? Let's say you wanted to work for a finance firm. Learn the fundamentals of C++ and some algorithms and data structures. Let's say you wanted to build a robot that greets you by name when it sees you. Get a degree in computer science with a concentration on machine learning and computer vision. Want to make a website? Learn HTML and css.",1520715316.0
jits_and_pieces,"I haven’t heard of required science and biology classes, unless you’re on some sort of computer engineering path. Some universities require a few levels of calculus, but in general, the relevant math classes include discrete math, linear algebra, and statistics. Computer Science as a whole is kind of, “How can I apply software tools (such as programming languages) to solve problems and create good software products?” 

There is very much a focus on programming, but there is some “fluff”. Still, if you only know how to code and don’t understand algorithms, development methodologies, source control, software maintenance, design patterns, etc, you’re very much missing out on the best stuff. I wouldn’t recommend skipping the “fluff” unless programming is only tangential to your career path.",1520716087.0
sayubuntu,"What’s your goal here? You can totally find positions where you could be as qualified/competitive as a CS grad without doing all of the difficult math/analysis, but honestly over the course of your life the technical skills your calling “fluff” might be easier to master early on then keeping up without it.",1520727902.0
hamtaroismyhomie,"Stanford lagunita has a free, online, Compilers course.
You won't be at a disadvantage without kn",1520723792.0
PinochetIsMyHero,"> compilers which is typically standard

Really?  It was a 500-level graduate course at my university.

Anyway, if you want to develop down on the metal, you should be fiddling around with low-level stuff -- assembler, microcode, C at the highest.  The techniques when you're hand-coding assembler are a bit different from instantiating class objects and the like.",1520715297.0
ryani,"If you like functional programming and want to do self-study, you might like Simon Peyton-Jones compiler book, ""Implementing Functional Languages: A Tutorial"".  It guides you through the creation of a simple compiler for a lazy functional language similar to Haskell, in Haskell.  I went through it on the weekends over the course of a summer and had great fun building a little toy compiler while learning Haskell at the same time.

https://www.microsoft.com/en-us/research/wp-content/uploads/1992/01/student.pdf

(The book references the language ""Miranda"" but the language it uses is almost identical to Haskell.)

It doesn't go 'all the way down' to the hardware (most people use LLVM these days anyways!), but it gets you 'close enough' that you can see how you might get there if you wanted to.  If you want to finish the backend, there's a pretty good tutorial for combining Haskell and LLVM here: http://www.stephendiehl.com/llvm/
",1520739564.0
TedW,"I'm taking OSU's compilers course on-campus right now.  The teacher (Rob Hess) adds a lot to the class, but we're doing things you could learn on your own.  If you are motivated to do so.  We are basically writing several versions of a compiler that translates a subset of python to c/c++.

Actually, [here's a link to the coursework](http://web.engr.oregonstate.edu/~hessro/teaching/cs480-w18).  It looks like you should be able to access (almost) everything without logging in.

I have an assignment due on monday so try to get caught up to speed quickly so you can help me with it kthx.",1520733515.0
supersaiyanyolo,"Yea my program doesn’t offer compilers either as far as I know but does offers a course on programming languages which covers relevant topics like grammars, lexing, parsing",1520730919.0
chrisgseaton,"It's a library of computer science research.

I don't think it includes access to O'Reilly books does it? Where did you see that.",1520710718.0
cclites,"If you are a Comp Sci student, you may already have access.

https://www.acm.org/membership/membership-benefits",1520710287.0
jmite,"It's primarily research papers, if you're looking for books then it's probably not your best bet.",1520790672.0
njetwerk,"It's the repository for all ACM publications, e.g., all the research papers from ACM conferences and journals are published there. It's mostly used by researchers and practitioners that care about (and are capable of understanding) the cutting edge research in CS.",1521112706.0
Kris--,"It's essentially a database of Computer Science journals, conference proceedings and the like for academic research.

It wouldn't advise membership for learning materials, unless they are specifically advertising materials on a subject(s) that you're interested in. Membership is primarily formed of students and academics.

Membership of Packt publishing's platform, O'Reilly, Treehouse, Lynda.com or a similar service would serve you much better.",1521700186.0
Captain_Flashheart,"Don't hit refresh more than twice, though. That place bans you automatically if it suspects any kind of scraping. ",1520728727.0
eholk,"I've found knowing things that like rings, groups, and fields exist help me think about generic programming. It was long enough ago that I've forgotten most of the specific terminology, but taking about operations in the abstract, like this is an operation on members of this set that has an identity and is commutative, is basically what you need for generic programming. For example, you can write sorting algorithms in terms of things that have a compare operation without having to care what those things are or what comparison means to them.

I frequently wish I'd taken more statistics. Sure, machine learning is all the rage these days, but it also helps make sound answers to questions like ""did my change make the code faster?""

Finally, there are deep and fascinating connections between logic and type systems, which I find also help me think about programs better. My experience is that you can learn the logic as a side of studying type systems, although there is probably a lot I missed by not taking a sufficiently serious logic class.",1520693175.0
MathPolice,"* Abstract Algebra will have the most eventual usefulness for your CS work.

* Complex Analysis isn't useful for the average CS person but it's very important if you want to pursue math further or to do any advanced Digital Signal Processing.

Also, Complex Analysis is the most *beautiful* ""basic"" math course, with Abstract Algebra a close second.

",1520716498.0
welshfargo,Discrete math and [this textbook](https://www.amazon.com/Concrete-Mathematics-Foundation-Computer-Science/dp/0201558025/ref=sr_1_1?s=books&ie=UTF8&qid=1520706375&sr=1-1).,1520706462.0
crabbone,"Languages, in the context of generating functions / automata theory.  If you are looking to be a researcher rather than coder.  Also logic, something like finite-model theory.  These are the things that are really at the heart of programming, and there's a lot a mathematically-minded person can do there.  A lot of unsolved problems, more than one way this can be applied to programming.  I just don't have that kind of mathematical thinking to get into it, but if I was younger, and had a better brain :) I'd definitely do that.",1520762960.0
onfire9123,Combinatorics and discrete structures helped indoctrinate me to a paradigm of thinking useful to all things computer science.,1520732345.0
Narbas,">I don't see how I have skills that they can't acquire quickly, however they have skills I could never just pick up, an entire different level of insight too.

This is because with mathematics you are learning a language. In other exact fields like computer science you are making use of this language. Because of this reason, if you are trained in mathematics, you are very comfortable in computer science courses as you are already done with the hardest part. You have already learned the language and can now decipher it quickly while the regular computer science students are still struggling to understand the mathematics before they even get to understanding the theory it explains.

>at the beginning I thought CS must be very math heavy since everyone talked about it in such a way, people said ""the difference between a CS student and a math student is that the math students decides to study math and the CS guy does not"", in my experience this could not be further from the truth.

It is kind of maths heavy, but not when you compare it to a major in mathematics. It is maths heavy if you are not used to seeing mathematics. I also like to scoff at the ""cs is all hard maths"" folks because in the grand scheme of things the maths an average computer student sees is really elementary, even in prestigious programmes. I also agree with your assesment of the comparison between maths and cs students. I am in both a pure mathematics and theoretical computer science Master's programme at the moment and it's ridiculous how much harder the maths programme is. But this is partly caused by the fact that pure maths doesn't concern itself with applications and is therefore way more general. By the time the cs courses catch up the material they treat is often only a minor variation of a more general theory you have seen way before then in maths.

But at the same time you are selling computer science short. Computer science is not a pure field and concerns itself more with transforming theory to useful results and applying these results. This is something tangible that you will find sorely missing in pure mathematics if that is something you like. Being good at computer science also involves building up skills in designing algorithms and learning to navigate yourself through possible choices in implementations of these algorithms. The field can certainly stand on its own.

>so what options do I have?

If you love maths, you love maths. You could change programmes or stick with cs and choose maths courses as electives. Considering the courses you have done there will be no further benefit of learning new maths to your cs courses, but they will help you if you decide to become a researcher. You could choose measure theory as this will surely rear it's head if you go deeper into statistics or by extension machine learning and data science. You could take abstract algebra as graphs can be seen as algebraic structures and computer science relies heavily on graphs. You will however never use functional analysis, differential geometry and the likes unless you decide to do research in extremely niche areas like homotopy type theory, manifold learning etcetera. Going into these would certainly require a degree in mathematics too.

I myself am opting to do research in computer science areas but with a highly mathematical component, for instance cryptography. The theory (algebraic geometry) in my case comes from mathematics but once you take that and run with it you end up in the realm of computer science. In essence I am choosing an applied field but try to work on the extreme outer edges where I can use my knowledge of maths.",1520927426.0
Peter-Campora,"Abstract algebra, topology, mathematical logic, and category theory are all pretty common in programming language theory research. I took abstract algebra, but I still wish I had taken courses in topology. ",1520988186.0
astrolabe,"For those interested in this, there is currently an open source effort that you can contribute computing power to, particularly if you have a graphics card. It is an attempt to copy a later deepmind go player called alpha zero.

https://www.reddit.com/r/cbaduk/comments/7e8d0n/leela_zero_links_and_status/",1520702072.0
vznvzn,"nice but believe it or not this is already significantly out of date. even more substantial the same/ similar algorithm now plays master chess. ie ***Alphazero*** this is a really substantial breakthru not widely appreciated yet. nobody is saying this has any implications for ***AGI*** but think it actually does. ***RL/ reinforcement learning can be thought of as general way to explore an environment***.

in this case its a contrived/ simulated environment ie a complex game. but similar algorithms can be unleashed on other types of environments eg robotics + simulated physics. and yes, think that these approaches have some relevance/ connection to ***AGI***, more on that here, stay tuned for updates

https://vzn1.wordpress.com/2018/01/04/secret-blueprint-path-to-agi-novelty-detection-seeking/

***RL/ deep learning*** has been criticized as ***data intensive*** but looking at human learning, considering mass quantity of sensory input, thats more a feature, not a bug...",1520692062.0
rore256,First,1520674024.0
timmyotc,Have you considered reading the sidebar?,1520634932.0
maladat,"Are you sure you've implemented the algorithm correctly? You can do this in time linear in the number of nodes and edges.

Also, isn't Redis in-memory storage? If you have enough memory to store the data, why not just store it in a huge array? Redis is bound to be slower.",1520633360.0
sturdyplum,"So you could create a bitset for nodes that you've visited in the dfs so you get a nodes/64 amount of memory to store a visited array. If you ever run into a nodes with an edge to a then your done and there is a cycle. This does not allow you to reconstruct the cycle since that would require you to have an additional nodes count amount of memory. With a 64 * reduction to memory you could easily get away with a large amount of nodes, even in the billions. 
",1520639897.0
Terr_,"> What is the best programming language / storage method 

I'd like to hear more about your use-cases and the size of the dataset you're working with.

If it's small enough I'd do everything in one single program on a desktop. No latency from network connections to databases, just loading into memory from a file.

In contrast, if you have jiggabytes of data sharded multiple ways, then you've got a different set of problems.",1520646187.0
asdf2100asd,"The best language for it?  Well, the more unfun the language, the better it probably is.  You can't get much better than C.

I use python for this sort of stuff.  There are many extensions to make it very competitive.

I would not use adjacency lists unless my graph is very sparse.  I would use an array repesenting an adjacency matrix, the tightest array I can find if you are worried about memory.  A c array, or something very competitive in another language (like a bcolz ctable).

If you don't want to implement the details yourself, I would recommend python Networkx.  It has many cycle detection algorithms available, and they return a generator, which without getting too indepth is very memory efficient.  Depending on the computer and how much traversal you are looking to do, you may end up not needing to store anything outside of memory.

P.S:  Are you aware that depending on your goals, what you are seeking to do may be exponential in complexity and likely unfeasible ?",1520644062.0
Matholomey,"If speed is all you want then use a gpu implementation of bellman ford and store all the stuff in the vram. If you want to keep it easy then just use a queue based multicore implementation of it in C#, Java or C++ (or c but it will be nasty). Add more ram, load everything into the ram. If your ram is too small then you need to read packets of edges from a db into the ram calculate shit and then store the results, load new edges etc. Use an ssd or you are fucked. 
Buy more ram.",1520654760.0
SteeleDynamics,"Yeah, others mentioned Bellman-Ford. It has cycle detection, not DFS, and is `O(|V|•|E|)`",1520705673.0
timmyotc,"What kind of help?  With research?  No, people with doctorates aren't going to volunteer their time like that.",1520635885.0
,[deleted],1520627481.0
umib0zu,"Other people have answered your question, but you are aware you can change the RE to a DFA right? The picture of the DFAs for these two regexes are quite different, and make it really obvious.",1520630967.0
Dphily500," Yo OP I had an exam today on regular expressions, best of luck to ya. You can do it ",1520629760.0
lelanthran,"For a PhD (or any dissertation) you need to start off with your research questions.

What are they? The shorter the question the more vague it is, but short questions usually lead to long questions about a specific topic or area.

Those long questions are what you would be publishing as research papers.",1520629823.0
east_lisp_junk,"There's a particular difficulty in publishing design work: People are inclined to wonder, ""Is this actually research?"" Obviously, not every piece of software is publishable research. It could just be straightforward engineering -- plenty of intellectual heavy lifting, but nothing sufficiently novel. I expect detailed criteria for what makes a design paper publishable will vary a lot by field, but you will almost certainly have to argue for the novelty of your work.

In order to argue that your work expands the boundaries of human knowledge, start by writing up what your design makes possible that established/prior designs do not. What shortcomings of theirs did you set out to resolve, what old limits are you beating, what ideas are you bringing in that weren't being applied before? You may get a lot of ""everyone knew you could do that,"" and you'll have to argue that there's a reason nobody actually did -- which you have now overcome in an interesting way. So if you say you're solving an old problem, explain why it took until now to solve it. If you're solving a new problem, explain why it became a problem recently.

Either way, you'll need a pretty thorough literature review to convince people to believe you when you say this thing hasn't already been done before. ""Nobody's done this"" can be a pretty strong claim in a world with billions of people. You'll also still need some sort of evaluation/validation for your design to show that it actually accomplishes the purpose you gave as the motivation.",1520630443.0
Matholomey,"I have never published anything but I've read a lot of papers. I always think of them as ""scientific tutorials"" that are trying to solve a problem using an ""unknown"" approach (unknown in a ways that the author didn't know that someone else has already done something like this at that time). The ""new"" thing can be translated to ""different"".

You write down whats the problem you are trying to solve, then what stuff are you want use to do it and how each method works, then how all these methods you've gathered work in combination to solve your problem. The methods are described in medium- up to high- detail with proofs where they are needed.",1520627782.0
Rangsk,"In most programming languages including C++, symbols and keywords are often reused in contexts where there's no ambiguity. Think about the symbol \- which can either be the binary subtraction operator or the unary negation operator. `a = b - c` vs `a = -b`.

In the case of `for` loops, the two semicolons are required, and you aren't allowed to use any extra ones. They aren't the same as the semicolons which separate statements. They instead serve to separate the three expressions of the ` for` loop: the initializer, the condition, and the iteration.",1520623517.0
neilmoore,"The second and third components of a for loop in C and C-inspired languages are expressions, not statements, and expressions don't usually end in semicolons in those languages. (The first component also isn't a statement, but it isn't just an expression, either.)  The semicolon was presumably chosen for the for loop syntax because it is a convenient separator character that (unlike comma) can't occur inside an expression.

Ritchie could have required a trailing semicolon there, but it would have been an extra character to type and read that didn't ever reduce ambiguity.",1520623849.0
Python4fun,"to elaborate on what /u/Rangsk said.  The semicolons in the for are kind of like commas separating the fixed set of three arguments to a for loop.  Also the ) ends closes that part of the statement.

also, this is valid:

    int i = 0;  
    for(;;){  
        if (i >=3) break;  
        i++;  
    }

",1520623695.0
jmite,Because the people who made the language decided it would be that way. Syntax is completely up to human prefence ,1520658000.0
NotInUse,"As others have pointed out, the language designers get to set the rules.  There are some middle age languages where you can use simple regular expressions for a process called lexing and a formal grammar specifies how the resulting tokens are parsed.  Other languages involve a far more twisted set of rules which make it impossible to statically parse the language.  The same shift from formal lexing and parsing to randomly made up and poorly documented rules has made its way through a number of internet protocols.

For a very small taste of when a semicolon is required in another language, here is one for [Pascal](http://wiki.freepascal.org/Else).",1520671098.0
anamorphism,"it's just how the language was specified.

the other folks provided some context as to why that decision was potentially made, but it's mostly just an arbitrary decision at the end of the day.",1520624722.0
sunnysushi,"Thanks for the answers, guys! I appreciate it!",1520903976.0
BaconBoyReddit,"Hi

I'm a computer science student at a liberal arts college in Mass. Before coming here, I had no experience in computer science. There were required courses I took on computer theory, linear algebra, and so on, but most of the time spent here was programming software.

My senior project has involved my working with React, Node, and a MYSQL database I host on a private server. It's coming along, but I feel like I need to use stackexchange and online tutorials for so much of it.

I'm scared. I'm scared I'm not cut out for this industry every time I get stuck. I feel like I don't know how to be a computer scientist, I just know how to program (and even then, I feel behind). What can I do? I want to learn, I want to make things and innovate and create but I feel like my courses never told me *how* to make something, they just told me what to do. ",1520611033.0
maladat,"You should be able to convince yourself very easily that {b^m | m>= 0} can't possibly be the answer. 

Is 'c' in L1? Yes. Is 'c' in L2? Yes.",1520545117.0
PM_ME_UR_OBSIDIAN,First example is wrong. The `A` leaf is an unival tree as well.,1520554848.0
conflicted_panda,"I found a stackoverflow post offering an O(n log n + m) algorithm: https://stackoverflow.com/questions/48681194/queries-to-figure-out-if-point-lies-inside-polygon

It doesn't even require the polygon to be convex (but still puts a restriction on the shape of course) if I understand correctly, so there may be a more optimal solution.",1520544139.0
Indie_D,"If you’re trying to figure out if a point is inside a convex polygon, consider this: each edge is two points, p1 and p2. Which side of the edge pA is on can be determined by taking the dot product of pA - p1 and the perpendicular vector of p2 - p1, and the sign of the answer tells you which side pA is on. Do this for all edges clockwise (or counterclockwise - as long as it’s all the same) and that will tell you if pA is inside",1520541516.0
humor9268,"This is an online-convex-hull problem. State of the art is O(n lg(h)) time, where h=# of hull points. This is considering serial processing of the points. 

Very famous algorithm devised by Chan in 1996 is still used at many places. He combined already existing algorithms in a beautiful way. Prior to that I guess Alk-Toussaint was a big thing.

There is a library called CGAL, very creative name for Computational Geometry Algorithms Library, which contains 5-6 benchmark algorithms. It also comes with data of 500 points. If you happen to get CGAL working, you will be able to see their speeds (in miliseconds) as well.

Apparently, developing new convex hull algorithm is not that easy. Your algorithm has to be accurate to exceptionally small floating poing accuracy. I still do not understand how exactly things falter beyond a certain point, but this is what great minds in the field talk about.

Edit_1: by the way, the library is beautiful and highly recommended for anyone going to extensively utilize CG. It is in C++. Last time I checked latest version was 4.8",1520550602.0
Indie_D,"Not really sure what you’re asking about, but at the very least, you can fit an axis aligned bounding box around a convex polygon. ",1520540794.0
nevabyte,"Not sure if I understand correctly but maybe you could use the separating axis theorem. Albeit, it is for intersecting convex polygons, it should be able to be used for assessing if a point, which is essentially a 1*1 convex square, is inside or outside of another convex polygon.

I may be wrong though!",1520543647.0
,[deleted],1520616364.0
leftofzen,">  there has not much work been done

Please don't make false assumptions; there has been plenty of research into computation geometry. This problem is called the [point-in-polygon](https://en.wikipedia.org/wiki/Point_in_polygon) problem. There is no O(logn) solution unless you sort something first, eg vertices by X coordinate. Sure, then just finding the answer from your preprocessed data is O(logn) but the overall time including pre-processing is O(nlogn). You can otherwise solve this in O(n) such as by using the winding number algorithm.",1520546674.0
PM_UR_FRUIT_GARNISH,Read your textbook. This is undoubtedly homework.,1520531691.0
ParanoydAndroid,"I'm going to be a bit more charitable and assume you have read your textbook and still need help.  I won't give you the answer, but the key is to break the problem down:

1. What's the difference between BCNF and 3NF?  It's got something to do with the types of attributes the rules apply to.  This answer will just require that you re-read your textbook if you don't already know it.  As a hint, recall that 3NF is the exact same as BCNF except BCNF has one less type of freedom than 3NF.

2. Find your keys.  What are the superkeys for this table?  What are the candidate keys?

3.  What are the prime attributes?  What are the non-prime attributes?  Does each dependency apply to only prime attributes, only non-prime, prime on the left, or prime on the right?

Now apply your answer from question (1) to your answers from (2) and (3).  Although are you sure those are the only two answer available?  At first glance I think it's in neither form and the more usual question would ask you to define new relations to establish a normal form.",1520538858.0
LessCodeMoreLife,"This sounds vaguely like someone's homework.

But... Views normally aren't stored on disk, they're basically named queries.

If you delete from S, unless you have a second trigger that updates R, nothing will happen to the rows in R.

So, if you've inserted into S, which caused an insert into R, but then you deleted from S, your tuple will still be in R.

It's kind of hard to think about these things when they're named by letters though. It would be easier to get help if they had full names.",1520529553.0
MUDrummer,I think you need to do your own homework. Come back with more specific questions. ,1520527302.0
DonaldPShimoda,"I prefer the quantum bogo sort, where you shuffle only once. There are now *n* universes (where *n* is the number of possible permutations), and you simply throw out the universes where the shuffle was not sorted. This leaves us with only one universe — the one in which the elements are sorted correctly. Much better runtime than regular bogo sort.",1520524747.0
,[deleted],1520520037.0
__interface,"lol this is awesome, I wonder how ineffective this really is",1520524801.0
tungstenwave,"Shooting from the hip with two minutes of thought:

I could see a project that roughly has the shape of the following: Using machine learning techniques (statistical analysis, etc.) to process social media data sources (Twitter API, etc.). Evaluate the interaction between football team media coverage and fan participation in social media. Specifically you could compare pre-game coverage especially from official sources (directly from teams, players, other sources considered official), and fans reaction or engagement before, during and after a game.

You could use machine learning to compare the types of language or phrases used in the social media not just the sheer volume of content. Does this make a difference in engagement. Throw in a few words about NLP (natural language processing) and you should be able to grab the attention of some prof.

You could frame this problem by asking questions like: Does the amount of media generated from an official source change fan engagement on social media? Do player expressions change the way fans engage? To what extent do successful clubs (based on dollars?) have more engaging players? Can you predict wins/losses based on player comments before a game? Should clubs encourage players to Twitterbook more? Did your professor read your thesis closely enough to realize the Southern Kings aren't a football team?

Then extrapolate your analysis technique to another sport. Test the validity of your model by showing/disproving a similar result for say baseball. 

This would be an applied thesis more than a theoretical research project which should make it easier. There are a ton of resources on how to aggregate and process tweets and several good machine learning models on processing language. Find the right building blocks and answer some questions.

Or whatever, I'm sleepy and words are hard.",1520481706.0
Catalyst93,"Find general topics you would like to work on and then go talk to professor's about possible projects.  No offense, but you probably won't be able to come up with a good idea for a research project on your own.  Unfortunately this is just a very difficult task without lots of real research experience.  Thankfully this is exactly what professor's are good at (in addition to carrying out the research), so they can help you out there.",1520484773.0
bremby,"Search for any potential supervisors at your uni ASAP, call those that seem in the field you're interested in and set up a meeting, again ASAP. Don't use e-mail, that would take you months. Call them and ask for a meeting. Explain your situation, ask for recommendations on project topics and also if there's something you can do about the deadline.

Coming up with your own topic is hard alone, and then you'd have to convince a supervisor to supervise you on top of that. Talk to (potential) supervisors first!",1520500836.0
VIM_GT_EMACS,"maybe unrelated, but consider x-posting to /r/cscareerquestions as i'm sure there are plenty of people with advanced CS degrees on that sub.

edit: nvm i see you've done that :). best of luck man!",1520476697.0
Tommy_ML,Read this wrong,1520530184.0
foreheadteeth,"I'm directing two MSc's this summer, one of them is on neural networks. I'm not 100% sure what exactly will be in it but the fall-back position is to compare various learning algorithms. For example, the visualizations on this web site are fantastic: http://ruder.io/optimizing-gradient-descent/

The final ""application"" doesn't matter: the usual MNIST dataset is plenty, what we're interested in is the learning algorithm. http://neuralnetworksanddeeplearning.com/chap1.html",1520531911.0
TomvdZ,What have you tried? Why can't you figure it out? Where do you get stuck?,1520444754.0
thereisnosuch,"define good jobs. Big 4 companies usually don't care about grades. I know someone who had a 2.0 and still somehow got into amazon after working a year at a low pay company. 

Phd on other hands depends on grades and references.",1520443228.0
locotxwork,"If you have no work experience, then GPA is all you can be judged by.  You need work experience to be able to showcase work achievements that are by far more valuable and impressive than simply GPA.  I just don't get why these things aren't more focused on while going to school.  Long gone are the days of internships which were the mechanisms by which you were able to get real world experience.   I graduated with a 2.7 from UTA and I'm doing just fine.  My suggestion is avoid those companies that filter via GPA.  I have this struggle with my son right now.  Although grades are important, they are not everything . . the problem is that if grades are the ONLY determining factor then you are screwed.  You don't get grades for teamwork, for being a good person, smiling and having a happy disposition, you don't get a grade for your attitude and willingness to listen to older seasoned managers, you don't get graded on your perseverance in helping work through a business issue . . . THESE things are much more important.   Here's a secret.  Once you get out of the academic world and into the working world . . no one will give a shit about what GPA you have, they will only care about your achievements in your industry.  I just thought you should know that GPA is not a reflection of understanding or knowledge.  I hope that makes sense.  I know people who have doctorate degrees who don't know what Bitcoin is . . who couldn't use a debugger . . .who have no clue what social engineering is  . .and they have PhD's in the industry.   Not only that, but they have to be called ""doctor"" and yet they couldn't change a flat tire on their car to save their life.   Just do your very best and at the end of the day, whatever GPA you earned is what you get . . .but you'll have the peace of mind knowing that you did everything in your power to get the best GPA possible.   ",1520450298.0
klingon33333,"It really depends, there are some jobs out there that do look at transcripts (I know google asks for them) and I have interviewed with several companies that have GPA limits for students coming directly out of college. Typically these limits are 3.0 and 3.5 (I know Boeing had a 3.5 limit for a while, idk if its still in use) but there is some play there. With all that being said, the VAST majority of companies don't care about your GPA and as long as you aren't at risk of dropping out you will be fine. Sure it makes you look better if you can say that you graduated magnum opus summa cum laude from whatever university, but after your first job companies care about your work history and what projects/platforms you have experience in. I have seen people with absolutely garbage GPAs get great jobs following graduation because they nailed the interviews and the company didn't ask about grades.

I would also mention the other side of the coin where companies will recruit from specific classes and their top performers. I have heard of classes (at the graduate level) where companies know that the students that come out of the classes will be well taught and as a result will recruit directly from the pool of top performers in the class. I have some friends who have seen this behavior with Disney and some grad programs focusing in visual arts and virtual experiences. That doesn't mean that if you aren't in the top of the class you cant get a job, it just means that its easier if you are at the top of the class.

As for PhD programs grades matter, but admissions committees care about research ideas, publications, and letters of reccomendation FAR more than grades. If you have a meh GPA but some awesome publications with some co-authors who can speak highly of you, you will likely be in a great spot for PhD apps. Masters programs are a bit of a different story and they tend to care more about grades than others, but its a mixed bag and it depends on the program. 

Obviously a high GPA is always better than a low GPA but if yours is lower than some of your peers its not even close to the end of the world. ",1520449822.0
DrKedorkian,"Mine was a 2.7 and things are going just fine.  GPA matters for (at most) 1-2 jobs out of school and then no one ever asks again.

edit: in my case it mattered for 1 job, never put it on a resume and the only company that has ever asked was Accenture.",1520450264.0
jet_heller,"Almost no one cares about college GPA. What's more, about 2 years in a job and no one cares about GPA at all. No one has cared about mine and I've not cared about anyone who I've interviewed.",1520450522.0
magnificentbop,"GPA is definitely used to filter out applicants.  I've heard many of the arguments you gave that GPA doesn't matter, but it is one of the few objective measures of performance that an entry level employee has.

As far as PhD programs go, most of them have strict minimum GPAs which they will accept.

Don't let anyone put you down, most people are not motivated enough to apply themselves at college age.  Show them what you are doing to improve yourself now.",1520450570.0
UncleMeat11,"CS PhD here. Currently working at one of the big four.

GPA does matter for PhD programs (at least the top ones). It does not matter as much as research experience, so if you have solid published work from undergrad this can outweigh a low GPA but in general the acceptance rates for top programs are so low and resumes are so strong that you really do need great grades in addition to research experience. Somebody with a 2.5-3.5 GPA is going to need something pretty special on their resume to get accepted to MIT/Stanford/Berkeley/etc. If you really want to go to a strong PhD program, get started on publishable research *right now* so you can have something to point to on your resume other than grades that will make you stand out.

For the major tech companies, grades also matter a lot if you are applying in the normal way. Companies like Google and Facebook only hire a very very small percentage of applicants and they use early filters using GPAs to avoid interviewing everybody. That said, if you know people at these companies who can vouch for your work you can get interviews without having strong grades. At this point your grades matter much less if you can really nail the interviews.

For smaller companies grades matter much less. A startup that is desperate to hire and needs people right away is going to look much harder at skills and background than grades. If you have real work experience with trendy systems then this can make you stand out, though you risk having your skills become obsolete when the next JS framework becomes popular.

After your first job, nobody is ever going to care about your GPA ever again.",1520450662.0
jmite,"First off, nobody expects a first year undergrad to have published papers!

For areas, HCI and Software Engineering tend to be a little less mathematical, though you'll need enough statistics to analyse the results of experiments.

Systems is not mathematical in the traditional sense, but there's lots of low level bits an bytes to track, along with lots of data structures.",1520439122.0
GNULinuxProgrammer,"Instead of doing research, study math. Seriously, no math no CS.",1520453870.0
jhillatwork,"A lot of CS is applied mathematics, but there are some fields within CS that aren't. OS Design and Computer Architecture Design aren't as intense in math as more algorithm-driven fields. But they do touch on it. OS Design is now intrinsically linked with Security Design; the latter of which is highly math intensive (as you're likely well aware).

I hate to say it, but the more technical the area of study you want to delve into, the more mathematically intensive it becomes. This is mostly driven by the fact that mathematics is a language to model the science we are developing!",1520454353.0
hendawg98,Computer Science is for the most part Math and Logic applied using a programming language and computer as a tool. If you don’t understand math you can’t go very far in research,1520455222.0
justworkingmovealong,"CS is pretty much always going to have math involved, especially if you're doing research. How much math? That's up to you, but Algebra will always be necessary. So will some Statistics. There are definitely areas where you don't need it, and others where you only need enough to apply someone else's algorithm. 

The thing about math though, is that it's all about logic and algorithms... which is exactly what you're doing in CS. If anything, math helps train you to think more logically and think about repeatable processes... again, something you do in CS all the time.  ",1520465370.0
umib0zu,"/u/daredevildas what math do you not want to do? I figure it's easier to figure out what you don't want to do first then work from there. ""All math"" is impossible to avoid.",1520456273.0
,"I think the best idea is to just take a few more classes and a lot of it will sort out itself.

In your first undergrad year you're normally supposed to learn the basics and no one expects you to do research.",1520470709.0
tulip_bro,"I can't think of a CS area that doesn't involve math-intensive requirements, unless you consider _software engineering_, which will be more subjective. E.g., design patterns, development life cycle, etc. 

Since you are in your first year, check out other degree paths that are certainly less math-intensive, but still share intersections with CS, like:

* Management Information Systems
* I.T. 

I'd wager there is exciting research going on in those areas as well.",1520809996.0
Raamakrishnan,Give computer architecture a try. There is no math involved. Everything is pure logic.,1520454185.0
tkitta,Nothing to shocking here - maybe the fact that this was not better peer reviewed to find the problem before it got this far.,1520446502.0
xiaomai,"gedit is a fine basic editor, but I think for someone who will spend a lot of time editing text you should learn something more capable.  I'm partial to vim, but emacs or vscode are great choices as well.",1520404081.0
,"It's used in the same way notepad is used in windows, only when you want to quickly look at a file. Generally not for programming purpose. It's not really used, people usually use something else (but it depends on the language). It doesn't matter much anyway if you are starting now. It's enough. Later you will find other editors you like more.",1520428373.0
Hollowprime,"I use it all the time. On c/python/html projects in the university. It's an ok tool but you'll have to run it alongside the terminal to see results. It's not a complete IDE (it won't show you errors,at least not a way I know of) and it requires a few plugins to run . Other than that it's a standard tool for text editing in ubuntu.",1520404135.0
jmite,"There are 5 popular text editors these days: Vim, Emacs, Atom, Sublime, and VSCode. The first two are very ""old school"" and have very different ways of doing things, but are very mature with tons of extensions and packages. The latter three are very modern, and tend to also have lots of add-ons, but are not as fast or lightweight, and don't have decades of maturity like Emacs and Vim.",1520444436.0
JokdnKjol,"Check out freecodecamp.org you said you had some familiarity with HTML/css, so go through the site and master those in addition to JavaScript (fcc covers this) and/or design (Photoshop etc). Plan to work on that for maybe 6 months, though it really depends on you. After that, build at least a couple reasonably big projects with what you've learned. If you can do that, you should be in a fine position to start as a junior front end developer.",1520384160.0
ajiired,"This is a pretty solid list on how I approach a lot of my problems while at work. Also on desktop the site experience was great; it was straight to the point, didn't break the list into multiple pages and, had a refreshing lack of useless content (images that add no value).

Thanks for sharing this article, I really enjoyed it.",1520388149.0
brett_riverboat,"These are excellent tips and I've applied almost every one of them.

One other tip I would add is to unit test as you go. Certain syntactical decisions can make testing extremely easy or extremely hard. You could end up with a perfect module that has to be almost completely rewritten to allow for proper tests.",1520391781.0
aiPh8Se,"This was surprisingly good; most ""N things"" articles are useless clickbait.

> 3. The third time you write the same piece of code is the right time to extract it 
> 23. ""Not Invented Here"" is not as bad as people say.

I also learned this over time. Trying to abstract things away early usually fails; the abstract will be leaky, so it's not good at abstracting, plus you pay the overhead for having more abstraction.",1520388731.0
dnpmpentxe,"I typically disagree with lists like these but this one is spot-on. Unfortunately, I think that everyone needs to learn these lessons the hard way. It's not until you see things go wrong that you say, ""Well, I'm never doing that again.""

Every developer I've talked to has experienced - and unintentionally contributed to - the [Lava Layer Anti-Pattern](https://mikehadlow.blogspot.com/2014/12/the-lava-layer-anti-pattern.html). This is my favorite example because young developers will see the problem, understand it, try to fix it, and in so doing contribute to make it worse. Sometimes you can only learn the hard way.",1520396110.0
Fletcher91,"In point 7, not all permutations are possible, but the following cases are more prone to bug ime:

* The null/undefined-case 
* Empty array/string
* Array with only 1 item 
* Special character string (i18n, emoji etc)

Also, returning an empty array keeps the signature simpler than returning null, since the receiver will probably do a length check anyway.",1520404941.0
fr0stbyte124,"I've always hated the YAGNI principle. Most of the work I do is inside legacy codebases, and it's a mistake to think you'll always be able to go back and flesh-out functionality or do things properly at a later date. Maybe by the time it comes up you won't even be working there, and the next guy is completely screwed.

Instead, I follow the principle of least regret: make a guess about what you think you'll need, and then determine what the consequences would be if you're wrong about that. If it's serious, disregard what you think and do the thing that avoids that outcome, even if it seems costly, inelegant, or a waste of time, because one day it won't be a waste of time.",1520403473.0
istarian,"Bunch of splintered points...  
  
Testing seems a bit overrated. I definitely agree that, since commenting is hard, conveying intent should be the priority. Avoiding endless code duplication by factoring stuff out into separate functions (call it modular if you like) is also a wise idea.  
  
I think seeing code as the enemy is a mistake. Certainly you shouldn't write more code than necessary, but the 'less is better' logic is hazardous imho. That road leads to obfuscation and lack of clarity.",1520431181.0
SledgeHog,"Ban this fucktard

https://imgur.com/ZbLTE2U",1520536247.0
combinatorylogic,"Knew it will be full of unit testing zealotry before even reading. Many good points though, if you ignore the unit testing ones.",1520411872.0
psychologicalX,Thanks for reddit gold kind stranger,1520389752.0
ThePandaGuitar,Amazing article! Thanks for sharing.,1520410282.0
sarkie,"Thank you!! 

Brilliant",1520410235.0
fnork,"""Code is the enemy"" 
Right then, thanks for playing.",1520406852.0
ProgramTheWorld,"Isn’t the question just a bunch of inequalities?

B NE A really means the following 

-	B_x > A_x
-	B_y > A_y

Seems like this would be an easier way to understand the question.",1520373365.0
cdo256,"The only hard thing about this is the fact that the problem is so poorly phrased.

Once you decode the following, then it’s just a cycle search on two digraphs.

A N B means A.y > B.y  
A W B means A.x < B.x  
A NW B means A N B and A W B",1520373368.0
hhhax7,Why do they ask such hard questions for a job where you literally just pick people up in your car and drop them off?,1520357642.0
tkitta,"Interviews at Uber must last a long time or maybe I am just not worthy of the stuff as a programmer. 

I mean this sort of makes sense for a company like Uber given that programmer would be required to do routing for... a cab company BUT still seems like a very involved problem unless the interview is like a whole day or most of the day.",1520369973.0
bnelo12,I got asked this question for an internship position. The guy interviewing me used to work at Uber. Very interesting.,1520361150.0
,"Cool problem! After considering and rejecting a SAT-based approach (anologous to build a knowledge base: Will work 100%, but huge overhead), I found a quite similar solution that I like more tho:

(Simplified problem statement with only N-E-S-W instead of the mixed cardinal directions, but they should not pose a problem).

-----------------

1. Have 4 directed graphs, one for each cardinal direction.

2. Do the following for all rules: If, say, `A N B` is encountered, add a edge from B to A in the North graph and a edge from A to B in the South graph.

3. Check every graph for cycles. Iff every graph is cycle-free, then there is no contradiction in the rule.

-----------------

I like this approach more because now finding the answer is purely graph-based and has a better worst time complexity: O(|E| + |V|) with [Tarjan's strongly connected components algorithm](https://en.wikipedia.org/wiki/Tarjan%27s_strongly_connected_components_algorithm)

E: Thanks to /u/FalconTaterz : It is possible to do with only two directed graphs, for example for directions N and E. If `B S A` would be encountered then, this would be considered as if `A N B` was encountered. What a nice observation which puts us closer to the truth: If the four cardinals form a vector space, only two unit vectors are needed.",1520369805.0
Buckwheat469,"I haven't seen this problem asked in an interview, nor could I find it in the acceptable list of questions. That doesn't mean it's not a question that was asked, but I wonder if it was a person's one-off question rather than one from the available list. Most of our questions are standard data structures, binary or n-trees, some mapping questions unlike this and much simpler, perhaps some ML questions. Questions are also picked based on the job category. We wouldn't ask a question like this for an eng 1 or 2, however a senior eng 2 or principal might get something like this.",1520371244.0
__GG,"Solution in Python + z3 (SMT solver). Haven't written z3 in a while, so there might be some more efficient ways to do this. The short of it, assign N to greater than, and S to less than. Same for E and W. Set up one set of conditions for NS and one for EW. Ask the solver if the conditions can hold.

    from z3 import *
    
    conditions = '''
    A N B
    B N C
    C N A
    '''
    
    symbolic_variables = {}
    conditions_ns = []
    conditions_ew = []
    solver = Solver()
    
    count = 0
    for line in conditions.split('\n'):
        toks = line.split(' ')
    
        if len(toks) < 3:
            continue
    
        tok0 = toks[0]
        tok2 = toks[2]
    
        if tok0 not in symbolic_variables:
            symbolic_variables[tok0] = Int(tok0)
    
        if tok2 not in symbolic_variables:
            symbolic_variables[tok2] = Int(tok2)
    
        var0 = symbolic_variables[tok0]
        var2 = symbolic_variables[tok2]
    
        condition_ns = Bool('cns_{}'.format(count))
        condition_ew = Bool('cew_{}'.format(count))
        count += 1
    
        if 'N' in toks[1]:
            solver.add(Implies(condition_ns, var0 > var2))
            conditions_ns.append(condition_ns)
        elif 'S' in toks[1]:
            solver.add(Implies(condition_ns, var0 < var2))
            conditions_ns.append(condition_ns)
    
        if 'E' in toks[1]:
            solver.add(Implies(condition_ew, var0 > var2))
            conditions_ew.append(condition_ew)
        elif 'W' in toks[1]:
            solver.add(Implies(condition_ew, var0 < var2))
            conditions_ew.append(condition_ew)
    
    if solver.check(conditions_ns) == sat and solver.check(conditions_ew) == sat:
        print('satisfiable')
    else:
        print('unsatisfiable')",1520386760.0
svick,"Do I understand it right that the problem statement is basically just: 

You have two directed graphs, N-S and W-E. (They can share names of their vertices, but that's not relevant.) Find if one of the graphs contains a cycle.",1520376726.0
rz2000,"Aren't the transitive properties for north and south different than for east and west?

For example 

Japan E India  
Japan W Canada

Spain E Canada  
Spain W India

Japan ? Spain  
Canada ? India",1520386484.0
ren_at_work,"Another way to solve this (I think), but inefficient, would be an inference engine type thing; check for contradictions before inserting into the knowledge base.  

As in:

1. `INSERT(A NE B)`; `KB = {(A N B), (A E B)}`
2. `INSERT(C N A)`; `KB = {(A N B), (A E B), (C N A), (C N B)}`
3. `INSERT(B N C)`; `Contradiction found (C N B)`

Looking for cycles in two graphs sounds like a better approach though.",1520377094.0
aonghasan,"Wasn't Uber involved in a case where they giving interviewees *real* problems so they would try and give them a solution, but never hired any candidate?

~~Looking for it now.~~

Only found a [reddit](https://www.reddit.com/r/pittsburgh/comments/4lao0l/uber_apparently_treating_interviews_as_market/) and a [YCombinator](https://news.ycombinator.com/item?id=11783326) thread linking to a dead Imgur album.",1520379425.0
nhum,This question is trivial...  Just two directed graphs and a search for cycles.,1520384244.0
pmwws,"I mean there's always p vs np, that's my favorite formal language problem. ",1520343523.0
dgryski,"The move from FSM to PDA is usually with trying to matched paired brackets.  That shows the limits of the regexp/FSM model, so you augment your machine with a stack to allow solving it, then you can show a CFG for paired brackets and do the CFG -> PDA construction.",1520351763.0
SOberhoff,You could make a simple calculator for evaluating expressions such as ((1-3)*(4+2)+2).,1520362051.0
PM_ME_UR_OBSIDIAN,"[Manufactoria](http://pleasingfungus.com/Manufactoria/) is a free online game about:

* Finite state automata over alphabets of two symbols;
* Queue automata over alphabets of four symbols.

It is cute and layman-accessible, though it gets difficult quite fast.",1520384573.0
help_throwaway96,"I never warmed up with formal language theory because of this reasons. sure, you use some of it in building compilers but I am missing the great usage of it. it's not comparable to physics where theoretical physics is of enormous importance for the field. in CS you can use graphs for things like pattern recognition but that's not really the most impressive thing either",1520690639.0
crabbone,"Recognizing some text fragments is popular, but... I don't think it does justice to the theory.  It diminishes its value by making future programmers believe that it's a very special use for something related to text processing, something that they may never need.

Once I was asked to give an example short lecture on an ""interesting"" subject of my choice.  I chose as my subject the following problem: using DFA to prove some aspects of correctness of OO code.  I still think the idea is neat, even though it never got past the committee:

1. Observation: objects in OO languages are mechanism to work with state of the program changing over time.
2. State changes can be described as automata (not necessarily DFA, but DFA can approximate other automata).
3. DFAs are one of the very few things in language theory that have lots of closure properties, things can be proved about many of their aspects, and, they are relatively easy for people to understand.

#Idea

Why not attach DFAs to objects, describing possible state changes?  Consider this example: `File` object can have these states: `open`, `read`, `seek`, and `close`.  But there are many sequences of states that don't make any sense, and, in a practical language would probably result in an error, s.a. eg.: `close -> read`.  So, we could come up with automaton describing the valid states: `open (read | seek)* close`, and then automatically verify that our code never puts the object into a state that doesn't conform to our rule.

I'd imagine this not to be a great subject for a complete beginner, as someone needs to have experience of encountering the problems this is attempting to solve and different solutions offered, still it doesn't seem to be a very difficult concept.",1520781655.0
karthik102938,"Keep yourself upgraded with the latest software techniques running in the market by getting trained with the real time faculty of Sun Trainings who can drive you through most latest & upgraded software skills.  Attend our free demo session online and get direct connect with our trainers. We offer  trainings in USA, UK, Ausralia and Canada also. Kindly register your query on contact@suntrainings.com / (M) 9642434362 .",1520334306.0
davidsiegel,"In this [blog post](https://blog.quicktype.io/markov/), we explain how we used a Markov chain to detect whether a JSON object represents a class (fixed properties) or a map (dynamic keys) to infer its type and generate the same code a human programmer would for representing that data.

For example, given this Bitcoin API data:

    {
        ""0000000000000000000e222e4e7afc29c49f6398783a94c846dee2e13c6408f5"": {
            ""size"": 969709,
            ""height"": 510599,
            ""difficulty"": 3007383866429.732,
            ""previous"": ""000000000000000000552a7783efd39eaa1c5ff6789e21a0bbe7547bc454fced""
        },
        ""000000000000000000552a7783efd39eaa1c5ff6789e21a0bbe7547bc454fced"": {
            ""size"": 991394,
            ""height"": 510598,
            ""difficulty"": 3007383866429.732,
            ""previous"": ""00000000000000000043aba4c065d4d92aec529566287ebec5fe9010246c9589""
        },
        ""00000000000000000043aba4c065d4d92aec529566287ebec5fe9010246c9589"": {
            ""size"": 990527,
            ""height"": 510597,
            ""difficulty"": 3007383866429.732,
            ""previous"": ""00000000000000000009025b9e95911a4dc050de129ea4eb5e40ef280751a0cb""
        }
    }

You'd expect the corresponding Swift code:

    typealias Blocks = [String: Block] // a.k.a. Dictionary<String, Block>

    struct Block {
        let size, height: Int
        let difficulty: Double
        let previous: String
    }

Rather than:

    struct Blocks {
        let _0000000000000000000e222e4e7afc29c49f6398783a94c846dee2e13c6408f5: Block
        let _00000000000000000043aba4c065d4d92aec529566287ebec5fe9010246c9589: Block
        let _00000000000000000009025b9e95911a4dc050de129ea4eb5e40ef280751a0cb: Block
    }

    struct Block {
        let size, height: Int
        let difficulty: Double
        let previous: String
    }

We taught [quicktype](https://app.quicktype.io/?gist=6fc8b13b26d892bda95dae15f2f16bbd&l=swift&just-types=true) to make the same decision by evaluating the JSON property names with a Markov chain trained on simulated class property names. Our blog article goes into detail and lets you play with our Markov chain!",1520317951.0
TortugaSaurus,You can get an even better complexity bound using binary trees to implement the second level map.,1520317769.0
mercurycc,"Why can’t the key and time be hashed together?

To answer my own question: this thing requires the get function to return the value at the nearest time instead of the exact time.",1520311928.0
cryptoshito,"""If we set a key at a particular time, it will maintain that value forever or until it gets set at a later time.""

this is a normal hashmap?",1520311303.0
nrlb,"I once did this with Pandas multi-index dataframes using an IntervalIndex which uses interval trees under the hood for range searches along with conventional string-valued attributes.  Essentially even though the author is stating queries with one set time, it's really sets of intervals since you have to return to the last set value.  I thought it was neat at the time as I hadn't run across that data structure before.

https://en.wikipedia.org/wiki/Interval_tree

Like /u/dr1fter said, you can also do this with SQL ranges.  Basically, there were multiple records with different identifier types which were only relevant for set periods of time.  Some constraints were applied, basically you can have only 1 valid value for an A1 attribute during any specific interval, and many valid values for an A2 attribute.  So the requirement was to be able to query in different ways, for example (1) find all valid A1 attributes at this time, (2) find all A2 attributes which match this A1 attribute in this time period, (3) find the time periods which had this value for the A1 attr.  etc.

It was an interesting problem.",1520346428.0
thundergolfer,"Got this question in Stripe final round and pretty much flunked it. Got off to a bad start worrying about potential collisions if a coarse `current_time()` function was used and it didn't get much better from there. 

Good question though. Requires thinking about data structures and also code structuring; not just a bog standard leetcode question.",1520324697.0
PeterSR,"> A sorted map would fit the bill, but python standard library doesn’t have one.

What about OrderedDict from collections?",1520319359.0
FearMonstro,"I believe in working prototypes. Have them test out a small piece of core code -- whether it's loading in and parsing data, scraping a website, or getting a GUI to work. I think organization is something that emerges from actually writing the code. Thinking ahead is good for larger projects of course, and is an excellent skill to have, but organization stems from experience. Without experience, it's hard to lay out the progression plan. So I'd do something like have them come up with an idea, and get the project moving in stages. No outlines, no charts, just working code. Taking that initial step from a blank .py to having some code down is a big step, so try to get them over that initial hump. If you need something more concrete, perhaps have them *log* their progress, instead of planning their progress. Maybe even teach them how to use SVN or git and require they commit changes along the way. Something like that.",1520296800.0
MonkeyOfAvon,You could have them use Trello and do something Scrum-like. I think it could be fund for them to create cards to work on. It's also a good way to sketch out a plan and see progress.,1520297121.0
kidtech0,Flowcharts or pseudocode,1520387819.0
ThatGuyJimFromWork,"you might want to take a look at a methodology like Agile, the PMs i work with employ this method for practically everything now.",1520296848.0
luckygerbils,"None of those are really ""bugs"" per se, although they certainly cause bugs when misunderstood and people may regret them, they are as designed (I or others can go into detail on why if you want).

Are you looking more for bugs or ""features that have caused problems""?",1520276186.0
DarthSanity,"There are some classic bugs out there in the literature - check the jargon file. One that comes to mind is the FORTRAN loop statement

DO 10 I = 1,1000

This statement created a loop condition that ran until line 10, incrementing the variable I 1000 times. Compare to this statement:

DO 10 I = 1.1000

This statement set the variable DO10I to the value 1.1

I believe this particular error caused an early mercury test rocket to explode on the launch pad.

This is how we got declared variable names and strong typing.

",1520302349.0
olliej,"0.2+0.1 != 0.3 in any binary floating point format that isn’t a bug in JS any more than it is a bug in any other language you choose to name.

Ditto {}+[] isn’t a bug either it’s a failure to understand the language.

For example, {}+3 is valid in the majority of brave base languages, including C,Java,C#,...

{...} is a block statement.

I assume you just went through the WAT presentation, but frankly a bunch of that is either deliberately trying to be confusing or the author was willfully ignorant.

I’m not saying JS is flawless, but a lot of the “flaws” are people not understanding (or not trying to) understand computers and then saying “look how bad it is”",1520279412.0
eholk,"One that I find most interesting is the [Patriot Missile Failure](http://www-users.math.umn.edu/~arnold/disasters/patriot.html), which actually has the same root cause as your 0.2 + 0.1 != 0.3 example. The missile software was counting time in tenths of a second, but in binary 1/10 does not have a terminating representation, the same way 1/3 goes on forever in decimal. Over time, this led to a significant round off error which mean the missile missed.",1520298870.0
fakehalo,"Not sure if this is directly relatable, but C:  People figured out format string bugs could be exploited around ~2000, ie. printf(buf) when it should have been printf(""%s"", buf).  And buffer overflows/memory corruption with C in general.",1520279343.0
ProgramTheWorld,None of your examples are bugs. It’s just that you don’t understand how the language works.,1520288925.0
peaceyadig,"The python datetime module's strftime() has unexpected behavior using ""%Y"" for years less than 1000.  

The module uses the OS's POSIX formatting, which means you do not get 0 padded values on Linux, but you do on Mac.  The worst part is that on Linux, doing a strptime() with the same ""%Y"" will give you an error as it requires 0 padding on all systems.

    >>> from datetime import date, datetime
    >>> datetime.strftime(date(1, 1, 1), '%Y')
    '1'
    >>> datetime.strptime(datetime.strftime(date(1, 1, 1), '%Y'), '%Y')
    Traceback (most recent call last):
      File ""<stdin>"", line 1, in <module>
      File ""/usr/local/lib/python3.5/_strptime.py"", line 510, in _strptime_datetime
        tt, fraction = _strptime(data_string, format)
      File ""/usr/local/lib/python3.5/_strptime.py"", line 343, in _strptime
        (data_string, format))
    ValueError: time data '1' does not match format '%Y'

Note: this is only applies to python3.  In 2.7, you get a ValueError trying to strftime any years before 1900.",1520290258.0
eholk,"Another of my favorites is a Linux kernel [vulnerability](https://www.securityfocus.com/bid/36038/discuss) due to a null pointer dereference.

The code rightfully had a null check, but the problem was that the pointer was unconditionally dereferenced earlier in the function. The C standard says that dereferencing a null pointer is undefined behavior, meaning the compiler can do whatever it wants if it detects null pointer dereference. In this case, at the point of the null check, the compiler knew that either the pointer that was dereferenced was not null and therefore we were still in a defined state, or the pointer was null in which case the compiler could do whatever it wanted. This meant the compiler chose to omit the check because if the pointer were null all bets were already off.",1520299376.0
nerga,"Those aren't bugs. Bugs are when things happen that shouldn't according to the specification.

One bug I can think of is Java hashtable had a bug. Well, it didn't specifically have a bug but it was a bug in the algorithm and computers in general I suppose. The hashtable if you are not aware puts values into a bucket (implemented with a linked list) based on the modulus of the size of the array and the hash of the key, using statistics to keep the hash table efficient and stable. But since the real world has hackers, people were able to take out websites and get jvm dumps by attacking hashtables on websites by in putting large numbers of several hundred character long strings that also have the same hashvalue. This Essentially just turns it into a linked list, unbalances the code and crashes. This is why java removed hashtable and implemented hashmap, and python changes the hashing value as it's used. 

Bucketed hash tables work as they should, but hackers can attack it exploiting it's statistics base.",1520291841.0
IJzerbaard,"Ok so not bugs but common issues.

C#:

- control over whether a lambda captures a variable by value or reference is implicit and mysterious. In a loop that contains a lambda, using the loop variable directly has a different result than copying it into an extra local variable and then using that.
- structs cannot have a default ctor (for good reason, but may be surprising at first).
- readonly fields in a struct are a lie, the ""entire thing"" can be overwritten even if the fields are readonly. Yes, you can even assign to `this` (in the `this = thing` sense) and ""overwrite yourself"" if there is a readonly field.
- containers cannot return things by reference, so their indexer has a separate getter and setter. Seems harmless so far, and it mostly is. *Except* when its elements are structs: you can assign to a field of a struct in an array, but assigning to a field of a struct in a container is useless since you're modifying a copy.
- an ulong cannot be negated the regular way (you can subtract it from 0 however).
",1520280102.0
wimcolgate2,"While not a language bug, how about a math conversion bug? http://articles.latimes.com/1999/oct/01/news/mn-17288
",1520292404.0
No-More-Stars,"    java.net.URL:hashCode() => ... => DNSNameService:lookupAllHostAddr(String host)

Every time you call `.equals()` or perform a collection membership check with a collection of URLs, you will perform a DNS query, (a slow operation if not cached). Placing a relatively small number of URLs into a sorted collection class (For example: a TreeMap) will cause minutes to hours worth of avoidable delays.

http://grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/6-b14/java/net/URLStreamHandler.java#URLStreamHandler.hashCode%28java.net.URL%29
",1520297420.0
chx_,"The biggest and most hilarious WTF moments in a big pile is this survey http://www.cl.cam.ac.uk/~pes20/cerberus/notes50-survey-discussion.html about C in practice. It describes various practices and then asks among others ""Do you know of real code that relies on it?"" and the ""Yes"" answers and also the ""no, that would be crazy"" answers *both* have above 10% share (there are five possible answers). :D",1520302920.0
RenaKunisaki,"Somewhat famous bug: the Wii OS and bootROM used `strncmp` instead of `memcmp` to compare the expected and actual (binary) hashes of programs.

It appeared to work fine. Accepts valid signatures and rejects invalid ones. But all you needed to do was brute force a signature whose first byte is zero (a maximum of 256 attempts) to bypass the whole thing.

Then there was the PS3 that used a fixed constant where the code expected a random number, allowing hackers to compute the secret signing keys.",1520308310.0
spyhunter99,"There's a whacky issue in the jre for writing zip files on windows that will lock the file until the vm exits,  unless the stream is wrapped in another steam.  ",1520293680.0
crabbone,"A historical reference: https://en.wikipedia.org/wiki/Therac-25 perhaps the first most famous software bug that actually was directly responsible for human lives.

A recent bug with huge and wide-reaching consequences: https://en.wikipedia.org/wiki/Heartbleed .

Another bug of the same magnitude, but unlikely had been exploited: https://en.wikipedia.org/wiki/Meltdown_(security_vulnerability)

A massive flaw in Ruby's standard library implementation: https://blog.rapid7.com/2013/01/10/exploiting-ruby-on-rails-with-metasploit-cve-2013-0156/ which didn't get a catchy name.

There was a less famous, but also quite devastating bug in some C# software naively using `Math.Abs()` without `try...catch` and as a consequence crashing one of the major stock exchanges, but I cannot find a reference.

Not quite as devastating, but still quite annoying: https://www.connexionfrance.com/French-news/Rail-services-resume-after-computer-glitch

And here are some more oddities: https://www.cs.tau.ac.il/~nachumd/horror.html",1520329339.0
magnificentbop,"The Mars Rover had a serious bug due to priority inversion.  In a nutshell, a low priority task shared access to a bus with a high priority task using mutexes.  The low priority task would get the mutex, be preempted by other tasks, and the high priority task would be unable to complete.  Watchdog timers would notice the missed deadline and reset the system.

[See the full write-up here.](https://www.rapitasystems.com/blog/what-really-happened-to-the-software-on-the-mars-pathfinder-spacecraft)

In general, there have been many famous bugs in space software.",1520360652.0
jmite,"ML was not type safe for a long time because of the interaction between polymorphism and mutable references. This was eventually fixed by putting the ""value restriction"" in place: only values can be polymorphic in a let binding.",1520363694.0
XxZozaxX,why there are no answer ??,1521753427.0
jikki-san,I’m confused; is this a new development? The post on the ACM website is dated 2016.,1520261666.0
cthorrez,Is it cloud based or use react? If not I'm not sure there are enough buzzwords for this to be interesting.,1520265054.0
_foobie,"With an Ethereum based adversarial neural network, you could have a computer that is as smart as a garden slug within 1000 years and only have to pay $3trillion in transaction fees. Fantastic!",1520270357.0
TomvdZ,"From a legal perspective, you can't publish your cleaned-up version without permission.

On the other hand, I don't think anybody would care and you'd be doing the community a service.",1520254521.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1520238925.0
ellisonch,"I would stick with operational, small-step semantics.  It's definitely the easiest style to understand without a mathematics background.  E.g., https://en.wikipedia.org/wiki/Operational_semantics

There are plenty of lecture notes online if you search for ""small step semantics"".",1520252033.0
PM_ME_UR_OBSIDIAN,"Vol. 2 of Software Foundations is a great introduction to programming language semantics for the stubborn self-starter. Though you'd have to work through volume 1 first, so call this the scenic route.

As for denotational semantics: the idea is that, to reason about programs in a not-well-understood programming language, you should map them to well-understood mathematical structures in a way that preserves the properties you care about. Once that's done you can write proofs about the mathematical structure and ""pull them back"" or ""lift them"" into the programming language world, where they should still apply.",1520261300.0
magr95,Did you try Semantics with applications by Nielson and Nielson? I'm currently using it at university and I think it's a good introduction.,1520277027.0
combinatorylogic,"You don't need any complex mathematics for representing *operational* semantics with term rewriting systems. In fact, hardly anything can be more fundamental (and, therefore, free from external dependencies) than a TRS.

I suggest to start with this book: https://www21.in.tum.de/~nipkow/TRaAT/",1520372140.0
Lucretia9,There’s a guy who blogs about it and has an intro for beginners. On my mobile but I think I have a link at home.,1520283388.0
inephable,"They are one such set, yes, and I think you have already explained to yourself why that is.",1520234323.0
,"I am a beginner in this area, and really uneducated about Kernel Security, but maybe I can help out a bit:

First: Keep in mind that we are always talking approximations here: For any giving method, there are always some systems about you can't decide whether whatever you want to prove holds.

-------

> Could Formal Methods be used to show a system is insecure rather than secure and if so would it be trivial to implement ?

Symbolic Execution comes to mind - a method that finds some (but not all) errors in a system.

Trivial: I have no experience. Like every domain, you will need some time to get used to it. The library situation should be good enough for you to only focus on the method itself. (There are good SAT-solvers etc.)

-------

> Has a system like sel4 despite being proven to be secure still fail due to the assumptions made ?

I don't know whether that happened, but it is an entirely plausible scenario. We always verify a model of a program, and wrong assumptions spoil that model.

-------

> What are the significant difficulties of implementing formal methods ?

I think the main challenges for a newcomer will be on the theory side of things. Unfortunately, there is overloaded jargon all around.

-------

> What are some fundamental papers should one read in the area ?

Paper: Don't have one available, but two topics crop up often enough and serve well as introduction in the verification methodologies: ""Hoare Tripples"" and ""Abstract interpretations"" - maybe there are lectures online. I learned what I learned in lecture and by torturing my brain.
",1520548181.0
Infintie_3ntropy,This sounds a lot like university course work... And you have posted an identical set of questions to like 4 other sub-reddits. ,1520244545.0
beefsack,"It's great you're looking towards FOSS projects to broaden your knowledge, high quality FOSS projects are a great learning tool and learning to read code is a very valuable skill.

Go is an interesting language as it's relatively easy to pick up, has good performance characteristics, static builds by default which can make your software quite portable, and a strong community which is a great source for support and libraries.

I've found a good way to discover code to read is to search GitHub by language and sort by stars to find the most popular projects, eg. [for Go](https://github.com/search?langOverride=&o=desc&q=language%3Ago&repo=&s=stars&start_value=1&type=Repositories). Don't pick large scale projects like frameworks though, aim for small sounding libraries or CLI utilities as they tend to be small and neat. [FZF](https://github.com/junegunn/fzf/tree/master/src) might make a good example, and it's also a great piece of software!

As an aside, this mightn't be the best place to ask this sort of question, you might have more luck in /r/programming or /r/AskProgramming.",1520212310.0
blazingkin,"Shameless self-promotion.

[blz](https://github.com/blazingkin/blz-ospl) is a free and open source programming language written in Java. We're always looking for new contributors. It would also be a new language to learn.",1520214691.0
darrenldl,"There are a lot of utilities written in Go, so you will absolutely find interesting projects to contribute to. Usually ""awesome-*"" lists are pretty good as a start, e.g. [awesome-go](https://github.com/avelino/awesome-go), often they are very high quality and highly curated and updated.

Most important advice is probably find something that you actually like and will actually use. If you have no need for something, then it's unlikely you'll work very hard on perfecting it without pay.

Also note that you don't always have to find existing projects to contribute to in order to learn stuff. If you find yourself needing something, then go ahead a make one. Obviously you will learn different things compared to working with other people on existing projects, but the goals you mentioned(e.g. memory management, threading) can be achieved with your own project.

Shoot me a message if you're interested in a run down of different langauge choices and opinions/advice on learning/picking languages.",1520243603.0
umpc,"An interesting and useful Go project that could use some occasional help would be [Tile38](https://github.com/tidwall/tile38), a geospatial database with geofencing.

The list of active developers is very small: usually just the author and whoever else is temporarily interested in improving it for their use-cases.

The author has always been polite and he has always gladly accepted any respectable, in-scope, PRs for adding requested features or fixing bugs.

I used to more-actively contribute. I still usually respond to opinionated protocol issues, often vouching for backwards-compatibility, and typically respond to issues related to its internal R-tree structure, though I am less familiar with some of the other parts.

Edit: Replaced redundant uses of a word.",1520362384.0
OptimisticElectron,Might wanna look at LastTry. It's an open source game to replicate Terraria written in Java.,1520245468.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1520186441.0
notenoughstuff,"What does ""pattern"" in this context mean? I am asking because education quality can vary quite a lot, and I do not quite recognize ""pattern"" despite having an education in CS from a university (bachelor + masters) and being a professional. Is it ""pattern"" as in ""Gang of Four"" pattern (https://en.wikipedia.org/wiki/Software_design_pattern)? If so, that sounds really weird; training in being able to solve basic computational problems using the basic primitives of your languages (like functions and values in functional programming languages and functions, values and statements in imperative programming languages) seem like a much better approach at the beginning of programming. But I am no expert in CS education.

EDIT: Focusing on recursion and iteration, their properties, advantages and drawbacks, and using them and training in using them to solve problems also sounds very right in my ears. But ""patterns""?

EDIT2: Do you have any algorithmic courses? Like how to prove the correctness of an algorithm? Or proving its running time, and describing its running time formally as in O(`n^2`) asymptotic running time?

EDIT3: Projects can give good experience in certain regards, but they can take a very large amount of time and they can be really inefficient in learning certain skills (unless of course you are sneaky and take pauses during the projects to effectively learn subjects that come up as needed, which you can do if it is a hobby project).

EDIT4: Gang of Four patterns are also a bit controversial: it has been some time since I encountered them, but they have multiple problems; they are in my opinion very bad as an approach for problem solving, and are much better used for describing and communicating concepts or solutions that you come up with, but they still have the issue of being somewhat fuzzy, leading to potential misidentification and miscommunication.",1520184182.0
nonsane_matty,"I think you're really missing the point for why coding cute recursive patterns is important, and how it will gradually expand from a simple concept.

Data analytics, especially big data requires a good understanding of algorithms, data structures, bigO, and recursion. I'm not going to dive deeply into why these are important, you can just google it.

Drawing these patterns helps learn how to use recursion; Blindly using recursion can lead to a situation like so: 

    // Calculating the 45th number of the Fibonacci sequence.
    Binary: 1134903170
    Linear: 1134903170
    Tail: 1134903170
    Total execution time (Binary): 6572ms
    Total execution time (Linear): 0ms
    Total execution time (Tail): 0ms

You can see how naively implementing a simple algorithm wasted about 6 seconds; if I had gone on to calculate the 55th number I would have wasted around an hour.

Other reasons why this is an important topic can be seen in LU Decomposition. To keep short LU Decomposition can eat up a lot of memory; especially with big data sets (NxN).
So a trick used to keep memory consumption low is to store both L and U in a single matrix. Then doing your calculation using this single LU matrix is going to require some recursive algorithm; which brings us all the way back to day one of Comp. Sci. printing pyramids made of asterisks to the console... but you'll also need knowledge about things such as data structures, algorithms, bigO, different kinds of recursion, discrete mathematics and so on.",1520193284.0
foreheadteeth,"I'm a math prof and I teach numerical analysis (solving numerical problems with computers) and we program in MATLAB (similar to python).

Your reaction is very typical so you're in ""good company"". I show my students the [cup stacking girl](https://www.youtube.com/watch?v=j54yGxuk0yo) as an example. She didn't learn to stack cups this well by reading books or listening to lectures, and programming, like cup stacking, is a ""doing"" skill, where you learn 90% by doing yourself. You start out slow and build up your skill and speed over a long period of time.

I do teach them what I call ""the basics"" (about 15 minutes of lecture time). Never write any code directly into the editor; always fully debug a single line of code at the prompt and make 100% sure it is 100% correct before copying it into the editor. I also teach them how to manually unroll a loop to see what it actually means.

When we get to the lab, fully one quarter of the class ""freezes"", unable to do anything, and when they put their hand up it is obvious they have never heard of my ""basics"". I sometimes have to sit with one student for 15 minutes while they wonder if they should type something into the prompt. Or, they refuse to engage with the loop rolling-unrolling mental exercises. That right there is the mental block that is preventing them from getting further.

In any case perhaps you are way ahead of all this, the point of this reply is to let you know that if you do want to learn programming, you'll have to do the actual ""training up"" and it won't magically happen on its own.",1520210622.0
chris24680,I'm going to make a neural network that can choose datasets to put into a neural network in order to get the maximum number of 'Someone made a neural network that can XXX' headlines.,1520182247.0
stathibus,"I'm no poet but I think the evocativeness is a bit oversold... ""Sentence gradient"" is a neat idea though.",1520179482.0
crispy_tapud,Looks like colourful randomness..,1520182753.0
muddymindua,"I like this one: “Oh, that’s not the morning, even in Asia”.",1520180336.0
PM_ME_IRL,Doesn't read much better than a good old markov chain text generator,1520254156.0
IkonikK,I an John was -- That's why there was no record of them!,1520182115.0
_foobie,"Programming does not equal computer science. Computer science is for learning the science of computing, which is not programming. Many programmers do not have computer science degrees or ever studied computer science in university.

If you do not wish to study the science of computing, don't feel the need to stick to computer science.",1520134046.0
sachanganesh,"I agree with the other comment only in part. Computer science is very much about the ""science"", but it's also so much more than that too. Unless the school has a data science degree, computer science with a focus on machine learning at the moment may be helpful.

I'm currently a junior in college doing a ML concentration with my degree, and I participate in research of computational genomics (bioinformatics). After having the privilege of taking a graduate bioinformatics course, in my opinion the difference boils down to this:

If you want to create new ways of pushing the science of bioinformatics forward like the tools and techniques, computer science with machine learning will give a strong foundation to understand how data is stored, processed, analyzed, etc. You'll understand at a deeper level what's going on behind popular algorithms and tools. That's what my research involves, I'm working on new ways of classifying the non-coding parts of the genome.

However, if that doesn't interest you, and you'd rather use the existing tools to push biological knowledge forward as best you can, then you may not need a computer science background. If you just want to extract the information out of the DNA sequence, you don't need to know how the program works as long as it does what it's supposed to do.

It's important to note that there may be a benefit to being a biologist first and a computer scientist second; my professor is like that and it's never held him back, in fact he understands the algorithms and methods just like a computer scientist.

If you don't enjoy programming you might be in a tight spot, but perhaps machine learning may pique your interest. Don't force it though, it's not for everyone. The best part about it though is that you can learn ML and programming on your own time and see if it's right for you.

Also I'm not sure how pharmaceutical research works, but in the graduate computational chemistry course I'm taking now, the emphasis seems to be on knowing how to use the tools and not how the tools work. Find your balance of both!",1520159726.0
llambda_of_the_alps,"Another important thing to note is that when it comes to how understanding AI and machine learning work having knowledge of statistics and linear algebra is often as important as the compsci aspects.

One of the problems with studying 'computer science' at a community college level is that it is mostly just programming at that point. You're really not learning any of the theory yet.",1520610833.0
ldpreload,"> I didn't know it a the time, but my friend Edsger Dijkstra thought the null reference was a bad idea. He said: ""If you have a null reference, then every bachelor who you represent in your object structure will seem to be married polyamorously to the same person Null"".

> I did know there was a solution based on the idea of discrimination of objects belong to a disjoint union class; that is, two sets in which there are no members in common. For example a Vehicle class that has subtypes Car and Bus; the Car may have a luggage carrying capacity property while the Bus has a person carrying capacity. You would then have a discrimination test and do different operations based on whether it was a Bus or a Car. ... This allows null to be represented as a different class ... The types of the pointer could then be implemented as a union of either a pointer to the null type, or a pointer to the type.

> This led me to suggest that the null value is a member of every type

which was the mistake. It's easy for us to call it a mistake with the benefit of hindsight, but representing ""not initialized yet"" as a value of the same type was appealing and easy within those programming paradigms, which didn't support disjoint union classes.

Null _is not a pointer_. It doesn't point to anything. It doesn't support any of the operations that a pointer supports. You can't read through it, write through it, or free it. If you have a function like C's `malloc`, there are two possible things it can return. It can return a pointer to some data, or it can return null. It happens to be the case in C, and Algol, and similar languages that null is represented in memory the way a pointer with value 0 would be represented, by making a rule that nothing will actually be stored at 0 and so there's no need to point to it. But that just means there are two disjoint possible return values: 0, for null, and nonzero, for an actual usable pointer.

The ""disjoint union class"" referenced above is a kind of data type that's been popular under the name of ""sum type,"" mostly in languages of a completely different family: ML, Haskell, Scala (where they're called ""case classes""), etc. But they've shown up in Rust and Swift, and C++17 has `std::variant` that is basically the same idea: you have a data type which is combined of an _option_ between one or more other data types. In a hypothetical C++ that had native sum types, you'd write the Bus and Car example something like

    class Car { int luggageCapacity(); };
    class Bus { int personCapacity(); };
    class Vehicle = Car + Bus;
    
    void printCapacity(Vehicle v) {
        match (v) {
            case Car c:
                cout << ""This car can fit "" << c.luggageCapacity() << "" suitcases"" << endl;
                break;
            case Bus b:
                cout << ""This bus can fit "" << b.personCapacity() << "" people"" << endl;
                break;
        }
    }

The important thing is that you _can't_ call `v.luggageCapacity()`; you have to ask the language to make sure that your `Vehicle` is actually a `Car`. (The plus sign in my hypothetical syntax indicates that the set of possible vehicles is the set of possible cars plus the set of possible buses; ML and Haskell tend to prefer `|`, the ""or"" operator, to indicate that a vehicle is either a car or a bus.)

This pattern just doesn't exist in the Algol / C family of languages. So the person writing libraries in C would never think of the following sort of pattern, because it can't be expressed in C:

    class Null {};
    class NullablePointer = (char *) + Null;
    
    NullablePointer malloc(int size);
    
    int main() {
        NullablePointer p = malloc(1000);
        if match (char *addr = p) {
            *addr = 'x';
        } else {
            /* you can't say *p here! */
        }
    }

In this version of C, a `char *` is, necessarily, a valid pointer. You can never accidentally dereference Null, because there's no way to write that, just like there's no way to dereference 0.5. (You could forcibly cast either Null or 0.5 to a pointer type, sure, but you can't dereference a Null or a float while it's still a Null or a float.) And to take Dijkstra's example, someone who is unmarried is _not married_; they're not married to Null.

The fact that Rust and Swift support this approach to null makes them far, far safer / more reliable than C, and more pleasant to code in, even if they had no other language features beyond C. If you're interested in seeing how sum types and this approach to null look in practice, the [enum chapter of the Rust book](https://doc.rust-lang.org/book/second-edition/ch06-01-defining-an-enum.html) (which, full disclosure, I wrote an early version of) has a bunch of examples.",1520119164.0
lrem,"Yes and no.

I've done a somewhat traditional, very broad and selectively deep masters in ""informatics"". This gave me understanding of both maths and how CPUs work. Then went into a PhD in combinatorial optimisation. This gave me understanding how to express nearly anything as a mixed integer linear program. Then I landed an industry dream job. This made me work on a legacy code base for copying jillions of files from one place to another.

I could have done the same job without all the education. Some would call that a waste at that point. Industry needs simple things done and typical management would like fancy theory contained in a small R&D unit.

But then, just start looking around. Wait, how much data are we storing here? Look, a nontrivial expected number of hash collisions! A production-only heisenbug haunting the system for years solved. Why is this limited to just a few doohickeys? Right, I know an algorithm that scales better. Damn, a few years in I even got to solve a major problem space with MILP, surprising everyone with how intuitive and elegant alternative to the usual mess of heuristics this can be.

CS knowledge is not really needed to land *a* job. But in the right job, it will make it way easier to go above and beyond, with all the associated benefits.",1520099339.0
tical0,"I've been programming for around 15 years and doing it professionally for close to 12. I have seen standards change drastically, programming languages I used professionally go obsolete, and entire industries grow around budding tech. Everything will change constantly for nearly every industry. That's why it's your responsibility to constantly grow and adapt until you find a niche that isn't going anywhere and that you can use to multiply your salary figure. One of the reasons why software pays so well is because burn out and obsolescence are always looming and you will need to be smart about investing and maneuvering. ",1520100281.0
Chandon,"There are different layers of knowledge. 

In BASIC for the TRS-80, you mark an integer variable by appending a % character to the variable name. This is level 1 knowledge - it's very specific to a particular system. It's also been obsolete for 30 years.

Programming languages in general track the types data stored in variables. Historically, some languages made this information part of the variable name, but that's rarely done in modern languages. This is level 2 knowledge. It makes learning new level 1 facts easier, and will take much longer to become obsolete. Having level 1 knowledge, even if obsolete, can enhance level 2 knowledge by providing context and examples.

Seeing things change provides better understanding of them. ",1520101150.0
,[deleted],1520098067.0
funk_transcender,"I know this article is on a slightly different subject, but I think there's a lot of overlap. I really recommend reading it. It really transformed my mindset. I think you shouldn't see knowledge as just having a bunch of memories of specific facts - it goes way deeper than that.

https://medium.com/the-polymath-project/its-okay-to-forget-what-you-read-f4ef1c34cc01",1520100482.0
combinatorylogic,"I used to study hardware design on discrete TTL and core memory. Still using *all* the knowledge, it's still applicable in an era of FPGAs and 10nm lithography.",1520372342.0
,[deleted],1520099360.0
alexgmcm,"> I hope to be able to get a decent research engineer job after graduating

It's hard to get a research position without a PhD - many of my colleagues have PhD's (I mastered out) and they still do typical data science work rather than research-level stuff.

",1520100960.0
Hollowprime,"It's not about meta questions,it's about the hardest part of mathematics: Statistical analysis of huge amounts of data. That's what AI specializes nowdays and it's nothing philosophical about it. I suggest you read this book:
https://www.google.gr/search?q=artificial+intelligenc+norvig&oq=artificial+intelligenc+norvig&aqs=chrome..69i57.4855j0j7&sourceid=chrome&ie=UTF-8

If you don't like the way it interprets the basic artificial intelligence algorithms you're probably better off this major.",1520103938.0
Screye,"OP, you should check out ""Computation social science"". It is a subfield of AI/ML which focuses on modelling people, cause-effect relations and even moulding public policy. You experience in Economics and Philosophy would not be wasted there.

Imperial college London is really good for CS is general, so in terms of being at a top institute I think you are good.",1520104670.0
onionKnightKreggle,"A lot of the questions you’re asking are meta questions about the field of AI. AI currently presupposes a model so that we can apply math to achieve tangible results. I feel like you might just want to pursue a phd in philosophy somewhere that offers interdisciplinary research. Most of AI is applied math. You should seriously consider taking some courses in computer science while you’re in undergrad and have the chance to get a feel for it. You might only spend 0.05% of your degree actually contemplating these questions that are driving you towards the subject. 

I don’t really know the details of your school but you seem to get to take some decent courses. My only concern is that there isn’t any real data structures and algorithms courses which is foundational to any of those advanced courses. ",1520101913.0
kupo1729,"You might want to look into Computational Economics, Algorithmic Game Theory, Computational Social Choice, Decision Theory, and Multi-Agent Systems.",1520136850.0
gedge,"I don't know much about radio tomography, but just wanted to comment on one thing you mentioned:

>Some that I’ve seen identified is that with vision-based mapping objects need to be in the line of sight and must have good lighting for it to be identified

Good lighting is helpful for passive techniques, but high contrast is perhaps the most useful. Active techniques, however, project information into the scene, so both lighting and contrast become less of a concern since you're now in control of both.
",1520099672.0
foreheadteeth,"I answered your question about how many samples you need but the bad news is, it's likely to be too many samples in high dimensions.",1520025479.0
eveninghighlight,Monte Carlo?,1520038859.0
IJzerbaard,"> For each one of the tests I run the verilog, the emulator, then I diff the reg file, memory, and the instruction logs for both. I only pass the test when they all match.

How about the other microarchitectural state, in the OoO versions? Eg I had a hell of a time debugging leaked physical registers (I accidentally allocated 2 for some instructions that had 1 output, so when retired only 1 tag would make it into the RRF so only 1 tag would later be freed)..",1519995943.0
combinatorylogic,"Nice! But why using x86 ISA as an intermediate encoding, instead of generating your RISC ISA directly? Are there any advantages in this approach?",1520006252.0
phonard,"Awesome work, thanks for sharing!",1520051751.0
robsmi1974nyc,"I respect you, but you should just contribute to RISC-V instead :-)",1520047969.0
,[deleted],1519997047.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1519991206.0
rabinabo,LaTeX with tikz.,1519968545.0
bastelmaster,"Tikz is probably the most powerful tool. But using tikz for graphics can also cost a lot of time you might not have when the deadline comes close. [Draw.io](https://www.draw.io/) is a nice online editor, which provides a lot of useful shapes. Another graphical tool that is worth a look is [Dia](https://wiki.gnome.org/Apps/Dia).",1519982669.0
zach_does_math,"Usually I'll use Geogebra to draw the thing I want, then export it as  TikZ, then make adjustments directly in the TikZ code.",1519997015.0
LgDog,"As /u/rabinado mentioned, LaTaX + TikZ. Take a look at their [examples](http://www.texample.net/tikz/examples/tag/graphs/). ",1519969790.0
crabbone,"LaTeX with tikz for simple hand-coded stuff. But also take a look at https://github.com/UoYCS-plasma/GP2-editor . The language itself is very interesting, but that aside, the editor for the language lets you represent it as a graph. It uses https://github.com/ogdf/ogdf for layout of graphs, which is a very smart library.",1519979104.0
ggchappell,"I mostly use asymptote (with LaTeX). It's a programming language with a C-like syntax, for drawing stuff.",1520026605.0
gentzen,"There was a recent [meta-thread on cs.se about graph editors](https://cs.meta.stackexchange.com/questions/1476/integration-of-an-interactive-graph-editor), in which various tools came up, among others yEd, ipe, and TikZ. I personally only worked with yEd so far.",1520177955.0
fnybny,"I don't suggest using tikz (or tikzcd) for commutative diagrams because of the insane resources that it takes to compile.  I would suggest xymatrix, but the syntax is a little terse...


Edit:  also for more general diagrams, tikzit is a good wysiwyg editor.",1519975862.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1519965455.0
ChemicalRascal,"There's obviously GeoNames, which is basically point data -- if a location has a name, it's in there with the lat/long.

I remember there's a dataset from Microsoft called GeoLife or something that might also be relevant, from a study of roughly a hundred people as they moved around Beijing for a year -- so they're trajectories, location+ time, with a large amount of clustering.

Ultimately, though, it kind of depends on what you're trying to show, right? What are you planning on using the dataset for?",1519967989.0
NowImAllSet,"This is likely a stupid suggestion, but have you looked through Google's Big Query datasets? I feel like I saw something related to geotagging, but can't be certain. ",1520019857.0
ThomasAger,"Geonames, Foursquare, OpenCYC",1520035165.0
KatsuCurryCutlet,"As an undergrad who's very interested in one day doing research in complexity theory (I have not decided on which sub-field yet, but right now it seems like structural complexity) , I fear I don't have the math background required to do well, which leads me to be afraid I wouldn't be able to convince any professors to take me on as a supervisor when I eventually apply for a postgraduate course.

At the current university that I am at, the basic math courses offered to a CS major are: Linear Algebra I, Calculus, Discrete Math, probability, and statistics. As for the relevant CS courses, I have covered 2 courses on basic algorithms, one on an introduction to the theory of computation, and am currently in a course of computational complexity theory. Also, I plan on taking advanced algorithms courses and automata in the future. However, I realise that probably a more pure math background is required. In fact sometimes I regret not having gone for a pure math undergraduate instead of a CS one. 

So I plan on taking basic set theory, algebra, and formal systems/logic courses from the math department in the future.  

I was wondering is there anything else I should be covering or focusing on to prepare for a possible postgraduate in complexity theory? Maybe good survey papers or closely related fields that would provide better insight? 

Any help would be really appreciated! ",1520127116.0
,[deleted],1519961394.0
sailorcire,"It isn't that hard, but you shouldn't use this in practice.

Suppose I have a NxM matrix and I need the data at an arbitrary location. This algorithm has to do a look up, verify if valid, if not initialize, and update for every single access.

This must be done for every cell used in the program.

Instead, suppose I initialize the cells first. Then no matter how many times I access the data, I don't have to perform any additional operations.

The solution proved is fine for sparse access, but for repeated access, the latter will perform better.

Edit: the original solution *can* also use more memory than initializing at first. If all cells are occupied (or statically declared) then there is wasted space with your lookup table.",1519929619.0
SteeleDynamics,"The solution is also in Programming Pearls 2nd ed by Jon Bentley. (Column 11, Exercise 9, contains the answer on page 207)",1519959131.0
vlykarye,"tl;dr cool memory ""trick"" that only works for niche cases and would otherwise defeat itself if used for cases outside this niche

is my understanding correct?",1519942723.0
xiroV,"I think I've mentioned this before, but I think some of the concepts I've been using the most is vectors, matrices and algebra. If you understand the basic concepts in these areas, and you have a good sense of logic, I think you should be fine. Also maybe read up on discrete mathematics if you want to improve your logical reasoning. Some people might mention calculus, but to be honest I've used very little from that mathematical area.

Btw. I'm from Denmark (currently studying in Finland), working on my master's degree, so it might not be completely the same for the UK. ",1519915710.0
lubutu,"Speaking as someone from the UK who studied CS without a Maths A Level, and then went on to do a PhD in theoretical computer science (lambda calculus and the like)... You should be fine.

As others have said, you may want to brush up on discrete maths — sets, functions, graphs, matrices, logic, etc — but I found I had enough time to do that during my course anyway.",1519919311.0
,"Basically just discrete math, which includes:
- logic
- set theory 
- graph theory
- algorithms
- functions 
- proofs

But to be completely honest, you don’t need to master all of the math before going into CS. You’ll be learning a lot if the math (and applying it directly) as you learn CS. However, a good understanding beforehand might help.",1519916043.0
ThomasAger,"You should be able to read scientific notation, aka mathematical notation.  Discrete mathematics, probability, and as somebody else said, vectors, matrices and algebra are also important. I assume all of these subjects will be a part of your study.

>I am now very keen to establish a mathematical foundation before uni so that I am not at as much of a disadvantage when I begin my course.

It's probably worth knowing there will be plenty of people worse-off than you. This is giving you an advantage, not necessarily keeping you up to speed. That said, it's likely that if you want to make a real impact on your own understanding, or in the field, being one of the people who are at an advantage is important.",1519916160.0
NamerNotLiteral,"I did A-Levels as well, though International, not UK. For me, P1-3 covered Uni-level Calculus (for most of my first year), while I only did S1 and M1, which did cover the basics of vectors and matrices. I don't think none of what I learned in A-levels specifically are important for CSci, just the basic concepts.

That said, you should have the basics you need from O-Levels as well. I learned actual Linear Algebra and Discrete Maths only after I got into Uni.",1519919354.0
,"Although it shouldn't be neccesary to prepare  beforehand, it will most certainly make it easier. I'd honestly suggest you use one of the American calculus books to self study
Calculus. 

I say this because they're a good way for people with basic math knowledge to build up their mathematical maturity quite fast. Mathematical maturity is more important than the knowledge in any particular topic. As for why use the American books? They're on their like 20th edition of something ridiculous. For people who already know calculus they're about 6 times thicker than need be, but for people who self study the spoonfeed-level explanations are perfect.

If by some chance you've already had a rigorous introduction to math. Meaning you have seen proof for every new concept introduced then the lack of rigour might bore you to death.",1519921786.0
__horned_owl__,"Okay, I'm a first year CS major and I'll try to give you a good list of the topics which are pretty much essential.
1) Calculus - limits are very very important, derivatives, Taylor series and integrals can be encountered also. Sums and power series are also very common. You might want to familiarize yourself with generating functions. Recurrence relations are extremely common and you need to know how to solve the linear cases and how to analyze the more complicated ones.
2) Linear Algebra - matrix operations are common so learn at least the basics. 
3) Algebra - basic algebra, numerically solving equations, approximations, coefficient extraction etc. etc.
4) Discrete math - fundamentals of logic theory, graph theory, combinatorics, probability. 
5) This may be strange to hear but statistics can help if you're analyzing randomized algorithms or you're working with data.

More complex things like differential equations do pop up here and there but if you know at least one mathematical software like Matlab this shouldn't be a problem.",1519931242.0
uint64,"Discrete maths and linear algebra. Most comp sci courses will cover what you need to know and it's simple enough if you pay attention to the lectures, even without A Level maths, but it's good to have a head start.",1519933583.0
hamtaroismyhomie,"If your coursework requires calculus, make sure you are solid in algebra and trig.  Use Paul's online math notes to review. It will make it so much less painful.",1519934531.0
fear_the_future,"you need some basic understanding of calculus, logic and linear algebra. Beyond that, your algebra needs to be top notch",1519929058.0
johnyboy6000,"I left school at 16 with rubbish gcses never mind a levels.  Now I have a first class masters in a good uni and have been a dev for 5 years. It was a bit stressful knowing less than nothing at first but you soon pick it up :). Have fun! 
",1519937747.0
Dextertheprogrammer,I think I've got a pretty good idea of what I should be looking towards. Thanks for all the advice guys. I appreciate ya,1519939798.0
singsong101,Literally in the same boat as you,1519944890.0
chrisname,"They will teach you all the maths you need, which is logic, linear algebra, calculus, trig and a bit of set theory - most of which is pretty easy. If you go into machine learning (post-grad) you will also do dynamical systems and non-linear dynamics which I'm finding pretty tough, but a solid foundation with linalg and calc will make it easier.

Most unis require you to have a numerical A-level like maths, physics, economics or chemistry but you might get away without it, or have to do a foundation year. Try to have some mathsy portfolio work you can link to in your personal statement.",1519947901.0
green_meklar,"Do you know basic algebra? Do you understand power laws and logarithms?

Because that's like 99% of it right there. The more advanced math hardly ever shows up in CS, unless you're doing graphics or physics simulation stuff. It's more important to just be comfortable with algebra and powers/logs.",1519953285.0
imcostaaa,"Hey! I’m currently studying computer science at western in Ontario Canada, the math focus wasn’t mandatory aside from a calc and Lin alg first year course. After that it turns more into discrete math along with logic and taking any other maths are pretty much up to your discretion. Try accumulating some knowledge in calc and lin alg basics and from there it should be smooth sailing(doesn’t truly exist in comp sci) if you like what your doing and are semi-intelligent/work hard :) ",1519970028.0
,[deleted],1519927534.0
anitakirkovska,"Wow, we needed this research to happen. Nice job HackerRank. On the same topic, we built an infographic ( https://adevait.com/state-of-women-in-tech/) that represents the status of women in tech in 2018, and we will definitely use some of your data to improve it. Thanks!",1520332522.0
daisyr317,"I'm confused as to why there isn't a mid-level employment level on the HackRank tech report. I rarely see a jump from junior to senior, maybe it was to show the infographic contrast?",1522352078.0
ash663,"You could go ahead with any controller compatible with OpenFlow. If you like Python, I highly recommend Pox controller! Plays well with mininet.",1523841873.0
338388,"> I really like maths 

Then you should go for it. University doesn't exist solely to help you get a job, it also exists for you to pursue things you're interested int",1519894501.0
inephable,"An additional degree in anything can never hurt you. More education and intellectual curiosity always gives you an edge in life, not just the workforce.",1519886726.0
,It depends on the industry you're interested in. Computer vision/graphics or finance would both very much like to see a mathematics degree.,1519883948.0
jacksonmills,"I got a Minor in math. I would say it’s been very useful, both in direct application (particularly linear systems and Vector mathematics), and in developing  abstract thinking skills which help with data modeling and refactoring code.

You also get a bigger toolbox to work from, although a lot is specialized. I used cosine similarity once to do a really rough recommendation engine and it blew a clients’ mind.",1519912174.0
OptimisticElectron,"I'm interested to know as well. What if I want to pursue PhD, specifically specialising in AI. Is having maths degree worth it?",1519886738.0
HellAintHalfFull,"I've had a 25-year career in CS, in pretty math-y fields, in America.

I doubt the math degree will benefit you in a financial sense, especially taking into account the year delay on getting into the workforce. Also not sure what this extra year will cost you (if anything).

That said, it may well lead to a career working on more interesting problems than without it.

My take: If you love math, and if the extra year won't cost you too much, go for it. It certainly won't hurt you, and may pay off in ability to do interesting work, if not actually financially.",1519915654.0
insomeloop,"Most employers would look pretty favourably on a Maths degree, especially if in addition to your main subject - and even if not directly relevant to your job. It will show them you should have great capacity for logical thinking - as well as hard graft if doing it as extra.

EDIT: Tbf my only experience is in finance, but I can’t see how an employer wouldn’t be impressed by someone with an extra maths degree. Such a cv would stand out and I’d want to proceed to interview to see if they are also a good personality fit.",1519890664.0
jake_morrison,"I think it's worth it, particularly as we ride the machine learning wave. In normal times a math degree all by itself signals ""smart but a bit impractical"", but a minor/dual degree signals ""extra smart"". 
See also http://steve-yegge.blogspot.tw/2006/03/math-for-programmers.html",1519897114.0
derive_and_thrive,"I picked up a maths degree towards the end of my CS studies directly after taking my first course in A.I. Considering that maths is the backing for CS, it can only help! It definitely gave me a better understanding and way of thinking about things in unique ways, and courses such as linear algebra, number theory, abstract algebra, and combinatorics really help in a lot of fields within CS. 

Most of all, LA is almost required if you want to conduct research in A.I. and is phenomenally useful for graphics work. Algebra is a frequent companion in more theoretical applications, and number theory is the direct foundation of cryptography. 

Honestly, aside from all that, the best part of all is that it was genuinely **fun**. Hard, but fun. I can not recommend it enough!",1519914630.0
bblackshaw,"Yes, it is useful. I got a degree in mathematics first and then a second in CS. The maths was useful for complexity analysis and relational algebra in the CS degree, and in the workforce, while not overly useful, has helped me a lot a various times. For example, once I had to write a library in C++ that does various map projections, which uses a lot of maths. Another project was a population simulation which needed some maths. My statistics knowledge has proven to be useful at times too.

Besides, maths (to me) is just so enjoyable to study :)",1519891093.0
Captain_Flashheart,"It's only an extra year but the workload won't stay the same because you might need to take more classes at the same time, even just because classes are usually given just once a year. 

Does it hurt? Of course not. But consider that if you just want to get out there the CS degree is enough - especially since by then you'll have a small portfolio and possibly pet projects to show off. ",1519903671.0
unpythonic,"I've got a double major in math & comp sci. On occasion I think if I had it to do differently I might have changed my CS degree to Comp. Eng. or something, but I've never had second thoughts about my math degree. Having a solid background in formal mathematics has helped me surprisingly often.",1519919533.0
fhayde,"I've been in tech for a long time. When I started, the academic options were extremely limited so I don't have a degree in anything tech related. Getting a degree in math is probably the one thing I would change about the direction of my career over the years. I work with machine learning and NNs now and teaching yourself some of the math involved isn't easy or fun at times. Granted, it's not necessary to use much of the technology, but if you want to understand what's going on, or contribute in any way, understanding the mathematics behind everything helps tremendously. ",1519920672.0
fajitagod,"The math degree would certainly add another skillset. As a statistician with skills in both mathematics and computer science, people view me as adequately equipped for both analyst roles, as well as technical computing roles. Though I may not be as skilled as one or the other, the overlap certainly makes me marketable to companies. ",1519925019.0
nablachez,"Doing my masters right now in cs and wish i did more math. I'm even considering doing a bachelor again because  the importance of math will only increase the deeper you go into research. I feel like math students have a big advantage over pure cs students because cs curricula is often  not rigorous enough in math. 

Inb4 but my school taught us x!!!",1519925416.0
Gusfoo,"> Is an additional degree in Maths useful?

Are you interested in maths-heavy fields? For example big data or hedge fund ops. It would certainly be useful for those two. ",1519927506.0
Peter-Campora,"If you're considering grad school, then the extra math can definitely pay off. However what courses you should take depend on the field. Linear Algebra and statistics are the most broadly used subjects across many areas of CS. For programming language theory, I'd recommend abstract algebra and topology. If the program has an undergraduate course in mathematical logic, set theory, or category theory then those are even more useful in PLT.",1519927784.0
BrightLord_Wyn,">Even though I really like maths I want to work in the computer science field.

This does not tell us what you actually want to do. Do you want to write algorithms? Do you want to be a programmer? Do you want to start a consulting firm? What do you want to do? ",1519928314.0
mdempsky,"I got dual degrees in CS and Math in undergrad.

I work mostly in systems engineering (e.g., security and compilers). I've found my math background to be very helpful, but mostly because I think it helps me recognize and apply patterns. A lot of upper level math classes are about learning elementary laws/axioms/etc in various fields, and then figuring out how to compose them into tractable proofs. This helps a lot in coming up with maintainable and easy to understand code.

I've dabbled in web frontend programming and other more ""business logic""-y roles, and don't think it's been very applicable there at all.",1519935548.0
Dr_Geoff_Fairchild,"I double-majored in math and CS in undergrad, and it was tremendously helpful getting through the tougher Ph.D.-level courses. I was also told explicitly that the math degree was a little cherry on top when getting hired at my current job (scientist at a national lab). Of course, it helps that I'm functionally a data scientist doing a lot of mathematical modeling.",1519946591.0
Neebur,"It's certainly not required. If you want to work in software development or similar field, I would work on your portfolio and apply for jobs. An additional degree won't get you further up the career ladder or to higher paying jobs, unless it's possibly a masters degree.

If you do it, it will show that you've at least been working towards something during that extra year and getting the degree will give you more options like jobs with mathematics.

I can't say from an employers view whether or not they would care because I don't have experience of that, but either way you'll manage to get a job as long as you understand what you'll be working for.

Good luck! ",1519881446.0
FudsterWong,In the end it All adds up 🤯,1519905167.0
Python4fun,"Depending on how far along you are, and your schools programs, you may consider computer engineering.  ",1519917115.0
onway444,You could refactor this joke,1519877100.0
Peter-Campora,The Little Schemer is up there.,1519872457.0
arkrish,"Design of the Unix Operating System by Maurice J. Bach

Talks concisely about the fundamental structures of Unix. Listed among the most important CS publications (https://en.m.wikipedia.org/wiki/List_of_important_publications_in_computer_science)",1519884215.0
WhackAMoleE,"K&R. The world's thinnest language book. They never say anything twice. Once you've worked through it, you've learned something.",1519852179.0
GNULinuxProgrammer,"[Algorithms](https://www.amazon.com/Algorithms-Sanjoy-Dasgupta/dp/0073523402) by Dasgupta, Papadimitrou, Vazirani. This is the most concise algorithms book I've read. It is very to-the-point and has a good collection of algorithms with their proofs.",1519891782.0
xux-xux,"Ingo Wegener: [Theoretische Informatik: ― eine algorithmenorientierte Einführung ](https://www.amazon.com/Theoretische-Informatik-algorithmenorientierte-Einf%C3%BChrung-XLeitf%C3%A4den/dp/3835100335) (German).

Incredibly dense work. Forces you to think through the topics presented. Definitely makes the lessons stick.

Great book for anyone interested in the theory of CS (Turing machines, Church-Turing thesis, P/NP,  Chomsky Hierarchy,  ...).",1519896940.0
sweml,[Clean Code: A Handbook of Agile Software Craftsmanship](https://www.amazon.com/Clean-Code-Handbook-Software-Craftsmanship/dp/0132350882) it explains how to write elegant code in a simple and clear manner,1519851979.0
bstamour,*Term rewriting and all that* is the most jam packed yet concise book I've ever laid eyes on. One day I'll finally get through it.,1519877178.0
MmmCurry,"Among theory books, my favorite for return per word read is Sipser's *Introduction to the Theory of Computation*. Most people just call it ""Sipser.""

It deals with formal languages, automata, and computability/complexity. It's not only clearly and tersely written but also a physically tiny book that covers a spectacular amount of ground. Mostly proof-based, and the exercises are not easy, but it might be the most rewarding text I got through as an undergraduate.

https://www.goodreads.com/book/show/400716.Introduction_to_the_Theory_of_Computation",1519903940.0
bradrlaw,"Mythical Man Month.  It is a must read in my opinion for anyone in software development / engineering.

https://en.wikipedia.org/wiki/The_Mythical_Man-Month
",1519880775.0
rbbr12,For beginners Computational Fairytale was really good for me and it’s very short.,1519863728.0
pastermil,"""Operating Systems: Three Easy Pieces"" by Remzi H. Arpaci-Dusseau and Andrea C. Arpaci-Dusseau

This is the book I was using for undergrad operating system course. They got it up for free on their site. Great explanation on how things work. Light and understandable language. Also great sense of humor.

Link: http://pages.cs.wisc.edu/~remzi/OSTEP/",1519883028.0
nonconvergent,"Since others have pushed the window with development rather than CS books, I like Apprenticeship Patterns. It's a ""software as a craft"" book. Quick read. Mostly common sense patterns for early problems with solutions for what to do when you don't know what to do.",1519915830.0
vangrif,"For me it was Data Structures and Algorithms by Aho, Lam, Sethi, and Ullman. It's 427 pages long, but at least a third of that is homework problems. The material itself speeds by, and even though the examples are in Pascal it holds up remarkably well",1519878289.0
dvogel,Michael Abrash's Graphics Programming Black Book. It was over 1k pages IIRC but at least 3/4 of that was ASM listings. His actual explanations and stories were short and to the point while still being complete. Somehow I remember everything except the graphics programming lessons from that book.,1519884893.0
welshfargo,[Programming Pearls](https://www.amazon.com/Programming-Pearls-2nd-Jon-Bentley/dp/0201657880/ref=sr_1_1?s=books&ie=UTF8&qid=1519937974&sr=1-1),1519938000.0
crabbone,"J primer. I think, it's written with an eye for people who don't like long words: http://www.jsoftware.com/primer/contents.htm . It's a very interesting language, and, if you are looking for something very concise, you'll find a lot of that stuff in it.

CS intro style books: PLAI by Krishnamurthi. It's about 200 pages long, and tells it like it is.",1519979719.0
74AC153,"Data structures and network algorithms by Tarjan

https://www.goodreads.com/book/show/1416941.Data_Structures_and_Network_Algorithms

",1519891392.0
hnerixh,"[Competetive Programming](https://cpbook.net/)

Let's just say it starts with

    typedef long long             ll;
    typedef vector<int>           vi;
    typedef pair<int, int>        ii;
    typedef vector<ii>            vii;
    typedef set<int>              si;
    typedef map<string, int>      msi;",1519933166.0
CappaGino,"You would typically use either inline or ideally internal styling to achieve this.

You would directly place PHP code as values for CSS properties. For instance internal styling which you'd put in the head of your document would look something like this:

    <head>
      <style>
         body {
             background-color: #<?php echo $some_hex_value ?>;
         }
      </style>
    </head>

or in the case of inline:

    <p style=""font-size: <?php echo $some_font_size ?>"">Some text</p>

Ideally, though you'd want to limit the mix and matching of PHP and CSS and delegate as much of the presentation aspect of your application to CSS (php styling can get really icky quicky). Try to opt for classes where you can even if it means having to perform if and else statements in order to set classes based on some condition.",1519848260.0
TKAAZ,/r/PHPhelp/,1519852075.0
promach,"1) Why is received DLLPs being redirected to 'TLP retry buffer' ? 

2) Why is the output of 'TLP error check' redirected to transmitting 'DLLPs' ?",1519837585.0
fuzzynyanko,I heard PCI Express tends to be extremely fault-tolerant. ,1519856504.0
ilikegamesandstuff,"If you want to do this properly:

You'll need to use the [Google Sheets API](https://developers.google.com/sheets/api/). Sadly they don't seem to offer any official C++ support, but there's probably some libraries around if you google it. Alternatively you can use something like libcurl and program the requests directly.

I access real time data in gsheets all the time, but I use python with this handy module called [pygsheets](http://pygsheets.readthedocs.io/en/latest/), so if you can get around the ""C++ project"" limitation, this is the easiest way that I know of.

Edit: Simpler workaround:

Open your sheet, go to File -> Download as -> Comma separated values (csv) to download the file, then copy the download URL (from the browser downloads list) and make your project download that file every time before reading. This is probably really shitty since you download the entire spreadsheet instead of just the data you're trying to read. And I think you'll need to make your sheet public (ie: anyone with link can access).",1519836814.0
rban123,well you could just right some code to re-read the data file every 10 seconds or so. And the output will update every time 10 seconds when your program goes through all your code again. ,1519834901.0
bremby,"Machine learning is a niche area? IoT is a niche area? I can't ask ""since when"", because the answer is ""already aren't"".

Reversible computing is a niche area. The concept is not new, but recent progress is.",1519822309.0
frezik,"IMHO, there's a whole subfield waiting to be opened up on experimental techniques on programming language design and productivity. For instance, a paper by ES Roberts in 1995 cites an experimental study around the loop-and-a-half problem, and whether it's OK to break with the purity of Structured Programming in this case:

> The best known study of this form was conducted by Elliot Soloway, Jeffrey Bonar, and Kate Ehrlich at Yale University in the early 1980s.  In their experiments, student programmers with varying levels of experience were divided into experimental and control groups.  The students in each group were then asked to write a program to average a list of integers terminated by a sentinel value. Those in the control group were instructed to solve the problem using Standard Pascal.  Those in the experimental group were asked to use a variant of Pascal (Pascal L) in which the standard looping statements while and repeat were replaced by a new loop construct having [a form like C's break]

...

> Subjects overwhelmingly preferred a read/process strategy over a process/read strategy.  When writing a simple looping program, those using the loop . . . leave . . . again construct were more often correct than were those using the standard Pascal loop constructs. 

And so we have things like `break` in nearly every programming language. Even Pascal variants have given up the fight.

Despite the study above going back to the 1980s, this kind of experimental testing is the exception, not the rule. You'll often hear things like ""strong typing is the correct way to do things"" without any data backing it up whatsoever. Our opinions in programming are often founded on nothing but personal experience.",1519831736.0
the-wumpus,you can always give computational complexity a try. ,1519878298.0
SmokingGundam,Some sort of specific algorithmic design honestly. Something like Geometric approximation Algorithms is real niche. ,1519881756.0
rPrankBro,GPU programming using c++ and things like CUDA is a fairly niche thing that will be becoming more popular.,1519884454.0
tulip_bro,"Program Analysis. I knew of the general idea and field from under grad and working as a software engineer, but learning about it in graduate school has helped me label this body of work and appreciate the research in the area.

I believe the field is fascinating as it is an attempt to deal with the [Halting problem](https://en.wikipedia.org/wiki/Halting_problem).

I might add that because of the Halting problem, we have a industries and research in compiler analysis, testing, and verification. 

I am just getting my feet wet in this niche area, but I am thoroughly enjoying the mixture of theory and practicality of what I'm learning.",1520115938.0
nablachez,"I think quantum computing. Machine learning and iot/big data are hyped right now tho, they def aren't niche.",1519845580.0
ImaginationGeek,"Anything work with data, such as Data Science and Big Data (and ML, but you already mentioned that).

Cloud Computing

Cybersecurity / InfoSec

Bioinformatics and other scientific computing

and you mentioned IoT of course.

Also, don’t neglect some of the old fundamental areas that are “evergreen”.
",1519848984.0
Prometheus01,"Refer to the Curriculum...note the Learning Outcomes, Syllabus, and recommended readings.",1519827240.0
crocodilem8,"One of my favourite classes so far, what you should be really knowledgeable about when you finish the course:

* UNIX philosophy
* Basic history/Linux is not Unix
* Permissons
* Basic sh scripting
* sh -variables, path etc
* File systems -types, pros, cons
* File hierarchy
* Everything is a file concept
* Inodes
* Vi(m)
* Kernel - types, pros, cons
* Processes -life cycle
* Regular expressions
* Sed
* Awk
* Forking

Anything else guys?",1519848902.0
istarian,"CLI  
- navigating the file system  
- creating, deleting, moving files/directories  
- how to configure and use a console text editor (e.g. vim, nano, emacs)  
- creating and extracting archive files (.tar, .gz, .zip, .tar.gz)  
- connecting to a remote server with ssh  
- use of 'secure copy' (scp)  
- finding help with ""man <command>""  
- the use of *pagers* like less and more.  
- pipes -- program output to file, file as input to program,  passing one program's output to another as input  
- changing file/directory permissions as needed and the difference between those permissions meaning each context  

MISC  
- how file permissions work  
- everything is represented as a file
 
Really it's kind of a mixed bag of UNIX concepts and how to use various standard command line tools.  
  
Whether the class is about UNIX programming or not is relevant to whether they should talk about the kernel, threads, system calls, etc.",1519871719.0
infected_funghi,"Im currently studying for my operation system exam (which basically is only about UNIX). So here are our topics:

- processes
- threading (POSIX threads & syscalls, whats the difference to processes..)
- mutual exclusion methods (test&set variables, active wait, semaphore, petersons algorithm, fishers protocol...)
- communication mechanism (ringbuffer, non-blocking write protocol)
- shared memory (shmem_get)
- networks (stream vs package oriented, TCP/UDP (-IP) & their syscalls, iterative/parallel server, IP-fragmentation
- scheduling (which one are supported in UNIX-kernel? preemtive multitasking, fairness...)
- userspace scheduling (differences to kernel scheduling, benefits?)
- virtual memory (relocation, paging/segmentation, translation-lookaside buffer, clock algorithm...)
- access control lists

Maybe its just our prof but we had to do a lot of modell checking too.
In general you should have a good knowledge about what tasks an OS has and how it solves these. 


Maybe this will help you a bit. If these topics seem to match with the content of your course you should maybe check [A. Tanenbaum: Modern Operating Systems](http://stst.elia.pub.ro/news/SO/Modern%20Operating%20System%20-%20Tanenbaum.pdf). Its quite nice literature about the above topics.",1519813789.0
sybrandy,How to quit Vi/Vim. :P,1519867712.0
SketchySeaBeast,I was under the impression even with US dollars you'll want to use a fixed length decimal value rather than a float.,1519777501.0
Hougaiidesu,"$0.009 is just under one cent, not a tenth of a cent. A yen is usually roughly equal to a cent.",1519778399.0
nerga,"You have to track money at smaller values than you actually have. I work at a financial services company, we  use Big Decimal, and track out to I think 6 places officially, but the calculations are using 12 places in actuality. I don't personally deal with any of our Japanese stock trading, but the point stands. It doesn't matter what 1 yen is, or if they don't have a decimal system for their money. You still have to be able to account for 1.3% interest on a loan or whatever that will bring decimals.",1519785582.0
Isgrimnur,"I work for a consumer financial institution.  Our core system represents all money as cents.  Decimal points are visual only.  If you tell it to post a financial transaction of $5, congratulations on your nickel.

And all rate calculations are in basis points.  ",1519780009.0
Samrockswin,"I'd imagine both dollars and yen use fixed-point. I know for the US the smallest legal amount of money is 1/10 of a cent. There may be some limit of precision for Yen as well.

Double-precision floating point gives you about 15 digits of precision, and modern CPUs do so much floating point arithmetic they are quite efficient at that as well. Keep in mind that percentages as rationals may not be ideal as well; rationals can get quite nasty if you're multiplying many percentages together. For example, suppose you're compounding interest daily and want to get an annual percentage rate.

Also 0.009USD is about 1 cent not 1/10 cent.",1519777747.0
rcfox,"Why would Japanese financial institutions do their base calculations relative to the USD? Currencies' values relative to each other fluctuate all of the time. Heck, their value relative to themselves fluctuates over time, and at different rates from other currencies.",1519783832.0
MassiveDumpOfGenius,"Now the question is, how do they work with Zimbabwe dollar? Ignore the last 6 zeros so they can use int?
",1519795544.0
ioquatix,"The way I've done this in the past is to use fixed point math, but at the boundary between computation and billing, round off to a fixed point, e.g. for yen it would be 0 decimal places.

As a practical example, on an invoice, I'd compute a sub-total by adding up all the items, and then round that subtotal and save it as part of the invoice. If you don't do that, you might bill someone for an amount they can't pay, e.g. $10.005 USD, and then when you sum up the invoice and the bank statement, you'll end up with a discrepancy of $0.005 USD which is an impossible amount to pay AFAIK. So, at the point where you are actually telling someone to pay - round the amount and save it, exactly what they are being asked to pay, and use that in future computations.
",1519778048.0
cpxchewy,I work on payments as a software engineer and ours (and google too) uses micro currencies. 1 USD = 1 million microcurrencies. ,1519786513.0
Filthschwein,"The real question is, “Do the Japanese look at our (USD) money system and think it’s too simple?”",1519820757.0
Ramin_HAL9001,"Not just financial calculations in Japan, all monetary calculations use integer math. But this is true for almost all currencies, even Dollars, Pounds, and Euros.

Also, unrelated to your question, but all currency denominations below 1000 JPY (about 10 USD) are coins, there are only 4 kinds of bank notes: 1000, 2000, 5000, and 10,000 JPY, although the 2000 note is almost as uncommonly used as a 2 dollar note. The largest coin value is 500 JPY (about 5 USD), the most commonly used coin is the 100 yen coin. And a 1 yen coin is made of aluminum.",1519824904.0
daperson1,"Just think of the opportunity for fraud if you wrote bank software to use floating point in round towards zero mode, and scooped up the excess yourself.

That'd be an interesting TV series... ""Breaking Bad"" for software engineers.",1519837856.0
clownshoesrock,"64 bit signed Integer: milli-cents.  Seemed like the right way to do it, but I'm not in that field.
__________
I do know that floats are the devil.",1519840529.0
philly_fan_in_chi,"The way Stripe handles this is that you represent the smallest unit as 1. So you'd have 1 cent, 1 yen, and then all transactions are made up as a multiple of that unit. So 1.00 would be represented as 100 cents. But it's delegated to the currency at hand for the canonical smallest unit. ",1519841535.0
stefantalpalaru,"> Since 1 yen is ~0.009 USD, it doesn't really make sense to consider fractions of a yen for most financial calculations

If you think a currency's exchange rate has anything to do with ignoring its subdivisions in calculations, you should not be anywhere near financial software.

You should also learn the difference between computer science and software engineering.",1519826283.0
magnificentbop,"Unfortunately, to get a good idea of how people pilot drones right now, you will want to look at example code.  For example, Pixhawk has a simulation that illustrates how to conduct a flight and what all the moving parts are.

There's a lot of interesting work on flying in formation that touches on the theoretical side more.  [This MIT write-up is a good starting point.  ](https://www.google.com/amp/s/techcrunch.com/2016/04/22/mit-creates-a-control-algorithm-for-drone-swarms/amp/) The problem of avoiding obstacles is largely unsolved, you could also look at the literature on that.",1519821826.0
Phleau,"This guy did some good work here that may help, (brush up on your vector math and kinematics) 

http://andrew.gibiansky.com/blog/physics/quadcopter-dynamics/

I'm.in the same boat as you actually, but my. Background is Aerospace so I'm designing every inch of my future quadcoptor

Anyway, hope this helps, and good luck! ",1520009022.0
foreheadteeth,"Deconvolution of a moving average is typically ill-posed and numerically unstable. If your observation c is the convolution a*b of your signal a and your filter b (in your case b = [0.2,0.2,0.2,0.2,0.2,0,...,0]) then (in the periodic case), you have

c = real(ifft(fft(a).*fft(b)));

All these code snippets are in MATLAB. It would seem that you can recover a via

d = real(ifft(fft(c)./fft(b)))

but this doesn't work because fft(b) has some zeros. You can do instead

Fb = 1./fft(b); Fb(~isfinite(Fb)) = 0;
d = real(ifft(fft(c).*Fb));

but again this is pretty bad (relative error in my test case here is approximately 33%) because of the zeros of Fb.

If you want to do better, people usually use some sort of regularization, see e.g. here: https://stats.stackexchange.com/questions/205891/tikhonov-regularization-in-the-context-of-deconvolution

",1519774132.0
Pandoma,"Depending on your input, there might be multiple time series that resolve to your moving average. Each successive value in a moving average can be calculated from the previous moving average by doing the operation

    x̄_i := x̄_i-1 + x_newval/n - x_oldest/n

So first you run into the problem of coming up with the first (or last if you go backwards) average in the sequence to provide your base case (because there can be multiple base cases, you can have multiple solutions). If you have your base case, you can solve the rest of the values by plugging into the above equation to recover each successive x_newval.",1519760351.0
IJzerbaard,"Very simple: just do a good old Ax=b (ie x = A\\b) with A = [[1, 1, 1, 1, 1, 0 ...], [0, 1, 1, 1, 1, 1, 0 ...] ...] and b=average. It's a little under-determined but solving it in a least-squares sense is probably fine?",1519757745.0
krum,"Impossible a moving average is a low-pass filter so you lose the high frequency information from the stream.
",1519776802.0
DeepBurner,"You can find the inverse system of the moving average system (should be two difference operations if I'm not mistaken), take the DFT/z transform analytically, invert it, and transform back, and you will have the system that complements moving average. Then you can implement it digitally and feed your moving average's output to inverse system's input and you will have the original sequence.",1519769164.0
maladat,"For a time series that exhibits reasonably random behavior, you can probably get pretty close.

It's trivial to construct pathological sequences where the error in the reconstructed sequence is arbitrarily high.

E.g., for simplicity, consider a window of 2, and the moving average is always A. The sequence that repeats (A+x), (A-x), for ANY constant value x, will give you this moving average, because the window always has one (A+x) and one (A-x) in it.",1519772719.0
amrinorini,"Here is a link that really helped me understand the different normal forms 😊 if you still have questions after reading this, I‘m glad to help.

http://www.andrewrollins.com/2009/08/11/database-normalization-first-second-and-third-normal-forms/",1519748938.0
,[deleted],1519751860.0
FearMonstro,"[**Nand to Tetris**](http://www.nand2tetris.org/) (coursera)

the first half of the book is free. You read a chapter then you write programs that simulate hardware modules (like memory, ALU, registers, etc). It's pretty insightful for giving you a more rich understanding of how computers work. You could benefit from just the first half the book. The second half focuses more on building assemblers, compilers, and then a java-like programming language. From there, it has you build a small operating system that can run programs like Tetris.

[**Code: The Hidden Language of Hardware and Software**](https://www.amazon.com/Code-Language-Computer-Developer-Practices-ebook/dp/B00JDMPOK2)

This book is incredibly well written. It's intended for a casual audience and will guide the reader to understanding how a microcontroller works, from the ground up. It's not a text book, which makes it even more more impressive.

[**Computer Networking Top Down Approach**](https://www.amazon.com/Computer-Networking-Top-Down-Approach-7th/dp/0133594149/ref=sr_1_3?ie=UTF8&qid=1519756841&sr=8-3&keywords=networking+textbook)

one of the best written textbook I've read. Very clear and concise language. This will give you a pretty good understanding of modern-day networking. I appreciated that book is filled to the brim of references to other books and academic papers for a more detailed look at subtopics.

[**Operating System Design**](https://www.amazon.com/Operating-System-Design-Approach-Second/dp/1498712436)

A great OS book. It actually shows you the C code used to design and code the Xinu operating system. It's written by a Purdue professor. It offers both a top-down look, but backs everything up with C code, which really solidifies understanding. The Xinu source code can be run on emulators or real hardware for you to tweak (and the book encourages that!)

[**Digital Design Computer Architecture**](https://www.amazon.com/Digital-Design-Computer-Architecture-Harris/dp/0123704979/ref=sr_1_2?s=books&ie=UTF8&qid=1519757210&sr=1-2&keywords=harris+harris+computing+architecture)

another good ""build a computer from the ground up"" book. The strength of this book is that it gives you more background into how real-life circuits are built (it uses VHDL and Verilog), and provides a nice chapter on transistor design overview. A lot less casual than the Code book, but easily digestible for someone who appreciates this stuff. It culminates into designing and describing a microarchitecture to implement a MIPS microcontroller. The diagrams used in this book are really nice.",1519756572.0
,[deleted],1519757050.0
Curdflappers,"Lynda.com is a wonderful resources with thousands of play lists that walk through individual languages, IDEs, developer tools, really everything. It covers introductions through advanced usages, and it's a subscription model (choice of $25 per month or $250 per year) for access to all courses. It's great for professional skills in general as well",1519745280.0
onionKnightKreggle,"I’m taking a course on machine learning right now in undergrad (graduate course though), and I took Andrew Ng’s machine learning course on coursera before taking it. Andrew Ng’s course is really good and provides you with good intuitions about machine learning without going into too much detail about the theory. Having said that, it is a course in octave/matlab, so you might have to refresh your memory of linear algebra. ",1519745415.0
cheese_crisps,Also attend hackathons for real world applications. Many companies sponser hackathons and you can win money just from learning something in 24 hours,1519747922.0
rayreaper,"Surprised this hasn't been posted already but the OSSU has a great curriculum on free Computer Science courses.

https://github.com/ossu/computer-science",1520015843.0
RippledBarbecue,"If you just want to learn languages Codeacademy is good as well as the many channels on youtube,Quill18 who plays strategy and other games has a progrmaming channel as well,Derek Banas on Youtube has plenty of tutorials for C#,making games,blender etc,Khan academy I've heard can be useful but not used it personally.",1519738295.0
dreyno3,Codecademy is definitely a good start for some topics. Udemy has more in depth courses on just about anything you could think of. And most of the time you can get $200+ courses on sale for $10. ,1519741540.0
chewedwire,"Author here, happy for feedback or to answer questions. Thanks!",1519700666.0
GreatCosmicMoustache,"This is such a great topic. 
 
First off, you may be interested in the nascent field of Computational Agroecology ([talk](https://www.youtube.com/watch?v=KE3XS6oLKxQ) / [pdf](https://www1.icsi.berkeley.edu/~barath/papers/agroecology-chi16.pdf) ). 
 
Second, the project you propose is actually multiple different parts acting together. The model that would simulate plant growth to estimate yield would be some kind of biophysical model (see e.g. [here](http://marswiki.jrc.ec.europa.eu/agri4castwiki/index.php/Crop_Simulation)), and they usually simulate how plant growth stages are driven by local soil chemistry, topology, and climate. The part dealing with the ecological interactions - how different species interact with each other synergistically or antagonistically - is more in the realm of network ecology, and you may be better helped asking r/ecology in that regard. It would also be worth your while to read up on how the field of Artificial Life deals with ecology, see e.g. [Dorin et al 2008](https://pdfs.semanticscholar.org/1955/a213c446782342761730fc67ee23fa517c83.pdf) for a review. 
 
Once you have that going, the next part would likely be some way of modelling the spatial distribution of species, e.g. planting tree _X_ at location i,j would interact positively with species _Y_ and _Z_ at locations i+1,j+1 etc. Once that's done, you can start throwing optimizers at the system to select optimal species combinations and placements. 
 
I'd be interested in contributing to this if you decide to bust a move on it. Bringing ecology into agriculture is going to be absolutely critical for our species during the next century. ",1519713136.0
east_lisp_junk,"Do you have a more detailed description of the sort of constraints you need to deal with? Presumably it's more than just ""don't have both of these two plants,"" otherwise you'd solve it easily by only planting one thing.

Also, what form does a solution take? Is it just a set of plant species to use? Or should it say how many of each to plant? Where to plant each of them?",1519744837.0
m--w,"Hey! Not sure if you are looking for discrete math or data structures (given the name of your course), but for discrete math this youtube channel should help! 

https://www.youtube.com/channel/UCGYSfZbPp3BiAFs531PBY7g",1519687058.0
DripDropFaucet,"When in doubt, search the topic on YouTube! These days there’s countless videos on discreet that are super helpful. Searching things like “discreet math demorgans law” should give you videos going through practice problems that explain things very clearly!",1519694826.0
bogomiller,Hang on induction proofs,1519704063.0
xchek32,"It'd be in your best interest if you try to understand the book, the teacher probably picked a book that has the same notation they use (or something not far off). Most books teach the notation in the early chapters as you go.

I couldn't find any (good) discrete math videos when I took the class, which forced me to get and read the book.",1519696732.0
dangkhoasdc,[Concrete Mathematics](https://www.amazon.com/Concrete-Mathematics-Foundation-Computer-Science/dp/0201558025/ref=cm_cr_arp_d_product_top?ie=UTF8) - one of the best textbooks to study discrete math ,1519699124.0
SunkCoastTheory,Just survive. I enjoyed matrices. The proofs made me want to jump off a building. ,1519695070.0
batteryramdar,"go to the MIT website and check out their online videos of lectures.  They have almost every STEM course available, I always watch it in addition to my engineering lectures.  Helps a lot.",1519705739.0
OptimisticElectron,Google The Book of Proof. It's an open source book on the subject. It has helped me a lot.,1519707264.0
Darkfeign,http://www.cs.cornell.edu/courses/cs2800/2017sp/,1519721294.0
welshfargo,https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-spring-2015/,1519781192.0
abdolfhitcoln420,"You’re at the point of a midterm and you JUST started asking for help? Why didn’t you go to your professor, or your TAs?You honestly should have been studying and grinding the first day of class instead of waiting till the last minute. Did you really expect any other result?


College isn’t going to get easier and if you keep this habit up you won’t survive. Leave that high school stuff behind",1519711087.0
Ubermensch-1,"Memorize your truth tables and remember your common equivalencies (modus ponens/modus tollens, domination, etc.). Good luck on the midterm. ",1519694274.0
maladat,"A* always finds the optimal (best) path. It is impossible for a genetic algorithm to find a better path than A* - even if the GA finds the best possible path, so will A*.",1519673348.0
iwantashinyunicorn,[Start here](https://link.springer.com/article/10.1007/BF02430364).,1519679447.0
FUZxxl,"You can implement A* yourself, it's not that difficult.",1519682225.0
east_lisp_junk,"""New"" in the sense of enumerating all possible variable assignments *and then* ruling some of them out rather than testing them each in turn while enumerating them?

Motivation seems a bit thin too -- why use this over something like DPLL?",1519669495.0
hextree,What do you even mean by 'non-polynomial'? That isn't a term.,1519689667.0
swxxii,Dual lens camera like iPhone and pixel ,1519640730.0
wattagebazookaa,Try ORB-SLAM. It's open source too.,1519646053.0
Parzival_Watts,This is not computer science.,1519659054.0
hnerixh,"log, without any specifier, is most likely context dependent. In computer science 2 makes sense, while e or 10 fit most other fields. It doesn't really matter most of the time, the other logs are just a constant factor away. 

I've seldom seen log used, at university level, in a way so that the base mattered, but wasn't obvious from either explicit notation or context. In most of computer science, the base of the logarithm is foobar anyway, we just smack an 'O' in front of it.",1519603130.0
Ryand735,"No worries, your assumption is correct. It's just a convention used by CS people because almost everything we do is base 2. ",1519602567.0
Ubermensch-1,"Personally, I had never heard of lg(x) being used for base two logarithms, but an internet search says people do use this notation sometimes, so I guess your teacher wasn't wrong.

In general, logarithms, when used to do things like finding computational complexity, are in base two, so if you understand the context of where the logarithm is coming from in computer science, you'll understand why it's base two. Most data structures and sorting algorithms are using comparables (as in comparing two things), so it's understandable why their computational complexity is base two. Maybe look for an explanation for the Big O of mergesort on youtube, that helps explain it.

The TL;DR would be that yes, it's usually base two in computer science (as far as I've learned), but it should be pretty implicit why it is for any given situation.",1519602703.0
JayShenoy,"In computer science, log(x) indeed refers to the binary log, or log base 2. To avoid confusing it with log base 10, you can denote the binary log as lg(x).

However, when analyzing the efficiency of algorithms, whether log(x) refers to base 10 or base 2 is irrelevant because O(log(x)) = O(lg(x)/lg(10)) by change of base for logarithms. Since constants can be discarded in Big-O notation, O(log(x)) = O(lg(x)).",1519602714.0
UniqueBox,"I was explained that we use log(x) firstly because we're lazy, and secondly in big-O notation the base doesn't really matter. Since there is a log rule that allows you to change the base of your logarithm. ",1519607009.0
andermorandev,"**In general if there is no specified base:**

Engineering/Science - log base 10

High level math - log base e

Computer science - log base 2",1519658224.0
viperlight89,"Assuming log(x) has a base 2 in computer science is useful in the majority of cases you'll see it in college (sorting algorithms for instance). So yes I'd consider it a computer science thing.

More importantly though, if you're referring to algorithmic complexity and Big Oh notation, then the base doesn't matter. Logarithms of different bases differ only by a constant. To convert from log_2(x) to log_10(x) you can use
log_10(x) = log_2(x) / log_2(10)

Constants can be disregarded when referring to algorithmic complexity, so O(log_2(x)) = O(log_10(x))",1519602733.0
TezlaKoil,"It's a computer science thing. Essentially, a base 10 logarithm correlates with how many digits a given number has when written in decimal (i.e. base 10) notation - for example, `log10(100) = 2`, `log10(1000) = 3`, `log10(10000) = 4` and so on. Similarly, the base 2 logarithm correlates with how many digits a given number has when written in binary (i.e. base 2).

Binary is quite important in computer science, so computer scientists often reserve the *log* notation for the base 2 logarithm.",1519602671.0
,">or is my professor/teacher wrong?

Neither is wrong - your sixth form teacher said you should _assume_ log(x) is base 10, and you should, unless you have more-specific knowledge about the context you’re working in.",1519644032.0
Pour_Louis,"Never assume. All computational environments have their own conventions, though many are similar. Check the documentation. ",1519660272.0
Nerdlinger,"It is a CS thing since so much of the work in CS is done with powers of 2. And when dealing with things like algorithm time and space requirements, it doesn’t really make a difference as they differ by a constant factor, which just gets abstracted away.",1519602497.0
cruyff8,"It's a computer science thing. On the other hand, it's trivial to [change bases](https://proofwiki.org/wiki/Change_of_Base_of_Logarithm), so it's really a non-issue.",1519605011.0
bende511,"in compsci, log(x) usually means base 2, but it also almost never matters because big-O notation makes it irrelevant. log(x) usually refers to base 10 in other contexts, and I've seen ln(x) used for natural log, but in physics log(x) is often natural log since that is the one that matters.",1519614852.0
ChezMere,"Most of the time you use log it's in the context of big-O notation anyway, in which all logarithms are equivalent.",1519620256.0
green_meklar,"You can't always assume log is some particular base without context. For algorithm analysis it might be base 2 (in fact a lot of the time it doesn't matter what the base is, and/or they are interchangeable), but in actual code it's often base e (for instance, Javascript's Math.log() function).",1519627600.0
barburger,"On the other hand, in machine learning classes we always used log as natural log. Never used log 10 and rarely log 2.",1519641357.0
TomSwirly,"As an applied mathematician who makes his living programming, `log` is pretty ambiguous.

If I'm doing pure math, it's log to the base `e`.  If I'm doing data science, it's log to the base `10`.  If I'm doing computer science, it's log to the base `2`.  

Luckily, it has never really caused issues - because these values only differ by a constant multiplier.  If you are consistent within a project, then it almost never matters which one you're using.  And if you're talking O() notation, then they're identical because of that.

In notation, I often write `log2` and `log10` a lot. If I write `log` it almost certainly means log to the base `e`.  However, once you're within a specific project, in my experience you just talk about `log` and know from context which sort of `log` it is.

Oh, and I never use `lg` because it's still ambiguous, so there's no advantage.
",1519644590.0
WetSound,"There does not exist an ultimate true form of notation, so he can't be wrong per se. You can always discuss if notation is convenient or confusing or something third.  ",1519657152.0
Revrak,"it is, in my university they used lg for base 2 in computer science",1519680572.0
drvd,"The letters ""log(x)"" mean whatever you (or your boss/professor) want them to mean. If the person posing the questions of the exercise wants (and clearly states) ""log(x)"" to mean ""the nearest positive even integer of the logarithm to base 17.4 of x"" than it means that. Such a definition of ""log(x)"" would be very strange but log(x) to mean logarithm to base 2 is perfectly fine as the constant factor by which the different logarithms differ doesn't matter for big-O.  ",1519808862.0
auriscope,"I tend to use lg as the binary log, but in my experience anything that looks like a log in CS literature is binary unless explicitly said otherwise.",1519603883.0
ProgramTheWorld,"A general rule of thumb is that your professor is rarely incorrect. If you think he’s wrong, it’s probably not the case but you should still do a quick check either using your textbook or online resources.

Considering you are asking this on reddit, I assume you have more trust in random strangers instead of instructors that have proven experience and degrees. This is an interesting mindset.",1519617899.0
_sonnyjr,"None of those positions will be very hardware orientated, unless the job you choose required you to be very familiar with the underlying hardware (such as developing for embedded systems or something). 

But the positions you listed are pretty much all the same except database administrator and data scientist. Most of them won't require much hardware knowledge since you'll be writing code more than anything. The best way to figure out what you want to do is to find what your passionate about and pursue that. It doesn't sound like hardware is interesting to you so don't pursue those positions. 

You'll learn more about what positions you'll enjoy after you take internships, start working on side projects, or after you're in the industry for a few years. I started working full time after graduation as a software engineer doing software load testing, thought it was cool at first, but hated it after a few months. After that I switched to mobile app development and enjoyed that way more. 

You'll figure it out over the next couple years, but my biggest suggestion would be start side projects now and apply for internships. That way when you graduate you'll have some experience and some idea of what you enjoy working on.",1519550234.0
ImaginationGeek,"I’m not entirely sure what you have in mind that the difference between “Software Engineer” and “Software Developer” are...  when I see these as titles in job postings, they seem to be pretty much interchangeable - it just depends on which name a particular company chose to use.

(That said, “software engineering” does have a specific meaning in academia, but that doesn’t seem to carry over to the terminology used in industry. Both job titles are doing “software engineering” by the academic definition.)

Mobile Dev and Web Dev are just spécifications of software development that spend most of their time working on a specific type of software.

(I’m referring to the jobs. If someone says they are a “web developer” by training, it may indicate that they have the specialized training but lack the generalized training to do other developer jobs.  By contrast, a “software developer” by training has the generalized knowledge to do - or learn on the job to do - development of any kind of software; they may simply happen to be doing a specialized job at the moment.)

Database Administrator is a different beast because this isn’t (primarily) a programming job.  (Although there are probably also jobs out there that are 50/50 DBA and developer.)  This has more to do with understanding your data, creating a good structure for storing your data, configuring the database to run efficiently given the data stored and types of queries (questions) that will be asked.  Of course keeping the DB secure is also an important part of this job.

Data Scientist is also a different beast.  This is all about using large amounts of data to do interesting things or answer interesting questions.  You want to learn some math, especially statistics and related areas to do this.

What degree are you earning?  Computer Science?  (I would suggest that since it can prepare you to do ALL of these jobs, if you take the right elective classes.). But if you’re thinking of something else, let us know and we can say which of those jobs your degree can prepare you for.",1519580101.0
crabbone,"First is an IT course. Something nobody really needs, unless for an easy degree / points collection path. A lot of people working in IT never had any formal computer-related education, and from my experience with people who did study IT, they rarely become IT specialists.  In my experience, business degrees related to CS are intended for people who don't really want to study anything / couldn't make the cut for EE / CS proper degree program.

Second is an actual CS course. I cannot vouch for quality, but, at least on paper, it looks like a regular CS program.",1519566661.0
flexibeast,Maybe things like [Feynman's lectures on computation](https://books.google.com.au/books/about/Feynman_Lectures_on_Computation.html?id=-olQAAAAMAAJ&redir_esc=y)?,1519524898.0
DontKillTheMedic,I received degrees in Physics and CS. My research was in Computer Vision + Edge-Detection in the domain of Manganese-Enhanced MRI Contrast Agents. I liked how it was a melding of two fields. ,1519531297.0
bstamour,"If you consider information theory as a part of CS, then you're in luck. There's some cool stuff going on in theoretical physics with quantum information.",1519532640.0
arcticfox,"What does computational physics look like?  Fortran.  A whole lot of Fortran. 

:-)",1519526342.0
ebek,"You can use category theory to essentially ""translate"" physics into computation and vice versa, as noticed by John Baez and Mike Stay in [Physics, Topology, Logic and Computation: A Rosetta Stone](http://math.ucr.edu/home/baez/rosetta.pdf).",1519565218.0
fnybny,"For models of finite dimensional quantum mechanics/quantum information theory, read up on categorical quantum mechanics.",1519554462.0
the_aligator6,"I'm sorry OP, you're getting a lot of answers that are exactly what you didn't want. I would suggest you start with Digital Physics and go from there. ",1519567003.0
jmite,"There's some interesting crossover between special relativity and the theory of distributed systems, with the modeling of causality, and Lamport clocks, especially physical Lamport clocks.

But to my knowledge, there's not a ton of crossover. There are concept like entropy that are borrowed from physics, but I think they're fairly liberal adaptations of the concepts. 

But in terms of P vs NP, computability, etc. there's really not much. The reason for this is that these are all modeled as purely mathematical systems, independent of any physical rules.

Physics might have something to say about the Church Turing thesis, and whether a stronger model of computation exists. But if you assume a Turing model of computation, there's really no way physics will come into play.

Now, once you're building an actual computer, or modeling wall clock performance instead of polynomial complexity, physics is important, but that's not TCS.",1519528988.0
physics_is_fun,CS and Physics comes together via differential equations that are solved with numerical methods.  See it in action at [myPhysicsLab](https://www.myphysicslab.com).  Has full [open source available in github](https://github.com/myphysicslab/myphysicslab) and extensive [documentation](https://www.myphysicslab.com/develop/docs/index.html) which is designed for beginning to experienced programmers.,1519548322.0
blindingspeed80,"It looks awesome. Simulations of particles, fluids, atomic interactions, etc. You might want to check out the visualizations that come out of SC or SIGGRAPH.",1519583347.0
physixer,"> ... I'm referring to things in the vein of the relationship between Thermodynamics and Information Theory ...

http://bayes.wustl.edu/etj/articles/theory.1.pdf",1519608574.0
bmacswag,QIT,1519561178.0
pavelchristof,"Statistical field theory can be used to analyze neural networks: https://arxiv.org/abs/1710.06570

[Spin glasses](https://en.wikipedia.org/wiki/Spin_glass) are closely related to Boltzmann machines.",1519545889.0
Cyfar,Maybe you'll be interested by quantum algorithmics ?,1519564597.0
tulip_bro,This is what I come to this sub for!,1519577511.0
ReginaldIII,Lattice Quantum Chromo Dynamics,1519577606.0
endlesslope,"I guess quantum systems is the greatest overlap but I see some overlap of themes in statistical treatments of poorly constrained sets (like terrestrial exoplanets which can't yet be reliably characterised, but for which we will be able to characterise very soon... but need to know where to point the telescope to save time), and in inherently entropic questions with even weaker data sets like astrobiology (how and where life will form and develop).  I feel like right now that's a little tenuous though and more to do with the overlap into machine learning.  A lot of these themes are being adopted into astronomy and physics right now as far as I can see.  There are datasets too large for humans or traditional computation to run through, so you'll run into machine learning there (to the point where a significant portion of a group will have a CS background rather than physics).",1519578128.0
gruehunter,"One rather controversial topic is that memory access cannot possibly be O(1).  When analyzing an algorithm's performance, the time complexity of memory access actually scales at O(sqrt(N)).  This partially reflects the nature of the cache hierarchy as it stands today, but it has a strong connection to the geometry of a sphere.",1519584658.0
QKD_king,"So this is a great question and I feel I have a pretty decent answer. I am admitted only graduating this upcoming May, so feel free to take what I say with a grain of salt. 

I focused on physics and CS and have been interviewing for primarily two types of jobs and graduate schools. I'm primarily interested in Quantum Computing for graduate school, and I've gotten some incredible grad school offers. Turns out a lot of schools want a blend of physics, math, CS, and EE. I'm primarily interested in design, theory, and algorithms (I have experience in each of these areas at a national lab). I can easily say there are not many people with my kind of background and interests, so grad schools have been recruiting pretty hard (which is nice!). 

As far as jobs go, I have interviewed at two companies (small pool by choice, as I would prefer grad school). Both are physics research groups that need far more software engineering. Both of these are private industrial research labs and one has heavy interest in quantum computing. Most of what I would do at both is set up physics simulations on super computing platforms. Also optimize code. ",1519586328.0
I_had_to_know_too,"I'm guessing you're talking about the physical properties of computers?

Materials Science plays a big part in hardware design. Processors can only get so small due to the size of atoms and they're usually really flat so heat can be dissipated out.",1519587501.0
saadmrb,"Computer Science and Physics are going to give you a great head start. Make sure you are programming at home, and that you're studying hard to ace those Physics exams. Math programming and 3D graphics will help you think about the world in terms of computations, and you can do this at home during nights and weekends.

I didn't participate in Robotics clubs when I was in high school, because they weren't popular when I was a student.

But I aced my tests, worked on some cool programming projects, and ultimately went for a PhD in Robotics from CMU.

High school project ideas for computer science students:

Write a planet simulation. Using Newton's law of gravitation and euler's method of solving differential equations you can build a simulation and then watch the planets move around some fixed planet (the sun) in ellipses.

Make a 3D plotting program that let's you plot parabolas and different shapes in 3D.

Do something with images. Letting robots ""see"" is important and hard. Can you write a computer program to detect a face in an image? There's some cool open source code that does this, so perhaps just worry about using the code. Then ask yourself what it would take to put this code on a robot. But a person following robot, or one that says ""Hi"" when you look at it, is just cool.

Welcome the world of robotics. This is the future, and we need more people going down this path as teenagers.",1519595692.0
seal2434,"Look at the BB84 Quantum key distribution system for Quantum Encryption and, as mentioned by other commenters, Quantum Computing (spintronics being once aspect). Really, the bridge between them is pure mathematics so things like P Vs NP ext... This isn't really physics as such but I think this question more goes into mathematics. To take a step back, and look at the 'less interesting' things, there's the physics of electrical circuits on conventional PCs as well as air flow of a system and thermodynamics of water cooling, and that's about all I can think of. :)
",1519597210.0
Mechdrone,"Surprised no one mentioned this, but Computer Graphics is pretty damn Physics intensive. Example: https://en.wikipedia.org/wiki/Radiosity_(computer_graphics).",1519598667.0
,[deleted],1519609147.0
,I was doing my Master's at Physics on Quantum Information and I was just working on a thing you are looking for. Here is a paper : https://arxiv.org/abs/1610.01829,1519629477.0
seba,There is an interesting connection between black holes and theoretical computer science: https://www.youtube.com/watch?v=zJdTtL3ajaI,1519629761.0
vgurianov,"This question me interests for a long time. This issue has just been discussed at r/Physics /, see https://redd.it/8002gs. You can view a book: http://digitalphysics.ru/pdf/Elementy_informatcionnoi_fiziki_-_G_V_Vstovskii.pdf",1519636452.0
redditEnergy,"For me it has definitely been ray tracing. Balancing physics, math tricks,  and computation tricks. 

The main branch of physics being used is anything about light. ",1519579832.0
dvdmuckle,"If I had to go by what my CS professor, who has a degree in physics, talks about most, probably quantum computing.",1519582629.0
marfr960,http://www.physicsbasedanimation.com,1519560689.0
giskkard,"This [Linear Algebra] (https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/index.htm) course by Prof. Gilbert Strang~~e~~. [edit spelling]
",1519498850.0
profhelios,"If you are asking for favorites... and which ones I considered the best?

I liked Computer Architecture and Assembly Language a lot -- even though I stunk out loud at it, I learned a lot about computer architecture and how computers actually worked.

Operating Systems was a favorite, especially when I got to build a mini-OS.

I also liked my course Intro to Networking. It's amazing how fragile, yet robust a thing, the internet is.",1519503520.0
tom808,"I know a lot of people hated it but we had a course on compilers. Really interesting to see all the tricks and techniques 'behind the scenes' when your code is run to make it optimal.

What really made me interested is that I knew nothing about the topic but I guessed at some of the tricks and operations which could be performed and I was pleasantly surprised I was pretty close when we did learn about them properly.",1519494619.0
wilkesreid,"Discrete mathematics was really interesting, data structures, networking, and operating systems were probably the most useful.",1519499981.0
pokepokepokepoker,Cryptography gets my vote. You get to learn about what keeps your stuff safe (or not) and how it all works from the bottom up.,1519501652.0
stimpakish,"Compilers, operating systems (and its focus on multithreaded programming), and numerical methods - how to approximate floating point arithmetic, since computers are after all binary. How do you do floating point math when the only two numbers you know are 0 and 1?",1519502323.0
the-wumpus,unfortunately all of the required CS courses in my BA and MS were absolute crap. I think in general there is an overabundance of terrible CS profs (anecdotal from 3 universities ). The best course I had for CS was MIT OCW mathematics for computer science; didn't cost a dime and had some really good insights. ,1519522176.0
CorrSurfer,"Clear answer: Complexity Theory! I find it incredibly fascinating to find the intrinsic difficulty of computational problems. 

Also, studying this topic means hearing/reading about many great ideas and observations in rapid succession.",1519493185.0
hnerixh,"Computer Construction - Build your own CPU with some kind of VGA output in VHDL, then create an assembly language for it, and write a game or something in your own assembly, running on your own microcode, running on your own CPU.",1519508507.0
mjgood91,"The courses that I remember really changing the way I thought about computer science are (in no particular order...)

 * Data Structures (this is also what got me started thinking about how good or poor algorithms are)

 * Principles of Programming Languages (Why functional programming vs imperative? etc. This helped me think of programming languages as tools)

 * Operating Systems (never understood how they ""worked their magic"" before taking a course on them)

 * Software Enginnering (arguably the most important course you'll need if you work for any sort of business or on a team or programmers)",1519567522.0
Rurouni,"My favorites for being the most interesting, and later the most relevant, were my Compiler Construction and Functional Programming classes. Compilers material runs the gamut from syntax, parsing, tree manipulation, interpretation, and optimization. FP taught me new ways to approach problems. Both were invaluable.",1519496273.0
DonaldPShimoda,"Took a course in functional programming at my university. Totally changed how I approach software development in general. I definitely struggled with it at first because the mentality is very different, but it was hands-down the class I learned the most from.",1519493824.0
coolnat,"Database theory was incredibly useful and interesting to me.

I went in with knowledge of how to write SQL queries, but learning the theory behind databases really has helped me become better at designing my databases and writing efficient queries.

I think the best part was studying how a query optimizer works, which lets you avoid things like writing a query that forces the DBMS to do a full table scan.",1519539738.0
TheWildKernelTrick,English,1519500034.0
skeetskeetskeetskeet,Harvard's cs50x with david malan,1519516001.0
,Artificial intelligence ,1519496979.0
sailorcire,"Best and most useful?

OS (undergrad and grad)

Networking (undergrad)

Architecture (undergrad and grad)

Algorithms II (undergrad)

Compilers (undergrad)",1519502589.0
mnestorov,The UNIX course and fundamental data structures were hands down the best for me,1519514084.0
GayMakeAndModel,"Data structures with asymptotic analysis (big OH), hands down",1519533557.0
Overlord-of-Robots,"Computational Complexity at Johns Hopkins APL. Awesome learning. I was happy just to get 50% on the homework but it really was a class where knowledge was gained and horizons were broadened. 

I also liked Operating Systems, Computer Architecture (both undergrad), and Client-Server Systems. 

EDIT: I’m a bad alumnus!",1519539577.0
gamerwalebabu,"Highly recommend Havard's CS50 by David J. Malan. Can't say enough! This course gave me a really nice headstart into the world of Computer Science. 


[Here it is.](https://www.google.co.in/url?sa=t&source=web&rct=j&url=https://www.edx.org/course/cs50s-introduction-computer-science-harvardx-cs50x&ved=2ahUKEwizopjXuMDZAhUDI5QKHYlBDKQQFjAAegQIBhAB&usg=AOvVaw0Jvwc_9Nzv6rRZFAlmLKur)

",1519540374.0
ImaginationGeek,"Not sure how specific you want, but...

Machine Learning with Prof. Charles Isbell at Georgia Tech.  I took it in person, but videos of the online version are available... well, online.  Both versions are excellent.

Computer Systems with Prof. Peter Dinda at Northwestern, which is a clone of his advisor’s (in)famous 213 at Carnegie-Mellon (created by Profs. Bryant and O’Halloran)

AI Programming with Prof. Chris Reisbeck at Northwestern.  Awesome not for AI content but for producing really good code because each assignment was pass-fail: if it wasn’t good enough, you had to fix it and resubmit as many times as necessary until you got it right. (And “right” meant good code, not merely correct output.)
",1519514480.0
AsymptoticUpperBound,"Software Integration was by far the most useful to me, but not very science-oriented. For actual Computer Science, I'd say Computer Architecture.",1519500984.0
hubbahubbawubba,"Nonlinear optimization was definitely my favorite. At my undergrad it was taught as a mix of theory and algorithms, so it was cool switching from geometric concepts to clever algorithms designed to exploit them and back. ",1519517865.0
whycantibeanon,"Best as in most useful would have been networking.
The best as in most fun was embedded systems, which boiled down to here's an Arduino you have a semester to make a project have fun.",1519519339.0
brett_riverboat,"I really enjoyed my High Performance Optimization class. You really need to know how a computer works, down to the transistor, even including the software, to squeeze every last bit of performance out of it. It's really the ultimate test of all of your CS knowledge.",1519530810.0
stupidCSstudent,I really enjoyed my computer vision course. We all had final projects that were interesting. Mine was doing object detection. We followed a tutorial but it was still fun learning about how machine learning worked and how models are trained ,1519533628.0
MrPopperButter,"This video about operating systems really made a lot ""click"" for me: https://www.youtube.com/watch?v=9GDX-IyZ_C8&t=1s",1519537632.0
rumi-goes-fishing,"Harvard’s CS50 programming course is really good.  They have the entire course for free, including the assignments pertaining to each week.  It actually helped me brush up a bit before I went back to school.  Highly recommend.",1519582494.0
no_detection,"This makes the bundle smaller, and not immediately decipherable to a user who opens up the source in browser. It is by no means true security, but it is efficient. 

Minification in general is a common practice in web development:  https://en.wikipedia.org/wiki/Minification_(programming)",1519492775.0
geon,"As someone who have had to reverse engineer uglified code; No, there is no security benefit. It just makes the job a bit more annoying. ",1519507080.0
crabbone,"It doesn't really make sense to do this because this cripples your debugging abilities, while offers next to no benefits, since you would normally gzip the whole thing, not to mention that the libraries are typically loaded from popular CDN, i.e. will be already cached by the browser.",1519632200.0
tRussianPlayer,Is it A.S.Pushkin ?,1519481114.0
maladat,">Selection: at each generation, 30 individuals are chosen randomly, and the 10 best are kept for the next generation.

You might try proportional selection - in other words, an individual's likelihood to be chosen as a ""parent"" is proportional to its fitness.

>Mutation and crossover rates default to 50%. They can be modified with sliders.

How are you defining these probabilities? Typically, the mutation probability for a genetic algorithm is the probability a given BIT in the genetic sequence will be mutated. This probability is often chosen to be something like 0.1%. 

A 50% per-bit mutation rate is absurdly high. If it's a 50% chance of an individual having mutation, how do you then choose how the mutation occurs?

Here's a survey I found with a quick Google about using genetic algorithms to solve the TSP.

https://iccl.inf.tu-dresden.de/w/images/b/b7/GA_for_TSP.pdf",1519591397.0
,"I would learn some undergrad AI if you can. Things like simple neural networks, A*, Bayes Theorem, genetic algorithms, expert systems and brush up on your propositional logic if you don't already know it well. Other than that, it's good you have some stats background, you'll do fine regardless ",1519446079.0
jmite,"Learn data structures and algorithms. You *need* an excellent grasp of trees and graphs in AI, as well as knowledge of local search and discrete optimization strategies in general.

Learn your theory: complexity and computability. Know when things are computationally hard or computationally impossible, and know the heuristics and workarounds for solving these problems in practice.

As an aside, learn how to write clean, abstract, modular code. This isn't AI specific but is one of the skills I find many lack coming from an engineering Bachelor's.

And learn multiple languages, and the concepts behind programming languages. You should be able to pick languages up quickly, and not rely too much on knowledge of a specific language as a crutch.",1519450695.0
TheMitraBoy,"If your background in Mathematics, especially Probability and Statistics, is excellent... You'll cruise through AI courses.",1519456859.0
taauji,Hey I got into UB AI group as well. See you there!,1519485307.0
welshfargo,"http://aima.cs.berkeley.edu/

http://norvig.com/",1519514705.0
Proggoddess,"Congrats on your acceptance! I did my undergrad at UB in CS.

When I took AI in grad school at CMU, I found I needed my background in calculus 3 (3D calc, some of the transforms), and diff eq would have been nice for me to know. There were some statistics/probability formulas that I didn't understand, and the professor would always talk about what is known a priori, but I got by.

It was not a programming course, but you did have to be able to read and modify already-written programs to do some of the labs.

I would guess the UB version would be similar... and much less expensive than CMU. UB was a very good school and I didn't feel unprepared at all in either grad school or my first job.",1519485337.0
kupo1729,"I'd brush up on your programming tbh. If you have a strong background in probability, you'll have an easier time with  the technical content in the ML/vision courses than your peers with a CS background & less of a probability background. 

Brushing up on your programming will likely make some of the assignments easier. ",1519495883.0
cheese_wizard,/r/im14andthisisdeep,1519433257.0
hextree,?,1519438901.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1519424369.0
VermillionAzure,"Degenerate cases. For example, null pointers.

If you have permutations of different variable values, check all possibilities or classes of possibilities. ",1519416459.0
Rebelgecko,"For anything involving time:

Leap years

Leap seconds 

Daylight savings time

Crossing the international date line

Dates before 1970

Dates after 2038",1519416958.0
arnedh,"Read all the articles you can find with the header ""Falsehoods programmers believe about ...."" 

https://github.com/kdeldycke/awesome-falsehood

Loads of edge cases in those articles, possibly too much. Have you adequately accounted for 30.02.1712 in Sweden, for 23:59:60 for leap second, for people without an actual name or only one name, for times moving in odd ways when an area changes time zones at the same time Daylight Saving Time kicks in? For conjoined twins, who may be one person now but two after an operation? Areas that in odd ways are under a different country's jurisdiction? Persons who are a mosaic and have two different genomes? 

More trivial stuff: 

For numbers: 0, negatives, beyond maxint (and maxint can be different sizes depending on which number type you have used)

String: empty, non-alphanumeric, very long

 edit: 

string value ""NULL"", ""0"", ""nil""

string values to trigger SQL insertion: ""'--; drop table Students;""

Lists: empty, long

Data structures: loops

Dates: invalid dates, leap years, purported leap years that are not (1900, 2100)

Times: 24:59, higher numbers

",1519418966.0
benderTheCrime,root user permissions.,1519416912.0
jmite,"You will never cover all cases. There are some common edge cases, like 0, empty list, infinity, etc. But to a degree, every case is a special case. Proofs are the only way to know that there are for sure no bugs, unless your input domain is finite, in which case you need exhaustive testing.",1519455136.0
Neker,"* Large volumes of human-generated inputs.

We programers do not seem to realize what real humans can do with a keybord.

* computations

Keep in mind that numbers in computers *never* are real numbers (&#8477;). Truncations occur. Approximations occur. Overflow occur. Throw tons of oddballs at them. Seek for the one-in-a-million case.",1519460715.0
dwkeith,"For anything involving text input: Emoji everywhere❗️

I am always amazed at where they can't be used, and it is a strong indicator that the software will be hard to localize later.",1519425102.0
crabbone,"Testing is a whole separate profession... it's not really possible to cover it in a paragraph. Unfortunately, today, QA engineers are perceived (and compensated) as if they were lesser being, when compared to programmers. Which is probably a factor in how uneven the quality of software on the market is.

Now, to answer your question in a meaningful way, I'd need to first relate to what testing classes are there. Each class is intended to solve different problems: you wouldn't test the same things in different classes.

# Unit testing

Is for testing minimal units of your program. If the minimal unit in your languages is a procedure, then unit tests test procedures, if it is a class, then these tests are for classes etc.

The general approach that would work in here is to think about Hoare's triples, think in terms of precondition, invariants that must be upheld during the unit's execution and postcondition.

# Integration testing

These tests are there to ensure different units honor each other's interfaces when they need to communicate.

The general approach here would be to try to formalize the specification, so that tests can be automatically generated against it.  For example, if your program is a web server, you could use Swagger to help you test all API automatically.

# Regression testing

These tests verify that the program does what it used to do before. I.e. that the inputs previously accepted by the program are still accepted, the rejected ones are still rejected etc.

I don't really have any ideas on generalizing this one.

# Sanity testing

Is there to verify that the product as a whole is minimally usable (otherwise it is known as ""dead on delivery""). This is needed to not waste QA's time, where they'd go through the bureaucracy of accepting a new build for, say, performance testing only to realize it doesn't work at all.

In these tests, time is your worst enemy.  This is the kind of tests which runs before a change-set / pull request / etc is accepted into upstream.  If it takes more time to run this test than the time you have between pull requests, this test is unacceptable.

# Performance testing

Tests that the program's performance doesn't degrade over time / under load more than is advertised in specification.  This is often confused with benchmarking.

This is not my field, I don't know much about it.

# Benchmarking / Stress testing

Also, a kind of testing, where the objective of the test is to produce the highest possible output from the program.

Here, people would be usually be tempted to fiddle with statistics... but really, you are doing this for the sales / marketing people.  Learn how to make decent looking pie charts :)

----

Over time, however, I've noticed that the most important thing for tests is the ability to understand the failure.  Whatever you are doing in your test, log it.  Don't fail with messages like ""Not found"" or `ENOENT`, or other rubbish programmers like to produce when the program fails.  Anticipate failures and write in full sentences, providing as much data as possible, preferably in parse-able representation.  Ideally, also try to anticipate the reason why something might have happened.  Don't throw away backtrace information.  An exceptionally good test might even try to bisect the program's state in order to figure out where did it go wrong.

On several occasions I wrote tests which examined the call stack of the program and bisected it looking for anomalies in the values in order to report them.  In other cases, libraries would not pass the important information together with the error, and I had to, again, examine the call stack in order to salvage that information.  In both cases, these efforts resulted in finding more than a handful of bugs.",1519634589.0
generic12345689,"Make sure it does only what it is intended to do maybe?

As a simple example This means unintentional behavior like script injections are validated against throughout your application. ",1519413643.0
minno,"On the other hand, [prefer meaning](https://refactoring.guru/smells/primitive-obsession). You can stop all sorts of bugs if you use a type that can only be easily used in the context that it is intended to be used in. Like passing a number of pixels to a function that expects a percentage, or a user ID to something that expects a timestamp.",1519409916.0
bstamour,"> Furthermore, an int can point into any type of array

This is heavily language/platform dependent. For example, in C++ the type `size_t` is designated to be able to store the size of any possible object, including an array. `size_t` isn't guaranteed to be the same size as `int`, and `size_t` is also unsigned. Meaning `int`s cannot portably index into all arrays. The issue probably won't turn up in most programs, but `int` is definitely the wrong type to use for array indexing.",1519415296.0
eario,"Another important advice: If you want to divide an int by 3, then don´t use the inefficient ""/"" operation, but instead just multiply it with -1431655765.

It´s theoretically a tiny bit more efficient, and thanks to overflow it gives the same result. (at least if the number you wanted to divide was actually divisble by 3)

Also it shows that you have some grasp of modular arithmetic, and are aware that ints are approximations of 2-adic numbers. Other programmers really appreciate it when you demonstrate just how smart you are. 

/s",1519418410.0
kkjdroid,"Don't pretend you're using C. Use the appropriate type instead of trying to be clever and use ints for everything.

 |Python|C|JS
---|---|---|---
x | 'b' | 'b'  | 'b'
x + 1 | TypeError | 'c' | b1'
x - 1 | TypeError | 'a' | NaN",1519460305.0
purely-dysfunctional,"> It's generic

Then why does Go have it? 🤔",1519428986.0
ninjeff,This is just a list of some things you can do with int. What's actually being argued against here - what are we meant to be using int instead of? Floating-point numbers? Structs?,1519435892.0
RuttyRut,"I worked on a project a couple of years ago that consisted largely of deciphering the binary values of an interoperability standard using C# and C++.

Our tech lead would insist that we use data types that match the bit length of the particular words in the standard. For instance, Unsigned Short for 16 bits, Unsigned Int for 32 bits, and Unsigned Long for 64 bits. We also used internally developed data types as C++ objects to represent intermediate bit ranges, or higher bit ranges.

There were wayyyyy too many bit-shift operations in that code base. It became a nightmare trying to read anything afterwards, but it was “efficient”.",1519447696.0
Zarutian,"Is that int 32 bit big endian twos complement or 64 bit bi-middle endian one-two complement? (it is four big endian 16 bit ints, with the most significant ones on the ends, yeah I want to fry clean the brains of whoever subjected a production code to this)",1519486530.0
,[deleted],1519411391.0
magnificentbop,"The naive way is to have 5 separate vertices representing the 5 passes through the loop.  If you've taken theory of computing, think of it like a state machine; a fixed number of iterations is a DFA, and a variable number is an NFA.",1519395066.0
east_lisp_junk,"> With that being said, how would you represent a for loop that is going to execute a pre-determined number of times?

I would lean towards representing it as a typical loop. If you want to unroll the loop, go ahead and do so, but you may not gain a whole lot from it.

> In my head there is a sort of contradiction in trying to draw the graph to match the execution of the program, but also wanting to represent the correct number of paths.

The set of paths through a control flow graph is just an approximation of the set of paths through a program. There is no algorithm that will always make them match perfectly.",1519395515.0
cs_throwaway_3462378,"Think about what a finite automaton does. It transitions between a set of states depending on what symbols it encounters in a string. Consider this finite automaton https://upload.wikimedia.org/wikipedia/commons/9/9d/DFAexample.svg that recognizes binary strings containing an even number of zeroes. If you take a snapshot of the machine where all you know is what state it is in can you say anything about what happened earlier in the string? Say you can see that you are in S1, what would you know? What does that mean the machine is representing about what happened earlier in the string?",1519352425.0
TKAAZ,"If you are reading Sipser, the book definitely has the answer, albeit rather implicitly. A finite automaton uses states to represent what ""happened earlier"" in an input string string. The number of states, as the name 'finite automaton' indicates, is finite and in this sense it only has a finite number of ways of representing what ""has happened"" in a string.

So while it can recognize languages such as {a^n b^m | n and m are a natural numbers} which needs some information of what has happened in a string, it can't recognise languages such as {a^n b^n | n is a natural number}, in which it needs to ""count"".",1519352308.0
Padandler,DFA’s have no memory associated with them. Looking at their run functions should clear up why this is I would think. ,1519353986.0
Segfault_Inside,"I want to figure out some way of leading to the answer, rather than giving it, which is strangely hard here. I suck at teaching, but this is a thing I'm trying to practice.

The intuition here is that as the automata processes the string, the state itself changes based on the input string. Let's say for a given automata, we stop the automata mid-process. Our little automata here has 2 states, starts at state 1, and transitions between state 1 and state 2 on every character. We stop at state 2.

Q1: What can we infer, based on the state, about the characters the finite automata has already seen?

Q2: Can we fully reconstruct the string?

",1519352662.0
CrypticSolace,"Okay so I'm currently taking a similar course, so take my answer lightly. 

My first thought is, yes, a machine does represent what happened earlier in the string due to the nature of a Finite Machine. As it goes through an input, the machine moves to the appropriate state. So I would say that our transitions from one state to another (which could be a state moving to itself) is what represents the data previously processed. We can't necessarily back track, but we know that the processed data of the string led us to this state in the machine. 

But then again. This question is weird. And I feel like I'm talking out my ass. But maybe as long as you defend your statement you'll probably get some credit. 

Let me know if you find the answer. I'm curious now. ",1519350827.0
,[deleted],1519350443.0
ReneDiscard,Why is this sub kinda slow when /r/cscareerquestions is pretty active?,1519373612.0
fancytuna45,"Which crash course/convention/hack-a-thon in the United States would anyone here recommend going to and why? I'd mainly be interested in something teaching basics of a language or certain practical skills; however, simply just checking out new advancements and projects made by other computer scientists would be quite fine too. The closer to NC the better :)",1519379121.0
rexyuan,"What are some resources to teach myself system&network programming to compensate the lack of ""lab"" courses?

I'm a senior in CS and have been taking more theoretical courses over the past four years while not taking any of the labs. Recently I have found myself in a quite difficult position-I'm not as good at theoryCS as math majors but I'm also not as good at practice as my peers.",1519820453.0
HeraclitusZ,"Firstly, I'm glad that the courses you don't find relevant to your field do not seem to include math and theory courses, which too many people don't realize is important to CS.

As far as the educational content of the degree, you will find that computer science at a university is typically an all-encompassing degree ranging from theoretical math to software engineering and everywhere in between. So it probably isn't as if CS is a worse degree for software engineering. Ironically, it is typically the opposite, as software engineering specific degrees will have a greater tendency to cut out the important theory that is necessary to developing new tech in such a fast-moving field, and instead favor teaching a practical understanding of current tech. BUT, these are just vague tendencies based on name, and they could just be the same. It will come down to really looking at what the degree programs offer/require and how prestigious you care to be. 

That's probably as much as I can comment with the detail I have.",1519335086.0
rdldew,"Personally, I would go with the accredited school. You’ll likely have a better, more thorough education. Someone else said this already, but I’ll say it too; those extra classes that seem useless will turn out to be a huge help in understanding or how to do other parts of your typical job. I hated those courses at the time, but I totally see the relevance. 

I remember reading this article when I was considering the route to go. [this link ](http://www.cnn.com/2010/LIVING/worklife/03/29/cb.employers.online.education/index.html) 

Good luck on everything! ",1519352946.0
midgetparty,"Go CC and then state university. 

Avoid this one: https://en.wikipedia.org/wiki/New_England_Institute_of_Technology#Accreditation_and_curriculum",1519339326.0
sdg_eph1,"Honestly, I'd be very wary of any institution that accepts all applicants. And, it's pretty hard to give advice not knowing what exact universities these are; ""technical university"" could mean a wide array of possibilities from diploma mills to actual good schools.",1519336261.0
jake_morrison,"Software Engineering about managing the software development process in an analytical way, so it makes sense for it to be a masters degree program. You are better off studying Computer Science first. You will not get a job fresh out of school as a manager.
The State University seems better. You will get a more general degree, and it will look better on your resume. Technical schools have a ""vocational"" feel to them, and it can be hard to distinguish good ones from weak ones. ",1519364910.0
HellAintHalfFull,"Take this for whatever it's worth, but I've been a software engineer for over 20 years, and a manager for many of those years, and I have never hired a single person from a technical school. I'm not a snob about education at all, but I do require a good understanding of CS fundamentals and not just an ability to write programs, and I've just never gotten a qualified applicant from a technical school.",1519387519.0
nerga,"There are legitimate technical Universities, MIT, RIT, RPI, WPI, etc. But they don't really differ from a normal University much at the end of the day. That said, this school does look more like a diploma Mill and not a real University. I would definitely go with the public University.",1519390778.0
thecakeisalie1013,"I think this really comes down to what you want. At my university, software engineering and computer science are almost the exact same thing. But the technology school you listed doesn’t offer a software engineering degree, it offers a software engineering technology degree. It’s a little lower level in my opinion because it involves more handling IT systems for departments as opposed to developing software. But some people really enjoy the IT side of things! In computer science, degrees only mean so much. Experience is much more important. If you were to go to community college, you could have some more time to learn programming on your own a little and find out what kind of stuff you really enjoy. I would stress that I do not believe the New England institute of technology is a software engineering degree and I do not believe it will prepare you for the same kinds of jobs as a state university. ",1519372639.0
maxymczech,"I am from Ukraine, I have studied Czech language for a year and now I study an undergraduate course in Cybernetics and Robotics in Prague. Education here is top notch level (by my standards) and it's free if you do it in Czech.",1519333173.0
beansandgreens,"Not sure I'd say I'm a fan, but I consider his article (rant?) ""Agents of Alienation"" a must read. It's one of the papers that shaped my work. I've forced a number of colleagues to read it. They usually hate it, but it always leads to productive discussions. ",1519347842.0
xxtruthxx,"I'm a huge fan of his thought process and ideas. He's different from most Silicon Valley heavyweights in that he still represents the positive, hard-working, creative, less money motivated, etc. generation of software engineers that created the first wave of innovative products that millions of people used in the world!",1519412527.0
exorxor,What you are doing is not called research.,1519512005.0
metaphorm,"vague questions like that are intentionally posed as invitations for you to flesh out the requirements by asking good questions. I would respond to a question like that by asking questions kinda like these

* what is the scope of the service? is this an intranet service? is this a web service open to the entire internet? 
* what are the expected clients for the service? who are the expected users?
* what kind of information is going to be received and stored? how will it be structured? textual or multimedia data?
* what kind of querying capabilities are needed to retrieve data after it has been stored?
* what are the anticipated security and legal compliance requirements?
* what are the anticipated performance requirements?
* what are the anticipated budget requirements?
",1519316880.0
rcfox,"That's called a system design interview question. The interviewer is looking for you to work with them to establish a set of requirements (number of users, response time, etc) for the system and then propose a solution to address them.",1519314339.0
crabbone,"This is super vague... I'd ask them to elaborate on the kind of service they want to provide. Based on that, you may guess which consistency model might fit the storage requirement. And once you know that, you could just take some example storage with that consistency model and describe that without naming names.

I mean, distributed filesystem has very different storage requirement than that of a distributed CDN, or collaborative text editor, or data analysis / modelling software.  There's no one size fit all solution.",1519314429.0
JobsHelperBot,"*beep beep* Hi, I'm JobsHelperBot, your friendly neighborhood jobs helper bot! My job in life is to help you with your job search but I'm just 158.2 days old and I'm still learning, so please tell me if I screw up. *boop*

It looks like you're asking about interview advice. But, I'm only ~29% sure of this. Let me know if I'm wrong!

Have you checked out CollegeGrad, HuffPo, LiveCareer, etc.? They've got some great resources:

* https://collegegrad.com/jobsearch/mastering-the-interview/the-eight-types-of-interview-questions
* http://www.huffingtonpost.com/rana-campbell/10-ways-to-rock-your-next_b_5698793.html
* https://www.livecareer.com/quintessential/job-interview-tips
* http://people.com/celebrity/worst-job-interview-answers-askreddit-answers/
* https://www.thebalance.com/questions-to-ask-in-a-job-interview-2061205",1519360422.0
yyzjertl,Can't you do this by just solving the [orthogonal Procrustes problem](https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem) (using the SVD) and then checking if the resulting distance is 0? Or is there something I'm missing in the setup that prevents this?,1519406615.0
offByOone,Couldn't you just check how far away each point is from the origin? If they are equally far away then you could rotate one about the origin in some way to get the other.,1519317612.0
,[deleted],1519327325.0
PM_ME_UR_OBSIDIAN,"Anything used for language-aware diff, operating at the level of the syntax tree?",1519407604.0
TomvdZ,"CLRS Introduction to Algorithms is pretty much *the* book on algorithms, Algorithm Design by Kleinberg and Tardos is also quite good.",1519293719.0
OriginalPostSearcher,"X-Post referenced from [/r/bioinformatics](http://np.reddit.com/r/bioinformatics) by /u/niemasd  
[Data Structures: An Active Learning Approach (new MOOC from UCSD)](http://np.reddit.com/r/bioinformatics/comments/7zdgyz/data_structures_an_active_learning_approach_new/)
*****  
  
^^I ^^am ^^a ^^bot. ^^I ^^delete ^^my ^^negative ^^comments. ^^[Contact](https://www.reddit.com/message/compose/?to=OriginalPostSearcher) ^^| ^^[Code](https://github.com/papernotes/Reddit-OriginalPostSearcher) ^^| ^^[FAQ](https://github.com/papernotes/Reddit-OriginalPostSearcher#faq)",1519293145.0
Sir_not_sir,"What sort of OS programming? As long as you build for ARM architecture, you can do anything you want on a Pi.",1519265644.0
sailorcire,"You got the whole enchilada there!

If you mean bare metal, then any language *should* work assuming you have a compiler for it. That said, the boot sequence is weird on Pi's as the GPU handles it. Here is an unvetted link:

http://wiki.beyondlogic.org/index.php?title=Understanding_RaspberryPi_Boot_Process

IIRC, the GPU code is a blob and not open source (if that matters to you)


--

If you mean Linux hacking, then yes.",1519265942.0
dasdull,"Büchi recognizable sets are closed under complementation. Thus we can just as well show that the set of all words which are not eventually periodic is not regular.

This can be done by proving that every nonempty regular language must contain an eventually periodic word.",1519299894.0
harlekin-,"As an idea for a proof using only an automaton argument: Suppose there is a deterministic parity automaton for that language. Then you can find two words that loop in the automaton from the same state with different words on each loop (exploit finiteness of stateset). Now you move along the first loop-word once and then along the second loop-word, then along the first loop-word twice and then along the second one, then along the first one thrice, etc. Since both loops were individually accepting, the union of parities is still accepting.",1519322841.0
TheHowe,"If you're looking to go into commercial software development, then these kinds of skills are absolutely critical. I would 100% do this. Being able to write good documentation means your users don't bother you when they have questions that could have been answered by the documentation.",1519257528.0
skelterjohn,"Technical writing is not just for documentation. It's also for design documents. 

Any time you need to convince someone or some group that some design, decision, or course of action is correct, you're going to have to write about it.",1519260830.0
ianwold,"There's room for a documenter on every team, it's incredibly helpful.",1519259958.0
satanikimplegarida,"Very helpful!

Technical Writing helps you improve communication and makes the dissemination of every kind of written information much more potent. It will furthermore improve your abilities as an editor too, should you ever need this, e.g. for paper/proposal/standards editing.

As a researcher, my technical writing and editing skills have been far more appreciated than my *actual*^* technical skills.

*^hopefully_this_doesn't_reflect_too_bad_on_my_technical_skills",1519273060.0
gixxerjasen,"You will always be documenting something.  Adding to kb, writing instructions, documenting the environment.  Having better skills will definitely help.",1519260330.0
spacegirlmcmillan,"Very. In my internship, the difference between people who wrote documentation well and wrote documentation poorly was very clear. Furthermore, people who communicate clearly do much better in business than people who don't.

I'd see it as a personal investment.",1519265931.0
WeirdEidolon,"In addition to all of the other uses mentioned, if you wind up in a more research oriented position, you will almost certainly writing up reports and presentations for that research.",1519272924.0
eholk,"I spend more time these days writing about what code should be written than actually writing code. Being able to effectively communicate technical ideas is critical, as well as being able to understand what others are saying and why.",1519281778.0
manjistyle,Extremely. The guys in my position that got quick promotions were the ones doing excessive technical writing. ,1519269898.0
jake_morrison,"I studied aerospace engineering, but when I graduated the industry was in a slump, so my first job out of college was as as a technical writer. I taught myself programming, and my career has been as a software developer. 

The skill of writing is incredibly important. The ability to present your work in a way that can be easily understood, particularly by non-technical people, will distinguish you from other engineers and really help your career.

Taking on the work of writing documentation is great, having the title of ""technical writer"" is a bit dangerous long term, you should be moving towards programming work. Taking a class in technical writing is valuable, as is almost any other writing class. I have heard of CEOs who get their managers to take a class in journalism so that they get practice writing and learn to get to the point.

It's helpful if you can get someone to critique what you write, not just doing it. Your university probably has a ""writing lab"" where you could get feedback. ",1519284704.0
MET1,"You have to be able to explain what you need to do, need to have done, what you did do, what you are planning to do... so yes, technical writing makes a huge difference. It is not just for documentation.  Being able to explain some technical thing in terms that people understand will make you seem more trustworthy and knowledgeable than your coworkers. ",1519316481.0
damn_it_so_much,"There is not a lot of overlap between technical writing and coding when it comes to professions. Is writing skills important? Of course. But if a company needs technical writing, they won't hire a dev to do it. They'll hire a technical writer.",1519270934.0
coshjollins,"If what you mean how many bits are read by the processer per clock tick, there's a thing called a data bus which is the processors main way communicating. If the bus is 64bit, then it can pass 64 bits at a time. ",1519251544.0
metaphorm,"it really depends on what data is being read and what program is reading it. not really possible to make general assumptions, but if you really must, then the most common pattern is probably for data to be read in ""words"" which is a grouping of bytes with a size determined by the operating system. a 64 bit OS has a word size of 64 bits, or 8 bytes. ",1519249077.0
sailorcire,"Other people have provided ""correct"" answers, but I want to get to the heart of your question:

Does a byte not cover the range of an octet? If so, then is there any difference between these two decimal numbers: 25 and 025?",1519254438.0
Karimloo,"Maybe I'm alone, but I never felt the graph theory pedagogy I experienced was very confusing or boring (I did think this about plenty of CS topics!). There's lots of motivating examples to pull in, and better/clearer diagrams than this in may places. This just seems like a not very lucid description of theory that is very formalized and that many lectures and textbooks deal with.
",1519317144.0
TheWass,"Depends what you want to do. Some parts of security like cryptography are heavily math focused anyway so doing it together with a math number theory course might actually be helpful, more practice and seeing more of the dots connect from two viewpoints rather than one. But it's up to you if you are enjoying it. Good luck!",1519243598.0
mnestorov,"I think this depends on how difficult the courses are in your university. I am finishing my third year as a CS and IS major with a math minor. Speaking only through personal experience and what others have told me - if you get pass by calc 2, calc 3 and 4 are not that difficult in terms of difficulty gaps between the topics.
There is a benefit to having a mathematical background if you plan on working in field that needs such skills (AI, ML, cryptography, computer graphics, etc). But in terms of creating software (something that isn't based so much on mathematics, such as web apps, enterprise solutions, etc.) it's not mandatory to be an ace in math.
Also the 20 credits you will save are probably from some higher-end math courses and are highly math specific and won't always benefit you in terms of being a great coder.
Having said all of this, I do encourage mathematics as a major! Do not let complexity stop you.",1519251550.0
BrightLord_Wyn,"Do you plan on going into the industry after your undergrad or moving on to grad school? If you are planning on grad school, then you should definitely talk to the grad department you are interested in joining about what they recommend. If you are planning on going into the industry, then I would say tapping the brakes a bit and getting a minor in math instead of a second major is appropriate. Only your first job is really going to care about the degrees, and everywhere you go after that it will all be about your experience in the field.",1519254471.0
nit3rid3,"Change one into a minor. I majored in math and minored in CS and I've had no issues getting development jobs if that's what you want to do.

Double-majoring in math and any other engineering track isn't very easy, at least where I went to school unless you stay longer than four years. And honestly, I don't think you'd be that much ahead of anyone who minored in one or the other.

Otherwise, keep doing the double major until it becomes too much. You're still early in the math so it will get substantially more difficult (or maybe not) and you can then just change one to a minor and focus on the other.",1519267578.0
karma000,"Im considering the same as you. I think I'll do a minor instead of the major. As others have said, grad school and some fields in CS require math. If you're interested in those, taking as much as you can now might be good. Personally, I've decided to go heavier on CS and give myself the foundation in math to learn more advanced stuff as needed later on.",1519258759.0
,You can take summer classes. I did full time summer classes every single semester and I got two degrees in about 3.5 years ,1519262202.0
atreayou,"Most schools offer slightly different degrees and paths. The important part to your employer is if you took the classes required for your job. My employer will only hire if you have completed a decent amount of Math. Also, degree paths with more math are usually nationally accredited. Which is nice if you aren't going to a big name university. ",1519303830.0
dhjdhj,What a great discussion. Thanks for posting. ,1519235078.0
,You'd just need to break it down into the six valid cases out of the nine possible. They'll all look something like (111)\*1(000)\*00 etc. Then just or them together.,1519221482.0
albenzo,"The modulo 3 portion makes it tricky but judicious use of union should make it possible.

((111)\* u 0(000)\*)|((111)\* u 00(000)\*)|(1(111)\* u (000)\*)|(1(111)\* u 00(000)\*)|(11(111)\* u (000)\*)|(11(111)\* u 0(000)\*)

Here the u is concatenating and | signifies union.

Edit: formatting",1519221517.0
arnet95,"Here are some hints for one way to do it.
1. Use the union operator to split it into a finite number of cases, for example n = 1, m = 0 (mod 3). 
2. To generate the string consisting of 3n+2 1's, the regular expression 11(111)* should work.",1519221839.0
Triple_Elation,"Honestly, this kind of exercise being put in a final exam -- not HW1, a *final exam* -- is an outstandingly bad idea, didactically speaking. It requires 5% understanding of the material and 95% elbow grease.

The correct question to ask is whether this is even a regular language at all, and why. Then the student can answer ""Blah blah Myhill–Nerode theorem"" or ""blah blah DFA"", and if someone *insists* on actually trying to compose an explicit regular expression then that's on them.",1519228964.0
spacelibby,"Honestly, my first thought is to make a finite state machine and work backwards. But that might be more effort than it's worth. ",1519237920.0
,[deleted],1519220537.0
_--__,"Hi,

I have removed your post from /r/compsci - as per the sidebar we discourage posts of this nature.  I recommend /r/askcomputerscience or /r/cscareerquestions.",1519207164.0
foreheadteeth,"I'm a math prof. I've never heard of this university so I can only say generalities. There are two points of view on the value of a university degree: you either do it for your own betterment, or you do it for [signaling](http://www.latimes.com/opinion/op-ed/la-oe-caplan-education-credentials-20180211-story.html). ""Signaling"" means putting the degree on your resume to show employers you're good to hire, this seems to be aligned with your question.

For signaling purposes, getting a computer science degree from any university is probably good. You should keep in mind the ""big picture"": there are scam universities (""diploma mills""); these don't add any value to your resume. There are ""elite universities"": ivy league, Stanford, etc. These add tremendous value to your resume and will let you work for Google or Goldman Sachs. Then there's everything in between. This place Southern New Hampshire University looks like it's in-between so the value is intermediate.",1519207873.0
LithiumEnergy,Do the beginners make a discord server using C++ through competitive programming?,1519208425.0
lepuma,/r/titlegore,1519209345.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1519184383.0
oneillcode,What do you guys think of the way I presented that? Was it easy to follow or should I try a different method?,1519184423.0
rosulek,"I'm not sure exactly what you mean by ""minimizes correctly"" but I have a hunch that anything of this form would not admit an efficient minimization procedure.

Even a pattern like ""*a....."" (kth character from the end is `a`) is exponentially more succinct than the resulting minimal DFA, which has 2^k states.

So these kinds of objects are much more like NFAs & regular expressions, and questions related to minimizing NFAs & regular expressions are generally PSPACE complete. But of course this is just an intuition, and the details may be very dependent on specific capabilities of the model.",1519171608.0
orksliver,"I think your intuition seems correct -- mostly due to my view that Petri-nets have equivalent attributes and behavior to the automata you are describing.

From a previous [post about state machines](http://www.blahchain.com/posts/firstpost.html#vector-addition-systems-are-petri-net-equivilant) 

I've been using Petri-Nets to generate a state machine in the form of a [Vector Addition System](https://en.wikipedia.org/wiki/Vector_addition_system)

It seems that your observation about 'position' also holds true with Petri-Nets.
> where the wildcards are pretty much just filler in order to make sure that the ""a"" and ""b"" match in the correct position rather than having any importance themselves

With the Petri-Net - the 'position-ness' of the automata is defined by a 'place' element. 
Which seems to be similar to what you describe here:
> My best idea so far is to sort of build a tree with the required states in the correct position, but that sort of creates a lot of overhead.
",1520517025.0
ScaryestSpider,"I don't know about that, but i know Neir: Automata are possible. I'll go now...",1519169632.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1519162008.0
TomvdZ,"Here's why it doesn't work: different pulses are going to overlap. Consider the network

A-B-C-D-E

where C is the initial city. This happens:

1. C emits a pulse

2. B emits a BC-pulse, D emits a CD pulse

3. C sees *simultaneous* BC and CD pulses. It cannot tell this apart from a BCD pulse. A sees a CB pulse, emits ABC. Similaly, E sees CD so it emits CDE.

4. B receives BCD (from C) and ABC (from A). It cannot tell this apart from an ABCD pulse. It emits ABCD. Similarly, E emits BCDE

5. C receives simultaneous ABCD and BCDE pulses. It cannot tell this apart from ABCDE, so it decides the solution has been found.

If you want the pulses to be distinguishable, you either need:

 - Exponential time to space apart all the possible pulses temporally

 - Exponential resolution in your wavelengths and the necessary processing power to simultaneously detect and transmit exponentially complex combinations of wavelengths.

Moreover, different wavelengths, if emitted simultaneously can't just be separated. You'd have to carefully pick the wavelengths so that any combination of them is distinguishable from all other combinations.",1519156182.0
ImaginationGeek,"If you strip out all the physics and look at this as a computer science solution to the fundamental computer science problem called “Traveling Salesman”, what you propose is basically a parallel implementation of breadth-first search.  (So yes, that is a good idea.)

If you understand that algorithm and then build your physics stuff back up on top of it, that may help streamline your solution.

Although if you’re designing a physical thing for the real world, then you should also revisit the problem statement and understand what it is you’re really trying to accomplish. That understanding is the foundation of any good engineering design.",1519158900.0
0x6c6f6c,How do you know the actual route though? What you're effectively solving is that there is a path through all nodes. Not what the path is. ,1519153471.0
,"Learn Java, get good at the underlying concepts.
Mixing languages too some will just confuse you.

Once you are good at java.
C++ is a next good step as it is the same in many ways.
Then something very different like python or GO

As for the other questions...
Where are you in the world??


Also learn some basic electronics, understand what the chips are doing.
",1519110418.0
cdo256,Why do you want to do a CS degree?,1519110939.0
amberlini95,"Hi! I’ll try to answer all your questions best I can. Graduated not long ago, so while I’m sure some things have changed, I’m sure there’s still similarities to when I was in school. 

After you learn java, C/C++ is a good next step, because they’re so similar. For practical languages, I recommend doing Python. I did my senior project in python, and use it at work. 

I had 6 job offers before graduating. I got 5 of them from networking. Be sure to go to career fairs, club meetings, and anything else that allows you to shake hands and collect business cards. It will make getting a job that much easier. 

Most companies will do internships. It’s probably too close to summer to apply now, but some companies may still be hiring. Again, go to career fairs if your school puts them on. It’s how I got 3 internship offers, and almost all of my job offers! 

Have a bachelors degree, may go back for my masters in the next couple years. Haven’t decided yet. 

I’m in the security field. I enjoy it immensely. I love learning something new every day, and the challenge of constantly conducting research to make sure I’m on top of everything new going on. 

As for advice, grades are important, but almost equally important are what employers refer to as soft skills. These skills are your ability to communicate, to work with other people, and to express yourself effectively. You could be the smartest person in the field, but generally, employers won’t want you if you can’t collaborate with other people or if you can’t socialize. You don’t have to be charismatic or the biggest extrovert, but don’t be a loner and seclude yourself in your dorm room. Try to develop those soft skills. If you have the brains and can communicate effectively, you’ll have employers lining up to snatch you after graduation. 

If you’re not the greatest at socializing (and a lot of us aren’t! We picked this field for a reason after all. ;) ), I’d recommend joining some clubs. Most colleges have student chapters of professional organizations, and they’ll give you a chance to socialize with other students in CS. See if your college has a chapter of ACM or IEEE and get involved! A lot of these clubs will often have professional speakers as well, which will give you a chance to network. If you snag an officer’s position, that’s very attractive on a resume, and can help build soft skills as well. 

Feel free to ask any questions! Best of luck as you pursue your degree! There will be many nights when you ask yourself if it’s worth it, but I can tell you with 100% confidence that it absolutely is! ",1519135176.0
fibbel,"General College advice

If you do go to college, outside of your core CS curriculum take some psychology courses. Especially ones that teach you how to assess and interact with people. 

No one will tell you this but you'll use that the most in your daily life because people are crazy, be it your boss, your coworkers,  your customers, or your friends and the most frustrating parts of your job will involve people. Plus it'll help you in dealing with things on a personal level that programmers deal with a lot e.g burn out, motivation, stress. 

Also mix in something in the accounting realm if possible. Understanding how and why the bean counters think the way they do is another skill that'll serve you in your daily work life. It sucks but accountants run the business world and your projects will go though one at some point. 

I personally don't think you have to have a degree to be a programmer, but I still think most programmers should get one regardless. It's later in your career that you'll need the degree. Don't listen to anyone that convinces you otherwise. You have a time in your life when studying is easy both mentally and in your everyday life. Once that passes it gets exponentially harder to come back to it because life will get in the way. One day you'll most likely want to take on a greater roll, e.g. manage a team of programmers, or move out of pure code monkey work and that's when the degree(s) will be needed. Future you will thank you.

Unless you're really into research IMO there isn't much need for an MS or PhD in CS. You'll learn if this is you pretty early on in your studies so don't fret about that one. Chances are the advanced degree that'll bring more value is an MBA type of degree. 

Best of luck in your endeavors! 😉",1519136090.0
elcapitaine,"> 1) I'm just learning to write code using Java, it's my understanding that Java is used across a majority of apps, computers etc like Windows based tech (rather anything that isn't an Apple product) What are some other coding languages that I should look into to further my studies be it during my free time, or potential classes?

Java is the most common programming language, but so much of that code is just custom enterprise apps. It's good to know, but it's definitely not the only option, and I personally dislike using it and haven't in many years.

To your comment about ""Windows based tech"" - while companies may write apps for Windows in Java, that's definitely *not* ""Windows-based tech"". Windows based tech would be the .NET platform (like C#), although that's moving cross platform too. I can't think of any project at Microsoft written in Java... C++ and C# are be the most common. (Also Java does run on macOS)

A key when picking a language is to pick the best tool for the job. Python is a great general purpose language for both web and command line applications, and also has easy to read syntax which is great for beginners. JavaScript is the primary option for client-side web code, and works in a web server too. C/C++ is great if you need performance. C# is great if you're on Windows. And there are many many others... I haven't even touched on functional programming languages like Haskell or OCaml.

Java is probably the best tool if I need to write a program that runs locally (not on the web) and also MUST work on all platforms without any cod change. For any other scenario I'd pick another language, and even in that scenario I'd probably go Python simply because I prefer it over Java.

For a beginner, I'd always recommend Python first. The syntax is easy to understand which is great, it lets you focus on solving problems which is what CS is really about, as opposed to fighting with syntax.

> 2)Granted, I put in the work, would I be able to find a job after college? 

Depends on where you live. Probably. Note that ""where you live"" could mean where you're *willing* to live - many of the larger companies in the US at least will cover relocation costs. If you live outside the US but want to work here, the main issue is visas, which depends on the country you're from. I don't have any experience with the job market outside the US so someone else would have to comment on that.

> 3)Do any, if at all, of the tech companies out there do internships, say...during summer time? 

Many many do. Particularly the larger ones almost universally do. I work at Microsoft, and hundreds of interns we're here last summer. There are even programs for high school students and college freshmen here, although in the industry those are significantly less common.

> 4)For those that have a degree, how far did you go? Bachelor's and work up the chain? Masters? PhDs?

I have a Master's degree. Decided at the end of it that I don't really like research so much, so I went into the industry.

> 5)For those with a career related to CompSci, what do you do? 

Software engineer at Microsoft. Primary development language I use at work is C++, although I do use others from time to time.

> 6) Do you enjoy what you do?

Very much so. It's a great time. I love the folks I get to work with, and I am always learning something new.

> 7)Advice for a 19 year old college freshman trying to pursue this line of work?

Make sure it's something you truly enjoy. Sounds like you're already on the right track here. Go to classes, do all the work. Apply to jobs even if you don't think you're qualified. Above all else, make sure you like learning. This industry changes fast, far more than most, so it can be a lot to keep up. Even once you leave school, you should never stop learning. If that sounds like your jam, then it's a great field to work in.",1519137741.0
xAdakis,"I am a senior graduating with a degree in Computer Science with some work experience and quite a few personal projects.

**Question 1**:
Java is not used as much as it used to be, but JavaScript is used everywhere. That is the type used in web pages and standalone/server-side like NodeJS. That would be a decent language to pick up as you can do some practical things with Node. 

To get ready for the academic side, C/C++ is the go to language. All of my core classes used C/C++ for everything. 

NodeJS can be easier to work with and prototype things very quickly. C/C++ can deliver better performance, but it takes longer to make something useful and can get very complex.

**Question 2**:
It depends on where you live and/or if you are willing to relocate. For example, here in Knoxville, Tennessee, there are a ton of companies hiring computer scientists and computer engineers. Internships are very competitive, but full-time positions are plentiful. At the last job fair at the University, we had people coming from all over this side of the US interviewing students for a variety of positions. 

**Question 3**:
Again, it varies by location, but the majority of internships are during the summer. If they are paid internships and you do good work, some will keep you on as long as you have time for them. A friend of mine got an internship at a local company during his freshman year, and he practically worked for them part-time until he graduated, then got promoted to full-time.

**Question 4**:
I am graduating this semester, so cannot answer this one very well. . .from what others have told me. . .Bachelor's for a decent chance of getting hired, Masters almost guarantees a job with higher pay (starting at senior level, instead of entry level), and PhD if you want to go deep into theory and teach.

**Question 5**:
Again, my experience is limited but have done some work. . .most of your work will have you sitting in front of computer either fixing problems with existing code, or implementing or expanding existing features. Lots of meetings to make sure everyone is on the same page, and then just grinding away until your task is done.

**Question 6**
There was a point during my sophomore year of college where I ditched video games in favor of working on my own programming projects. It just became more fun to make things and play with things I have made, than to play with things other people have made.  It brings back a lot of excitement that playing Lego as a child used to be bring. I actually look forward to coming home and knocking something else off my TO DO list or experimenting with some new API or library. 

Working for a company will not be all fun and games, you may have to deal with people who are ignorant about how something works- and these people will often be the ones paying you -but with a good passion for solving problems, you'll be able to work through it, pay rent, and continue going.

**Question 7**
Don't slack on your studies, spend the time you need to learn what is being taught. When it comes to programming, don't limit yourself to just what they teach you in class. Too many of my classmates- when faced with actually finding a job and working in the field -have come crying that whatever the employer was looking for was not taught at the school. 

The stuff you will learn in class is the foundation, mostly just good coding practices and how the computer actually works with what you give it. It will not teach you how to develop web sites, video games, or fully-functional operating systems. However, you can take what you learn in class, add a little extra knowledge and ingenuity, and make something great.



",1519160357.0
welshfargo,Learn about databases and become proficient in SQL.,1519226255.0
cdo256,"Solution-1’s recursive function works by ‘deciding’ for each number whether to add it.

Solution-2’s recursive function works by ‘deciding’ which number (if any) to add next.

Solution-1 has one recursive call for each number in *original set*. So it’s recursive depth is always nums.size().

Solution-2 recurs every time it adds an element to sub.",1519108926.0
crazygamelover,"I'm too tired and lazy to do the complexity analysis, but I can explain how it works. Sorry I'm on mobile. So every time genSub runs it adds the current sub subset to the list. So for {1 2 3 4} the first call gives {} before entering  the loop. The first number is added to subset when you enter the loop. Now we begin recursion *inception bwa*. genSub is called again with {1} being added to the list and the loop starting at 2. Then we go deeper*inception bwa*. Rinse repeat till we get to the end. Our current list is {} {1} {12} {123} {1234}. Now stop that rewind it back (usher's got the sound...) Now we are one level back in our recursion. But because we are using pass by reference we need to get rid of some elements so we remove 4 from {1234} and iterate but i = size. We can now exit the loop which pulls us back another layer. We comeplete the iteration in this layer (pop the last element) and begin recursing again ending up adding {124}. You continue doing this till you can get back to the first call to genSub removing 1 and doing all the subsets of {234} rinse repeat.

Think of it like sweeping forward till you hit the end. Then slowly going back and forth till you hit the beginning again. Stepping forward one and doing it all over again. Like a shitty sorting algorithm.",1519110246.0
gravity_low,"I have to recommend that anyone seriously working in the CS field take time to study algorithms at a college level. It's not that you won't do well without it, it's just that it's such a useful toolkit in understanding the world of programming. It may seem abstract, but it will cement concepts that will serve you well in all of your applications, AI included (and maybe especially so).

Since you're a freshmen I think it's a good idea to go ahead and take the algorithms class if you can, even if you aren't as enthused about it as you are about AI. You will definitely have plenty of opportunities to have experience with that in or out of college, but I don't know that a truly academic study of algorithms will be as accessible. ",1519098055.0
VermillionAzure,"> proving algorithm correctness, the algorithmic design domains, efficiency analysis

If you ever want to create your own, sure. Otherwise, you'll be using machine learning algorithms that others create (though understanding their efficiency is also important).",1519098701.0
welshbrad7,"As a programmer, your main general tool will be your ability to visualize the space and time costs of what you write, and how the data structures you use affect the overall performance of the application. That being said, you can get by without a full working knowledge of how to prove correctness, finding recurrences, or having advanced algorithm design techniques on your belt--even if it's an attractive and useful skill anyways.

Machine learning is a highly researched field with a lot of theoretical support behind already-existing algorithms. Take the k-means clustering algorithm for example. You can literally implement k-means in like 3 lines of Python code using existing ML libraries such as SciKit Learn. The implementation is made easy for you and you don't need to concern yourself with the inner workings if you don't want to. 

If I were you, I would pursue my interest in machine learning and focus on developing something interesting, while studying the underlying theory that makes it work. Tweak the parameters and try to optimize your solution to a problem by understanding which model fits the problem best. As a first year, you are inevitably doomed to take algorithms anyways, so I would save the effort for the actual class itself.

Machine learning doesn't require advanced algorithm knowledge, so you can play with it for now. Algorithms are important, but you can hold off on that for a while without being totally confused. 

",1519098908.0
stathibus,"Basically, no.

There's a lot of practical skills you need for ML. Best way to spend your spare time at this stage would be learning to use popular tools.",1519097459.0
zergling_Lester,"Am I missing something, or is the OP missing https://en.wikipedia.org/wiki/Priority_queue, with 8 variants per the wiki page, and the basic heap variant probably beating everything in the OP anyways? 

And it's not even that Heap is a recently invented data structure, one of the links says ""Williams, J. W. J. (1964), ""Algorithm 232 - Heapsort""""",1519163034.0
HellAintHalfFull,"[Here](https://www.reddit.com/r/compsci/comments/7ypu62/where_is_the_best_place_to_read_about_recursion/).

Sorry, I couldn't resist.

Seriously, try [this](https://www.topcoder.com/community/data-science/data-science-tutorials/an-introduction-to-recursion-part-1/).",1519072405.0
jmite,"So, the heart of recursion is not, in fact, self reference, but deconstruction: calling yourself on a smaller instance.

Once you've got a grasp of the basics, there's a paper called ""Bananas, Lenses and Barbed Wire"" that gives an in depth look at recursion schemes, patterns of recursion that are guaranteed to halt.",1519088293.0
schwarzfahrer,"One of the most seminal texts in computer science is the Structure and Interpretation of Computer Programs. In it, you’ll find (among many other things), some great detail and theory regarding recursive programs. [Here is a great ebook version](http://sarabander.github.io/sicp/)",1519083076.0
DonaldPShimoda,"A little while ago, I wrote an introduction to writing recursive functions in /r/learnpython. [Here is a link to that comment.](https://www.reddit.com/r/learnpython/comments/7ojz3w/trouble_with_recursion/dsa1uvn/) I'd be happy to answer any questions you have about it! :)",1519089486.0
dooges,[The Little Schemer](https://mitpress.mit.edu/books/little-schemer) - one of my favorites.,1519097088.0
clownshoesrock,"Always start with a base case.  As recursion must stop at some point (infinite loops aren't recursion).

Let's say you want to know what the ""sum from 0 to 888"" is using recursion.  888 being an arbitrary number.

First you build a ""base case""  **People screw this up so amazingly often**

    addup (int input){
        if input <1 return 0
    }

ok now this break pretty bad for anything, because it doesn't do anything for real input.  So now we make it hunt for the base case.

    addup (int input){
        if input <1 return 0
        else return addup (input -1)
    }

This still returns zero, but now it hunts down to a ""base case"" which is zero, and always goes there.

Now we break down the problem of adding all numbers to 888 downto zero into parts.

Adding 888 down to zero is the same as adding 888 plus (887 downto zero)  **Understanding how something is related to a smaller version of itself is vital**


    addup (int input){
        if input <1 return 0
        else return (input + addup(input -1))
    }

This code works.. it takes 888, then adds it to 887..0 which is found by adding 887 to 886..0  and so on till it's just 1 plus 0..0, which returns a zero, and starts the whole addition process moving.

",1519107508.0
noppanit,"I watched this https://youtu.be/gl3emqCuueQ and the whole series 8, 9, 10, 11",1519080968.0
,"When I was learning it my friend said:

“Recursion is easy, it is just like recursion only easier”",1519099601.0
TheWildKernelTrick,"Q: What is Benoit B. Mendelbrot's middle name?
A: Benoit B. Mandelbrot

Use substitution and you have a recursive grammar. Now use the pumping lemma to prove such.",1519107360.0
Workaphobia,"> So far my approach has been to reduce it to the Halting problem with the following argumentation:

Terminology nit: reduce the Halting problem to it. (Everyone makes that mistake, many many times.)

> If <M> does not halt on w, either L(u,v) or L(v,u) would loop.

No, you assumed that L(u, v) is a deciding machine, so it wouldn't (if the assumption held). The way you set it up, R(<M>, w) always halts.

The approach I'd use is: If this thing were semi-decidable, you'd have a machine that could tell you affirmatively (but not negatively) whether u is in the language and v outside the language. It'd kind of be nice if u and v could mean the same thing, so you don't have to worry about picking a suitable arbitrary v (if you pick v wrong, then you can't interpret the answer to be about u). But without knowing anything about the underlying machine, you can't pick v.

So change the underlying machine. Transform M into another machine M' that works just like it except it does something extra, while preserving the property that if M halts/loops on x then so does M' on the modified input. Then given a single input w, you can form both u and v from w.

And since we're assuming only that L(u, v) is semi-decidable, any machine that uses it to solve the Halting problem would have to try both the notion that w is in the halting set of <M>, and that w is outside the halting set of <M>, in parallel.",1519048968.0
rosulek,"There is a stronger version of Rice's theorem, which is as follows:

> If a property P is *non-monotone* then { M | L(M) has property P } is not even recursively enumerable.

A monotone property is one where: If A has the property and A ⊆ B, then B has the property. Your property is clearly non-monotone (take anything with the property and add element v for a counterexample) and hence the associated language is not recursively enumerable.

For a presentation of this stronger Rice's theorem you can check out Kozen's book (Automata & Computability) or [these slides](https://drona.csa.iisc.ernet.in/~deepakd/atc-2011/tm-reductions-rice.pdf).

",1519069496.0
semidecided,Maybe,1519068108.0
Baalinooo,"Sorry for intruding, but I'd like to know: in what courses can I learn these things?",1519067781.0
pendolare,Why I'm even subscripted to this place? ,1519075668.0
Bobknows27,"Sorting integers can be done in linear time with counting sort, so in addition to being a sort only applicable to integers, this sort isn't even the fastest such sort. 

Also the bound he put on radix sort isn't right. You do need log(max #) passes, but there is no reason that as your list grows that has to grow. Duplicates are allowed by most common sorts.",1519065966.0
flexibeast,"Full abstract:

> We present gradual type theory, a logic and type theory for call-by-name gradual typing. We define the central constructions of gradual typing (the dynamic type, type casts and type error) in a novel way, by universal properties relative to new judgments for gradual type and term dynamism, which were developed in blame calculi and to state the ""gradual guarantee"" theorem of gradual typing. Combined with the ordinary extensionality ($\eta$) principles that type theory provides, we show that most of the standard operational behavior of casts is uniquely determined by the gradual guarantee. This provides a semantic justification for the definitions of casts, and shows that non-standard definitions of casts must violate these principles. Our type theory is the internal language of a certain class of preorder categories called equipments. We give a general construction of an equipment interpreting gradual type theory from a 2-category representing non-gradual types and programs, which is a semantic analogue of Findler and Felleisen's definitions of contracts, and use it to build some concrete domain-theoretic models of gradual typing.",1519009325.0
jmite,"From the abstract, looks very cool. Anyone know offhand ihow this relates to [Abstracting Gradual Typing](https://www​.cs.ubc.ca/~rxg/agt.pdf)?",1519021647.0
hsfrey,What is the problem this is designed to solve?,1519024914.0
timmyotc,Head on over to /r/learnprogramming ,1519000719.0
,[deleted],1519016352.0
ProgramTheWorld,Does that form live in a web browser? It sure looks like some angular UI to me.,1519050134.0
MeshachBlue,"I'm keen for any feedback anyone might have. Cheers, Simon.",1518999738.0
MeshachBlue,"So originally there was a comment here regarding the choice of licensing. I am aiming to have an AGPL-3.0+ license with additional terms from the Apache-2.0 included. How I went about it previously came across in a confusing manner.

I have adjusted my wording of the license text (https://github.com/SimonBiggs/scriptedforms#software-license-agreement) and I would really appreciate any feedback people might have.

Cheers,

Simon",1519105510.0
lrem,"There are plenty CS researchers who have some interest in this kind of topics. However, Meltdown is a bug in a particular implementation (only Intel, only because they decided to sacrifice security in favour of speed), with no interesting theory. Spectre is more fundamental. The issue underneath it has been considered since at least 2002, with some pretty good practical attacks shown in 2013. The development in 2017 was about making this practical in general.

Edit: I've glanced at the references of the paper itself. The timing attack of 1996 mentions branch predictions, but doesn't really work on them (uses them as one of the explanations that go into a black box they work on). But *Predicting secret keys via branch prediction* (Feb. 2007) seems closely related.",1518969367.0
MrPutuLips,I'm not sure if this should be more directed at computer scientists or computer engineers.,1518982927.0
sailorcire,"I would say not necessarily. The cache coherency idea is good, but it was how chip manufacturers implemented it is what caused the problem.",1518968401.0
clownshoesrock,"Perhaps, but it's more of a security style bug.  I think the people who hardened SSH would have a better shot at it.  I think that because writing code that implements SSH encryption is straightforward, implementing securely it is really hard.  There are all sorts of side channel attacks against SSH, so I think this bug is more in their wheelhouse.",1518983590.0
jmite,"Formal verification is a huge field with plenty of brilliant researchers and relatively high usage in industry. But there is no silver bullet.

Meltdown and Spectre could have been found if P and NP were equal, the halting problem were decidable, or testing budgets were infinite. But in the real world budgets are limited, exhaustive testing is slow or impossible, so the most likely bugs are found but some are missed.",1519088458.0
Dr_Legacy,"Is it me, or is this article stupid?",1518972117.0
IJzerbaard,They're really not that rare though.,1518962202.0
,[deleted],1518971612.0
CurryMath,"I don't understand why this has been upvoted. Invariants are first-semester stuff and they do not grant ""superpowers"" (they are usefil tho).",1518974681.0
gwern,"That's a tough one. I've read a number of Solomonoff's papers, and papers on optimal discrete sequential search too, but I've never seen this square-root trick for a memoryless search. I've tried a few dozen searches in Google and Google Scholar for various queries and skimmed through some more Solomonoff paper titles, but also nothing. Searching for the result itself doesn't turn up anything either.

The audio is hard for me to understand but doesn't Minsky say in the box discussion twice something along the lines of 'people don't know this' or 'I sometimes check to see if anyone has realized this yet'? That suggests that Solomonoff might never have published a paper on the result. Minsky did know Solomonoff personally for decades, after all (eg at the Dartmouth workshop), and it was a very small field. (I suppose that means it might be worthwhile for someone to read up on reactive/memoryless policies and package it and a few other results up in a paper to get it out of folklore.)

EDIT: nshepperd offers this attempt at a proof:

> Curiously, you can solve it in average _n_ attempts where _n_ is the number of doors, either by Thompson sampling, or by checking doors uniformly at random, so the optimum is actually somewhere between those extremes.
>
> Where _p\_i_ is the probability door _i_ has the treasure, and _q\_i_ the probability assigned by your strategy that you check door _i_, the former is _q\_i_ = _p\_i_, the latter is _q\_i_ = _1/n_.
> So this comes down to minimising E[number of attempts] = sum\__i_ {_p\_i_ / _q\_i_}, subject to sum\__i_ _q\_i_ = 1.
>
> Which is easy once you remember that the [Lagrange multiplier method](https://en.wikipedia.org/wiki/Lagrange_multiplier) is a thing.
> Minimising Lagrange loss _L_ = sum {_p\_i_ / _q\_i_} - λ (1 - sum _q\_i_), results in _q\_i_ = sqrt(_p\_i_) / sqrt(λ). or, basically, ""square root the probabilities to make them more uniform, then normalize"".",1518991754.0
yangfuchian,"Based on additional research,

https://arstechnica.com/gadgets/2018/01/heres-how-and-why-the-spectre-and-meltdown-patches-will-hurt-performance/

^ suggests CPU with only PCID is hard to support

https://mail-archive.com/linux-kernel@vger.kernel.org/msg1546880.html

^PCID-only CPUs were too complex

https://groups.google.com/forum/#!topic/fa.linux.kernel/IV8D8p9uR9g

^INVPCID only speeds up TLB Flush (so not really different from PCID, but just another invalidate instruction added)

http://hypervsir.blogspot.com/2014/11/improve-performance-for-separating.html

^ seems to suggest that INVPCID only ""assists"" in programming

Now, what I don't understand is how the invalidating PCID is supposed to help with programming, or make PCID more efficient, and I don't think I am ready to read the Intel SDMs...",1518942059.0
worst,"The tier-1 CS venues for different communities are listed if you expand the areas on the left side of http://csrankings.org/

It's not 100% comprehensive, but there are few that would argue any venue listed there is not ""top"". The complaints with the listings are more complaints of omission.

Be warned though... Conference registration fees are not cheap, and most talks are going to be epic boring unless you are really into that community.

Edit: just to add on, if you happen to be in California, http://icwsm.org is at Stanford this year, and is one of the cooler and overall accessible venues in the general sphere of CS.

It also happens to be one of the complaints of omission from http://csrankings.org :)",1518897264.0
CorrSurfer,"What kind of conferences are you thinking of? The best scientific conferences take place in various places on all continents, so taking your CS club to a few excellent ones will most likely not work simply due to cost (many of them are in the US every second year or so).

But I learned that the term CS is also frequently used to mean IT technology, and then non-scientific developer's conferences such as DockerCon could also be in scope, hence my question.",1518901089.0
IndependentBoof,"Most of the top CS conferences are affiliated with an [ACM Special Interest Group(SIG)](https://www.acm.org/special-interest-groups). However, that also means that they tend to be fairly specialized.

You might also consider a few other options:

* [CCSC](http://www.ccsc.org/) has small, regional conferences that tend to be more student-friendly (less concentration on research and more on practices in CS classrooms)
* [Grace Hopper Celebration](https://ghc.anitab.org/) ""world's largest gathering of women technologists""
* [Tapia Conference](http://tapiaconference.org/) ""Celebration of Diversity in Computing""

As /u/worst pointed out, conferences tend to be pretty expensive. However, if you plan ahead, they usually have discounts for students and sometimes have opportunities for scholarships and/or volunteer opportunities so they can attend for free.",1518909306.0
HalfTime_show,OsCon!,1518918120.0
Qbit42,Sigraph is the go to graphics conference,1518895086.0
nerga,"This is undergrad I take it? It might be hard to find relevant conferences honestly. CS conferences are usually a combination of expensive, and also specialized, and are more geared towards graduate students, professors and researchers. On the other end of things, there are plenty of software engineering conferences, but they are more focused on software professionals, and are usually geared more towards the IT end of things, and vendors trying to sell your company things.",1518910209.0
GuyOnTheInterweb,How about the [Web conference](https://www2018.thewebconf.org/)? You'll be surprised about how little the talks mention the web and go straight to CS instead..,1518909604.0
royalhawk345,"Big Ten probably, but I'm biased.",1518943574.0
jmite,"For Programming Languages, the top conferences are POPL, OOPSLA, ICFP and PLDI, each with a slightly different focus. I think ECOOP is pretty high up there as well.",1518996686.0
snekslayer,NIPS,1518895417.0
redditEnergy,If you like C++ I love CPP con videos. Going is a little pricey though...,1518911864.0
mustang0168,"I love LambdaConf in Boulder, pretty good functional programming conf",1518893805.0
steviepatel,Reflections | Projections at UIUC if you're interested in free student-run conferences,1518948977.0
NoobStudios,GDC,1518972244.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1518878249.0
carette,"For practical, Algorithms. For true enlightenment, SICP.

You will be taught algorithms in school. They're great, you need to know them. But they are 'knowledge'.

SICP will radically change the way you think about programming. You will no longer be just another hack, but have much deeper understanding of program design.

Knowing algorithms inside out will make you an excellent programmer. Understanding what's in SICP will make you an excellent designer.

Your choice.",1518879394.0
hamtaroismyhomie,Read the textbooks that your school's courses actually use. Do exercises too.,1518853520.0
priestlyemu,"You can watch Sussman and Abelson teach sicp here. I highly recommend

https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-001-structure-and-interpretation-of-computer-programs-spring-2005/video-lectures/",1518888562.0
ankush97,Good Luck :D,1518932315.0
fried_green_baloney,"The Sedgewick book is breathless run through what you need to know.  SICP is more leisurely.  Sedgewick first, unless you are very mathematically inclined and have more time.",1519022243.0
xShadowProclamationx,you will need both. start with one move on to the other. ,1518888148.0
sailorcire,"> Was a real tedious task

No it wasn't...it's like three or four things to do with Apache, and probably similar for ngnx.

I'm sure it's a wizard in IIS..

You can force all of your sites to be HTTPS by doing the ol' 301 on your HTTP versions.",1518863340.0
cheese_wizard,"sounds like me. never got above a C in algebra in high school. just find a good homework partner who wants to succeed.  Do your homework together on campus with hopefully teacher office hours and math tutors available in the library.  take advantage of those resources.  just always do all your homework and get a head start on it.  it WILL start to click.  Calculus is just a lot of algebra.  One thing that helped me was a concise reference book of all trigonometric identities, all common math formulas of every type, common calculus derivations, etc.  It helps when things get tricky. I graduated in CS in 2007 having taken Calc 1,2,3 linear algebra, discrete math (Twice), stats, etc. I found linear to be the hardest single topic, pre-calc to be the hardest class given all the different topics.  Calculus at least is back to algebra and just a little bit headier topics",1518831231.0
Zulban,"I often tell people that my degree in computer science is like a degree in ""the mathematics of computers"". So math is definitely a major component of CS studies, despite what people here are generally saying.

However, I believe that almost everyone has the natural ability to learn advanced math. We have this myth in USA/Canada with Jimmy Neutron and [Einstein's Brain](https://en.wikipedia.org/wiki/Albert_Einstein%27s_brain) that you're either born a genius or you're not. This is crap. Every person I know that I consider smart also works hard.

I had an interesting time with my CS degree because I was good at programming but compared to my peers, I was behind on math. So we'd have assignments sometimes that would take people 20 hours but would take me 1 hour. Other times, there'd be assignments that take them 5 hours but I'd have to review half an online course to review the math I needed.

>I really have turned my habits around

That sounds great. On top of ""turning your habits around"" though, realise that being organised and studying is also a skill. There's a ton of research, good advice, and bad advice out there on how to study effectively. Lots of things you've been told your whole life are wrong. A few things to consider:

* Don't study all at once. 30 minutes of studying for 10 days (5 hours) is far better than 15 hours of studying all at once. If you cram you are wasting your time. Those 10 hours could have been spent playing games and it would have been just as effective.
* Don't read articles like ""OMG 12 studying tips you can't live without"". Read long boring articles that are researched, written by people with credentials in that field.
* There are free online resources/videos that are better than any teacher/lecturer you will ever see in real life. Explore a variety of resources to learn.
* Don't listen to random reddit comments too much for advice. Find reputable sources (like long newspaper articles online, or university websites) to learn more about how to study and manage your time. You may want to start a personal organisation system.",1518837010.0
tjorg35,"Go back and master algebra and trigonometry, it will make learning calculus much easier (and probably a lot more enjoyable). The concepts in calculus can be challenging enough, even more so if you have to relearn all the prerequisite material.",1518831875.0
anikan1297,Mathematics is a skill. Practice more and you'll get better!,1518834287.0
RandomHero492,"I sucked at math and now have a minor in math, BS in Computer Science and am 3 months away from my masters degree in Computer Science. It will be hard - but so, so rewarding.
 ",1518837094.0
misterZ3r0,"Unless you plan on pursuing a Ph.D in Comp Sci instead of going to work after your B.S. or Masters, don't stress. However, some bleeding edge CS may require you to have a much deeper mastery of math, for example, when Machine Learning was first gaining a foothold, however, over time tools and APIs get developed so that run-of-the-mill software engineers can apply the same techniques without the theoretical background. Now, if you want to be the person to design those tools and define those APIs, also make mucho bucks, then you are going to need a lot of math. 

BTW, Computer Science is technically a branch of Mathematics. 

Edit: Computer science as taught by many schools in most courses that comprise undergraduate level is more appropriately called Software Engineering because, strictly speaking, coding or programming isn't computer science, but the field of computer science as an academic discipline is an offshoot of mathematics. The luminaries in the field, especially early on, were mostly mathematicians. ",1518863992.0
cclites,"Calc kicked my ass - Made it through Calc I, but had to take a couple tries at Calc II. 

Get as much help from your professor as you can -",1518831512.0
JH4mmer,"Watch the 3 Blue 1 Brown videos on YouTube. He's got lots of great videos, including calculus. Khan academy has lots of good ones too. Most people who fail at it are really just bad at algebra, so do what you can to make up any deficiencies with that first. You'll be in a much better shape then.

Side note: it is possible to be a competent programmer without a strong math background, but it's absolutely necessary to be mathematically literate in certain computer science specialities, especially machine learning, image processing, NLP, and even in 3D game design. If you can find something to latch onto that makes you think ""wow, this is cool"", it will be so much easier. Best of luck!",1518841654.0
vn-nv,"Be aware that you might need to spend much more time on your math subjects and allow for that. Go to every class, ask every question you have and do every practice problem the teacher sets.

I was so nervous before my first math class, it’s not my strong point but being aware of that is the  key.  To date my marks in Math have been my highest at university but it is easily 1.5x the work for me compared to other subjects.",1518835906.0
barsoap,"Procure the book ""How to study as a maths major"" and read it.

I'd say your personalised tl;dr of that book is ""How to stop worrying and love proofs"".

(There's also editions titled ""How to study for a mathematics degree"", same book. Author in both cases is Lara Alcock)",1518861478.0
0day1337,im not a math genius either. but its worth it to grind it out! my passion for software motivated me to push through it :),1518854695.0
Nichololas,"You don't have to be good at maths to do CS. Doing CS will help you make sense of some maths. Both are wide fields. If you like CS then do it. Source: suck at maths, phd in CS. ",1518831973.0
nathan424,"Coming from an engineering student with a CS minor who also wasn’t naturally good at math, this is totally fine. Understand that there is a lot more to the major, but calc is a requirement. You’ll have to put in extra time if it takes it, and being able to put in the time to do the hard things is even more of a pre rec than any math class. 
Good Luck!",1518831266.0
LordBlackberry,"You’ll do fine in CS without being great at math. I got my BS in CS and then did my minor in Business instead of math (I was an utterly average math student) and it made me qualified for jobs. Just get a minor (if your school requires it) in a different field. I recommend an area you’d like to code in (physics, chemistry, microbiology) or do a business minor (it’s always good to know how the people telling you to code think). ",1518834083.0
zaccus,"You're going to crush that calc man. Just decide right now that you're going to do the problem sets. That's the only way to learn it. 

Studying for tests the night before isn't going to cut it, and it isn't necessary anyway. If you have discipline and do your problem sets you'll be ready for the tests. So do the problem sets. They can be tedious and boring. That's ok. Do them anyway.

You don't have to wait until fall either. Find a pdf of spivak or something and get a head start now. And do the problem sets! ",1518837654.0
Lord_Fredrick,"Im not a CS major but I have taken through Calc III. The most difficult part of calculus is the algebra that's required. Make sure you know your trig really well, like really really well, as you will be eventually ADDING trig to problems to solve them. You have to have a strong base in algebra to get through Calc. Also remember practice, practice, practice. Knowing a formula is very different than using it. Also as you use it you are helping your self remember it. And keep your work organized, if you can't follow your work numbers and signs just disappear, that means an incorrect problem.  Good luck. ",1518839642.0
JimmyTwoTaps,"The thing about college is that it's what you make it. If you currently have problems with math you can still get a CS degree, but you will need to do more studying and focus on the math you don't understand. The college you choose should offer tutoring at no cost, and the professor is there to help. You can also utilize online resources or 'For dummies books' for extra help. Khan Academy is a really good free resource. 

I wouldn't worry too much about it. I was horrible at math, but Calculus worked out for me.

Also, if you don't mind me asking, what college are you attending? Some offer degrees that are similar to CS, but require less math.",1518840692.0
Iroastu,"I was always awful at math and did CS. Just use tools, tutoring, any help you can get, study your ass off and you'll be ok. ",1518840860.0
CARGLE,Got a D in pre-calc once and failed calc I once but still graduated with a CS degree on time. Not giving up is a big part of achievement. ,1518842199.0
Bowlcutz,I owe my calc grades to PatrickJMT on youtube.,1518849989.0
ianwold,"I'm wishing you good luck here! In my calc 2 course, I was slated to fail but I was saved because I answered C on every question on the final. I passed my CS program last year and I have the best job in the world. There's more like you and there's hope! Study hard and stay strong",1518851183.0
AvPrime,"Be ready to study a lot. But you can make it if you stick to it.

I am a non traditional student, so I hadn’t taken math in years when I started back as a freshman. Math was always my weakest subject in school. I was so far behind that I had to do remedial algebra. I also had to spend a semester taking college algebra and trig before I could start calc. Now I’m a CS senior taking my final math course.

There were, and still are, some rough times. Some of the material comes easily to me (linear algebra). Some of it is difficult (a good third of calc). I have never felt dumber in my life than I have in my math courses. But you can learn it if you’re driven enough.

My advice is to:

* Space out your math courses throughout your college career. Don’t overload a semester with too much math. These courses tend to require more study time and practice.
* Stay diligent with your studying. Do all the homework. If you have access to extra practice problems, work as many of them as you can stomach. It’ll pay off.
* Find a tutor. I used to think only dumb kids needed tutors, but now I’m the dumb one! And honestly, who cares what anyone else thinks. Either visit your university’s tutoring dept (if they have one) or find a private tutor. Receiving an extra, personalized lecture every week will help you stay on top.

Good luck and don’t give up!",1518851345.0
ItzWarty,Check if your program leans more theoretical computer science or practical computer engineering. ,1518859675.0
flikibucha,I’m not naturally good at math but I put the effort in reasoning and learning. Takes more time.,1518863590.0
Prcrstntr,Trying to find a Software Engineering program instead of computer science may also be a good idea. ,1518864094.0
GNULinuxProgrammer,"Math and CS are inseperable fields. Without understanding certain Math you simply can't even begin to understand CS. But that's ok, use your motivation to learn CS to learn Math. You'll see that math is not as bad as people make it. In fact, it's beautiful!",1518867396.0
foreheadteeth,"I'm a math prof and I just had a master's student in my office who studied a lot but got a bad midterm grade. Studying a lot is good but he's not getting the results he wants, which means he's studying wrong.

[This guy explains it better, especially the distinction between rote memorization vs comprehension.](https://www.youtube.com/watch?v=IlU-zDU6aQ0&t=19m50s)",1518867994.0
seitengrat,"i graduated with a CS degree but all my math subjects were a struggle. unlike others, it never ""clicked"" on me (i.e. once you ""get math"" the struggle disappears, nope didn't happen). I did my best, studied hard for the exams and barely scraped by. Never cheated though!

What I did was I studied the material as soon as I got home. I had bad handwriting, so after class hours when I'm back on my study desk I just rewrote my notes by hand. It helped me a lot to retrace how the lesson flowed from beginning to end. 

I also answered a lot of practice questions, asking my friends in uni when I'm stuck. I also organized a number of study groups esp when exam times were coming. 

Trust me, a lot of the time, having a great study group will do wonders for your confidence. I used to ""punish"" myself for not doing so well in school by not interacting with others, but when my friends came into my life they helped me to help myself. I stopped getting pressured, took it easy, and I slowly regained my focus. ",1518868706.0
,"I was shit at maths, still got a cs degree. BSc. (Hons.) Internet Applications Development to be precise.",1518869422.0
jaweissavl,"As someone in the same boat but with a very good job, I would say make sure to just work hard and just critically think it through and eventually (at least for me) the mathematical logic started being easier for me.",1518871907.0
deadmushrooms,"Sounds like me. You'll make it, just put some dedication to math in the first semesters.",1518873638.0
RebornPastafarian,You don’t have to be amazing at math. ,1518879472.0
dusklight,"It might not be that you are bad at math, you just might have had really bad teachers. Read https://www.maa.org/external_archive/devlin/devlin_03_08.html for more context. Basically a lot of highschool math is optimized towards getting you to get good grades in the exams instead of trying to actually teach you anything. My advice is to read something like Cryptonomicon or Anathem, step away from the textbooks and learn what math is really about, by discovering the rules yourself instead of being forced to memorize them. For example you can learn calc by making your own physics simulator in a 3d engine. Can learn some linear algebra and matrix math that way too. ",1518879843.0
pythonicus,"If you can think abstractly enough for CS you can think abstractly enough for your math requirements. Don't tell yourself it's too hard: it's a lie. Ignore your internal FUD and do the work. 

Consider using the Feynman Technique. Be engaged with your peers and your professors. 

Also, maybe this will be helpful to you:

https://www.math.uh.edu/~dblecher/pf2.html

Good luck!

",1518880762.0
MarimbaMan07,"Do web development. You literally don't need good math to do be an amazing web developer. No offense anyone, I work at a massive e-commerce site and at least in my experience, no one knows difficult math.",1518885313.0
thelastapostle,If you can read books i would like to recommend [a Mind for Numbers - Barbara Oakley!](https://www.goodreads.com/book/show/18693655-a-mind-for-numbers). I am applying the concepts described in this book in my daily life and i can confidently say that i am improving at maths and science. This books is for any student regardless of the field of study. Its a book which would teach you about how to learn effectively.,1518885451.0
drboyfriend,"I switched from an art degree to CS and the thing I struggled with the most was math. After having a rough time in pre-calc I decided to go back a few courses and take a ""college algebra"" course over the summer before taking calculus. This helped me re-learn some of the basics over a short period of time. Aside from that, I always did my homework right after class let out and made sure I understood every problem and concept. Sometimes I had to look up a few different youtube videos or go on math exchange for the really hard stuff. It gets easier but you really have to understand everything even if it means going back a bit. ",1518889928.0
hwswDev,"I've done a lot of front end, back end and embedded coding.  Not sure I've ever used any math above trig.  Closest thing was a motor controller that required clark/park transforms.  Even then I didn't need to understand the calculus behind the formulas.  Maybe if I were doing something crazy like 2 of these motors with some sort of linkage needing a multivariable differential equation to solve, but that type of project will never happen for a freelance guy like me doing projects by myself.  Who knows, maybe someday I'll need to pull out my control systems textbook or calculus textbook but I doubt it.",1518893046.0
zvive,"I've never really needed a ton of math but I'm mostly using web frameworks, etc. Not doing ml or ai. That said, I've been wanting to learn ml but want to know the math before hand.

I've found Khan academy to be pretty cool you just keep testing to a higher and higher level. It really helps you master things. I went all the way back to prealgebra since it's been ages since I was in h.s. (I'm 38). I'm trying to simply spend one hour per day. Eventually I'll get through calculus. I chart my progress and goals on a whiteboard in my office.",1518894231.0
nullbnx,"Practice, practice, practice. Ask lots of questions when you don't understand things and always build a relationship with your professor/TAs. 

Effort can overcome deficiencies.",1518895851.0
Kerbobotat,"Math is like playing music. Very very few people are naturally gifted at it. You cant just walk up to a piano and play.

What you *can* do however, is practice. Keep practicing. Start with simple problems. Keep doing them with sifferent values until you understand it. Then move to harder problems and *keep practicing*.

Remember when you learned multiplication? Or long division? That shit was difficult wasnt it? But what did you do? Practiced. You learned your multiplication tables and little tricks here and there until it becomes second nature. More complicated math, especially math for computer science/programming, is the same.

I was terrible at math. I didnt understand calculus, differenciation from first principles was particularly hard for me. Still is tricky. In CS, doing things with hexadecimal was a pain. Remembering how ones complement worked in binary was awful. But with time and practice, its gotten easier. Im a competent programmer, but math wise I still struggle at times. But the trick is to just keep practicing learning and going over material until you can do it like muscle memory. ",1518898884.0
nonconvergent,"Keep at it. For starters, there no such thing as good at math. Math is hard analytical problems and all the resources you have are from people who already 'get' it, not people like you who are in the middle of learning it. If you fail to understand a concept after reading or of being taught, one of two things happened. Either you didn't here (distinct from not listening) the whole thing or they didn't teach you enough. There's no value judgement there, that's just how it is. Assume you didn't ""hear"" everything and try again. After a while, consider reapproching the subject from a different angle and seek out a different source. There's a lot of good videos on calculus on YouTube and a lot of good books that aren't your textbook.

Honestly most textbooks are pretty terrible at this as their motivation isn't to teach but to push a new product. Find a copy of Sipser's calculus book if you can. It helped me a lot when my texts fell short.

A lot of this requires work on your part. Practice, Feedback loops, and self assessment. There's a software craftsmanship book that can provide some guidance in this area.  I'm pushing senior developers at work to read it now, but I also think it's a good fit at your level. It's called Apprenticeship Patterns. You can read some or all on OReilly's site even without buying it.

I'm also ""bad at math"" but what that really means is that there are a lot of areas in mathematics that I have a high level view of and very few areas I have in depth knowledge of. Conscious ignorance is just the first step.

Keep at it, and make sure you take time to find your own path, find a subject that drives you.",1518899913.0
DrLoomis89,"Hard to give good advice other than ""Study hard, show up in class, try to follow along and participate, work the practice problems BEFORE the teacher goes over them in class, so you're prepared to focus on the things that you're not getting"". We all have the ability to learn, but learning disabilities in certain areas are very real. In my degree, I had to do things where I had to pinpoint my location on a topographic map and use it to guide myself. And it seemed like no matter how hard I tried, I was lacking in that area, and other students seemed to be able to do it with ease. Of course, it's so easy to compare yourself with others

To me, it seems like if you are good at Comp Sci and programming, then you have the ability to think like a mathematician as well. You are taking a set of actions you want your program to perform, and basically converting it into a kind of equation. Working with variables, trying to isolate certain issues, breaking a really big problem down into several smaller problems. Completing a proof, for example, can even be a similar satisfaction to identifying a problem and getting your program to run properly. Maybe try to think of it that way. As a programmer, you're already training yourself to think in a mathematical way. If it's physics you're struggling with, try to think of it in terms of application to video games, for example. Trajectory and velocity, energy transfer between two bodies, etc. etc. All the kind of real world physics involved in golfing and driving and almost all modern day games, even stuff like angry birds or online pool",1518975443.0
Fwizzle45,"Can depend on the school. In my case, I attend a small private college and am pursuing Computer Science there. They only require Discrete Math and a class on Algorithms for the ""math"" stuff. Other than that, all the other classes are over different CS concepts, like data structures, networking, data wrangling, and computer organization. If your school requires more math classes, then I guess just buckle up and study hard. As others have said, math is a skill. Practice makes perfect. Do all the homework problems and if you need help on a certain topic, ask the professor to go through some practice problems with you during office hours or schedule and appointment. If you're at a small school like me you should be able to easily get some 1 on 1 time with the professor.",1519000878.0
PM_ME_IRL,"You don't have to be good at math, but you have to _like_ math. If you don't, you're gonna be miserable.",1518839581.0
iftheseaisblue,spend less time on reddit and go to class,1518859918.0
bogomiller,Go ahead ,1518837635.0
Sir_not_sir,Check your local library. They may be able to get a copy through Inter-Library-Loan.,1518807696.0
LesbianChemicalPlant,"Found a free pdf of it here: http://b-ok.org/book/2705697/6cc1c0

There's also a djvu copy: http://b-ok.org/book/502893/9eaf1c",1518862863.0
uh_no_,I give it a week.,1518803630.0
Drupyog,"I would really like if we had a rule ""if you post an article pretending to solve PvsNP, you need to justify that this article might have a chance of not being complete nonsense (a reaction of a member of the scientific community, for example)""

Because, otherwise, it's borderline spam.",1518811443.0
jmite,"I'd really like to see an effort to formalize complexity theory in Coq, so that any time one of these purported proofs comes up we can just disregard anything that's not formalized.",1518845253.0
mcdowellag,"I haven't read through this one yet, but I like to see these attempts. If P vs NP is solved by some massive effort I may never know enough to understand the solution - but if it is solved by some good idea that comes out of left field, then I may be able to understand it - and I might just see it posted here first. After all, the halting problem is a far-reaching result with a relatively simple proof, once you've seen it. Why not P vs NP?",1519067824.0
crikey-,I imagine the router will send ICMP TTL Exceeded as the router will process IP(Layer 3) before/if it will process the datagram(Layer 4),1518805771.0
meddlepal,What kind of question is that for a CS entrance exam? That feels like some esoteric trivia... and really has nothing to do with CS at all. ,1518825763.0
,"I remember it as TTL before all other processing.
",1518817060.0
ProgramTheWorld,What does this have to do with computer science...??? This is more on the information technology side and not really CS.,1518837498.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1518781277.0
Rioghasarig,I'm pretty sure you can. Can't you just try it and see? ,1518766375.0
joeroloff,"As a Certified Arborist with the ISA, I find this article rather confusing.",1518758102.0
wildptr,"TL;DR inductively-defined structures admit analysis by induction.

Snark aside, you shouldn't think of this strategy as unique to trees. You can 'crush' any problem this way if you find its hidden inductive structure.",1518760774.0
fckoch,"For the second problem, wouldn't it just make more sense to do a bfs search through the tree? The last node to come off is the deepest. No recursion necessary.

Note that for huge trees a recursive algorithm may suffer stack overflow.",1518783322.0
frootflie,deepest(NULL),1518767312.0
fredisa4letterword,"Surprised not to see the terms ""BFS"" or ""order"" (as in ""preorder,"" ""inorder,"" and ""postorder"").",1518800589.0
ChimpVision,"Best: O(logn)
Worst: O(n)",1518767668.0
Fazel94,"It should make distinction between rooted trees and regular trees. 
The offered approach mostly works for rooted trees. ",1518814205.0
p_pistol,Maybe if you don't want the job. These methods don't scale. Other comments have mentioned why: the size of the recursive call stack. Do these iteratively.,1518800953.0
fl2re,"I'm currently in my last year of my comp sci bachelors and I'm taking a numerical analysis course dealing with matrix solvers like Jacobi, gaussian elimination, guass-seidel, etc and I'm really enjoying the content. We'll be diving into more complicated subjects as the course continues. I'm currently a mobile developer but I'd like to change course after I graduate and I'm looking into different career paths. What's the best way to learn about a career in this work and is it possible with just a bachelors? Thanks ahead of time.",1518788103.0
Zophike1,Is their any Compiler Developer Internships around for people who have no experience in the area ?,1519187160.0
Sack_of_Fuzzy_Dice,YouTube is thy friend,1518736852.0
ShadyDaDev,First think of something you can use. If it is something that you can use you will more than likely follow through and push yourself to learn the new material. ,1518743986.0
mendeza503,"Hey all, creator here! I made the Math Art creation last week, and I loved all the excitement! This time, I wanted to focus on simulating parametric curves using only code, and I am super excited with the results!

Here is a link to view and share the lens on snapchat! https://www.snapchat.com/unlock/?type=SNAPCODE&uuid=0839f3e2ab104feba920f3417749fbfa&metadata=01

Please share with other people and threads, and let me know what you all think!",1518710078.0
jamuza,"I want to see more of this kind of thing, very nice!",1518731601.0
me_again_21,"I'll be honest man, I don't really understand what's going on behind the scenes here but it still seems impressive to me",1518744939.0
ThePoorlyEducated,"Pretty cool project. 
“I think software developement is sexy” -Trump in the future",1518790499.0
5225225,"It feels like the kind of thing that only makes sense as an explaination if you *already understand what it's saying*.

In which case, you already know what it's saying, and therefore don't need it.

Or at least, I can't work out what some of it means. I'm not saying it's bad instructions, just that it might not be the right way to teach some people. ",1518823078.0
chrisgseaton,"Everyone already knows this. People who say something is Turing complete implicitly mean 'or it would be if we pretend it has infinite tape', and everyone gets that.

It's fine, because running out of tape is an easy to see and understand failure condition.",1518700087.0
feedayeen,"A calculation on an ideal Turning machine that actually uses all of its infinite memory can never finish. It would take infinity long to ever read or write that memory. This means that for any Turning machine, the real requirement is a arbitrary large memory region that is large enough to contain the memory required by the algorithm used on the problem set.

Somehow that description which is more precise doesn't really add much. But hey, at least we clarified that we aren't trying to create a machine that is so physically impossible, it's mere existence would literally create a black hole that consumes the universe thanks to the Bekenstein limit on entropy. ",1518716277.0
_georgesim_,"Because being that pedantic doesn't add any value to any discussion about computability. On the other hand, calling something Turing complete which doesn't really have unbounded memory is very useful. It tells you something about the inherent complexity of that system or the potential complexity if you will.",1518818175.0
achNichtSoWichtig,"> - If your 'language' has a finite number of memory cells, it is not Turing complete

What language has a finite number of cells? When I hear someone say 'C is turing-complete' i know he is talking about the formal specification of the language. Nobody says ""Python on my 8gb memory laptop is turing complete"" since it would be a weird mixing of domains... Right?",1518721815.0
krawallopold,Could you provide the survey using a secure connection?,1518690103.0
nerga,This does not work on Firefox mobile for me.,1518698539.0
,[deleted],1518698949.0
omniron,"For the 2nd scenario, i only saw the Test cases, i didn't see the other code until i clicked ""finish review"", is this expected?",1518719306.0
cthulu0,"Wanted to participate but then saw the code fragment is in Java. Don't know Java.  Know C, Perl, Verilog, and Matlab.
",1518720111.0
kadathsc,"Another thing is that you ask when a person first started coding in a Java, when I believe what you’re really asking is the years of experience in that language.

I started programming in Java but I haven’t worked all my career in Java and don’t currently use it.",1518754700.0
WSp71oTXWCZZ0ZI6,"Meltdown has nothing to do with branch prediction: that's Spectre. Meltdown does exploit out-of-order execution, though, specifically around system calls.

If you have an interrupt instruction before a memory access instruction, the CPU will reorder the execution of those 2 instructions in the pipeline. Then, if the memory access instruction is for a page that does not allow read access (such as a page for kernel memory), the CPU will erroneously still fetch the data. This is a bug in the CPU.

Even though CPU will fetch the data, an attacker cannot see the value directly. You need another instruction which will take the fetched value and use it as an index into *another* region of memory. Afterwards, you can measure how long it takes to access that other region of memory to try to determine which part of the array was brought into cache, and hence what the original value in the kernel memory was.

Intel's bug was to allow the memory access to kernel space if the memory access instruction immediately follows a system call. The fix (sadly) is to just unmap all kernel memory when a user process is executing.",1518687800.0
martins_m,"Branch prediction is exploited by Spectre. Meltdown relies on speculative execution.

Instead of waiting on current instruction to finish before executing next, the CPU executes multiple instructions ahead. This is useful because some instructions can take a lot of time - like loading data from memory.

Meltdown exploits this by making CPU to read memory from location its not supposed not read (kernel memory). In normal user process its virtual address space contains also kernel memory pages, but they are marked not accessible from user level. Meltdown attack is able to ""read it"" by measuring time to know whether speculative read succeeded or not. And by manipulating array index it can index any location in kernel.

Meltdown patch unmaps kernel memory from your process virtual memory space. And speculative execution won't help to read it - because it does not exist anymore in your address space. So on every syscall you need to map kernel memory back in, do the kernel work, and then unmap before transitioning back to user space.

Here's a good article on this: https://medium.com/@mattklein123/meltdown-spectre-explained-6bc8634cc0c2
",1518687658.0
Xalteox,"Another good link on this topic: 

https://ds9a.nl/articles/posts/spectre-meltdown/",1518689825.0
agumonkey,"Authors: Fei Wang, Tiark Rompf

12 Feb 2018ICLR 2018 Workshop

#Abstract: 

Current and emerging deep learning architectures call for an expressive high-level programming style with end-to-end differentiation and for a high-performance implementation at the same time. But the current generation of deep learning
frameworks either limits expressiveness and ease of use for increased performance (e.g., TensorFlow) or vice versa (e.g., PyTorch). In this paper we demonstrate that a “best of both worlds” approach is possible, based on multi-stage programming
and delimited continuations, two orthogonal ideas firmly rooted in programming languages research.

#Keywords: 

multi-stage programming, delimited continuations, computation graph, PyTorch, TensorFlow, compiler",1518655061.0
emayess,a bit lengthy but nice,1518643060.0
clownshoesrock,"Go hard, and take the hardest courses you can manage.  Sure, don't stuff all hard courses into a semester, because that will hurt.  Falling back on something easier isn't horrible, but trying to upgrade after the fact is hard.

AI will have jobs.  It's still a frontier that only a few people understand how to reach.  Lots of people who try wind up in the weeds, and just fake it.  So there will be jobs and money of you can show off a good portfolio of work.

Web dev, is hard to show off.  The toolkits to do a vast amount of stuff are out there, so it's hard.  Browsers are still in flux, so there will be plenty of work changing the pages to work with newer browsers while keeping compatibility.  To me that is soul crushing.  But if it sounds like something you want to do...  do it.

Base Computer Science..  Make sure you grab some sort of side specialty, biology, stats, physics, business logic..  The competition is rough if you are only bringing a general skillset to the table.",1518636610.0
tobysblog,"It kind of depends on how intensive the course is. For mine, I would say put in a little more C practice and learn a little about computer architecture. In terms of books, Operating System Concepts by Silberschatz, Galvin, Gagne is a pretty popular one. ",1518633249.0
nude-fox,"It really depends on the curriculum at your school. OS classes can differ quite a bit. You should consult the class description which should give you a general idea of what you need to know. You can also ask the comp-sci department or professor who teaches the course. 


The undergrad course at my uni builds upon a MIT toy operating system xv6 which is written in C. You about double the size of the code base doing various stuff. It is the hardest core comp-sci undergrad class.  The class  itself should teach you the concepts Relating to OS you need to know. Remember to plan before you code and make sure you understand what is already there and how it works before you start poking at things. If you are working on a Toy OS like xv6 small changes can have unpredictable effects in weird places so you really need to understand the effect of your changes. 

If the course is in C (it probably is) Re learn how to do all the things in C you might need. Linked List, Pointers, casting, etc. Take the time to learn GDB if you do not know how to use it well. It will be very helpful in figuring out where your stuff is broken. ",1518674117.0
Hookless123,"This is a good resource outside lecture slides and textbooks. Good luck. 
http://beej.us/guide/bgnet/html/multi/index.html",1518664654.0
WhackAMoleE,"[Go to dinner with some philosophers](https://en.wikipedia.org/wiki/Dining_philosophers_problem).
",1518669577.0
Lucretia9,Is it a theory only course or will you be building an actual os?,1518762663.0
RexPowerColt69,"You're probably going to do both.  Learn theory in class then have homework that covers some theory and has implementation projects where you either implement them in an already working os, or just standalone program that run on *nix.  One major deficiency most students had with the course is not being familiar with system calls rather than just programming in C.  Typical OS text books don't cover these in depth since they focus on theory.  The Linux Programming Interface would be a good supplementary book, and other books that cover the Linux kernel, but most use an outdated kernel.  You'll probably get hands on with that in your lab though.  I wish we had an OS lab at my school.

Since you've taken algorithms and data structures before, you're going to be fine.",1518928700.0
dragonnyxx,"I was baffled by this headline, as we've had good heuristics for the traveling salesman problem for ages.

Turns out that by ""real-world version of the famous 'traveling salesman problem'"", they are referring to the asymmetric traveling salesman problem (cost of going from A->B is not necessarily the same as the cost of going from B->A), which was previously not easily solvable.",1518616683.0
Chiralmaera,"> ...the paper has not been peer reviewed yet, Regan said it has a good chance of withstanding the computer science community’s scrutiny.  ...  “There is one potential sensitive [technical] point … [but] very solid, very promising, well structured, and well broken-out.”

Still neat.",1518622845.0
noideaman,"Here’s a link to the blog post in the article. It gives a more technical breakdown.

https://rjlipton.wordpress.com/2017/09/11/a-tsp-breakthrough/",1518638865.0
spacegirlmcmillan,Did I miss what the solution actually was?,1518633518.0
julianCP,https://merascu.github.io/links/SS2017FLAT/Dexter%20C.%20Kozen%20-%20Automata%20and%20Computability.pdf,1518606259.0
methegrey,"Idk about tutors, but this professor from Portland State University has a great video series on ToC that was helpful to me when I was studying Automata at my university
https://www.youtube.com/watch?v=TOsMcgIK95k&list=PLbtzT1TYeoMjNOGEiaRmm_vMIwUAidnQz

Good luck! Keep working hard and you'll get through!",1518629261.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1518600123.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/latex] [TeXmacs–maxima interface by Andrey G. Grozin](https://www.reddit.com/r/LaTeX/comments/7xgsty/texmacsmaxima_interface_by_andrey_g_grozin/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1518599589.0
IJzerbaard,"How is this base 5? Anyway, assuming that was a mistake and you mean 5 bits, we have

    00011
    11110
    ------- +
    00001

Which is clearly 1. There is also a carry-out, which would be relevant if we were interpreting the addition in an unsigned context, but we're not.

> Do I just ignore the overflow digit so that my answer can be 1?

Yes. By assumption you're working with 5 bits, a 6th bit does not exist, because you said so.",1518590946.0
dakateavi,"I know two ""do it yourself"" sources: 

https://github.com/ossu/computer-science

https://functionalcs.github.io/curriculum/

Those will not grant you a degree but will give you some insight ",1518562556.0
Sir_not_sir,GT OMSCS is pretty inexpensive and in the top ten in the US.,1518575996.0
dzdaniel84,Berkeley currently has a CS ethics course (CS 195: Social Implications of Computing– the course website is [here](http://inst.eecs.berkeley.edu/~cs195/sp18/)) that's been a graduation requirement for all EECS majors for several years now; not sure why the article didn't mention it.,1518588099.0
tachyonflux,"This isnt a reflection of training or background, its a reflection of a society that doesnt give a shit about morals and ethics. Executives could care less about right or wrong. ",1518604381.0
istarian,"The technology IS neutral, but *people aren't* and so any implementation or use of a technology will be subject to the people involved. The idea of asking for permission later is far more widespread that silicon valley. That's a corporate business model...  
  
I agree that ethics is important and I wish  things could be better, but they need to address the reality that getting ""permission"" is also bound to flaws people have. Whether you can even get it or not to do X can be dependent on the fears and desire for control, profit, etc that other people have...  
  
Whether a self driving car is ""bad"" is  very different than ""should we use self driving cars to replace human drivers in general"".
  
This also has nothing to do with computer science itself... If anything it's a deficit of humanities/social science.",1518557676.0
netsecisquitefun,Does anyone know a good MOOC/online course/materials on this topic?,1518611549.0
MaroonLance,Ethics in computer science is part of the Computer Science A-Level so it's odd to see universities so far behind. (A-Levels are kinda equivalent to the last 2 years of High School),1518603442.0
asaltandbuttering,"""Dark side? What dark side?"" - The NSA, probably

In other news, the NSA wants to leak details regarding a few professors' ""dark side"". ",1518607482.0
spinwizard69,"I'm sorry but have the problems in this world can be traced down to academics trying to promote their sense of ethics.   More so we don't need policy makers in Washington, that don't understand technology, making decisions about what is ethical or not.

In the end the larger community of man must be involved in setting policies.",1518566774.0
xdert,"Your question makes no sense, neither problem is NP-complete and you can only compute the square root of arbitrary numbers with a given accuracy since it can have an infinitely long decimal representation. Which means your runtime depends on the accuracy.",1518550104.0
dakateavi,"You probably mean the problem of ""integer factorization"", not ""finding a single prime divisor"". And you have to keep in mind that the problem was nether proved to be P nor proved to be NP-compleat, so we say it belongs to just NP.

Now, about the ""finding the square root problem"". It is a problem of the form ""x^2 - a = 0"" , where ""a"" is the number which you want o to find the square root of! To be able to calculate the solutions for the ""x^2 - a = 0"" equation, we usually make use of numeric methods which are not NP-compleat ether. 

About factorization: 
https://en.m.wikipedia.org/wiki/Integer_factorization

About numeric methods: 
https://en.m.wikipedia.org/wiki/Methods_of_computing_square_roots",1518552097.0
hundredPercent-beef,Exactly how many subs are you planning to spam this on?,1518528248.0
Gupchup,"Hi there. Engineer here with 8+ years of industry experience with a PhD in CS. Given your description, it sounds like a survey paper. Survey papers are quite useful if you can cover the topic broadly, clearly present tradeoffs and provide examples/situations where different options can be applied. The paper doesn't need to full of numbers or algorithms - a well researched qualitative study is just as valuable. ",1518507253.0
nerga,"Unless your masters thesis is in something niche and you are going into that field also, like machine learning or natural language processing, in most of industry no one will ask or look at your thesis at all.",1518526062.0
crocodilem8,"As long as it is a quality paper I think you'll be fine. Displays communication and technical writing skills which employers love according to my prof. 

Another angle is don't limit yourself to a paper that you might have all your answer lined up for, isn't experimenting part of the masters experience?

Advice from a undergrad student so take care. ",1518499852.0
dvirsky,"Senior engineer with no CS degree at all here - if you're good no one will give a shit. And just having a Master's should help you land your first ""real"" job relatively easily - especially if you've performed well in your internship. From there on it's up to your performance and experience. ",1518511764.0
WhoIsTheUnPerson,r/cscareerquestions,1518539284.0
swxxii,It doesn't. Switch to software engineering and do all the project management stuff. ,1518505233.0
Bobknows27,"You don't specifically need to add counts as they suggest: for numbers you could just as easily walk through your count array and insert that many of that number at each step.

That said, the way that they do it gives it the nice property of being a stable sort. What that means is that if you have extra data in each item that is not used for the comparison then all of the items that compare equal to each other will remain in the same relative order they were when you started the sort.

Additionally, their method can have better cache and branch prediction behaviour, so it might perform better.",1518494530.0
RomSteady,The example at the top of your first link explains why you add the previous counts: it gives you the starting index of where that particular number will show up in the output.,1518540795.0
feedayeen,I think that it's telling you that you need to keep track of the total number of elements already added into the array to know where to start inserting the new element. ,1518493087.0
,[deleted],1518480083.0
mkgs,Note the transaction status field - any transactions with the 'completed' status can be considered as final ledger entries. There doesn't seem to be a lower layer that is accessible with this API.,1518468993.0
paradiselost79,"Had someone ask, this is not to help me grade. This is for the student to get feedback on their idea. Does it seem feasible, make sense, should they do it, etc. I already graded it, this is for the student not me",1518462582.0
Spam_Detector_Bot,"*Beep boop*

I am a bot that sniffs out spammers, and this smells like spam.

At least 85.71% out of the 7 submissions from /u/basikconcept appear to be for Udemy affiliate links. 

Don't let spam take over Reddit! Throw it out!

*Bee bop*",1518455944.0
mredaelli,"Could someone recommend an introductory but serious book giving a birds eye view of the current status of theoretical computer science, covering all the points mentioned in this paper at least enough to give an idea of the matter? ",1518463892.0
TomvdZ,"The paper consists, for more than 90% of explaining extremely basic concepts. The actual ""proof"" is only one or two pages long, and includes solid ""intuitions""  such as:

> Thus the Algorithm A has to avoid to get stuck into Y' . To avoid doing so, for every e that A adds iteratively, A has to check exhaustively all the supersets of the sub-solution reached to see which one is augmentable. Hence A would be exponential. In the worst cases, A has to backtrack.

This isn't the ""formal proof"", but arguments such as ""every algorithm has to exhaustively check all the supersets"" don't bode well. Anyway, on to (one of the many) real problems with it:

**Lemma 2 is false.**

> *Lemma 2.* A Search Problem Π is in P only if all its sub-solutions are augmentable.

Maximum matching is a counterexample. The relevant definitions are:

> Y' is a sub-solution of PI if there is a minor G' of G such that Y' is a solution of PI on G'

> A sub-solution Y' is augmentable if it is possible to find in polynomial time an element e \in X such that Y' \cup e is a sub-solution.

Consider maximum matching on a graph G with 4 vertices, connected in a line. Call the edges E(G) = { (a,b), (b,c), (c,d) }. The edge (b,c) is a sub-solution, because it is a maximum matching on G \ (c,d). However, it is not augmentable.",1518427119.0
Decateron,here we go again,1518418500.0
chx_,"We will see. I am not an expert enough (by far) to say anything about this but a) greedoids are not widely known or used, so their usefulness might've been overlooked b) and yet, the Greedoids book by Bernhard Korte, Laszlo Lovasz, Rainer Schrader does mention two NP-hard problems.

Also, the author has peer review published material https://www.sciencedirect.com/science/article/pii/S0012365X07001410 so not a complete kook.

Beyond that, let others find the holes.

Edit: https://www.reddit.com/r/compsci/comments/7wyhsg/180203028_p_not_equal_to_np/du4cnbq/ there it is.",1518422703.0
raviqqe,Is this a real proof?,1518422028.0
Godot17,"We did it humani-

>Lemma 2 is false.

We did it Reddit!",1518433491.0
happartox,I’m not sure but Eli the computer guy has lots of similar stuff. His YouTube channel is failed normal redux. But again I’m just guessing :),1518423265.0
crabbone,"Are you perhaps thinking about ""Worse is Better"", i.e. Standford vs MIT rather than Harvard vs MIT?",1518525745.0
doingthingsalot,"For anyone interested, I found the blog. I remembered it had a comparison of computer latency times. It is https://danluu.com and the MIT vs Stanford comparison is here: https://danluu.com/mit-stanford/.",1519311439.0
Sinistersnare,"Don't ask to ask, just ask the question.

Try to figure out the issue you are having, and ask that. Don't hand people code and then say 'it doesn't work'",1518314299.0
trident162,please,1518296105.0
trident162,"pls
",1518296180.0
VeganBigMac,"Like Monkey said, I don't think you should skip Operating Systems. I can't really give you a theory based reason, but if you are going to take one applied course, you should take that. No other course helps more with learning how things work ""under the hood"" (and its also a super interesting course). Honestly, I would just skip another Software Engineering class if you could.",1518295067.0
MonkeyOfAvon,"I don't know what discrete math I & II cover, but I've found the following quite useful: probability and statistics, linear algebra, logic (including proofs), combinatorics, graph theory, abstract algebra, number theory, set theory, type theory... But it depends a bit on what you want to do, because theoretical computer science is quite a broad term that includes a lot of different things and you can't be master of all. 

I personally think it's good to have mathematical breadth in CS rather than lot of calculus. I haven't had much use of calculus during my studies, but very much linear algebra and probability and statistics, which fits your interest in your interest in machine learning and graphics, as well as the more ""odd"" maths.

I've taken a couple of courses in numerical analysis, but it felt less like CS and more like engineering to me. It also worries me a bit that you want to skip operating systems. ",1518293586.0
Andy_Reds,"I did my BS in math, and I'm doing my MS in computer science. Like you, I tend towards the theory heavy classes, but one of my favourite classes in undergrad was OS.you should definitely take it, as others have said, there's no better class for teaching you how computers actually work. As for math, real analysis is great for machine learning, and intermediate stats is also good, along with advanced bayesian stats. I've found abstract algebra very useful in cryptography, but that doesn't seem to be one of your interests. If there's a linear algebra 2 class, you should definitely take that. You can never know enough linear algebra in CS.",1518350408.0
conflicted_panda,"Uh, your title could really use better wording. If your article is a tutorial on data structures, you probably should mention that. As it is, reading about a linked list that offers O(n) lookup and O(n) insertion/removal was pretty funny, but not what I was expecting at all.

As a tutorial it's really thorough, though, and the illustrations are nice.",1518308275.0
sobeita,"> arr == 0x5000

...

> arr[0] === arr + 0th size

Should be be &arr[1] or \*(arr + 0th size), no? Adding the 0th size would give the **first** element's **address** as far as I know. Unless there's another meaning to the ===, but I'm not aware of one (I thought it meant identically equal as opposed to ==, which is equal by value but not necessarily identity. A member would only be equal to an address if it were assigned as such, and I don't see anything like that.)",1518299176.0
,[deleted],1518291332.0
hextree,"Q2: Unless I'm misunderstanding the problem, if you were one move away from completing the puzzle then the manhattan distance would be 1, the second measure would be 1, and I don't know what the 3rd measure is, but the overall sum would certainly be at least 2. Which exceeds the true number of moves from the goal (1).",1518361034.0
ProgramTheWorld,One way would be to approximate the volume by taking the integral over the height of the shape.,1518222771.0
teawreckshero,"The problem isn't described well enough. Can we assume the volume is convex? How do we know if it is or not just from the points? Any point cloud representing a concave shape could just as well be interpreted as a convex shape, and vice versa. First you need some algorithm to generate the mesh you want from the points, then find the volume of that mesh.",1518229222.0
TomvdZ,"The provided answer is wrong (okay, it's not technically wrong, anything that is O(n log n) is also O(n^2 )); the entire point of using StringBuffer is to avoid the performance hit that comes with repeatedly appending a string the naive way. I suppose that while technically there is no guarantee on the performance of StringBuffer, the answer could be correct if they're presuming some very weird nonstandard implementation of StringBuffer.

However, O(n log n) isn't actually correct either. The answer is actually O(n), where n is the length of the output. The doubling strategy doesn't incur a log-factor penalty in the running time.",1518193982.0
player2,"This answer seems all sorts of wrong.

* StringBuffer exists precisely to make these kinds of operations fast. I don’t know its exact performance characteristics, but O(N) append would make it quite useless, and it is indeed common for these kinds of data structures to double the length of their internal buffer when allocating.

* If N is the number of characters in `sentence` (meaning it’s the sum of the character count in each of the `words`), then you are looping at *most* N times in the pathological case (each element in `words` is one character long). But this trades off directly with the number of iterations that `append()` must do over `w` while copying into `sentence`. There’s no way to get O(n^2) runtime out of this loop.

Perhaps a previous draft of this example did not use StringBuffer, and the editor failed to catch the change?",1518195259.0
tapdncingchemist,"It should actually be linear, or O(n).  You're totally right about the array not needing to be recopied every time.

The array doubling does happen log n times in total, but there's a technique called amortized analysis that actually allows you to think of it as constant time per operation.  Think of it this way.  You have a piggy bank and you pay coins for each operation.  For each append, you pay 3 coins.  Let's say you're not in a doubling step.  One coin is spent adding to the buffer and 2 coins go in the bank.  Then, when it's time to double, you should have 2 coins in the bank for every element in your buffer that was added this round.

While you were adding to the buffer, you added 2 coins for each element.  One can pay for moving the element that paid the coin to begin with.  But there might have been elements from a previous iteration that didn't pay any coins this time.  Since you are always doubling in size, you know that the number of previous elements is the same as the number of elements who paid to the bank this round.  So they can each use one of the coins to pay for their copying.

So each element pays 3 coins when it is inserted and all work is paid for, so the total work is 3n, or O(n).",1518213486.0
EScar21,"If you don't run it through n times then that means you didn't see all the values which means any possible run time you can come up with is incorrect, unless you implement it certain ways.",1518208239.0
ProgramTheWorld,Why not both? O(f) is an upper bound for complexity so there can be more than one correct answer.,1518193547.0
,[deleted],1518197076.0
skylarkReddit,I always knew these stl libraries are going to fuck up the real essence of c/c++,1518241913.0
EScar21,"There are many proofs out there that can show you why running through a string is in n times.  Pigeon hole one is a great example, the fact is if you have n items and you say you can beat n that's already a lie, the run time at most has to be n items, why you ask ? Anything less of n would mean you didn't check every in this case character which means I can take any character and show you a contradiction which means your proof is flawed which means it's incorrect and you can't achieve the run time you say you did.  They show it's n^2 because it's the naive solution to copying a string over that size n characters.  There are obviously faster ways but in the worst possible case it's going to always be n^2.  One to read them all size n a second time to copy them all also size n, n x n = n^2.",1518192606.0
,"No, it appears to be correct. The part which may be confusing you is that for the `StringBuffer`, if it's implemented like you say it is, then each append operation (per character) will be amortised `O(1)`. However, there are `n` characters to append, and so the overall complexity of `append` is `O(n)`. Then you have the outer loop which iterates over the `words` array, and so the overall complexity for `makeSentence` is `O(n^2)`.

EDIT: To all people downvoting, I am talking about the general analysis of the method. The given explanation mentions `n` to be the total number of all characters, but that seems to point to the fact that the explanation is faulty. If the question itself mentioned that `n` is the total number of all characters in the `words` array, then yes, my explanation is incorrect, but I doubt that's what the question states.",1518193998.0
samoson,That’s a troubling thing to say. You should do some soul searching. ,1518142807.0
emshon,What do you do to stay sharp and keep your skills up to date?,1518178874.0
LanR_,What are working on or reading currently?,1518164885.0
The-Rubber-Bandit,Oh god I saw “Pointers” above and i nearly had a heart attack,1518195725.0
mathsive,"Now, ~~communicate~~ _formalize communication_ with this dumb rock.

Communicating with rocks is the messwork of slovenly engineers.",1518128625.0
everything-narrative,"Tricking a rock into doing sums is the greatest achievement of computer science; though not to oversimplify, you have to put lightning inside the rock first.",1518133968.0
,The real situation is that a computer is electrified rock though so this is a good analgy ,1518128751.0
GodFatherSnoo,Boy this gotta be the realest shit on Reddit ,1518131549.0
accidentally_myself,"computer engineering*

comp sci: the same but there is no rock. ",1518134865.0
VeganBigMac,Now imagine the rock is in a quantum superposition.,1518132490.0
redragon11,How does one set a rock on fire?,1518129836.0
droden,you will name things and write loops.,1518134570.0
r30ng1n3rd,And the rock can grasp 32 fire states at once. ,1518126225.0
RunHomeJack,what happens when the rock is on fire?,1518133544.0
michaelmalak,Computer Science in a nutshell: overuse of colons.,1518143891.0
Pie-Robot,Lmao,1518245741.0
philthechill,I would hook that rock up to a speaker so it could beep when there was a fire. QED.,1518127393.0
Pie-Robot,"If temperature = 1 rock is on fire
If temperature <0 rock is not on fire
Am I right? 

FYI I actually know nothing about coding ",1518128987.0
DamnableNook,This poster is a spammer for a content theft blog.,1518133639.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1518102676.0
bdtddt,BlueJ was built for novices.,1518102615.0
caekmonstar, I teach using eclipse which meets requirements 1/2/4... and it works well. ,1518101979.0
pokepokepokepoker,Intellij? Other than eclipse i think that's the only other choice.,1518102140.0
igglyplop,"Eclipse is truly a pain in the butt sometimes but it is powerful, portable, and works with few hiccups. When it does sneeze, restarting usually does the trick. However important projects can be cumbersome.

In the end the most stable method is Sublime 3 + command line compiling. However the learning curve for this (especially for beginners) can be steep.",1518119568.0
AdrianWillaert,"Personally, I can only speak from the student's perspective. But in my experience, starting to program in a plain text editor (with syntax highlighting maybe) really helps to understand the basic concepts of programming. Especially compiling code manually at least once before moving on to using an IDE.

Our teacher in school used [BlueJ](https://www.bluej.org), but I never really liked it. This is due to it's weird concept of creating objects and calling methods manually. My suggestion would be eclipse, although I know the interface can be overwhelming as a beginner.",1518104295.0
maladat,"It's not a great IDE for a production environment, but in a classroom setting for new-to-Java students, DrJava is pretty good.

It's free. 

It's written in Java and can be distributed as a Java jar - put the jar on a Windows, Linux, or Mac computer with Java installed, run it, and you're good to go.

It's dead simple. Open a directory or a bunch of files. Push the ""Compile"" button to compile everything that's open. Push ""Run"" to run the program. Push ""Test"" to run any JUnit tests present in any of the files that are open. You don't need any special project or configuration files.

It also has a Java REPL/interpreter which is occasionally useful.
 
http://www.drjava.org/",1518109962.0
rboto,"Dr. Java is pretty decent for new programmers. The interface is simple; installation is basically downloading an .exe file. Also, creating, compiling and running files are easy to accomplish with Dr. Java. As a previous newcomer to programming, Dr. Java was a nice introduction. Good luck finding an IDE!",1518123934.0
tatorwatt,JGrasp is also straightforward and a free download. Has just the necessities and good for beginners. ,1518103654.0
chadwickofwv,"For beginners I would say a text editor with syntax highlighting may be the best.  My favorite is [Kate](https://kate-editor.org/2016/01/28/kate-on-windows/), which is available on Linux, Mac, and Windows.  You could also look at [these others](http://www.wpbeginner.com/showcase/12-best-code-editors-for-mac-and-windows-for-editing-wordpress-files/).",1518115008.0
tawalaya,"I teach first-year university students. We use eclipse over IntelliJ because eclipse is entirely free and works on older machines and any OS. Teaching students to use it is also relatively simple, you can provide them with settings files at the beginning of the semester, and then everybody has the same setup. For younger students, I would recommend BlueJ it is excellent in teaching object-oriented programming.",1518188689.0
algn2,"As many have mentioned, there are several good ""free"" IDE options. 

* IntelliJ Community Edition
* Eclipse
* VisualStudio Community Edition (yes, folks, it actually works -- on Windows10)


Regardless of ""free"" or not, an IDE will need ample CPU, memory, and disk to be useful. Otherwise, the experience for the student will be nothing but frustration due to extremely long waits to get anything done. 


Unless it's guaranteed that all your students have systems at home that can support an IDE setup (eg. Intel core i5, 16GB RAM, 500GB disk, Windows10 64 bit). give up the IDE-on-a -USB-stick idea. 

**However**, I do have one suggestion: **https://ideone.com/** .
I'm not connected in any way with this site, but it's an excellent way 
to get students interested in programming in **any** language. 
It supports Java, JavaScript (2 versions), C, C++, python, perl, etc..

Have a look.",1518238614.0
GenericBacon,jGrasp?,1518487106.0
crabbone,"Java is a weird language in that sense that the tools for working with it are kind of screwed. Good code editors don't like Java, and people who use good code editors generally don't like Java.

Typical choice of editor for Java would be Intellj IDEA / Eclipse / NetBeens.  They are all bad, but you need to choose from what you have... The paradoxical thing about these editors is that while they are bad they are... extremely popular.  Smaller / less popular ones are, basically, modeled on them with very slight changes.

Now, you could try to teach students using a better code editor, but then you will have to teach them how to use `javac`, `mvn`, maybe `ant`. How to set up `JAVA_HOME` and a bunch of other environment variables.  How to configure the size of Java heap, mess with JVM configuration file...  Java infrastructure simply doesn't have good tools that will allow you to ""just write your code"".  It's either you suffer by using a monster like Eclipse, or you suffer because you don't use Eclipse...

Finally, there's kind of a middle-ground solution: `eclim` (Vim plugin) and `eclim-mode` for Emacs.  They try to combine good code editing abilities of Vim or Emacs with code intellisense of Eclipse.  I tried `eclim-mode` long time ago, probably in its infancy, it was too slow for real work.  Needless to say that for someone who's unfamiliar with both Eclipse and Emacs, configuring this combination will be a complete nightmare.  Another ""problem"" I had with this approach was that it had to use Eclipse's concepts like ""workspace"" and it's cumbersome and stiff rules for managing your code projects.  Finally, it's not really possible to run Eclipse in headless mode.  I mean, it's technically possible, but Eclipse has garbage API, and implementing it is obviously very tedious, so `eclim` doesn't expose all of the Eclipse's functionality.  Every now and then you'd still have to pop Eclipse open and search for some combo-box with a radio button in it you need to tick.

---

Somewhat unrelated, but still may be useful. `SCons` has a very simple interface to compiling Java code into `*.class` files or `*.jar`.  Much better than `Maven` or `Ant`.  You can basically just drop a `SConstruct` file in your project with `env.Jar()` in it, and it will likely build a JAR from your project.  So, if you are considering ""know your tools"" approach, this may come in handy.",1518108437.0
LithiumEnergy,Now tell us why Kalai is wrong,1518074354.0
sinrin,"TL:DR;

To correct the noise in a quantum computation, you need to represent a logical qubit as a bunch of ""physical"" qubits. The number of qubits you need to correct an error increases exponentially with the number of qubits that need to be corrected. 

Basically, trying to reduce the noise makes the problem infinitely more complex.",1518115362.0
vznvzn,"noise/ decoherence is a problem and do think its very experimentally challenging,  possibly even an achilles heel along lines of Kalais ideas/ intuition/ arguments. see preskills new paper on **NISQ, Noisy Intermediate Quantum Stage.** also endorsed by Aaronson. it would appear that _even proponents have now conceded that noise is a serious issue interfering with scaling unlikely to be mitigated in the near future._

https://arxiv.org/abs/1801.00862

but another substantial argument/ possibility might be that **BQP = P** and am surprised nearly nobody seems to be espousing it.

btw martinis google group is doing much better with ""clean_(er)_"" qubits with low noise than Dwave and they claim to have a viable scalable architecture at this point.

recent developments/ overview on quantum computing mid 2017 here

https://vzn1.wordpress.com/2017/07/17/qm-computing-summer-2017-update/",1518115497.0
ghostnet,People who say it cannot be done should not interrupt those who are doing it.,1518081500.0
_ACompulsiveLiar_,"Taking more resources to implement is a very broad and sometimes wrong way to put it. Sharding can be incredibly simple and/or incredibly complex, and it covers a wide range of capabilities and it can also save resources.

It's a horizontal partition, which means it's a load balancer of sorts. Imagine you're Bezos and you need to store all Amazon products in one big table. Instead of putting billions of rows of data in a box of computers next to your desk in Seattle, you put 10% of the rows somewhere in the US, 10% in the UK, 10% in Australia, etc.

Often times an aggregate database has subsets that have more use in different locations. Why would you store US based retailers and products in a database in the UK? Why would you sell kangaroo saddles from a database in Seattle?

You also now have the option to backup data across multiple shards. Instead of having two giant boxes with identical data, you have many complex algorithms that can help you store your data with redundancy for protection across multiple shards from around the world, lowering points of failure.

This does come with downsides. Sharding can be incredibly complex to implement and if done incorrectly, you can often have corrupted/mismatched tables, lost data, more critical shards, etc. And evne if you do it correctly, it can be a pain in the ass to no longer have a single point of entry when trying to mess with your data, as you now have to run around the world collecting whatever you need.

Generally though, at large scales, all giant companies will shard. When done right it'll simply offer you faster databases and better redundancy/backups. You might say that you're using extra resources to make all this happen, but you might also look at it from the point of view that if a nonsharded system were to fail, the resources you would expend to recover would be far more than if you had just implemented a proper sharded system.",1518059133.0
herple_derpskin,"Any application that needs to be able to handle a high volume of data access will benefit. 

Let's say the machine your database is hosted on is currently maxed out spec wise and it's having performance issues. 

  Sure, you could try to optimize the way you access the database (make your sprocs more performant, schedule jobs to run during off hours if possible, etc), but that will only be a stop gap if your company continues to grow, and tbh that is a lot harder to do (slogging through legacy code with loads of tech debt when there are only a handful of devs that are still around from when that legacy code was written...).  Sharding will be a permanent and possibly easier solution to these performance issues. ",1518058998.0
fj333,"> Also, it seems only search engines and databases currently use sharding

No, any serving system can benefit from sharding. Imagine a DMV serving customers. If there is only one attendant serving all customers, and the line gets too long, a second attendant can come to the desk. If the first attendant takes people with last name initial A-M, and the second attendant takes N-Z, then boom! You've just sharded your DMV service. And presumably increased customer throughput by 100%. To be a more technically accurate analogy, imagine that when the second attendant joined in, all of the file cabinets and other resources the attendants need to access magically got shoved to either half of the building, each set of resources closest to the attendant who needs them. :-)",1518572956.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1518043573.0
Learfz,"How about computer graphics? You could walk through a simple fragment shader, have them do a few pixels by hand, and then show them how that math gets done 100,000,000 times per second.

Plus, most people have daily access to a machine that can easily run WebGL these days.",1518061519.0
PM_ME_A_STEAM_GIFT,"Do you want to talk about actual HPC i.e. using supercomputers to predict weather, or is it more about a general demonstration about how fast computers can be?

You could talk about AI and machine learning. For instance, that it took DeepMind only [4 hours to learn chess](https://arstechnica.com/gaming/2017/12/deepmind-ai-needs-mere-4-hours-of-self-training-to-become-a-chess-overlord/) from zero to world champion level.

An other example might be video game graphics. I'm still amazed at how much calculation goes into rendering a single frame of even a simple 3D game. And then you do all of that 60 times a second. Although, maybe 20 minutes is a bit short on that topic.

Or any kind of iterative simulation might be interesting, such as fluid or n-body simulations.",1518038641.0
Commander_B0b,"The following is anecdotal; the reasons for my own being excited/interested in HPC are the problems that can be solved.  Twitter just posted an article talking about their use of machine learning to show the most interesting portion of uploaded pictures in the caption.  This caused a small implosion in my brain, Twitter has trained a neural network to identify ""interesting"" areas in a picture.  A measure that is so very subjective yet this is the power of HPC and by extension machine learning.  I'm not familiar with the pi demo but whenever I'm trying to talk to someone who is not a C.S. person about these topics I try to use examples like these, examples of computers working in ways and solving problems that seem to be out of the realm of computation(at least in the way we are accustomed).",1518038801.0
Ozone777,"I always find Astrophysics simulations impressive  https://www.sciencealert.com/most-advanced-illustris-next-generation-computer-model-universe-simulations  
You could show a toy n-body simulation https://www.pandastrike.com/demos/nbody-simulation/ and explain how HPC allows us to scale this up to galaxy formation by adding many more variables and particles",1518039794.0
heyandy889,"[Folding @ home](https://folding.stanford.edu/)

[SETI](https://setiathome.berkeley.edu/)",1518043850.0
Samrockswin,"Wow, thanks everyone for the ideas! I think I'll do something similar to what /u/Commander_B0b mentioned and work with Twitter. I will probably mention the sort of research the folks at Twitter can do and perhaps see if it's viable to find the shortest path between two users in the twitter graph.

I really like the computer graphics ideas, maybe I can convince the person after me to do something like that! However, graph algorithms are closer to my research than graphics.",1518074375.0
TezlaKoil,"The first thing you should do is start reading the source code of similar projects.

The old [Reddit source code](https://github.com/reddit-archive/reddit) is object-oriented (in the sense used by your course). It's also notoriously crappy code. The reddit clone [Voat](https://github.com/voat/voat) is written in C#, and is also object-oriented in the same sense. I'm sure you can find open-source reddit clones written in PHP too.

^([Please note that the old Reddit is free software, while Voat is not.])",1517994716.0
wavy_lines,"You can make anything in an object-oriented manner.

Post is an object.

Comment is an object.

User is an object.

Reply box is an object.

Forum-section is an object

Post-tag is an object.

Basically anything that's a table in the database is an object.

For everything that does not naturally feel like an object, you can just make it one using the typical OOP fashion of sticking the word ""Manager"" to the end of class names.

ThreadPostingManager

UserMessagingManager

etc.",1518277445.0
a-buttclown,"I would guess it’s more focused around storing and fetching from a database like mysql. You can easily write the functions for that in a php class. 

I dont really understand what you mean with an object oriented framework? But for php you could use something like symfony to autoload your classes.",1517993112.0
sheikhy_jake,"Honestly, if you want to teach programming principles and best practice, having a language that is substantially more readable will really help. Programming isn't hard to learn, but for a novice, simply having brackets and semi-colons adds confusion. Plus, with python, you can have a single .py and don't have to explain away all the 'bloat' C comes with.


I think it basically comes down to intimidation-factor. Python is undeniably the easier language to get the ball rolling. Once they've got the idea, its so much easier to later explain the additional complexities of other languages.",1517962770.0
_--__,"There's a [thread](https://www.reddit.com/r/AskComputerScience/comments/7vo7ll/were_there_any_programming_languages_edsger/) in /r/askcomputerscience discussing Dijkstra's opinion on programming languages, which might help give some perspective.  Personally, I like the MIT approach - I'd favour programming languages that give the students something to ""get their hands dirty"" so they can get an appreciation of *why* we are interested in programming (e.g. solve some problems algorithmically); before giving the students languages like C which tend to emphasise why we need to program *correctly*.",1517963601.0
heroltz998,"If I have to choose from those two, I'd go with Python. Python is more readable and it holds students hands a lot more than C. C is still a valid language based on how many established it is, but for beginners it's dreadful. My programming teacher told us that when they used C as the language of choice in the programming courses they had to reboot their machines every time they checked students practice works because of all the memory leaks et al. that made checking multiple works in the same run impossible. Mind you the students in question are from a diverse background, some are opening a text editor for the first time during this course. C just has so many critical pitfalls that it's a difficult language to learn and/or master even for experianced developers, so I would never recommend it as a first language.

That being said I do think that even Python is not the best language, mostly because of dynamic type system. Python is said to speed up the development because it's so flexible with the types, but flexibility and implicit types are bad (in my opinion) if you have just started thinking about types and programming in general. Too many things going on behind the scenes. That's why I'd start with something like C#, even tough I'm not the biggest fan of the language itself. Few good properties it has for a first language are

1. Explicit types. Students see their types and are able to follow the reasoning in their code better than if there were no explicit type declaration.
2. It looks like C, C++ and Java, which all have good and bad sides but they all (C# included) are heavily used in the ""industrial"" applications. This way the students are faster ready for the workforce.
3. The imperative/procedural (how you want to put it) style of the code helped me understand better how ""the computer views the program"", as in there is not that much magic: you give computer instructions, the computer follows those instructions and whatever happens the computer just doesn't do anything else but what you told it to do. As a side note I think functional languages have very good properties and ways to think about programs, but I think they are too high level that the underlying consepts are harder to follow because they cover the lower level stuff behind the scenes too much in my taste.

Hope this gives some perspective.",1517978218.0
not-just-yeti,"Having taught intro-CS for many years (including in python, C, Java, and racket -- and some colleagues who've taught Ada, and having grown up on Pascal and Basic myself) --

Java and C are definitely problematic for beginners.  There is a lot of cruft, and a lot of pitfalls, that a reasonably good high school student can work through, BUT:
(a) it gives the impression that programming is about appeasing the compiler with the right arcane keywords in the right places (rather than being about clear, clean thinking), and
(b) for students that aren't super-motivated or don't already have a compatible way-of-thinking, these language take a difficult task and make it more difficult (and less rewarding).

Python is a hands-down improvement, for beginners.

A scheme-like language (like Racket) has the advantage that you only have to introduce five
 bits of syntax(*), and are able to spend your time on problem-solving for interesting, non-trivial tasks.  (In my senior-level class, we implement our own entire language using just those five constructs.)

(*) The only syntax I teach is:

- `define` to name to a value,
- open-paren to call a function,
-  a different `define` to create&name a function,
- `define-struct` to make a struct/class, and
- `cond`, as an if-else-if.

Compare that to Java, where you have to teach the above, PLUS (pretty quickly): when to ignore `static`, where to us `public` and `private` (but for weeks and weeks most all programs will have but one class, making public/private moot), remember the keyword `return` (unless your method doesn't have a return-type, which is what that word `void` means), writing getters/setters (the most boring methods-to-write conceivable) calling a method like `toString` vs calling a static method `Math.sqrt`, precedence of operators, constructors (it's just a function, but you call it with the special keyword `new` and you don't list a return-type and you don't say `return`), `System.out.println` (care to explain that those two `.` each mean something different?), how fields and local-variables have the same syntax but are fundamentally very different ... and after all that you get to write a program ""prompt the user for a price, and we'll calculate the sales tax"".  Ugh.

(Sorry, I got a bit heated up there.  I'm calming down now.)

I had completed grad school (and, taught some intro and intermediate programming), when I came along the ""[How to Design Programs](http://htdp.org)"" approach (or ""Program by Design"") -- it was an epiphany, and transformed how I thought about programming myself.",1518020087.0
motorsports,"I used to teach Python and have since moved over to Processing. Created by some MIT students as a tool to teach programming back in 2001. Great for projects involving pixels, images, audio, etc so students can see some results right off the bat. It also translates immediately to the world of physical computing and Arduino or go towards the browser world with a .js version.",1517968378.0
PM_UR_FRUIT_GARNISH,"Even though types and dynamic memory are important, I would suggest the python route because that gets people to understand what algorithms, variables, functions, and classes are without having to deal with troubleshooting too many compiler errors. I remember my first semester with C++, and that was a LOT to grok.",1517963244.0
crocodilem8,"My first actual experience learning programming was in C, and it was terrible because of how it was taught. At the same time I started learning python on my own and for a little while disliked C.

Now, with a little programming maturity I appreciate C more and while I learned it first I wish I had better instruction. A big advantage to learning C first is understanding pointers (or being introduced to them). C also gives a good base to move with ease to higher level languages IMO. Finally C also makes the idea of compilation clear. I think it would be a nice benefit teaching the process of compiling without IDE's, which are not necessary for beginners to worry about.

In the end I think you should consider more how well you can present the language to beginners and create appropriate projects / tests.

Edit - not an educator",1517974334.0
shadeofmyheart,"I am a Department Chair and educator who has taught basic programming for over a decade:

Python is definitely a better language to get started in for a variety of reasons, the larger one being memory management. If students can get started with Arrays being data structures they can put stuff into rather than having to understand pointers and memory overruns they can focus on the fundamentals such as expressions, control flow etc.

We use C# at the moment mostly because we like how it is strongly typed (Python is not). 

(Readability isn't really an issue for C IMHO)
My first programming language was Pascal which is super readable and verbose.",1517976329.0
Kaidelong,SML/NJ was the first language I was taught in a college setting and it worked great for me because of its interpreter. If you want to teach C you should maybe find a C that has a REPL or other interactive environment. Python has this sort of thing from the start. It encourages tinkering and helped me quickly pick up the syntax.,1517974816.0
joroshiba,"I feel like I am in a somewhat unique position where I have taken 5 intro programming courses in 4 different languages.

I think learning C is hugely valuable, but IME students who start in C have a harder time learning basic programming principles. 

The focus of an intro course IMO should be on algorithmic thinking and some fundamentals in most languages (logic control flow, variables, functions). In my C based intro class, which I later TAd, you could see that students were spending so much time wrestling with syntax details and trying to remember that, that they had not learned how to think about their problems very well. They also had a tendency to format code only because that was what was required for a grade, not because well it is is easier to program that way. One nice thing about python is it forces good whitespace habits that carry over well into other languages.",1518013794.0
ioeatcode,"I think it depends on what the focus is. If it's more application-based, other than systems software, Python is an awesome, modern language that has MANY valuable applications today. But if it's more theoretical based, I think having a solid understanding of C or some strictly OOP language like Java would be more beneficial in that regard.",1517962982.0
Bumbibonki,Have you considered a functional language such as Haskell?,1517962609.0
springsprint,Not so serious comment: studying a programming language should start with studying Backus-Naur form.,1517966536.0
mrexodia,"Consider C# instead of python. It’s a high level language but actually teaches you about types which are really, really important if you want to learn programming languages in my opinion.",1517962660.0
feedayeen,"As much as I like C and I think that it teaches a lot of crucial fundamentals, they're not really important for most people to know as their first course. Those details are completely irrelevant to the typical Physics, Bio, Math, or Engineering major. They'd benefit a lot from an easy to use programming language which they'd see in intro classes. 

Even CS majors benefit because it's easy to make interesting side projects in Python and that language has a very well supported community of libraries to help. The barriers to do the same in C are prohibitive so fewer students are going to practice in their free time if the most interesting thing they can make has a text interface.

C or a similar very low level language does need to be taught to CS majors. But if you move it away from an intro course then you can teach it in relation to the higher level concepts that were previously abstracted away. ",1517972253.0
misplaced_my_pants,"Check out Harvard's CS50 for how to teach with C done really well. It gives students an understanding of memory and an appreciation for higher level languages' abstraction. 

It really boils down to the quality of the lecturer.

Fuck Java as a teaching language, though. Too much ""you don't need to know what that means right now"".",1517997326.0
TomSwirly,"No question - Python.

There are a lot of good arguments here already, so let me add a new one - the command line interpreter is extremely useful for teaching.

I gave an introductory course in Python a few years ago and I didn't get out of the interpreter until the third lesson!  Being able to sit there at the command line and type:

    >>> 2 + 2
    4

is just so intuitive for students.  

When you learn a new idea, you can immediately try it out without having to write a program to do so.  If you write a little function, you can then import it into the interpreter and try it out to see how it works.

It's so much easier, particularly for people who aren't ""academic"" and want to see things work.

C has no such thing.  There is no way to just experiment - you need to go through a whole edit/compile/run phase every time.",1518000449.0
l_lecrup,"I don't teach programming, but I do teach other topics in computer science. Also, my partner happens to have learned some java, c++ and python over the last couple of years as a complete beginner, so I have some interest in this.

A general educational principle is as follows: it is not a good idea to have students memorise things they don't understand. Of course, at some point, some memorisation will be necessary. But as far as possible, students should be helped to *understand* what they are doing. If you accept this principle, it would seem that python first is the way to go.

My own experience suggests that learning c++ first is somehow ""better for your brain"". It is less forgiving, and therefore requires you to think through every step. But when I learned c++ I had already been doing proofs in mathematics for a few years. My partner's background is in social science and linguistics. She has great skills for learning human languages, one of which is to just try and communicate, and don't worry too much about getting things exactly right. Obviously, this is a terrible approach to computer languages. I think a lot of people who are learning programming, who wouldn't have learned it anyway as a hobby, will be in a similar sort of situation. 

Overall, I think python is the way to go. Your students will do more self-learning (which after all is the only real learning) if they enjoy what they are doing. With python, you can much more quickly get to some ""cool stuff"" that will show them that a few lines of code can change the world.",1518000530.0
,"To learn good programming practices probably Python. To learn how an actual computer works, C.",1518002043.0
MayorOfBubbleTown,"To write a program that just prints out Hello, World in C you first need to include a library that has input and output functions and then you need to write a main function with a return value. That by itself is really confusing for beginners. That's why when you read code written by beginners learning C they always have a bunch of libraries they don't need included because they are just learning to program and they don't understand that stuff and just want to avoid error messages when they compile.",1518006275.0
istarian,"Depends on what you're actually intending to teach. If you want them to understand how a computer works, C is better. If you just want to introduce them to programming, go with Python.",1518017879.0
afiefh,"Teach the principles of programming in Python, it is much more to the point than C because Python hides those details.

However I do think that before going crazy with Python and learning about advanced stuff like oop, generators and decorators you should take a C detour and learn what's actually going on in your system.",1518023082.0
hwswDev,Almost any language can teach about for loops and file io. My first class was in pascal and I turned out ok.,1518025547.0
victorestupadre,My daughter's high school CS 1 and 2 classes are in C++. She's one of the brightest in the class and honestly the language has been more of a hindrance than a help. I think programming basics taught in Python or some similar language would be better. There should then be a required systems programming course for CS majors and that should be in C. Just my $0.02.,1518026866.0
crabbone,"I've been programming in Python for about 10 years on and off.  I rarely had to do any large-scale C programming, mostly tiny bits of code for some utility programs, foreign-function interface bindings etc.  So, I know Python better than I know C, but given these options, I'd absolutely choose C over Python any time.

#Here's why

Python is a good language to ""get things done"".  It's less so in recent years, but it's still better in this respect than, say, C.  Python makes it comfortable for people with little insight / uninterested in how computers work to actually do things using computers.  Think about driving automatic vs driving stick.

But, if we continue with the driving metaphor: teaching to program is more like teaching a car mechanic, not a driver.  And for this Python is inadequate.  Python obscures and distorts simple truths about computers.  Suppose, you wanted to explain how sorting works: Python doesn't have a true array data structure, so your arguments about sorting an array will be fishy to any student with more than superficial understanding of the language.  Suppose, you'd want to do it with lists, rather than arrays: again, Python doesn't have lists.  You want to explain how hash-tables work: Python doesn't expose their implementation to programmer, and you'd still need to drop to C to understand how they work.  Actually, the situation is even worse because everything in Python is an object, so you cannot ever guarantee any speed / space invariants about anything because inheritance will instantly destroy any such invariant.

But, Python doesn't just obscure the reality.  It also creates a smoke-screen reality with far worse theoretical and didactic properties than the one closer to the metal.  Python isn't a ""designer"" language.  It's a combination of many contradicting ideas all mixed together because its creator at some point in his life decided that they are worth implementing.  You cannot derive Python from a small set of first principles.  To really understand Python you need to know a lot, and not just about Python, but also about C.  This is so because a lot of design choices in Python were motivated and sometimes simply a by-product of the fact that the major implementation is CPython.  Python has objects, which it implements in a bunch of different inconsistent ways.  It has some elements of functional programming, which poorly compose with those from imperative side of Python.  

Other examples of inconsistencies include: Python has `return` in one kind of functions but not in another kind.  It allows testing exception in multiple cases, but in non-exceptional situations it doesn't allow such tests.   Many API in standard library don't distinguish between lists and generators, but some do.  Objects are supposed to encapsulate state and resource management, but because it doesn't work very well, there's also `with` construct which departs from OO principles and introduces a new mode of managing state.  Oh, and you can also have closures to deal with state.  There are three built-in ways to do concurrency in Python, but all of them suck.  There are four built-in ways to print objects into string templates, and all of them suck.

Don't get me wrong... C isn't a ""designer language"" either.  It has a lot of absurd built into it.  It is definitely not a language that was built from first principles.  It is also inconvenient to use, if compared to Python.  But, in my view, the ability to explain simple truth about computer with actual working examples trumps all.  Also, to understand C, you need to memorize less syntax, less concepts.  You'll pay the price of being more verbose and more explicit, but, I think that for a student it's a good thing.  Students need this to be explicit to be able to understand how it works.  Students who only learn ""high level"" languages usually don't develop good understanding of how computers work.  They appear to learn some tricks, but no real understanding.

Don't get me wrong... this is not just about ""high"" vs ""low"" level languages.  Sussman's SICP and Krishnamurthi's PLAI use high-level language to do a good job at explaining how programming works.  Python is just not cut for this task.  The language Sussman and Krishnamurthi used in their books was built from first principles with a lot of care.  Python is just a random combination of popular features tied together with a duct tape.",1518339794.0
hipstergrandpa,"Python. I first learned ""Java"" in high school and it was so completely over my head because I wasn't really sure why I was doing what I was doing, and was mainly trying to remember syntax over the reason behind it. Next one was an intro course in C++....again, didn't really learn much because it still didn't click. Finally, it wasn't until I started with Python that it clicked for me, and I think that was because I wasn't so caught up behind all the rules behind the syntax and could just focus on the logic behind it. So, I guess it depends on what group you're targeting. If you're an absolute beginner, stay away from ""lower level"" languages and go for something syntactically easy. Even something like Scratch is a good place to start for an absolute beginner.  C comes when you're familiar with programming and want to understand the ""why"".",1517971167.0
BlaiseGlory,"As someone who programmed in C++ for 15 years, and Pascal on the Mac for several years before that, and is only a recent convert to Python, I would definitely recommend starting with Python.

Simplifies the learning process and teaches programming concepts rather than implementation details, such a C pointers and memory management.

Plus it teaches good indentation habits from day 1",1517982865.0
Farsyte,"BOTH.

The two languages are sufficiently distinct that learning each will give insights into software engineering and computer science that are not easily found via the other.",1517992223.0
frenris,"Who are you trying to teach?

If they are computer engineers teach them C first. It will help them understand how the computer processor actually works. It's also more relevant for topics like embedded development.

If you have general science students then python I believe is preferable. It allows you much faster to start developing scripts which actually do something. Developing a model for how a processor operates is less important in this case. Probably the most important thing you can teach them is how to analyze data and create graphics. 

For computer science students they should probably learn both at some point. Starting with either I think is acceptable. Alternately I think it's acceptable to start computer science students with languages like lisp or scheme which have advantages in terms of focusing on algorithmic structure. ",1518012669.0
jh125486,"Can I put a vote in for Go (golang)? It’s way better than C in my opinion. 
Simple language (fewest key words) but has strong typing and is objected oriented through composition and interfaces. 
The languages class I taught really liked it as it was easier to explain some concepts. There is also the online playground that provides a REPL environment for quick examples. 
I even do all my presentations now with “go present” hosted on GitHub as it let’s me embed runnable code examples. ",1517963882.0
lezorte,"I would actually say Java is a great way to go. I personally don't like the language all that much but I think what makes it so great as a first language is that you can easily choose to go higher level or lower level after learning it. Java was my first language and after learning it, C++ was incredibly easy to learn as well as high level scripting languages like Python and JavaScript.  But I can't possibly imagine how frustrated someone would feel going directly from Python to C++. Not to mention that Java is still a huge language for anyone who wants to get a job in a big corporation (even if it's slowly loosing popularity)",1517971015.0
myrrlyn,Python,1517979218.0
onfire9123,"C is not appropriate for the first day beginner. Python's readability is substantially easier to pick up. The first things learned in programming shouldn't include tedious syntax that the older languages exhibit. 

Python has my vote.",1517968990.0
rkempey,Python and it's not even close.,1517969368.0
MatrixSenpai,"Personally I believe Python is the perfect beginner’s language. Assuming I’m not misunderstanding, and you just want to get started in programming in general, Python is definitely the way to go. It has the capability to be both procedural and object oriented, is extremely flexible, and very useful. Give it a try!",1517962737.0
sullyj3,Pointers and manual memory management are mostly orthogonal to algorithms and the essence of computer science.,1517995726.0
ModernRonin,"I'd say it depends on what grade level, and what your end goal is.

The older the students are and the more you're trying to teach a ""serious"" computer sciencey type class, the more you would lean towards C.

Most real programmers program in multiple languages anyway, so I don't see any harm in starting with Python.",1517998755.0
American_Libertarian,"I dont think that c is good to start out on. It's a little too much all at once. I think it's a good idea for newcomers to start to right simple, readable code in an easier language.  

What my University did though, instead of python, is Java for the first year. It's not as simple as python but that's good. Python hides too much from the programmer imo. I think java splits the difference between easy to write, and allowing you to really get your hands dirty and see what programming is all about.",1518007417.0
,"Python no question. There is a reason basically every uni starts first year cs students in python.

 What exactly does adding the overhead of header files, memory management, and arcane syntax add to the learning process?

Toss in a dearth of formatting, env management and editor tools and you get a productive environment very quickly. Heck, we started by booting up IDLE and typing our code into it. No futzing about with environments, just writing code.

You want students to jump in and learn concepts in a clear manner, not get bogged down by syntax.",1518008702.0
AddemF,"Call me crazy but I think the real choice is between Python and logic circuits.  C takes its starting place too close to the middle of this spectrum on several axes: difficulty, abstraction, ease of cross-platform installation,  helpfulness in making progress from amateur to expert. The point of a consumer computer is that it allows users to think of data with physical and linguistic analogies. C steps too far from that right at the start. If you're going to abandon many of those analogies, you might as well go down so far that you get back to something concrete but which still teaches the idea of computation.",1518015557.0
orangejuicem,"My first language was C and I’m not sure it was the best choice. That being said, going from C to other languages felt like I was “upgrading” and coming into the world of high level programming. Java was my second and it felt like a luxury. I think if you start with something as easy as python, switching to another language might be painful and tedious",1518032993.0
piratejit,I found learning C first helped me a lot.  It is a little harder up front but I found I had a lot easier time later on in my computer science education than my peers who learned Java or Python first.,1518038415.0
magnificentbop,the answer is yes,1518039536.0
OishiiYum,"Like many folks here, python is more likely to be easier to beginners. I was taught java as an intro language but I found it to be too cumbersome. Looking at python I think it’s better especially since beginners need to focus on concepts and not the nitty gritty details of the language. ",1519693735.0
combuchan,"Python to start because it's readable, object oriented, and easily executed.

C's memory management and compiler optimizations were important with the constraints of the 1970s and today for high performance applications, but they're pretty much irrelevant for modern beginners.  ",1517972900.0
herefromyoutube,"Python. Python. python. 

The syntax and compiling of C will scare a lot of people away who might actually enjoy programming. 

",1517984221.0
Noxious3,"I'd say Python. It was my first language and I definitely believe it helped ease me in to programming and get a solid understanding of the fundamentals. I think C would have been overwhelming for me, but everyone's different.",1517970625.0
DrunkenMonk7,"I’m an amateur in programming, but professionally look after continuous improvement, process improvement, etc.  This whole conversation, the back for and forth over languages make me want to scream!  Why on earth do we still continue to have multiple languages for programming computers??  It is so blatantly inefficient to have all of these languages competing for our brain space, not to say anything of maintaining programs and language evolution.   
If we can send a Tesla Roadster to Mars (oops missed Mars, now Kiper Belt) then we can bloody well create one language to rule them all.  
That’s my 2 cents.  Thanks",1518006838.0
MyRobotDidIt,"I think that C is the way to go. My first language was Javascript, and looking back, I really wish I had started with C. When I finally did learn C, I couldn't stop trying to understand the material in terms of other languages, which ended up working against me, since many languages don't have some of the stuff that C does. I think C is probably the best language to develop good programming practices, and a real solid understanding of how computers work before moving to languages with easier syntax. ",1518019751.0
pengusdangus,"I am a huge proponent of C first.

Beginners should tackle the tough concepts early on so they are aware of how programming works and not how to hack something together.

FWIW I am not a systems engineer and am actually a Python/React dev. I still think C is a great starter point.",1518021163.0
ThisIsPlanA,"Java.

* You can cover object oriented design in the first semester
* The syntax is C-like and explicit
* Strong typing should be an absolute requirement for any beginner language
* Handling memory manually is way too much to ask of beginning programmers

I actually taught Intro to Programming classes for several years when I was a grad student. I've programmed in C, C++, Java, and Python (as well as a bunch of others I would never consider for an intro course). Most of my work is now done in Java and Python. My personal preference is usually Python, unless I'm working on a larger server-side or multi-threaded project and even then I would seriously consider using Python. But I wouldn't want to have to teach it to someone that's never coded before.

The difficulties that arise for students new to programming are seldom about how complex the code *appears* on the screen. The issues are generally (a) understanding program flow, (b) understanding the specific syntax of the language, and (c) ambiguity. 

(a) and (c) are best addressed by using languages that are explicit, even if that means additional verbosity. (b) is also absolutely requires compile time errors for syntax. Strong programmers or those who are naturally inclined are going to learn pretty much however and whatever you teach, but most students- particularly those that have never tried programming before- benefit greatly during an intro course from rapid feedback. 

Java checks every box, C most of them, and Python almost none.

tldr; 
Former Intro to Programming instructor: Java is a superior to either C or Python for true beginners, but if I'd take C if those were my only two options.",1518031064.0
,"I was going to cast my vote for Python because it lets you get to higher concepts with a minimum of syntax and boilerplate distractions.  But that's all said well here.  

So I cast my vote for Verilog.  I've been at this for two decades.  And if I was going to build a car, I'd make it out of atoms.  So why not start with the clarifying low-level binary logic of Verilog?  

FIRST, you design a computer with code.  THEN you get to write code for it.  At MIT, CS students have to build logic gates and processes out of physical things like water and marbles.  So then you really know what's happening.",1517979414.0
Finesse808,JAVA,1517964116.0
maronnax,"I'm no educator nor was I formally trained in computer science, but I say Python.  The reason being that you simply can do more cool stuff faster with Python than C or most other languages.  

It's pretty useless for most people to be a good computer scientist but a bad programmer, and 90% of programming skills come from just writing code.  Python lowers the barrier to getting cool stuff running and doing more exciting projects in a quicker period of time for the beginner.  

So I'd vote for making concessions, doing what I could with python and leaving types, linking, the details of everything off, more precise analysis for a second semester.  ",1517985997.0
sparr,"Are you teaching programming or computer science? If the former, Python. If the latter... much closer call.",1517992425.0
waz-wazy,"My first semester was Java after 3 semesters I used C for system classes. I would recommend Ruby because it is pure object oriented and in my opinion it is better than python. Between python and C I would go with python so you would not deal with pointers for now later you can teach your self C and there is a good book called ""learn C in hard way"" ",1517982780.0
maladat,"You need productions that keep track of how many 1s or 0s have been written.

Let's do 2p zeros and 3q ones so I don't have to write as much:

Something like:

NoZeroNoOne: {epsilon} | OneZeroNoOne0 | NoZeroOneOne1

OneZeroNoOne: NoZeroNoOne0 (go back to no zeros because now we have two) | OneZeroOneOne1

OneZeroOneOne: NoZeroOneOne0 (go back to no zeros because now we have two) | OneZeroTwoOne1

OneZeroTwoOne: NoZeroTwoOne0 (go back to no zeros because now we have two) | OneZeroNoOne1 (go back to no ones because now we have three)

NoZeroOneOne: OneZeroOneOne0 | NoZeroTwoOne1

NoZeroTwoOne: OneZeroTwoOne0 | NoZeroNoOne1 (go back to no ones because now we have three


",1517934846.0
ssbr,"You won't ever fuzz a function that adds two ints.

Generally you will only fuzz functions that take strings, and maybe some extra options. You call your function with a string and some options that you pull from the input (eg you can hash the string to generate some boolean flags). Check out the example fuzz tests that already exist, like for RE2: https://github.com/google/re2/blob/master/re2/fuzzing/re2_fuzzer.cc

Sorry for brevity, on phone and can answer more tomorrow if you still have questions.",1517907241.0
_--__,"Hi,

I have removed your post from /r/compsci - as per the sidebar, we discourage posts of this nature.  I recommend asking in /r/askcomputerscience.",1517894584.0
,Computer scientists hate them. Click here to see why!,1517885627.0
philly_fan_in_chi,"The MIT Advanced Data Structures course goes over this and other cool data structures like Van Emde Boas Trees.

https://www.youtube.com/watch?v=T0yzrZL1py0&list=PLUl4u3cNGP61hsJNdULdudlRL493b-XZf",1517923919.0
ninijay_,you could also speculate conditional outcomes and cache them. Oh wait.. Spectre,1517908871.0
,[deleted],1517902210.0
DamnableNook,"FYI, this guy is a spammer for SyncedRevies, some shitty spam content theft blog.",1517880002.0
NowImAllSet,"You'd be best to include what language you want to do this in. But even moreso, this isn't really the right subreddit for that type of question. You should remove this post, add the language and repost it on r/learnprogramming. Make sure to flair it with the homework tag.


For general advice, you'll need to understand how HTML works and can be read. You can then, using your languages libraries, read in the websites HTML just like a file, then read through that and find the appropriate information. For instance, if you are interested in a text field that is surrounded with `<p id=""score"">52</p>`, then you can read through the HTML programmatically until you find a paragraph tag that has an ID of ""score"". Using string manipulation, you can then strip away the excess noise to just get the value 52. Continue that logic for all the fields and values you need. Hope that helps. ",1517863006.0
DamnableNook,"FYI, this guy is a spammer for SyncedRevies, some shitty spam content theft blog.",1517880018.0
jmite,"It's undecidable! At least in the case where you have some specification for all inputs and outputs.

If you only need a finite number of input output pairs marched by your algorithm, then you can do it probably in linear time by building a program with a giant case statement. But then you get garbage for any inputs not specified i.e. there's no interpolation going on.

There will obviously be exceptions to this, for example, if you know your program is computing a polynomial than fitting to any number of inputs is decidable. But the arbitrary ""given a spec, give me a program"" is not.

To give some intuition, you could make a spec that says something along the lines of "" write a program that prints 0 if this other program halts, prints 1 otherwise"". And if you could generate a program matching that you could solve the Halting Problem.",1517851311.0
WhiteQuill,"You need to first specify what your complexity is with respect to, the rules such as the input and output?

Also slightly related, you try reading about kolgomorov complexity here https://en.m.wikipedia.org/wiki/Kolmogorov_complexity it might be what you're looking for",1517850821.0
maladat,"I'm surprised no one has mentioned it yet, but this problem is referred to as ""program synthesis."" It's a pretty active research area. It's closely related to ""program verification"" (given a program and a specification, ensure the program adheres to the specification).

https://en.wikipedia.org/wiki/Program_synthesis

The technical name for the computational complexity of this problem is ""really, really, really, really, really hard."" (I'm kidding - there isn't really a general complexity class for synthesis because the complexity varies tremendously based on how you define a program specification.)

To give you an idea, with general, high-level program specifications, the best we can do right now is synthesize programs of a few tens or MAYBE hundreds of lines of code.",1517938445.0
bremby,"The description
>creating a program out of a set of definitions or an algorithm with defined inputs and outputs

does not really mean programming, that can be just viewed as compilation / interpretation. Programming is not really just about implementing an algorithm, it's also about everything else, like choosing algorithms, considering which tools you use, thinking about UIs, data representations, documentation, ...

Programming is basically conscious thinking about a problem. If you want to classify that, you need to classify general human intelligence, and I don't know how to do that. But compilation is [classified as O(n^2 )](https://cs.stackexchange.com/a/22439).",1517851034.0
DamnableNook,"FYI, this guy is a spammer for SyncedRevies, some shitty spam content theft blog.",1517881691.0
Terr_,Yyyeeeaaah I have no freaking clue what your badly-formatted homework question is.,1517826038.0
timmyotc,Please read the sidebar. ,1517797908.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1517777653.0
erkaman,"In this artice, I describe a technique that can be used to
seamlessly copy-and-paste one image into another. I also provide
[source code](https://github.com/Erkaman/poisson_blend), that
demonstrates the concepts in the article.",1517774290.0
mendeza503,"Creator here! Developed a procedural mesh generator to visualize mathematical surfaces and view them on snapchat!

Some of the surfaces you see are enneper surfaces and klein surfaces. You can download and share the snapchat lens with this link: 

https://www.snapchat.com/unlock/?type=SNAPCODE&uuid=04094836871A404181546D8D52B68C05&metadata=01

If the link doesn't work, you can scan the snapcode here: https://s3.amazonaws.com/andrewmendez.me/files/snapcode.png

Let me know what you all think!",1517771303.0
greenflights,Cool! :D,1517784771.0
oijbaker,I thought that was a Pokemon for a second,1517857664.0
,[deleted],1517779528.0
vznvzn,"all in favor of the _history-making/ unprecedented_ class but the general field has no underlying fundamental theory! _until now! curiosity_ lies at the heart of AGI theory/ engineering.

**secret/ blueprint/ path to AGI: novelty detection/ seeking**

https://vzn1.wordpress.com/2018/01/04/secret-blueprint-path-to-agi-novelty-detection-seeking/

the new **MIT AGI** class slack channel is up to ~5k users. hope to hear from hackers, would like to set up slack channel for development. see deep-mit.slack.com

also note **MIT president Reif just announced university-wide MIT intelligence quest initiative** with research + industrial elements and goal to simulate _infant level intelligence_

https://iq.mit.edu/

http://news.mit.edu/2018/mit-launches-intelligence-quest-0201

",1517775554.0
mendicantbias343,https://en.wikipedia.org/wiki/Pretty_Good_Privacy#Early_history,1517765919.0
sailorcire,"People like Larry David! /s

https://youtu.be/O_05qJTeNNI",1517774039.0
,It'll give you part of the key if you ask it in a very stern voice. AGP (amazinglysupergreat good privacy) is better for this reason.,1517812152.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1517762975.0
irabonus,"Regarding which projects to do: If you're not super sure yet in which industry you want to end up, it really doesn't matter. Just having a few projects that were done in your own time, no matter the complexity, will place you above 90% of other people. The bar is really quite low.

Of course, if you already now roughly what you might want to do later, choosing something that's related to that area is useful, but not necessary.",1517745764.0
NowImAllSet,"Disclaimer: I'm at the very start of my career, and so take this with a grain of salt.

To be honest, I think you should focus a bit more on depth. I obviously don't know your experience, but what struck me as odd is that you started off saying you're in your first year but already fairly experienced in Java and know a bunch of other languages, and that you also are about to start learning C. But then also state you have little to no projects. Obviously, you're in your first year, so you're not going to be some guru. And there's also merit to learning as many things as you can to ""widen the net."" But don't do that at a cost of depth. I would be willing to bet that these languages you learn are much more complicated, and have far more nuances, than you realize right now. Knowing the syntax of a language, how to make for loops, variables, functions, classes (if applicable), etc is all scratching the very surface of a language and I personally wouldn't count that as enough to say you ""know the language,"" and I definitely wouldn't add it to a resume. I can learn the syntax of a new language in a couple days, and write some small to medium level projects in a language within a week, if not less. That's akin to learning how to change a tire on your car and fill up your gas tank and then claiming you're a mechanic. 

When I am considering if I know something enough to put it on my resume, I like to go and Google ""entry level XXX interview questions."" For example,  [Java interview questions](http://www.buggybread.com/2015/06/java-interview-questions-and-answers.html). If you read through these and find that you can't answer a majority of them, then that's a strong indicator that maybe you're skimming these too much. 

If you graduate with ""knowledge"" of a hundred different languages and frameworks, but no depth in any of them, you're just going to find a bunch of jobs you can apply to, but never actually get. 

Once again...graduating this semester, and been working professionally ~10 months now. I am very much a junior developer. So take it with a grain of salt.",1517761768.0
that_one_dev,This sort of stuff gets discussed on /r/cscareerquestions,1517755765.0
duskhat,"Candidate for what? The answer depends on whether you want to be a better candidate for grad school, software engineering, other roles in industry (teaching), etc",1517740613.0
FuriousMatt420,University of Manchester by any chance?,1517743237.0
FlinchMaster,"This happens as a result of applying de Morgan's law to eventually reduce to true. Here's a breakdown:

    ~(m * ~n) + ~n
    (~m + ~~n) + ~n  // Apply de Morgan's law
    (~m + n) + ~n    // Simplify ~~n to just n
    ~m + n + ~n      // Drop unnecessary parentheses
    ~m + true        // Simplify n + ~n to true
    true             // Simplify anything + true to true

EDIT: Had the tab open and spent so long before hitting save that other people already beat me to the explanation. I'll leave this here, since I do think the formatting is a bit clearer in this answer.",1517737105.0
angererc,"To avoid misunderstandings: ~ is not, * is and, + is or

~(m*~n)+~n = (~m+n)+~n=~m+n+~n=~m+1=1",1517736710.0
howicodestuff,"From de Morgan's theorem: ~(A.B) = ~A + ~B

So you have: ~(m * ~n) + ~n = ~m + ~~n + ~n =

(we know that ~~n = n)

=  ~m + n + ~n =

(we know that n + ~n = 1)

= ~m + 1 = 1

EDIT: show steps and rules used",1517736676.0
parliboy,Did you apply DeMorgan correctly?,1517736776.0
,[deleted],1517736835.0
JH4mmer,"Another way to prove expressions like these is to build a truth table (provided the expression itself isn't too monstrous.) Add two columns with four rows for each of the possible combinations of m and n, and then start working from the inside out.

In this case, add a column for ~n, and then another for m * ~n. Another for ~(m * ~n), and then finally one more for the entire expression. You will find that for any input, the result is True, which means the entire expression can be simplified away. 

In the usual case, the result won't be a tautology, so you if you want a simpler expression, you'll need to compare it to each of the standard operations (AND, OR, XOR, etc.) There are 2 ^ 4 = 16 possible circuits using 2 variables, which can be enumerated fairly easily, especially since 6 of them are basic operations or their negations. 

I wouldn't recommend this approach as a substitute for learning the properties of Boolean expressions, but I have found that many students who learn visually find truth tables easier to understand intuitively, which is quite important for real CS work.

Cheers!",1517757785.0
dabombnl,"The `~` (not) cannot distribute like that. In regular algebra (as negate) or in boolean algebra.

    ~(m * ~n) != (~m * n)
    ~(m * ~n) = (~m + n)
",1517767185.0
ajoakim,"I got my comp Sci degree back in 2007, don't remember any of this!!! ",1517737515.0
meridious3,"This podcast is exactly what you are looking for

https://softwareengineeringdaily.com/",1517633684.0
Samrockswin,I saw this on a similar question but the [Microsoft Research Podcast](https://www.microsoft.com/en-us/research/blog/category/podcast/) may be of interest.,1517639108.0
nayocum,[Talking Machines](https://www.thetalkingmachines.com/) is great if you are interested in AI,1517649708.0
selamtaB,"I like listening to ""How I built this"" podcast by Guy Raz. ",1517640442.0
robotikOctopus,"Reply all has some hilarious and tantalizing tech stories but they are more entertaining than technical. Recode decode has several interviews with high profile figures in the tech world. Software engineering daily is probably your best fit. Recently, Software engineering daily had an interesting machine learning hardware episode. ",1517638090.0
tema_emelyan,The best programming podcast: Codingblocks,1517648059.0
EmbeddedEntropy,"Along with Software Engineering Daily, I’d recommend Software Engineering Radio http://www.se-radio.net. ",1517665214.0
yelnatz,"Probably the best podcast that's software related that I've listened to: http://thisdeveloperslife.com/

",1517689622.0
ksmithbaylor,"I enjoy listening to Soft Skills Engineering (https://softskills.audio/), which is about non-technical challenges and experiences in the software world and is usually hilarious. Also The Changelog (https://changelog.com/podcast), which has a ton of episodes interviewing various people in the open-source community. More technical: Lambda Cast (https://soundcloud.com/lambda-cast), which dives into functional programming topics. Someone else mentioned ATP (http://atp.fm/) already, but it's pretty entertaining.",1517703749.0
bearinbowl,I really enjoyed Chaos Monkey though it’s not too on tech side but lots if insight on SV,1517649009.0
classhero,"The Jeff and Casey Show, like [the episode on the ABA problem](https://mollyrocket.com/jacs/jacs_0004_0047.html). This falls into the humor category. The two speakers work or worked on the RAD Game Tools products, and Casey is also known for [Handmade Hero](https://handmadehero.org/).",1517674914.0
naked_dev,You really want a good night sleep in this flight.,1517681499.0
,"accidental tech podcast is all about apple. 

Under the radar is about app development",1517682192.0
Roticap,Embedded.fm is a long running podcast a bit closer to the metal than most CS podcasts. ,1517683465.0
glavata,"Some of my favorite podcasts:

* [Software Engineering Daily](https://softwareengineeringdaily.com/)
* [Hanselminutes](https://www.hanselminutes.com/)
* [Software Engineering Radio](http://www.se-radio.net/)
* [Developer on Fire](http://developeronfire.com/)",1517695157.0
washeduplol,"Blockchain technology is fascinating to learn about. I’m not sure of podcasts, but there’s bound to be several.",1517632317.0
DamnableNook,"The OP is a spam account for some Russian software, btw.",1517602620.0
GNULinuxProgrammer,Take more math classes instead.,1517553103.0
_monstah,If you want to get better at CS then double major with Math. Number theory and combinatorics will help. Do public relations if you dont want to do Software Engineering and want to be a product manager.,1517556275.0
trout_fucker,No,1517551345.0
,"Yes. Your social skills would be well refined, which is something a lot of CS grads lack. Although, a business degree can often do the same, yet prove to be more valuable when combined with a CS degree ",1517589299.0
codegreen_,"Can someone explain the x86 instruction set, and how a processor is designed to fit with the x86 instruction set? Could you design a cpu completely differently, but make sure the x86 instructions work?",1517536609.0
Dubanx,"Bytes are measured as a power of two because of the way memory works. You need to be able to reference each potential location the information you're looking for might be in, and it takes exactly 10 bits to be able to reference any of the 1024 potential memory locations in a kilobyte. It makes more sense to organize data storage by powers of 2 because of the implications it has for memory allocation, and thus bytes are organized by powers of 2.

Hertz, on the other hand, are a measure of frequency. Simply put, it's the number of cycles the CPU executes in each second. There is no benefit to measuring them in powers of two like with storage. Thus hertz are measured as 1000 hertz = 1 kilohertz. Hertz is also a unit of measurement that predates modern computing, thus would not be in powers of 2.

Your teacher is almost certainly confusing bytes with hertz.",1517533282.0
ohnoigetit,"1000 is the correct answer (see [here](http://www.wolframalpha.com/input/?i=khz+to+hz)). Hertz is an SI derived unit (s^(-1)) and the factor for the k (kilo) prefix is 10^3 (see [here](https://en.wikipedia.org/wiki/International_System_of_Units#Prefixes))

He may have been thinking about bytes which can be described with either SI or IEC prefixes. SI prefixes use 1000 as the base where IEC prefixes use a 1024 base. Some fields classically use IEC prefixes for measures while most storage device manufacturers use SI prefixes (this works out for them since they can market a larger number).

For example:  
SI: 1 kB (kilobyte) = 1000 bytes  
IEC: 1KiB (kibibyte) = 1024 bytes

Some places will use kb or KB when in fact they really mean KiB, so that's something that's good to be aware of.",1517533832.0
jimsankey923,"Bytes are in powers of 2 because it works well with binary-based computations. Frequency (hz) is just a measurement, and I’m pretty sure the only reason your computer knows it’s CPU frequency is because someone hardcoded it to read out that way. So in short, I’m pretty sure your teacher is full of shit. - no source available",1517533101.0
everything-narrative,"Ther exists SI prefixes for 2^(10)-multiples.

- 1000 : Kilo :: Kibi : 1024
- 1000000 : Mega :: Mebi : 1048576
- Giga :: Gibi

Etc.

Nobody in their right fucking mind uses Gibihertz.",1517552727.0
fin_wiz,Send this to your teacher http://www.tomshardware.com/answers/id-2321432/ghz-1024-mhz.html,1517538959.0
kckcbbb,"/u/phlummox 's answer below is the correct one. If your professor's pedantry game was on point he'd know that what he's talking about is a kibihertz. If he doesn't like it, he can take it up with NIST. ",1517541192.0
,[deleted],1517534612.0
LongjumpingBoot,"A more technical look: in many CPUs today, frequency is created through an oscillator. That’s usually a piece of quartz, which has the property of vibrating at a fixed frequency when you put current through it. Now, the frequency depends on the shape, size and quality of the piece of quartz, so that means that no two pieces oscillate at the exact same frequency. So manufacturers like Intel, for their high-performance CPUs, build the oscillator, then measure its frequency to determine what class of processor it is (and how much they can sell it for!).
But that goes to show that frequency is just a measurement of a piece of quartz’s characteristics, and there is a good amount of randomness in it (physical phenomena aren’t based on powers of 2!).
Hope this helps!",1517540866.0
tetroxid,"To be ultra pedantic, 1024 bytes are a kibibyte. 1024 kibibytes are a mebibyte. A kilobyte is 1000 bytes, and a megabyte 1000 kilobytes. This was done to ensure consistency with the SI prefixes. 

But to be fair, no one I know gives a fuck and everyone still uses the SI prefixes for powers of 2 instead of 10.",1517553571.0
modeler,"There are many great answers here, but I'd like to add something in the attempt to be more intuitive.

CPUs address memory using an address bus - a group of wires where the memory address is the binary number on those wires, 1 for high and 0 for low. If you add another wire to the address bus, you are adding one more bit to the address bus, which doubles the maximum address you can write on the bus. (Similarly, adding a digit to a calculator's display allows the calculator to display number 10x bigger).

Now for frequency. Where did the duration of a second come from? It's 1/60 of a minute, which is 1/60 of an hour which is 1/24th of a day which is 1/365.25 and then some of the time it takes for a particular rock to orbit a rather ordinary star. In other words, the duration of a second is completely arbitrary. An accident of the way Babylonians used a 12- and 60-based counting system. If the earth took a little longer to orbit the sun, or span a little faster like it did millions of years ago, then one second would be a different duration, and 1000Hz would take more than 1 of those alternative seconds. From this, you can see that there is nothing physical that could tie particular frequencies to the operation of a chip.

Your Professor's mistake shows that they have do not have a deep conceptual model of the physics of a computer, nor of the world around them.",1517555334.0
MjrK,"> I can't find anything on Google if he's right or wrong but I don't think he is.

Maybe the Google is broken on the Internet where you are? https://imgur.com/xZAO8tk

Have you tried the Wikipedia? https://en.wikipedia.org/wiki/Hertz#SI_multiples
",1517583962.0
GhostMan240,Get a new teacher,1517580947.0
,"Your teacher is wrong. The Greek prefix ""kilo"" refers to a quantity of a thousand. Its only in the specific context of measuring data using multiples of bytes that we overload the term ""kilo"" to mean 1024 instead of 100, though there have been proposals to use ""kibi"" to refer to 1024. It's almost universally accepted that 1kB = 1024 bytes, and kilo = 1000 in almost every other context.",1517576308.0
NotInUse,"http://lmgtfy.com/?q=kilohertz

The first item to come up says:

“a measure of frequency equivalent to 1,000 cycles per second.”

Doing another search but adding the term 1024 doesn’t reveal any initial tin foil hat theories that contradict the first search which suggests your teacher is the only one who believes this.

Important life lesson: people in power are wrong more often than you think.
",1517599005.0
Coloneljesus,Tell him 1024Hz is a Kibihertz.,1517610983.0
Centropomus,"The DEC Alpha typically used a 1024 Hz interrupt timer for wall clock timekeeping and CPU scheduling. That might be where he's getting this from. CPU oscillators are designed by analog electrical engineers, and they work in base 10, even on the Alpha.",1517614982.0
cp5184,"As far as I know no.

Traditionally they were based on the oscillator.

So, for instance, say you have a 33MHz oscillator and a ""100MHz processor"".

Your processor would be running on a 3:1 ratio with the oscillator, so it would be running at 99MHz ideally.

I think you could run it at a 3.030303030303 ratio running it at something close to 100MHz, but iirc 99MHz would be more stable. e.g. 3.03 ratio would run at 99.99Mhz, 3.0303 would run at 99.9999Mhz

I'm not sure how much things have changed/become more complicated since a time when things were done in that kind of basic fashion.",1517616393.0
mcandre,"Neither clock rate (Hz) nor storage space are measured in powers of 2. The former, because it is more concerned with arbitrary speed in units of seconds, the latter, because the metric system and humans have been conditioned to think of things in base ten rather than base two. If only we had six more fingers (or two fewer)...

As Wikipedia and QI point out, computer storage is now retroactively metrics-ified, so 1 MB = 1 Megabyte = `1000 * 1000` bytes = 1,000,000 bytes = 1 million bytes. This is stupid, because even people in the industry forget, and old values have to be carefully reevaluated. The more precise way to express storage is 1 MiB = 1 Mebibyte = `1024 * 1024` bytes, which uses unambiguous notation and maps better to how storage is implemented, which tends to be factors of powers of 2.",1517553640.0
Bowlslaw,"Going to college in 2018. 

You fools...",1517581287.0
codepimp2020,Answer: it depends if you are referring to the speeds in decimal (e.g. 1000) OR in binary (1024),1517538272.0
,[deleted],1517565549.0
vanderZwan,"> Suppose you have a mysterious box that takes one of two possible inputs — you can press a red button or a blue button, say — and gives back one of two possible outputs — a red ball or a blue ball. If the box always returns the same color ball no matter what, it’s said to be constant; if the color of the ball changes with the color of the button, it’s balanced. Your assignment is to determine which type of box you’ve got by asking it to perform its secret act only once.

> At first glance, the task might seem hopeless. Indeed, when the physicist David Deutsch described this thought experiment in 1985, computer scientists believed that no machine operating by the rules of classical physics could learn the box’s identity with fewer than two queries: one for each input.

>  In a 2007 paper, Calude broke down Deutsch’s algorithm into its constituent quantum parts (for instance, the ability to represent two classical bits as a “superposition” of both at once) and sidestepped these instructions with classical operations — a process Calude calls “de-quantization.” In this way, he constructed an elegant classical solution to Deutsch’s black-box riddle.

So I [looked up the paper](https://arxiv.org/abs/quant-ph/0610220v1) and have some probably really naive questions about the first algorithm described in the article.

Here is the same puzzle in Claude's paper, described as a formula 

> Consider a Boolean function f : {0,1} → {0,1} and suppose that we have a black box to compute it. Deutsch’s problem asks to test whether f is constant (that is, f(0) == f(1)) or balanced (f(0) != f(1)) allowing only one query on the black box computing f.

Ok, straightforward. I'm going to skip the quantum solution and go straight to Calude's supposedly classical solution:

> We de-quantise Deutsch’s algorithm in the following way. We consider Q the set of rationals, and the space Q[i] = {a+bi | a,b ∈ Q},(i =√ −1). We embed the original function f in Q[i] and we define [a classical analogue of the quantum evolution Uf] (...) The following deterministic classical algorithm solves the problem: Given f, calculate (i − 1) × f(1 + i). If the result is real, then the function is balanced; otherwise, the function is constant. (...) The explanation is not deep, just the fact that classical bits are one-dimensional while complex numbers are two-dimensional. Thus one can have “superpositions” of different basis vectors.

So here is what I am struggling with: can you really say that you're solving the same question, if you redefine the function to the complex plane, and use a complex number as input? (ignoring the discussion of whether that is an interesting and useful approach in and of itself)

Can you really claim to do ""one query"" in this case? I mean, let's first quantify what counts as *one* query, shall we? One way would be to look at the fundamental amount of information exchanged, which means looking at the number of input and output values. With the non-quantum, non-complex *f : {0,1} → {0,1}*, you have *one* input value and *one* output value, each with two possible states. Calude's solution extends function to the complex plane, and we have *two* inputs (real and complex) and *two* outputs with *four* possible states in total. Similarly, the quantum version requires a set-up with a superposition of two states, each with a probability attached to it. Which again could be considered having *two* input variable (one per probability) and *two* output variables, and if you look at the paper, there are four possible output states.

So in the classical version, to determine if f is constant requires inserting and extracting *one* unit of information *twice*, and in the other two, we insert and extracting *two* units of information *once*. In both cases the total number of units of information inserted and extracted is the same. If we go back to the analogy of the mysterious box with a red and blue button, it kind of looks like both the quantum and complex solution pretend that pushing both buttons and getting a ball that is neither fully blue or red counts as one measurement. I mean, it still is a neat trick if you can use that to determine what will happen if you push *one* button, and I'm sure the other algorithms mentioned really only work with quantum computers, but still: it sounds a lot less ""magical"" when described this way.

So my question is basically: am I understanding this correctly or not?",1517623712.0
,So could it excel in not classical algorithms?,1517549912.0
possiblyquestionable,"I'm skeptical about Claude's claims. For one thing, his dequantization argument was published in 2007, and it has only been cited 18 times in the last 10 years. If true, his claims that any quantum algorithm can be dequantized to a classical one without any loss of efficiency would have been a revolutionary breakthrough.

In any case, the remaining sections of the article questions the ability of quantum computing companies to find enough of a foothold to gain traction and eventually cannibalize it's classical siblings. That's definitely a valid point, but I don't think the two competes in the same niche. Having quantum computers does not obliviate the need for classical ones, and vice versa.

I believe this article brings up some fair points about the market competitiveness of quantum computers. However, their assumptions that QC sits within the same niche as classical ones is misguided. In particular, even today, we have a thriving and diverse fanout of domain specific computing hardware, each saturating it's own separate ecosystem. QC will likely be no different.

On the other hand, the question of its theoretical abilities by citing unverified research on dequantization seems to be a big flaw in this article.",1517589348.0
awhaling,That was really interesting. ,1517571990.0
johanvts,"Start by solving one of the many difficult subproblems, like speech recognition. Good luck",1517527304.0
BlckHorizon,"Not to discourage you, but that fact that you’re asking us with not even a clue how to start means that what you’re trying to do is beyond your current skill level. However although you might not succeed in building one, you should try since you will learn a lot on the way, regardless of if you actually complete it.

If you are extremely motivated to do this then I would start by make sure you have a solid grasp of linear algebra, multivariable calculus and probability theory. Then start diving into fundamental ideas in machine learning, and build up from there to stuff like natural language processing, information retrieval and speech recognition. I’m happy to recommend resources for the earlier stuff.
",1517547250.0
foreheadteeth,"[Alpha Zero](https://en.wikipedia.org/wiki/AlphaZero) plays a variety of games. Edit: [this](https://arxiv.org/abs/1712.01815) is the paper.

If you're just playing chess variants, I made an ok AI for a couple of chess variants using the standard [alpha-beta pruning](https://en.wikipedia.org/wiki/Alpha%E2%80%93beta_pruning) used in standard chess engines. You just have to implement a move generator for your game using your custom chess rules.",1517522050.0
SolarFlareWebDesign,Article is short and to the point. Interesting that each thread gets it's own store buffer. Wonder where L2/L3 comes into play?,1517537694.0
heyandy889,"You know, I didn't really care how x86 worked until I started seeing more and more exploits for it...

FPGA all day? :-/",1517537538.0
SevenVirtues,"If I read that correctly, 2^k should be greater than or equal to n so that the number of elements in the array is a power of 2 but also large enough to contain all n elements.

    2^k >= n. 

Using logarithms (base 2) you rearrange the equation:

    log(2^k) >= log(n)
    k * log(2) >= log(n)
    k >= log(n)

We want k to be the smallest integer that satisfies k >= log(n) so we just round up to the nearest integer.

    k = ceil(log(n))

So in the equation 2^k * 8h we replace k with ceil(log(n).

    2^ceil(log(n)) * 8h
",1517517253.0
TomvdZ,"NP does *not* stand for ""non-polynomial"".",1517489074.0
VermillionAzure,"I suspect that it could possibly be neither, but be a part of one of the even harder complexity classes, such as EXPTIME or PSPACE-complete. Go's position winning evaluation is known to be PSPACE-complete, and given the large amount of interactions and permutations of deck possible with Yu-Gi-Oh and the possibly large branching factor and the multi-phase system. The video game Braid has a paper around that considers an upper-bound on its complexity to be EXPSPACE.",1517493492.0
sschoener,"Let `L ⊆ ∑*` be a language. Then `L^+ := L ∪ L^2 ∪ L^3 ∪ ...` and `L^* := L^+ ∪ {𝜀}`.
For more information, see [the Wikipedia page](https://en.wikipedia.org/wiki/Kleene_star), especially the section on the Kleene plus.

Thus the statement `L^+ = L^*` is effectively equivalent to `𝜀 ∈ L^+`. Note that this is *not* the case when `L` itself is empty. In that case, `L^+` is also empty, but `L^*` is the language containing *only* the empty word `𝜀`.

Now, by definition `L^+` is the union of all non-zero finite repetitions of elements of `L`. Note that `w^n = 𝜀` implies `w = 𝜀` for any non-zero natural `n`. Hence `𝜀 ∈ L^+` implies `𝜀 ∈ L`. The converse is, of course, also true.
Therefore, `L^+ = L^*` iff `𝜀 ∈ L`.

Edit: and if Reddit had a preview feature, I would certainly have formatted this differently... :(",1517486410.0
VermillionAzure,"The definition of L^+ is `L, L*`, since the regular languages can be described as the composition of non-terminals and terminals using three operators: `,`, `*`, and `|`, or sequencing, Kleene star, and alternative, respectively.

Because the accept/reject set of `L` is different than `L*` most of the time, usually the two are not equal. However, the case you specified is when `L = {<empty>}`. This is because the empty string is a fixed point of the three operations, from what I understand.",1517477466.0
LoveOfProfit,"I'm a junior data scientist at a company that does marketing on behalf of universities across the country. My job is fun, challenging at times, fairly flexible, and overall pretty low stress. Salary could be better but I'm in a fairly low col area, getting 80k after bonus. I work 4 days a week though and of that, often 1 to 2 times a week from home.",1517459773.0
InfusedStormlight,"I work for a startup in a semi-major tech hub making 47,640. I luckily don't have to work more than 44 or so hours a week if I don't want to, and I often don't. I'm passionate about my job and work in a ton of different areas, so I'm gaining experience. ",1517461821.0
frenris,"test sub*_script_*

how make subscripts work? I tried

    test sub*_script_*

above and similar in my OP based on https://www.reddit.com/r/jamt9000/comments/beuzq/subscript_and_superscript/",1517454365.0
,[deleted],1517447566.0
Jaxan0,"I don’t think a good analogue exists. Computational geometry solves real problems in engineering. And I cannot imagine music helping there. 

However, the field is called algorithmic music, or algorithmic composition. I’ve seen some fun programs based on cellular automata or fractals. But that was a long time ago. ",1517468968.0
,[deleted],1517472873.0
GNULinuxProgrammer,"Computational geometry is pretty intimate with other parts of CS and mathematics such as differential geometry, algebraic geometry, algorithms etc... I can't find a reasonable analogy to music. Back when I was a researcher my research was on Math&Art from the perspective of CAD systems, computational geometry etc... So, I can kinda see where you're coming from, it is certainly possible to apply mathematical and engineering solutions to art. But in the case of music you'd have completely different algorithms, independent of geometry, so maybe just search the field itself?

TLDR what you've in mind is possible but I can't think of an easy relation to that and geometry.",1517476100.0
VermillionAzure,"**There is structure to music, and not all of it is ""boring"" like today's 4-chords deal:**

- Here is an insane example of how technical music theory can get to decompose a piece and compose something very similar: https://www.youtube.com/watch?v=rDwCamxxNyI
- Instrumentalist composers from every time-period and culture seem to have their own flavor and structures that contribute to the history of music, and, thus, to musical structure and composition.
- Today's wide range of instruments and the ability to control the sound of your instruments with synthesizers and post-processors is amazing! Every part of modern DAWs is amazing IMO, from the processing power to the GUI to the way they manage the logistics of music and playback. Perhaps you can start there!


",1517496371.0
,[deleted],1517446916.0
fin_wiz,Stats,1517425999.0
,[deleted],1518461770.0
stupidCSstudent,"I have two bachelors because I made the first choice poorly but many people have said in my tech interviews they like that I have a non tech degree because it allows me to effectively communicate.  If you can do something like english or something as another posted suggested linguistics, it'll help when you have to communicate complicated ideas to managers and coworkers",1517449831.0
Learfz,"So it says they're installing units at 281 intersections to improve traffic.

I guess they'll probably train an algorithm to minimize time cars spend waiting at those intersections by tweaking the light timings? I haven't been following machine learning that closely, but that does sound like a promising application of neural networks, right?

I just wonder what'll happen to the intersections without the cameras.",1517454743.0
johnyma22,"Optimizing driver experience in high density human population areas has the effect of increasing car volume in areas you want to reduce air pollution.

But in theory moving forward you could tweak this to prioritize access for cars which emit lower emissions or to shift traffic through lower pollution areas.

If it's just a dumb light timing system without other external factors then mheh",1517468881.0
joker2895,"GPUs do have parallel execution units and memory . Technically , it could replace a CPU but the performance would be obviously terrible.",1517389111.0
lrem,"In a sense, maybe. It can perform computations independently of any other components. However, this comes with huge limitations:

- The computation is hugely inefficient in face of branching, making execution of your arbitrary logic (normal programs) very slow compared to CPUs.
- It can't really do i/o, you need something to provide it with programming and data.

Thus, you quickly arrive to the conclusion that even for GPU intensive workloads you still want a traditional computer.",1517393546.0
drgrd,"A GPU is not one cpu, it is thousands of really simple cpus. These computers can make decisions but are much slower at it, so they work better when most of the decisions are made by someone else (a cpu). The gpu is like a coprocessor for a cpu. The cpu sends data to the gpu and tells it what to do, and the gpu does it.

The other thing that is different is how they make trade offs. All processing contains trade offs, and most modern hardware optimization (pipelining, cache, tlb, branch prediction etc) is in response to these trade offs.  

A cpu is designed to do things fast, but one at a time. It is designed to be as fast as possible at finishing each task as it comes in. The tasks are unpredictable, but it tries to predict anyway. We call this a latency core- the goal is to increase the *speed* of work done. Sometimes we make the wrong decision or prediction and have to back up and try it again. That’s the trade off

A gpu is designed to the same thing to hundreds of different pieces of data. The work is more regular and it is done all at once. We call this a throughout core- the goal is to increase the *amount* of work done. Sometimes, some of the results won’t be used because of the efficiencies involved in doing everything at once, and that’s the trade off. ",1517405778.0
TarMil,"A graphics card does indeed have a processing unit (what you call ""its own cpu"", but is technically the component that is called the GPU) and its own RAM. So yeah, it's like a mini computer, with some differences. The main difference between the GPU and a classic CPU is that the GPU is massively parallel: it is designed to execute the same computation on thousands of bits of data at the same time. That's what graphics programs spend most of their time doing: applying the same transformation to thousands of 3D vertices, and then to thousands of pixels. So it is programmed in quite a different way: you can't just take a program written for a classic CPU and run it on a GPU, you would need to adapt it.

TL;DR: yes, but a different kind of computer.",1517393830.0
irrationalskeptic,It would be like attempting to ride 100 duck sized horses (I guess a CPU would be a horse sized duck?) ,1517418993.0
cirosantilli,https://superuser.com/questions/308771/why-are-we-still-using-cpus-instead-of-gpus,1517434073.0
thedude42,"Huh, I’d expect there to have been at least some mention of modern GPU’s being turing complete as the measuring stick for them being “computers”... or has something changed in the compsci world where this isn’t the popular measure of what a minimum computer is?",1517411678.0
mendeleevorama,"GPUs complement CPUs, not replace them. ",1517413991.0
AndyTheAbsurd,"Yes! But also no.

GPUs *can* be used for general-purpose computing, but it's kind of a hack. GPUs are optimized for typical graphics tasks - rendering, texturing, keeping track of the polygons that make up most 3D models - which means that while they're very good at those, they're kind of crap at everything else.  However, all of the stuff that GPUs are good at requires massive parallelism, so they can also be used for stuff where you want to do thousands of variations on the same task simultaneously - like cracking password hashes, where you need to try every password (or, if the password is known but the hash's salt isn't, every possible salt) and there are literally millions of possibilities.

If you want to know more about this stuff, google for ""GPGPU"". The first ""GP"" stands for ""general purpose"".",1517417801.0
fj333,"GPU's don't have CPU's.

Graphics cards have GPU's. And that doesn't make them computers, anymore than motherboards with CPU's are computers.",1517461584.0
Sfloy029,Paper is in polish. Translated into English with google: [here](https://translate.google.com/translate?hl=en&sl=pl&tl=en&u=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FKrzysztof_Wecel%2Fpublication%2F322530592_Cechy_artykulow_oraz_metody_ich_ekstrakcji_na_potrzeby_oceny_jakosci_informacji_w_Wikipedii%2Flinks%2F5a5e8cb6aca272d4a3dfc6bb%2FCechy-artykulow-oraz-metody-ich-ekstrakcji-na-potrzeby-oceny-jakosci-informacji-w-Wikipedii.pdf),1517391051.0
Cleanumbrellashooter,"I'm no expert, but a few things I've noticed. There is a high correlation between people who end up being successful developers and those who are passionate/enjoy it.

Also, software development and computer science are not the same thing.

Start early, and try to teach yourself as much as you can. Make a GitHub account and start using it for projects, your school classes will most likely not be enough to get you an internship or a job, you'll need additional projects and experience.",1517373946.0
VermillionAzure,"- Read the [FAQ](https://www.reddit.com/r/compsci/comments/pkitn/frequently_asked_questions_on_rcompsci/).
- Computer science, as a discipline, is closer to mathematics than programming much of the time.
- Mathematics is often required and desired for medium to high-paying jobs in niche areas.
- Starting early and learning to program is often a big plus, as once you go through college and maintain your programming skills, you're probably going to be able to combine both quicker than new grads and be more successful.
- College is competitive, and money is very important. Apply for scholarships before you can't go to your dream school.",1517496571.0
balefrost,"It depends on a lot of things, including language and compiler settings. Asserts are often automatically eliminated in non-debug builds. ",1517376863.0
GNULinuxProgrammer,This is off topic in this sub.,1517401070.0
,[deleted],1517369630.0
DSrcl,"Are you asserting some expensive-to-compute expression? If so, yes.",1517537548.0
fin_wiz,/r/computerscience,1517426538.0
iwantashinyunicorn,Is he still looking for people who are prepared to work 100 hour weeks for minimal pay until they burn out and get replaced by the next generation of interns?,1517347346.0
fin_wiz,I used to look up to him too until I read his job posting.,1517426161.0
chrisgseaton,"Sounds like it shouldn't be a problem. They'll care you can do research, which your research experience shows, and they'll care you can program, which your internship shows. Make sure you have a good idea about what you want to research in your PhD and you'll be fine I'm sure. Lots of people love some interdisciplinary experience even.",1517326302.0
Darkfeign,"If you've already done a master's in CS, it's perfectly feasible. You might have more luck if you're aiming it towards research at the intersection of your to fields but if not, it will come down to you motivating why you want to do the PhD you're applying for and demonstrating that you have switched to focus on an area you're really passionate about etc.",1517326297.0
wanze,"I personally know a bunch of people with a master's in communication/social sciences who then go on to take a softer CS PhD. I'm sure your path would work just as well. If your advisor thinks you'll be able to pull it off, then that's all you need.",1517331782.0
ice_planet_hoth_boss,"It depends on what you want to focus on in your PhD.  Generally speaking though, you should have a good command of the ""theoretical"" side of CS, which is mainly discrete math and algorithms.  ",1517332286.0
MarcusP,How does this work for graphs with weights that are all the same?,1517248619.0
Krypotn,"That would probably not produce an optimal max-cut, since informally speaking, a min-cut algorithm would still minimise the number of edges, while a max-cut includes as many edges as possible.

One could think about a graph where all edges have the same weight (or no weights at all) to understand why your reduction does not work.",1517248652.0
_--__,"Imagine a graph with one edge, weight 3, from S to A, and two edges, each weight 2, from A to T.  The max-cut of this graph is {S,A},{T}; but the min-cut of your transformed graph is {S},{A,T}.",1517263369.0
,[deleted],1517248495.0
IMightExist,"Before you finalise topic, make sure that,
1. There is someone who can guide you
2. Your group members are interested and willing to contribute
3. It's not binary(i.e works or doesn't work), but rather something that could be decreased in size if things don't go as per plan

I didn't care about the above 3, and now I am left working alone on a project, with my team members doing nothing, the guide being useless as he knows nothing about this, and the project is basically doomed as it's not going to be usable by the end.

That said, see if anything in the blockchain, blockdag, deep learning fascinates you(and the team), otherwise, even picking something that is not taught in your university might be a good idea. Also, while you use research papers for getting ideas, remember that this stuff is cutting edge, so getting help is very hard if you get stuck.",1517247231.0
ITwitchToo,"Oh, you should definitely do a Linux kernel project if you can (and want)! Check out https://wiki.linuxfoundation.org/gsoc/google-summer-code-2017#linux-foundation-gsoc-project-groups for some ideas (assuming they weren't all implemented last year).

Another hot topic you may find interesting is fuzzing (to find bugs and security vulnerabilities), especially coverage-guided fuzzing (i.e. instrumenting the code so the fuzzer knows which branches are being taken and can keep mutating the most interesting test cases found so far and/or try to maximize the code coverage). Check out [AFL](http://lcamtuf.coredump.cx/afl/), [honggfuzz](https://github.com/google/honggfuzz), [libFuzzer](http://llvm.org/docs/LibFuzzer.html), and [oss-fuzz](https://github.com/google/oss-fuzz) to start out.",1517249333.0
lordalch,What about building a something involving blockchain technology? It's blowing up in industry right now. Getting some experience in a subfield that isn't typically taught at the undergrad level could really set you apart.,1517245694.0
theantigod,"
https://www2.units.it/ipl/students_area/imm2/files/Numerical_Recipes.pdf

10.9 anneal traveling salesman problem by simulated annealing
",1517242430.0
VermillionAzure,"Implement non-trivial net code for a simple video game! A blog called IT Hare goes through a physics simulator with balls and its very in-depth with optimizing its net code down to bytes and going through different strategies and testing with different packet reliability scenarios. Fighting games and FPS games are two areas where net code is extremely important, and I bet people spend a lot of time on this.

Perhaps you can try OGRE3D, SFML, or Cocos2DX or even Unreal Engine in C++, and then switch to something like Boost.Asio or use the raw sockets API available. These are all C++ code bases that will broaden your knowledge. But it sounds like you want some sort of networking experience.

Alternatively, write a web application using alternative technologies so you can get web experience using the classic backend/front-end APIs in something like Node and Javascript. But theres many different technologies for this, and everything ends up leading to C or C++ for the language cores for most languages. Python is usually CPython, Ruby's implementation is in C, and JavaScript VM's are written in either C or C++. Stuff like Rust and Elixir, however, appear to be more niche but more unique. ",1517247734.0
bstamour,"You say you are interested in C++. How well-versed are you in template meta-programming? Do you think you could rewrite some functional algorithms so that they execute during compile-time, instead of run-time?",1517250138.0
ThomasAger,You should do something the advisor for your project can help you with.,1517254701.0
spinwizard69,"I'm afraid you aren't prepared for this step in your development.    You should be focused on exploring concepts not looking for a canned topic.   

I'd take inventory of your hobbies and other intellectual interests and see if you can't find a way to apply new comp-sci tech to one of them.    Why?   Well at this late date you need to have something that drives you and which with you might find a novel computing solution.

Baring no good fit there, the next best thing to do is to take a dry subject in comp-sci and see if you can expand upon it.   By dry I mean something that you haven't personally gotten involved in something new to you.    Frankly if you look at large projects like LLVM/CLang there is plenty of areas ripe for improvement, some known just waiting for a person to research and implement and some likely completely unknown to the industry.   LLVM's suite of tools involve all sorts of technologies not just the world of compiler development.

Speaking of large code bases, much of the global warming suite of tools gets little hard reviews for software quality.    Development of tools to do quality reviews such large code bases might be a passable subject.   Of course this would have to be adviser approved but i find it to be a fascinating sub field.    Of course this is also an area of interest to the LLVM crowd with many tools having been developed by parties in that world.   The idea here is that software quality and the automated monitoring of code bases is a very broad area and may spark an interest in you.",1517257554.0
mobilecheese,"Do not worry about your ability to do ""anything advanced"". Half of the point of the project is learning to do something advanced. You will be researching for much of it!",1517258669.0
DustD,Do some cool stuff with Kubenutes and containers,1517245307.0
Jaxan0,I can imagine that most of the research is done at universities and not so much in public or online communities (like many niche topics). Did you search for workshops or conferences? Those might lead to places. ,1517247004.0
VermillionAzure,"The Theoretical Computer Science Stack Exchange website is a good place to post questions. Finding an IRC channel may be a good idea, or emailing a researcher to join their mailing list or chatroom.

I've found the /r/programminglanguages subreddit to be a great community for me because we have an IRC channel that's relatively popular. Perhaps you can create a group as well and advertise it? ",1517248305.0
remigijusj,I used to be interested in this topic a few years ago. It is partially overlapping with quantum computing which is much more trendy nowadays. You could try to find quantum computing groups or sites and find other people interested in reversible computing.,1517253439.0
motionSymmetry,do you mean [abelian groups](https://en.wikipedia.org/wiki/Abelian_group)?,1517242732.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1517213660.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1517198466.0
StandardAir,"If it was novel, what would you do with it? 
I can think of two reasons why you might not discuss it with a professor. 
1) You think it's profitable
2) You are worried about not getting credit.

For topic one, if you think it's profitable, the best proof of its value is using it to make piles of cash.

With topic two, just do something to make it clear you had the idea first. Most college professors aren't going to steal your ideas. If your professor thinks it's really awesome and novel they will help you get it published which would be super cool. They might get their name on the paper, but you would in all likelihood be the primary author and get most of the credit. As a byproduct the university would also get some credit too, but that doesn't diminish from your accomplishments in anyway. 

In reality though, it's probably not novel and your professor will really help save you a lot of wasted time. ",1517196019.0
Jjangbi,"What advantages does this data structure have over another data structure? Is it faster in a certain situation than all other data structures, or fast enough with other strengths? How can you know this if you don't have a general knowledge of most existing data structures and their strengths and weaknesses? There are so many different variations of a tree. I think it'd be best to reveal it to your professor or colleagues and discuss it with them. It may prove to be a good exercise.",1517237571.0
cognificent,"Well, what operations does your data structure support? Is it a new implementation of an existing abstract data structure, or entirely new? If entirely new, do you have an algorithm in mind that uses the operations it provides?

I feel like source code and software are usually the things that are at risk of being commandeered, but it depends a lot on the personality, workload, etc of the professor. Is he working on a lot of publications already, is this his area of research or is it just a class he's teaching?",1517196415.0
r2m2,"Your professor will be extremely helpful in finding relevant literature on the subject. However, if you want to look yourself, I'd start with trying to narrow down the particular area of data structures that yours falls into and then read several survey articles on the subject. That should give you a decent sense if you're on to something novel. 

Also to what extent have you rigorously worked through the properties of your DS? Do you just have an implementation? A proof of correctness? Worst-case vs best case vs amortized efficiency of insert/delete ops? These all are critical factors in whatwvee next steps you take  ",1517242991.0
VermillionAzure,"From what I understand, copyright over a data structure is considered non-kosher. Can you spend the money to legally defend it? One thing good about sharing ownership with the university is that they can fight for themselves and you can also gain the benefit. One of my former professors at my school did research at Carnegie-Mellon and it turns out CMU won a patent case over one of his patents. He shared it with a few people, and got millions apparently. So he went bye-bye.

Sharing the idea as research may be economically viable if you cannot currently apply it in some major flagship product. 

",1517248685.0
nlcund,"The only thing they might get is a patent, and that would require some useful application.  There's a small chance that someone could steal the idea and claim credit for it, but that would be plagiarism.  

If you're worried about not getting credit, code it up and check it into github first.  ",1517280414.0
IncendiaryFarrow,math is a major key for theoretical concepts and most algorithms. you can not avoid it if you want to be a successful computer scientist.,1517188971.0
netfiend,"Some examples of possible jobs include, but are definitely not limited to: Systems administration, medical software development, graphics hardware development, computer security, and video game development.

I too was intimidated by the required math courses, but got through it. It's generally a good idea to keep up with the course material as new concepts are taught. New lessons tend to build upon old ones. Work on solidifying your understanding of the class material alongside classmates and/or your professors.

Do everything you can, regardless of what you're studying, to maintain an overall (or major) GPA of a 3.0 or higher. You can still find jobs with a lower GPA, but having at least a 3.0 makes it easier. Most of the jobs that I've applied don't ask about my GPA, but it does matter to some companies.

While in school, try to create projects on your own time. Make things for your resume that show your ability to code, implement data structures, and maybe even think outside of the box. Additionally, getting one or a few internships throughout your educational career could increase your chances of getting a job while other people are still searching.

Edit: Most of my fellow computer science friends need little to no math in their jobs. ",1517169686.0
internetcharles,"I would recommend taking a compiler course, as it will likely cover different parsing strategies, which will give you insight into constraints of most language grammars, and I think you will find a lot of overlap with your linguistics background. The “red dragon” book is one of the seminal compiler books. I forget the name, but I think you’ll be able to find it with a google. 

A course in something like programming language theory / design would also be beneficial. My book for the course I took in college was “Structure and Interpretation of Computer Programs.”  This will give you a good basis for understanding metaprogramming strategies, type systems, and language attributes. You will also gain exposure to the lisp family of languages. 

Good luck in your learning!

Edit: I remembered one more book which you may find useful, ""How to Design Programs,"" which is similar in theme to the SICP book.",1517165593.0
foreheadteeth,"I'm a math prof so perhaps my point of view will be helpful since you're a math major.

The basics of formal languages (context free grammars etc) is very easy, you can either get a book or google it, it's not hard.

I think what is much more interesting is the next level up. I was recently playing with [Coq](https://coq.inria.fr/) and [Isabelle](https://isabelle.in.tum.de/) (my favorite right now is Isabelle).

You can use those to do [formal proofs of important mathematical results](https://www.ams.org/notices/200811/tx081101382p.pdf), but in my opinion most use cases are for formally verified software and hardware.

For example, I'm thinking of trying to get a student to do a formally verified eigenvalue solver. You take whatever standard algorithm you want and you write a proof in Isabelle that it is correct, and then you get an efficient implementation out of your proof. There's a lot of stuff to be done here because there are a lot of algorithms that are hard to implement, so a proof is helpful.",1517166169.0
nacraile,"Details will vary from compiler to compiler, but usually they'll just choose a conservative baked-in default; rather than trying to get clever.  So, for example, x86-64 gcc will by default produce machine code that can be executed by the oldest x86-64 processor in existence.

You can use command-line arguments to target newer instruction sets (this trades compatibility for performance).

You can compile for different architectures than the one executing the compiler.  This is called cross-compiling.",1517151822.0
nthcxd,"The terms used here are “host” and “target.” Host machine refers to the machine that does the compiling, ie taking source files and does it’s thing to produce machine code. Target is the machine for which the produced output is expected to run on. 

In a simpler case, the compiler would simply produce runnable on the machine it is compiled on, and in such “default” case, it would read cpuid (cpus have instructions that can be used to identify its ISA - instruction set architecture - and other features) and produce the binary that’s compatible. 

It is possible to compile for a different ISA and doing so is called crosscompilation. So you can say produce an ARM (target) binary on an x86 (host) machine using a cross compiler.

Well known compilers have modular architecture where they can ingest source files written in different languages (compiler front end) and generate code for different target architectures (compiler backend aka code generator). ",1517160167.0
myrrlyn,"""Compiler"" is a very overloaded word, and furthermore it oversimplifies what is actually going on.

GCC and LLVM, the two major compiler toolchains, are composed of very long and complex transformation passes, and only the very last couple target a real instruction set and architecture.

GCC has an internal representation about which I know little, LLVM has LLIR, and there is also a project called Cretonne which has an IR. These ""intermediate representations"" abstract over all supported processors and ISAs, and are basically a target-independent ASM.

The last stage of compilation turns these IRs into machine code, and you can select any IR&rarr;machine code transform you want.

With GCC, you typically install a version of GCC that is specially compiled (so step 1 is to use your native GCC, self&rarr;self, to compile a new GCC that is self&rarr;other, and step 2 is to execute THAT GCC, still on self, to make an image for other) for a target processor family, and use [the `-m` flags](https://gcc.gnu.org/onlinedocs/gcc-4.5.3/gcc/i386-and-x86_002d64-Options.html) to further narrow the targeted range. For example, `-march=native` will target the processor on which GCC is executing.

With LLVM-powered compilers like `clang`, `lldc`, or `rustc`, you just have to tell the front-end what target to feed LLVM, or you can invoke `llc` (the LLVM project's LLIR-to-machine compiler) yourself with `-march` and `-mtriple` flags. (Note: specifically, GCC and LLVM's `llc` actually emit *assembler language* for the target, and then invoke the GNU assembler `gas` or the LLVM assembler `llvm-as` to finish the job, but assembler language is target-specific ANYWAY and the assembler is just a text-to-binary transform.)

>Ancillary questions include: could I compile for, say, ARM or PowerPC on an x86 machine?

This is the *only* way to program embedded devices, or to bootstrap a new processor type. Microcontrollers cannot execute a compiler themselves; you run the compiler on a host machine and transfer the image to the target.

The term [cross compiler](https://en.wikipedia.org/wiki/Cross_compiler) refers to a compiler that is specifically enabled and configured to generate an image for an architecture other than that on which it is executing, but general trend in compiler design is to have this be a final-stage process, and make the transformation of source code decoupled from the target machine.

----

For a specific example, the Rust project declares `rustc` to be a cross-compiler by default, and just ships it with the ability to target its own platform by default. In their [target list](https://forge.rust-lang.org/platform-support.html), they have checkmarks for the standard library, `rustc`, and `cargo`. If `rustc` has a checkmark, then there exists an executable that can be run on that target, and that executable can emit a working artifact that runs on *any* of the targets where `std` has a checkmark.

----

Targeting a specific operating system and machine is ""just"" transforming abstract or independent representations of code using a table of known abstraction-to-real maps. These are built by basically saying ""on x86, the abstract instruction `iadd` maps to `ADD`, on AVR it maps to `ADD+ADC`, etc""

There's no requirement that a compiler only be able to generate an image for the machine on which it's running, and there's absolutely a requirement that *some* compiler be able to target a machine on which it *isn't*, because that's how new machines get started. We don't spin up a new architecture from first principles anymore, and certainly nobody writes an assembler in machine code and a compiler in assembler just to make a new CPU sellable.",1517161236.0
MeoMao555,"> How do compilers determine which processing arch they are running on? 

If no platform was specified, I assume the sensible default be the architecture the compiler has been compiled for. If the compiler itself has been compiled for x86, produce x86 code, and be conservative (i.e. don't use advanced instructions unless specifically enabled). 

Most compilers have options to change the target, so yes, you produce ARM binaries on an x86 machine. It's impossible to run a modern compiler on some of the supported targets (e.g. micro-controllers with limited memory), so you *have to* do this sometime.

> Are compilers ‘pre-programmed’ with a limited set of arch’s for which they can produce assembly for? 

There's a list of supported targets for each compilers. It's not always limited (GCC can probably compile to machine code that runs on your toaster), but of course, code generation has to be specifically written for each target. Clang has an intermediate representation (LLVM) and a flexible backend which makes supporting new targets easier. ",1517160150.0
dnabre,"The simple answer is they don't. They don't care. 

The architecture that a compiler runs is the one it's compiled for to run on. It's just another program and while the process of turning source code into a specific platform's executable is very complicated, the compiler's interactions with the system are very simple. It reads a source code file and outputs an executable file.

The compiler when build from its own source code (if you are pondering the chicken/egg-ness of this, good, it's an interesting question itself), it is often build to work for a single platform (architecture+OS combination) or it is hard coded to default to a single platform and use command-line options to tell it which platform to produce. In practice, this is most often the same platform the compiler is built to run on, but there a lots of uses for a compiler built to generate code for a different platform (called cross-compilers). ",1517161098.0
Madsy9,"A compiler which runs on one system and builds binaries that run on another architecture is called a *cross compiler*. A compiler which produces binaries for the same system the compiler runs on is just a special case of a general cross compiler.

gcc works like this: The actual gcc/g++ program you run acts as a driver for the tools underneath, which are the individual programs for the preprocessor, compiler and linker. The architecture and underlying platform (if any) which a gcc toolchain supports was configured when the gcc toolchain was built. The overall architecture and system a gcc toolchain supports can be found from the toolchain's name prefix. Examples:

* i686-w64-mingw32-gcc
* x86_64-linux-gnu-gcc
* arm-none-eabi-gcc

..where /usr/bin/gcc is a symlink to the gcc driver program /usr/bin/x86_64-linux-gnu-gcc. So what can be gathered from the gcc toolchain prefix? It is called a *target triplet* in the form *cpu-vendor-os*, in order to support GNU Autoconf.

Usually a gcc cross toolchain supports multiple CPU families within an architecture. For example, arm gcc supports all ARM families back to ARM7, or the ARMv4 instruction set. It also usually targets a specific platform, for example Windows or Linux. A ""bare bones"" compiler which does not assume an underlying platform for the final binary image is called a freehosting compiler.

Internally, the gcc compiler is dependent on the BFD library and the GNU binutils. The binutils are low-level tools such as the linker 'ld' and assembler 'as', and other tools such as strip, strings, nm, ar, objcopy and objdump. The BFD library is responsible for converting the gcc AST and psuedoops into valid instructions for the target architecture. So basically, BFD is a huge lookup table with some conversion functions. Adding support for new instruction sets to gcc is then relatively easy. But BFD is used twice, both by the gcc compiler, but also by the GNU binutils (the assembler and linker).

It is possible to build a single gcc toolchain which cross compiles for multiple architectures, but it's still kind of hairy. Building a gcc cross-compiler for a single target architecture is easy however.

Most compiler toolchains uses the same general idea as gcc. Generalize everything which can be generalized. Use generic operations in the AST and push all domain-specific stuff into a separate library, so the two can be maintained separately.",1517177401.0
mrmodojosr,"I don't see this in any of the other answers this far so probably worth mentioning JIT compilers here.

Just-in-time (JIT) Compilers take an Intermediate Language (IL) and generate machine code.  If we take Java as an example, a Java compiler translates Java code into Java byte code, you then have a JIT that runs on the target and translates byte code to machine code as it is needed.",1517202463.0
Wynro,"I think he was refering to the different representations: https://en.wikipedia.org/wiki/Gibibyte vs https://en.wikipedia.org/wiki/Gigabyte

The stuff about having space in reserve makes no sense in HDD. Maybe he was refering to SSD over-provisioning?",1517126354.0
IMightExist,"As it is an OS lecture, it's more likely he that he was referring to how some File Systems reserve part of a partition for their own purpose, and not that you get less HDD than advt. I think the latter is just marketing tricks, using different units to con buyers.",1517152942.0
crabbone,"There are many parties involved in measuring the size of SSD, and chipping into it for their purposes.  Advertisers, obviously, try to give you some more attractive figures, so they don't tell you whether it ""includes tax"" or not.

1. First comes the controller of SSD.  It needs redundancy for dead sectors and other kinds of garbage.  Depending on device, it may take either fixed size, or some % of your disk, usually not more than 5%.
2. Second, comes partitioning.  It needs to store its technical information somewhere (s.a. partition tables).
3. Third comes filesystem, it needs to store its technical information somewhere (bitmaps etc.)
4. Now come sales and marketing people and fish for the best number they could put on the box to sell it to you...",1517156750.0
fucg,Just talking about this the other day. Bits vs bytes is like metric vs imperial system if that helps. ,1517126512.0
bhrgunatha,Unless there's something more complex involved a simple [breadth first](https://en.wikipedia.org/wiki/Breadth_first_search) or [depth first](https://en.wikipedia.org/wiki/Depth-first_search) search will tell you if 2 nodes are connected. ,1517115802.0
Leheria,"That doesn't sound like it should be embarrassing at all.

""Hi, I'm Joe. I just switched to Computer Science and I'm interested in learning more about your company. What can I do to be prepared for an internship or hire opportunity at XYZ Corp down the road? Are there any particular courses or stacks you'd recommend learning?""

Don't necessarily take the recruiters advice, but it's a good way to get a sense of what employers are looking for, and it's a good way to build relationships.

Some companies do offer internships for students with minimal to no background. Some technologies, like web development, are not covered at all in some programs, so you're actually not far behind some more senior applicants.",1517112845.0
curiousGambler,"Can you explain how ""career fairs"" work at your school? And what country you're in?

I ask because this sounds much more stressful than my experience when I was in school. All career fairs I attended were as useful as you made them- if you aren't ready to talk to employers, just wander around and observe for a bit and engage more text year.",1517111969.0
stickybobcat,"Don't buy the it's so hard to get a internship stuff. Three tricks will help your odds astronomically.

1. Good resume(no accent key whoops) doesn't mean a full resume. I do a small section about past work, link to a github, contact info, and usually 1 long term goal.


2. Git gud. Git is awesome and having some projects that demo everything you know, libraries, databases, good design, etc, are an easy way to show you care. Also use these to grow new skills to really set yourself apart.


3. Skip Microsoft and Google. Every year I go it's a 3hr wait for 30 seconds to talk to someone from these companies, while almost every other stall is open. Use this to your advantage spend 3hrs talking to everyone else.


Note maybe Google is the goal, still skip them. Get in with a company, usually someone there knows a guy who can put in a good word for you at a more prestigious company, and work your way in like that. Unless, you are the top of your class or something of the like at which point jobs will come to you.",1517119215.0
DaBritishyankee,"There's not much you can do in two weeks to improve your resume.  One thing worth doing is maybe starting a personal project, so you have something to talk about at interviews.  Ultimately, though, most companies don't expect much from students in their first year or so.  The most significant thing you can show them is enthusiasm about whatever field they're working in.",1517113222.0
MathPolice,"Have you heard of Verilog?

",1517083826.0
,"First, /r/compsci might not be the best place for this, I'd suggest you try out /r/programminglanguages instead. Also, ""how to implement own programming language"" seems to be a promising google query for me. [This article on medium turned up and seems good](https://medium.freecodecamp.org/the-programming-language-pipeline-91d3f449c919). [Here is a tutorial how to do it in LISP](http://lisperator.net/pltut/)

--------------------------------------------------

Actual, really short answer answer: I describe a rough way to do it: 1) You will need to build a parser (or parser and lexer). This is a program that takes your sourcecode and turns into a datastructure called  AST (abstract syntax tree). 2) Now you can do work on your AST. You can interprete it (translate it to an value, i.e. a state where each gate has its output after the program execution) or compile it to another language.

Some tipps: 

* I would look into LISP for building this compiler - working on ASTs in LISP is quite comfortable compared to mainstream langs

* Rather than building on big compiler interpreter/compiler, split up the tasks into smaller interpreters and compilers. For example ""There should be as many lines in a gate's code as there are outputs for said gate. "" - I would build an interpreter that returns a boolean if the AST fullfills the condition.

* patience - making your own PL the first time is quite an undertaking and can lead you into deep theoretical waters... Don't worry and ask and enjoy!

Good luck!",1517084179.0
frenris,"Look up verilog, VHDL, chisel.

These are RTL (register transfer language) or HDL  (hardware description language) type languages. They are used for logic design. Languages like this are how you design microchips, or how you write a design you'll load into an FPGA.

Code in these languages is what engineers who work at Intel, AMD, Nvidia, Qualcomm, Broadcomm, ARM typically write. ",1517091582.0
yawkat,VHDL?,1517087715.0
chaotic_biscuit,"I'd recommend checking out SystemVerilog, I used it in school for a bit and I think it's pretty similar to what you're aiming to make. ",1517083390.0
Funktektronic,I would recommend starting out by reading a book on beginning compiler theory.  You'll need some grounding in the concepts and language of parsing before any of it will make much sense.,1517087986.0
Singularity42,"IntelliJ has a tool for making a language with ide support (cant remember the name off the top of my head).

It has some issues with complex languages. But if you just want a simple DSL that is only used by yourself or your company it is a simple way to get something made quickly.

If you actually want to make a language to share with the world, then I probably wouldnt use it though.",1517102859.0
TKAAZ,Why was this deleted?,1517139663.0
calligraphic-io,"Great article, thanks. Any thoughts of a follow up with PostgreSQL's three non-B-tree index algorithms?",1517063024.0
,[removed],1517030212.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1516993555.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1516990375.0
magnificentbop,"Read this book:  Kurose & Ross, Computer Networking: a top down approach, 7th edition.  Not being sarcastic, the book was thorough and clear cover to cover.  The included lab exercises also cover most of what you need to use Wireshark effectively",1516985830.0
mz1337,"As a professional security researcher and Ph.D. student, I can recommend 'The Hacker Playbook' (http://thehackerplaybook.com/)

and/or checking out https://www.offensive-security.com/information-security-training/

for security newbies.",1516993434.0
OkeepTooth,Computer Networks 5e by Tanenbaum ,1516997140.0
djingrain,"Does anyone have any recommendations for books or online courses (think khanacademy) for the purpose of learning Discrete Mathematics. I have to take it in September (or possibly over the summer) and everyone I ask says its hell, so I'd like to get a jump start on at least some of the material.",1517271933.0
,"spend this time dealing with anything external imo, you don't need to do any work prior to starting! just make sure you're comfortable in your new place, explore the city (assuming you've moved), and then continue studying if you really wanna. ",1516919678.0
galaxaxy,Learning how to use a debugger to fix your code will save you countless hours of printing out stuff to the console. It's an inevitable skill you'll need to further a career in software engineering (if that's your goal). Using the debugger will show you how your code **actually** runs versus how you **expect** it to run. Errors are almost always the programmer's fault - keep that in mind!,1516923827.0
ryanstephendavis,"If you're not familiar with using Linux OS's, I'd suggest loading a laptop up with one and start playing with the command line.  I did this six years ago when starting grad school and although it was painful at first, it was the best idea ever now ",1516937578.0
ProgrammaticallyRule,"I don’t know those projects but usually, open source projects have a FAQ section on their website/GitHub to tell you how to get started. Try to have a look at the ongoing “issues” and pick a small one to fix. Don’t hesitate to comment to ask questions and do a Pull request relatively quickly to gain feedback ",1516929561.0
Kazou1388,"Being a student at a midwest college studying CompSci it isnt too bad. I've always gotten a's in math so that definitely helps. Most classes consist of a lecture and lab section. Most CompSci classes early on are heavily about coding. After you get past the weeder classes surrounding programming various strutures and algorithms you'll most likely start moving on to more theoretical classes with less coding. Experiences probably differ from other universities, but this is what I've experienced thus far.",1516916031.0
,"In school you start by learning foundational courses like discrete mathematics, writing, programming, general electives, often calculus and linear algebra also. Then in later semesters you learn more languages, computer architecture, operating systems, more advanced math, networks and network security, web development, then sometimes more advanced topics like AI or machine learning or computer graphics. 

During your degree it's generally a good idea to try and find an internship on a semester off school, perhaps during summer time. This gives you some valuable work experience and gives you a feel for what it's really like to work in the field. 

The jobs are extremely vast, and there are many roles for CS grads. Personally I work in game development which is a lot of fun to me but long hours are the norm. The pay is generally good for CS grads, and there is plenty of demand in the tech industry. Some examples of jobs: game developer, database admin, Web developer, machine learning engineer, software engineer, IT consultant,UX analyst, information systems manager, and many more 

If you think you might be interested in a CS degree, coding should be something you are interested in, as you will spend a lot of your time doing it! CS is challenging, and getting into a CS program means you need to ensure your high school marks are up to par. So if you think it's something you would like to do, planning ahead and ensuring you keep your grades up will be important ",1516923894.0
zagbag,"Crypto miners, gamers, Big Fourers and internationals.",1516926405.0
DC-3,"Computer Science is quite a weak A-Level right now, and universities (especially top-tier ones) far prefer subjects like Maths, Further Maths, Physics, and Chemistry. In fact, a considerable proportion of CS applicants to top universities in the UK study those four subjects. While the A-Level isn't particularly well regarded, the subject itself is fascinating and you can study it independently thorough the many good resources on the internet. Wikipedia is a good place to start.

Source: current Yr13 student, intend to be starting a CS undergraduate at Cambridge in October.",1516927791.0
TomvdZ,Why don't you use a sorting algorithm? It sounds like you want to sort the areas by the sum of their user distances.,1516911193.0
bonafidebob,">The number of users are unlikely to be greater than ten, the number of areas rarely greater than 200. Brute forcing this as O(n2)...

You're right, 2000 additions and then sorting a list of 10 items will be unnoticeably fast, you should not waste any more time trying to find a better algorithm when you can get the best answer quickly and with very simple code.",1516920718.0
squareA,"I don't think Dijkstra's algorithm would improve efficiency, because you would need to run it  n  times (where n is the number of (area) nodes).   ",1516911343.0
Mrfudog,Not sure but maybe with a virtual machine,1516906865.0
minno,"You can reduce core count and RAM by running in a virtual machine. You can reduce GPU clock speed using a utility like MSI Afterburner, and CPU clock speed using Windows's power settings. I don't think there's a way to reduce GPU cores or VRAM without flashing a new BIOS onto it, which is time-consuming and kind of dangerous.",1516909711.0
habys,cgroups,1516913205.0
heyzo,"On the GPU side if you're using CUDA/OpenCL, in your code you could lower the number of blocks/workgroups so fewer cores are utilized. There are also tools to underclock your GPU cores and memory.",1516912051.0
masterwit,pubg,1516953783.0
Liz_Me,"I'm not sure what tool you could use to underclock your GPU, but in Windows 10 power settings you can set the cpu to only use like 5% of the available computing power. This underclocks the CPU. Here you can also set the cooling to ""passive"" where the cpu will downclock before the fans speed up (IIRC).

I use OpenHardwareMonitor to check the clocks and loads on various components.",1516915599.0
,[deleted],1516915183.0
anamorphism,"most of the tools i'm aware of just generate load on various components of your system. something like this: https://www.jam-software.com/heavyload/ (disclaimer: haven't used it)

i don't really know of something that will attempt to make your machine mimic a machine with different specs. you could arbitrarily define constraints in a vm as you state, but that still wouldn't really give you results that are that useful (especially when you're talking about gpus).

nothing really beats compat testing. probably best to just build a box with the minimum specs you're looking to support and use that. we generally have around 6 hardware configurations (a couple of low spec machines that use embedded intel video and a couple specs with nvidia and amd gpus) that we test on nightly and tens if not hundreds of hardware configurations that we run through when we're getting ready to ship a product.",1516908058.0
zushiba,"In most modern day systems you can turn off cores / underclock the CPU in the Bios. This is the best way to run your code on bare metal without the overhead of a virtualmachine while still testing on slower hardware. 

As for the GPU, most modern day GPU manufacturers have some kind of utility designed for overclocking, but they can be used to underclock as well. 


So to summarise, Underclock/turn off cores in the bios and use your GPU manufacturers included tools to underclock your GPU.",1516945422.0
Ramuh,"As for cpu cores, you can change how many are available to a program in Windows task Manager",1516945897.0
MrSnowflake,I think you could use speeds for this. It allows you to set cpu speeds. Not sure for gpu,1516920538.0
,It's called Furmark ,1516949397.0
emilvikstrom,"The CPU frequency can be set with `cpufreq` on Linux. You might have to install it, but at least for Debian it is available in the repositories.",1516953265.0
namikofficial,"Vm vm
Income guys",1516955904.0
lrem,"I suggest Docker. Not only you can set up the available resources, but it also allows you to ship the whole system where you tested.",1516957616.0
Coloneljesus,Not sure if it fits your workflow but slow machines can be had for pretty cheap on thw second hand market.,1516984287.0
igor_sk,"here’s some suggestions for Windows:
https://blogs.msdn.microsoft.com/vijaysk/2012/10/26/tools-to-simulate-cpu-memory-disk-load/",1516984943.0
billiron,[cputhrottle](http://www.willnolan.com/cputhrottle/cputhrottle.html) is an option on MacOS.,1516987823.0
txmasterg,"You can simulate low memory situations with application verifier (should be in the windows sdk). It's a great tool to use in general to verify you aren't doing something wrong that just happens to work. Some of the options are more focused toward native code and will be less helpful in .net languages or similar.

I think task manager or resource viewer has the ability to lock a process to only select cores.",1517281403.0
elcric_krej,"Keep in mind that downclocking the thing will not simulate a weaker machine entirely,  since your cahce size remains bigger and you r ram fq higher. 

VMs (and possibly cgruops,  if you can test on a window vm,  ar the way to go here). 

Downclocking 6cpus or gpus at 20% the original frequency mau not even put a dent in performance. 

Also, compiler with the instruction set available to the kind of machine you are testing for, yours will likely be more advanced possibly resulting in better performance in places. ",1516952579.0
Olao99,Open try opening one or two windows of minesweeper. ,1516914396.0
American_Libertarian,It's basically impossible for the os to 'reach down' to the bios layer and manipulate hardware directly. You need to go into the bios to do this.,1516906714.0
_Bia,A book on discrete mathematics.,1516902733.0
PhiteMe,"To help with that specific example: remember that a => b does not imply anything about causation (it doesn’t mean that a ‘causes’ b to happen), it simply means that if a is true, b must also be true.

In other words, you’ll never find a situation where a is true and b is not true. So, a => b (a implies b, or if a then b) must be the negation of (a and (not b)), which is ((not a) or b). If this step seems a bit strange, read up on [De Morgan’s Laws](https://simple.m.wikipedia.org/wiki/De_Morgan%27s_laws), which serve as a good foundation to propositional logic. In the notation in the link I gave, a bar above a statement corresponds to the negation of that statement, a dot means or, and a plus means and. Also, like people before me recommended: when in doubt, write a truth table.

An interesting consequence of (a => b) being an equivalent statement of (!a or b) is that if a is always a false statement, then a necessarily implies b. This is probably the most unintuitive thing about this property, but again, don’t think of “a implies b” as a statement of causation like “a causes b”, but rather as “it is always the case that if a is true, then b must be true” or “it is never the case that a is true and b is false.” ",1516904653.0
inephable,"a then b is equivalent to if a, then b

which has the following truth table:

a | b | a then b
1 | 1 | true
1 | 0 | false
0 | 1 | true
0 | 0 | true

these last two cases are “vacuously” true

“a then b” is true when (not a) or b.

see?

the above is right, it’s just discrete mathematics",1516904621.0
haowanr,"I'm sorry I don't have a specific book recommandation but I'd suggest having a look at the [wikipedia article on Boolean Algebra](https://en.wikipedia.org/wiki/Boolean_algebra).
For the specific example write down the truth table of ""a then b"" and then of ""(not a) or (b)"" and compare them ;) ",1516902918.0
parellano1997,Epp’s Discrete Mathematics with Applications is quite good and can get you introduced to these concepts. Specially the second chapter. ,1517069648.0
HomeTahnHero,"If you want to delve even deeper, check out Klenk’s Understanding Symbolic Logic",1516929057.0
kekspernikai,"With conditional if-then statements, my discrete math textbook used an example I liked. It's like a promise, or an election campaign. 

It's only false if a (before) is true and b (after) is false.

Didn't promise, didn't deliver? Truth
Did promise, did deliver? Truth
Didn't promise, did deliver? Truth
Did promise, didn't deliver? Lie

Edit: important to remember that as an analogy this takes some liberties with definitions. Just using it to convey when the A->B case is T|F",1516985913.0
Jugs_A_Poppin,"If it makes you feel any better, I'm a sophomore in college, but I'm supposed to be a senior. I took nearly two years off and decided to major in CS. I have minimal coding and hardware knowledge. Also, I'm 22. I'm going for it. I've observed the same things you have. You aren't alone. Lol",1516903199.0
,"I'm in my 30s and just getting my BS in CS. There is a learning curve, there is ageism (some) in tech. But you know what? It's a vibrant field, and if you put in the time to really know your topic, you will always be able to find something. Moreover, you're still very young and in industry, no one's going to blink an eye if you're 20 or 23 at your first hard job.

If you're really interested in Financial Tech, you are going to want all the basics that you'd get from a normal CS degree, and you're also going to want (in the best case) a fair amount of math: numerical analysis, stats, bayesian probability, stochastic processes, linear algebra. Don't be daunted by that, it's just a little more. Stats for sure is critical for any data analytics.

As for where to take your classes: don't skimp on your CS classes - take the best versions you can. Think about it: when you're trying to do some stat modelling, which experience would you rather be drawing on? An excellent English class you had where you talked about Othello? Or an exciting data structures course that really challenged you to learn what it was all about? That said, competency in CS is all about exposure: you got some ideas about python books. Those are good. Some online courses like coursera are good if you're dedicated enough to stick with them. Any exposure like that helps. But, you're going to want the real classes, too. You're developing the tools for your trade that you want to use for the rest of your life. Don't skimp on the time and care to really learn them.

Edit: If you're going to start with python, look into Automate the Boring Stuff with Python. Also, Harvey Mudd has a school-wide course they teach to everyone, which is basically first-semester python. It has everything an intro CS class has, but also tries to be accessible to people who aren't CS focused. And the book is free online! https://www.cs.hmc.edu/csforall/ . There's even an edX course that will give you assignments and structure, but with all online courses...you have to be motivated to stick with it. ",1516905829.0
davvolun,"I see you deleted the text, but you're absolutely not lacking. Don't give into the 'keeping up with the joneses' feeling. Someone out there will always be able to make you feel like you don't know anything. Usually, they're wrong; sometimes, they aren't. It's pretty unlikely any of us are actually the best at anything. But just be the best you can be.

As for the rest, there's such high demand for programmers everywhere, I wouldn't worry about it. Things go up and down, try to keep an eye on trends, but don't be a slave to them.",1516949355.0
obp5599,"Im a comp sci major at uf(senior) and i never took my first coding class until i was in my third year of college. I went to miami dade college for my AA then transferred to uf for electrical engineering. Fell in love with comp sci so i switched.

Honestly a lot depends on either how much you plan on working or natural talent. I was pretty good at it and had a decent mind for it. A lot of my friends don’t and they just work hard and get it done.

You have to want it and you have to like it. Youll never make it through ufs program(garbage profs mostly) without enjoying doing it. 

Comp sci isnt about coding, that is probably the easiest part. You can learn most languages pretty quick, they’re all pretty similar. It gets to a point where you’re just learning the differences between them rather than a while new language. At uf we take 2(i guess 3 but its a stretch) coding classes and THATS IT! Shows how important it is. Its about learning how computers work, and how to maximize efficiency. 

So i wouldnt be worried about “being behind” just be dedicated.

Also never consider a “coding” school! Again comp sci is not about coding! Very big misconception! These coding schools/bootcamps are straight up scams

All in all, if i can switch majors at 21 in my 3rd year of college and only extend my grad date by a year, you’re definitely not late. Just be sure to take all the math and sciences you need! This is extremely important!! As most college/community colleges will not tell you to do this!!!",1516903446.0
skeeto,"Don't waste your time and money with coding boot camps. They're
deceptive, they skip all the fundamentals, and they only teach you a
tiny bit of non-transferable web development knowledge. They're mostly scams.

Instead, grab an introductory programming book and just start learning
*and* practicing. Don't put this off. It's not too late for this. Try
out every concept as you learn it. Build little throwaway projects based
around those concepts. Consider how they interact with each other beyond
what the book teaches, and test it out in your own projects. If you need
project ideas, try solving some of the ""easy"" problems on
r/dailyprogrammer. Be prepared to practice on a daily basis.

What book? Actually, I don't know. Since you'd be learning on your own,
probably a Python book. I mentor high school students and I start them
with C (via [*The C Programming
Language*](https://en.wikipedia.org/wiki/The_C_Programming_Language) aka
*K&R*), but that works because I'm there to get them past some of the
early difficulties.
",1516904665.0
sinrin,"First semester at university, there was a dude in my Intro to Programming class who was older than 50. He asked the most inept questions about the most basic concepts. That was 3 years ago, and now he's getting his Master's Degree.

There's no such thing as ""behind"". There's just you and your skills.",1516905447.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1516899709.0
DrKevinBuffardi,"> I am an MBA Marketing student, who is self-teaching himself C++. I am more passionate about programming, than management, and would like to work as a programmer rather than a manager after my MBA.

Why not get a degree in Software Engineering or Computer Science instead, then?

No matter your dissertation topic, it will never qualify you as well as actually getting a relevant degree.",1516870858.0
magnificentbop,"How about this: profiling the effects of trackers and other advertisement-related code on the infrastructure of the Internet at large.  It would be worthwhile for businesses to understand when the extra costs of network infrastructure and peering exceed the advertisement revenue.  Likewise, if you can show that excess advertising is actually hindering businesses and content providers, you have an opportunity to suggest an optimal mix or an alternative solution.",1516888974.0
jh125486,"I’m not sure how MBA topics work, but for CS usually it’s “background ->novel idea->related work->experimental results->conclusion”

So maybe “Something something blockchain”  would work for an MBA?
Such as “Exploring AWS Machine Learning for crypto currency trading arbitrage”?",1516889155.0
dvogel,"Take a look at a company like Lead Pages. AFAIK they make a tool that let's marketing staff generate lead-generating websites without any technical skills. I'm sure you can find a project in that area.

However, C++ might not be the best language for exploring these web-centric options. A few ideas in the C++ sweet spots:

1. Consistency of branding in the age of tech. You could make a program that accepts a couple of images and selects the best candidate image and transformations to keep a consistent brand appearance across a range of devices from desktops through tablets and feature phones.
2. Finding customers can be aided by statistical analysis of past sales. You could create a system that takes in sales information with geographic metadata and then calculate which areas of the country to focus your marketing campaigns on.",1516896492.0
LelouchVB,"Since you're in marketing, one thing i can see where the two major converge is Big Data. ",1516896943.0
hamtaroismyhomie,"Below average pay, below average work-life balance, and higher job instability (large companies will layoff programmers between projects, and smaller companies go belly up pretty often).

But, you get to work on video games!",1516861456.0
alwaysonesmaller,"It's best to just study CS in general and perhaps minor in literature or game design. That way when you get a bit older and experience shows you the difference between your passions and your career (which may be huge or might be small) you're in the best position to do what you want when you're a full-fledged adult.

The idea being that your CS degree will be what you need no matter which path you choose. Making video games, for many, isn't the utopia that it seems in youth.",1516884346.0
artsybitbegins,"If you love programming, and video games, I'd recommend working at a place where you can have the stability of a programming job.. then doing video game projects on the side.",1516870450.0
PostSeptimus,"I have heard that a lot of companies will hire you for a single project contract and that's it. Once the project is over, you're looking for a new job.",1516862249.0
espergrafs,"Simply being able to program will put you above most of your incoming classmates. If you want your first year to be really easy, I would suggest focusing on developing an in-depth understanding of java (the language most   of your intro classes will probably use), basic data structures (linked list, binary tree, HashTable, etc.), and how to use those data structures in Java.",1516851520.0
lezorte,"The funny thing is we go to school to get ""computer science"" degrees, and then we usually get jobs in ""software engineering"". While related, they are different enough that you first should figure out which (one or both) you are interested in. You can't avoid the computer science while in college but if you are interested in getting a head start before college like you say, you can really set yourself up for a successful career in the area you truely enjoy.

Computer science is the basis for innovation. You will need to love math and logic if you want to be a true ""computer scientist"". Computer science is about inventing algorithms, data types, compilers, etc. They're the people who create new languages, invent better blockchains, innovate better quantum computers, design more efficient game engines, come up with new AIs, or even become college professors, and the list goes on. If this interests you, study math now. Linear Algebra, Calculus, Probability, Statistics, Number Theory, and Numerical Analysis are the big ones, but I'd also recommend checking out Predicate Calculus and Lambda Calculus. Also, believe it or not, but there are a lot of areas of computer science where Physics is very good to know.

Software Engineering is what most people go into as a career (even though they get degrees in computer science). They are the ones that  businesses can't survive without these days. They write the websites and manage the data just to name a few responsibilities. Being a successful and respected software engineer means being really good at programming. Personally, I think part of what makes a good programmer is having an intimate knowledge of programming paradigms and their design patterns. Study object oriented vs functional oriented vs asynchronous oriented, etc. Also, get good at a couple different languages using different paradigms. You said you already know Java (object) and JavaScript (functional, asynchronous) which is a great start for you. Learn Java's Spring and Hibernate, learn some SQL, and keep practicing your web skills and you will have absolutely no problem finding a job as a full stack developer out of college. Play around with a few other languages, maybe even contribute to some open source, and you'll become a programming pro in no time.

I'm making these sound like two different boxes, but remember that you can be whatever combination of the two that you want to be for a career. My current job is very much a software engineer job, but I'm studying math textbooks in my free time because I'm interested in becoming the expert in machine learning and cryptography for my team. Find what interests you.",1516858563.0
lobomos,"If you have a school picked out, see if you can acquire a study plan/syllabus and just get to work on the intro level course work. I would probably recommend something like the book Clean Code and get the book Introduction to Algorithms. You should be able to code most of these concepts using Java without issue if you are somewhat familiar with it so maybe dabble in Python as well just to see a different language. As far as podcasts, I find the Microsoft Research podcast to be kind of interesting.",1516858077.0
maweki,"I would suggest just getting familiar with python, especially the following stuff: sets, set comprehension, generator expressions, tuples, itertools, maybe lambda.

This will allow you to implement and play with most of the theory stuff in the first semesters. Implementing what you learn on the theoretical side is really good to learn the stuff.

I think it's best to focus on complementing the theoretical part of your education instead of just bolstering up practical experience. Even though programming is related to computer science, often design decisions in programming language either have no basis in computer science (and are just a neat idea of the authors) or use advanced ideas that will be of no help during the first semesters.

A proper grasp of the theoretical background is key!",1516860399.0
CanYouSaySacrifice,"I was a math major in college, but I dabbled in CS. For the lower division CS courses, the typical computer nerds in my group were beasts in Data Structures and Algorithms, Compilers, Computer Architecture, etc. Where I saw most of them choke was in Discrete Math, Analysis of Algorithms and formal Logic. They could code their asses off but when it came time to prove things, they would grind to a halt. 


If you can code now and have a decent grasp of Java, I would suggest learning your data structures and algorithms (as everyone else suggested), but I would also strongly recommend working through a book called ""How to Prove It"" at some point before (or while) you take discrete math. Google the authors name and you can find a syllabus for his class and use that as a guideline. This will probably be a bit intimidating, but you can definitely do it. The book is great. If you need something a little bit easier (and free), google ""Book of Proof"". Its a gentler introduction to proofs.


This will give you a strong foundation and round you out a bit more.",1516864718.0
dakaiiser11,"You already having knowledge, even if it’s basic, of those languages puts you ahead of like 75% of all the other incoming people in CS. Like that guy said, totally get Data Structures like linked lists, stacks and queues down. Pointers and recursion really mess up people too. Binary trees and time complexity are huge too. ",1516869259.0
madame_robot,"Check out the BaseCS Podcast, which is explaining some common CS patterns in easy to understand language (I loved it!): https://www.codenewbie.org/basecs",1516871801.0
kvothe1956,"Just start coding and doing tutorials online the interest you! I would recommend python, and writing some fun scripts.",1516851349.0
Prcrstntr,"Like others have said, being able to program is already prepared. 

Something that would be very helpful though, and is something that I've just started recently and wished I started much earlier, is 
 everyday, program whatever algorithm you talked about in class. Start building up all those little programs, so that you get your daily practice, you learn it better, and you are prepared for the projects. 

You can start by making some of the basic data structures and algorithms, like the various sorting ones. Read pseudocode and figure out how to implement it. Figure out what programming language your school does for its 'Computer Science 101' ",1516861287.0
,"check the syllabus,  choose 1 - 2 subject that you feel are interesting and study those by the course book. Making some courses easy and accessible during the semester will give you extra time for the rest of the courses, giving you a strong start. I'll choose the math courses since I think they are usually the hardest for first-year's",1516882883.0
heyzo,"Get used to solving these types of problems:  

[https://projecteuler.net/archives](https://projecteuler.net/archives)



Learn to love the hard problems.",1516893659.0
FormallyIntroduced,"If you're just starting out, a good thing to do is check meetups and code camps in your surrounding area. ",1516894478.0
kvothe1956,"Just start coding and doing tutorials online the interest you! I would recommend python, and writing some fun script s.",1516851289.0
comicidiot,"Probably 30-45 for the HTML and another 15-30 for the CSS.

The three elements and the child elements at the bottom would all have the same class names so the CSS would apply them all. The image and bio could have the same class as well, to give padding info.

Don't worry, you'll get there. I can only do it so fast because my editor auto-completes as I type. So it's a matter of thinking what I want to do then typing in the first few letters of the styles I want to apply. Try your best to not compare yourself to others because that's how you'll lose motivation. 

As for the bordem. Yeah, it happens. Debugging and fixing issues breaks the monotony; Test on multiple browsers and try and fix any discrepancies. Also, this is pretty static CSS so far. You can do animation and even responsive designs with CSS, so things will start getting interesting and fun soon. Since it seems you're in a front end coding camp, you'll probably start JavaScript at some point which will help style websites as well.",1516853213.0
MondayMonkey1,"Start by completely ignoring the style.  Imagine the document as if you jotted it down in a notebook.  What structure would it have?  Now code that using HTML tags.  Now you have your structure.  Assign classes to group together similar blocks, and then it's pretty trivial to style it.  The only trick is how to centre and position: I'd use the fuck out flexbox or CSS grid if that's allowed.  

Edit: an hour, hour and a half tops.  Maybe longer if I fuck with the colour gradients to get them right.  ",1516854807.0
smthamazing,"20-30 minutes, but I work in the field of webdev, it's only this quick because I've done it hundreds of times.

Also, I don't think this is a right subreddit for this question. /r/webdev would be more suitable, but I don't see much interesting discussion here anyway, so I'm not sure if it would be welcome there.",1516878089.0
alnyland,"Personally, that page would take me .5-5hrs probs, but for a beginner (depending how quickly you pick up the core concepts of css) probably 10-30hrs for a working (not glitchy) version. Did they specify if it’s responsive or a certain width? This would make a big difference in how you design it. ",1521145111.0
quantumcolor,"For the record, the plagiarism software your professor talks about is entirely real and not just for scares. It is called “Moss,” hosted by Stanford theory department (for any academic institution to use). Source: am a CS TA and have used it before to devastating effects.

The policy on collaboration should be determined by your professors. Some professors will be absolutely fine with it and some professors will forbid it. Whether or not it is academic dishonesty depends on your professors individual policies. You should ask him or her if it is not explicitly specified in the course syllabus.",1516850943.0
Bluedude7,"In my undergrad I used to sit on the committee that reviewed violations of the Honor Code.  The CS department was known for its ability to discern plagiarism.  They never failed to prove a case.  Professional and collaborative programming may have style guides and best practices, but there is always something distinct about each programmer, especially students.  Even subtle changes in style can be glaring, but the department was also strict about collaboration.  You could whiteboard algorithms and discuss ways of solving the problem, but sharing or writing code together was not allowed. ",1516859140.0
ifeellazy,"I'm not saying that you shouldn't do this, lord knows I took plenty of help at University, but if the projects aren't supposed to be group projects then you are cheating, just to a different degree than people who are copying straight off the web.",1516858955.0
MrAppleseed,"I graded back in Uni in 2015 for a low-level core-CS class. You would be surprised what people try to pass off as their own. There were frequently people who would submit their assignments (which were supposed to be worked on individually) that were identical to someone else's in class - down to the comments. Those were pretty much the only one's I flagged. If you had an identical method but the rest was different enough, I generally let it slide. ",1516853756.0
thewataru,"It happened to me. Back in Ms we have to write a simple Http 1.0 servers. Almost everyone got a similar function, which got a http code and with a switch case instruction returned some string. The course assistant accused everyone of copy-pasting. At that time we were able to convince him, that he was wrong.",1516877160.0
IMightExist,"Once a lecturer(what is the word for the person who takes practicals, but is not a professor, but is a lecturer?) was calling each student in his lab and telling them which programs of their's matched with whom. Then he'd ask for an explanation.

He called me in and said my very first assignment matched with that of my friend. Now, this was a very simple program( something like inheritance or something), so I told him that if I were to be a copy-cat, I'd have copied the hardest(the last ones), and maybe done easier on my own, not the other way around. That explanation was good enough for him.

just in case someone is wondering, I hadn't copied it, it was a coincidence.",1517152749.0
ryanstephendavis,"As a former comp sci Masters TA, as long as you're forthcoming/transparent about what code you share with others and aren't blatantly copy/pasting from other people or online searches and acting like you created the code, you'll be fine.  At some point, almost everyone is sharing and running a lot of the same code nowadays so it's good to learn how leverage open source code and work with others to create code together.  Simply give credit to others where it's due in comments.  

",1516856665.0
,[deleted],1516849749.0
,[deleted],1516839825.0
MathPolice,"You could make roughly identical microarchitectures for each of them.

In that sense the achievable parallelism is the same.

However the x86-64 will always have the *additional* overhead of cracking the x86 instructions into more RISC-like sub-instructions plus a small ""side machine"" to handle the really rare or obscure CISC-ish instructions which aren't optimized.

In that sense the x86-64 will always have the worst power for a given amount of performance. Similarly the ARM will still have some legacy v7 craftiness that will require some additional power to deal with (strange condition bits, load-multiples, weird things like loads causing branches, etc. etc.)

Note that ARMv8 is almost a rip-off of MIPS64. So if you don't need v7 compatibility that power overhead goes away since a lot of the terrible legacy ideas go away.

ON THE OTHER HAND, the more dense encodings of x86 and ARM Thumb may translate to some power saving in the instruction cache. But in general thus would probably not be enough to offset the aforementioned issues.

So, all else being equal (manufacturing process, micro architectural partitioning, etc.) your ""Bang per watt"" order will look like this:

1. MIPS64

2. ARMv8 only

3. ARM

4. X86-64

",1516841474.0
mrexodia,"Start with your initial idea, if it turns out to be easy, come up with interesting challenges :)",1516871534.0
Kibouo,"The **best** language depends on what problem you wish to solve.

Python and C are especially usefull for NetSec. ",1516828122.0
wlewis16,"If your plan is cyber security, you should absolutely learn C. ",1516828126.0
nerdshark,"There isn't a best language. Programming languages are tools, and they *aren't* one-size-fits-all. Any computer scientist or software engineer worthy of the title will learn whatever languages and other tools necessary to complete a task.",1516828082.0
dhjdhj,"While I agree that there's no best language, what's really worthwhile is learning languages that use different programming paradigms. For example, there's no real difference between Pascal and C, they're both imperative languages, you really just need to learn the keyword syntax and the libraries. Pascal gives you way more protection from beginner errors which is why it's a better teaching language.

Similarly, you should understand object-oriented programming (OOP) and you can do this with Smalltalk (the classic) and Objective-C (not recommended though). C++ is only kinda/sorta object-oriented (see Alan Kay's famous comment on this) but important to know nevertheless. Java is a bit better except I hate the fact that you have to include the code in your definitions unless you just make interfaces for everything, not practical for initial learning.

Declarative languages such as Prolog (anyone using that anymore?)  and SQL are yet another paradigm worth understanding.

Functional programming is becoming really invaluable, even if you just use functional techniques with other languages so Haskell or Erlang are worth considering there.

Dataflow and visual languages are yet another very interesting paradigm, whether you play with something like MaxMSP (which has some dataflow) or LapView, among many, so you can get a flavor.

In the scripting arena (which I'm never sure should actually be categorized separately, such languages seem to be just lazy ways (no variable declarations needed!) of writing code with lots of room for errors) there's the Python/Perl/Ruby universes, all complete with their own frameworks (libraries) but essentially different ways of just making the same mistakes (oops, sorry, different ways to to manage databases, munge text and data, respond to web page forms and so on). My sense is that Python seems to be the best choice these days. I'm convinced the developer of Perl was high on something when he came up with Perl and although I used to love Ruby (very Smalltalkish), it seems to have been overtaken by Python. But again, learn any of them and the skill is transferrable.

Javascript - well, just icky.

(Disclaimer - just my experiences from 35 years of software development and both teaching/designing programming languages. Some of my opinions may appear religious in nature and no doubt it will get downvoted by the fanboys of particular approaches)",1516832399.0
dgryski,"I implemented the hamming distance search from ""Detecting Near-Duplicates for Web Crawling"".

Code here: https://github.com/dgryski/go-simstore",1516832185.0
AIforce,This is a nice example of some sort of text / semantics mining. You might find better support in [r/artificial](http://www.reddit.com/r/artificial) the artificial intelligence reddit.,1516810212.0
jrdagger93,"Well it may be naive but the easiest way it looks like is contain a list of nouns and emojis. Use the noun to access the emoji information. In some cases, like soundtrack, it would have multiple emojis attached to it. 

Easiest off the top of my head would be like 

Map<noun type, vector<emoji index>>

In order for it to work you'd need the list of nouns and the list of emojis and just index in those. Obviously this is just kind of psuedo code but it's the best way I could think of off the top of my head. Hope this helps! ",1516815588.0
sheep_cmdr,"Statistics can be helpful if you want to get into research, ML, AI, etc",1516796359.0
foreheadteeth,"I did math+cs as an undergrad. I'm a math prof now but I used to work in industry and I've had lots of jobs in industry.

The math credentials on my CV never helped me get an industry job, they only ever cared about the CS credentials. What happened though is that I was able to perform better at the especially hard technical interviews, e.g. at Google, because I was a mathematician.

For example, at some point, an interviewer asked me how many things you can put into a hash table of size n before you start getting collisions. The interviewer's plan was to spend half an hour working it out but I just immediately gave the answer (sqrt(n)), and explained that it's the [birthday problem](https://en.wikipedia.org/wiki/Birthday_problem) and that you do it by using Stirling's approximation or whatever. Sometimes that sort of overwhelming performance at job interviews can prevent you from getting the job if the interviewer is intimidated.

That being said, doing math is the hardest way to get a job so I usually tell people to only do math if they like it for the sake of doing math. It is way, way easier to get a much higher paying job e.g. by taking [a public speaking course](https://www.cnbc.com/2017/02/03/4-public-speaking-lessons-that-changed-warren-buffetts-life.html) and doing no math at all.
",1516794544.0
xiipaoc,"Ethnomusicology.  It gets you to learn about musical cultures around the world, and there's very little overlap with CS so you know you'll be learning a lot.",1516819003.0
veltrop,"Depends on what you are interested in and where you want to go.  I did Japanese.

I wouldn't do one just for the sole sake of diversifying.  After you get into the field, it won't matter for much.",1516795286.0
albatrek,"I'm doing philosophy. The logic parts pair with CS really well. In general, philosophy teaches you how to argue and construct proofs, and those skills transfer right back to CS, even if the philosophy arguments were on morality or religion.",1516823566.0
abmiram,Animal husbandry. ,1516849251.0
DonaldPShimoda,"There is no universal ""best"" double-major with CS. It all depends on *you*. The purpose of college is to set yourself up for the rest of your life, so major in something that helps you get where you want to go. Don't know where that is? Then pick something you enjoy and see if it takes you places you like.

For a suggestion: linguistics. I minored in linguistics, emphasize on natural language processing (and programming languages) in my undergrad, am finishing my MS this year, and have applied to PhD programs in NLP for next year (hopefully). I'm not saying it's for everyone, but I really enjoyed it a lot. Plus it's different; I'd bet that well over 50% of CS double-majors chose math or statistics as their other major. Not that there's anything wrong with that, but I wanted to ""be different"" or something.",1516815263.0
TheUltimateFuckUp,"Definitely not business..business schools teach students how to be corporate drones. Even the ""entrepreneurship"" degrees teach you basic things you can learn by googling. You don't need a class on making pitch decks or kissing VC ass.

Go for electrical engineering or math (or maybe even physics) or perhaps just use the spare time to learn what will interest you down the road. Nothing will grow you intellectually like sitting down and actually writing code/algorithms/designing/reverse engineering stuff.",1516791189.0
ianwold,"Philosophy - it helps with logic, a lot of employers like seeing humanities along with the CS, and if it turns out software isn't your cup of tea then you've got taxi driving to fall back on!

Source: had a double major in CS and Phil",1516845398.0
formybrain,"probably a unpopular opinion, but i find cs + quantitative finance and can lead to a more lucrative but less academic career ",1516809446.0
IHeartFaye,"Philosophy; a lot of the mathematics and logic coursework that I took coincided with my CS upper-division. That, and it really teaches you how to critically-think. Sounds cheesy, but being able to parse and argument and make sense of it is a very valuable skill that you can apply in any pursuit",1517330453.0
random314,I don't know if you need to go to school these days for project /team management or if it's something you learn on the job... But the most successful developers are great programmers with management skills in either people or project.,1516798483.0
karma000,"You might consider a double minor, instead of another major. I’m taking technical writing and math courses on the side.

Technical writing: communication can be hard for people in tech fields. It’s a skill and we aren’t required to practice it. Non tech people get frustrated with tech people that can’t get their ideas across.

Math: my brain is just sharper when I’m doing math on the regular. As others have stated, stats is probably more practical than straight up math.",1516828202.0
generic12345689,Math or something engineering related. Comp sci in medical space is a new thing but not sure what major would help there. ,1516801279.0
PostSeptimus,"I don't have enough time to double major so I'm settling with an Astronomy minor. Dunno where that'll get me but I get to do stuff I love, so I'd say it's worth it. (Side note, I was just offered a research opportunity combining CS and Astronomy. So the combination is technically possible, despite my inability to come up with my own ways of combining the two.)

Most common double majors I've seen/heard of:
CS/Math
CS/Physics
CS/Business
CS/Linguistics
CS/Engineering
CS/Pre-Law

Honestly though, don't be afraid to go for something weird if you're interested in it. If you want to double major in CS and Funeral Services, go for it. If you can figure out a way to make those two work well in a unique way, you'll be far better off than someone who sticks to the generic combos.

A few years ago I went to NCECA (National Council for the Education of the Ceramic Arts) and I saw one artist who had built a 3D printer specifically so he could print off these gorgeous ceramic pieces. I'm not sure what his educational story is, but that would fascinate the h*ck out of me as an employer.

TL;DR Don't choose something you aren't interested in simply because it looks good on a resume. Find something you're passionate about and make it work.",1516802657.0
_foobie,"Anthropology! Well, it worked for me anyway.",1516808048.0
kcdragon,"Computer Science and Internship.

Math and CS would probably be the easiest since it would have the most overlap but I think getting an internship/co-op/job part time will teach you much more than a second major. A computer science degree is going to leave a lot out that an internship would teach you. You would eventually learn that at your first job but doing so now will give you a heads up before graduating. If you don't have time to work at an outside company, ask you department if they have any positions available (teaching assistant, research assistant, application support).

Source: I was a computer science major, math minor at Drexel who learned a lot during my internships and also was a teaching assistant and research assistant.",1516808190.0
Humble_Weaver,"I went with geomatics/GIS (through geography, so I also had a practical understanding of what I was actually working on)

Over at r/gis , among the most common pairings suggested for geography/GIS majors is Computer Science, and many people in the field would like to have a better understanding of programming.

GIS can be programming heavy, but most people I know right now have either learned by themselves and/or are paired with programmers/systems administrators. Of course, this depends on where you are in the world. Being a developer for GIS solutions all the while being a GIS specialist is pretty rare in my network, for example.

It's a field that is growing really fast, with all the location data being produced, web mapping/delivery/location oriented apps, etc. There's also satellite and lidar imagery that are becoming increasingly available and in demand. Throw in some machine learning/AI and big data and you'll be in a field with lots of room for research if you decide to do a masters after.

All in all, it really depends on what kind of work you want to do and what kind of company you want to work for. I'd ask myself where I want to be and what kind of life I want for myself, and then aim for the things/fields that'll get me there.

Best of luck!",1516816814.0
Prcrstntr,"If you want to do graduate school, math would help a lot. ",1516821978.0
Zerocool947,"This is going to sound insane and might be very difficult depending on your University. If a double major isn't possible, a major/minor is worth looking at. This will only work if you actually like the second topic.

Major/minor in English. 

I always found that having an English class helped me take a mental break from my engineering responsibilities and it sets you apart from your peers if you can leverage it.
",1516833715.0
onfire9123,"Math. Bar none. 

Nothing helps you understand how the concepts work better than a solid foundation of math. The core math classes they make every CS major take are not enough. 

If you're trying just complement your programming skill instead of understanding CS with a second major, then I'd say choose either EE or physics. But, again, knowing math is extremely important to those disciplines, as well.

It all goes back to math. Do yourself a favor and learn math.",1516837719.0
DarthSanity,"I minored in math, but only because the CS curriculum came out of the Math school. Back before CS was a recognized discipline in its own right, the degree programs were typically aligned with whatever school created the program: Math, Engineering or Business. At my school we had all three degree types and you could get a Computer Science, Computer Engineering or Information Technology degree.

Today things are so specialized that your better off minoring in the field your interested in. Want to develop terrain models for oil exploration? geoscience. Want to go into computer animation (not just be an animator - develop the tools that animators use)? Fine Arts. 

Many of the current fads - block chain, cryptography, AI - rely on a specific subset of a variety of disciplines so it might make sense to not minor in anything - just take the courses you’re interested in to achieve the baseline knowledge set you need to be successful in your chosen profession.",1516838268.0
hextree,"In my view, Maths is hands down the best pairing with Computer Science. It will make you the strongest computer scientist overall.

> Also is it worth it in the long run to go for a master or phd in comp sci?

If you're doing it for career then you're doing it for the wrong reasons. Postgrad generally doesn't help career, it isn't *designed* to. You do it because you enjoy doing research.",1517048788.0
frickenfriedchog,"Cognitive Science is getting pretty chummy with CS these days. What with all the Machine learning and Artificial Intelligence going around. 

As for the Masters/PhD value? That's something that you should decide on your own. Do you *want* to learn more about CS than what you get in an undergrad degree? If so, do it, and stay excited about it. If you want to jump into industry, do it, and get excited about it. You only get one lap around your life, what's more interesting to you? ",1517270590.0
MostlyHarmless___,"computational finance, it'll get you money quick on the side as you learn to just trade for yourself",1516816634.0
Iroastu,"It depends, the place I work does a lot with finance and money so either business, finance, stats, or actuarial science. However for places in general I think it's most beneficial to have a  degree such as math (most popular I've seen), stats, or something like that which can be used in any application. 

As far as going for an advanced degree, if you want to work in industry probably a PhD would be overkill (I work closely with recruiting). A master's wouldn't hurt, but you might want to look at some companies pay increases for a master's vs not, and career advancement it can open. I personally started going for my masters, and my company paid for it in the form of a reimbursement after the classes ended, but I stopped because it cost too much every month ad I just bought a house. So just make sure an advanced degree is worth the time and work you'll put in. ",1516801532.0
Oriumpor,I'd you want to stay on the leading edge in the field physics (especially quantum electrodynamics) and CS are essentially prerequisites if you want to go anywhere with quantum computing.,1516813581.0
BrightLord_Wyn,"You have to decide what you want to do if you are going to double major. Your second major will open extra doors for you, but you need to decide what doors you want opened.

",1516816827.0
recurrence,Math unless your program already focuses on math such as the BMath Computer Science at Waterloo.,1516818072.0
deong,"Statistics for AI or machine learning specializations.

Mathematics if you're into theory of Computation, programming languages, etc. Probably graphics as well. 

But really you should probably choose what interests you. Having CS knowledge can be valuable paired with almost anything. Biology, physics, sociology, whatever. There is probably a job in most fields for someone with software development skills. 
",1516820219.0
sweml,"Math, physics, and stats would all be great pairs. Just take from my experience and avoid pairing economics with comp sci. The only thing it helped me with was realizing that I do not want to work in economics.",1516825725.0
amandarosecook,"My college required you take at least one ""Logical reasoning"" class.  it was offered under the math department, and while the class was FULL of CS majors who wanted no part in it, I found it one of the most useful non-CS classes I took.  maybe look into something a little more concentrated than just math.",1516830210.0
CapableCounteroffer,"stats, math, electrical engineering, physics are all good ones. a lot of jobs like to see a masters, but I would recommend getting a job first and doing a masters part time or work a year or two then do a masters so have a better idea of what you want to specialize in.",1516830590.0
LongUsername,"What do you hope to gain by double-majoring?

The classic one is Physics: Computers are used extensively in modeling and physics makes a good match with computer science.

The other classic one is Electrical Engineering for people interested in going into embedded systems. This has been replaced these days by the Computer Engineering degrees to a large extent.

Matching CS with any degree allows you to specialize in software for that field. I've seen places that really loved getting people who had CS and Music degrees as they were working on software and tools for professional music development.

So my best advice would be to figure out what sort of software you want to write and pick a degree that gives you knowledge of that area.",1516832643.0
Slagerlagger,"Want to double major for both more knowledge, more career choices, and a more lucrative career",1516832856.0
salgat,"You're better off getting a masters than double major. Most places only care about that cs degree, the second major is a ""huh neat"" tack on.",1516833570.0
chromaticgliss,"I'm a Math/CS double. Doubling in Math or other computer-y engineering courses gets a ""oh more of the same"" response from a lot of employers in my experience. Specializing in some field (AI, Graphics, etc) in a masters will be more lucrative than a second major in a similar field. Ph.D. is probably not a good move unless you're established in some specialized field in industry already that you truly love and want to gain a further edge in that field (e.g. I worked on a team that developed compilers, so a Ph.D. in programming languages would have been useful there, but at my current job not so much).

But if you for sure do want a second major, I'd recommend doing something completely different than CS. Employers will see that you're not just a programmer robot and will perhaps be an interesting human to work alongside. Do music, art or history or something. Something you really enjoy. Ultimately, your major doesn't matter a whole lot once you have industry experience.

",1516837617.0
12GallonPoopBucket,"Math, hard sciences, and the various engineering disciplines are always a great complement to comp sci. Many of the topics overlap and reinforce each other. If you go this route you will have a solid foundation for further education (masters or phd) and a successful career.

IMO, the real benefit of studying comp sci is being able to apply that knowledge in novel ways. There are many fields that are desperately in need of modernization and enhancement via comp sci (this even applies to the hard sciences and engineering). Branching out and studying something off the beaten path isn't a bad move either.

There is no easy answers, but this is a good thing. You have freedom to pursue what you find interesting. A better question to ask yourself is, what do you want to do after undergrad? Once you can answer this I think you will have a much clearer idea of what is the best double major for you.",1516840656.0
datlanta,"Computer engineering for embedded badassery
Statistics for just plain being a badass 10 years from now.",1517118711.0
gerradisgod,"Physics is a very good major to double with. Terribly underrated by CS students, imo.",1517179481.0
WhackAMoleE,Math if you want to do computational complexity or quantum computing. Business if you want to start a company or sell your own services. ,1516787219.0
unnamedn00b,MATH.,1516815379.0
Buza-WasAlreadyTaken,i  need updates on this too but if i save it ill probably forget it. Anybody kind enough to leave a comment here if somebody posts a nice answer?,1516786939.0
StruBoy,Business. ,1516786947.0
flexibeast,"Full abstract:

> We present in this paper a new type and effect system for Java which can be used to ensure adherence to guidelines for secure web programming. The system is based on the region and effect system by Beringer, Grabowski, and Hofmann. It improves upon it by being parametrized over an arbitrary guideline supplied in the form of a finite monoid or automaton and a type annotation or mockup code for external methods. Furthermore, we add a powerful type inference based on precise interprocedural analysis and provide an implementation in the Soot framework which has been tested on a number of benchmarks including large parts of the Stanford SecuriBench.",1516765521.0
WSp71oTXWCZZ0ZI6,"[Link to the code](https://github.com/ezal/TSA)

I'm eager to play with this when I have time. In another life, I was an academic whose work was largely dependent on Martin Hofmann's. As soon as I saw the title, I guessed it was probably his work, but it's interesting how his career has developed. He's definitely a genius, but his early work was very much 
on the theoretical side of things, only of interest to people who working in the theory of type systems and restricted models of computation. About 10-ish years ago, he started transitioning into more practical applications. The first author (Serdar Erbatur) appears to be a PhD student or post-doc of his.",1516784340.0
,[deleted],1516768990.0
Chandon,"I've never seen a CS1 (intro to programing for CS majors) course use a textbook. Whatever source you find, I'd be suspicious if it implied the existence of any standard curriculum at all. ",1516816117.0
metgame,"I would approach this problem by first converting 0x20 to decimal 32. Then you add 4*4 (each integer takes up 4 bytes and you are offsetting by 4 integers) to 32 giving you 32+16=48. Finally, all you need to do is convert decimal 48 to hex 0x30. ",1516756629.0
rrkpp,"Maybe I'm confused but the address size is kind of irrelevant in this case isn't it? If your array starts at 0x20 (or let's just say byte 32), each element is 4 bytes, then 0 = 32, 1 = 36, 2 = 40, 3 = 44, 4 = 48, etc..",1516771610.0
IJzerbaard,"Just start at 0x20 and count it out. Where is X[1]? Well the elements are 4 bytes each, so count 4 ahead: 0x24. X[2]? 4 steps further, 0x28. Then X[3] is at 0x2C, and X[4] is at 0x30.

I didn't use decimal, the conversion just adds extra steps and counting in hexadecimal is not any harder.

> I have no idea how byte-addressing works

No, I think that's some sort of mental blockade because you're confusing yourself. There isn't really much to ""get"", this is really the simplest model, where memory is an array of bytes. You can draw it out by taking some squared paper and just start numbering the squares from 0 up. You can draw the X array too, X[0] is a 4-in-a-row block starting with the byte labeled 0x20, X[1] is immediately adjacent to X[0] and so on.",1516785578.0
nexes300,"    x[4] == *(x + 4 * sizeof(int))

Edit: or more generally

    T a[N]; a[x] == *(a + x * sizeof(T))",1516786267.0
emdeka87,"Just figure out which lead is longer.

More seriously, have you ever seen an actual electrolytic capacitor that wasn't marked with polarity? In my experience they're simply always marked.",1516756412.0
Sir_not_sir,"Take a look at glassdoor and yelp. Anonymous ratings can be gamed and sold. Even verified ratings, ala Amazon, can be untrusted. If your rating model is more akin to Uber than it may work.",1516748259.0
YeDoDohDontYaDo,Popularity contests rarely provide accurate results on anything other than a persons kerb appeal !,1516749978.0
kibleh,"As long as there isn’t a photo of you naked alongside the review, it is ethical. ",1516752178.0
Jaxan0,Computer Graphics. A well known conference is SIGGRAPH. There has been several seam carving algorithms on this conference. ,1516823559.0
_Bia,Computer Vision.,1516753467.0
rdrop-exit,"HP-16C Computer Scientist, or an emulator thereof.

https://en.wikipedia.org/wiki/HP-16C",1516733993.0
gboycolor,"I have an HP 35s (one of the best calculators ever made, in my opinion). It's an RPN calculator, so it will take a bit of getting used to if you don't know it already.

It has built-in bitwise logic operations: AND, XOR, NOT, OR, NAND and NOR.

For example:
17 <Enter> 4 <OR> returns 21, as you would expect. 

You can also download RealCalc for Android, which can be RPN or Algebraic, and has not only bitwise operations, but also you can switch between base 2, 8, 10 and 16.",1516728851.0
IJzerbaard,"Any programmable one, if you program it.",1516728478.0
arnedh,Windows calc always open in scientific mode?,1516731457.0
int-elligence,You can have a look at these notes. Short and useful.[OS Notes](http://pages.cs.wisc.edu/~bart/537/lecturenotes/titlepage.html),1516707502.0
CheifOfFury,"The best book I’ve read on OS concepts is Operating Systems: Three Easy Pieces ([Textbook](ostep.org) ).  It’s a free book segmented by chapter on the site! 

Additionally a small educational OS that you can use to implement the concepts without having the overhead of a huge OS is the xv6 which is mentioned in OSTEP! ",1516724672.0
celestrion,"If your library has [Programming with Posix Threads by Butenhof](https://www.goodreads.com/book/show/987956.Programming_with_Posix_Threads), pick it up.  It does a very good introduction to synchronization concepts and continues on to examples of using them in a Unix environment.  It's a twenty-year-old book, but the concepts haven't changed much.  The primitives are a little different on Windows, but there are direct analogues in their applications at the next level of abstraction up.

For the rest, John Kubiatowicz's lectures on operating systems (as part of Berkeley CS162) are great.  He's really great at giving both a historical perspective and non-technical analogies to OS concepts.  The [slides and lectures from the Fall 2010 semester of CS162](https://www.youtube.com/watch?v=feAOZuID1HM&list=PLggtecHMfYHA7j2rF7nZFgnepu_uPuYws) make for great listening.

For me, though, the best way to ""get"" those concepts was to see completely different implementations of them.  Modern computers are plenty fast enough to emulate gigantic older computers.  With [SimH](http://simh.trailing-edge.com/) you can emulate a VAX or two running VMS.  With [Hercules](http://www.hercules-390.org/) you can emulate an IBM System/360 all the way up to a modern-day zArchitecture mainframe (albeit slowly).  Playing with the old OSes and learning **why** they were occasionally so weird to my relatively young eyes helped drive home the difference between theory and convention.  Writing software that had to run on both Windows and Unix helped, too.

As far as C goes, it's the lingua franca of operating systems development, but certainly not all OSes were written in it!  You can write an OS in LISP, assembly, or even .net!  You're going to need something very low-level for bits that are timing-sensitive (interrupt handlers), or need to do ""unsafe"" things like write data to very specific locations in memory or on the I/O buses.  Everything above that thin layer can be written in anything you like.

That said, C is popular for OS development because the semantics of C are very close to the semantics of hardware from when it was developed (and again in the RISC age).  You can easily reason about what the hardware is doing from a bit of C code in ways you just can't with Python or Ruby.  Since OS development is primarily concerned with making the hardware available to applications, that mindset lends itself to hardware-like tools.  If you're not fluent (to an intuitive level) in C, you should get there.  Answering questions like ""what actually happens when I call a function"" are fundamental to answering questions like ""what happens when I call a function through a dynamic-link stub"" or ""what happens when I call a function that's implemented in the kernel?""",1516719014.0
cirosantilli,"I have created two resources which might help you:

- https://github.com/cirosantilli/linux-kernel-module-cheat : compile the Linux kernel and image from source, and allow you to easily GDB step debug it on QEMU
- https://github.com/cirosantilli/x86-bare-metal-examples : allows you to play with individual x86 systemland features, either on QEMU or real hardware

There are also several well known tutorials that build simple OSes from scratch, e.g. http://www.jamesmolloy.co.uk/tutorial_html/index.html and https://github.com/SamyPesse/How-to-Make-a-Computer-Operating-System I am maintaining a list of those in my [x86 repo's bibliography](https://github.com/cirosantilli/x86-bare-metal-examples/blob/master/bibliography.md#progressive-tutorials).",1516746190.0
igor_sk,write your own toy OS. see http://osdev.org for pointers. ,1516718553.0
WSp71oTXWCZZ0ZI6,"> Is it like a necessity to learn OS by coding in C?

I'll put my 2 cents in (as someone who's lectured OS design courses many times) and say ""yes-ish"". It doesn't necessarily have to be C if you're using some other systems language (like assembly or C++ or something), but I think, at some point in your education, you have to actually experience the edge between userspace and kernel space, as well as the edge between software and hardware. C is, I think, the best option to do that.

Asking if you can properly learn OS concepts without a systems language like C, to me, seems akin to asking if you can learn physics without calculus. You can sort of get an idea of how things usually fit together at a high level, but you won't really understand it in any depth.",1516765568.0
daholino,There is a MIT teaching os called xv6 (you can find it on github) and a more simplified version called JOS. You can find some labs to do with JOS and literature at: [here](https://pdos.csail.mit.edu/6.828/2017/) . I think that this is a good way to learn because you will be doing a lot of things by yourself.,1516722261.0
elcric_krej,"Robert Love has some nice books that will help you understand how the linux kernel works, at least enough to go take a look at the modern codebase yourself, though by now there may be better works than his.

There's a book called [""What every programmer should know about memory""](http://futuretech.blinkenlights.nl/misc/cpumemory.pdf) by a guy at readhat which can be very good when it comes to getting a ""programmers"" understanding about the underlying hardware.

For the ""higher level"" concepts using a diy distro can really help. Try using Arch or Gentoo as your main distro for a while, that will teach you a lot about the ""essentials"" of an OS beyond the kernel and the drivers.

In the end the definition of ""OS"" is very vague and changes depending on the year, part of the world, industry and person you are talking to.",1516734297.0
LAN_Rover,Take after Tannenbaum and make an OS from the ground up,1516750272.0
nulldragon,"Ok, so this may be a bit backwards. but i really found working with embeded systems to be a huge help with learing OS concepts.  

I found that writing code for such simple devices (arduino's, tiny arm chips etc) that interface some simple components (say a ps2 keyboard and a lcd screen) gave me a great understanding of memory, io, storage etc.  

All these other suggestions are great as well!",1516766369.0
Sh1ttyScience,"I'm old, but [this](https://www.amazon.com/Operating-Systems-Design-Implementation-3rd/dp/0131429388/) was the bible back in the day. Old basics are still relevant.",1516737299.0
LAN_Rover,Take a page from Tannenbaum and build one,1516750020.0
sharjeelsayed,"* Operating Systems: Three Easy Pieces
http://pages.cs.wisc.edu/~remzi/OSTEP

* Computer Science 162, 001 - Spring 2015. Operating Systems and System Programming - John Kubiatowicz UCBerkeley
https://archive.org/details/ComputerScience162001Spring2012

More at http://learn.sharjeelsayed.com",1516876734.0
shbilal001,"How did you manage to go through the computer science course if you didn’t understand all those concepts mentioned above? Do not lose hope though as you have many options to make yourself better if you did not understand. I would suggest that you go to a boot camp and learn for a few weeks the topics you feel that you didn’t understand while you were in school. It is not going to be easy for you to find a boot camp that offers all that you do not have a proper grasp about but if you do find one it may be worth is. It is important to note that these boot camps offer intensive learning for a few weeks and that will help you understand better what you didn’t understand while in school. Alternatively, find a tutor who is good at computer science to help you out. This can be inform of a mentor or just a friend who is more knowledgeable. The good thing about computer science is the fact that it has many resources online and you can’t miss finding the one that suits your need. You will have to dig deep in order for you to find the appropriate resources for your needs but there is no way around it. Good luck and all the best.
Holberton School (https://www.holbertonschool.com/)",1516915339.0
feedayeen," 'make' is a Unix command which executes a 'makefile'. A makefile contains a set of instructions on how to assemble a program, or do other tasks using command line calls.

The syntax of makefiles is a bit weird so you'll want to search for simple examples of them. Your professor might have provided one if they're nice.

For C based programs the idea of them is it contains a list of source files, include directories, and compiler and linker arguments.

This post is a reasonable explanation, a lot of the examples become very cryptic because almost everything can be represented by a variable. 

https://stackoverflow.com/questions/1484817/how-do-i-make-a-simple-makefile-for-gcc-on-linux",1516680407.0
sowpi6,Sounds like the Makefile may have already been written for you. scp is a program that lets you copy a file to\\from a network.,1516690172.0
TomvdZ,">  I'd like to know whether my method is correct

Well, have you tried *proving* it correct? (It is correct, as far as I can tell)",1516646279.0
iwantashinyunicorn,"Depends upon what the consequences for screwing up are. Right now you can kill a few people and not get in trouble for it, so software engineering wins. If anyone is ever held responsible for the consequences of bugs, things might change.",1516653622.0
zergling_Lester,"1. Formal logic has an immense relevance to software development, but most of it isn't even noticeable because it's entirely internalized, it's like a fish doesn't recognize that it's breathing water. More to the point, whenever some formal logic approach becomes widespread (like using RAII in C++ for taking a mutex for example) it becomes ""just an industry best practice"" and ""common sense"".

2. Will we see even more formal logic becoming standard practice in existing and new languages? Like, even more formally verifiable correctness guarantees? Definitely, but it's anyone's bet how fast.

3. The bad news is that in the field of self-driving cars for example the actual relevant algorithms are not designed by humans but are grown and trained like pets. You can have a 100% formally verified algorithm that runs a pedestrian detection neural network, but the real algorithm that makes decisions is embedded in that neural network's weights, and nobody knows how it really works.

    Really, nobody knows. For example, relatively recently (a year or two ago) big news in the field were that it turns out that you can confuse image recognition deep-learning NNs with an image of whatever, like a toaster, by changing the brightness of some pixels by less than 1%, to make it recognize that toaster as a dog. Using another NN that was trained to fool the recognizing NN.

    The mindblowing part was that such images fooled not only the particular instance of a NN they were produced against, but also other instances of the same NN-training algorithm, and other instances of _completely different architecturally_ NN-training algorithms, with a smaller but still >50% probability. And nobody knows why, as far as I know.

    We know that our NN-training regiments produce really good NNs, like 99% accuracy on usual data, but we don't know how or why exactly, and we do know that it's possible to fool them with adversarial data at 50%+ probability (also produced by NNs) and we don't know why or how it really works either.

4. And don't get me started on stuff like https://www.washingtonpost.com/news/monkey-cage/wp/2016/10/17/can-an-algorithm-be-racist-our-analysis-is-more-cautious-than-propublicas/, that shows that even using Machine Learning at all can be inappropriate in some contexts because it punishes people for something that other people do.",1516662051.0
cephalopod1,The new Microsoft Research podcast is decent.,1516634754.0
Stamden,"Software Engineering Daily is pretty great. 

It covers a wide range of topics though so you might not connect with every one. Like the past 10 episodes have been about Kubernetes, distributed systems, etc. But he also has stuff about ML, Networking, Data, and a whole lot more. ",1516634006.0
balefrost,"Not CS per se, but I always enjoyed listening to [Hanselminutes](https://hanselminutes.com/). It's more of a programming podcast than a CS podcast, but Scott Hanselman is a good host and always gets good guests. ",1516633464.0
gravity_low,"Here's a few that are related:

[Accidental Tech Podcast](https://overcast.fm/itunes617416468/accidental-tech-podcast) - iOS / Apple focused development

[Security Now](https://overcast.fm/itunes79016499/security-now-audio) - Network Security 

[The Changelog](https://overcast.fm/itunes341623264/the-changelog) - Open Source focused

[FLOSS Weekly](https://overcast.fm/itunes140847216/floss-weekly-audio)  - Also Open Source

[Programming Throwdown](https://overcast.fm/itunes427166321/programming-throwdown) - CS concepts and issues",1516634184.0
vastcuriosity,"I’ve really been enjoying [Data Skeptic](https://dataskeptic.com). While it’s not strictly a CS podcast, they cover a good amount of algorithms and computational complexity. ",1516651661.0
CatchingZzzzs,LevelOneTechs with Wendel is a good one. Not exactly podcast but still a good listen. ,1516637114.0
Aceofsquares_orig,I've currently found a podcast called data skeptic.,1516642003.0
alekhka,"If you are into Machine Learning, TWIMLAI is good!",1516643991.0
gokapaya,"I have some more language specific ones:

- [New Rustacean](http://newrustacean.com) (Rust)
- [CppCast](http://cppcast.com) (C++ ;)
- [Go Time](http://gotime.fm) (Golang)
- [Request For Explanation](https://request-for-explanation.github.io/podcast/) (Rust)",1516657062.0
hwaldstein1997,"I enjoy the Type Theory Podcast (http://typetheorypodcast.com/). It's slow to update, but the talks are excellent.


Another good podcast is Functional Geekery (https://www.functionalgeekery.com/). It sometimes feels like the host could drive a more interesting interview, but the guests that speak on the podcast are excellent.


Finally, I enjoy the Haskell Cast (http://www.haskellcast.com/). It's also infrequently updated, but the talks are enjoyable. ",1516657918.0
Sonder777,"Programming Throwdown. They have a main topic they talk about as well as side things such as tools they found that are cool, tech news, and book reviews. Some episodes are even spent interviewing someone on their company and cover computer science concepts used in their company.",1516659101.0
5454a1,Breaking Maths has plenty of computer science based episodes along with maths ones as well. One of the main speakers is a computer science graduate,1516653834.0
InconspicuousTree,[Talking Machines](https://www.thetalkingmachines.com/) if you are interested in AI/machine learning/math etc.,1516660910.0
strongjz,https://softwareengineeringdaily.com/ ,1516664394.0
ArtVandalay7,"Try Soft Skills Engineering! Two guys running an advice show for software developers with non-technical questions around navigating office politics, salary negotiations etc. Heaps of valuable advice and they’re both hilarious",1516714268.0
grok_it,"I like Software Engineering Radio (http://www.se-radio.net/).

I find it of much better quality than Software Engineering Daily (which has been suggested by someone else).",1516829588.0
darkoptictwo,"It's not about CS, but ReplyAll is a great podcast about the internet.",1516641516.0
AmatureProgrammer,"Should probably mention that it doesnt have to be strictly CS, can be programming related stuff.",1516635064.0
johanvts,Thore Husfeldt has a CS podcast http://video.itu.dk/page/castit/,1516652240.0
PM_ME_NSFW_STUFFS_,The jeff and casey show features software devs. It's funny and they usually talk about things they don't know about,1516668284.0
americk0,There are a bunch of good ones at devchat.tv. I'm a big fan of the Javascript Jabber one but they have ones for several languages,1516690379.0
dEv0iD72,"Not a podcast but some good learning can be found by watching **CS50** youtube videos.  Entire year of content is on it, not to mention the current lectures from 2017-2018.

David J. Malan is an engaging lecturer.",1516705119.0
wajanga,Basecs is a good one http://pca.st/yWt4,1517164061.0
Thesoulesscarrot,Idk I've been wondering the same thing... ,1516631994.0
jmite,"If you like both philosophy and CS, you might be interested in dependent types, type theory, proof theory, and the new foundations of mathematics, aka intuitionistic type theory, homotopy Type theory, Martin-Lof Type theory, etc.

In general, one strategy for finding a thesis topic is to look at recent papers you find interesting, and to look at the future work sections of paper.

For relevant papers, I recommend ICFP, OOPSLA, POPL and PLDI.",1516606432.0
Aalok1996,"Looking into research papers is a great way to go about it. Look for conferences in programming languages, for example POPL. Read the last section of the paper where you may find scope for further work and possible extensions. Feel free to browse though the author's webpage and contact them if the topic really interests you.

It's a time expensive process. Few cool topics that interest me are type theory (I'm currently working on homotopy type theory), automated reasoning, proof assistants, kappa calculus, and domain specific (functional) programming languages. Do let me know if anything in these areas interest you. I'm more into verification and proof assistant kind of stuff. ",1516670758.0
Serpens-Caput,"You could try to develop a simple language in Racket, first. Have you ever read sicp? ",1516968118.0
Serpens-Caput,"Check out this

https://en.wikipedia.org/wiki/Brian_Cantwell_Smith

https://www.amazon.com/Origin-Objects-MIT-Press/dp/0262692090/ref=sr_1_1?ie=UTF8&qid=1521454792&sr=8-1&keywords=on+origin+objects+smith

https://dspace.mit.edu/handle/1721.1/15961

http://repository.readscheme.org/ftp/papers/bcsmith-thesis.pdf

",1521454827.0
BobFloss,"Why don't you just try compiling it into a program and debugging it? Sorry for the somewhat unconstructive comment, I would help, but I don't know assembly.",1516597780.0
Gavcradd,"the two MOV instructions will load the two given values into the registers (eDx and eBx), then these two values will be pushed onto the stack (increasing the stack pointer eSP every time). The top value of the stack, which is eBx, will then be popped off (ie removed). On mobile so not flipping between screens to copy out the values from the screenshot!",1516603353.0
UnendingWinter,"I'm not sure if this is the right subreddit for this post but I'll try and help anyway. If the stack is growing downward (stack pointer is at a lower address than base pointer) then you are right about 9999 being overwritten by  0E80. The stack pointer will now be pointing here (address 0x0000). I'm really at a loss as to what will happen with this next push because we are at the lowest address, regardless I'll just assume address 0x0000 will be overwritten. The stack pointer should still be at this same address but the value should now be 0190. The pop instruction should pop 0190 into eBx and the stack pointer moves up to 0x0004. 

I'm currently pretty tired so it's certainly possible I've made a mistake but this is my understanding of the stack. ",1516603504.0
Yeah22,"Definitely been there! So I can’t talk too much on the client class, that sounds like it could be a few things depending on what you’re teacher is talking about. My college professors like to refer to “clients” as the people who hired you to write them code. 

Private and public are both reserved words that mean the same regardless of what is attached to them (variable or method). Private means that, that method or variable’s scope is only within the class it belongs to. Public means it is visible to all classes in the package. An example would be if you had a student class and a teacher class. The student class may have a private variable socialSecurityNumber. Because it is private, you would not be able to access the students SSN if you were not within the student class, meaning the teacher could not view it. Now if the situation was reversed and the variable was public, then the teacher would be able to access the student’s SSN. 

Static was something that gave me some trouble until I took Object Oriented Programming, then it became very useful. Java is an Object Oriented programming language. To expand on that a little, I’m sure your teacher has told you that a class is a “blueprint” which is true, each class is a blueprint to create an object of that class. Teacher class makes teachers, student class makes students. When you have a static method or variable, that means the method or variable is class level. This means when you make an Object is the class, it will not be able to use that method. Static methods and variables are very useful for calculations, or variables that will not change throughout the program. The way I like to think about it is that if an Object is calling a method it should be non-static, if not it should be static. Example might be teacher.addNewStudent(); this should be a non static method because the object is working on the method. If you were to do something like: int bestGrade = getBestGrade(student[] allStudents); this might be static because it is a class level calculation.

Hope this helps a little, this stuff can be tricky until you are exposed to what they are actually used for. Once you dive a little deeper into objects and how they behave this stuff will make much more sense.",1516583085.0
Yeah22,"Of course, feel free to message me if you have any other questions ",1516583933.0
generic12345689,"Someone smarter can give you a better answer but public and private declarations are about scope. If something is public you can access the object or method outside it’s deceleration like object.variable if the variable was public but a private one you cannot. 

Static objects can’t have multiple instances. It will always give the same value no matter where you use it.",1516582399.0
sdocy,"Methods and variables declared as private can only be used within the class they are declared in.  Any function within the class can access the variable, but code outside of the class cannot access the variable, even if the code has a reference to an object of that class.

Public methods and variables can not only be accessed within the class itself, but from any code that has a reference to a class object.
",1516583381.0
east_lisp_junk,"This is a common abuse of notation. For functions f and g, take f+g to mean the function whose value at x is f(x)+g(x). Then you can lift to sets of functions: if F and G are sets of functions, then take F+G to mean {f+g | f ∈ F, g ∈ G} and f+G to mean {f+g | g ∈ G}. Unfortunately, = is often abused to mean ∈ or ⊆ instead of =.",1516581540.0
TheWildKernelTrick,"As far as my rusty understanding goes, this is true. There's oftentimes an abuse of notation going with asymptotic notation and this seems to be your source of confusion. Because obviously you can't add sets to non-sets.

This (more or less) can be read as *n^3 plus a linearly upper bounded function has an n^2 lower bound*. ",1516583339.0
HeraclitusZ,"I agree its pretty abusive notation, but I think it works out true (if these functions of n must be asymptotically non-negative). Then, for large enough n, on the left we have n^3 + <non-negative term>, which is at least n^(3), and thus is contained in Ω(n^(3)).
",1516583103.0
gasabr,"If I understand correctly The Introduction to Algorithms (page 51), big notations are used when functions on both sides have the same power, but in the case they are different there is small-o and small-w. 
Sorry for English — not native speaker. ",1516617974.0
harassinator,"false because the notation does not work in this situation, you should try to ascend before you digress",1517717004.0
tony5381,Pretty sure that's undefined.,1516582228.0
sarahbau,"It really depends on your overall class load. Both classes require learning a lot of new stuff, but at least in my case I don't recall either having a huge out of class workload. As long as you aren't otherwise overloaded, it should be fine",1516577841.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1516573683.0
_--__,"In general, since these things may not have closed formulas, it is common (in CS at least) to approximate them with big-O.  In many cases the [Master theorem](https://en.wikipedia.org/wiki/Master_theorem_(analysis_of_algorithms\)) can be used for this purpose - either directly or after some massaging.",1516560216.0
StandardMilk,"Akra-Bazzi is more general than Master Theorem and is great in most real life situations like Linear Time Median Finding, but I believe it does not work for recurrences with square roots. In these situations, your best bet is to unroll the recurrence.",1516590863.0
TKAAZ,Is it a specific recurrence in question?,1516567675.0
011111011010,Summations are often useful.,1516563277.0
vladmir_zeus1,"I've heard quite nice things about this course from the **University of Helsinki**  
https://mooc.fi/courses/2013/programming-part-1/",1516444355.0
xyclade,"I’m not sure about java specifically but there are lots of online learning platforms for free. Some of my favorites are:
Coursera
Codeacademy
Udacity
Edx
Codeschool

And then there are paid ones too, I have a subscription on pluralsight, which is pretty good! 

Like others said Java courses are not outdated very quickly, especially the OOP part stays the same. I learned Java from a book that was 10 years old *shrugs*.  

You might find that the IDE looks a bit different but that shouldn’t be a problem. 

Now if you were about to to learn about javascript libraries, it would be a different story, as those are really moving allover the place ;-)",1516444877.0
notdeadpool,There is a course on Udemy.com by Tim Buchalka called 'complete java masterclass' which I would highly recommend.,1516440708.0
Elekhyr,Check out Derek Banas videos on YouTube ,1516436842.0
,Assembly.,1516417045.0
dgryski,Scheme and lisp.,1516413947.0
,[deleted],1516413616.0
nerdshark,"> Isn't that beautiful?

No, that looks like it's different solely for the sake of being different, thus annoying as shit and terrible design.",1516414157.0
hzhou321,"> Isn't that beautiful?

Did you simply find the function form beautiful or find the idea of homoiconicity beautiful? Do you find the setters and getters in a oop language beautiful?",1516420309.0
conscioncience,As an aside := is the notation usually used for assignment in maths,1516417394.0
combinatorylogic,"Why stopping there? You can go without any assignments (and any named variables/arguments) at all - see de Bruijn indices, combinators, tacit programming and so on.",1516443992.0
Yamakaky,"[Lambda computing](https://en.wikipedia.org/wiki/Lambda_calculus), while being equivalent in power to the Turing machine model, doesn't use assignments at all. Hint : it uses function parameters.",1516450055.0
tailcalled,"Generally, the variable name is treated fundamentally different in assignment than in other contexts. For example, suppose you have a variable named `myDog` which initially has the value `""Spot""`. Usually, when you call a function with said variable, you will pass `""Spot""` to the function, rather than some sort of reference to the variable. This means you can't use a function for assignment, at least not unless you do things somewhat differently.

(There are ways of doing things differently. I once made a programming language where every identifier just resolved to a string with the content of the identifier. E.g. if you wrote `myDog` it would evaluate to `""myDog""`. Then there was an implicit global map from strings to values, which you could look up in using `@` and set in using... some operator that I don't remember, let's go with `:=`. So `myDog := ""Spot""` would be equivalent to `""myDog"" := Spot` or `(""my""+""Dog"") := ""Spot""`. This is terrible language design, btw, and I recommend not doing it.)",1516450630.0
DartIvan,"In the modern programming all the “memory stuffs” are assigned to intern routine of compilator. So, = assignment reflect this tendency. 😊",1516442251.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1516411150.0
flexibeast,"Full abstract:

> It is well known that the length of a beta-reduction sequence of a simply typed lambda-term of order k can be huge; it is as large as k-fold exponential in the size of the lambda-term in the worst case. We consider the following relevant question about quantitative properties, instead of the worst case: how many simply typed lambda-terms have very long reduction sequences? We provide a partial answer to this question, by showing that asymptotically almost every simply typed lambda-term of order k has a reduction sequence as long as (k-1)-fold exponential in the term size, under the assumption that the arity of functions and the number of variables that may occur in every subterm are bounded above by a constant. To prove it, we have extended the infinite monkey theorem for strings to a parametrized one for regular tree languages, which may be of independent interest. The work has been motivated by quantitative analysis of the complexity of higher-order model checking.",1516400250.0
carette,"On first reading, the most important part is the exact statement of the main theorem (bottom of p.4) and the commentary right afterwards on p.5.

The result, once one understands the definitions, isn't as unexpected as it seems at first.  By fixing order, arity and the number of used variables, roughly the only way to make enormous terms is to re-use the fixed number of variables that do exist. Because of the types, this also forces what to be re-used to be *functions*.

In other words, their setup forces most terms to be extremely non-linear. And such terms have very long reduction sequences, as they show right from the start. [The cleverness in this work is indeed to set things up 'just right' so that they force non-linearity to be pervasive, and then be able to use that to prove the results.]

Restricting things to linear terms would be quite interesting. There might be vanishingly few extremely long linear terms in this setup. And certainly I would expect their reduction chains to be much shorter.",1516546491.0
blabheinn,"Thank you, the whole series looks quite interesting :)",1516398293.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1516351755.0
welchyy,Positive discrimination is still discrimination.,1516345569.0
leftoversn,TLDR; All the best candidates were male (=VERY problematic) so they threw skill out the window and only targeted the schools with the highest amounts of females for their hiring.,1516339554.0
Shin_Ichi,"This is pretty dumb...

They admit that they found the best candidates for the job, but since they happen to all be male, that’s a problem? They then go on to admit that they extended offers to several female candidates but they all declined. 

So, what’s the issue here? I’m not understanding why a 50/50 split is necessary. Why should we care what someone’s gender or race is? Why can’t we just hire the most qualified candidates and be done with it? 

Could you imagine if the article read, “We hired the most qualified candidates but they were all [insert minority here], so we needed some more white men”. Sounds absurd doesn’t it? I hate these kinds of people.",1516344778.0
Oni_Kami,"Why does a 50:50 gender ratio matter?

If this was something like ""How Duolingo hired only the best software engineers."" I would actually click the link and actually give a flying fuck.",1516341605.0
SepahX,"This is not positive 
",1516351719.0
pure_x01,So their assumption is that there is a bias against females in the hiring process. I have personally hired lots of developers and many women. When I have hired women I have had to lower the bar when it comes to technical skills just to get them in. There have been guys that were on the same level but I would never lower the bar for them sadly. ,1516341033.0
bkkgirl,">Perhaps not surprisingly, we were spending time at colleges and universities based on past experience, rather than their percentage of female computer science undergraduate majors.

This is satirical, right?",1516350162.0
,Thank God I don't patronise their shitty products. It takes a sick mentality to propagate sexism in the name of fighting against it. Ridiculous.,1516347202.0
-Supp0rt-,"Lol, I love the part at the bottom that says ""This study is based off male and female genders, although we do recognize that many other genders do exist"" 

So...if they're all about equality then why haven't they hired people from those other ""genders"" as well. ",1516347991.0
zeb251,"It's about time, all those dudes were throwing the ratio wayyy off",1516340291.0
fritz42,"Definitely doable, but you will have some days where you might feel a bit miserable.",1516337427.0
untraiined,Dont ever put off classes youll always have hard classes together youre an engineer but taking them asap allows you to retake in case you fail. ,1516337574.0
formeinredditdo,"OOP and CS 1 won't be too hard, you'll have a lot of fun in those classes. Plus, you get more exposure at once. This helps identify what areas you're good at and where you can improve.

Typically, ""Physics-1"" covers Classical Newtonian Physics. This normally involves calculus, but can usually be reduced to solving algebra equations. Sometimes the professor will show how the Calculus leads to the algebra equations. It's mainly dependent on the type of physics problems you receive. More important to know how to setup the equations and when to use what formulas. ""The rest is algebra"" as they say. Lastly, if your Physics 1 involves concepts from Electricity and Magnetism, you may need to be strong at Calculus 1, especially integrals. Again, depends on the problems. That was my experience at least, hardly any Calculus 2 notions for what we covered. 

Calculus 2 will benefit you to take early! :) Good for you, it will seem confusing at first, but that goes away quickly. If you're ever stuck, be sure to explore YouTube, or see the prof early if you can. It will help you advance quickly if you do well.

Lastly, have confidence, you'll be amazed if you put your mind to it. Have fun and best of luck :)

tl;dr Go for it!
",1516344189.0
codemagic,In hindsight i really wish I had completed Calc 2 before starting my Physics series. My Physics 201 had a prerequisite of Calc 2 and thought I could take them concurrently. Didn’t work out so well,1516338740.0
torebek,Thank you very much ,1516501996.0
IJzerbaard,"I'm looking for a decent way to figure out the minimum number of XORs to implement a carry-less multiplication by a given constant. The issue here is that doing it ""all at once"" is often not optimal, obvious example: multiplication by the number with all 2^k bits set would take 2^k -1 XORs when done naively, but can be split into `x * 3 * 5 * 17 ..` which takes only a logarithmic number of XORs. Of course, 3, 5, 17 etc are the factors of -1..

So this problem seems equivalent to decomposing a number into its carryless factors, but it isn't. Sometimes it is, but not always, because you can use the left-edge of the number to get rid of bits that you didn't want. For example with 8 bits, `x * 0b11010101 = x * 0b101 * 0b10010001` (going from 4 to 3 XORs) even though 0b11010101 is not divisible by 5 in the normal sense (indeed 0b11010101 is a prime polynomial over GF(2) so it has no nontrivial factors in the normal sense).

This can be solved with some circuit SAT queries, finding the smallest number of set bits (not counting the lsb of every factor, since it doesn't represent a XOR) that can still CLMUL to the constant being decomposed, but it's often slow and anyway it's a brute force hack for a problem that seems to have a lot of structure. It seems to have something to do with repeating patterns that are ""cut off"".",1516355109.0
Zophike1,Does anyone know of research(internships) for soon-to-be undergrads or undergraduates in the field of Quantum Information/Quantum Programming/ or Formal Verification/Topological Data Analysis/Category Theory ? If so PM me it would be greatly appreciated ,1516409580.0
Sam443,I'm currently applying for jobs. Where all should i look? I know of indeed and linkedin. anywhere else that might get overlooked?,1516470530.0
JdaAnCap1,"Some insomniac ponderings late at night....

SO....the annual extravaganza in 'pulco is coming up (big TDV Infowrarior here; not able to go but want to, wanna see the debate between Ms. L S and Mr. L R), whacha think J B's annual pick is for a crypto to look out for in the coming year (haven't heard it leaked yet, so it's being saved for the conferences to be announced)?

Why pick N E O over Ve/Qtm in the big C (sell for a newb vs. the other two alleged coins of Commiland)?

With the Mits Bank exchange news that recently broke, this a skyrocket point for A D A with a helping from the fact cryptos are taking off in the region in SK and CH?

Any others this year/next year taking off? Personally have had a good feeling for N E O for awhile now (was right about B T C going from 1k to over 10k from last spring to the end of last year and N E O was around $30 when introduced to it and figured it would skyrocket and it's already x4 atm....am firing on all cylinders here and am feeling a HODLing of some A D A is in order).

Compare/contrast on all ""privacy coins""? Which IS the BEST (P I V X, S U M O K O I N, X M R, D A S H, Vs/Zs, etc.)?

A r d o r over other platformers?

Cheerz.

P.S.

Thoughts on E N G and T R O N?

Does I n d i a have any particular affiliation for a coin? Think there's at least one, otth, but don't remember the name. Am looking forward to the future a bit more than a year or two and it seems, based on market trends in other fields, such as prowrestling, that the country is on the verge of entering the global market/having companies enter into it and, thus, modernization, thus further more, cryptocurrencies.

Does P o l a n d have any particular support for, or sponsorship for, a cryptocurrency? Read G N T has a P o l I s h team behind it, what's the speculation on G N T over A r d o r and other ERC-20 tokens?

WHAT IS A MORE COMPLETE LIST OF COIN NAMES THIS NEW A S I C VAULT HARD WALLET SUPPORTS AT LAUNCH? CAN'T FIND A FUL LIST ANYWHERE, JUST BITS AND PIECES....LOOKS AMAZING THOUGH!

Can sum1 eli5 the ""other"" B T C brands like B I t c o r e coin, B T C D, and B T C G for example? Is B T C D a privacy coin like S U M O, X M R, D A S H, P I V X, etc.? Why does B T C G have such a name? Is it backed by the actual commodity somewhere? /newb Thanks.
",1516597605.0
androidjunior,"Here is what I put, I just want to be sure that it is enough. I did an android application which is pretty good in my opinion. Those are the slides: 

Introduction

Purpose

Tools

Firebase(since we studied sql in uni, to explain it a bit)

Firebase-Database(part 1)

Firebase-Database(part 2)

How is notification sent? (part 1)

How is notification sent? (part 2)

Images",1516295656.0
Gollgagh,"am I missing something obvious or is this literally

    for(int i = 1; i < 100; ++i){
        int j = f(i) + g(i);
        print(j);
    }
¿?

^(minus the UI part)",1516318299.0
LightThief,"If you're looking for the comprehensive list of encoders/decoders might want to start here: https://en.wikipedia.org/wiki/List_of_open-source_codecs

Remember that compression has both subjective and objective results (how it looks to humans vs how accurately it resembles the original). There are awesome benchmarking tools written by ITU-T that are free to use. More reading here: https://en.wikipedia.org/wiki/Video_quality",1516294427.0
linkazoid,What happened to the number 9 in the first code example?,1516306854.0
Workaphobia,"I love the linear time median algorithm because it's a recursive algorithm that isn't just divide-and-conquer. The two recursive calls are on not just different partitions of the data, but different kinds of data entirely.",1516310403.0
llamatatsu,"Isn't this Quicksort/Hoare's select algorithm?

Edit: Well, just tell me if it is or it is not. Down-voting does nothing.",1516287873.0
vanderZwan,"> This could be an interesting application of radix sort if you were attempting to find the median in a list of integers, all less than 2^32.

I recently had this simple idea that you could *modify* radix sort for an even faster median find (or any selection algorithm really).

Let's take Malte Skarupke's [Radix Sort implementation](https://probablydance.com/2016/12/27/i-wrote-a-faster-sorting-algorithm/) as a starting point, since it's probably the fastest in-place radix sort available at the moment.

Basically, the only thing you would have to modify is these steps: (from the summary)

> - Sorts one byte at a time (into 256 partitions)
> - Calls itself recursively on each of the 256 partitions using the next byte as the sort key

Instead of recursing on each of the partition, you can find which of the partition contains the *nth* element you wish to select (like the median), and only sort that partition.

You would need to be able to pass an offset and total size, to be able to keep recursing until the last byte (or until the partition is of length one).

For uniform distribution you would expect to reduce the amount of elements sorted by a factor of 256 every time you recurse, so even for long keys that should be very fast. In the absolute worst case we would just degrade into radix sort.",1516313200.0
Maristic,"The graph at the end looks nice, and makes the algorithm look good but it's misleading. The text describing it concedes:

> It doesn’t count the work to compute the median-of-medians.

… which means it’s not a fair comparison. 

There’s a reason this algorithm isn’t used in practice. This is a classic theoreticians algorithm; great if you imagine performance as a mathematical game with contrived rules. 

",1516332489.0
moocharific,I think it had to do with fetching the link not so much the HTML. Iirc it was a javascript exploit ,1516247878.0
gnarly_surfer,"https://raw.githubusercontent.com/Philippus229/chaiOS/master/chaiOS/index.html https://raw.githubusercontent.com/peid/peid.github.io/master/index.html
",1516304103.0
,"Your logic looks airtight, but you need to work on your indicators of cardinality. In the database classes I've taken, lines and arrows wouldn't have cut it. A one:one relationship should be designated by a line with two cross hatches at the end rather than just a line, you know? But beyond that technicality, great job.",1516256274.0
mredding,"I don't actually know how to spell his name, but it's something like Mr. Bahein (bah-heen, as we pronounced it).

This guy taught calculus and linear algebra. A black man, from Germany, spoke only German, came to Yale to get his MS, couldn't read the paperwork and accidentally failed to fill out the admissions papers that said he couldn't speak English. If he had, he would have had to take a language course. Instead, he pursued his MS in mathematics and *no one noticed* he didn't speak English for a year. As he told us, you don't speak English in a math class, you speak math, so it didn't matter. He taught himself English from the footnotes in his math books.

Thick accent. Students who didn't like it were students who dropped out. Pretentious assholes. This guy would basically dance the robot in class to illustrate matrix rotation and translation. Take a bunch of students who don't know shit-all what they're doing or what's going on and he would tell us today, we're going to learn the chain rule. ""It's simple. Easy,"" in his thick, effectively Jamaican accent. I don't know how it comes out that way, but we adored this man. And his lessons really were easy. He demystified everything. He regularly took a rag-tag group of miscreants who had a fear of math beat into them from a lifetime of public education and teach us how to love it. I miss him.

Then there was a Programming I teacher, I forget the name of the class or his name, but he clearly didn't take his adderall. Guy was hilarious, never stopped talking or moving. You couldn't not pay attention to every word this guy had to say, not because he was a hot mess, but because it was so entertaining as to be riveting. It's like if Robin Williams taught programming. He was great.

I had a teacher who taught optimization and low level hardware stuff, how CPUs worked, compilers, assembly, cache, hardware latencies, all neat stuff. He was a nerdy old guy, gray hair, mustache, always wore these dorky old man shorts and sandals. He was in Nam, and had no qualms making it too real in an off handed comment. ""I've had to put a gun under people's chins and pull the trigger..."" While I'm screaming in my head ""OH MY GOD!!!"" You think you're a young *man* until you're reminded you're nothing more than an old *boy*. This guy loved the hardware, hacked hardware as class demonstrations, and really spoke to my heart because *I* love hardware. He had this demo program to teach threading models, a simple curses video game where every element on the screen had it's own thread. Insanity. Made his point, though.

There was a lab aide we called Siberian Tim, a husky, portly fellow with very spherical geometry. Terribly interesting guy. Did tech support for ISPs. Not for end users of ISPs like you and me, but of, say, Zimbabwe's on line 3, says their internet is down for the whole country. He claims he can get you online with a log and a length of coax, if you're willing to hone the log out into the shape of a parabola. The stress of the job made him lose control of his right eye for 6 months and he had a nervous breakdown. He had personal space issues ever since. He would help you while being on the other side of the desk and leaning over your laptop, reading upside down. He got good at that.

Dave Arneson was one of my teachers, *the* inventor of Dungeons and Dragons. Gary Gygax invented the dice system, Chainmail, but Dave invented the modern concept of role playing. He was mildly sexist, and referred to girls butts as ""having a nice backpack, if you know what I mean."" The girls typically didn't like him.",1516298247.0
DrApplePi,"I don't think it is possible as it seems that to solve for n unknowns, you'd need n pieces of information stored.  Which seems like a regular rule in physics/math.  

But there are some techniques to do so.  
It is possible with exponentiation, because every number has a unique prime factorization, you can definitely store a set of 5 numbers as such 2^a * 3^b * 5^c * 7^d * 11^e.

This is usually how I've seen the bijection done for integers.  I'm not 100% certain, but I think if there was a simpler way, then it'd be more well known. 

",1516230472.0
alin1popa,"How about: take the number of numbers (in your example: 5). Add to its left the last digit of the first number, than the last digit of the second number, and so on - and then repeat with the second-last digit  and so on. For example, let's say we have 3 numbers: 111, 2222, 78900. We will build as follows (only illustrating the outer loops):  
0213  
0210213  
9210210213  
8209210210213  
7008209210210213  
Would get trickier if the numbers could have decimals but I think it can be solved similarly",1516225469.0
CeruleanOtter,"Just because no one mentioned this before: Gödel’s encoding. 

You can encode any n-uple of natural numbers as the powers to increasing primes. E.g. 3,1,8,5 = 2^3 x 3^1 x 5^8 x 7^5 . This is unique and reversible, even though it requires prime factorization. 

Yes, the numbers used for coding will be huge. This is used in some Computer Theory proofs (for example to determine the cardinality of the set of decidable problems). 

It is not efficient in the sense you want it to be, I guess, but being such a well-known idea in CS, I think there’s value in mentioning it. 

Edit: sorry for eventual formatting errors, I’m on mobile. 

Edit2: specifying *natural* numbers. ",1516266356.0
zokier,"The most basic way of doing this (for two numbers, a and b) is `x = (a<<n) | b`, where n is some predetermined suitable number, typically `n = ceil(log2(max(a,b)))`. Extracting the numbers back would be `a = (x>>n); b = x & (pow(2,n) - 1)`. While I demonstrated the case of two numbers here, the principle is extendable to arbitrary number of numbers.

If for some reason you can't make such fixed-width blocks, then take a look on how for example UTF-8 solves basically the same problem.

edit: because this is /r/compsci, here is some actual CS background for the topic: [Prefix code](https://en.wikipedia.org/wiki/Prefix_code)",1516227467.0
rabinabo,"I didn't see anybody else suggest this, but you can use the Chinese Remainder Theorem, which can give a fairly space-economical representation.  CRT says that the system of modular equations x = a1 mod p1, x = a2 mod p2, ...., x = ak mod pk with p1, ..., pk pairwise relatively prime, then there exists solutions x, which are all equal mod p1 * p2 * ... * pk.  If you want to represent the values a1, a2, ..., ak, then find moduli p1 > a1, p2 > a2,...., that are relatively prime.  Then CRT gives you the solution x.  To get the a1,...,ak back, you have to just compute x mod p1, x mod p2, ..., x mod pk.",1516245524.0
MaunaLoona,"You can interleave the digits. Let's say you have numbers 100, 352, 111, 3, and 25.

First pad them with zeroes: 100, 352, 111, 003, 025.

Then interleave the digits to create a big number: 131000510202135

You can get your numbers back as long as you know how many numbers are interleaved. ",1516272290.0
cclites,"This turned into a really fascinating conversation. You all are pretty dang smart, and I'm all 'string concatenation' - what a dope I am :)",1516288240.0
Singum160278,"The number problem is interesting. If you need to store it as a number then you could. It would depend a little on how many digits your number variable could hold.

We first need to set limits on the size of the numbers we need to store. 
Also I am going to assume we are working with integers.

MaxDigits = 7 ; % numbers between 0 and 9999999 ;

for I = 0 : 5 ;  
   input ' Enter number ' , number ;

   if I == 0                                                                     
     V = number     ;
                                                  
  else                                                               
     V = V + number * 10 ^ (MaxDigits*I)   ; 

   end    ;

end ;  
 % The numbers are stored
 % To get them back you divide and use an INT function to get rid of the decimals. And minusing away the bigger numbers first to reveal the smaller ones.  
For I = 4 : -1 : 0    
      Number(I+1) = INT(V / 10 ^ (MaxDigits*I)) ;

     V=V - Number(I+1)*10^(MaxDigits*I)    

 end ;

write Number
% this should give you all the numbers back, but untested so might need tweeking + you might need to adjust the MaxDigits depending how big your single variable can be. 
",1516228122.0
sellibitze,"Just interleave their digits to form a new integer out of five integers. You could do this in binary to make it a little more efficient. Another way of looking at this are space-filling curves. Interleaving bits gives you the Z curve.

Files can be interpreted as numbers. You could also encode your integers in base64 using ASCII, build a text file by separating the base64 numbers using a Line-Feed character, save it to disc and interpret this sequence of bytes as a huge number in base 256. :-)",1516260660.0
cclites,"EDIT: Hopefully I didn't just do your homework for you.

**************


Have you heard of string concatenation? Basically you want to string your numbers together, but use some character to separate them. By doing so, you create a string.

Example: 1 + ""|"" + 2 + ""|"" + 500 

Wen you are done, you have a string that looks like this: 1|2|500

When you want to recapture the original numbers, you use some function to split the string back apart, and there you have your original numbers.

NOTE: When you do string concatenation, numbers are converted to strings. If you want to reuse them as numbers, you will have to cast them back to a numerical type.

NOTE 2: String concatenation and manipulation are core CS/programming principles, and every language has a way to stick strings together, and split them apart.

NOTE 3: Concatenation works differently in different languages. Some languages us a plus sign, some use a period.",1516225196.0
vlykarye,Message me if you wanna learn the real stuff. I'll show you de wai.,1516216598.0
DCSpud,"I'd say this is the wrong subreddit for this kind of question. /r/compsci is focused more on higher level stuff(I could be wrong about this).

That being said, figure out exactly what doesn't make sense. Which parts of Object Oriented programming do you not understand? Create of list of whys, hows, and whens. Be as specific as possible. Then using those questions you can ask a professor, fellow student, redditor, etc to help you get answers to those questions. Also, knowing the questions will make googling way easier.",1516218905.0
TrumpetSC2,"Hey there!

I too had a rocky undergrad time and I’ll tell you what I learned:

1) Goto all your lectures and take notes. 
2) After lecture, review your notes and mark things you don’t quite understand.
3) Goto your professor’s office hours. They will be able to rephrase what they taught in class in a way you can better learn in person. Also, you will need letters and contacts for your professional life later. Professors have all been through uni, so most of them love to help!
4) Treat hw the same way. Do it eary, and then take it to your professors and ask questions. If you are nervous to talk to TAs or profs, just be honest. I’ve sometimes gone in and said “Hey I don’t know the right question but I definitely need help!",1516219384.0
NowImAllSet,"The powerpoint one is definitely my favorite. Some other good ones are [Magic: The Gathering](https://www.toothycat.net/~hologram/Turing/HowItWorks.html), although that one is a bit of a stretch since it technically requires human interaction. And although I can't pretend to understand half of it, the [Turing Machine implemented in Conway's Game of Life](http://rendell-attic.org/gol/tm.htm) is very cool, as well!",1516238174.0
ITwitchToo,"A bit different, but the creator of Braid also created The Witness and somebody encoded SAT instances in its puzzles: http://makercasts.org/articles/the-witness",1516262985.0
heyandy889,"Nice. It continues to blow my mind that Turing-completeness is such a seemingly low bar (minesweeper? really? ha ha), but at the same time it implements the fundamental limit of what we can compute. I would expect that through R&D we could create a ""Turing Machine 2.0,"" but it simply cannot be done. 

We can make faster and faster machines of course, but my understanding is that even quantum computation does not operate outside the rules of a Turing machine. ",1516244381.0
Zophike1,"So mathematically speaking how would one define Turing-completeness and what are the consequences of an ""object"" being Turing-complete ?",1516506359.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1516211057.0
gintonicisntwater,Both are great. HCI is in my mind essential for being able to write software that people actually can use.,1516192009.0
MeoMao555,"HCI is more of a soft-field, after all, a human is part of the interaction. If you like subjects like cognitive psychology, HCI is for you. Apart from designing and coding UIs, your future job might involve more dealing with humans, including user testing.

AI is more theory and math oriented. If you like abstract theoretical concepts like algorithms, this is better for you. Personally, having tried both, I think AI is more difficult -- a basic graduate-level AI class is the most difficult one I've ever taken.

I think AI work is likely to be paid more as well, but don't do it unless you really like the subject and can handle the load.

If you have enough time, I would try an AI class or two (not just the 101, but also at least one graduate-level), and see if you can handle the material and how much you enjoy it.",1516193160.0
macnor,"I'd also suggest taking a upper division or graduate level class in each subject to get a better idea of your preferences.

If for whatever reason you like both equally when it comes to grad school HCI is less competitive than AI right now.",1516198810.0
garnett14,"As said in the comments, I'm sure it would be very helpful to take high level classes of both and make a decision after that. Two fields with both strong potential for influence in the future. Both should be pretty employable, so choose whatever one you are more passionate about!",1516429793.0
dsrptr,"why not aim towards both? 

 [The term AIA *artificial intelligence augmentation* is starting to do the rounds....](https://distill.pub/2017/aia/) 

> the use of AI systems to help develop new methods for intelligence augmentation.

> Ideally, an interface will surface the deepest principles underlying a subject, revealing a new world to the user. When you learn such an interface, you internalize those principles, giving you more powerful ways of reasoning about that world. Those principles are the diffs in your understanding. They’re all you really want to see, everything else is at best support, at worst unimportant dross. The purpose of the best interfaces isn’t to be user-friendly in some shallow sense. It’s to be user-friendly in a much stronger sense, reifying deep principles[20]  about the world, making them the working conditions in which users live and create. At that point what once appeared strange can instead becomes comfortable and familiar, part of the pattern of thought.





",1516198694.0
AndreaDNicole,[A professor](https://ait.ethz.ch/people/hilliges/) at my university is quite successfully combining the two fields.,1516199071.0
Vierergruppe,Why not the mixture between them. Affective Computing! ,1516205502.0
Coloneljesus,HCI is full of gimmicks.,1516215057.0
DSrcl,"I would advice not to use end-products/results of a field as the (sole) criteria of judging -- what matters the most is if you enjoy *working* on it. If you can, find a couple professors in these fields and try them out.",1516221823.0
TheLastJuan,"I enjoyed HCI. It was a nice change, after all the theory stuff.",1516200769.0
GNULinuxProgrammer,Take classes from both and decide yourself. I can't see how reddit can help you.,1516192327.0
kokobannana,What about all the other buzz words? ,1516210577.0
TylerDurdenForTheWor,I personally don't have any experience in coding but I think that both fields are necessary for the advancement of technology.  HCI is important for development and can act as a sort of check/balance for AI which historically has acted up (I heard a story about AI's developing their own languages to communicate with each other).,1516197604.0
flexibeast,"i'm old. i still initially parse 'ML' as a reference to [the programming language](https://en.wikipedia.org/wiki/ML_(programming_language\)), before realising that 'Machine Learning' is probably what's meant. ",1516190322.0
catboy96,They should add “Computer science majors are guaranteed a 6 figure job upon graduating”,1516181050.0
Kenny1239,Would definitely disagree with #4.,1516234451.0
ggchappell,"Generally a nice list.

I'm not terribly happy with #3, though.",1516184384.0
violenttango,"For fear of coming off elitist, I will say that my personal experience rendered number 4 to in fact be true, but I think it depends a lot of someone's background before coming into uni.",1516221099.0
tjorg35,"My first advice when trying to solve a problem is disregard the language you intend to implement the algorithm in (at least initially). Think about the problem at hand and figure a way to solve systematically. 

Consider things like: should I do this iteratively or would a recursive solution be better? What data structures will the problem need in order to be solved. These things become more apparent the more experience you have. 

When you have a general idea of how you want to solve a given problem then start worrying about language specific things. One suggestion since you are using java is to think in an object oriented manner. This isn't a requirement of java, but may help you depending on what you trying to do.
",1516158951.0
,"You can study example problems in textbooks and other people’s code online (stack overflow is a good one) to see how they do it.

Breaking the problem into smaller pieces or simplifying the problem by making assumptions are very common techniques in general.",1516170634.0
LeetHackerKid,"You will be making algorithms when you don't even know you are. An algorithm is simply a function with a bunch of instructions. Every time you write a good piece of code for a function that follows certain instructions, you have made an algorithm.",1516297440.0
BAwesome999,"There are a few very common algorithm design techniques. Picking the right approach is really a matter of practice and intuition. Here is a short list of a few of the most popular algorithm techniques. I've left off algorithms that leverage randomness because that is a subject that really requires its own answer.

Greedy algorithms: Solve the problem by identifying the most valuable choice are each step. This approach requires you set up the problem in such a way that there is a clear metric to determine the ""greedy decision"". Ex: picking a set of flights to get to a destination in as little time as possible. Choose the flight that gets you closest to your end goal at each step. Note that the metric by which we make our decision (distance) does not need to be the same as the one that governs the problem (time). You can usually identify problems that work with greedy approaches fairly easily. They tend to be the most intuitive approach.

Divide and conquer: Solve the problem by dividing into smaller versions until you reach a base case you can solve and work you way back up. Ex: MergeSort. If you can break up the problem into at least two sub problems that are of the same form then this may be the solution for you.

Graph Problems: convert the problem description to a graph and use a known algorithm. This one is kind if cheating but sometimes the way you describe the problem helps you solve it. We have lots of handy algorithms for graphs. Ex: shortest path algs (Djikstra).

Dynamic programming: Solve by finding an underlying common substructure to your problem and solve that. I can't recommend this one with out a lot of good explanation and practice. Ex: the knapsack problem. You can usually identify these problems by an inability to solve it any other way. DP problems also usually don't ask you to recover the process by which you got your solution. As if someone wanted to know which cities they could get to but only drive for 3 hours. You might return a list of cities but be unable to give them directions.

",1516215731.0
DSrcl,Do exercises.,1516221995.0
fj333,https://www.amazon.com/Algorithm-Design-Manual-Steven-Skiena/dp/1849967202,1516476099.0
LeetHackerKid,Java sucks use Scala or C#,1516216407.0
programmer01135,"You don’t need to be fluent in multiple languages but it is very helpful to be exposed to multiple languages. Your CS classes will do that some by (hopefully) giving you a mix of c/c++, Java, python, and possibly others (although those three are the usual suspects). 

You will do yourself a big favor by taking what you’re learning in those languages and applying it to other languages. Teach yourself at least the basics of functional programming. By doing that you will gain an understanding of how those other languages work and different ways of solving a problem. 

In the end you’ll find that a given workplace may look for primarily a c# developer, for example, but you’ll also be maintaining some php code, or writing new stuff in JavaScript, etc. What is really looked for in a developer is someone who has a grasp of the concepts and can apply them to whatever language / framework is the task at hand. ",1516147846.0
Leheria,"Hire a lawyer to write her a cease-and-desist letter (about $150). You don't need proof for that because it's basically a warning letter. What she is doing may be considered libel, and a letter explaining that will probably scare her off. What she is doing can cause real financial harm, and I think it's worth the expense to put this one to ground. It also gives you better footing in case it becomes a larger issue down the road.

If you have a day job, some corporations offer legal consultation as an employee benefit, too, so you may be able to get a letter written for free.",1516123189.0
magnificentbop,"Some sort of centralized service split into high level tasks of storing/profiling users, retrieving and categorizing job postings, and pushing notifications about said jobs.

Finding jobs would be a combination of using APIs or web scraping to start, then perhaps a pass through a NN to pick out spam/undesirable jobs.

Profiling users might be done by a thorough questionnaire to build a base profile, and a feedback mechanism to build a training set for desirable jobs.  Over time, you could incorporate ML to categorize collected jobs and use the training sets to get personalized recommendations.

Pushing notifications could be done via text, email, or an app depending on what you're comfortable with.

Plenty of meat on this project, start small and work iteratively.",1516052023.0
sir_sri,"In terms of functionality Indeed and LinkedIn already attempt to do something like this.   Resume building, 'one click' application to a job, alerts about jobs, subject matter and location automatic searches that sort of thing.  Indeed seems to scrape data from many pages.

When on the job hunt years ago, what Indeed does is more or less something you did yourself.  The first thing might be a web crawler or search result grabber at least that automatically hunts based on keywords.

Scaling up from there and were you'd get something useful for a portfolio/resume is to have it take a resume, (probably you need to curate the creation, so skills/keywords end up stored in their own way), but then automatically score job postings based on skills, and then visualise that score.  Maybe even sort based on score.  

So to do that I'd start with something like the Google JSON search API, then spit out the results of simple keyword searches.

From there it's a matter of automating the process with multiple keywords and scoring.

There's nothing specifically 'mobile application' about it, and I can't really think of any reason it needs to be a web app rather than a website that works on mobile, but if you want to make it a native app for educational purposes I'd grab one of the free cross platform development suites (Adobe phonegap, QT, Unity etc. etc. etc. ) and work from there, that way you have something that works on web as well as a native app in several places.  ",1516059677.0
voronaam,"Not as a mobile application. Because it is going to be another one of those that only run on phones with Google Play or Apple's AppStore sources enabled, will have no option to support smartphones on other OSes or regular Android with some different App Store installed.

In other words, it will only enforce the existing selection basis among students looking for an internship, making it even harder for people from unorthodox ways of life to have a chance in our society.",1516052064.0
Lendari,College cant replicate the results of working 40/wk on real problems with an experienced team. All it can do is get you in the door.,1516050118.0
lpmusix,"To be fair, compsci is *not* a programming degree, so you're not bound to learn much programming unless you go out on your own.",1516052136.0
formeinredditdo,"I would personally focus on building your algorithmic strength. Algorithm practice is best. You'll be able to solve problems easier when you recognize them from your previous practice.
Programming itself is just a means to deliver your algorithm, right? In addition, the principles of programming languages are a bit more important. i.e. principles of C++ are applied to Java, and you'll learn about the machine in this form. Loosely type languages versus strongly typed, casting, etc. appear in many languages. 

In my opinion, if you were to leave college now:
1) You should be comfortable in SQL, any version just to start, Oracle's, MySQL, popular ones with lots of support to start is better.
2) Know the design patterns of object-orientated software and applications to software engineering. 
3) Be familiar with principles of programming languages.

Other languages you'll learn and pick up pretty easily when you have those others aspects in the background.

Other things to consider for projects:
If you're wondering what to do coming down the line for senior projects, look at what your school, and other's have done for their projects. It's a good reference as to how far behind/on-par you are. There are various projects ranging from AI with IBM Watson to VR graphics that involving learning API's to web-focused platform delivery. 

Python is strong for data science, so you can always checkout the API/library. 
A lot of learning comes from reading manuals/API's, and/or experimenting. Sometimes you have no choice because of a lack of documentation. 
My friend who was recently hired had to learn an uncommon languages used specifically to his company (and a few others)
He said that he excelled in learning it because he understand what the language was trying to accomplish. 
It was an odd hybrid of JavaScript and was very ""stringy"".

Web development you can probably get the core idea in about 3 weeks, pretty easily. Study over HTML, SQL, PHP, JavaScript, and you'll understand a lot more about the web, but also why you're writing some applications. Java can be a back-end, the same as PHP, and that also offers different problems. Same with .NET, that's the sort of aspect the programmer's are exposed to/forced to learn about programming. 

But just have confidence that you learn a language/use an API to find what you need.

I hope this helps? It's just my opinion. Best of luck :)",1516048495.0
BrightLord_Wyn,"Try contributing to some open source projects. Check out your Linux User's Group at the college. I'm sure you have one. Don't fret. Most of the people I graduated college with couldn't program their way out of a box, and they all still have jobs. I get paid the big bucks because I come in and clean all that crap up.",1516048040.0
onfire9123,Most people that started from nothing and work hard to get through CS school feel like frauds as they go. Continue to work hard. That's the competence part.,1516064815.0
TheWildKernelTrick,"I learned web development. Front end, backend, and now microservice vs n-tier architecture. How to write and use APIs, how to build a UI.

I became an astronomically better programmer. CS projects felt super easy and more interesting because my comfort with programming didn't hinder my attention from the theoretical concepts at play.",1516048090.0
lrem,"Both C# and Java are pretty much enough to build *a* career on. It's up to you to decide if that's the career you want. If not, figure out what else you'd like to do and learn how to do it.",1516056682.0
neznein9,Whatever project work you do (the more the better) try to write it cleanly and put it up on your public github account. This is your portfolio as a dev and it is just as important as a solid resume.,1516068403.0
tkitta,"Almost 20 years ago I asked one of my professors whatever I can for my 4th year project develop a pseudo-random number generator in C. 

He told me if I want to learn programming I need to go and take a community collage course. 

I ended up writing a paper about different methods of pseudo random number generation. 

I only learned basic programming in college with most of it not having much impact on my work.",1516061320.0
_--__,"Mathematically, regular languages and recursive/r.e. languages are quite nice to work with - they are preserved under a lot of operations (e.g. union, complementation, homomorphisms, determinization, etc).  Context-free and context-sensitive languages are not so nice (though context-sensitive languages are closed under complementation).

Algorithmically (from a verification viewpoint at least), regular languages are very nice too, with many problems (e.g. emptiness, language inclusion) being decidable.  Since these sorts of problems are undecidable for CFLs, it is usually of more interest to work with finite state automata (or small extensions of) when modelling ""computation"".  If we can ignore the undecidability issues, then it is usually more useful to go right to the other extreme and consider the full expressive power of Turing Machines (or small restrictions of) to model ""computation"".

So CSLs are not mathematically ""nice"", and their associated decision problems are not algorithmically ""nice"", but that doesn't really prevent research into LBAs as a ""more realistic"" model for computation - so what's the issue?  The difficulty is that when dealing with **resource-bounded computations** it is (currently) much more useful to work with large classes of ""transformations"" - i.e. polynomial-time (or log-space) reductions - and (N)LINSPACE [i.e. CSLs] is not closed under such reductions making it a somewhat less-than-useful complexity class to study.

This is, of course, just my personal opinion on why the situation is as it is.",1516062983.0
penguinland,"LBAs are a good model for a computer that has limited storage. This hasn't been much of a concern for decades (hard drives are cheap), so the difference between doing something on an LBA and doing it on a Turing machine is rarely significant in practice. 

I imagine that if hard drives were still expensive while CPUs were cheap, LBAs would be more popular.",1516049226.0
moocharific,"This is an interesting question, and I don't really have any authority here but my guess would be that linear bounded automata aren't a really great model for how regular computers work so there aren't huge pay offs to researching them.

I'm really interested to hear otherwise opinions on this, I don't really know a lot about them.

Edit: by ""aren't a really great model"" I mean that with regular computers we don't (normally) have to restrict memory usage to be dependent on input size. In this way they're not a great model. ",1516043721.0
,[deleted],1515963477.0
MinecraftyJedi,Turned out pretty well from what I've heard,1515940452.0
hextree,Sounds geeky and esoteric. It'll never catch on amongst the common folk.,1515944553.0
,Does anybody know if it ever took off?,1515945088.0
eviltofu,I mocked it when it first came out. How could it beat gopher?,1515960900.0
jet_heller,This looks interesting. I wish to subscribe to the newsletter.,1515944836.0
EasilyAnnoyed,How does one claim a warranty on the Internet?,1515961837.0
cheese_wizard,What is Internet?,1515953778.0
mercurymarinatedbeef,"HAHAH, this guy is the biggest pretentious fraud in computing history.  Right up there with Shiva Ayyadurai who ""invented email"", HEH.  

So let's examine his nebulous claims of ""inventing WWW"". 

* He certainly didn't invent the internet. 
* He didn't create TCP/IP
* He didn't create or even pioneer multimedia applications communicating over networks on personal computers 
(BBS scene and especially FidoNet was around YEARS before his facetious claim of ""inventing the web browser"".  

Did I miss anything?  Oh yeah, he didn't create HTTP either, for that matter.  I guess people just love hearing oft repeated lies. ",1516448514.0
,[deleted],1515965802.0
redog,"> A man went to market and bought two fish. When he reached home he found they were the same as when he had bought them; yet there were three! How was this?"" [The answer is](http://He bought two mackerel, and one smelt!)",1515945075.0
,"Learn how to SSH into the Raspberry Pi, learn to use command line tools like vi and nano.  

You’d be at your PC but working with the Pi, and you’d be modifying the files directly—what I often do is have two windows open, one is the editor (like nano) and the other i use to run the script... you edit/save in one, and then execute in the other.

",1515938791.0
tenmilekyle,"Here's the only part can help with...you need to tighten up your title.  Replace the parenthetical with keywords, or just type those keywords into google.",1515938237.0
InterestedListener,"You can use plugins to develop python with visual studio. You can run and debug it on your computer but there still is the file copy that has to happen. I'd just script the scp command and throw it in a script you can run with one click.

Filezilla also makes this easy to copy over quickly. I've used that for moving code to pi projects many times. 

Props for getting into coding so young! If you enjoy it and stick with it  you can make some nice cash with it (maybe even before you graduate)",1515939858.0
Microscopian,"Investigate some Citizen Science (https://en.m.wikipedia.org/wiki/Citizen_science) projects at NASA, the XPrize foundation, or PLOS. Links at the end of article posted above.",1515898294.0
combinatorylogic,"Anything that automates jobs adds a value by liberating human beings from drudgery. And it's quite a wide range of things, you won't be able to simply list them all.",1515951939.0
bremby,"Do you have any experience at all with doing that? Because jumping straight to a compiler written in C++ isn't exactly the easiest or quickest way to go. I would suggest first defining the grammar and semantics, then writing an interpreter in Haskell, probably, and only then I'd consider a compiler in any language, especially in C++. 

I haven't done it myself, but I have done similar things at the university.",1515896175.0
watsreddit,"Seems like a pretty cool idea. I think it would be a good idea to remove the .vscode directory from version control, especially if you're looking for contributors to your project.",1515887134.0
Bonjourm8,"Forgive my limited technical understanding. Is this inspired at all by the recent Intel flaws found within kernal isolation tables and speculative execution?

If I'm not making sense, please do ELI5.",1515905529.0
,"Over on kern/entrypgdir.c, a few macros could help:

    #define LVL0(i)  ((i) * PGSIZE) | PTE_P | PTE_W
    #define LVL1(i)  LVL0(i), LVL0((i) + 1)
    #define LVL2(i)  LVL1(i), LVL1((i) + 2)
    #define LVL3(i)  LVL2(i), LVL2((i) + 4)
    #define LVL4(i)  LVL3(i), LVL3((i) + 8)
    #define LVL5(i)  LVL4(i), LVL4((i) + 16)
    #define LVL6(i)  LVL5(i), LVL5((i) + 32)
    #define LVL7(i)  LVL6(i), LVL6((i) + 64)
    #define LVL8(i)  LVL7(i), LVL7((i) + 128)
    #define LVL9(i)  LVL8(i), LVL8((i) + 256)
    #define LVL10(i) LVL9(i), LVL9((i) + 512)

...then the whole table becomes:

    __attribute__((__aligned__(PGSIZE)))
    pte_t entry_pgtable[NPTENTRIES] = {
    	LVL10(0)
    };

I'd submit this properly, but I'm not currently able to test this any more than staring at the badly formatted pre-processor output this makes (I'm still working on how to fire up the kernel in an emulator); hopefully this still helps. ",1515888803.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1515878977.0
fj333,YouTube is not really the right place to learn computer science.,1515926209.0
,[deleted],1515900622.0
stvaccount,"There are massive bug reports from Intel CPUs flooding in. My PC got 100% unstable with the recent 'quick fixes' that were hacked together from Intel.

If you are from Comp Scien. and please watch your dmesg under Linux.",1515877183.0
Haversoe,"> Is it all the stuff that we input plus the stack symbol? 

It can be but it doesn't have to be. [Here](https://en.wikipedia.org/wiki/Pushdown_automaton#Example), for instance, they read 1's and 0's from the input but the stack alphabet consists of A's and Z's. In other words, the stack alphabet is what you ""push to"" and ""pop from"" the stack, so to speak. There may or may not be some overlap with the input alphabet.",1515880177.0
static_motion,"It can be whatever you want. If it is convenient for you to have 3 different characters that are not in the input alphabet + the stack symbol in your stack alphabet, so do it. That's the beauty of pushdown automata. ",1516663954.0
SteveDyr,"It's whatever symbols you push on the stack, which may not all be in the input alphabet",1518128060.0
balefrost,"> Does anyone have any idea what my lecturer is going on about?

Well we don't really know what your lecturer said, so no, I don't know what they're going on about.",1515944782.0
curiousGambler,"What about a mathematically oriented piece of art? Not like a poster of algorithms or anything academic, but rather real art. I can't offer anything specific, but like a Klein bottle or something could be fun. If she has a favorite equation or something you could learn a bit about computer generated art and even create her a piece of art based on it and have it printed on a canvas. 

More generally, you might want to avoid anything too technical or hands on. I say this because she's surely plenty busy with mentally rigorous work and has plenty of reading and such to do for her studies. Something that's beautiful and relates back to her work/interests tangentially might be better than something mentally rigorous.

I've gotten gifts like academic books and stuff and it just stresses me out. I'm plenty busy, and now I'm compelled to do/read this other mentally rigorous thing because I got it as a gift, and that's not what a gift should be. It should be pleasant and relaxing.

Good luck! Your friend sounds super cool, her specific interests in graphs and algorithms are some of my favorite subjects.",1515872125.0
PurpleWallet,"I suggest something simpler than what others have recommended. 

Get something that sits on one's desk while they program, browse the internet, play games, etc etc. 

Could be a fun looking stress ball/animal. Could be a tiny lava lamp or other suitably eye catching piece of sleek architecture. Could be a thermos/mug with a bit of programming language on it or something.

You may have to look online for a specific style/genre (ie: CS related text/art/design/whatever), but otherwise it should be easy to come up with a gift. Just look around at your own computer desk and ask yourself ""what would I like to have?"". ",1515909407.0
PM_ME_UR_OBSIDIAN,"FYI: there exists a little contraption that you sit on your cup of coffee, and the steam activates a piston which rotates a set of gears. It looks *really* nifty, my old manager used to have one and it was pretty entertaining.

I'm googling a bit and I can't find anything about it. Hmmm.",1515911013.0
sarnthil,"Books are always a good gift (e.g. Gödel Escher Bach), however as a computer scientist she might already have the textbooks that are in her field, so you could buy her some other book that is still technical but not on her specific domain (e.g. “Soonish”, “We have no idea”). if you look for something more personal you can try to create an interesting puzzle. e.g.: you can design an interactive text game https://jerz.setonhill.edu/intfic/",1515868366.0
LoveOfProfit,Get her a raspberry pi 3. Cheap but can be great fun and creative.,1515868447.0
PM_ME_UR_OBSIDIAN,"The thing to know about theoretical CS is that it's not quite software engineering. So I suggest you take with a grain of salt any suggestions that you get your friend hardware as a gift. Edsger W. Dijkstra, one of the greatest theoretical computer scientists of all time, was famously incapable of operating a computer. Things have changed a bit since then, but the bulk of theoretical computer science research is still done with pen and paper. You reach for the computer when you want to typeset your findings, look up papers, communicate with other researchers, prepare a lecture, etc. None of this is particular to computer science as a field.

Recommend also against Godel/Escher/Bach. It's a divisive book. I hated it with a passion, and most people I know also hated it.

Depending on precisely how theoretically inclined your friend is, a good gift might be Gowers' *The Princeton Companion to Mathematics*. It's a highly interesting survey of the many subfields of mathematics, a ridiculous number of which come up in theoretical CS work. (For all this talk about computer science being essentially discrete math, I couldn't understand foundational papers in the theory of programming languages until I'd taken a couple classes in analysis. But I could have read this book instead!)",1515870477.0
djimbob,"Learn CS yourself, work through the latest volume of TAoCP, find a mistake in it, and get her a [Knuth Reward Check](https://en.wikipedia.org/wiki/Knuth_reward_check).",1516064673.0
noodlesquad,"As someone who just graduated with a BS in CS, I can tell you now that I would not like any friend of mine to gift me something educational. From family I'd expect it. Are you sure she's into getting these educational books? I also feel like it'd be super hard for someone not in the field to know what is up to date but also what is desired in the CS field. 

A nice headset, wireless speakers, a badass backlit keyboard, etc. would be pretty nice to get. CS people love their gadgets and a lot of them are into their computers (like building them and gaming hardcore lol). 

I'm into cute socks, plushies, graphics Ts, and anime stuff. Does she have any other interests?

Good luck!

Edit: just wanna also say to not get her wireless speakers if she's not the type to use them and don't get her a keyboard if she doesn't have a computer, this all depends on how much you know about her life and financial situation

Edit 2: here is another idea, it's a power strip tower that I asked my dad to get me for Christmas :) it's pretty nice and has normal outlets but also USBs, if she lives in a dorm or small apartment this would probably be great for her, unless she's already rich and has everything D:
https://www.amazon.com/dp/B01FHMPV1S/ref=cm_sw_r_cp_api_s5OwAb9FP2G0M",1515883952.0
DrApplePi,">Last year I gifted a hardcover of ""The Art of Computer Programming.""

Dang!  I wish someone would get that for me...

Books are kinda tricky to suggest for a college student, a lot of the good ones that I've used seem like they also tend to be common university books so she probably has most or all of them... but here are some good ones I've used:

Concrete Mathematics by Donald Knuth, I believe it's largely supposed to expand the math techniques needed for Art of Computer Programming.  

Artificial Intelligence: A Modern Approach

Computer Organization and Design. 

Barring a book, I really like the raspberry pi suggestion. They are fairly inexpensive but they can be used for a variety of projects and programming. ",1516074664.0
crabbone,"Overwhelming majority of tools for computer science are free.  (However, some aren't, but you'd do best to ask your friend if she may need something like https://en.wikipedia.org/wiki/Maple_(software) ).  When you are working towards terminatory degree or beyond, you rarely read hard-cover books, it's mostly the articles you get from subscription to SC journals.  So, I wouldn't try to gift your friend professional literature.

But if it has to be CS-related, and it has to be a book, then I'd look for something odd, rather than for something mainstream.  It's a gamble, but it could be a memorable gift.  For example: https://www.amazon.com/Divine-Proportions-Rational-Trigonometry-Universal/dp/097574920X -- this is a book by the one of the very few mathematicians who don't believe in real numbers.  To me, his lectures on linear algebra were very inspiring (unlike the ones I had to suffer through in the uni).  This is less controversial, but still: https://en.wikipedia.org/wiki/Homotopy_type_theory I'm not sure if there's a hard-cover edition, it's available for free as a PDF.  Yet another controversial theory in mathematics, which should appeal to a CS student is finitism.  I didn't read this book: https://www.amazon.com/Finitism-Mathematical-Applications-Synthese-Library/dp/9400713460 but I would love to when I have more time.",1516091609.0
ImTheTechn0mancer,"If she likes math and programming or art, a framed image of the Mandelbrot set would be cool. ",1516140270.0
glec,"PQ is the state-of the art index structure if you don’t care about 100% Accuracy: https://lear.inrialpes.fr/pubs/2011/JDS11/jegou_searching_with_quantization.pdf
You can build the index once and keep it in memory since the descriptors are very small.

Be aware that most tree-based index are not very useful in high dimensions as discussed in the famous VA Paper: http://www.computing.dcu.ie/~sblott/docs/VLDB98vectors.pdf
The index structure described there is also the best one for high-dimensional vectors if you need 100% accuracy.

An attractive method if you don’t care too much about accuracy and easy compatibility with sql is LSH: https://en.m.wikipedia.org/wiki/Locality-sensitive_hashing

I contributed to a research project which built an open-source database for high-dimensional vectors amongst other things. You can find a paper discussing implementation and comparing index structures here: https://link.springer.com/article/10.1007/s13222-015-0209-y (If you don’t have access send me a PM and i’ll send you the PDF)

Last but not least, consider dimensionality reduction methods such as PCA.

All of those index structures do not necessarily require the search vector to be in the DB (Accuracy for LSH degrades massively though)

Maybe you can give us some more info? What do those 300 dimensions represent? How many vectors do you have?",1515877630.0
freekzindel,"Would an R-Tree library be a solution? There are some implementations that keep an on-disk store of the data points, but I do not know if they load all data into RAM when searching.
 ",1515867117.0
chetty365,K-means clustering,1515868092.0
Mister_Abc,https://github.com/facebookresearch/faiss,1515911418.0
SteeleDynamics,K-d tree!!!,1515875689.0
Healbadbad,"I haven't looked too much into exactly how they work, but check out faiss and flann, since I haven't seen them mentioned yet:

https://github.com/facebookresearch/faiss

https://www.cs.ubc.ca/research/flann/",1515902801.0
Liz_Me,"The differences in distance in high dimensional spaces are unintuitive. Without a *manifold embedding*, the points become lonely, where there isn't much difference between the furthest and the nearest neighbor. 

Here's a graph that shows a zero mean, unity variance gaussian distribution and the distances between the points in the cloud. You can see that for d = 2 or d = 10, you have points that are close, and you have points that are far away. At d = 200 all of the points are about 20 units away from each other.

`https://imgur.com/a/fX8Mo

Beyer, Kevin, Jonathan Goldstein, Raghu Ramakrishnan, and Uri Shaft. ""When is “nearest neighbor” meaningful?."" In International conference on database theory, pp. 217-235. Springer, Berlin, Heidelberg, 1999.",1515927351.0
moondoginyoureyes,Must be based in L.A.,1515866583.0
rkp768,"Hi there, use the timedelta method from datetime module for this. See code below.


    from datetime import datetime, timedelta
    x = timedelta(seconds=50000)
    hours = (datetime.min + x).time().hour
    minutes = (datetime.min + x).time().minute
    seconds = (datetime.min + x).time().second

Hope it helps.",1515864025.0
jsakia,"There are specific time modules in the Python language. Easiest would be to import the module and use its functions;

  import time
  hours_clock = time.gmtime(70000)   # accepts seconds and returns a time object which contains all the units
  
  hour = hours_clock.tm_hour

  minutes = hours_clock.tm_min

  seconds = hours_clock.tm_sec


Though, I am guessing that you need to do the math yourself. So, you've basically got that already.

  total_time = float('70000')   # I guess you are casting to float because the input by the user is a string

  hours = int(total_time / 3600) 

  minutes = int( (total_time % 3600) / 60 )    # the modulo (%) is a nice function that gives remainders

  seconds = int( (total_time % 3600) % 60)


Then print out the answer can be simple or fancy.

Simple  --->    print(""The current time is"", hours, minutes, seconds)

Fancy  --->    print(""The current time is {0:02}:{1:02}:{2:02}"".format(hours, minutes, seconds))    # here the {} mark the position of a variable passed to it from format(). The first number is the position of the variable. The colon separates the request. The last number says to always use 2 digits and the number right before it says to fill any missing digits with '0'. 

You can play around with the {} commands. Try {0:2} on some lower value inputs. Since no 'filler' character was defined, you'll get a space on single digits values, like 11: 2:30. ",1515864671.0
atlacatl,StackOverflow is your friend.,1515863993.0
DaveVoyles,"Oh this is nice, great work!",1515852290.0
fuzzynyanko,"... and the code looks like it's pretty good quality.

The author could have easily removed some lines by removing some whitespace and using Stroustrup style instead of K&R, but used a style that puts in more lines than reduces them",1515876478.0
07dosa,Is this a software rendered? Cuz i haven’t seen one for such a long time. I thought everyone is so into accelerated graphics. :p,1515840558.0
faintedremix009,"Dude, nice.",1517170331.0
KillerNo2,"If you don't go to a top 10 CS school, then any school is fine. Go to a local Uni, you probably have one within 50 miles of you no matter where you live in the US. In-State tuition is much cheaper. If you're willing to pay, you're accepted at most Universities.

If you can't do your own research on what it takes to be accepted in a top 10 CS program, then it's a safe bet you're not getting in one. If   you never looked at college before and joined the military to get away from it, then it's also a pretty safe bet you're not going to be competing for a spot with kids who have spent their entire lives preparing to go to college.

Computer Science is Computer Science, it's not a degree in ""programming"", it's a degree in Math. You need a CS education to be a good programmer, but you don't need to be a good programmer to be a Computer Scientist. So don't set yourself up for disappointment. Associates Degrees in ""Computer Science"" are very much degrees in programming and mostly worthless.

If you have more questions about college and your future career, check out /r/cscareerquestions. 

I appologize if my post sounds condescending, I've just answered this question a lot.",1515819154.0
shbilal001,"Getting a good programming school can be a challenge especially if you have no idea where to start. As you have said that you are willing to move states to get to a good school, I would suggest that you take a look at our good programming school called Holberton School (https://www.holbertonschool.com/) and see the offers our school has for students who wish to pursue a career in Computer Science or programming related field. Our school offers a 2 year full-stack course to help students attain the level of being regarded as full stack programmers and that is in a short while. Moreover, our school offers financial assistance to students who may require financial aid for them to learn freely until they complete their course. This arrangement has been made in order to benefit the student as they only get to pay a small fee at the end of the program and that is after they have gotten a good job. Try to visit our school and check out what is being offered and find an enrolment option which suits you best. Programming is a good skill to have today and the industry offers lucrative deals for programmers who are skilled and efficient in what they do. You too can become one of them. Good luck.",1516904472.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1515785761.0
Revrak,This is just marketing. You can do the same with just the camera setting the sensor at maximum sensitivity  even a smartphone camera is good enough for this,1515776194.0
stdlib,Tom Scott did a video about this that's pretty good as well https://www.youtube.com/watch?v=1cUUfMeOijg,1515784610.0
kbob,"FYI, this was first done over 20 years ago.

https://en.wikipedia.org/wiki/Lavarand",1515790824.0
teddyknox,"Wonder if you could max out all RBG values in the video feed to 255 by shining a very bright light at the camera at night through the external window, and then use that to predict the key. ",1515803401.0
Booty_Bumping,">Computers have a real problem when it comes generating truly random numbers

Yeah, but this is overkill. Cloudflare is just having a bit of fun.

>Cloudflare has good reason not to trust algorithm-generated code, relying instead on the effectively erratic movements in groovy retro fixtures set on shelves instead

You almost never extract the random numbers directly out of the seed data (there are [exceptions](https://qrng.anu.edu.au) though). If your seed data looks like this:

    0802010802010801010801010801020801010801010801020801010801010801
    0308010108010108010108000208010208000208010108000208000208000308
    000308000308000418ff0518ff0618ff0518fe0618ff0618ff0618ff0618ff05
    18ff0518ff0518ff0418ff0318ff0318ff0318ff0218ff0218ff0118ff0118ff
    0208010108010108010208010108010108020108030208030208040208050108
    0501080400080400080300080300080300080300080300080300080300080300
    0804000803000803010802000802010802000801010803010802010802010802
    010801010801010801010802010802012801ff2801ff08010108010108010208
    0101080101080102080102080201080102080202080101080102080102080101
    0801020801020801020801020801022801ff0801010801020801010800020801
    010801020801000801000801000801000801000801002801ff2801fe2801fd28
    00fd2800fd2800fc38fffc38fffc38fffc38fffc38fefb38fefb38fefb38fefb
    38fefc38fefb38fefc38fefc38fefc38fefd38fefd38fefd38fefe38feff38ff


(this is raw input from semi-erratic mouse movements) then you don't want to use that data as your random numbers as it's locally predictable, just like an image of lava lamps will have many rows of white-ish pixels where the wall is.

Instead, you feed it into a secure pseudorandom number generator that can constantly be fed new data, constantly scrambling the generator's internal state. Then you'll get numbers that look more like

    6fdb2326426f56d9462bbf8bec5c9619b6db281229d4ef3c126ae76ad7438927
    f24e0d254cf46613b78c8041dfb1712b3fb93ad0a484d07fd21af727c49db10d
    805d1213bacc0db772fff95c33eda9856c29f2f8c338674960293a249aaf55ed
    235486a0b238a45895b4d47b50ec86d1ca5180d1d1fab366f923eade28bdf72a
    3bb734744b6e995e5f957e5b8053da8208191e9100ee3ed5393fbb29b0f38ddb
    948f149ece55e4a21741f280852f733ff7194559ee9b7679df2f6194e73e6a7c
    eaab817b42f7ad5f6297ebd9df1c15eeb71bac9b752d5164f1ee0761c99f9c7f
    8a2f242891bf13ccbd52195b606bed1442ce9d0b6432462141c6c22cc4a849f9
    fda5e8460965dff9dde978a8a447a059a0cf5591936a62c5d98cdb6145abe126
    0764b632369d869237bde33ef5a1876c018d30b21cd757590bb4d88f37729575
    a6d9d4e3f5eb2bf5b8b502950784b2f6a6f7792aaf0f10b92ccd9a048e143d09
    780d54291ce65134504028bbb9e1b72dd5bd8e0866d5d6ab6283cfbd6e919b37
    9e359367c74a67477c636c93b5cfbc4bc44d0b466a4afea22968aa70a20553b7
",1515805421.0
Isvara,"More like 0% of the Internet, since it's only there in case the local randomness sources are compromised. They even say they hope they never have to use it.",1515803393.0
rro99,"Fun proof of concept, but is it actually used?


Edit: I'm questioning if a billion dollar company actually has it's production grade hardware in any meaningful way hooked up to a bunch of plastic lava lamps in their lobby. It's fun PR, but please, this is a Comp Sci subreddit, use your critical thinking skills.",1515789513.0
PumpkinCougar95,"Ok , there are n+1 total greater than or equal to signs.

Let r of them be equal to signs

So first select which signs are (=) in 

(N+1) C( r ) ways 

Then select out the remaining n-1 numbers (0-n both excluding)  n-r distinct numbers.

Now obviously there is only one way of arranging 

Hence total number of ways 

=  SIGMA (n+1)C(r) * (n-1)C(n-r) 

= (n+1 + n-1) C (r + n-r)

= (2n) C (n) 

Edit: 
Sorry didn't realize you wanted to know the meaning of the Question..

basically you have to find the number of ways you can select n numbers with each of them greater than or equal to the last one.
",1515768023.0
JimH10,"Try it with *n=2*, *n=3*, etc.",1515756786.0
hamup1,"Another solution is to consider paths on the integer lattice from (0,0) to (n,n) moving only up and right. For any path, let b_i be the largest number for which (i-1, b_i) is on the path. In other words, b_i is the y-coordinate of the highest point on the path along each vertical of the grid. Then every sequence b_i satisfies your condition (since you can never decrease by going down), and any sequence satisfying your condition can be drawn as such a path. Hence, the number of sequences a_i is in bijection with the number of paths from (0,0) to (n,n), which is simply (2n choose n).",1515820076.0
Wildcatace16,If they don't specify the length of the sequence and each element in the sequence must be less than or equal to the next there are an infinite number of such sequences. If the length of the sequence is specified or the inequality is instead strict there are a finite number of such sequences.,1515819956.0
aliqk1603,"Unlike most people at my university, I hadn't programmed before uni, but I was pretty much hooked at ""Hello World"" in my first class.


I was in finance before and that sucked. I knew for sure this was gonna be my career when we had to write a server from scratch in C in second year. I really struggled with the assignment but while working through it realized why this field pays so well (if you're good). 


Now I work as hard as I can everyday, and things are really going well. My GPA is decent and I have an internship lined up for after. 


If you enjoy solving problems, figuring out the logic, don't mind struggling with something for hours, y'know how it goes, then you'll be fine.",1515764232.0
Vi0lentByt3,"TL;DR if you like projects and programming, try and do software engineering, if math and theory are more your speed computer science is most likely a better choice. If you are indifferent I would recommend, personally, that you do CS since I think programming is easier to learn because it can come with experience if you have the aptitude.  

similiar to aliqk's comment, I had very little programming/CS experience before taking it in college(had an amazing middle school teacher that showed us how to make webpages in HTML). I was hooked when I started running the most basic user input, using a scanner in java to read the console and then just doing some arithmetic/validation, that to me was incredible. 
I think you distinguish the very important difference between Computer Science and Software Engineering/Development:
http://www.drdobbs.com/architecture-and-design/software-engineering-computer-science/217701907
(day to day software engineering does not require too much CS if you are just solving business problems, a tech company is another story. Both fields are inter related even though their subject mattter areas are not directly dependent on each other, you can be a programming without formal CS knowledge)
So if you really enjoy the project aspect and the sense of accomplishment, an engineering degree might be more aligned with your interests, because CS is definitely considered the more theoretical, mathematical based discipline, say hello to discrete, algorithmic analysis, Set theory, Number theory, Statistics, theory of computation, etc.
now there is some crossover though, such as programming language theory. So IMO it is easier to learn programming because it is really experience related but CS is a bit harder, for me at least, because the mathematical concepts behind computers can be so grand it is hard to grasp. 

Hope this helps!",1515765914.0
CreativeCoconut,"I worked as a programmer for 3 years, then did something else for 2 years. During that time I had a small project where I needed to program again and it was this ""Holy shit, I love it even more than before feeling"" that made me stick with it. 
Sure I doubt it every once in a while but never too seriously.",1515755762.0
,[deleted],1515763819.0
ZinNotFound,"I realized that I wanted to do CS at a very young age. A friend of mine showed me Ubuntu while I was in middle school. I thought it was really cool, and wanted to know more.


The ""Eureka!"" moments are the ones we all strive for. It can be frustrating, but it's worth it [for me].",1515755509.0
thedude42,"I just stuck it out because it seemed like the most relevant degree path to my interests.

In hindsight I’m so lucky I decided to stick it out because when I run in to peers in the IT world who don’t have CS degrees and don’t have any grasp of the theoretical underpinnings of the field, I see what kind of weird misconceptions of reality people have. Fundamental misconceptions about how numbers and quantity work which result in wild misunderstandings of the implications of certain computational conditions, e.g. exponential growth vs polynomial growth, cardinalities of infinite sets, etc.",1515769906.0
timmyotc,I took a cs class and realized that I had been formulating my morning routines like a programmer my entire life.,1515761292.0
_waltzy,"As with another poster I also knew its what i wanted to do from a very young age, but I have friends who ended up doing it because ""they like computers, and didn't like working in a shop"" and the ones that completed are doing just fine and making good money. I would say, if there's nothing else you'd rather be doing, stick with it. ",1515758393.0
spinwizard69,"Actually I didn't.    This after taking a few quarters of CS as a minor.

There are many reasons but looking back upon it, the decision was sound.   It isn't the right job for me and my personality.   I could elaborate but really it is a personal decision.",1515763736.0
IMightExist,"During my senior school days, I was pursuing the science branch. I had taken the CS class just cause people told me it's easy.
Parallely, I was introduced to Linux(Ubuntu) by my sister, and that thing was the best thing that happened to me. I got very interested in Linux. Learned Bash programming. Asked and answered tons of questions on AskUbuntu. Learned Bash. Improved on my C and C++ skills. Solved maths problems from Euler website. Spent hours writing 8085 assembly code, and optimising it to remove every byte I could, using weird quirks like XORing a register with itself, instead of moving a zero to it, to save one byte :P

I hated Physics and Chemistry. Even in maths, I was not interested in Geometry, only in Algebra. It was clear that CS was the only thing I could excel in.

Eventually, my adventures took their hit on my grades, and I ended up in a shitty college. While I do regret choosing this college, and even my decision to pursue engineering, I never regretted choosing this field.

Heads up: There is a lot of maths in CS, you better like it. If not, go for Information technology(or whatever it's called in your country). Also, CSE(E for engineering) is a good choice.",1515764366.0
EmptyPoet,"Maybe when I got my first job? Or at least a couple of years in I stopped having doubts, and at the same time there was never any doubt. Not really. 
I always knew CS was for me because Ive always found it interesting, even though I wrote my first ”Hello, World” program at University and I really struggled at first.",1515773041.0
shangothrax,"In my first CS class, when I realized that it was less physically messy than mosts sciences.",1515775257.0
VeganBigMac,"I am a senior undergrad and still have my doubts to be honest. However, I can tell you a few things that have kept me from leaving.

1. Working. There is a satisfaction you just dont get as an undergrad seeing your code in the real world. Everything just feels more real. Its not that the work itself legitimizes the code, its that the routine, shallow labs in school sort of delegitimize it.

2. Personal projects. On a similar note, when I started working on personal projects, I was suddenly way more inspired. Seeing your code get made into non trivial projects that are thrown away the moment they are graded inspires you to dig deeper and thing harder.

3. There is somewhere around late sophomore, early junior that things start rapidly clicking. It is after you have gotten all of your fundamentals, a few languages, perhaps a deep dive in assembly. It just feels great. Suddenly you can talk to other programmers, contribute online without being a bother, etc.",1515788899.0
aedinius,"In my second year of CS I was struggling with some of the higher maths I had to take. I was thinking maybe it wasn't for me, but I did well in the CS side... so I started looking at an Information Systems degree. I took a few classes and *hated* it. 

I ended up getting a minor in Information Assurance, but continued with CS and weathered the math requirements. A few strings were pulled and I was secretly let into one of the honors classes in an attempt to get a certain professor, and ended up taking a few classes with that professor. If it weren't for him and his teaching style, I wouldn't have made it through to the end. 

I'm glad I was able to finish. Comp Sci, Elec Eng, and Comp Eng have a special ""badge of honor"" type thing where I work. By staying in CS I met some people I wouldn't have otherwise met, and they introduced me to a small community and, through some networking and no small amount of hard work, I got to the job I have now. 

**TLDR:** Stay in school kids. Do what interests you. Just don't get a [Liberal & Fine Arts degree](http://www.machall.com/comics/20001204.jpg).",1515800123.0
mdatwood,"I didn't own a computer until I was a freshman in college. I started college pre-med, but took a programming class because I was constantly playing with the computer I had bought for college. All it took was the one class, and I switched my major to CS and the rest is history I guess.

Looking back, it was pretty obvious that computers and electronics are what I enjoyed. Even though I didn't own a computer I enjoyed math and puzzles and was a typical nerd. I did have an original Nintendo as a kid, but it was normally in parts because I would take it apart, curious how it worked (and yes, it still functioned :) ).

Anyway, if you enjoy solving problems you'll do fine.",1515805546.0
ianwold,"In my experience most programmers enjoy programming the most when they've solved a difficult problem. In addition, CS school can be many times less fun than the career - depends on the college and what you end up doing for work, obviously, but in school you need to focus on many aspects of CS combined with the uphill Battle of learning everything. When you're working, you may be able to specialize in what you like to do. For example, I thought my Networking class was a huge drag, but right now I'm mostly engineering user interfaces and business logic, which I enjoy. Hope this helps, and good luck with your studies! Most importantly, do what makes you feel the best.",1515823582.0
kibleh,I glued two computer monitors together and got an A+,1515824971.0
jet_heller,I haven't yet. I've been out of college for over 25 years.,1515762771.0
WhiteQuill,"For me it was mostly when I started trying to do things as efficiently as possible. Not from a programming standpoint, I only learned how to code much later in life. Also there was always this big question of how to prove something is mathematically impossible to devise as a fixed process (an algorithm) or how to analyse how long an algorithm has to run with respect to its problem size. (computational complexity). 

*incoming spiel warning*

Probably the first of the two hugest things that made me feel like computer science was for me was when I stumbled upon this [video](https://www.youtube.com/watch?v=YX40hbAHx3s) quite a while back. It explained the whole question of P vs NP, which in simple terms is asking whether we can ever verify the solution to certain problems as fast as we can verify them. (I know I'm skipping lots of details here), and the second being Alan Turing's work on the Halting problem and computational biology. 

It was interesting for a few reasons. Mostly because prior to high school I had never seen this aspect of the world, the notion of computation and algorithms everywhere, and that we had formal frameworks for analysing and studying them, proving there are things computers can never do, or things our computers can never finish in a feasible amount of time. Being able to prove that games like super mario is NP-Complete or that Chess is in EXP are really amazing feats to me and it just goes to show that a lot of real-life problems can be neatly classified into how efficiently we can solve them. 

Secondly, it's because questions like P vs NP are exciting, it really feels like an important question about our universe to me, just like how scientists in other disciplines are curious in other aspects as well. I'm sure most people say that about any big question in their field and I wouldn't dismiss it, but I probably love this mystery the most. 

Lastly, it's mind boggling how many things are capable of doing what your average computer is. I don't mean in terms of speed like CPU frequency or like RAM space. What I'm talking about is the ability to compute just like your computer can. For example your computer can sort numbers, calculate means and medians in a spreadsheet, well one could make equivalent systems that are turing complete to do those things as well! this [link](https://www.gwern.net/Turing-complete) mentions a few but my favourite will always be just using *mov* from the x86 instruction set. I think it speaks a lot again, about the nature of our universe, how so many things seem to work out like this. 

Reasons like that, are why I study cs, not because of the software engineering or programming it's so closely associated with. Though I can't deny it's important, I just feel like a lot of people tend to miss out on the theoretical aspects as well because they dismiss it for just writing code. 

Also honorable mention to recursion! When I first learned about it and its role in cs I was so mind-blown. ",1515776478.0
Jorslu,You mean I get paid to basically sit down on my computer all day?,1515766321.0
Cleanumbrellashooter,"After my first self study class in highschool. I finished the two AP classes and petitioned to add a self study class where I could code without interruption every day in class and once every week or so, I had to show the teacher what I had been working on. That was really the jump off point for me, haven't looked back since.",1515768405.0
laxatives,"I started physics and business. I got fairly deep into physics coursework before I took a single CS course (maybe 2 years of physics classes) and a little less than a years worth of business courses (mostly econ). The physics program highly recommeded the 3-course introductory CS series taken by CS majors. By the end of the series I was knocking out CS assignments before even looking at the physics problem sets. It wasn't really a conscious decision at first, but I realized I liked doing the CS a lot more than the physics. Ended up dropping business and doubling physics/CS. Gradually my internships shifted from research labs to tech. I work as a machine learning engineer at a unicorn now. Before I did machine learning at a few startups.

As an aside, the upper division CS courses had an awesome project schedule. Most of the classes had no weekly problem sets, just a handful of intense projects. So you were free to budget your time month to month and focus on clubs/sports/hobbies most of the semester and bang out the projects. Physics had a terrible schedule where problem sets were generally due Monday/Tuesday without getting the relevent discussion section until Thursday/Friday, so you often had busy weekends without much of an opportunity for office hours between section and due dates.",1515773908.0
alex-o-mat0r,"When I started studying. The more I got into it, there more I wanted it. Didn't know much about it before and had completely different expectations, but I was still fascinated from day 1. Apparently tho, I had some luck with very good teachers/professors.

I wonder, do you often think about CS-related stuff in your free time? How certain problems should/could be solved? What (potentially) great software could you do with current technologies? Any (big or small) free time project you eventually want to tackle? These should be signs, that you're certainly on the right track. 

If nothing of that applies to you, do you at least feel fine when writing software? If so, than just studying it for the career is still a sane choice. If you actually hate it, you should consider switching. Just imagine how it would be, doing this full-time every day.",1515775434.0
midwestmitchell,"I had similar feelings my first couple semesters in CS. The projects were very fulfilling when they finally worked. I fell in love with my first systems class. We used a discovery board that uses an ARM processor to do simple things like make a compass out of LED lights on the board, graph the pitch and roll of the board on a screen and eventually we built small games that used a wii nunchuck and other hardware. I also found set theory and proofs fascinating so that helped. 

Does your university have an informatics degree? It is definitely a more project based path and allows you to include other fields you are interested in easier. ",1515777737.0
,Ever since I can remember I have been in love with computers. I was programming and tinkering with PCs since I was quite young so going into CS felt like the right thing to do. ,1515777833.0
TheWildKernelTrick,"I went through school with the ""For each credit that I don't know how to fill, I will by default fill it with a computer science course"" mentality. Soon enough I was beyond the requirements for a minor and stuck it out. Around this time I also got into hobby web development which landed me multiple internships which cascaded into a data-sciency software engineering position.",1515783554.0
sstinkoman20x6,"I was coming from astronomy and physics, which is a whole different ball game, but tons of people in those fields end up in the software industry after college anyways.  Taking a few CS classes during my 2nd and 3rd semesters, I realized how much more I enjoyed it pretty quick...  I ended up signing up for as many CS classes as I could squeeze in to get a better feel for it.  I didn’t actually change my major until it was almost too late though, and ended up getting the astronomy major anyways... but I don’t recommend putting it off! I was swamped trying to fill out the requirements for both of em.

I guess my own take is that if you’re feeling good vibes, try going all in! If you aren’t super thrilled, keep your options open with some other electives (most liberal arts colleges will give you the room to try new things, make some mistakes, and figure out what you like). Best of luck!
",1515784510.0
dantheman252,Took a CS class my first semester of college as part of my electrical engineering curriculum. Really enjoyed it. On the second CS class I enjoyed it so much that I decided to shift my focuses to CS. Downhill ever since,1515788118.0
chidoOne707,Back in 2005 when i realized everything will be done in computers and before the big social websites rose and before the iphone and smartphones existed.,1515788164.0
BrightLord_Wyn,"When I realized that I could contribute to any field I wanted to with a CS degree, then I got into the real world and now I'm stuck in document management. Not that I hate document management. It's just a lot more boring than what I originally thought I'd be doing.",1515788263.0
EIGHT_,"In my first year of university I had a 60% average and I failed one course. I jokingly told my friend who was in the same predicament that I was in that we should try Computer Science because all we did all day was spend time in front of a computer and do nothing. I hadn't been exposed to any Computer Science prior but for some reason I just naturally picked it up, I went from averaging 60% in my first year to a 85% average by the end of my fourth year. I'm graduated now and work full time in one of the big four. I don't think I actually enjoy CS, but rather I enjoy being good at something naturally. ",1515789060.0
funciton,"I always liked making things, and I always liked maths. Then I learned how to program in my early teens and I was sold. It wasn't a conscious decision, it just sort of happened. ",1515795550.0
shutjohnkeets,"So I’ve been working in finance for 4.5 years (investment advisor for 2 year and fixed income trader for the last 2.5 years) and just recently went back to school to pursue the full computer science degree. 

I’ve never been able to get out of my head how much I learned and expanded my mind by just simply taking my first programming class (it was to satisfy an elective). Go figure, I only landed my first job in the investment advisor role because I could automate on VBA and they needed someone on the desk to make things a bit more efficient when it came to a handful of tasks. And ever since I started working it’s only become a major value added skill in all my roles.  

Now I find myself back in school taking more intermediate classes while working full time. Call it crazy? Maybe... but I’m not regretting it one bit right now and I know there are some challenging times ahead on the learning curve.

I’m pretty skeptical on the job growth potential both wage and mobility wise in traditional finance and want to have this skill set officially under my belt. Some people recommended to just go to an academy for coding but I didn’t want to feel like I was “cutting any corners” (no offense to those hardworking people at those academies) comp sci is a very intense subject that requires you to submerse yourself in it, and a university environment with abundant resources is where I feel it best... don’t care if I’m a little aged to go back for another degree, this is def my calling.",1515804217.0
balefrost,"So I've been messing around with programming since I was a kid. And the thing is, when you're a kid, you don't have a lot of control over your life or the world around you. I think what captivated me about programming is that it gave me some power. I didn't have to be satisfied with what was put before me - I could tweak and change and create and use my computer in a way that other people couldn't. I think that's when I knew that this was the field for me.

And programming isn't the only field that's like that. I'm sure that woodworkers, artists, chefs, and gardeners get that same feeling whenever they make something new. The thing that is neat about programming, and which probably sets it apart from many others, is that we're building systems out of systems... and then creating new systems on top of that. While there are fundamental limitations on what we do, a lot of the limitations are imposed by other choices we've made. And that [leads to one of my favorite quotes of all time](https://www.youtube.com/watch?v=2Op3QLzMgSY&feature=youtu.be&t=8m29s) (watch until about 10:30): ""in building a large program, there's not all that much difference between what I can build and what I can imagine. ... The constraints imposed in building large software systems are the limitations of our own minds.""

Computer science has changed the way I think about systems. I claim that it has changed how I perceive and interact with the world. I see everything through a slightly different lens than most people do. That's not unique to computer science, but I think that this hints at the profundity of computer science. 

If you like completing projects, and you like the challenge of figuring out *how* to solve them, and if you like always learning new things, then a programming career might be right up your alley. Keep in mind that the tone of an academic CS program is a bit different to the tone of a professional software development job. The CS program gives you fundamental knowledge that will be useful to you as a developer. But there are a lot of skills that they don't really teach in a degree program (or at least they didn't in *my* degree program, oh so long ago). ",1515828045.0
FonFalleh,"The realization actually kinda snuck up on me when chosing my major in uni, when I realized all my favourite courses were CS courses.

I went into IT because my general interest of technology, and programming for a living seemed sweet. Basically like being paid to solve puzzles.

Anyway, courses like Artificial Intelligence and Compiler Construction along with some computability theory have opened up my eyes for what to me are the most interesting problems (with or without solutions) imaginable.

Formal reasoning and its applications are rad as heck.",1515860000.0
AtlantisCodFishing,"I was in Grade 9, and we were writing Hello World and Who Wants To Be A Millionaire in Visual Basic. I got the material so quickly that I spent most of my classes helping other people with their projects.  

I started college in 2009, and still haven't finished it; projected graduation date is 2019. In that time I have sold a startup, released a library, and am currently finishing development of a desktop/mobile software suite for an abroad client. The fact that you aren't great in school doesn't necessarily mean you won't make it in the real world.",1515936587.0
PurityLake,"Since I was 12 (10 years ago). Went to a computer camp, did programs in Scratch and some HTML, it sparked my interest in making a computer do what I want it to, now I'm in a dev team making games and I plan on starting to write a dynamic programming language in C/C++. I have doubts a lot, thinking I'm not good enough and that I am not able. It is just a thing with me, but when I put my mind to something, I get it done. Just gotta learn to kick those bad thoughts to the curb, might take some time to learn how but it'll be worth it",1515947446.0
DrApplePi,"My math teacher in my sophomore year of high school showed us how to make a program to solve the quadratic formula on our calculators.  Within a few months, I started spending a few hours everyday programming during class and after school.

I was making all kinds of programs, simple games, massive math and physics programs to help with solving problems.  

So I knew what I wanted to do by the time I was a sophomore in high school. Now I'm about a year from graduating and I'm a tutor for the school.  

I would say that in your first few classes there isn't usually a lot to be excited about.  It really depends on the teacher and university.  I spent my first 2 years at a different university and none of the classes had hardly any exciting projects.  The university I'm at now, actually does have some pretty cool projects during the second semester.  

But I didn't really start to enjoy any projects until my fifth semester when I took an AI course.  Even now, enjoying projects can be hit or miss. Its easy to be discouraged that you might have made the wrong choice, but things can be a lot of fun and those moments are worth it and then some.  ",1515775286.0
AnyhowStep,"Hey everyone, I've been working on a personal project and been using vis.js to visualize my graphs. My graphs are always,

+ Directed
+ Acyclic
+ Multiple sources (I think that's the term for when there are multiple root nodes)
+ Multiple sinks (multiple leaf nodes?)
+ Connected (if we treat the edges as undirected, they're connected)

vis.js' node sorting for the visualizations wasn't doing a good enough job for me. So, I've been trying to roll out my own sorting algorithm.

I've managed to come up with an algorithm that does the following,

+ Only draws edges which are part of the transitive reduction
+ Does a topological sort to compute where to draw each node on the Y-axis; root nodes go on top, child nodes go lower and lower. No child node is at the same level or higher than its parent.

It looks really good to me on the Y-axis. I don't have any formal education with graphs, and this is as far as I got.

My problem is that I don't know how to handle the X-axis. So, it's being handled by vis.js at the moment. The results are... Random and usually ugly.

I know that 99% of the time, my graphs don't have an upward planar drawing (I looked it up on Wikipedia and I'm pretty sure that's the case). So, I can't use an upward planar drawing algorithm (even if I could, I don't know of an algorithm to do that).

Does anyone here know a way I could approximate a planar drawing that looks pretty? Or a way to compute a sorting for the X-axis, given that I know the transitive reduction, and depth of each node? I don't mind if edges cross. It just has to be upwards (which it currently is), and the crosses should be minimized or localized.

Here is a [screenshot](https://imgur.com/a/vXQ9f) of one of my graphs. As you can see, it's terribly messy, and could use some prettifying. Root nodes are in red.",1515741218.0
Bromskloss,"The Manchester code makes sure that there are no extended periods of time without level transitions (voltage level transitions, say), which could make you lose synchronisation with the signal you're trying to receive.

https://www.youtube.com/watch?v=8BhjXqw9MqI",1515700304.0
ScaryestSpider,"There is so much variety in the Computer Science field, this is hard to answer. A lot of things are mixed between the two. It really depends on what you enjoy working on. Cyber security tends to be more IT/system admin related, but also lots of studies of implementation of various systems and how to exploit them. Software Engineering is more design and coding, building software rather than using software. Working to requirements and implementing features. But the two are heavily related. My degree focused on Network Security, but I'm doing pure software engineering now and love it. Its perfectly possible to go back and forth between the fields. Best option to try to decide is do personal projects, participate in hackathons, and learn. Then decide what you want to do career-wise. Most of the knowledge is interchangeable, so no matter what you field you focus on, you wont be shut out from doing the other.",1515691016.0
Nerdlinger,"> All in all, what kind of people should do software engineering,

The kind of people that enjoy software engineering. 

>  and what kind of people should do cybersecurity?

The kind of people that enjoy cyber security (or at least some aspect of it as the field is vast). And for those who enjoy both, there is plenty of crossover.

Seriously, life is too short to work in a field you don’t enjoy. You’ve got plenty of time to sample what these two fields (and others) offer; take advantage of that time. Depending on where you live there may well be free or low-cost (especially for students) conferences or meet-ups that you can attend to see what kind of stuff goes on in those fields. There are tons of online resources like blogs that you can start reading and tutorials that you can follow along with.

The earlier you start getting some exposure, the earlier you can start focusing on how you want to guide your future. ",1515692357.0
n06,"I see a lot of people on here buying into the premise that you cannot do both software engineering AND cyber security, that’s not true! 

Cybersecurity is a field of study and practice, where, in my mind, software engineering is a skill set. Most cybersecurity companies sell software, and the people who build that software are software engineers!

My advice would be to try to get hired as a software engineer at a cybersecurity company. That way you can continue to build software eng. skills while learning about the cybersecurity industry. If at any point you want to move into a different discipline in that industry will have a much better shot!

Good luck either way. ",1515718434.0
evman182,"The one thing I would say that is common to both fields but not exactly the same is staying up to date.  
  
As a developer you definitely want to stay current to sustain a career and be able to compete, but if you fall a little behind, it's not likely to be a big deal catching up. It seems like on the security front these days that you'd have to basically stay current on a daily basis. Doesn't mean there's going to be some huge vulnerability discovered on a daily basis, but any given day, there could be something critical discovered that has to be mitigated very quickly that otherwise puts your company's or its clients' data at risk.",1515705143.0
faultless280,"Software engineers plan and implement software systems. It is a combination of systems engineering and computer science. They are involved in all stages of the software development lifecycle including requirements, architecture, design, implementation, and maintenance. Software engineers tend to not commute as much as cyber security personnel and can telework in many cases.

Cyber security involves identifying all potential vulnerabilities for a given system (in terms of confidentiality, integrity, and availability) and proposing mitigations for those vulnerabilities. These can be administrative, technical, or physical controls. Cyber security personnel tend to travel a lot, which can be pretty demanding for a family. Red teams perform threat simulation (more realistic, operational threat) while blue teams work with system developers and maintainers to find as many potential vulnerabilities as possible. Due to security concerns, teleworking is less frequent.

I have industry experience in both roles. If you like building stuff, software engineering might be a better pick. You identify requirements, plan the system, and help with system implementation. If you like breaking things, cyber security is a better fit. Cyber security is more like testing than anything else. You perform a lot of scans, traffic monitoring, intelligence gathering, and source code / binary analysis. Based on that information, you run possible exploits (either off the shelf or custom) to test your hypothesis. Like you said, much less typing than in the movies. Good hackers limit their actions to only what is needed. Many of the tools require time to run so it can be pretty boring in real life.

In either case, I suggest obtaining a computer science degree. This will allow you to perform either role and is therefore more flexible. It is also a more difficult degree to obtain and is therefore more prestigious. I hope this helps.",1515692170.0
generic12345689,One thing I can tell you is that both fields continue to evolve over time. So enjoy learning and doing new things. ,1515697731.0
Helen___Keller,"No point focusing now IMO, just learn more about whatever seems interesting at the moment and align that to your career when you’re out of college.

Both engineering and cybersecurity are fairly vast and have quite some overlap, but at the highest level I would say engineering is about building products and protocols, cybersecurity is about taking those products and protocols apart.",1515710138.0
ktnguy,"Noticed a lot of people explaining the one side of cyber security on here (finding vulnerabilities to create mitigation or more of “IT” and hunting). One of the more fun cyber security fields that’s growing with popularity these days is a vulnerability researcher. (Which is those people you see in movies, but work at a much slower pace, unless you’re some genius. ) They are the people that gets paid a ton of money to just learn the underlining systems and performing static/dynamic reverse engineering. Creating proof of concepts to exploit those vulnerability. Which will be used to eventually mitigate the flawed system. 

The biggest thing I will say that separates between Software Engineering and Cyber Security is that: Engineering you are creating something new (or adding onto it). Cyber Security is more about learning what already exists, become an expert at it and defend/attack the flaws. 

",1515710794.0
NetwerkErrer,Can you major in CS and take classes in each domain and figure it out as you go?  Both are constantly evolving and have a great deal of overlap.,1515716196.0
,[deleted],1515730847.0
xaminmo,"Cybersec definitely needs good people, but there is no reason to choose only one.  So much of security needs to be part of the dev process.  Also, security has more clout in big companies and government, which comes in hand.",1515783778.0
escherfan,"Yep spot on, 16kB is 16 * 1024 = 2^14 = 16,384 bytes which need 14 bits to address.",1515669026.0
hilberteffect,Did you mean to post this in /r/shittyprogramming?,1515655816.0
maxbaroi,It wasn't anything official but I was trying to determine how much alcohol you can consume yet still retain some basic functionality. It failed. ,1515645860.0
chrisgseaton,"I wrote a new programming language with mutable syntax and semantics. This was back in 2006, and people were still citing it in new academic papers as recently as 2015. Not bad for an undergraduate final year project! My work today is still somewhat related to it.

I'd have commented my code more. I read it now and I have no idea what is going on.",1515636520.0
Roachmeister,"I had switched majors from Music Theory and Composition to Computer Science, so I decided to write a program that would create randomized music using music theory rules.

My main issue was that the only language I was proficient at was C++, but I couldn't figure out how to play music in that language. Then I found out that Java had a built-in MIDI API, so I decided ""how hard can it be, Java is like C++, right?"" Not that the project didn't work, sort of, in the end (the music sounded terrible), but learning an entire new language in the middle of a project isn't something I'd do again.

On the plus side, I never touched C++ again, and today I'm employed as a Java Dev.",1515637892.0
Sinujutsu,"We decided to do a network security project. The idea was to use something like wireshark to monitor a small test network created for the project, and attempt to detect intrusion attempts.

About 3-6 months in, when we realized how impossibly large a problem this was, and how difficult it was to get current knowledge on the subject curated all in one place, we turned it into a research paper on the current state of network security, and why intrusion detection is incredibly difficult.

How we would have done it differently was do much more research into the viability of the idea before committing to it. It was fortunate our professor didn't mind us swinging things into a paper on the state of things once we realized how deep we were in over our heads.",1515633271.0
,"I double majored in computer engineering and computer science, so my project had to encompass both majors. An electrical engineering friend and I got dragged into some professor's fantasy project.

We had create a super sensitive microphone that would be placed in the mattress of prematurely born infants. This microphone was supposed to pick up digestive sounds with a frequency of less than 1Hz. Premature infants have trouble digesting food sometimes, so we wanted to listen for digestion to make sure they aren't overfed or just have food sitting in their system doing nothing.

So we built the hardware, one part included a microcontroller that was programmed in C and would take electrical signals from the microphone and amplifier we created and convert those into a digital signal and send those out via RS232. Then I had a Lua script on a PC read that signal and call on a C audio library to generate a .wav file.

It kind of worked in the end. I wouldn't do it again though. Was way more work than what everyone else did. Our school did a showcase for final projects and top 3 projects would get automatic 100% marks even if they didn't bother to finish their reports. We came in tenth... and first place was a group that did their project in one day and thought they were going to fail because it was way more simple than they thought was acceptable.

So yeah, I wouldn't go for the complicated, months-of-work project again. Small and flashy seems to be best.",1515674271.0
teacoder,"Computer graphics.

I would have been much clearer about the scope of the project from the beginning, and at various milestones. One could say that failing to do so led to me no longer being in control of it. Those who would grade my work became in control of defining the project, and the scope of the requirements quickly blew up to something that could not be accomplished within the timeframe. I had to decide between graduating on time, or going very deep down the rabbit hole.

tl;dr: I would have planned better and been more clear about defining the scope.",1515635988.0
browner87,"I built a oscilloscope, with the design parameters of being able to do as many labs as possible from my first years (eventually defined as signals <=1MHz) with a $100 production budget. The idea was to solve the problem of always having to be on campus to do labs that needed an oscilloscope for simple things.

First mistake - I hate working in teams and there are few people I trust/trusted to pull their weight on their half of the project and meet my personal high standards. I convinced my profs to let me do final project solo and they assured me that part of their reason for allowing it was out of curiosity. They wanted to see if I could actually manage it solo whereas most of my class would probably have had trouble even in a team of 3 (we did have some bright students, just not many and mostly in another section). If I did it again, I'd have convinced someone to join me so I could get to a more complete state in the allotted time.

Second mistake - too much research, too little experimenting. Wasted way too much time just trying to read online and learning about how Techtronix would do things, which were obviously well beyond the budget and scope (haha) of the project. I also had virtually zero guidance in my first semester working on it. One prof hated me (I got him in trouble now and then calling him on his bullshit) and the other wasn't very knowledgeable in this exact area of design. Second semester we had 3 more profs added to the oversight roster and they were helpful. If I did the project again, I'd have proactively used my other professors as research resources. I could have moved along much faster with their guidance and suggestions. 

Other than those, I wouldn't change much. With a second person my software interface would have been actually practical, and with more progress early on with better support and more trial and error I think I'd have gotten to a much nicer final circuit board. The #1 takeaway was to make better use of other people, even if you only trust a very small handful. 

There are many more things I would change if I did a second version of the project - I would extend my deadlines so I could save a fortune on PCBs by getting them made in China (ten 4 layer boards for $15 instead of two 2 layer boards for $50), I would step out of my comfort zone with MIPS architecture and Microchip products and go with faster more modern ARM chips, and use something, anything, other than Qt for a gui 😂. But I wouldn't tell myself to do those things for a redo of version 1 in school.",1515640718.0
lubutu,"I wrote an optimising compiler for a second-order term rewriting system.

Term rewriting is sort of like pattern matching in Haskell, except a match can occur anywhere in an ADT, not only at the root. Being second-order means that terms can bind variables, and you can then specify in which subterms a variable may or may not occur. So you can define the untyped lambda calculus (beta- and eta-reduction) like so:

    app(lam(x.M(x)),N) -> M(N)
    lam(x.app(M,x))    -> M

If I were to do it again, I would focus a lot more on automated testing. I was young and foolish, and not in the habit of writing tests, and it came back to bite me — more than once I'd write an optimisation on De Bruijn indices and later discover a bug in some corner case. Oh well, I learnt my lesson.

It can't have gone too badly though, as I went on to do a PhD in the field.",1515658208.0
,"I wrote a physics and rendering engine for use in real time games and simulations. Vulkan API didn't exist back then but if it did I would have used that. I used OpenGL, which is what Vulkan is based off of ",1515652510.0
ORKB94,"My partner and I received full access to the past decade’s worth of meteorological satellite imagery, and used image processing techniques to study  weather characteristics across Africa, investigating the weather patterns that coincided with the Pacific El Niño/La Niña  events over the past decade. 

2015-2016 saw a one of the largest El Niño events on record so far, and Africa experience huge decreases in annual cloud cover over this period suggesting the weather patterns are linked. 

Amazing project to quickly improve large data analysis techniques, with real world application. The topic needs constant studying over the next few decades as we only have had the means to study this phenomena in such detail with the last 10 years since the high quality meteosat satellites were launched back in Dec 2005. 

Doing it again, I would have used a lot more of the data formats available on the EUMETSAT website, there’s tonnes of different image types such as precipitation and surface temperature images which would have been more reliable data types to use for the project. Was also coded in Matlab, which was easy for the numbers side but quite slow and clunky, so a faster language would have been a huge improvement.

Project length: 12 weeks 

",1515661578.0
vangrif,"My project team created a software suite for categorizing architectural/infrastructural defects. It was made for a combination of Google Glass, mobile and web. They would use glass to take an image of the defect which also gave location data. That information would then be sent to the mobile app to finalize the defect, which would be categorized using predefined hierarchical schemas for both the part affected and the type of defect. Once the record was finalized it would be uploaded to the AWS backend. The web app was for essentially sysadmins that would edit the schemas, authorize users, maintain job sites and existing defect records. The web app was necessary because we were using ontologies. I was responsible for the web app. I decided to use ASP.NET because I had 9 years experience with C# for a combination of school, competitions, and jobs. Big mistake, should have been made in angular, or react or basically any framework that didn't have postback hell. ",1515645225.0
kibleh,I made a computer run Linux on one screen and on the other I had windows 7. They worked interchangeably and I could drag programs across from one to the other with a program I built. ,1515701286.0
reitnorF_,"Ontology based knowledge management system ....

Still working on it .-.",1515665622.0
turkish_gold,My final year project was an implementation of futures and promises in Python. If I could do it again I would have actually open sourced my code since Python at the time didn't have an implemention. I came to.the incorrect conclusion that futures were the way forward.,1515694226.0
tkitta,"I wrote a small paper on pseudo-random number generation and use of it in cryptography. It was like 20 years ago, almost.",1516061684.0
parotta36,Linux from scratch.,1516719868.0
RickSagan,**HARDWARE LAYER**,1515631755.0
nit3rid3,Spectre can only be fixed with a new architecture due to its design flaw.,1515634653.0
ReverseEngineered,"The only complete and final solution is to fix the hardware. Problem is, that's really costly. We have to design new architectures, fix existing implementation flaws, spin new silicon, and replace all of the otherwise working hardware that's out there today. How many people will continue using this flawed hardware ten years down the road? There are no shortage of individuals and businesses that continue to rely on old hardware.

So since we can't fix all the hardware overnight, what do we do? We try to catch it at the other levels. Problem is, there are a lot of ways it could sneak through. OS, drivers, system services, even simple javascript in a web browser could be used to execute this family of exploits. So we have to try our best to patch them all and minimize the available attack surface.

Additionally, there's the idea of defense in depth. Perhaps one or two of these attempts doesn't work perfectly. If we only implement fixes in one layer, an attacker only has to find a single exploit at that layer. But if multiple layers are involved, they have to find a combination of exploits at all of those layers to really seal the deal. This is a lot less likely to occur at any one time. This helps isolate and compartmentalize and attacks that are successful.

One day we will design hardware that is immune to these attacks and everyone will have it. Until then, we will have to watch for it vigilantly at every other level.",1515641258.0
IJzerbaard,"What they're doing with the microcode updates is not transparently fixing the fundamental problem, but adding a way in which the kernel can protect itself. They're adding a couple of MSRs that control the prediction of indirect branches and returns. One of them is a barrier that makes the prediction for code after it not depend on code before it (possibly just re-initialized the state of the indirect predictors) and the other is a more selective barrier. The kernel has to actually *use* them for anything to happen.",1515644680.0
ifuckinglovegorillas,"I loved loved loved my first computer science class. I was super intimidated because I had never done ANY programming and I wasn't sure if CS was the major for me. I took the first class and immediately fell in love, and now I know it's the thing I wanna do forever :) enjoy your class! It might be tough but it's so worth it imo. ",1515623287.0
throughdoors,"My first one was a nightmare and scared me off for years. The teacher was totally unavailable for help, the TAs were terrible, I tried copying programs out of the book and they wouldn't compile, it was just a mess and I was missing critical stuff with no idea what it was or how to fix it. I was thinking, well crud, I guess this definitely isn't for me. Years later I took it at another school and had a much better experience -- the teacher was great and I took to it easily.",1515623972.0
nerga,"I made the mistake of taking a programming class in high school that was so bad it turned me off of programming for several years, I am not a software engineer. In university, I loved my classes in computer science.",1515627285.0
phire,"NUMA is typically used on systems which have multiple CPU sockets.

Each socket has it's own memory slots, and memory plugged into those slots are considered local to the CPU plugged into that socket.

There is a high speed bus between each socket, which allows the CPUs to communicate with each other, and share their local memory with the rest of the system. When a core tries to access non-local memory, a message will be sent by the CPU to the CPU in a neighboring socket, and the memory controller in that CPU will access it from its local memory.

NUMA simply means that while a CPU can see all the memory and it has been made to look like one big chunk, the cost of accessing the memory is not uniform with, some regions of memory taking longer to access.

A NUMA aware operating system will try to keep workloads on a CPU core which is close their data in local memory. ",1515622642.0
lurking_bishop,one example would be your DRAM access. Cores on the chipdie that are physically closer to the DRAM controller will have faster reads than cores that are far away because those will need to go through some internal Bus first.,1515619657.0
DarkMaster22,"Just FYI, In reddit-speak that would be [ELI5] for ""Explain-like-I'm-5"". There is a whole subreddit for this.",1515621893.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1515615705.0
IComposeEFlats,"Test it with a smaller range (i.e. no weekends, one weekend) and step through the logic to understand what you're missing.

Friday to Monday has 2 weekends, but only 4 days.

There are going to be edge cases as well, i.e. ranges which start or stop on a Saturday (or Sunday, depending on if you are including start/stop in your calculation or not)",1515599328.0
Singum160278,"I think you left out an important variable, the start day. Because depending which day it starts will determine whether the remainder includes a weekend or not. 
",1515616274.0
zck,"For this kind of thing, it's helpful to (as u/ComposeEFlats said), test your algorithm with cases that you can do by hand.

I'd even take it further, and say that before you start figuring out your algorithm, do the problem by hand, and write down the calculations you do to solve it. Then translate that into a more algorithmic style, and test it. Think about edge cases, but also the happy path.

Also, it's useful to see times that you're thinking as humans do, and not with the precision that computers require. For example, if you want to know how many ""weekends"" there are, what does that mean? You end up returning weekend _days_. So basically, you're looking for how many days between the start and end are Saturday or Sunday. Already, we've gotten to a point that's a little bit clearer than what we started with.

So if I asked you how many weekend days are between 2018-01-12 and 2018-01-15, how would you calculate it? How about (2018-01-13, 2018-01-15)? (2018-01-14, 2018-01-15)? Do it by hand.",1515605668.0
Stopher,So my algorithm would be to cut the ends off the time period to get the weeks even. Calc that at 2* weeks. Then add back on the ends based on which days. Sun = +2 else +1. At the other end sat = +2 else +1. ,1515605046.0
Singum160278,"Did you miss that you are calculating weekend days, not weekends. 
",1515616991.0
mumrah,"Brute force it!

    DateTime dt = startingDate;
    int weekendDays = 0;
    while((dt = dt.plusDays(1)) < endingDate) {
      if(dt.getDayOfWeek() == 6 || dt.getDayOfWeek() == 7) {
        weekendDays += 1;
      }
    }

 (please excuse any syntax errors)
",1515633553.0
hookedonbrooke,"I would add a logical step that tests if your date range has a number of days exactly divisible by 7. If there is a remainder, e.g. your date range is 33 days, then you can assume 2 weekends for 28 of the days and then handle the 5 day remainder how you want. If this were my task, I would add an error calculation and give my answer as 9 +/- 1 (in my 33 day example).

I’m not sure how to be entirely precise without prescribing some constraints to the starting range...",1515600341.0
,[deleted],1515616406.0
rotharius,"In any period of max a week, you have a case in which you pass both weekends, a case in which you pass one and a case that you pass none. 

If the period is one week or longer, you have at least one full weekend. There can still be days remaining.

Combine this knowledge, write test cases, and build your algorithm.",1515621291.0
ProtoHuman73,The horse’s name was Friday,1515629868.0
pythonicus,"So you have the first week, all the weeks in the middle and the last week. The first and last week may not be complete weeks including weekends. You need to determine if the time span from the start date to the end of the first week contains weekends. You need to determine if the time span from the end date to beginning of the last week contains weekends. Add the number of weeks in the middle  + 0-2 based on if the start or end ""weeks"" contain a weekend.

PS - if your week starts with Sunday than the last week always includes a weekend.",1515727652.0
OrangeFoil,"* there still is a *version* field in IPv6 headers. It's the first four bits of the header just like IPv4 so they can be distinguished easily.
* *header length* is no longer necessary because IPv6 headers have a fixed length of 64 bits. IPv4 headers had varying lengths because they could have up to 128 bits of [options](https://en.wikipedia.org/wiki/Internet_Protocol_Options). In IPv6 those are replaced by so called [extension headers](https://en.wikipedia.org/wiki/IPv6_packet#Extension_headers) which are not considered part of the IPv6 header itself.
* IPv6 has a *payload length* field, and since the header has a fixed size a *total length* field would be redundant because header length + payload length = total length.
* *identification* and *fragment offset* are obsolete because there is no fragmentation as there is in IPv4. Instead a sender does a [Path MTU Discovery](https://en.wikipedia.org/wiki/Path_MTU_Discovery) and then makes sure no packet is bigger than the discovered [MTU](https://en.wikipedia.org/wiki/Maximum_transmission_unit).
* the *TTL* field was replaced by *hop limit*, which in practice does exactly the same thing. It's just a more fitting name for what it does.
* IPv6 has a *next header* field that specifies the type of the next header like e.g. TCP or UDP, much like the *protocol* field does in IPv4. The difference is that in IPv6 *next header* might also point to an extension header first, which itself will also have a *next header* field that might indicate more extension headers coming up or a TCP/UDP segment.
* *checksum* was removed because it isn't ~~used anymore~~ really needed. ~~In IPv4 the checksum is neither calculated nor verified~~. Calculating and verifying checksums requires computing power and adds latency, ~~which is why routers typically ignore them anyway~~. Note that TCP and UDP segments have checksums that also take the IPv4/IPv6 headers into account, thus it is still possibly for a receiver to verify the integrity of the data.",1515557547.0
sparks88,"Interesting article on the subject:
http://www.itprotoday.com/management-mobility/whats-new-ipv6-header",1515556854.0
magnificentbop,"OrangeFoil has it right.  The RFCs are a great resource if you're interested in the rationale for changes.  They can be a bit dense, but all the information is there.

The documents for IPV6 have background on the answers you're looking for: https://tools.ietf.org/html/rfc1883

In a nutshell, IPV6 includes improvements based on how the Internet is actually being used, in light of years of observations and stopgap solutions to problems with IPV4.  Longer address space solves problems with running out of IP addresses.  Fragmentation is moved to endpoints to mitigate the unanticipated workload on a hop-by-hop basis.  Checksums are deferred to the transport layer and link layer, as they offer no overall performance benefit being duplicated at the network layer.  Fields that were intended for QoS were updated to flow IDs in anticipation of future traffic control measures.",1515593326.0
WSp71oTXWCZZ0ZI6,"Is this for a BSc undergrad thesis? Honestly, most of those, just pick something vague and kind of interesting. Don't worry about ""has already been explored in great detail"": that's to be expected at the BSc level. About two months in, you'll decide your original plan was stupid and you need to do something totally different, which is why the ""vague"" part of your proposal is so important. I've seen people just do like ""a mobile educational game"" or ""a programming language"" or ""a search engine"" for their BSc project. As long as you're halfway competent, you (or more likely your supervisor) will find a way to stick *something* unique or novel into it. It doesn't have to be Guernica, just a demonstration of your abilities.

If it's a MSc dissertation, a lot of those are just ""I read a paper that had a really good idea and then decided to implement it because nobody else implemented it yet"". Or, if you're more theory based, ""I read and paper and decided to extend it a little bit because nobody else has done that yet"". Has your friend been doing a literature survey?

If it's a PhD dissertation, your friend is fucked and should probably not be doing a PhD.",1515549906.0
add7,"> In 2018, let’s hope we can hold the companies, governments, and people using them to account, without letting the word take the blame.

So, fuck me for using QuickSort right?",1515569517.0
pythonicus,I suspect a Masters in CS + Math/Stat undergrad is a license to print money.,1515538363.0
3lRey,"Absolutely not. Math is for nerds and I won't have it in my workplace.     
      
JK ya you can do CS buddy.",1515533350.0
MCPtz,"Check /r/AskComputerScience or /r/cscareerquestions 

Try a Masters in Computer Science?",1515527241.0
Razark,"The software company I work at probably has 50/50 mathematician and computer scientist ratio (add a few physicist and the like as well). I work with quite a few of the mathematicians in my day-to-day work and you don't feel it. Sure, the start is harder as the mathematicians will mostly have to learn programming from scratch, but it seems like most of them are pretty quick to catch up on whatever they need as they go along.  
My answer would thus probably have to be: that depends on what you want to do. If you want to have an academic career in CS, you should probably get a few CS courses. If you want to work as a software developer or the like, I think you can do without. Theoretical CS is just mathematics, so it should be easy to wrap your head around, and for all the how-to-do-software-things, most companies have their own ways of doing things anyways (at least that my experience). Get the hang of a few of the major languages and you should be good to go.",1515527885.0
zultdush,"Honestly, cs bachelor's  or masters will give you a lot of additional exposure, but if you want to be a software engineer, its about coding. You need three courses for the interviews and they're really all you need to do the job on a daily basis. Everything else you need will be learned by working with senior devs, and coding coding coding :). 

I would take three courses as they're the core of all interviews and will teach you the basics you'll need to do the job:

- Object oriented programming in java or c++
- data structures in java or c++
- algorithm design and analysis, lang agnostic.

I have a biochemistry major cs minor and I focused on these 3 class's and have been working as a soft eng 9mo. If you want a masters to give you more exposure, make your work pay for it like I am :) Good luck and msg me if you want more info...",1515555897.0
evil_burrito,"Yes, for sure.

Speaking as an undergrad CS major/math minor with almost 30 yrs pro experience, you can certainly qualify for CS positions, particularly ones in math-heavy fields (finance, etc).

However, there will be some things you will be better at than others, and I would advise you to pay careful attention to feedback you receive from your CS coworkers about how to improve your code quality.",1515528724.0
_--__,"Pretty much everyone is suggesting that you take a CS Masters, but not really giving adequate reasons why.  While mathematics is closely related to CS, you so far haven't really demonstrated any CS ability, or, more importantly, interest in CS.  You need to keep in mind that your CS friends going off into 80k jobs have done **three years** of CS - think of how much more maths/stats you know than you did three years ago - that's how much further you have to go in CS!  It is going to be difficult to prove you are interested in a CS career with only a couple of CS classes under your belt. That is why you should be looking at a degree.

The good news is that with a mathematical background you will probably do quite well in CS.  But you should definitely look at doing something more than a couple of classes just to make sure it is right for you (and you are right for it).",1515549928.0
VorpalAuroch,"Hello, I have a math undergrad and took some algorithms classes, and I work at Google.

So: Yes.",1515567830.0
Porrick,"I have a math degree and currently work as a programmer.

Although, to be honest, the only part of my degree that was useful was a computer graphics course, because I took it my very last semester before graduating and one of the homeworks was to write a raytracer. My first interview was mostly about raycasting and ray intersection tests, so all that was very fresh in my mind. Once I was on the job, however, nothing I had learned in university was particularly useful. In my first job I used some 3d math and matrix manipulation, but at my current job is mostly scripting and automation. No math required there at all.

I sort of wish I'd done CS instead, but I've been able to have an okayish career as a programmer with my math background.",1515534784.0
Holy_Necromancer,"Every job posting that I've been looking at always entails ""Computer Science degree, or related degree such as engineering, mathematics, biology, etc."" It's as if the job postings prefer applicants to have CS, but would take any other STEM degree. 

There's a bunch of resources of how one can teach themselves computer programming:

https://www.edx.org/

https://www.reddit.com/r/learnprogramming/comments/5gr8nw/heres_a_sanitized_list_of_530_free_online/

I'm not sure if this applies to you, but my state has a messed up law stating that if anyone surpasses 150 credit hours in undergrad (any credit, even failures and withdrawals) they will automatically be charged out of state tuition. If this is the case, you have to develop skills on your own, or pay more for additional undergrad classes.

Also, getting a higher level degree with no real work experience may work against you, since employers may be disinclined to hire someone with a master's degree, when they can instead hire someone with a bachelor's degree for cheap.

Self teach yourself how to code, do some projects, try to get yourself into internships, etc. Good luck.",1515545543.0
nit3rid3,"Well, I'm a math graduate working in software development.",1515530360.0
QuarkzMan,"I can speak to this with personal experience. I was originally a Chemistry major, jumped over to math, and then in the last couple years decided to tack on CS. I never did get around to completing my CS degree (my university forced me to graduate since I had too many units and all the courses required for my Math BS), but managed to get a very well paying job starting out as an SDE. 

Your math background looks really good in terms of the kinds of math that are being used in the CS industry right now. Statistics in particular is a hot field with all of the ML stuff that's going on. As someone who studied Pure Mathematics, I've lost count the number of times that I've told someone in the CS industry I have a Math BS only to have them assume that I knew a ton of statistics. It's gotten to the point where I wished I had taken more stats classes back in the day. 

I think you should put some serious consideration into getting the CS degree if you can, even if the extra time seems daunting. There is a lot more substance to Software Engineering than the surface level of programming, and although I managed to make due, I felt as though I was constantly behind for my first two years in industry. If the extra time really is too much, at least do extra studying or make some quality side projects. 

Either way, the absolute BEST thing that you can do to solidify your chances at starting a career in CS is to get an internship ASAP. An internship in CS not makes for a good paying summer job, it also helps expose you to what the real work is like so you can make a more informed choice. On top of all that, if you get an internship with a big name software company, you can pretty much guarantee getting a job straight out of university. I can't stress the value of an internship enough. It's a bit late in the cycle, but if you start applying now and do extra studying for coding interviews you might be able to get one for this summer. ",1515535450.0
hextree,"Maths undergrad to CS Masters is the path that makes you the strongest Computer Scientist overall, pretty much. With a maths background you will most likely find a lot of the core CS stuff easy and intuitive, especially algorithms and optimisation.

I wouldn't bother doing an undergrad major in it, just do the Masters after. Before working on a thesis, the first part of the Masters is usually just an accelerated review of the whole undergrad syllabus.",1515540977.0
therealhrj,"Yep, I am math undergrad with comp sci minor working as a developer.",1515538418.0
Sinujutsu,"One of the most talented engineers I worked with at Amazon was a biology major. You can absoloutely get a job in CS without a CS degree. I suspect most of what will change is how you get the initial job and get your foot in the door. Most places will be willing to train you a fair amount on what you need to work on, so the question is simply can you teach yourself how to use a programming language you're unfamiliar with? If the answer is yes, you're gtg.",1515543311.0
tkitta,I have major in math & also in computer science and work as a web developer. So I guess the answer is yes.,1515543415.0
The_Anti_Life,"My schools CS degree was a mathematics degree until it got accredited. Now the Comp sci, is an actual engineering degree.

So yeah math major can totally work in comp sci.",1515545746.0
mghoffmann,">I kinda want to avoid that?

Why? If you're only a year away, just do it! That's a *huge* boost to your resume. You'll be overqualified for a lot of jobs, but the jobs you do land will treat you very well.

I'm in my 3rd year of school and might finish one major in another 2 years, if I can get the classes I need. You obviously have an aptitude for learning-use it!",1515546816.0
RyanCacophony,"Look into doing data science, with a strong math/stats background you can get far just learning how to to simple data analysis programming in like python or even R at some places (but do learn more about programming, the more you know the more hireable you are). If you have a decent comprehension for mathematical papers it wont be hard to keep up with data science/machine learning concepts and research.",1515549240.0
_ACompulsiveLiar_,"Most math majors I know go into CS because it's the easiest way to find an application of your major if you don't want academia (unless you know the exact field you're entering like financial mathematics, field-specific analysis, etc.), and the skills generally translate pretty well.

For CS though, experience is king though so you should be looking for internships asap. It's more important than majors, degrees, masters, etc. It's also the best form of education - you will learn how to code properly in a summer internship within a solid team better than you could over 2 years of courses. Target ones that lead into career paths you are interested in, and then self teach what you'll need for those internships.",1515549447.0
MishkaEchoes,"Yes, but you need to learn to communicate as a software developer. If you can be humble and understand the domain. Strong mathematics will eventually give you a strong edge but that takes time. ",1515551158.0
vosper1,"You might consider a bootcamp, there are some that specialise in data science or taking people with good non-CS degrees. I've went to a hiring session for one of them, there were some really impressive people there, physics, math, neuroscience. Can't remember the bootcamp name, but they were in SF.",1515551611.0
throwdemawaaay,"You are in a fantastic position. Your background in applied math is highly relevant to understanding and working with machine learning systems. The demand for this at the moment is incredible. Keep in mind that as astounding as the cutting edge ML stuff is, what companies need most are people who can solidly implement foundational methods like regressions.

Self study is certainly an option for you. The blunt truth is most of the online courses / MOOC's kinda suck. Even if the instructor and material are good, the interactions are somewhat lacking. But I'd still encourage you to look at doing some self study in Python, both to refine some of the analytical skills, as well as to get a bit more comfortable with the less glamorous parts of coding like dealing with file formats, slurping data in and out of a sql database, making a basic interactive web app, etc.

Something a bit more motivating might be looking at kaggle and other ML competitions and taking a shot. Again, keep the mindset that what's in demand is anyone with competence in this stuff. You don't have to be the 1st place rock star. In fact, if you did some demo work that's just competent, and then demonstrated very good communication skills in explaining it to a recruiter I think you'd find them quite interested in you.",1515552300.0
Deathnerd,Yes. My teammate has her PhD in applied mathematics. All of her programming and CS is self-taught. She's one of the best QA people we have. Before she was QA here she was doing something with geophysics and modelling in Austin making bank.,1515553926.0
D0wney,"Dude you can do anything you want if you put your mind to it.

But yeah a Math degree is awesome to have for CS. CS, imo, is basically Math and programming.",1515559401.0
Cazan,"Oh definitely. There have been plenty of people that I've worked with that had no educational background in CS and their focus was on math throughout University. It's definitely do-able. May be a bit tougher to get a bite but do-able.

I'd encourage you to take CS courses and self study. Try the algorithm courses on Coursera, they're tough but you gain a lot of insight missed through not having a formal CS background.",1515562963.0
jmtd,Do the masters . Much easier now than later. ,1515574138.0
Ondrysak,"Understanding data is really valuable thesedays, dont worry you will be fine even though R is not used that much in bussines. Python is. 

Doing a sideproject using the math knowledge you already have and learning some useful technologies as you go could make your decision easier.

Data Science from Scratch: First Principles with Python

Might be a good choice.
",1515574968.0
tankerdudeucsc,"Yes.  But if you can, get a MS in CS, focusing on ML, where more of your math and statistics will come into play.

You’ll be making MUCH more than 80K.  You would be well into the 100K+ range to start.  At the places in the world, you would be pushing closer to a senior engineer salary than a first year grad.
",1515575058.0
random314,"I've worked with successful developers who were

Sound technician

Waiters

Philosophy major

Economy major

Med school drop out

High school drop out

Music Major (there are actually multiple of this)",1515588866.0
bigbirdtoejam,"Your background sounds like you might have interest / skills in the machine learning/data science space.  In some areas, those skills are in high demand and low supply; they can net you incredible compensation packages depending on experience.

Good luck!",1515589893.0
,Yes.  I have a BS in mathematics and had people I know begging me to do programming work for them.  There's massive demand for programmers in the right fields.,1515594357.0
gct,Some of the best programmers I know majored in mathematics.,1515605193.0
silverstrike,"Professional software engineer here in a mid-sized US market.

Academic background is a MS in Comp Sci (Cryptography focus), BS in Math and Comp Sci, and a BA in Classical Literature.

Here's the thing.  Software Engineering is an pretty egalitarian field.  Unless you're trying to get in the door at a major company directly out of school (Google, etc), no one cares what your academic background is in.  Not really.  It's almost like an art field.  It's what you can you DO, what's in your portfolio.  

In my experience, University classes don't really prepare you well for commercial software development.  Almost none of my day-to-day involves anything remotely related to ""computer science"".   It's all programming and software engineering, if you like.  

CS programs do an excellent job preparing you for doing CS research and further education, but it's one of the least vocational degrees I've seen, oddly in one of the most vocational fields.  

A few intro CS courses to grab the basics will be helpful, but a full University is an expensive way to do it (at least here in the US).  

Almost all professional developers I know are self-taught in one fashion or another.  ",1515608269.0
contramonk,If a philosophy major can do it...,1515608498.0
crabbone,"I'm a programmer.  In every place I've worked or interviewed for the fact that the potential employee had any degree mattered very little, the major mattered even less.   In the department I work today two out of six have CS degrees.  The head of the department started his career in QA and become interested in programming later. He worked for Microsoft before he was hired by this company.

I've also interviewed candidates. Even though I don't have a lot of experience doing that, I can tell you that the value of your degree for the employer is basically a kind of warranty that you had enough of self discipline / cow-ish obedience (however you look at it) to put up with tons of nonsense and to jump through hoops for a few years until you got the certificate.  This is valuable because employer is subtly aware that most of what you will do on your job can also be described as tons of nonsense.  What you study for CS degree has very little to do with what you will be doing as a programmer.  In many cases that's not just little, that's nothing at all.  It may only matter if your future employer is a university, in which case they'd expect you to play the same game of nonsense and pretension like the one you played before graduation :)",1515768906.0
Unsounded,"I think the Masters Degree is your best route. To be completely honest you'll learn a lot more and it will look far better than having three bachelors degrees. A lot of the time no one will actually give a fuck that you have two bachelors, one masters degree is all they'll see and you'll be in. 



Maybe I'm biased as I'm a grad student about to graduate with my masters in CS, but job opportunities are abundant and I have a lot of choice. I also have learned more in the last year and a half taking grad level courses and doing research than I did in 4.5 years of undergrad. I switched my major late in the game and made the last few semester really count, I felt like I didn't get enough actual programming/comp sci experience to feel like I was done with school. Rushing to finish will leave you with a foul taste in your mouth if you're anything like me.



With your math background you'll be able to pick up the advanced computer science theory courses quickly, and I always find the application courses easy to jump into without much previous experience. The pre-requisite courses will set you up nicely and in my opinion are your best bet moving forward.



If at all possible find funded masters programs, I paid a semester of my masters out of pocket but the rest I made money on while working part time. There's plenty of funded programs, especially in computer science, so I would look around for those.",1515863876.0
algorithmgeek,"I feel like you would be better off getting a Masters in CS than getting another B.S. degree.  I have a Masters in math and am working on a PhD in CS at the moment.  There are definitely some things people with CS backgrounds do better than me, like coding.  However, I am much better with theory than they are.",1515536833.0
snot3353,"If you're willing to put in some effort on your own to learn software development and can demonstrate competency and knowledge during interviews, very few employers are going to care that you have a math degree instead of a CS degree. You will need to make up for the knowledge you won't be getting from classes on your own however, at least to some extent. Intro level and junior devs are usually going to be interviewed with some CS101 style questions around data structures, algorithms, what they've done for projects on their own or in school, etc.",1515528441.0
generic12345689,I know a few math/comp sci double majors. Every single one is gainfully employed. ,1515528885.0
int-elligence,"Check out this video by Sundar Pichai(Current CEO of Google). He says ""It is easier to train a a good mathematician in the field of computer science needed to be an expert in the field of machine learning than it is the other way round"".
So if you are a good mathematician and Google your aim, I think you are good to go.

The question is asked at [32:40](https://www.youtube.com/watch?v=x77DCtbjCjY)",1515543648.0
ZabulonNW,"I don't know, can you?",1515568384.0
AlexHumva,"Fort Hays State University in Kansas has an excellent, accredited program and affordable tuition. There's discounts for neighboring states and even the fill out of state tuition is less than most university's in state tuition. They currently offer a track for networking and a track for management, geared towards you getting an MBA afterwards. Its main benefit is that it is a State University and so carries that certain level of legitimacy that other purely online schools have trouble getting.",1515474765.0
ogre14t,"Franklin University has a good program.

https://www.franklin.edu/degrees/bachelors/computer-science",1515518619.0
rosulek,"If you have an existing BS, look at Oregon State's online post-bac degree.",1515523641.0
Conpen,I always feel hopeless with these advertised scholarships. I go to a top 30 University with a 3.7 but I feel like there are tens of thousands of kids better than me that would get these before me.,1515479422.0
PapaOscar90,Scholarships are so racist. There are none for white males only. So much discrimination. 🤔,1515512168.0
andrewcooke,"at a very simple level (sorry if this is too simple) computers work with numbers, but people work with words.  so a programmer writes a program in words and the compiler changes those words into numbers the computer can understand.  those numbers are the program that you download (i guess - i am not sure why you mention downloading so i may be misunderstanding something).",1515455279.0
timmyotc,/r/learnprogramming,1515455773.0
beknowly,Download at the end of what?,1515456410.0
wal9000,"The computer processor speaks a ""low level"" computer language called ""assembly."" There are different versions of this for different types of processors - for example, x86-64 assembly is what runs on most desktop computer processors, while ARM assembly is running on your phone.

Each assembly language has instructions to do things like add and multiple numbers, move data in memory, and you put those operations together to make useful software.

It's not the easiest thing to write though, and if you wanted something that works on your computer *and* on your phone you'd have to write it separately for each one. So instead, you use a higher level language like C.

But your processor doesn't understand C code. It only understands assembly. That's what the compiler is for. It's like a translator that takes one type of code and whatever you told it to do, figures out how to do that in the target language. So say you're making a game, the compiler will take the C code you wrote and it turns it into assembly instructions targeting a particular type of processor. The output of a compiler is the executable code that your computer knows how to run, like an EXE file on Windows. Similarly the compiler could take the same C code and compile it to run on your phone.

Both versions of the program would be based on the same code you wrote, but the desktop and the phone don't speak the same language so you use the compiler to translate separate versions it for each of them.

Compilers can also try to ""optimize"" the resulting code to make it run as fast as possible, but that part is pretty complicated.",1515455841.0
,"I apologize for my complete ignorance of the subject.  By the ""download at the end"" part, I meant like the finished product.  Like the zip file you get, or the app you purchase from the app store.

How does it go from compiler to file?  Is that a better way of phrasing the question?",1515461300.0
DSrcl,"Compilers (in the strictest sense at least) don't produce executable but object files (or assembly... but whatever), from which the linker gives you the executable.",1515471622.0
justworkingmovealong,"I'll give you one tip - no matter how much you do, you're always scratching the surface. Even after you finish school, there's always something else to learn. A new language. A new framework. A new architecture. A new application of existing technology. Legacy code / applications that need to stay alive. That's normal, and expected. 

As for what aspect to work in - dabble. You're early enough in your education that you NEED to try different things in different areas, and find out what kinds of things you like. 

For me, I found that I hated lower-level languages where you deal with actual bits and bytes. Assembly languages and C are simply not for me. And I learned that by taking classes in C and Assembly. I also found that I don't like trying to keep up with the always new front-end frameworks (there's always something new someone says will be the next big thing). Sometimes it's more about figuring out what you DON'T want to do than what you DO want to do. So pick something, try it out, and maybe run with it for a little while. Worst case, you got a different perspective on something new. ",1515446045.0
spoonraker,"I had the same experience starting out. It's overwhelming. I performed badly in college, mostly due to having a poor work ethic which caused me to just freeze when faced with the overwhelming challenge of programming. My grades were so bad I got put on academic probation and wound up dropping out in the middle of my sophomore year and never finishing my degree. Despite that, I still found a job as a professional programmer.

I've now been doing this full time for over 10 years. I still feel like an imposter. I still feel like a beginner. I still feel like all my peers are better than me.

But yet... I've been employed for 10 years. I get great reviews. I do public speaking at tech conferences sometimes. I mentor beginner developers. I'm on the hiring team. My current job hired me as a full time remote employee which means I impressed them enough to get hired in a pool of hundreds of applicants from all over the US. They pay me a ton of money and the benefits are incredible. 

The point is, the feeling of imposter syndrome never really goes away. What you're feeling is normal.

The trick is to just embrace the challenge. Working as a programmer is being a full time learner. That's just the nature of the job. If you're always learning that doesn't mean you're behind. We're all always learning. Even the most senior employees. 

Give it time before you give it up. Unless you hate the actual work, there is so much up side to programming that it's worth the pain of getting started. This is a skill with incredible demand and value. You'll be setting yourself up for one of the best possible career paths by sticking with it.

Feel free to ask any questions you may have. I can't imagine a better career choice. ",1515464517.0
JustinsWorking,"Just for the record, I've worked in finance & the video game industry. I now work for myself doing contract work and working on bootstrapping an indie game in my free time.

Professionally my reviews from employers have been very positive about my ability to program.

In university my first year I couldn't complete a single lab without help from another student, and in my second year I felt like I struggled the whole time. I figured was hopelessly behind other students.

I didn't land a single job in programming until after graduation, but one summer I did make a flash game that got me a couple hundred bucks.

Don't get discouraged, and try to find something fun to program, even if its just playing more with a school project. Its entirely about practice and perseverance and just because you're worse at pretending things are easy doesn't mean you're far behind. My first job was at a bank making websites, the only experience I had was that Java was a lot like C# (and we used it for a project once in 4th year) and that I had technically made a web page for a school project (using Java servlets, which is completely unrelated to anything.)

Just look for opportunity, take it, and keep looking forward.",1515447682.0
zerribert,"Just keep at it, it will get better over time. I would go as far as to say that it doesn't really matter what branch of computer science you focus on, at least not during the very beginning of your career. After a while you start to see common patterns in fields that look totally unrelated to each other on the surface. Even if you decide it's not for you after a few months, you will almost always learn something from your ""failure"" which will be useful somewhere else. After a few years this will ultimately lead to three things:

1) you will be able to pick up new concepts much more quickly

2) you will be able to solve problems from domain A with solutions similar to or derived from domain B you dabbled in a few months back

3) you will start to see the big picture and stop worrying, and after having tried 5-10 different things you will most likely know where your strengths and passions are. 

THEN you can start specializing really hard on that one thing if that's what you wanna do. Or just keep going and become a jack of all trades.

Don't give up, just go with the flow for a while. Don't be afraid to ask questions (especially to peers), and don't pressure yourself. I've been at it for 10+ years now and I still feel like you some days, but I learned not to sweat it. Use your curiosity and hunger for knowledge to fuel your motivation, don't get too worked up with the fact that there's more stuff out there than you could possibly learn during a single lifetime! Just keep going, bit by bit, one at a time. You'll figure it out... Good luck!",1515447720.0
papa420,"Literally every dev feels like this at times but you just have to move forward. You aren't as small as you might feel right now

If you're worried about your future, start to expand your skillset and do projects you enjoy in your free time. Know any web stuff? Learn it. Know any app stuff? Learn it. Know any graphics stuff? Learn it. If you show you have a skillset and are eager to learn, you'll get calls back from employers

BTW, I didn't get an internship until senior year so don't sweat it if you don't get something soon",1515456504.0
,"Congrats for sticking with it two years so far! Really, think about how much you've learned in that time.

We all didn't know things at one point and time, though. Staying curious and interested (and yes, actually making stuff) is what counts and how you can refine your work.

Also, try to not worry about planning where you'll end up. Before you graduate, there will be new kinds of positions being created for the changing market.",1515510831.0
weeeeelaaaaaah,"Yeah, don't worry too much about Data Structures and Algorithms. Some of that stuff is helpful, and it's all good to know, but it's the ""Science"" part of Computer Science. Very interesting if well taught, but largely not practical.

I say play around. It's where most great programmers got their start - just try to make something you think is neat and ask for help along the way.

I'd recommend starting with Javascript ASAP - others are free to quibble about the quality of the language itself, but it's easy to get started (pretty similar to Java), you already have a complete REPL/Interperter/Framework right in front of you (Chrome Dev Tools is AWESOME), and let me tell you: we can't hire JS devs fast enough *because they don't teach it in schools yet!*

But my #1 reason to play around with Javascript and web development is you can make stuff you can see! Binary trees are cool, but all the command-line stuff they have you do in school is just so boring. It's hard to get excited when the only outcome of your work is text and more text!!!

If you're really visual, look into canvas. It's part of HTML and Javascript that will let you just draw on the screen with code, which I always find fun. Just start with a loop, draw some lines and circles, change the colors, see what crazy thing shows up. It's like Minecraft, if you really want to get invested you have to make up your own goals, even if the end result is pointless and stupid to everyone but you. 

If you can make programming a hobby, something you at least kind of enjoy doing for fun - no matter how pointless it is - you're on the right path.

IMHO. YMMV. Have fun!",1515447572.0
TrumpetSC2,"So a common technique for large particle simulations is to not model individual bodies (N-body) but to use statistics to determine the amount of particles in you grid (Particle in Cell). For collisions, you define the momentum field at every cell and then update the overall grid based on your physics. There are no actual calculated collisions but the vector sums of momentum flow and the proper application of conservation laws leads to a realistic overall simulation.

If you are doing N-body (works for PIc too tho) you can also take your physics ( called a kernal ) and apply multipol expansions so that you aren’t doing an n^2 algorithm. This will DRAMATICALLY improvr efficiency of particle to particle relationships (magnetic fields, gravity, etc.)",1515446494.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1515424433.0
FieryPhoenix7,Shouldn't this be in /r/cscareerquestions?,1515430427.0
black_rifles__matter,Wrong sub,1515436167.0
hundredPercent-beef,Just what this sub's been lacking lately: Udemy spam!,1515421410.0
hundredPercent-beef,"Oh look, the Udemy spammer is back! And now he's branched out into... video game sales!?",1515419233.0
DutchDudeWCD,"A weird article. The author writes stuff like

> The steps taken to trigger the Meltdown vulnerability seem probable. 

While there are proof of concept exploits.",1515400553.0
DonaldPShimoda,"Programs like this can go by a few names, but I believe the most common is to simply call it a ""BS/MS program"" which is what my school (Utah) does. These programs do exist and are becoming more popular, but they generally follow a 5-year plan for the two degrees. These programs in general are not ""typical"", and it is certainly not ""typical"" to do it all in 4 years as you have mentioned.

My understanding is that degrees are handled differently Stateside than they are across the pond. In general, our BS degrees are 4 years, MS is 2-3 years, and PhD is 4-7 years (depending on a number of factors).

Hope this helps!",1515358059.0
boottrax,"I’ve seen it done in the US at a public university.  That said, you might consider a different game plan depending on your finances and well being.

What you are prescribing is very rigorous and will require you to have all requisite classes all line up perfectly. Some universities only offer a required course every other semester or some such thing. Additionally, if you want a particular course or elective because you are genuinely interested you may have to forgo it.

A plan that works well is to get a BS in 4 years and then get your employer to pay for your grad studies.  This eases the schedule burden and burnout and will significantly cut your financial debt (if indeed you are seeking loans).

Regards",1515361537.0
iwantashinyunicorn,"The UK thing started with a quirk of how tuition fees in Scotland are paid. Scottish students are allowed up to five years of free tuition, to cover degrees like medicine, but it only applies to the first undergraduate degree taken. A Scottish undergrad honours degree is four years (with the first year being potentially skippable), so to allow students to take advantage of as much free education as possible, the ""undergraduate masters"" year was invented. This was then copied by some English universities, to be added onto the end of their three year undergraduate degrees, even though the tuition rules are different there.

To be honest, I'd say that if money isn't an issue for you, it's better to move around for your masters. The integrated programs are only beneficial if they're saving you a hefty amount of money.",1515365271.0
Ader_anhilator,"r/humblebrag
r/iamverysmart

These subs can answer your question",1515357906.0
Iroastu,"There are some programs that let you do it in 5. I think 4 would be very difficult and it certainly would require a lot of courses each semester, as well as summer courses. ",1515358642.0
HeraclitusZ,"I'm sure it isn't available everywhere, but it certainly isn't uncommon. I know both UPenn and Cornell offer programs like that. ",1515359200.0
sgt_leprachaun94,"Pretty sure the uk is one of the only places that a four year masters is possible. In Ireland It’s 4 years for a level 8 bs honours degree, and an add on year for your masters. ",1515368083.0
Darkfeign,"It mentions Judea Pearl's NIPS talk at the end, but doesn't mention that apparently it was a pretty poor turnout compared to the rest of the speakers. A disappointing observation given how much he has contributed to the field of AI. While nobody should really be claiming that DL is doomed, it's always worth listening to those with greater experience when they try and point out flaws or areas of concern. Hinton has been doing the same lately.",1515365929.0
,"Really a great little read.  I'd love to see more movements towards more general purpose reuse ala ""differentiable programming""",1515366979.0
BobFloss,Old news,1515380646.0
ryani,"The statement at the end about dropping non-dominant variables is misleading, especially given the constraints shown:

    3 <= n <= 10^7
    1 <= m <= 2 * 10^5

The author then goes on to make the argument that since `m` is so much smaller than `n` that his `O(n+m)` algorithm can be considered `O(n)`.  But he hasn't shown any dependent constraint between `n` and `m`; if you fix `n` to a small value and increase `m`, the algorithms running time changes, so it's clearly `O(n+m)`.  (Although, if `m <= n` for all inputs then that analysis makes sense)

His second argument is that since `m` is bounded by 2 * 10^5, you can treat it as O(1).  But bounding the input sizes breaks big-O analysis.  All terminating algorithms are O(1) if you fix their input to within a certain bound.  It's not useful to say that an x86-32 implementation of C's `qsort` is O(1) because the input array can't be larger than 0xffffffff.  Big-O analysis is predicated on the idea that the size of the input can be chosen to be any arbitrary value.",1515269763.0
dantuba,"Summary: when most people say big-O, they mean ""the tightest big-O bound I can get"". That's usually good enough to give a rough estimate of the growth rate, hence why it's used so frequently. But I agree it's useful to be aware of the technical meaning of terms that you use.

I know *some* good data structures classes in university make all this very clear, but unfortunately it seems that many people don't have that experience.",1515266403.0
louiswins,"I liked the article! I liked how it pointed out the difference between worst-case and best-case analysis, which most introductions to big O (and, in my experience, most CS courses) don't cover. It would have been nice to expand a bit on the bubble-sort example - *why* it's O(n^(2)) in the worst case, and what it is and why in the best or average case.

One nitpick - on the big Θ graph, there's no way that c1·g(n) and c2·g(n) could be shaped like that. (For example, they can never cross except when g(n) = 0, and at every point either both should be increasing or both decreasing since their derivatives always have the same sign.) But the graph is still essentially correct.

_edit: fixed typo_",1515263163.0
paithanq,"There's a historical reason we use Big-O to describe algorithm running time: the existence of a Theta(f(n)) algorithm means that the problem it solves is in O(f(n)).  Perhaps there is a faster way to solve the problem, but we at least know that f(n) is an upper bound.

The reason Big-O continues to be used in place of Big-Theta is probably because most people assume you're talking about the tightest (known) case and that's fine for all but the most rigorous situations.",1515286104.0
R0VYWklS,"http://i68.tinypic.com/34ex16v.png
I kept seeing this [Math Process Error] thing everywhere and I thought this is some way to express that there is unnecessary amount of math involved in this. Waited for the ""real"" explanation ",1515260789.0
DebuggingPanda,Being pedantic here: the latex code for O(n log n) should be `$\mathcal O(n \log n)$` to make the `log` non-italic and stuff.,1515343747.0
destiny_functional,">What’s less obvious is that answers such as O(n³), O(2^(n)) and even O(n!) are also correct

Not for any math student who's taken at least the first week of numerical analysis. Maybe for a cs student.",1515289623.0
inconspicuous_male,It would be neat if the tones corresponded to notes on a pentatonic scale,1515264418.0
R0VYWklS,"This is mesmerizing like a dijital drug. If we were in the matrix this would be illegal.
http://i67.tinypic.com/2ex6gxw.jpg",1515271436.0
Zainy924,So many dick jokes here,1515217953.0
flexibeast,"Full abstract:

> Intensionality is a phenomenon that occurs in logic and computation. In the most general sense, a function is intensional if it operates at a level finer than (extensional) equality. This is a familiar setting for computer scientists, who often study different programs or processes that are interchangeable, i.e. extensionally equal, even though they are not implemented in the same way, so intensionally distinct. Concomitant with intensionality is the phenomenon of intensional recursion, which refers to the ability of a program to have access to its own code. In computability theory, intensional recursion is enabled by Kleene's Second Recursion Theorem. This thesis is concerned with the crafting of a logical toolkit through which these phenomena can be studied. Our main contribution is a framework in which mathematical and computational constructions can be considered either extensionally, i.e. as abstract values, or intensionally, i.e. as fine-grained descriptions of their construction. Once this is achieved, it may be used to analyse intensional recursion.",1515202078.0
carette,"It is kind of disappointing that the author seems to have missed all of [William M. Farmer](http://dblp.uni-trier.de/pers/hd/f/Farmer:William_M=)'s work on [Chiron](https://arxiv.org/abs/1305.6206); a lot of his recent work is all about finding ways to deal with quotation and evaluation (aka not just intensionality, but how to get back to the extensional world as well).",1515333107.0
zhrusk,"If you can manage this (High upfront cost):

I was part of a youth compsci outreach program in college. We would go out and talk about being a programmer to kids, and all the cool things you could do.

The big draw for almost every classroom was the robots. We had robots that were basically wheels with a laptop on it. We had built a java library that controlled the robots, and also let them say things with a text to speech library. We would talk about how robots/computers are everywhere (In your cars, on your desk, in your pockets, in your TV, EVERYWHERE), and how programmers are the people who can tell these robots what to do and how to do it. We then get into the idea that robots are really literal, and will only do exactly what they're told, no more, no less. We would split the kids up into groups of 3-4 with one robot and one tutor, and the tutor would guide the kids through how to make the robot drive in a square, and say things (usually this also introduced the idea of debugging, and the kids initial program would almost never work the first time). Then we asked the kids what they wanted to make the robot do, and guide them through how to have it do that.

The way their eyes lit up when they realized that THEY controlled the robot was magical. And being kids, the thing they usually wanted to do was have the robot drive up to another group, say something scandalous (""YOU ARE POOP HA HA""). and trundle back.

If you're strapped for cash/time, there's always the beginners programming languages out there that let kids program simple games. I'll let someone with more experience in that area give good examples.",1515193046.0
cerberus6320,"Simple logic puzzles are usually a good introduction to that sort of thing.

One logic puzzle that is pretty good is the weighted balls puzzle.

___________
#weighted balls puzzle
You need:

* 1 balance scale 

* 9 balls of similar size. (one of these balls needs to be either heavier or lighter).

1. each student will be give the task of figuring out which ball weighs differently in the shortest amount of moves (not time).
2. a move is counted as the student measuring a unique set of balls on the scale.
3. Students who can figure it out within 5 moves gets a piece of candy. Any less moves and they get triple.

____________",1515194045.0
boneillhawk,"Check out [CS Unplugged](http://csunplugged.org). They have a lot of good CS activities for elementary school students, and all on paper.",1515211463.0
afiefh,"Whatever you do, you have to use the wizard analogy: wizard speak commands to the world and the world changes to their wish. Non-wizards don't know the language to speak these commands and thus can only use magic written for them by wizards. Programming is literally putting magic words into a computer and seeing the effect they have on the machine, on the internet, or in case you have some mechanical parts, on the world. Programmers are modern day wizards.

(I take no credit for this analogy, saw it on reddit a few years ago)",1515219494.0
sukhmeets,"My team and I have been working with local schools and teachers to develop kid friendly CS curriculum and an easy to use app to build student confidence. We have a free version of our app and an ebook for teachers if you want to have a little hour of code event where the kids program art on to a grid. They can download what they made and print it out or save it to there Google drive. 

It's pretty doable if the school has a Chromecart that you can use during the event.

E-book: https://codenaturally.com/ebook-intro-programming/
Web app: https://app.codenaturally.com/try-it-out


I'd be happy to help get licenses and access to additional lessons if you'd like. PM me if I can answer any questions and it's awesome of you to take part in these events. Makes a huge difference for kids at that age.

Good luck and thanks for reading. Feedback is encouraged.
",1515197829.0
squidgyhead,"This is from a mathie perspective, but game theory is fun.  Get them to play hex, nim, and cirle-nim.  These are provably winnable games, and hex is provably not a stalemate, but it's not obvious - you can offer a prize (say a chocolate bar) for the first tied game and watch them go crazy.

Another nice thing is doing modulo arithmetic; see if they can figure out what makes zero-divizors happen.",1515215775.0
voronaam,"I bought a Cubetto for my 4 years old (who is now almost 6) and we had a decent amount of fun with it. Just solving trivial puzzles like ""collect those 3 gems and bring them home) on those grid-like carpets were fun.

Nothing too complicated about it, you may be able to do something similar fairly quick. The best thing for me was that there is no screen at all. Somehow it is way more appealing to kids when they can touch and bite things, not just rearrange blocks on screen.",1515197006.0
eJollyRoger,newspaper towers,1515214560.0
Mentioned_Videos,"Videos in this thread:

[Watch Playlist &#9654;](http://subtletv.com/_r7oey6h?feature=playlist&nline=1)

VIDEO|COMMENT
-|-
(1) [10 Amazing Science Tricks Using Liquid!](http://www.youtube.com/watch?v=HQx5Be9g16U) (2) [Spinning Wheel on Spinning Chair](http://www.youtube.com/watch?v=5cRb0xvPJ2M) (3) [Gyroscopic Precession](http://www.youtube.com/watch?v=ty9QSiVC2g0) (4) [Moment of Inertia Demo](http://www.youtube.com/watch?v=m9weJfoW5J0) (5) [Bruce reduces his moment of inertia on a swivel chair](http://www.youtube.com/watch?v=o_w3oZsoxrg) (6) [Wheel momentum Walter Lewin.wmv](http://www.youtube.com/watch?v=NeXIV-wMVUk) (7) [Gravity Visualized](http://www.youtube.com/watch?v=MTY1Kje0yLg) (8) [Anti-Gravity Wheel?](http://www.youtube.com/watch?v=GeyDf4ooPdo) (9) [EASY Pinewood Derby Car WINS using Science!!!](http://www.youtube.com/watch?v=-RjJtO51ykY)|[+1](https://www.reddit.com/r/compsci/comments/7oey6h/_/ds9d9bc?context=10#ds9d9bc) - There are a few videos of chemistry and physics demonstrations that the kids may like on youtube.  This one has relatively simple but really cool experiments using fluid mechanics This Youtube channel does really cool things with engineering and has ...
[AnnMarie Thomas: Hands-on science with squishy circuits](http://www.youtube.com/watch?v=5M3Dow20KlM)|[+1](https://www.reddit.com/r/compsci/comments/7oey6h/_/ds9idn0?context=10#ds9idn0) - Squishy Circuits
I'm a bot working hard to help Redditors find related videos to watch. I'll keep this updated as long as I can.
***
[Play All](http://subtletv.com/_r7oey6h?feature=playlist&ftrlnk=1) | [Info](https://np.reddit.com/r/SubtleTV/wiki/mentioned_videos) | Get me on [Chrome](https://chrome.google.com/webstore/detail/mentioned-videos-for-redd/fiimkmdalmgffhibfdjnhljpnigcmohf) / [Firefox](https://addons.mozilla.org/en-US/firefox/addon/mentioned-videos-for-reddit)",1515216153.0
gregoriohombre,Gödel's Incompleteness Theorems,1515222942.0
n0v3n,Kerbal Space Program,1515228985.0
barsoap,"In the talk SPJ gave about the new English CS curriculum there was a video of kids parallel-sorting themselves using a criss-cross pattern drawn on the ground, you might want to try that. You can then ask such questions like ""Would it also have worked if you started out in a different order"" and such.",1515230424.0
Ugly-Panda,"What about making games on scratch? Scratch is online and it's free, you don't really have to program but it uses basic logic and it's pretty neat",1515239191.0
nobigdiehlll,"Scratch! It’s a great website for kids that age, check it out if you haven’t already :)",1515253786.0
eXTeeGi,Scratch.mit.edu ,1515638437.0
dbenoit,"We have an old LEGO NXT Robot (and a new EV3) that solve the Rubix Cube. You get the robot factor, then you can talk about the differences between how a computer solves the problem vs how a human solves the problem. Our robot is fairly slow (90 seconds), so there is time to explain while it is working. It is also useful to have a race again the robot, so people can watch both the robot and a person solve the cube at the same time.
",1515199133.0
kswagerman,Try this website https://mathforlove.com/ it has heaps of great maths ideas and games like squareable numbers https://wordplay.blogs.nytimes.com/2013/04/08/squareable/  Dan Finkel is amazing saw him at a recent PD,1515199333.0
ReMiiX,"I helped with a similar thing. The two things that were the biggest hits were robots (specifically we had scribbler robots) and logic gates.

For the robots I just had one or two students helping the kids learn the basics functions and then they could do small tasks. For the logic gates, we had a few big breadboards set up and explained what logic gates were and how they worked. Then the kids could play around with a few and try to get weird outputs. Depending on their age you can give them a random more complex gate and see if they can determine all the components.",1515204370.0
Night_Eye,"- There are a few videos of chemistry and physics demonstrations that the kids may like on youtube.  [This one has relatively simple but really cool experiments using fluid mechanics](https://www.youtube.com/watch?v=HQx5Be9g16U)

- This [Youtube channel](https://www.youtube.com/channel/UCY1kMZp36IQSyNx_9h4mpCg) does really cool things with engineering and has a ton of videos to choose from - maybe watch some for inspriation. (rather large budget for some of them)

- You could also do some standard physics/engineering demonstrations. Some examples: [Conservation of angular momentum](https://www.youtube.com/watch?v=5cRb0xvPJ2M)  , [Gyroscopic Precession](https://www.youtube.com/watch?v=ty9QSiVC2g0), [Moment of inertia](https://www.youtube.com/watch?v=m9weJfoW5J0), [Moment of inertia 2](https://www.youtube.com/watch?v=o_w3oZsoxrg)

- [This guy is a legendary physics lecturer - here's another version of the Gyroscopic Precession example](https://www.youtube.com/watch?v=NeXIV-wMVUk)

- [interesting visualization of Gravity](https://www.youtube.com/watch?v=MTY1Kje0yLg)

- [More gyroscopic precession](https://www.youtube.com/watch?v=GeyDf4ooPdo) - slightly dangerous, but you could play the video - might need to ask video creator for permission

- Depending on budget, you may be able to get teams of kids to design their own simple cars using these principles: [Pinewood derby car](https://www.youtube.com/watch?v=-RjJtO51ykY)

Of course, all of these are more fun if you can come up with simple, easy to understand, explanations for them.

I'll keep adding more ideas as i think of them.

I don't really know your budget or what materials/resources you have available - how big/flashy do you want/need it to be?",1515208680.0
wookiecontrol,Minecraft ,1515210460.0
Roller_ball,[Squishy Circuits](https://www.youtube.com/watch?v=5M3Dow20KlM),1515215977.0
MCPtz,Are there any educational apps for a tablet that teach programming fundamentals through interactive games or puzzles?,1515217217.0
ranchgod,Programming a simple animation can hold kids attentiom for hours. I teach 3rd-8th graders p5.js and there is nothing like seeing their face light up when you hit run and they see something pop up on the screen.,1515224490.0
cognificent,Use cheat engine to mess with Minecraft,1515229272.0
fragmen52,"I've had several booths on 3d printing at maker faires, if it's taught me one thing it's that kids love the diagnostic view for the kinect.  Noone really understood the kinect was for 3d scanning though. ",1515231443.0
thisisbutaname,You could use soap bubbles to explain minimum sufaces: https://math.berkeley.edu/~sethian/2006/Applications/MinimalSurfaces/minimal.html,1515246049.0
danhakimi,"I feel like a node server they can modify and then access from their phone browser would be cool. But you might have to teach them a lot for relatively little payoff... Depending on what ui framework you use and all that. If you can afford a robot, that's probably better.",1515246607.0
desi_ninja,Show making basic games in Unity 3D. They will love it and video games are a big draw,1515352636.0
andrewcooke,the impression you have taken from your teacher is obviously wrong.  it has to be on the server.  it even says in the link in the last para that the other objects have no remote interface.,1515191341.0
ImaginationGeek,"It sounds to me like an optimization problem.  The metric you’re trying to minimize (optimize) is “error” ...and it sounds like you already have a fair idea of how to quantify the “error”  ;)

I would guess you’d get a pretty good answer for this from a simple gradient descent.  (It doesn’t sound to me like there would be a lot of local minima that are far off from the global minima - but I’m just making an educated guess at that!). Maybe there’s some way to make an analogy to a many-dimensional linear regression and solve it that way, but I don’t quite see it off the top of my head...

Is this for a class?  If we know what class, or at least what level of class, that will help use judge at least how complex a solution might be expected.  :)
",1515188564.0
andrewcooke,"[Force directed graph drawing](https://en.wikipedia.org/wiki/Force-directed_graph_drawing)

> An alternative model considers a spring-like force for every pair of nodes ( i , j ) where the ideal length δ i j of each spring is proportional to the graph-theoretic distance between nodes i and j, without using a separate repulsive force. Minimizing the difference (usually the squared difference) between Euclidean and ideal distances between nodes is then equivalent to a metric multidimensional scaling problem.

think of the items as being connected by springs that ideally ""want"" to be the correct distance.  as you say, it's unlikely that all will work out exactly, so they will push each other around and find some kind of stable compromise.",1515189176.0
ZenEngineer,"It's called Multidimensional Scaling in some circles. 

There are linear algebra based methods (related to eigenvalues/vectors) and iterative methods. R has a bunch of packages to solve it in different ways.",1515195815.0
SimpleLegend,"Try looking up KD Trees, not sure if that solves the problem",1515210628.0
nerdshark,"That sounds like an optimization problem, which would fall under calculus.",1515188008.0
theantigod,"assume the coordinates: 

position 1: x1, y1, z1

position 2: x2, y2, z2

The vector between these two positions is:

if ( x1 > x2 ) x3 = x1 - x2
else x3 = x2 - x1

if ( y1 > y2 ) y3 = y1 - y2 
else y3 = y2 - y1

if ( z1 > z2 ) z3 = z1 - z2 
else z3 = z2 - z1

The distance (length) of the vector is:

square root of ( ( x3 * x3 ) + ( y3 * y3 ) + ( z3 * z3 ) )

Just get rid of the z's if you want 2D.

Choose what ever units you want.



",1515188720.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1515146349.0
SwordInALake,"Just posted this over at r/CSeducation but if anyone has any insights into good computer science experiments for young children I'd appreciate some brainstorming help.
https://www.reddit.com/r/CSEducation/comments/7ok3md/good_portable_computer_science_demos/?utm_source=reddit-android",1515255527.0
IJzerbaard,"It's sort of like a byte-oriented version of Elias delta coding, I haven't heard of a name for it specifically.",1515114149.0
,[deleted],1515122194.0
skeeto,"This might not have been documented before because, in practice, you'd never need more than one level of indirection. If the length of the data cannot be expressed using 255 bytes (2,040 bits), then the data wouldn't fit in our universe anyway. You can count all the atoms in the universe using just a 273-bit number.",1515117568.0
user0872832891,"It reminds me on [BER encoding](https://en.wikipedia.org/wiki/X.690#BER_encoding), where you also have to set the length of the data.


Some other links:

[https://www.coursehero.com/file/p4hnb5uf/When-the-length-of-the-value-field-is-greater-than-127-bytes-the-length-field/](https://www.coursehero.com/file/p4hnb5uf/When-the-length-of-the-value-field-is-greater-than-127-bytes-the-length-field/)

[https://stackoverflow.com/a/18932655/113858](https://stackoverflow.com/a/18932655/113858)

[https://github.com/ajanicij/x509-tutorial/blob/master/x509-analysis.md](https://github.com/ajanicij/x509-tutorial/blob/master/x509-analysis.md)",1515137983.0
TomSwirly,"So I don't see the real advantage of this over some standard BigInt representation, where you have a fixed size digit count and then the actual digits.

Your system might be a bit more efficient than that for representing small numbers, because the minimum size of your representation is two bytes, whereas the minimum size of a BigInt is `sizeof(yourIntegerType)`.  In the context of ""really large numbers"", however, those six bytes or so are simply not important at all.

If `yourIntegerType` is a 64-bit integer and your radix is 256 (i.e. each byte is a digit) then the simple BigInt representation lets you represent numbers up to 10^4E19 - that is, all numbers with up to 40 quintillion decimal digits.

By comparison, [the machine with the most memory today](https://www.forbes.com/sites/aarontilley/2017/05/16/hpe-160-terabytes-memory/#4888d34f383f) has 160 terabytes of memory - about one hundred-thousandth of that maximum length.

",1515176645.0
fnork,Upvote to encourage guerilla science!,1515180287.0
,This kind of coding (or something analogous) is used in Kolmogorov complexity to create prefix free codes. Basically for a random number this is the shortest prefix free code you're likely to find to encode it. Usefully this coding scheme lets you directly concatenate a series of integers and you can easily access the n-th one with a pointer to the very first bite. ,1515176169.0
gentzen,"Such a coding scheme might be more interesting from a FOM point of view than from a pragmatic programming perspective. Assume an evil adversary which claims to tell you a natural number, but actually just continues to to say 00000000000000000000000...
The bound you get on the eventual natural number seems to be superexponential (in the number of received 0s), and hence probably beyond EFA (elementary function arithmetic), a formal system much weaker then PA (Peano arithmetic) used in reverse mathematics:

... I protest that PA is not a weak system. I find EFA much better in this respect, since the ""mathematician in the street"" ... will be in a much better position to understand the significance of the independence results for that specific system (like that it cannot prove cut elimination), what it entails to accept that system (the allowed computations are no longer ""feasible""), and what it entails to go beyond that system (exponentiation is an analytic function in complex function theory, but superexponentiation is not).",1515976403.0
GuyWithLag,"BTW, how do you encode something that is 0-length?",1515172239.0
JimH10,"Knuth says in his [FAQ](http://www-cs-faculty.stanford.edu/~knuth/taocp.html#vol4)

*And after Volumes 1--5 are done, God willing, I plan to publish Volume 6 (the theory of context-free languages) and Volume 7 (Compiler techniques), but only if the things I want to say about those topics are still relevant and still haven't been said. Volumes 1--5 represent the central core of computer programming for sequential machines; the subjects of Volumes 6 and 7 are important but more specialized.*

It seems fair to me to read into that an expectation that those books will not be written, and perhaps that they do not at this point need to be written.  (I mean no offense to Prof Knuth, obviously, but his ability to predict when he started how long it would take or what would be most relevant in a very rapidly changing field all these years later is good, better than anyone perhaps, but not perfect.)",1515152845.0
Revrak,iirc he mentioned as an assumption that he would complete subsequent volumes if life extension treatments become available. afaik this has not happened so his dead is not unexpected he might have a plan.,1515112573.0
AnalogOfDwarves,"I'm assuming he'll do like he plans to do with TeX and Metafont, and make his final notes Volume δ of TAOCP, where δ is the first Feigenbaum constant. ",1515159179.0
chrisgseaton,"SIMD means the same program is run on multiple pieces of data at the same time. We say they are in lock step because the operation is run on each piece of data at the same time. This is true both logically, because a single instruction does it for all pieces of data, and physically, because your processor includes actual separate silicon for each piece of data so they can run at the same time.

MIMD means multiple programs are run on multiple pieces of data at the same time. We say this is not run in lock step, because the programs aren't the same, so they can't stay in sync while they do the same thing. They can run at the same time though, if you have multiple complete execution units in hardware. This is more complicated, as when they aren't in lock step, all the other things like where they are in the program have to be independent.

To use your soldier example, SIMD is like a pair of soldiers marching in step, and MIMD is like two soldiers doing two different unrelated jobs but at the same time.

Source: soldier.",1515109261.0
ImaginationGeek,"I’ll add to this with examples of SIMD and MIMD architectures...  (these terms can be applied to programs that run in a certain way *or* to hardware that runs programs in a certain way)

Graphics cards are an example of SIMD architectures. Your display is made up of a 2-D grid of pixels. Often, you might want to do the same thing to every pixel. For example, if you want to dim the screen by half, you could multiply the value of every pixel by 1/2. Graphics cards specialize in this kind of doing the same operation to many different points (pixels, in this example).  It would do the operation to all pixels in parallel, and each instruction of the operation would happen simultaneously to every pixel.  (This is a bit of a simplification of what graphics cards *actually* do, but it serves to illustrate the point without requiring you to know much about how computer graphics really work...)

Multi-core processors are MIMD.  Each core is almost completely independent, so each one could be doing a completely different thing at any point in time.  If you have a quad-core processor, one core could be running your web browser, another could be running some data analytics code you wrote, another could be running a linear regression in Excel, and the fourth may simply be idle.

You also have SISD (single instruction, single data), which is basically just your classic single-core processor.

MISD is also a valid term, but is rarely used. I’m not personally aware of any notable real-world examples of this.

Terms like SP*D and MP*D are just the process-level variants of this, but the basic concepts are the same.",1515190092.0
IJzerbaard,"It's just emulating a bunch of ""narrow"" operations side-by-side with some trickery, for example

    z = ((x &~H) + (y &~H)) ^ ((x ^ y) & H)

calculates a bunch of additions, with the sub-fields of your integer indicated by the mask H. So if you want to add bytes H will be be rep(0x80), if you want to add words H will be rep(0x8000) etc, and you can use unusual/mixed sizes too (which normal SIMD can't).

This specific formula works by first calculating the sum of ""all but the top bit"", which can carry into the top bit of every field but that's fine, that top bit was cleared by `& ~H` so the carry cannot escape from the field entirely. Then the contribution from the top bits of the inputs is ""added"" into the top bit of the result, but without carry (so it becomes a XOR).",1515099912.0
cephalopod1,"> Will somebody be kind enough to help out the Karl Pilkington of computer science to prepare for this exam I have coming?

Lol, I’m taking this.",1515104385.0
NeoMarxismIsEvil,Chop up the adder by removing some carry gates. Wallah!,1515125540.0
Maleden,"AI + familiar, problem respecting language + problem that involves human oversight.

For problem selection, consider your interest, proficency, need, and what you could get paid for.",1515092710.0
mfiels,"Analyze a large natural language data set (tweets, Facebook posts, Reddit comments, YouTube comments, whatever) and try to identify some quality.

I'm not sure if capstone has to be original research or not, but look at something like this for motivation: https://www.google.com/amp/s/techcrunch.com/2017/11/27/facebook-ai-suicide-prevention/amp/",1515109231.0
codegreen_,I’m an app and web dev. Check out my freelance [page](https://www.guru.com/freelancers/ryan-shane) ,1515087881.0
SambaMamba,Try /r/forhire,1515087968.0
devLyfe,Im in my second year of CS and haven't needed one yet. ,1515054477.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1515041048.0
magnificentbop,"CAD programs handle the placement and routing of the actual wire and components, as well as the transistors that make up the logic gates.

Instructions in binary are handled by an RTL design for the specific architecture, which uses basic digital logic blocks to build more complicated modules.  Each of these modules might be responsible for decoding (figuring out what instruction), executing the instruction, writing the instruction to or from registers, and so on.

Somewhere in between, CAD programs place the RTL components on a die, route the interconnections, and evaluates the correctness in terms of logic levels, timing, and electromagnetic characteristics.  This is an iterative process.

This is a sweeping generalization of how a modern computer is made to work.  I would recommend Hennessy and Patterson's textbook on computer architecture if you want to read up more on how the RTL design works.",1515046039.0
shbilal001,"A firmware is not wired into a C.P.U. The C.P.U is only used to access protocols and help in execution of commands as stated by firmware commands which are stored in some memory location in a particular device. So a firmware is some software that is downloadable and can be upgraded at any time on a device and is stored in the memory of a particular device. A C.P.U on the other hand is usually located in a separate position and is used to fetch commands given by the firmware software which is located in the memory location of a device. To answer your question, the firmware is not hardwired into a C.P.U. The firmware is just stored in the memory of the device (can be downloaded and upgraded). The C.P.U is separate from all these and is just used to fetch commands and execute them as dictated by the firmware itself. You can learn all about hardware and software components of a computer by enrolling in a good programming school like Holberton School (https://www.holbertonschool.com/). From there, you can register for a Computer Science and learn all you can about hardware and software components of a computer and much more to guarantee you a job in the IT industry.",1515269350.0
shbilal001,"A firmware is not wired into a C.P.U. The C.P.U is only used to access protocols and help in execution of commands as stated by firmware commands which are stored in some memory location in a particular device. So a firmware is some software that is downloadable and can be upgraded at any time on a device and is stored in the memory of a particular device. A C.P.U on the other hand is usually located in a separate position and is used to fetch commands given by the firmware software which is located in the memory location of a device. To answer your question, the firmware is not hardwired into a C.P.U. The firmware is just stored in the memory of the device (can be downloaded and upgraded). The C.P.U is separate from all these and is just used to fetch commands and execute them as dictated by the firmware itself. You can learn all about hardware and software components of a computer by enrolling in a good programming school like Holberton School (https://www.holbertonschool.com/). From there, you can register for a Computer Science and learn all you can about hardware and software components of a computer and much more to guarantee you a job in the IT industry.",1515269439.0
daffme5,"Thanks for the book recommendation, i've got Henessy and Patterson's book and Code by Charles Petzold, hope they will clear up all the questions for me. 

@osztyapenko with a device like this one?
https://en.wikipedia.org/wiki/Programmer_(hardware)",1515290911.0
osztyapenko,"There is some PROM in the CPU for it, it gets uploaded through the JTAG interface.",1515071500.0
worldexe,This is how microcode updated nowadays: https://wiki.debian.org/Microcode,1515077266.0
LAN_Rover,"Transistors are put together into logic gates, which are assembled into small instruction sets",1515057292.0
phlummox,"Why on earth would you not? If you know one or two other languages reasonably well - and I would hope you do, if you're applying for PhD programs - then learning enough Python to get by should be a trivial task.",1515032746.0
nerd_guy,yes,1515030685.0
psqueak,"It's worth noting that, contrary to the title (and as stated by the author), insertion in the middle is still O(n)",1515020541.0
Maristic,"This implementation is part of the [Keith Schwarz](http://www.keithschwarz.com)'s [_Archive of Interesting Code_](http://www.keithschwarz.com/interesting/), which seems pretty cool. Apparently he's a lecturer in Stanford's CS department and is an advocate for nixtamalization.",1515018083.0
FanielDanara,"It’s documented pretty well, I was impressed by the readability. ",1515012786.0
Curdflappers,"Isn't this already the well-known method of using shadow arrays? I learned about this in Intro to Data Structures, and it seems exactly the same",1515021028.0
jpfed,Pretty neat!,1515012059.0
eigma,"Larger constants though? Each insert is actually two memory writes?

Also larger memory footprint, again by a constant. An array size n uses underlying arrays with 3n elements. Hopefully virtual memory reduces this by a constant again because only ~n elements are recently accessed at any given time?",1515026450.0
krum,isn't that a deque?,1515037555.0
TheCheesyPoof,damn ,1515036780.0
Jaxan0,"This is very nice. It reminds me of a general idea in Okasaki's book (functional data structures): one can make amortised data structures into worst case data structures by using an explicit scheduler. This way you control when things happen. In this instance, we explicitly schedule when we copy elements. ",1515055524.0
VermillionAzure,"So, it trades space for time? I guess everything has a tradeoff.",1515527308.0
taejo,The Ackermann function can't be computed with *bounded* iteration (where the program must specify an upper bound for the number of iterations before the loop starts).,1514961429.0
GNULinuxProgrammer,"The question of whether a recursive function can be written iteratively is a weird one, because most of my collegues had suspicions when they were explained that all recursive functions can be written iteratively, just like you. But think about it, recursion, under the hood, is implemented with a stack by your compiler. Every time you recurse, you allocate a frame in stack (i.e. the stack assigned by your OS). So, by simulating this stack (i.e. with a stack data structure in your heap) you can do just the same. Therefore, it is trivial that all recursive functions can be made iterative. You need to convince yourself whether there is a (recursive) C program that can compute the Ackermann function and whether your compiler can compile it, if the answer is yes, then the answer to the former is already yes too, since your compiler used your OS's stack to implement the Ackermann function.

Another thing that makes this question trivial is, a programming language without recursion can be Turing Complete too, by just using GOTO statements. Since Turing complete languages have the same computational power, they can both compute the Ackermann function, therefore a language without recursion can compute the Ackermann function.

Where it gets tricky is, as other answers said, putting a *bound* to your iteration. This cannot be done in this case, so you cannot bound your stack which (practically) means your heap can overflow when you write the iterative implementation. But this is true for the recursive case too. This is also fine since when we're talking about Turing machines we assume an infinite memory but this is not accessible in physical computers. Yet, programming languages exist in an abstract universe independent of both their implementations and physical computers. I hope this clarifies it.",1514972133.0
funshine,"> However, it's also my understanding that the Ackermann function, which is not primitive recursive, cannot be computed iteratively.

That's not correct. You can implement Ackermann iteratively with stacks.",1514969983.0
Zherom,"Posted this as a reply to whoever called me wrong on my massively downvoted comment, but I wanna make sure OP sees it. The source of my explanation: https://youtu.be/i7sm9dzFtEI ",1514992285.0
Zherom,"You actually have it backwards: any iterative function can be written as a recursive function, but not necessarily vice-versa. Recursive functions actually have ""levels"" to them; where basic recursive functions CAN be written iteratively, and complex recursive functions (such as Ackermann's) cannot.

Part of what makes the Ackermann function a 'complex recursive function' has to do with it's super-exponential behavior.",1514967869.0
Terr_,"Note that this approach can recurse to an arbitrary (unbounded) depth, but *probably* won't. Probably. Unless you're really unlucky or your coin is extremely biased.

Personally I find iterative versions easier to understand, since I think of this process as a simple ""retry"", rather than something which builds on previous operations.

    def fair_coin(biased_coin):
      first, second = None, None
      while first == second :
          first, second = biased_coin(), biased_coin()
      return first

P.S.: My intuition is that this is the same underlying issue as generating random integers in 0-to-N from a source that only supports 0-to-M: There's no bounded way to solve it without sacrificing mathematical accuracy. ",1514973328.0
voidvector,"If you know the bias, you can treat it as an encoding problem, where a sequence of N rolls can be evenly divided into partitions without discarding any entropy, thus generating M rolls

Treating it as an encoding problem also protect against worst case in extremely skewed biases (e.g. P=0.9999)",1514981756.0
HeSheMeWumbo387,"Very elegant. I believe John von Neumann described this solution in one of his publications. [https://en.m.wikipedia.org/wiki/Fair_coin](https://en.m.wikipedia.org/wiki/Fair_coin) 

[Paper](https://dornsifecms.usc.edu/assets/sites/520/docs/VonNeumann-ams12p36-38.pdf) ",1514963055.0
Ginden,">  we prefer high quality posts focused directly on **graduate level** CS material.

I had to prove why it works in high school (and it was mentioned in middle school).",1514992598.0
MjrK,"Just going based on my limited information..

* Computer science deals with algorithms with most careers focusing on software development. Requires a knack for logic + critical thinking.
* Data science is concerned with statistics and analysis of data; careers seem to focus on business intelligence and report generation. Probably fits better for someone who enjoys probability and statistics.
* Information science is concerned with the actual management and storage of information; careers probably focus on information reliability and security. I'd say this probably appeals best to someone that enjoys overseeing and maintaining systems.",1514929407.0
Zherom,"Apart from the ""differences"" others have mentioned; it may be important to note that Information Science could be seen as a subclass of Data Science, which could be seen as a subclass of Computer Science.

As a freshman looking to get into the industry/field, if you have no true preference or idea of what you specifically want to do, your best bet will be Computer Science, as it places you in a magnificent position to delve into pretty much any particular focus.",1514934711.0
teddystan,"I was a computer science major with a data science minor. My SO was an information science major. I feel somewhat qualified to answer this question! :)

* CS is a lot of programming, logical puzzles, abstract (discrete) mathematics, understanding computer systems/architectures, algorithms. You may delve into things like AI, Computer Vision, Security, Networks, or programming language theory, for example. It's a very broad field. CS majors more often than not become software engineers. A person who likes solving puzzles is probably the best description of a computer scientist.
* Data science is also a lot of programming, but a different style of programming than CS. Data science tends to pertain heavily to statistical math. You'll probably delve into machine learning, data visualization, and I'd like to repeat, a LOT of statistics. DS people tend to end up as data analysts or data scientists. Data science is very interesting because we have so much data in the world today and you can get some extremely exciting insights using a variety of different models and methods that you'll learn. Very hip field. 
* Information systems is a pretty vague term in my experience. For my SO it had some programming and some business courses. It mostly had to do with being able to manage databases and writing scripts for excel. She also did a lot of work in R to perform some sort of analysis on the data. She also used a lot of excel. Most of the people in the IS degree ended up being tech consultants (i.e. bridging business and technology). A few became database administrators (managing AWS instances, old school SQL, etc.)

Hope that helps a little bit. Clearly I know most about CS, some about DS, and a fair amount about IS. :P

edit: It's also possible you may just not be interested in any of these. Don't forget about that option. While your degree doesn't necessarily determine your career, it does play a huge factor on both your career and your happiness.",1514932503.0
shbilal001,"The difference between the three fields is small, yet very distinct from each other. Computer Science deals with the scientific ways of solving a problem and that would involve dealing with algorithms and software development. This field requires a person with love for logic and critical thinking. A person intending to go for data science should have a good background in mathematics as one should have knowledge in probability and statistics. Data Science is more concerned with statistics and analysis of data, with the industry wanting people with business intelligence skills and report generation. For a person intending to major in information science, they would have to have a deep interest in overseeing and maintaining computer systems. This is because information science is all about management and storage of information. A person with information science knowledge needs to know information reliability and security. Whichever field you end up choosing, you should consider where you will want to learn and be knowledgeable. I would suggest that you go to a good programming school like Holberton School (https://www.holbertonschool.com/) and learn with other students in collaboration with the tutors to ensure that you become an expert in the respective field. Only then would you be comfortable in the industry. ",1515514080.0
hubbahubbawubba,"You might enjoy some of the work of Arvind Murugan, a biophysicist at UChicago. ",1514943747.0
Terr_,"Reminds me of some quotes from Deus Ex (2000), which had *good* techno-babble. For example, an overheard conversation between two frustrated scientists in a top-secret facility:

> * The sequence came straight out of VersaLife’s software.
* Did you double-check the endonuclease? I’ve seen the tethering fail completely
in vivo.
* The damn program should just work.
* Cytoplasm isn’t exactly a eutactic environment. Certain operations just fail.

And a memo to staff:

> The cells of every major tissue in the body of a nano-augmented agent are host to nanite-capsid ""hybrids."" These hybrids replicate in two stages: the viral stage, in which the host cell produces capsid proteins and packages them into hollowed viral particles, and the nanotech stage, in which the receiver-transmitter and CPU are duplicated and inserted into the protective viral coating. New RNA sequences are transmitted by microwave and translated in to plasmid vectors, resulting in a wholly natural and organic process.",1514956292.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1514919879.0
jukuduku,I have taken his classes before. He is very knowledgeable and has a true passion for his field.,1514943872.0
libeako,"this title is misleading; it is not computer science; it is just a tutorial for c, java, php; should be named like : ""A practical introduction to programming""",1515164470.0
luckygerbils,"The total sum of the checksum must be zero, modulo 10. The total for the checksum could also end up being 10, 20 or 40 (with a different bar code). 30 it's just the closest multiple of 10 for this particular bar code.

Saying `X modulo Y is 0` or `X is equal to 0, modulo Y`  is just the mathematical way to say X is evenly divisible by Y with no remainder.",1514914161.0
pi_stuff,"From here: https://en.wikipedia.org/wiki/Check_digit#UPC

Looks like you take the sum modulo 10, which is the same as taking the last digit: 23 % 10 = 3

If it's not zero, subtract it from 10: 10-3 = 7

",1514913497.0
IJzerbaard,"> the next highest power of ten

Multiple of ten actually, and you can tell because the result is supposed to be 30 which is not a power of ten.

Anyway that whole ""take next higher multiple of ten and subtract from that""-deal is just a more laborious way express `-sum mod 10`, in the process losing any obvious connection to why it even makes sense to do in the first place. It is more layperson friendly, I suppose, since it avoids the word modulo which is not that widely known. But in general that is not a ""bad word"", if anything it's useful word since it (correctly) creates the expectation that we're doing arithmetic in Z/10Z so we can use our usual finite-ring intuition.",1514971572.0
anamorphism,"i guess the simplest answer is because the ""modulo check character"" is the value used in the modulo calculation to check that your barcode is valid ... and that space in the image is limited.

another way would be to think about how it would be implemented and not how to explain it in english.

you would implement it somewhat like this:

    return (moduloCheckFormulaValue + moduloCheckCharacterValue) % 10 == 0;

you wouldn't implement it like this:

    var current = 10;

    while (moduloCheckFormulaValue < current) current += 10;

    return (current - moduloCheckFormulaValue) == moduloCheckCharacterValue;",1514943746.0
TheWildKernelTrick,Awesome write up! I've been hunting around for something like this for a while!,1514914611.0
GiantRobotTRex,I'm new to containers. When would I want to use Kubernetes instead of just Docker?,1514938305.0
ignotos,"Simplifying a bit here, but:
 
Data science is mostly related to statistics and analysing data. Looking for trends, patterns etc - working out how to do useful stuff with the mountains of data which are generated every day. A lot of this is focused on using data to gain insights which are of practical value to a business or some other organisation. While some may work on theoretical stuff, or creating new tools and techniques, the role of a ""data scientist"" often suggests working on specific, concrete problems.
 
Computer science is mostly about the theory of how computers can be used to solve certain problems - what kind of stuff is it even possible for a computer to ""compute"", and how efficiently can it be done? Academic computer science can be entirely theoretical, and the things these people work on can sometimes resemble brainteaser-style puzzles. Try playing this game: https://www.jasondavies.com/planarity/ . A computer scientist might try to work out an optimal strategy for completing this game, or how to generate new ""levels"" for this game which are guaranteed to be solvable. Others might take this theoretical stuff and attempt to apply it to some real-world problem (maybe working out how to position a network of power lines and towers).
 
Both are mathematical in nature - data science is generally more about dealing with actual numbers and sets of data, and computer science is generally more abstract.
 
Both often use programming as a tool, although neither are synonymous with ""programming"", as such. Data scientists program because there are large volumes of data involved, and they often want to do something directly useful with it. Computer scientists because they want to prove that their theoretical ideas actually work, or because they're trying to apply some theoretical stuff to solve a real-world problem.",1514869032.0
GNULinuxProgrammer,"Computer science is mostly about algorithms and their properties such as analyzing their runtime, their relations, possible implementations in programming languages etc... Even though both fields use a lot of math, computer science usually uses a different kind of math, discrete math and probability theory. Data science uses mostly statistics and also some probability theory.",1514900212.0
B15h73k,"Simplistic and truncated response: Data scientists can do mathematics on large data sets, using R or Python, but might not be able to write a whole application, or design a programming language. Vice versa for computer scientists. ",1514869647.0
AustinCorgiBart,I'm on mobile; how's the speed on a normal computer? Is this a real thing or just a proof of concept?,1514870831.0
opinionist,"Cool! Now I just need the JVM as a clang target, and then I'll never have to use anything other than C++ ever again!! :D Webassembly has a tad more promise than ActiveX.
",1514910243.0
drWeetabix,Hey that's pretty good,1514892940.0
R4p354uc3,"Wow, I just compiled and ran a C++ program, within a browser, on my phone. That's really cool.

It's unfortunate how long that took though. Clang.wasm needed almost a minute just to load, and 11 seconds to run. Obviously as the technology evolves  these things will likely improve dramatically, but right now it feels like we're in the 90's.",1514915799.0
ReginaldIII,"I changed two lines in the example program to push a `std::endl` to `std::cout`.

    // clang runs in the browser and compiles C++
    // to WebAssembly, which the browser then runs.
    // See https://github.com/tbfleming/cib for known issues.

    #include <algorithm>
    #include <stdio.h>
    #include <string>
    #include <vector>
    #include <iostream>      // This line added
    using namespace std;

    int main() {
        auto strs = vector{""3""s, ""2""s, ""1""s, ""4""s, ""but this demo is real.""s, ""The 🍰 is a lie,""s};
        sort(begin(strs), end(strs));
        for (auto s: strs) {
            puts(s.c_str());
            cout << endl;    // This line added
        }
    }

It compiles fine:

    wasm size: 13682

But yields a run-time error:

    enlarged memory arrays from 33554432 to 67108864, took 17 ms (has ArrayBuffer.transfer? false)
    LinkError: WebAssembly Instantiation: Import #6 module=""env"" function=""_ZNSt3__24coutE"" error: global import must be a number",1514916386.0
liquidify,Interesting.  So why doesn't the cout stream work?  ,1514883642.0
,This crashed my reddit app (the page viewed in a webview inside reddit is fun),1514953043.0
trout_fucker,"How many times are you going to ask this in how many subs, without acknowledging any of the people who take time to answer?

And will you delete this too, like you did for the ones the other day?

If you have no people skills, you will not make it far in this industry.

Edit: Now your mutli account downvoting all my comments calling you out. -4 on all these comments with 1 refresh. ",1514834926.0
WSp71oTXWCZZ0ZI6,"I wish I'd had this when I was a grad student. Our university had subscriptions to all the major journals, but only if you accessed them from on campus. When I was doing work at home, I used to ssh into one of the boxes on campus and then use X11 forwarding to get a dog-slow Firefox session that I could download papers from.",1514860496.0
07dosa,"Thanks for sharing, though I haven't got any green tabs from what I like yet. It's either papers are already free or not available at all.",1514807978.0
dezzion,Why not just use sci-hub?!,1514861358.0
beeks10,This is awesome! Thanks!,1515314770.0
,Please consult a more appropriate subreddit like /r/cscareerquestions and not treat them like your servants. Thanks.,1514803887.0
WSp71oTXWCZZ0ZI6,"I never liked the LISP/Scheme approach to doing lists as pairs. I understand why they did it, as the two parts of the pair mapped onto hardware registers sort of conveniently, but it seems so inelegant, and makes the code much worse than it has to be. Church's original encoding of lists using constructors is much more sensible.

Using Church's encoding, you would have:

    def nil():
        return lambda n, c: n
    def cons(h, t):
        return lambda n, c: c(h, t)
    def head(l):
        return l((lambda h, t: h), None)
    def tail(l):
        return l((lambda h, t: t), None)

You can now do folds (effectively iteration) and maps and things in a natural way.",1514778187.0
VincentPepper,"Caching and branch prediction kinda do this for repeatedly executing the same code.

But it depends on the specific code and doesn't work for all instruction sequences.",1514769455.0
katsy91,"CPU Microarchitect here. Yes, it is possible. For each type of operation...

When it comes to memory operations: Yes, because of caching.

When it comes to control flow (if, loops etc) : Branch prediction (and to some extent Trace Processing)

For ALU operations: Value Prediction & Binary Translation (sort of).

Also, most processors these days use something called binary translation. The native x86/ARM is translated into an processor specific ""assembly"" (microcode) and the processor employs many complex optimizations on this microcode before executing it. Kind of like LLVM IR.
Hope this helps. Not sure how much detail you're looking for.",1514776151.0
IJzerbaard,"Entering the realm of Hypothetical Microarchitectural Trickery, yea maybe in some cases. Not 1+1 as far as I can tell though. 

A potential way you could do it, is implement some form of value prediction, and in case of high certainty speculate ahead under the assumption that the arguments will have the predicted values. That way the operation would not have to wait until its operands are known, and in many contexts that would appear to give that operation a negligible latency. In the context of useless operations it wouldn't really do anything though: with discarded outputs the latency of the operation is meaningless anyway, and with constant inputs the operation never had much to wait for.

That is not implemented in any architecture I've heard about, and probably wouldn't be very useful, but you could do it.",1514770715.0
Sir_not_sir,"No. Each instruction is hardcoded in circuitry.

I think there's been some work on genetic reprogramming of FPGA chps.",1514769431.0
Tip89,"Yes, at least for binary translation based cpus. E. g. x86 had  [transmeta](https://en.wikipedia.org/wiki/Transmeta#Code_Morphing_Software) and Arm has Nvidia denver",1514775982.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1514717563.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/programming] [How to build a simple compiler using gnu tools?](https://www.reddit.com/r/programming/comments/7n6zgs/how_to_build_a_simple_compiler_using_gnu_tools/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1514700949.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1514696071.0
kukulaj,"Very many calls to Random.NextDouble() behind this! A bit more than an hour of runtime on an Intel Xeon W3520 2.67 GHz. The code is in C# so not super efficient but still...
",1514696276.0
uh_no_,"nope. nobody really cares. what shines most is your classes/projects/outside stuff.

If you want to stand out, do hackathons or side projects or competitions or some such.",1514694131.0
penguinland,"No one cares about the name of the degree, but they _do_ care about your skills and what you learned. The engineering degree is likely to teach you more and go into greater detail on the topics. So, I think the BS is indirectly more valuable than the BA. but this is a small difference: a smart, motivated person can get the same skills from either program.",1514741928.0
VermillionAzure,"I find that the section on the lambda calculus is a bit strange. It has no mention of the alpha conversion, beta reduction, or eta conversion rules which are fundamental to the system, and begins to talk about non-primitives like ""list operations"" and Boolean constants when the only primitives of the lambda calculus are variables, lambda expressions, and function application.

I suppose you could firmly place these lecture notes in the ""abstract machine"" camp for theoretical computation, taking the NAND operation as its basis for computation, and then creating RAM machine/Turing-machine equivalents.

Does anybody know who the target audience is for the ""Introduction for Theoretical Computer Science"" notes? 

",1514718535.0
Gakster,Should have been posted April 1st,1514714079.0
TheOnlyHighlander,"""Anyone with half a brain can see that
   object-oriented programming is counter-intuitive, illogical and
   inefficient.""

lel",1514760285.0
alwaysonesmaller,"> IBM got sick of it, and invested millions in training programmers, till they were a dime a dozen.

The cycle never changes.",1514763098.0
nettapper,"How much truth is there to this interview, perhaps someone with more knowledge will know.

If not I'll spend some more time digging myself!!",1514715208.0
WSp71oTXWCZZ0ZI6,"Have you taken your operating system design course yet?

I tend to learn well through writing code. About 10 years I made myself a little weekend project:

* Add a new system call to Linux. It was something simple, like drawing a character on the screen.
* Add the same system call to Minix.

After that, I developed a new hatred for Andrew Tanenbaum with the intensity of 1000 suns.

You don't have to do exactly that, but if you're interested in operating system design, I think it might be worthwhile to do a few simple tasks with one or both kernels to see how they're organized. To start with, you'll probably need a lot of help just navigating around (and your first task might just be finding the right part of the source code where something happens).",1514678762.0
shbilal001,"You have big plans for your senior project, and you are doing a very good thing trying to prepare for that. You may wish to engage in any activity that can help you build your skills and confidence as you get near to doing your project and thesis. You can start of by building simple applications that you will be excited to do as you will not be willing to stop halfway, till completion. An example is an application that you can get to use with your friends, like a program that can automatically reply to new texts after reading them. For example if a friend sends a text “Hi what’s up?” the application can read through the keywords present in the message and create an appropriate reply like “Hi, how are you doing?”. These small projects can help build your confidence, knowledge and skills as you prepare for your senior thesis. Another way to build your knowledge is joining a good programming school like Holberton School (https://www.holbertonschool.com/) and learn all about software engineering and design. Remember that in programming, the more languages you have the more options you will have on your table on how to best approach solving an existing problem. Good luck and all the best. ",1515515770.0
Jutjuthee,I did take this course in college and we used to do one chapter every week. I would say reading the chapters understanding them and doing the exercises may take a few days approximately. Chapters 1-4 are really easy in comparison to the other chapters. But the last 3 chapters are really work intensive. ,1514672601.0
chhuang,"I did the first half in a week, since it is the part I was interested in and really wanted to know what goes lower level. It's pretty cool if you know a lot of stuff at a very elementary/surface level, not required of course, but that will create more curiosity.

The second half I didn't finish it since it is already something I've learned in uni. Syntax are certainly different, but the fundamentals are the same. It does require more work imo since they did cram in materials that took my 2 years to learn into 12 weeks.",1514681797.0
ontheintarnet,"I just completed  this via coursera a month ago. It was a great experience and I'd recommend it. The coursera version tackles the book in 2 halves, one that builds the hardware, and the other builds the high level language, OS and compiler. The second half is much more coding heavy, and can feel like drudgery at times, but once you are done it feels great.

It took me about 3 months to complete both parts, but I was working while doing it on nights and weekends, so if you are on hiatus, probably will be faster for you.

Let me know if you have any other questions about it.",1514693680.0
iTeachCSCI,"> Not knowing comprehensively how computers work at a low level has not hindered my work at all. 

Are you sure?  To some extent, being able to recognize when you're throwing an optimization blocker at your compiler or what the cache effects of your code are doing are things you can completely miss if you aren't aware to look for them.  

It's very possible you're right that the low level stuff hasn't hindered you.  It's also possible that it has and you're unaware of it -- your code worked, but wasn't as fast as it could have been, or there are ways you could have improved it that may or may not have mattered, depending on the system.  I remember learning about some of these things when I took my first compiler class and then I thought back to a past internship and realized how much better I could have made some things I did then.  

That having been said, I hadn't heard of nand to tetris before, but now I want to do it.  I wish I had the time -- gotta finish getting ready for the upcoming term.  Maybe this summer.  Thank you for bringing it to my attention.  Also I had no idea Noam Nisan had co-written a systems book, so that's going to be an enjoyable read on its own.",1514689010.0
,"I did it in high school as a personal project. It took me all of junior year and then some -the low level compiler was at that point the largest and most serious programming project I had ever attempted and i didn't know enough about high level languages to make it easy to write.

I would advise being strongly familiar with an oo language like Java before reading this book. The book guides you through the design process and gives you the outlines of what programs should look like. 

Getting to the end and having a whole functioning PC that you built from the ground up is incredibly satisfying. I put it on my college application resume and to this date it's one of my proudest achievements. If it sounds like something you might be interested in, go for it. ",1514746090.0
cirosantilli,Do it in Verilog instead :-),1514716012.0
dstalor,"I did some of this out-of-order as coursework for one of my classes as part of my degree.

In our course, we were assigned lesser-known languages and we had 5 exercises that had to be completed in our language (building a Jack interpreter, building a Hack compiler, etc.). I was given the language Haskell and some of the monadic parsing abilities in Haskell made it easier, but at the same time, it was very challenging to use a functional language like that.

I don't know what the full course entails, but that course was one of my favorites and really cleared up certain sections of my knowledge regarding how we can get from a high-level language down to the assembly level.",1514718860.0
fj333,It's phenomenal... do it. Took me around 2 months. I did it on the side while taking other classes toward an MS and also working full time. But I *really* enjoyed it and spent every free second on it. The most enjoyable CS book/project that I ever read/did.,1514900754.0
alwaysonesmaller,"Don't pigeonhole yourself into a corner. Choose a few topics that interest you, take some free/cheap courses on any of the various MOOC sites, and see what interests you. You'll burn out if you pick your next step based on what's in demand rather than what's interesting to you personally.",1514661043.0
WhackAMoleE,"> Factoring in stress

That's totally subjective. If coding to deadline stresses you out, you're in the wrong business. Because commercial programming is nothing but coding to deadline. 
",1514673948.0
dixieStates,"In order of preference as i see things now:

* Data Scientist 
* Python Developer
* Full-Stack Web Developer
* DevOps Specialist

You left something out which I think will become increasingly important and that's the Internet of Things.
In my opinion, you should avoid Rails.
",1514659514.0
pleaseholdmybeer,I don’t think you can go wrong with web development. Everything is moving to the internet and even desktop/mobile apps are starting to be made in web stacks.  ,1514668943.0
Dithot,"So basically the Blockchain version of those CS:GO roulette scams, got it.",1514647568.0
Roachmeister,"So, basically an attempt to create the world's most boring game.",1514655853.0
ImaginationGeek,"I'm hoping everyone else also noticed this right away, but I'll point it out just in case...  This was written by someone who has ""competing"" product to CS degrees.

(Full disclosure: I am a CS prof. so I have my own interest in showing that CS degrees *are* valuable...  but A. I'm telling you that up front so you know where I stand, and B. I'm not telling you to ignore my bias. I'm just telling you not to ignore his!)

Also, he's talking just about web developers. People with CS degrees can do *a lot* of different things; web development is just one of them. Even with my CS prof. bias :) I'll readily admit that if you just want to be a web developer and that's all, then a CS degree may be nice but probably isn't really necessary for the job. The reason to get a CS degree is so that you can do all the other jobs...

So although the unstated implication is that a CS degree isn't worth it, that conclusion isn't actually justified by this analysis. The only actual conclusion that you can really reach form this is that you don't need a CS degree _to be a web dev_. The degree may still be worth it, in general, though.",1514651933.0
blazingkin,"Why did you analyze the difference using linear regression?

If you are looking to see if the difference is significantly different, you need a Chi-Squared test.",1514651311.0
panderingPenguin,"I have a few musings/suspicions. Disclaimer: I haven't run the numbers on any of these, and they are entirely based on intuition.

First, it doesn't surprise me that the salaries for people with and without CS degrees, with similar experience, in a similar sub-area (like front-end web development, for example) are almost the same. If you're doing more or less the same job and have similar experience that would be expected. It's also not surprising though that those without a CS degree are slightly lower. I'd hypothesize that they start lower and never fully make up this gap, although as they gain experience and their educational background starts to matter less, this gap narrows. It would be interesting to see if CS degrees are more strongly correlated with higher salaries early in one's career than later on. My suspicion is that the answer is ""yes.""

Second, although the salaries may be the same or close to the same, that says nothing about your likelihood of achieving those salaries. It's entirely possible (and again, I suspect this is the case) that it is much more likely for someone with a CS degree to successfully enter the field and remain there as long as they wish than someone without a formal CS background. This would mean that, even if those actually working in the field have similar salaries, a CS degree would still be very valuable for getting you into the field.

Finally, I'm in the US and don't know a ton about Canadian labor laws, but don't they have much more stringent requirements about Professional Engineering, even in computing, than the US? Here many people in computing can and do refer to themselves as software engineers, even though most (almost all) aren't truly a Professional Engineer. In Canada, my understanding is that this is both illegal, and actively enforced. Since you wouldn't be able to become a PE without an engineering degree, it would make sense that CS (or perhaps Computer Engineering?) degrees are much more highly valued in Canada. It would be interesting to see the numbers broken down for them as well.",1514653033.0
FieryPhoenix7,"Good analysis. I've always seen it as a matter of location and experience. 

Personally though, money was never the reason for me to pursue CS. Passion was. ",1514645086.0
jcsf321,"I think there would be better ways to look at this other than salary, say increased $ promotion over time 5, 7, 10, career. Or do they stay in the industry over their career,  job title promotion over a 10 year period,  etc.   I don't think just salary is a good benchmark, because of the way the industry fills technical roles, it's purely based on language skills and experience.  But as you add these other axises you might find something relevant. ",1514647936.0
Berecursive,"A[:, :, :, 0]

Gets the first entry of the d axis. Shorthand for this is A[..., 0]

The index can go anywhere and the patterrn follows, A[:, :, 0, :] returns a slice with shape (2, 2, 8)",1514622901.0
kebabmybob,Oh lawd ,1514609373.0
PlymouthPolyHecknic,"It was possible to take advantage of a price abnormality/opportunity, it would already of been done by a professional algorithm trader. See the book: ""A random walk down wall street""",1514649495.0
Jaxan0,"This does not seem to be the right channel. I, an average computer scientist, know nothing about economics or trading and too little about statistics.

Maybe try [/r/algotrading/](https://www.reddit.com/r/algotrading/)?",1514625532.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1514604739.0
IJzerbaard,"There is circuit complexity for functions, which is not really about Turing machines, but is also about the size of a representation of something. AFAIK it's relatively paradox-free which is a nice bonus, though actual lower bound results are scarce.",1514602218.0
arteymix,I'm pretty sure that it can be brought back to Kolmogorov complexity by finding the smallest pair of machine and word such than it emits the encoding of a Turing machine that implements the function.,1514621107.0
twanvl,"You can extend Kolmogorov complexity to functions by looking at Turing machines that take input, for instance encoded as a bit string on the tape. Or, as you mentioned, the using some other programming language. It is still not computable.

Or you could say that functions are encoded as bit strings that represent Turing machines or lambda terms, and ask for the Kolmogorov complexity of the shortest string that encodes a particular function.

These notions should be equivalent up to some constant. ",1514608834.0
you-get-an-upvote,"Personally I've loved this idea for a while, and have also been a little puzzled at why I haven't been able to find anything about it.

Is it a coherent idea?  I believe so, but with the caveat that you can't actually look at every function from the real numbers to the real numbers, since there are too many such functions (infinitely many more than there are Turing machines).  A practical modification would be to instead assign complexity to functions whose range is the computable reals -- that is, to numbers that can be computed to arbitrarily high accuracy by a Turing machine.

Basically a number is ""computable"" if there exists a Turing machine which, given ""n"", spits out the first ""n"" digits of the number.  This set includes nearly any number you can think of (exceptions are generally hard to find; the only one that comes to mind is BB(100) (although I don't think it's possible to prove that).  These ""computable functions"" are basically the same thing, but in addition to receiving an integer, it receives the ""x"" input of the function.

The only problem that I see is that I don't think giving a Turing machine infinitely-long input is kosher -- after all, if you can give a Turing machine infinitely long inputs (and a real number certainly is), then you could feed a Turing machine the Halting number and ""solve"" (in principle) the Halting problem.

This seems easily remedied: instead of feeding a real number to the Turing machine, feed a computable number.  Unfortunately, this means we have to restrict ourselves to the set of computable-reals-to-computable-reals functions (rather than reals-to-computable-reals), but if we were already okay with restricting our range, we should also be okay with restricting our domain.

This is actually pretty interesting, since when we say you're giving the Turing machine a ""computable number"" as an input, what we actually mean is you're giving it the Turing machine that computes the computable number.  This means that a computable function needs to know what level of accuracy it needs from its inputs to achieve a given level of accuracy of its outputs.

For example, if I write a function that is f(x) = sin(x), on the domain [-1, 1], I might write, in c++-ish:

    ComputableNumber sin(int digits, RealNumber x) {
        ComputableNumber r = x;
        for (int i = 0; i < digits; ++i) {
            if (i % 2 == 0) r -= pow(x, 2*i+3);
            else r += pow(x, 2*i+3);
        }
        return r;
    }

(I assume this converges quickly enough).  But we can't take a Real Number as input, instead we take a program

    ComputableNumber sin(int digits, ComputableNumber x) {
        ComputableNumber r = x(digits); // hopefully this is enough accuracy that it won't affect our output very much
        for (int i = 0; i < digits; ++i) {
            if (i % 2 == 0) r -= pow(x, 2*i+3);
            else r += pow(x, 2*i+3);
        }
        return r;
    }

Edit: [Computable Reals](https://en.wikipedia.org/wiki/Computable_number#Informal_definition_using_a_Turing_machine_as_example) aren't just something I made up, but few people seem to be very excited about them.",1514625080.0
tholenst,"As an answer to [this question](https://math.stackexchange.com/questions/2305014/terminology-for-the-kolmogorov-complexity-of-computable-functions-sets-reals) on SO indicates, there is some information on this in Section 2.2.3 of [Shannon Information and Kolmogorov Complexity](https://homepages.cwi.nl/~paulv/papers/info.pdf).",1514643369.0
VermillionAzure,"**NOTE: I am by no means at all an expert.**

By *function*, I think we're assuming that we're talking about the class of Turing-complete programs with infinite memory, by definition.

I'm very much a novice at this type of work, but Kolmogorov complexity, with a given set and constant encoding, is computable for smaller and less powerful computations systems. For example, since deterministic finite automata (DFA) are compressable down to a theoretical lower limit as used in DFA optimization, the class of recognizers for the Chomsky hierarchy class of regular languages is thus compressable to a theoretical limit, and, since there exists an encoding for regular language parsers in string format, there must exist a Kolmogorov complexity measure for a given recognizer. The *encoding* seems to matter greatly however -- from what I think, Chaitin's insight about the halting probability is that the encoding and machine in question used to implement the computation system affects the properties of computation for the system it is describing itself. To me, this speaks to how language implementations must always inherit the properties of the native system, and then build its own systems on top of that.

Gregory Chaitin's work on the Halting Probability of an arbitrary Turing-complete computation system is probably a good place to start. The result that the halting probability for any specific Turing-complete system is arbitrary and random.

John Tromp seems to have published a paper back in 2007 about the lambda calculus using Chaitin's notion of algorithmic information theory and Kolmogorov complexity with his own given encoding.

It seems like there's a lot of work in the field -- Paul Vitanyi seems to have contributed to a textbook that's foundation to the field.

",1514637606.0
noop_noob,"My first idea is that the complexity of a function is the number of mathematical symbols needed to define it, given some fixed notation.

However, the number of functions between two infinite sets (e.g., functions from integers to integers) is uncountable. Therefore, most such functions are uncomputable and undefinable, and would not have a complexity under this definition.",1514663372.0
pngwen,"All computable functions are representable as Turing Machines.  Also, all Turing Machines are representations of computable functions.  Therefore the classes of TM and CF are equivalent, hence all notions of Kolomogorov complexity are in fact about functions.

See A.M. Turing's ""On Computable Numbers With an Application to the Entscheidungsproblem"" and Alonzo Church's ""An Unsolvable Problem in Elementary Number Theory"" to see that relationship.

If you want to see how far the rabbit hole goes, read my reading list outlined on my theory of computation course page: https://cs.maryvillecollege.edu/wiki/Theory/fall2017",1514672409.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1514586918.0
cogman10,"1.  Using copy on write, the copy can be avoided in some cases right up to the point where the data mutated. AFAIK, not many languages expose or even use COW.  I've mostly only ever seen it referenced in OS design for things like process forking.

2. Academics say to always use a linked list.  However, practically, Array lists are almost always preferred.  The problem with linked lists is that they are not cache friendly.  In the worst case, your CPU has to go out and hit main memory for every single node accessed (millions of cycles per node!).  

  In contrast Array lists keep all the nodes in a contiguous block of memory.  CPUs, when they access memory, will pull in a block of memory into cache.  For traversal of an array, the cost can easily be in the 10s of cylces per element.

  Further, the cost of adding to an array list isn't as bad as you might think.  These lists will often overallocate so they can avoid copying over the data when you hit the end of the array.

  And in practice, middle or beginning insertion just isn't all that common.  You almost always do tail insertions with lists and very often you don't do any deletions at all!

  > So how do you index a linked list, so that deletion is ACTUALLY constant time?

  You iterate and delete/split at the same time.  You still have n time, but you don't end up with 2*n time.  That is pretty much the case for a linked list, when you want to frequently go through the list and remove, split, or merge the list at various points based on the current element.  Again, in practice this happens almost never.

  > also, is a language's implementation, like JS, of an array ever a linked list?

  Depends.  In fact, in javascript it could easily start out as a linked list and later be transformed into an array list based on jit optimizations.",1514586421.0
olliej,"From experience in implementation (JSC but I believe it’s similar in other engines) array.shift and unshift are amortized but generally O(1). Splice is interesting that engines will try to avoid “real” splicing depending on how the outputs are used
",1514587891.0
crabbone,"It's been a long time, and the situation is probably different today.  There was a Tamarin project by Mozilla and Macromedia, which would become the next JavaScript engine (we are talking 2005-ish).  At the time, the way to implement JavaScript ""Array"" was to use a hash-table.  This is because you could do things like:

    var x = [];
    x[100000000] = 1;

And you wouldn't expect that to take zillions of Megabytes of memory.  In this light, `splice` would've been an amortized constant time operation, because it would amount to deletion of an element from a hash-table.

JavaScript engines changed since then, many times over.  So, who knows what do they actually use today to implement ""Array"".  And, this would be the general advise: in languages, which are too far removed from hardware, you need to test, and you need to test taking into account the expected problem size.  There could be so many unforeseen gotchas, not covered by simplistic theory of what arrays or other data structures should be, that you should only really use this theory as a working hypothesis, but not as a reliable source.",1514736750.0
SlothenAround,"Yeah definitely don’t think “not smart enough” is a thing you should worry about. Your passion for the subject will help you master it, and will help you be committed to bettering yourself. And any employer in the field will care WAY more about your passion and experience than your grades.

Source: I’m graduating with a computer engineering degree in May with a C average, but I have worked really hard to network myself and love the material, and I will have a job out of school, while many of my classmates with higher grades won’t.",1514577576.0
NowImAllSet,"I've been there before. Hard work trumps intelligence any day. Someone who's brilliant but lazy is going to be out played by someone who's average intelligence but works hard. I think if you have an honest passion and work ethic to put the time in, you can make up any gap in intelligence *and some.* ",1514575788.0
MockingBird421,"1) you're definitely going to be fine, I came from a very similar background to you 2) (and the reason I replying) look up impostor syndrome, it happens to the best of us (pun intended)",1514580073.0
FieryPhoenix7,Theory of computation is very fascinating indeed. I agree with everyone who said hard word trumps intelligence. ,1514579137.0
yakattak,"I feel like I related pretty well to your post here. I had an average GPA (< 3.0). I constantly felt really stupid in my math and more theory based CS classes. However, in the more practical classes, I fell in love. I was deeply passionate about them and like you, I dove very deep into the classes. 

Lots of times in school I would feel behind because of a lack of knowledge (specifically math, I’m god awful at math). In particular it was most likely because I lacked the passion

There is a big difference between knowledge, and your ability to learn. I found that while I lacked a lot of knowledge going into college (slacking off in my grade school years), I learned a lot with the correct drive and passion. 

In other words, just because you lack knowledge in something, it doesn’t mean you’re stupid. It simply means you have an opportunity to learn more and that is never a bad thing.

My other bit of advice is, do not be afraid to ask any questions. If you have a question but the class ended, ask the professor after class. Being curious is one of the best ways to learn.

I leave you with the corny (but super important!) proverb below. 

> He who asks a question is a fool for five minutes; he who does not ask a question remains a fool forever. – Chinese Proverb",1514611250.0
redditEnergy,High intelligence comes from hard work someone probably thinks you are a genius just like you believe others are. ,1514578950.0
fly2807,Grades ! =  skills. Remember that ,1514607274.0
ColdDemon388,"Another guy you may have heard of who didn't get particularly high grades is Elon Musk. His philosophy is if you work harder than everyone else, you'll be successful.",1514612353.0
,"My grandfather always said, ""Do what you love and the money will take care of itself.""

A friend of mine recently told me, ""Do what you're good at, and you'll learn to love it.""

I think the answer is hopefully something from one of those bags and not ""I DON""T KNOW, I""M SCARED, HERE WE ARE, THIS IS WHAT I""M DOING, AAAAAHHHH""

Can you live with not being the best in your field? Can you live with progressing in the subject outside of the wind chamber of motivation that is grade-assessed, pre-digested coursework? Are you okay making less and (probably) teaching? Because, to be an academic is usually to commit to those things. Those aren't terrible caveats...but they're real.

Being an engineer has *different* caveats. Can you live with working on someone else's product how they say, when they say? Can you live with conforming your education and interests to building a marketable skillset? 

Everything's a tradeoff. I'd say, understand the risks in whatever your options are, and pick the thing whose worst cases you can live with. 

My 2 cents though...a lot of people live their entire lives without finding something they're truly passionate about. Don't discount it.",1514617260.0
bigkoi,"From a guy that sucked at math and didn't think he could be a computer scienctist...

Go for it!!!

I had the same doubt about myself before going into Computer Science.   It will be lots of late nights studying, but I'd it's your passion you will enjoy it and people will recognize your work ethic and passion.   Don't underestimate your potential when passionate about a subject or career.",1514580963.0
DarkMaster22,"I'll echo the others in the thread and say that you'll be more than fine. If you'll keep this level of enthusiasm and willing to work hard you'll probably do much better than someone with inelegance alone.

Source: The other side of the fence, was considered intelligent but got bored quickly from my subject and was slacking off.",1514582082.0
semanticsdumbdumb,"Keep in mind that the German university process is heavily grade based. At my German uni, professors essentially flat out reject you as a thesis advisor or as a potential PhD student if you haven't gotten a 1.0 or a 1.3 in their class.

But I'd say get out of the German system after your undergraduate anyway since it's pedagogically terrible and there are generally better post undergrad options in Europe or the US, if that's available to you. I'm doing my MSc in CS and I've found the quality of education here shockingly bad.

I also fell on love with my theory of computation class. That shit is the shit. Unfortunately, there aren't a lot of classes like it. There are of course tons of advanced complexity theory classes...but I enjoyed the computation side of things (i.e. what is computable) over complexity theory in that course...and it's hard to get your Turing machine fix in other classes. Lambda calculus might be interesting to you.",1514583590.0
pun-doctor,"There's no need to worry about being smart. You've got the passion, and just work towards it a step at a time. 
I'm an undergraduate too and I just fell in love with compilers and programming languages, I didn't have a second thought and I'm now pursuing research and building tools in that area. 

It might sound preachy, but work hard towards what you love and you won't have to worry about smartness.",1514585993.0
,"Hey friend, I think you and I are in a pretty similar situation. I'm 23 and I had roughly the same feeling as you, a year ago. Here's where I'm at now:

You know you love computer science, and obviously you're smart enough for it because you have passion and opinions about the field. That's not the hard part; you know for a fact that if you were to apply yourself to building the knowledge, you would already have the passion and understanding. You would do well.

The hard part is applying yourself. And that's okay! That is what I have learned this past year. The kinds of question I'm asking myself as I go into my Winter quarter at uni is ""what does it mean to apply myself?"", ""how do i go about applying myself?"". These are the sorts of questions we Computer Scientists live to answer, after all. ;)",1514592409.0
MyTribeCalledQuest,"All that matters is your passion and work ethic. One of my favorite and most knowledgeable coworkers *never even graduated high school*.

Do what you love, don't worry what others think.",1514593493.0
shadowbannedlol,"what do you have to lose by pursuing it? you can always fall back to applied computing, which is an enviable position. ",1514609245.0
VermillionAzure,"At least in the US, graduate school is heavily influenced by your adviser and research community. Finding an adviser that will guide you, treat you well, and can get you connections can be difficult.

Why not do undergraduate research?",1514598339.0
terserterseness,"Go for it; you won’t be sorry you did. I had and still have to do pretty heavy lifting to work with formal verification as I am also not brilliant, but hard work and a lot of practice (which I find fun to do) pays off. ",1514606663.0
curt94,"There is a huge percentage of ""developers"" who don't know what Big O is or even that multiple sorting algorithms exist.  These people make good money.  You will be just fine.",1514622300.0
CorrSurfer,"I'd like to add some Germany-specific information to this thread.

First of all, not feeling smart enough to do academic work is perfectly normal in Academia, especially in theory. Before you can do proper research work in some sub-area (which is the only way to contribute to the state of the art), you need to develop an intuition for the standard approaches that sub-area and know the recent work. And this is always a lot. 

I think that the main question that you should be asking yourself is what you want to do with your degree. If it is fine with you to spend a few years researching and then to move to ""industry"", this is normally doable. There are often openings for ""Wissenschaftliche Mitarbeiter"" positions that are suitable for researching towards your PhD while working in such a job (once you have an MSc degree). You just need to prepare!

1. Specialize on one, or better two sub-fields of CS theory. Take all courses offered in the area.
2. Tell the relevant professors that you would be interested in getting a ""HiWi"" position connected to research in case they have an open position. You will get paid a little bit and earn experience.
3. Write both your BSc and MSc theses on relevant topics.

Others wrote in this thread that Professors will not take you unless you have 1.0 or 1.3 grades in the relevant subjects. This may be true at the very good CS departments in Germany. But at other universities, this is often not the case. If for example a good researcher at a medium-good university gets a huge research grant, they are typically unable to fill all of the positions and are already happy if they have more than one applicant who took *any* of the relevant courses. This is especially true for more junior professors, who have not established a ""pipeline"" of suitable candidates yet. If you then apply with a proof that you can do research in his/her rough area, you have an excellent chance of being selected.

The difficult part comes after getting a PhD. Competition among the permanent researcher positions is super tough, so you need an exit strategy. The best way to achieve this is to work on applied topics that are still relevant to your theoretical research on the side.",1514634663.0
Absuurdist,"Your learning trajectory is more important than your starting point, and your trajectory is already fine.

If you're having problems with learning the nitty gritty, I'd recommend reading the book ""A Mind for Numbers"" by Barbara Oakley, and also go through the online course ""Learning How to Learn"" alongside it which the author helped design.

The techniques you learn from those materials should help you greatly in mastering difficult material.",1514655574.0
shbilal001,"You indeed have a bright future if you choose to computer scientist as your profession. Being a data scientist is one of the most paying professions in the world currently and the figure is growing, according to Forbes website. You too can find yourself receiving a very handsome paycheck if you get the knowledge and have some experience in your field of study as a computer scientist. But first, you will need to acquire the knowledge first by going to a good programming school. I would strongly urge that you enroll in Holberton School (https://www.holbertonschool.com/) and learn the concepts and fundamental. You will get tutors and friends who will help you as learn up to completion. Depending on what field you decide to major in you may want to start creating exciting small applications that you and your friends can use. You will boost your skills, experience and knowledge in building applications to help prepare you for big projects that you will get to do for big companies or your very own company. Artificial Intelligence and Robotics are already here with us and many big co-corporations are rushing to be the first to create a working technology. So year there is a big future and the future is here to stay with us. ",1515515826.0
loquinatus,"Lots of great comments here, definitely hard work is more important. You sound like a wonderful student, I'm sure you have nothing to fear. Your worries are based on three assumptions if I understand right.

1. Intelligence is necessary to be successful in CS - not true, see all evidence in comments

2. Intelligence is set and immutable - absolutely not true, or at least currently under debate 

3. You're ""not smart enough"" - Grades are a poor proxy for intelligence. Even IQ tests, designed to measure intelligence, only do a half decent job. Grades only matter until you get experience anyway.

Check this out, it's an interesting perspective on success (hackernews is like better Reddit for CS if you weren't familiar) 
https://news.ycombinator.com/item?id=15894396

Pursue your passion in this field! To solve hard problems, most of the time you need hard work, courage and curiosity. You've got all of that, you're good to go.",1514596320.0
specialLimit,"Regis University (Denver, CO) has as OS course that is 100% online and ABET accredited. I took it myself a few semesters back. ",1514766276.0
jake_schurch,Something happen?,1514543539.0
matheussilvapb,"Virtually, there's no way. When you make a transaction for example, to Alice, you only need to know Alice's public key and that gives you almost no information about Alice. ",1514565675.0
WhackAMoleE,"Blockchain analysis. 

http://www.livebitcoinnews.com/possible-de-anonymize-100-bitcoin-transactions/",1514575012.0
EastCoast2300,tracking the location of a bitcoin wallet is physically impossible.,1514605754.0
Natethesnake81,I really want to hear the answer to this.,1514538377.0
haram6e,"I would say, at it's core, computer science is a lot of problem solving and invloves learning about how you would approach different kinds of problems in the most efficient ways. Coding is the means to get this logic to work on a computer. That being said, your brother will not have a great time studying computer science if he is averse to coding because it is a big part of the curriculum.

Source: am a cs major in college right now",1514534899.0
FieryPhoenix7,"It's absolutely not just programming. Code is a tool you use to solve problems. Computer science is perhaps better named 'computing science,' because it's not even really about computers. Logic, discrete math, and abstraction are at the core of computer science as a field. 

That said, you do need a passion for programming to get through a typical college program in CS. If your brother doesn't like programming one bit, he should consider a different but related major, such as MIS or electrical engineering. ",1514536813.0
CorrSurfer,"Allow me to add the perspective of a researcher in computer science to answer the part of the question that asks how much programming there is do be done in computer science.

In research, you spend most time on the stuff that you are *bad* at. A typical project will include literature research, coming up and designing a solution to an important problem, and implementing it. The latter means programming in many sub-fields of CS (in theoretical CS, it could mean doing a complexity analysis instead).

If you are good at diving into the literature and coming up with cool solution concepts, but bad at programming, you will spend 90% of the time programming (because you are bad at it).

If you are really good at programming but bad at the rest, you will spend ~5% of your time programming.

This is why in research, you need to be able to do *everything* that a computer scientist does in order to be productive.
",1514544633.0
groggyjava,"computer science is the physics, so to speak, of computation. 

what you do with it is engineering.

once you engineer something you have to build -- that's programming.

(my comp sci prof in 1986 said -- the first 80% of any project is planning, the other 80% is coding and debugging)",1514578456.0
arcticfox,"Computer science is problem solving.  It is expressing problems in a computable form in order to synthesize solutions that can be computed by an unthinking machine.  Computer science is about computability and how problems can be solved in an effective manner.  It is about understanding that there are classes of problems that cannot be computed ""easily"" and some not at all.

Computer science is about dealing with problems of immense complexity and making a convincing argument that you have covered all cases (or at least, the majority of the significant ones).  It is about understanding abstraction and system thinking, and how those ideas can be applied to remove apparent complexity.  

Computer science is about philosophy and logic.  Computer science is about communication.  And yes, a small part of computer science involves programming.
",1514535027.0
BrightLord_Wyn,"Here's the gist of it. CS majors typically go into programming after college (I would say at least 80-90% of the entry level jobs you gain access to after college will be programming jobs). Unless your brother is planning on getting a PhD and doing CS research, then your brother will not want to pursue a CS degree.",1514563287.0
TarMil,"By the way, why do you want your brother to learn CS if he doesn't want to?",1514569595.0
DSrcl,"It's possible to graduate as a CS major in my school (top-5 in US) with only 4 classes that *involve* programming, and to spend the rest of the 2 or 3 years doing proofs and writing pseudo-code. That's how little programming it involves. Saying CS is just programming is in the same vein as saying history (the major) is just writing. ",1514574755.0
mcandre,A fair bit,1514561208.0
wavy_lines,"Well, it is mostly programming, but what do you mean by ""just programming""?

That is like saying that math is ""just writing weird greek symbols"". ok, on a superficial level, it could pass a ""fact check"" or something, but that kind of misses the point.

Computer science is about the fundamentals of computation and how they relate to modern computers.

You will learn things like how the machine executes your program at the low level. How the internet works at the low level. How to analyze algorithms. You'll learn various ways to structure data.

Some people think programming is about using some library that already exists and doing some low effort project with it. If that's your idea of programming, that's not really what you will learn.

A lot of what you learn in Computer Science can feel like it's ""dated"". That's because you're so used to being a *consumer* of technology and not a *producer*. So when you get down and dirty it feels like you're using something archaic. But that's what it takes to produce real stuff.

Computer science education, when received properly, will enable you to _create_ a programming language and web framework (if you're so inclined to, and have the time and energy for it).
",1514569162.0
metaphorm,"CS is fundamentally a field of applied mathematics. My own CS program had relatively little coursework that was just coding and I wish it had more! 

What we focused on, primarily, was a bunch of math courses. The usual calculus sequence, and then also Discrete Math, Linear Algebra, Numerical Methods/Approximations, Theory of Computation, Probability and Statistics, and a choice of a few different math electives (I did one in Financial Math and another in Bio-informatics). 

The core CS courses themselves were also very mathy and involved a bunch of proofs and stuff in addition to coding assignments. The core here was Data Structures, Algorithms, Functional Programming and Logic Proofs, and a hardware lab course. 

Electives were all over the place but I took one on Operating Systems and Concurrency that was also very math-oriented. Mostly it was about proving that certain kinds of algorithms, messaging protocols, or data structures could be considered sound in that they avoided some known problems (deadlocks, livelocks, starvation/fairness, eventual consistency, etc.) 

Then there were some courses that were taught more pragmatically and focused a lot on coding. I took a course called ""Software Engineering"" that was about development methodologies (though it was useless and very dated), and a few electives in applications so Database stuff, Web stuff, etc. 

",1514571239.0
SesinePowTevahI,"Computer science is ""just programming"" in the way that the study of literature is ""just reading"". It's probably what you spend most of your time on, but really it's just a way of expressing the more abstract computational ideas we deal with.",1514571971.0
droppingbasses,"Programming is to a computer scientist as a telescope is to an astronomer. Yes, the astronomer will need to learn how to use a telescope to the fullest extent but don’t forget that they are there to study the stars. ",1514578571.0
gerusz,"More than mathematicians think, but less than self-taught coders assume. You'll definitely be expected to at least be able to code a proof-of-concept.",1514578638.0
flaming_sousa,"

Computer Science is essentially the study of figuring things out, programing is simply a way to figure them out.

CS teaches problem solving, analyzing solutions, and picking the best one for a situation. It is the flip side of higher level math. You have a problem (I need to get to place X, I'm at place Y), there are hundreds of different ways to find the path between X and Y.  You come up with a way, and then prove your way is the best.

Software Engineering teaches how to build and organize complex systems.  That isn't necessarily the same thing as problem solving; it might be how to organize people, articulating ideas, etc. Building Reddit for example; creating a platform for thousands of groups to talk and share isn't easy. Software Engineering focuses on best practices to do that.

Programing is vital to Software Engineering, but not CS. (There is little to no mention of programming in these cases; CS uses theoretical constructs called Turing Machines to act as computers, it makes our lives easier)





",1514578698.0
ImaginationGeek,"I'm a CS prof., teaching mostly computer systems and related courses, and I also teach some section of the intro course.  Here's my take on it...

Some definitions, as I will be using them:
- ""coding"" is taking a problem that *you* know how to solve and writing instructions (i.e., source code) that explains how to solve the problem in a language that the computer can understand
- ""programming"" is the process of creating a program (duh.) which involves ""coding"", but also the things you do before that (like planning) and after that (like testing & debugging)
- ""software development"" is the process of creating large, complex programs as part of a team, and usually in a business environment, using formalized processes to facilitate teamwork and business needs (so now, for example, instead of just ""testing"" you have unit vs. integration testing, BVTs, CITs, verification, validation, etc.)

If you are a ""programmer"" but not a ""computer scientist"" then likely all you know is ""programming"" or ""software development"" (but the latter only after some industry experience).  If you take a Computer Science major in university, you will learn to ""program"" in about a year (roughly, depending on the program).  (A Computer Science major in university may also need some industry experience before they really grok ""software development"", although different CS programs do better or worse jobs of teaching this.)

The remaining 3-or-so years in a CS degree involves learning the following things:

1. more intellectual tools you can use to write more advanced programs (e.g., if you realize something you're doing has an analogy to virtual memory that you learned about in OS class, then you can decide to use a data structure that looks similar to a page table)

2. and understanding of the systems that run your code (operating systems, networking, compilers, how languages work, a little bit of architecture) which helps to write code that performs well and avoids some of the weird obscure bugs that can come up uncommonly, but often enough, in complex or low-level code

3. intellectual tools to help you analyze your code for correctness and performance (algorithmic and complexity theory, discrete math, but also things like how to measure and evaluate systems, etc.)

4. the real kicker: everything a ""programmer"" does (per what I said above) is writing programs that solve problems they basically already understand how to solve.  A *big* piece of a Computer Science curriculum is learning how to solve more different kinds of problems
",1514592424.0
abhinavrajagopal," ""Programming is to learning computer science, as writing complete sentences is to learning English literature.""",1514841343.0
fj333,"How much of carpentry is sawing?

Not all, by any means. But if you hate sawing... don't study carpentry.",1514900880.0
libeako,"""Programming"" = (creating program) = ""software development"". Including the theoretical problem solving and practical implementation, everything. So ""programming"" is the very wide notion.

In my opinion programming consists of 2 fields : software engineering, computer science.

""Software engineering"" [a.k.a : ""coding""] is the practical side of creating a program : knowing your programming language well, the compiler flags, the available libraries, search for bugs, handling versions, deployment.

""Computer science"" is the mathematics behind programming. And boy there is a lot. For algorithms [including artificial intelligence, data mining] quantitative math is basic knowledge [analysis, linear algebra, probability theory, ... ], but also ""algorithms"" is a sub-field itself. Even coding is helped by a lot of math [category theory, abstract algebra] as mathematical sciences of efficient coding abstractions [see Haskell].",1515165260.0
shbilal001,"Computer Science is all about computers and the science of solving problems using modern computer applications. In general, I would say it would be impossible to go through a computer Science course without having to write a single line of code. There may be a number of topics which are more theoretical and don’t require coding but I don’t see how one would want to do Computer Science without wanting to do some bit of coding. Where is the fun in all that? Even engineering courses require students to create programs and write lines of code. In order to learn efficient programming, I would encourage you to enroll in a good programming school like Holberton School (https://www.holbertonschool.com/) and learn all you can on Computer Science. You may not have to like programming but by taking simple steps, you will improve your attitude towards that and in time you will be one good programmer. If you just do not wish to do programming, you have an option of enrolling in the school above but major on a different field which does not require lots of programming like Networking and Computer Hardware specialists. These fields do not require any coding at all, but other computer skills. Good luck and all the best.",1515618119.0
byzantinian,"39 credits worth of classes that involved programming that I had to pass to graduate in CS last May. If you can't or won't program you won't graduate, simple as that.

Edit: clarified because I'm on /r/Compsci not /r/SDSU and I'm retarded.",1514566769.0
matthew349hall,"If he doesn't like programming he shouldn't do CS. Doesn't matter what anyone says about problem solving and such, if a Mechanical Engineer didn't like CAD that might be a problem.",1514570003.0
Farsyte,"Oh, that brings back memories -- Fortran was my first language, and I did a lot of mixed Fortran/C work up to about ten years ago. I can confirm some of your hypotheticals from the readme files:

 1. Yes, you need to link the Fortran runtime into your program, when you are linking in object files produced by the Fortran compiler -- the compiler may make calls into the runtime for some intrinsic functions. You might *get away with* not doing this for some files, but the surprise should be the cases where it is not needed. There are similar issues for including occasional C++ objects in a C program. I'd expect the same for other language mixes.

 This goes double for mixing Fortran with C++, you still end up needing both the Fortran and C++ runtime libraries, which is probably why you had problems with the build in test4: if you are linking with gfortran, you'd have to also tell it to include the C++ runtime libraries; so if you have C++ code use ""g++"" to do the linkage step. It's too bad the GCC front end can't infer the set of runtime libraries needed by looking at the object files :(

 2. It was ""FORTRAN"" when I learned SDS Extended FORTRAN II back in 1974(ish), but by the time folks shifted to the 1977 standard, ""Fortran"" was getting more common. By the time the 1995 standard came into more common use, using FORTRAN was a sign that you were an old greybeard too caught up in ancient usage to move with the times ...

 3. POINTERS: Yep, Fortran is call-by-reference where C is call-by-value, so a C function called by Fortran will have a  bunch of pointer arguments, as will the C representation of the Fortran functions that it calls.

 4. Yes, Fortran 77 is probably the most common dialect you will encounter, in my experience. If you hit some Fortran of a newer version of the language, your compiler will inform you (probably by throwing a tantrum). Personally ... I only hit hard core Fortran 95 code once, to fix a small bug in it, so I still expect Fortran code to be Fortran-77 (and have to go dive into langauge documents when faced with F95 or newer). Some of the Fortran code in your repo looks like Fortran 90 or Fortran 95 or maybe newer. Not sure. I'm an F77 bigot ;)

 I'd be willing to bet that GNU Fortran defaults to expecting Fortran 77, and has parameters you would use to tell it that you code uses a more recent version. I don't have it installed at the moment, so can't check, but it should be easy enough to find out if you are curious.

 5. STRINGS: When your C code called from Fortran gets a string, it's not necessarily a good idea to dump a '\0' at the end; it's better if you can write your code to not presume you have a 0-terminated string. Pretty much only scribble at the end of the string, if you also maintain the Fortran code that calls it, so you can make sure the array is big enough ;)

 6. C ""extern"" and Fortran equivalent ... yeah, classical Fortran, you just called the subroutine, and the linker found it in the libraries for you when you linked the program. Apparently ""INTERFACE TO"" was added to Fortran, in the 1990 version I think, as an equivalent on the Fortran side, so that Fortran programmers could get all the benefits of the compiler knowing the correct data type for subroutine arguments. I must admit to never having seen INTERFACE TO before today, but knowing (and having battled) the challenges involved, I can see that it's a good thing. I like that it can be used to tell Fortran to please use ""call by value"" semantics for parameters, so we don't have to do the pointer stuff on the C side.

 7. When you see comments in C code mentioning ""st\_ruct"" you just *know* that some irritated programmer had to do a global search and replace of ""st"" to ""st\_"" because their Fortran compiler changed how it did name mangling ... I sense code from Cray! ;)

You now know everything I remember about writing C and Fortran (and C++) that plays together ;)

[edit: i need to find the ""preview"" button on Reddit ;) ]
",1514562802.0
zagbag,The expression in your avatar upset me.,1514564811.0
QSCFE,"I'm not computer science student (school drop out right now but I'm passionate about computer science) suggest me a books about the history of theoretical computer science, artificial intelligence, machine learning and deep learning. 

",1514572456.0
CRAZYhunteeerr,"Where to start?
It's a new year and would love some pointers or advice to get be confident by the end of the year or maybe half year. Any youtube channels and websites would help aswell! ",1515048703.0
harrisonthedingus,"basically integrate AI into anything that already exists, its what most comp-sci startups are nowadays.",1514501623.0
GrokFu,"Funny thing is I did all of that stuff in school and that’s not what I get $75/hr gigs to do - most of that is fixing someone’s spaghetti code or missing null checks, or optimizing SQL for a report that runs for 5 minutes. So while my heart bleeds piss for employers that are frustrated at the lack of pointer and recursion mastery, I think you’ll find (like anything else) that a good “java school” student can be molded into the kind of expert that you need. Otherwise, I reserve the right to bitch and complain about all of the grads who can shred linked lists, write recursive algorithms, and know pointers like the back of their hand but can’t do _______ just because they haven’t had that exposure. Yeah yeah yeah Google and MS need rocket scientist, but most of our industry is more like a dilbert cartoon with sociopaths running the show, poor or zero requirements, horrible personalities at every level, a spaghetti codebase, lack of process and a few jobs here and there that are actually good where, I’m sorry but, you’re still probably not writing your own hash tables or doing pointer arithmetic.  I’m cynical after many years of doing this, but it’s true.",1514482481.0
badlawnchair,"Reading the article, I got the vibe that the major schools that were singled out do ONLY Java. Is this really the case? I think Java is great for OO fundamentals, but it’s hard to imagine a reputable school that doesn’t have a weedout class in C or C++.",1514490806.0
jordanaustino,"Well fwiw I went to a school that taught in C and did pointers and such but uhh the kids who sucked st pointers somehow still passed cause university couldn't have classes where more than half the class failed. 

End result people who can't program very well graduate.",1514499252.0
All_things_taken_now,"As someone who didn't know that they were going to major in CS going into college, and didn't go to a big school at all, shit like this makes me nervous. I feel under-educated for making a decision when I had no knowledge going into schools. Is there other people in my position, that have gotten jobs in the field and can offer any advice?",1514493955.0
firecopy,I have never seen a university teach only Java in their cs curriculum.,1514743624.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1514425159.0
rieslingatkos,**[Computational Complexity](https://en.wikipedia.org/wiki/Computational_complexity)**,1514414611.0
kevstev,"[Algorithms and Data Structures]( https://www.amazon.com/Introduction-Algorithms-3rd-MIT-Press/dp/0262033844/ref=pd_sim_14_9?_encoding=UTF8&psc=1&refRID=XZ4ATD0B8WC2Q20768MV )


[Compiler Design](https://www.amazon.com/Compilers-Principles-Techniques-Tools-2nd/dp/0321486811/ref=pd_sim_14_44?_encoding=UTF8&psc=1&refRID=XVZ3Q6XJM0951JYS6EVN) how does code get from print(""Hello World!"") to machine instructions.



[Hardware design] (https://www.amazon.com/Computer-Organization-Design-MIPS-Fifth/dp/0124077269)

You can probably find all of these books as pdfs if you look hard enough... 

Those are the big ones in my eyes. Understanding how the internet works is pretty fundamental these days- look to the Stevens book(s) on TCP/IP for that.

Database systems would be another very fundamental area these days.  

",1514413485.0
DSrcl,Not directly answering your question because it's hard and I am lazy. http://jeffe.cs.illinois.edu/teaching/algorithms/,1514414703.0
t_montana,Abstraction,1514422360.0
phao,"I'm not so sure there is such a thing.

- If you're talking about a CS student who is going to get an undergrad degree and go to industry to work as a software project manager, then that's a set of ""fundamentals"".
- If you're talking about a CS student who is going for a master and then a doctorate program on some part of theoretical computer sciente, then it's another set of fundamentals.
- I had some friends who got a CS degree and went on to work on security and networking. Again, another set of fundamentals.
- If you're going to get a CS degree and then move on to work/research data bases, then you'll have an yet different set of fundamentals.
- If you're going to get a CS degree and then go to the industry to become a software developer, ...

I know some people who went to some of these areas. Some of them basically never program, or only do some basic programming. Some others program a lot, but barely have to invent or use any elaborate data structure or algorithm. Some others explicitly use AI techniques, but most don't. Non-trivial knowledge about the hardware is often unnecessary. Similar things for ""algorithms"", that is, generally speaking, basic knowledge of algorithms was enough (lots of things are already developed and put on  libraries), also, generally speaking, you can get by with a basic (no non-trivial topics) on complexity theory. *All* these things have exceptions (e.g. if you're going to be an AI researcher, then basic knowledge of AI won't do, obviously), but it's very hard to find some things which **every CS person has to know**. The field is too diverse.

You could argue that basic programming knowledge is useful to everyone. Many CS professors though don't program. You can argue that makes them incompetent or something, but a lot of theoretical CS won't involve programming (at least not in the traditional sense; these professors might get by with writing some pseudo-code outlines).

Here is the thing, if we are to take your question seriously, then if we consider a particular CS subject to put it on ""what every CS person has to know"", then we have to think of several drastically different but common or relatively common scenarios and ask ourselves ""is this subject useful here?"". So the kinds of things CS people go on to do (I've seen people majoring in CS doing all of these; and more):

- research in AI
- work in a company doing software project management
- work in a company as a DBA
- research in computer graphics
- work in security
- web development
- research in optimization
- game programming
- research in computational geometry
- work with geographical data basses (both in research and in a company)
- research in graph theory
- programming ""standard"" data base applications for business (lots of programmers end up doing this)
- ...

For all of those, consider matters from basic to complex tasks. For example, for a web developer, consider developing web sites ranging from simple presentation sites to complicated things like google, or web archive, or facebook. For a game developer, consider developing call of duty, world of warcraft, going all the way to candy crush and flappy bird. For a researcher, consider most of them who don't do any groundbreaking work to the few who are significantly contributing to whatever field they're working on.

Now, consider that a game developer might have to know about some of the path finding algorithms, like A*. And that might be part of fundamental knowledge for an AI researcher, but is it for a web developer? A standard algorithms course will (afaik) cover balanced search tress, but is that really useful to everyone in there? Even common standard search tree strucutres. A lot of people can do their work and come up with impressive things just by knowing what to expect from and how to use map strucutres (some times implemented through a tree data structrue) without knowing how it's implemented inside.

Consider expanding that list yourself. Put on some relatively common positions, and remove the ones you don't think are common. Whenever you find something you think everyone should know, then ask yourself if everyone in those areas have to then take the time to learn this thing you're considering. Take into account oportunity cost (which people should always do, but often forget). I'm guessing the answer will be no for **a lot** of things.",1514429373.0
simplethingsoflife,"Data structures. That was my ""matrix moment"" where it literally all made sense immediately. I've heard others (even comp sci profs) say the same.",1514430010.0
linkedlister,Data Structures and Algorithms.,1514441216.0
,[deleted],1514435092.0
Coloneljesus,First order logic,1514474762.0
,Recursion,1514418272.0
flekkzo,"These are a few things that I have found to be important:

Low coupling, high cohesion. Break this at your own peril. Many of the hard to understand and fragile frameworks out there breaks this to bits. Deviating from this requires a fantastic argument. 

MVC. Truly learn it until you understand it. So much fragile code comes from not understanding it (and the endless invention of alternatives that aren't actually better at all).

Keep it simple stupid. Simple is better. Much faster to code, debug, and expand. Overdesign is your enemy at all times. 

Understanding algorithms. From search to crypto, understand what they do and why. Not knowing will make you break LCHC and KISS easily. Also never invoke ""premature optimization"" as an excuse for poor algorithms or sloppy code. Short, concise, readable, maintainable. That's words that should describe your code. Make smart decisions.

Understanding the cost of things. No matter if you are a Lone Ranger or part of the machinery, understand the cost of things. How much server time will this eat up? How expensive is this to implement and maintain? Modify? And my personal favorite, the hidden cost of third party frameworks.

Do a job or seek perfection. Don't do both. If you are costantly using the latest fashionable bleeding edge stuff you constantly pour time and effort into something else than the product. Don't spend a week to save an hour. Don't make your users the alpha testers for some new framework you didn't even write. Dedicate separate time for learning and evaluating new things. And know the cost.

Phew, that's more text than I anticipated :) It's what's served me well and good luck! And remember it's a brain dumb, no more, no less. ",1514420582.0
bondolo,"Product lifecycle, development process, testing and quality. Books such as ""Code Complete"" and ""The Pragmatic Programmer"" plus classics like ""Mythical Man Month"" and ""Peopleware"".

That plus an education in what not to do (don't recreate CORBA, ""Eight Fallacies of
Distributed Computing"", etc.) will serve you well. This leads to suggesting books like ""Effective Java"" or ""Effective C++"" and ""Design Patterns"".

I often feel that an emphasis on algorithms is too abstract and not tied to good judgment. Having the perfect data structure is often very weakly correlated to delivering a success. There are a multitude of factors in building software system that must be balanced and prioritized of which algorithms is just one. I always keep [this story](http://franklinchen.com/blog/2011/12/08/revisiting-knuth-and-mcilroys-word-count-programs/) in mind.",1514424474.0
HorrendousRex,"This is _not_ a CS concept but rather a software design principle, but it bears repeating since you are a self-learning coder: ""Don't Repeat Yourself (DRY)"". It isn't always 100% accurate but more often than not, if you find yourself repeating the same line of code more than once, you should refactor your code so that you only need to write it once and then just reference where you wrote it.",1514475940.0
PM_ME_UR_OBSIDIAN,"Approximation, as in Domain Theory, Complexity Theory, and basically every single subfield of computer science. But this is more a philosophy of science concern, because it's equally as relevant in physics, chemistry, biology, etc.

A short writeup: [The Map is not the Territory](https://wiki.lesswrong.com/wiki/The_map_is_not_the_territory) on *Less Wrong*.

___

What computer science *really* is. Philip Wadler argues that the name is badly chosen: ""it's not about computers, and it's not science"". He prefers *informatics*, for ""information mathematics"".

Dijkstra: ""computer science is no more about computers than astronomy is about telescopes.""

___

Reductionism. The idea that problems can *generally* be decomposed into independent parts, and the habit of doing so.

___

Algorithmics: the idea that procedures to obtain a success can usually be encoded in reproducible, general ways.

___

**Invariants**, probably the most underrated idea of this lineup. Tracking which facts remain true as a situation evolves (or as an algorithm executes, etc.)

Currently, the best way for a CS student to learn in depth about invariants is probably Benjamin C. Pierce's workbook *Logical Foundations*.",1514476001.0
eviltofu,Binary number systems?,1514439629.0
Jamie089,"I found these very interesting and helpful. 

https://www.youtube.com/watch?v=tpIctyqH29Q&list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo&index=1&t=3s",1514452719.0
Neker,"Babbage, Boole, Turing, Goëdle",1514457975.0
donri,"Sure algorithms and data structures, but I'm surprised no one seems to have mentioned type theory. It's like the holy trinity of code.",1514458639.0
PlymouthPolyHecknic,"The huge disconnect in what is ""computer science"", between those who consider it a sub-field of maths, to those who see it from a business/social aspect. This is less of an issue when dealing with specific sub fields such as software engineering or algorithms, which are much better defined than the catch all phrase of computer science.",1514464985.0
PlymouthPolyHecknic,"What might be much more useful than buzzfeed's top 10 CS concepts would be an simple understanding of parts of the wide spectrum of concepts related to CS, from the electronic and logical foundations, the mathematical language of sets functions and logic, and history, all the way to high level concepts such as frameworks and languages, artificial intelligence, and how to create information systems as part of a wider social context, such as how to correctly identify social aspects of a technology use case and explain technical concepts to laymen.",1514465554.0
DLabz,Array starts at zero.,1514476239.0
John_Titor_2001,Structure and Interpretation of Computer Programs - 2nd Edition (MIT Electrical Engineering and Computer Science) https://www.amazon.com/dp/0262510871/ref=cm_sw_r_cp_apa_a-urAbBYD7CV5,1514491818.0
TechnologyAnimal,"Polymorphism, Encapsulation and inheritance.",1514493511.0
matheussilvapb,"Formal languages theory (and theory of computation), of course your data structures are the building blocks for all sorts of algorithms. If you don't know data structures you can hardly implement efficient solutions for your problems. ",1514504696.0
suraj123455,"Data structures and Algorithms,
Discrete mathematics,
Computer Networks ,
Operating system ,
Compiler design,
Computer organization and architecture,
Theory of computation",1514450236.0
Fizzlerr,"Basically Google ""the algorithms Bible"" and buy and study a copy of the CLRS book.",1514471118.0
Bacon_Unleashed,"Dont mess with Dijkstra, or he will mess you up.",1514473726.0
pppdddxxx,Whining,1514446834.0
Br3ttl3y,"The most atomic structures in computing are the following:

Transistor

And Turing machines

Once you *master* both of these concepts you will be able to understand all of computer science. ",1514440314.0
generic12345689,People he said fundamental. Loops. Conditional statements. A linked list. Variables.?,1514427838.0
Kel-nage,"I assume the quotes around the literals aren’t expected to appear in the text to be matched against?

These patterns look like they could be converted into regular expressions relatively easily. For example, 3N would become `[0-9]{3}` and 0A would become `[a-zA-Z]+`.

Then it becomes a matter of extracting the literals and escaping them, so if they happen to have special meaning in regular expressions then they won’t take affect. To match the appropriate quotation symbol, you could use back references (i.e. `([‘“])([^\1])*\1` to extract a literal into capture group 2).",1514406848.0
arushsh,"I think you can handwrite a DFA and then convert it to a regex. There’s an algorithm to convert a DFA to regex. After you have the regex you can just write it in the JS code. 

Here’s a quick google search result on converting I found. 
https://cs.stackexchange.com/questions/2016/how-to-convert-finite-automata-to-regular-expressions",1514406845.0
drunkdragon,2018 will finally be the year of the Linux desktop.,1514381511.0
robespierrem,"true AI is so far away from being a reality i wouldn't even mention it, crypto truthfully is a fad i'm pissed cause i brought a few coins years ago and forgot the password and the email address when i brought it i'm sure they were going for cents not the ks that they are going for but time will tell i just don't think its worth my time to try and get them back. im building a platform myself i will be using blockchain tech in a different way later on in my build.

truthfully its innovations within code that you should be looking the innovations within atoms are just fleeting.

chatbots i don't think they will take off , just my opinion but i think  the  search engine of web 3.0 is definitely doable google are aware of what it requires and are trying to be exactly that but it requires true AI not ML but true AI to really work computational statistics will not solve this issue ",1514380629.0
,[deleted],1514349951.0
Nerdlinger,"> But what if our Bioinformaticians were thrown into the Genomics in Python class Semester 1?

Then we’d see a lot of people unable to do anything because they don’t know how to code and hose who did know something would be churning out shitty inefficient code?

Also, modern scripting language… Perl?",1514344709.0
Madsy9,"> Imagine if first Semester CS included:
> Genomics in Python (This is a crash course to Scripting and using libraries), Simple Phone/WebApp Design and a hardcore programming class in a modern scripting language (I would say in Perl or Python).

Programming is a tiny part of Computer Science, and ""phone apps"" and ""web apps"" are an even a smaller part of it, if at all. Computer Science is an insanely large theoretical field, while programming is more about basic software engineering. Programming is something students are usually expected to pick up in their own time.

There will always be some tension between universities and companies on what the courses should contain, as companies often demand a constant influx of good talent, but most companies don't give a rats arse about how people are supposed to gain the work experience. They basically want perfect worker drones from the universities (which they don't have to train), which is an insane demand to make.

Universities however are interested in giving people a solid theoretical foundation, whether it's relevant to future employers or not. A university degree is supposed to be more than just helping you with job security.",1514397566.0
Helen___Keller,"I think having meaningful projects in intro classes is a great idea, for example at CMU the intro CS course has every student make a python application, usually a simple but polished game or something of that nature. 

That said, I think mobile app development is a horrible example of intro ""real-world"" development. First, there's fragmentation between android and iOS, and then between different versions of android. Second, a large percentage of learning app development is just learning how to use the behemoths that are android studio // xcode. Third, app development libraries and languages change a LOT every year, and with iOS it's not even possible to compile (as far as I know) with the older swift once they update your xcode. This makes supporting a class more difficult as the example projects must change every year.

All in all, mobile apps just have too much nonsense that you'd waste a large percentage of your students time solving tasks entirely unrelated to computer science or programming.",1514407701.0
Quantumkiwi,"I currently go to Rice University, and I think alot of thought has gone into the way CS is taught there.
1st Semester: 1 Course on Intro to Python and algorithms. Teaches you the basics of programming with python with emphasis on algorithm design.

2nd Semester: 1 Course as an extension to the previous one, but with serious projects where the emphasis is on the algorithmic design, with the python taking a back seat. Some big algorithms you have to design that touch on concepts such as Markov Models, Dynamic Prog, etc. Then these concepts are applied to intesting (although slightly simplified) projects in genomics, bioinformatics, and data analysis.

3rd Semester is where everything starts taking off. You learn Java with an emphasis on functional programming and you start to take more specialized classes.",1514348816.0
robthablob,"I think there should be separate disciplines of computer science and software engineering at University. Software engineering would be a vocational discipline, much as other engineering disciplines, while computer science could have a more theoretical emphasis.",1514373763.0
wavy_lines,"University is about teaching fundamentals.

It should never ever be a ""let's code a crappy angular app"" bootcamp. 

The market is already flooded with mediocre developers who have no idea what they're doing. We don't need more of them.

We need more people who are _really_ well educated and really understand the fundamentals of what they are doing.",1514568709.0
pcopley,"This might legitimately be the stupidest thing I've ever heard. Also lol @ web app design giving people ""background knowledge"" of data structures and lol @ Perl being a modern scripting language.",1514990829.0
ReginaldIII,"Buy the book. Read the book. When you don't understand something in the book, re-read the relevant parts of the book. Search for other materials about the topic. Read those things. When you don't understand something or want to know more, search for more resources about those things. If there is something interesting, and you feel you have exhausted the usefulness of the book, buy a different book on the topic of interest. Repeat.

I teach at a university, and it never ceases to baffle me how smart and intelligent students with a genuine interest and passion for certain topics become completely complacent to not read around the topics covered by the course or to seek external knowledge or alternative explanations of the same knowledge when they do not understand something. Do not be those students. Your education is not about getting a good grade on a course or being spoon fed a specific subset of a topic for the purpose taking an exam, it's about learning something that will serve you going forward. 

And come on now, if anything strive for an A+, don't go into it aiming for a B, don't try to game the system. If you know something you will get good grades, but getting good grades doesn't mean you know something. It just means you ticked the boxes your university asked you to.

If you plan on going into a career in computer science then now, when you are in university and have an abundance of time to spend learning, is a pretty good time in your life to spend learning things. To find the parts that you find most interesting. And to become knowledgeable in those areas so you can enjoy and be good at the thing you are about to spend the next half of your life doing professionally.",1514304867.0
pythonicus,"Don't listen to people telling you things are hard - it's a lie. I listened to that nonsense for years, doubted I was capable of doing any of it and tepidly approached my education. I was like the complacent students /u/ReginaldIII is describing because I was too scared to give it my all and still fail.

Then one day I put my big boy pants on, put in the effort and studied the material exactly as he described with positive results. 

tldr: don't be afraid of the material, ignore your internal FUD and just do the damn work",1514325522.0
FieryPhoenix7,"In my experience, crypto classes are either mostly theoretical or mostly practical. It's one of those areas that can be taught like a pure math class or similar to an introductory programming class (where you have a few deliverables throughout the semester). It's up to your professor. But if your class falls into the theoretical category (as in, your professor will put focus on the mathematical concepts), then it's not a bad idea to brush up on your discrete math and algorithms. If the math isn't going to be a focus, then you probably don't need to do much besides reading up on the various topics that may be covered. 
",1514304573.0
shbilal001,"Some courses we get to do in the university are usually very challenging, especially if one is doing a Computer Science related course. Creating new programs to improve efficiency or solve existing problems is not as easy as just writing a few lines of code. It goes way deeper than that, a good developer will need to be in constant touch with their clients and be able to draw conceptual diagrams and write documentation for any software he develops. For starters, if you wish to start learning computer science, joining a good programming school would be in order. In my case, I would suggest that you go to Holberton School (https://www.holbertonschool.com/) as they offer a 2 year full stack Software Engineering course. Students get to learn the core topics and important aspects of computer science that are in relation to the IT industry standards. For you to tackle challenging courses, I would suggest that you be in close contact with your lecturers and tutors for guidance and further teaching to ensure that you get to learn more and understand more. Besides that, you may want to spend most of your free time reading what you’ve already learnt in class and discussing it with your friends. At the end of the day, the more you teach others, the more you understand the topic yourself. Good luck. ",1515360001.0
theofficialdeavmi,"I like these old-style websites.

Better than the JavaScript cluster fuck of these days.",1514282868.0
a-buttclown,If you want to look at parsing from highlevel its analyzing a set of symbols according to some pre defined grammar. When looked at it this way a simple search will be a parsing problem. However the usefulness of looking at simple tasks from this level of abstraction is a whole other question.,1514386240.0
beached,Parsing is such a fundamental part of almost any programming task that works with people.  Good stuff,1514318725.0
theofficialdeavmi,If it loads ovee GPRS then it's lit af.,1514334764.0
TomvdZ,"The proof of theorem 3.4 is wrong. The claim of theorem 3.4 is basically ""At least one of X or Y runs in polynomial time"" but the proof only shows ""At most one of X or Y runs in polynomial time."".",1514279911.0
clownshoesrock,"Compilers.  I was too proud to ask for help, forged my own path,  painted myself into a horrible corner that was so far from expected path.  The instructor was generally surprised that I was able to handle the first parsing assignment by using the lexer creatively.  Explained that assignment four would require me to restart, as it would be impossible my way.  Said it again for assignment five.  Then got curious for assignment six,  Assignment seven took some mental gymnastics.  The final project,  I was totally boned, I didn't know how to implement templates, as I had done nearly everything in a lexer.  When I asked for help the instructor, while sympathetic to my cause, he had no idea how to even grasp my machinations.  My final assignment scored a zero, it was 20% of the grade, So I pulled a C..

I learned a huge amount of why compilers spit the messages they do.

I learned to not go so far off-course that no-one could rescue me.

Creativity, and determination can get past all sorts of impossible problems..  unless they are actually impossible.",1514254624.0
SL4M_DunkN,"What did your OS course cover exactly? I wouldn't rank mine that highly, so I'm curious",1514249397.0
link23,"Formal languages/automata, lambda calculus. (They were in the same class - it was kind of a hodge podge.) I ended up loving both subjects, TAing the course twice. After graduation, some obscure automata theory knowledge helped me during a job interview, and I've learned to love functional programming thanks to lambda calculus.",1514262087.0
Gatesunder,Discrete Mathematics.,1514238070.0
DiZZtr0ya,Data structures ,1514252924.0
aparziale,"system programming (mostly covered C with a focus on coding various “systems”) and computer architecture (covered processor architecture and filled the final gap between programming languages and bare hardware - machine instructions!) 


For me, these were extremely important classes because together, they bridged the gap between the software and the hardware worlds. ",1514253368.0
chrisname,Intro to FP for sure. I love Haskell now.,1514257294.0
onfire9123,Networking,1514258349.0
gregoriohombre,"At my university, we have this class called Great Theoretical Ideas of Computer Science. It was a survey of many of the major areas of CS, but it was a very challenging class where they would require you really dig into each subject. You would start with DFS and TM's, then make your way to graphs and basic complexity theory (P, NP, approximation algs.) and finally make your way to things like game theory, applications of probability theory, cryptography, and quantum computation. The class moves super quick but by the end I felt like the major areas of CS that originally seemed mysterious to me were now understandable, at least a beginner's level.

Edit: *DFA's",1514273587.0
wilkesreid,I'm gonna go with Networking. It really puts you a step ahead in the real world to understand the underlying structure of the internet.,1514275609.0
FieryPhoenix7,"Formal Languages and Automata 

IMHO, this should be a required class in every CS program. Lots of CS students graduate without having heard of the Chomsky hierarchy or having any clue about the limits of what can and can be computed. I truly believe that class made me a better programmer and thoroughly increased my appreciation of the field. 

Compilers is another class that was beneficial, but it's really just an application of some of the ideas covered by the former. 
",1514296218.0
TheWildKernelTrick,"Machine Learning, easily.",1514251043.0
rPrankBro,I did an artificial intelligence paper where we made path finding and image identifying programs that we wrote in Clojure. The AI was fun to learn but the most valuable thing was learning Clojure since it gave me a new way of thinking since it's a functional language and I had only used object orientated until then.,1514256622.0
Thegrinch1985,Data Analytics ,1514250329.0
belizabeth10,"Ranked: Software Engineering, Data Structures, Discrete Math, Data Mining (Machine Learning)",1514267923.0
GentlemanSavage,Functional programming (Scala and Haskell) and Databases (mostly SQL but some noSQL),1514299645.0
mcandre,"Discrete math helps to focus when dealing with off by one errors ha

Boolean algebra helps with nested conditions and bit manipulation

Concurrent programming gives you an appreciation for how things behave asynchronously

ECE 101 or whatever helps you understand what the CPU is doing",1514306566.0
libeako,haskell,1515165373.0
rosulek,"This is called **private set intersection**, and there are gazillions of papers on the topic, which you can now find by searching with this correct keyword.",1514251919.0
Nerdlinger,"If I'm understanding you correctly, the answer is no as if you only have two values and you know your value, then if you know whether or not the other value equals yours, you know the other value.

However, speaking more generally, there are ways to tell if two parties have the same secret value without either party learning more than that bit of information (search the internet for the socialist millionaires' problem for more info).

If I'm misunderstanding you, feel free to clarify.",1514229388.0
quantum_jim,"/u/Nerdlinger already answered this question, so I'll ask and answer a related one. Can the result of an AND be sent to a third party, without either party giving up any information about either bit.

The answer is yes, at least when you have quantum resources. You can use the bit values to determine measurement settings in a CHSH scenario. The measurement results contain no info about the original bit values, but after being sent to the third party for comparison, the AND can be computed.",1514236672.0
AaronKClark,Nice find. Thank you for sharing.,1514224083.0
floridawhiteguy,"Gawd, I truly hate academia sometimes. Almost as much as I hate theology or lawyers.",1514244317.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1514191202.0
foadsf,Could anyone El5?,1514194364.0
WSp71oTXWCZZ0ZI6,"I hope something like this catches on. Not necessarily this proposal, but any of the proposals that provide convenient mapping (or ""swizzling"" as they call it) of pointers within shared memory. They make a good point in the paper that using mmap() (beyond very simple cases, like memory mapping a file) never happen because it's so annoying to deal with pointers when every process has its memory laid out differently. mmap() is under-utilized, I feel, because it can be kind of a pain to set everything up for complicated use cases.",1514183262.0
Vitus13,I'm really having a hard time seeing how the rest of the paper has anything to do with NV RAM. From the application perspective everything described here is achievable today with higher latency with DirectIO and/or mmap.,1514219147.0
voronaam,"Linux kernel supports direct IO. I wonder if that interface can be used instead. Currently it comes with a somewhat headachy memory buffer alignment requirements, but I wonder if byte adressable persistent storage will let us lift this requirement.

P.S. BSD may support it as well, I think O_DIRECT is POSIX, but I am not certain.

Edit: This would be a reverse approach to authors. Direct IO solution is to get rid of RAM and treat NVM as a persistent storage, rewriting applications to use it for interprocess communication, etc. ""RAM"" would only be used for the state application does want to loose on power cycle, thus kernel cleaning it up in NVM. Perhaps authors idea is more advanced. Will be interesting to see the code. Did I miss the link to it? Can't find it in the article.",1514209879.0
ummwut,The only thing NV RAM is good for at the moment is keeping an inventory of unwritten changes to files.,1514409976.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1514151780.0
Franck_Dernoncourt,"According to https://www.snopes.com/alexa-orders-dollhouse-and-cookies/, the last claim ""Alexa devices in some viewers’ homes were again triggered to order dollhouses"" is ""questionable"".",1514158121.0
AstrodynamicalMoney,"Apple seems to fail to realize that FaceID is not necessary, maybe doing what Samsung did with the Iris scanner is better. Doing full face scans takes too much learning and the algorithm needs lots of data to train it thoroughly. Apple's claims are just empty. It depends on how much the algorithm has been trained, and obviously, you'd need someone to sit there on their phone for a long time showing different angles and doing faces. That is inconvenient for the consumer, so you get these blunders.",1514161798.0
MaunaLoona,If these are the worst AI fails of 2017 we are screwed. AI Apocalypse is near.,1514205512.0
autotldr,"This is the best tl;dr I could make, [original](https://syncedreview.com/2017/12/23/2017-in-review-10-ai-failures/) reduced by 92%. (I'm a bot)
*****
> Google Allo responds to a gun emoji with a turban emoji.

> Maybe too easy? In January, San Diego news channel CW6 reported that a six-year-old girl had purchased a US$170 dollhouse by simply asking Alexa for one.

> When the on-air TV anchor repeated the girl&#039;s words, saying, &quot;I love the little girl saying, &#039;Alexa order me a dollhouse,&#039;&quot; Alexa devices in some viewers&#039; homes were again triggered to order dollhouses.


*****
[**Extended Summary**](http://np.reddit.com/r/autotldr/comments/7m19al/10_ai_failures_in_2017/) | [FAQ](http://np.reddit.com/r/autotldr/comments/31b9fm/faq_autotldr_bot/ ""Version 1.65, ~272168 tl;drs so far."") | [Feedback](http://np.reddit.com/message/compose?to=%23autotldr ""PM's and comments are monitored, constructive feedback is welcome."") | *Top* *keywords*: **new**^#1 **emoji**^#2 **Alexa**^#3 **Google**^#4 **bus**^#5",1514206947.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1514146774.0
aznkazaya,Pretty sure they just track his actual sleigh using GPS.,1514133324.0
skelterjohn,You'll have to ask Santa. I bet there's a lot of optimization considering how many toys he has to deliver.,1514134905.0
CharBred,It's planned ahead of course! You think santa is winging his delivery route? ,1514136684.0
itsmrmarlboroman2u,You'll find that the routes don't match. ,1514143773.0
rfelsburg,"Umm, someone is lying. Norad has santa near Germany and Switzerland. Google's Santa Tracker has him in Croatia. So one of them is lying to the kids, that's a paddlin'.",1514154452.0
hubbahubbawubba,"Presumably Santa uses a heuristic or approximation algorithm to optimize the route. Even if he uses the latter I doubt it would be fast enough to run online given the input size, so it's almost certainly planned in advance. ",1514140328.0
realyuvallevental,"**ETA:** Most likely, the Google one was planned ahead of time.

Screenshot of the Google Santa Tracker from 2012, taken from the [Google blog](https://googleblog.blogspot.com/2012/12/count-down-to-christmas-eve-with-google.html):

https://1.bp.blogspot.com/-68Yyfm-9StM/UNEeWahO9rI/AAAAAAAAKfU/9dQ4pDKxrxk/s1600/Screen+Shot+2012-12-18+at+3.38.49+PM.png

Screenshot I took now from the one on the Google frontpage in 2017: https://i.imgur.com/dZ6jrEc.png 

Notice all the points match up perfectly of you compare the areas.  So they probably repeat it every year.",1514155070.0
,"Google maps works using a modified form of Dijkstra's with long distances between nodes using freeways because it assumes they are faster. It looks like google did the same thing for the Santa route: pick a bunch of major cities, pick the continent where he starts then move from city to city using easily obtained distance data then continent to continent using based off visual inspection.",1514169985.0
dijkstra-,"What you have is essentially a version of the [travelling salesman problem](https://en.wikipedia.org/wiki/Travelling_salesman_problem). The problem is, with rising difficulty (i.e. number of points on a graph), the difficulty and computational power to calculate the most optimal route rises exponentially.

Without having done any research looking into the actual tracker, I assume they simply use a simplified version of earth (picking a few major cities and the like) and apply the algorithm on that.",1514203313.0
dhjdhj,Really? You want a **serious** answer to how a non-existent entity is actually tracked?,1514210233.0
clumma,"Dijkstra and Backus had some correspondence about this paper

https://medium.com/@acidflask/this-guys-arrogance-takes-your-breath-away-5b903624ca5f",1514146374.0
theshadowhost,I think citeulike used to have something like this that showed you the graph of references ,1514129428.0
bastih01,I'd say start out inefficiently and work your way to efficiency from there.,1514115841.0
TomvdZ,"> I don't really like to share the whole problem because I want to solve it by myself.

Based off this little information, nobody will be able to say anything useful.",1514115039.0
Rioghasarig,"Loop through the matrix while checking if each number satisfies the condition. 


WIthout any more information that's the best I can do. ",1514118157.0
AccountNumber3000,What data structure are you using to store the matrix?,1514118528.0
_Azota_,"As others have said, more information would be needed to provide a meaningful answer. Since you want to ""solve it for yourself"", consider posting the full question, reading halfway though an answer, so that the answer serves to provide ""inspiration""?",1514119732.0
chipstastegood,I have a Computer Science degree and I have no idea what SAS is,1514095129.0
thornza,"You will need to take training from SAS itself: 

https://www.sas.com/en_za/training/home.html",1514095864.0
WArslett,One of my clients are SAS consultants. They sell online training (or classroom training if you are in the UK). https://amadeus.co.uk/,1514102078.0
Kel-nage,"My partner is a statistician and during their undergraduate courses, they were taught SAS, along with R. So it may be the case that your University will teach courses that use it.",1514109061.0
sleepydog,"To add to the existing advice, even if your school doesn't use it in a course, you may be able to get a free or reduced cost license and teach yourself using the documentation it bundles in.",1514147988.0
Peradox,I believe codecademy has an online course available for SAS,1514258348.0
Boba_Fetts_dentist,"I believe sas university edition can be downloaded by anyone who wants to learn. (Documentation lists ""students,"" ""adult learners"", etc.) 

http://support.sas.com/software/products/university-edition/index.html

",1514509030.0
CtrlPrick,posting here the github: https://github.com/idni/tau,1514077742.0
CtrlPrick,"Any one has a take about this?

is what published there have any significance?  
",1514100448.0
Indifferentchildren,"I am not familiar with the MP3 header structure, but if the duration is specified in the header, then you might be free to append bytes to the end of the file without side effects, maybe even without limit? ",1514057099.0
klohkwherk,"Flac allows saving of arbitrary key value pairs as tags. The real question is go you want to:
* Enable players to store metadata in the file if they desire.
* Force a player to save metadata to continue playing the file. 

I can imagine a vague way the latter might work based on encrypting different sections of a file at 10 second intervals with different keys, but I can't imagine anyway you could force a player to save this permanently. A player could simply create a copy of a file to play, and then delete it after finishing.

If it's the former you want, I don't see any reason why you wouldn't just use metadata tags with an existing format. I'm not sure if .ogg supports arbitrary tags or not, but if it doesn't then that might be a better bet due to file size",1514058514.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1514043887.0
bhrgunatha,"Head and tail  are the 2 ""ends"" of many animals.  

I'm not sure of the origin but computer scientists and programmers used the terms to talk about lists e.g. the list [1, 2, 3 , 4] has a head (the front) of 1 and a tail (the end) [2, 3, 4]. So head and tail became synonymous with start and end.

When you look at the execution sequence of program instructions and procedures there will always be a final - or tail - instruction [unless it loops forever](http://www.lel.ed.ac.uk/~gpullum/loopsnoop.html). 

Procedures use a stack to save the current position in the program when a procedure is called (so it can return to the right place once the procedure has finished) along with some state information usually. When there are a lots of recursive calls, the stack can grow very large, very quickly.

When the final instruction of a procedure is to call itself, there's no need to record that on the stack - the compiler or interpreter can just use a jump or goto without saving the location or state (because the procedure is finished). It's also true when the final call is to another procedure too, but particularly important to recursive procedures.

This  is called tail call elimination - the extra stack information is eliminated from the usual call stack handling process. 


lol by the time I write my answer. someone else has already answered. ",1514035002.0
GNULinuxProgrammer,"In lisp everything is a list, including programs. So at any moment in execution, program is in this form: `(head, tail)` where `head` is the current function to execute, and `tail` is the remainder of this function. When you're in a tail recursive function, the last thing this function does will be resolving `tail` which is a call to the function itself. In such cases, you can optimize this call by not assigning a frame for it, instead working on the current frame. Tail recursion is a very old terminology, coming from days when lisp was the biggest high-level language, so this mindset that *code is data* affected CS terminology.",1514037577.0
lsiffid,"Here’s an explanation offered by Guy Steele [in 1977](http://dspace.mit.edu/handle/1721.1/5753):

> Such a call is called *tail-recursive*, because the call appears to be recursive, but is not, since it appears at the tail end of the caller.",1514034476.0
ldpreload,"Tail recursion, or more generally tail-call optimization, only works if it's the last thing the function does.

If I have two functions like this:

    def a(x):
        some_variable = some_calculation()
        another_variable = b(x)
        return more_calculations(some_variable, another_variable)

    def b(x):
        return x+1

there's no optimization you can do on the call from `a` to `b`; you have to make a new stack frame to call `b`, and return back to `a` and `a`'s stack frame to finish up the work.

But you can optimize the call to `more_calculations`. Since there's nothing left to do in `a`, you can _replace_ `a`'s stack frame with the stack frame you're generating to call `more_calculations`. You don't have to return to `a`; you can return directly from `more_calculations` to whoever called `a`. That works simply because `more_calculations` is the very last thing `a` is doing before returning, which is sometimes called ""tail position"".

(If you're calling the same function instead of a different one, then it's tail recursion.)

Note that it doesn't have to be the very last line of the function. If you're returning early, you can still do a tail-call optimization as long as it's the last thing the function's going to do in this particular call, even if there are more lines left in the function.",1514046984.0
importUser,The definition of a tail is “the hindmost part of an animal” or “the end of a long train or line of people or vehicles.” A tail call is a call to a function at the end of another function. Tail recursion is a special case in which that tail call leads to calling the same outer function again.,1514034288.0
HeyGuysImMichael,"""How did tail recursion get it's name?""",1514048858.0
thornza,It was named after the inventor watched his dog chase its own tail. The dog was going round and round in circles and it reminded him very much of a recursive function... ,1514095509.0
voodooPractitioner,See [here](https://www.reddit.com/r/compsci/comments/7lofgf/why_is_tail_recursion_named_like_that) ,1514065467.0
EmbeddedEntropy,"As mentioned, in part it depends on what fields you want to go into.  In what I've done, what I've found personally useful are:

* Both Statistics and Probability - All the time, especially in Data Analysis software.  Somewhat in designing and writing software, but more in understanding white papers, people's arguments (both when they're good and bad), and how to present your own business data and be confident it's statistically significant and valid
* Calc I/II (and a little III) - Any ""real world"" software including Robotics and AI
* Linear Algebra - All the time, especially in Graphics and Data Analysis.  To me, it's as critical in CS as Discrete Mathematics and is a foundation for Linear Programming
* [Linear Programming](https://en.wikipedia.org/wiki/Linear_programming) - Has nothing to do with computer programming.  It's solving and optimizing a series of inequalities.  I used it in Data Analysis, Computer Based Training, and other consumer targeted software used for scheduling and optimizing available choices

As for DiffEq, I never took it.  I'm not sure what options were denied me because I didn't have it.  What you'll find is a lot of people will say, ""You don't need <math class>.  You'll never use it.""  That's flat wrong.  What I've found is that _every_ math class I took in college opened new doors later for me in my career that were closed to others simply because they didn't have an equivalent background.  If you intend to go far and take on a lot of challenges that other people just don't have the skills for, take all the math classes you can.


",1514044946.0
Jaxan0,"From the ones you listed, all seem relevant or nice to know about. In the end you will probably only use fractions of the things you learn (and it depends on what you will do in CS, the field is big). ",1514024428.0
Crazypete3,"It's highly reccomend you take linear algebra because they go over topics involving matrices which are also the same kind of containers that hold data (arrays, vectors etc.). The only other thing you might really need to understand is vectors in calculus 3 and maybe some power series. But all in all, most of the math you take like calculus and statistics/prob. isn't really related to CS unless it's a specific job outside. I highly doubt many software developers know statistics and calculus let along remembering it because they had to take it as a course in college. ",1514024927.0
kbbqallday,Quick maths,1514056928.0
,"The other comments sum it up well, but I wanted to point out that sometimes it depends on what area in CS you want to specialize in. For instance, with cryptography, you might want to take an abstract algebra course as well.",1514041015.0
infected_funghi,"Have a look into graph-theory. You will use it a lot in theoretical cs, but also for modellchecking/mutual exclusion or modelling stuff with petri networks in general. It helps proofing properties of algorithms like correctness, termination, scalability and complexity in theory but also with petri networks you have a tool to illustrate algorithms and maybe find something more effective for your problems in practise.

I think probability is interesting if your planning to go into research like cryptography, but you will barely need it in practise. ",1514037687.0
tetroxid,"Discrete math, linear algebra, analysis, computability and complexity,  graph theory are the first to come to mind",1514043917.0
AnnoyinglyEmotional,"Define what requirements there are for someone to be called a ""master programmer"" and you'll have an answer.",1513997939.0
Crazypete3,"What do you mean by mastering computers and the internet? Everything in a computer is broken down into a lot of different fields. You have the engineers that build the chips and monitors. You have the software engineers that design the operating system. You have people who specialize in security for protection of the computers network, code, and general security. You have architects who design a lot of this. Different companies who specialize in different softwares to help come together to build your apps, browsers, tools, and updates. There are so many different cogs in this wheel that turns, so youd need to be more specific. When it comes to engineering or programming, the limit is your imagination. You can literally build anything you can think of. You need a robot to catch the toast when it pops up, it's been done. So when you say the greatest thing achieved, the greatest thing achieved requires multiple fields and teams and backgrounds. If I want to design Starks suit from Ironman a programmer can't do it himself and neither can an engineer. You'd need Mastery in both, as well as physics, welding, and making money. So don't limit yourself. ",1514029596.0
iHicham,noob,1514232851.0
iHicham,i know u downvoted me if you downvote this i’ll downvote all your comments,1514232964.0
AndreiSipos,Strong AI,1514030626.0
remy_porter,"> Since 90 percent of Americans celebrate Christmas, Santa will visit that percentage of 0-to-9-year-olds in each county.

This is a bad assumption. Santa only visits good children, and good children don't exist. Santa makes no stops at all.",1513977650.0
mephistophyles,Is 2 hours Santa's travel time or how long a laptop would need to process this graph traversal problem ;),1513966593.0
ChrisC1234,This is so stupid.  Santa is non-deterministic and can be at any number of houses at the same time.,1513989745.0
okiyama,Does this assume instantaneous acceleration and deceleration? ,1513990191.0
dmwit,"Dunno what algorithm you used, but if you didn't know about it: For Euclidean TSP, you can choose any approximation factor you like and get a polynomial time algorithm out the other end to approximate it within that factor. [Wikipedia](https://en.wikipedia.org/wiki/Travelling_salesman_problem#Euclidean_TSP) has some info on this.",1514056688.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1513944653.0
agumonkey,"I'm sorry about the language constraint, but this is such a mindblowing topic that I thought I'd share. For anyone french capable I bet a dollar that it's worth every bit of effort.",1513927068.0
kingjacob,Has he ever given this presentation in english?,1513968203.0
wei2912,"What background does one require to make use of The Art Of Computer Programming, Volume 1? I've been planning to tackle it in order to gain a solid mathematical foundation in algorithms, but I don't know if it's suitable for me. Would a book such as CLRS be better?",1513931788.0
mrterrbl,"I'm new to computers (besides general use) and I'm looking to completely submerge myself into the culture. Unfortunately, many sources I find are either below my level (usually simple ""hacks"" from buzzfeed type articles) or far above it. Basically I'm looking for computing news that is within my intermediate scope.",1513953889.0
davidthearmo,what books or pdf's are you guys reading this weekend? would love to hear,1514083353.0
djingrain,"Does anyone know of some good online courses about system calls in C? I'm taking a Systems Programming class, but my prof is kind of lacking in the practical aspect, not to mention his crazy tangents",1513917678.0
completely-ineffable,"> **Abstract.** How did the theory and practice of computing
interact to generate a new discipline, computer science?
Studying the French scene in comparison to other
countries, reveals that in most cases computing
developed initially as an ancillary technique of applied
mathematics, with little awareness of the path-breaking
theories of computability elaborated in the 1930s. This
was particularly clear in France, where mathematical
logic was almost inexistent and disregarded by the
Bourbaki group.

>It was only in the early 1960s that researchers in the
emerging field of computing felt the need for theoretical
models, and discovered the Turing machine and
recursive functions. Simultaneously, an interest for
language theories and information structures, fostered by
practical projects such as machine translation, converged
with issues raised by software development and the
nascent theory of automata.

>The convergence of these diverse intellectual agenda was
central in the process of construction of the new
discipline.
",1513867807.0
Godzoozles,">On the spectrum of the different histories of computing, France constitutes a case where mathematical logic played no part at all in the early development of this technology.

That's a massively fascinating observation that clashes with everything I'd expect about computer science.

Thanks, this gives me something to read at the airport tonight.",1513887008.0
agumonkey,"probably online readable [google doc viewer](https://docs.google.com/viewer?url=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FPierre_Mounier-Kuhn%2Fpublication%2F308901846_Logic_Formal_Linguistics_and_Computing_in_France_From_Non-reception_to_Progressive_Convergence%2Flinks%2F59b6f9a1aca2722453a4f8e5%2FLogic-Formal-Linguistics-and-Computing-in-France-From-Non-reception-to-Progressive-Convergence.pdf)

ps: a few months ago I also stumbled upon this [Maurice Nivat : une vision à long terme de la recherche en informatique](https://interstices.info/jcms/c_32847/maurice-nivat-une-vision-a-long-terme-de-la-recherche-en-informatique) ~ ""Maurice Nivat: A long term vision of computing research"". Said Nivat being mentioned page 7, figure 6 of OP's link",1514289735.0
nerdshark,There's a reason APL isn't popular. This is even *worse*. What the fuck?,1513798435.0
brettmjohnson,"Less readable than APL, and a nightmare for color-blind programmers (like the one sitting 6 feet from me).",1513802416.0
drWeetabix,That website is pretty bad for mobile,1513802605.0
theofficialdeavmi,"This has to be the most confusing thing ever.

But kudos for you for getton it done.",1513800523.0
Workaphobia,That page is unreadable on mobile. Is this satire?,1513816320.0
drummeur,"Isn't pretty much every programming language based on symbols? I think programmers usually call them ""tokens"". 

 (Except maybe these: 
https://esolangs.org/wiki/Category:Non-textual)",1513818205.0
,"Looks interesting, as a visual person I quite like the idea. This is likely too confusing to gain a significant userbase tho. ",1513806162.0
dman24752,Where's actual code that I can look at?,1513809741.0
kckcbbb,Looks like they want to have a distinct glyph for every type. Am I wrong about this? Doesn't this disastrously constrain the expressivity of the language? ,1513894818.0
,Yeah that looks human readable. ,1513922239.0
skyjazzcat,This is what non-programmers think programmers do all day.,1514123725.0
,They've got five years to build at 10 q-bit chip. I hope they can,1513739677.0
MichaelDoesCode,Sounds like the Solar-Freaking-Roadways guys. Whitepaper or it didn't happen.,1513768770.0
mrterrbl,"Hi. I know this is the wrong place to ask, but can someone point me towards a computer news sub that's geared more towards beginners? Half the things I find are just over my head currently.",1513790827.0
d2ve,Those decoherence times tho...,1513796470.0
tenin2010br,Anyone know anything about the masters curriculum at this university? Thinking about enrolling from the US. ,1513815614.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1513719263.0
barsoap,"Or, you know, build a stellerator and not have the problem in the first place. I also very much doubt that prediction can get you even close to continuous operation, tokamaks are going to keep being burst machines.

We should really start planning out a full-scale stellerator now that Wendelstein right-out exceeded expectations. ITER is still useful when it comes to figuring out details of tritium breeding, though.

tl;dr: Don't throw AI at it but hardware :)",1513712950.0
stickmyfiddles,"As someone who has spent a lot of time in desktop development and hard real time embedded development and has transitioned to business logic in RPG, I have to say that truly modern RPG isn't that much different from any other procedural programming language you'd run into today.

The fixed format column specific stuff that is a lot of legacy RPG makes me want to jab my eyeballs out but newer RPG is pretty much what you would expect out of a programming language.  RPG itself is really straightforward and simple which is actually pretty great for manufacturing and business logic while interacting with the database.  I've made a mock up of it below:

    **FREE  // This is required for modern fully free format.  Also, // works as comments
    
    /Include DoSomething

    StartProgram();
    *InLR = *On;

    Dcl-Proc StartProgram;
      Dcl-Pi *N;
      End-Pi;

      Dcl-S Arg Char(10) Inz;
    
      Arg = 'Something';
      
      If DoSomething(Arg);
        Exec Sql Insert Into MyTable Values ('Hello, World');
      EndIf;
    
    End-Proc;

Also, yes that is all you have to do to run simple SQL inline with the code.  There's no ODBC, JDBC, blah blah to set up communicating with the database.  It's just there all the time which is super nice.  There's also older operaters that can be quicker depending on how you use them but that's getting into more of the unnecessary details at this point.",1513703761.0
hothotchickenjuice,"I learned RPG IV and COBOL in college in the mid 90s. I was in the last class before both were dropped. I never did anything with either, but a pair of friends a year ahead of me earned a small fortune working in Bermuda doing Y2K updates for banks and doctors.

We learned on an AS/400 system and it was so slow most people passed simply because they couldn’t be failed for being unable to compile their programs. 

Good times. ",1513685929.0
JH4mmer,"I did an internship with a small company a few years ago, and they used RPG to interface with their AS400. My manager wanted me to learn about it, so he placed a stack of about 5 huge reference books on my desk and told me to get started. As someone with a C++/Java/C# background, that was the worst case of culture shock I've yet encountered. ""Why do I need to write this particular symbol in column 37 for the program to work?"" ""Why is everything a file?"" ""Why am I spending multiple weeks trying to get anything but the most trivial program off the ground?"" 

I appreciate the experience, as it broadened my worldview significantly, but I'm also thankful I'm at a point now where I don't have to deal with languages like that anymore. *Shudders*.",1513699477.0
HughMunguz,I was an ILE RPG programmer from 1999 until 2012 and I do miss it sometimes. It gets the job done.,1513685948.0
QuantumQuark5,"i am in my early 30s and still in the platform. one really needs to understand the architecture to appreciate why they did the things and syntax the way that they were. granted my exposure to other languages isnt that great, RPG is good in its own strengths as a business language, in my own personal opinion much better than COBOL. plus they are enhancing it tremendously with the new versions. have a look and see what they did to improve it. ",1513700909.0
cheesemoo0,This is a computer science sub not IT help.,1513651760.0
supercargo,"Keep in mind the HTTP protocol is stateless, so closing your browser doesn't actually do anything Geico will know about.  If you don't want to wait for your session to expire, you'll need to explicitly log out.",1513652031.0
IronManMark20,"I've just started on a lexer for a toy language to test out working with llvm, so this post was quite interesting!

Also, your links to source files are broken. :/",1513558150.0
Luolong,"Just wanted to add that in addition to Packrat and PEG grammars, there’s another parser based approach that has pretty interesting properties that could be used to do language composition Earley parsers (specially in the latest Marpa incarnation with its Ruby slippers parsing) have ways to compose multiple grammars.

It probably falls into the category of wasting space though. ",1513617852.0
promach,"Could anyone explain the table at https://en.wikipedia.org/wiki/Asynchronous_I/O#Forms ?

1) How can synchronous operation be non-blocking as well ?

2) Why ""In Asyn model, I/O operations won't block even if it can't be completed"" ?",1513315542.0
promach,">All forms of asynchronous I/O open applications up to potential resource conflicts and associated failure

Why would this happen ? It is just an I/O operation.",1513496500.0
floridawhiteguy,I'd love to see someone do a Master's or Doctoral thesis on modelling and emulating patterns of animal pack panic in AI systems...,1513312491.0
,I’m a CS minor and I’m taking Data Structures next semester. What can I do to prepare myself?,1513704532.0
Agent_KD637,"I'm a psychology master's degree graduate interested in learning about neural networks, as it can aid my research interests in cognitive science. Are there any books you can recommend? Perhaps something theoretical and summative without a lot of math. I was recommended ""An Introduction To Neural Networks, James A Anderson, MIT Press, 1995"", but would prefer something more up-to-date.",1513810909.0
iwantashinyunicorn,"Which is a bit odd, because manufacturing has been using core AI topics like constraint programming and optimisation for decades.",1513286221.0
Darkfeign,It'll only cost you a 70-80hr work week!,1513262283.0
MassiveDumpOfGenius,AI/ML consulting company for manufacturing?,1513298953.0
Varelze,https://en.m.wikipedia.org/wiki/Mill_architecture,1513258621.0
mcorah,"The author of this article seems quite defensive, and he alternates between being hung up on the details and otherwise attempting to denigrate the results. The Alpha* line of work isn't representative of any particularly novel ideas, and I don't think anyone (at least anyone who has some reasonable understanding of the field) would claim that it is. However, each of these are representative of significant milestones for the community, and I don't think that the criticisms change that.

In my understanding, this is a quite significant result for general game playing, and it's definitely going to change how people look at related problems just given knowledge of the existence of methods that perform well in this domain. So, although I agree that the results do look somewhat overstated, there's still a whole lot of merit that remains.",1513228977.0
eigenman,They have a great hammer and are looking for nails.  And it seems to nail them really good.,1513231112.0
macosta_exe,"The answer is NO. Just part of sensational news. If you were to tell me both ran on equivalent hardware, it would have been awesome news. But having the power of custom microprocessors and google on one... and just 1gb of ram on the other one is just plain cruel for stockfish. Talk about a handicap 

Edit: grammar",1513199794.0
,About the non reproductability of the deepmind papers looks it is close to be false. The alphagozero paper is in the road to be reproduced. http://zero.sjeng.org/,1513258519.0
Octopus_Kitten,"I had to google img Shogi, also known as Japanese chess, looks pretty cool. Reminds me of Hnefatafl which is also known as Viking chess, there is a pretty cool app of this you can download for android. 

edit: getting a lot of downvotes, I apologize my comment did not specifically relate to computer science. I just find it interesting the different types of ""chess"" in the world. The encoding and programing process must be fascinating for each one. ",1513214340.0
thunderGunXprezz,"I have to laugh at the fact that the company I currently work for so desperately wants to be able to use ML to do something, ANYTHING! But nobody ever seems to solve the biggest problem (imho) first in developing any software tool. That being: Figure out what problem you're trying to solve.

The results have been one massive circle jerk of brown bag discussions, new positions - even new departments.  And no shortage of people who just want to goof around all day playing with cool cutting edge shit that never contributes anything of value to the rest of the development teams. ",1513208490.0
VermillionAzure,"Machine learning is a empirical and somewhat-statistical methodology. From waht I understand, it is not based on statistics alone, but also has roots in optimization. Unsupervised machine learning (e.g. AlphaZero) is great at learning quickly, but turning that learning into information or reasoning that is more in the form of traditional mathematical logic is not yet a mainstay of machine learning. You cannot ask a neural network model yet, in most cases: ""What are the top 10 rules that you use to most effectively do you job?"" You can ask for the features that have the most variance with something like PCA, and perhaps you can do something with a matrix representation of the node weights. Take into account that some models are not as ""interpretative-friendly"" as one might think.

The person who figures out how to divine information from these sometimes inscrutable models will be the person that turns machine learning from an ""alchemy"" into a science, where we can finally draw conclusions from the ""wisdom"" that the machine has gathered over its learning period in a cohesive manner.

",1513167172.0
Ravek,"If you get results and they are reproducible and you can leverage your understanding of what you’re doing into getting more results, then you’re doing science in my book. Was Faraday not doing science because Maxwell hadn’t written his theory of electromagnetism yet?",1513189085.0
combinatorylogic,I am not sure alchemy is the right analogy. I'd rather call it a cargo cult.,1513415989.0
skulgnome,It always was.,1513178345.0
psirenny,"Good on Yann LeCun for calling him out on his click-baity alchemy metaphor. 

Scientists had trouble making progress with alchemy because it was shrouded in mystery. In contrast, machine learning models are shrouded in complexity. Those are two entirely different problems.",1513202777.0
MangoTux,"Break it out into a series of smaller problems, ordered chronologically. From there, problems become easier to solve in entirety.

Here are the steps:

Prompt for Top-Pick videos (And store response)  
Prompt for Oldies videos (And store response)  
Calculate the total cost (And store the result)  
Output the total cost

From here, it's pretty easy to replace each step with pseudocode, which doesn't need to be more difficult than ""display 'Please enter value'"" or ""result = x * y"" - It's just meant to set up the flow of code that can then be implemented in a language of choice.

That being said, this subreddit generally isn't the right place for help questions. You might have better luck in one of the related subreddits on the sidebar, especially if you show what you've figured out by yourself so far.",1513107166.0
WhackAMoleE,"    input numTop and numOldies
    output 3*numTop + 2*numOldies
    ",1513107207.0
,"Comcast, AT&T, Verizon, BoA, and several other companies need to be broken up as well.",1513093005.0
lordalch,"We're headed for corporate feudalism, no doubt, but at least we'll have free two-day shipping.",1513092741.0
,How exactly is Apple a monopoly? ,1513099131.0
dijit4l,This needs to be posted to /r/mealtimevideos ,1513095297.0
jet_heller,"While I'm not arguing (and would add more, like some cellular providers), none of these companies are close to having monopolies that will allow the government to do this.",1513093229.0
psirenny,"I wish all of these talks about the downsides of large tech companies were less hyperbolic. It's such a turn off. For example, he compares Facebook to the Darth from Star Wars. Fear mongering is a great way to make a point /s.

I mean... the video itself is hosted on YouTube (owned by Google). Does he not find that ironic? Literally, none of the services give a crap and will host this video anyway. The point is that their platforms are open to others to post content. These companies aren't evil or good. They're just profit seeking.

There's also so much counter factual information. He asserts that these companies are media companies and not platforms. He then goes into some spiel about how they label themselves platforms as a sleight of hand to avoid regulation. I don't claim to know their intentions, but they clearly aren't media companies. They don't produce their own content or news. OTHER people share content on their site. That's the reason they're called platforms.

Man... misuse of language, hyperbole, and fear mongering really set me off. Maybe it's an effective way to galvanize other people to take action but I personally find it makes me not want to listen.",1513099661.0
Dr_Legacy,"Wonder why Microsoft, Ebay, and Paypal didn't get mentioned.",1513122129.0
my_shiny_new_account,"This belongs in /r/technology, not here.",1513113864.0
hilberteffect,Scott Galloway says a lot of shit.,1513122850.0
MCPtz,"I'm not sure how to break up Facebook.

I am sure there are very simple and straight forward regulations that could be implemented to limit Facebook's power, which is being used right now to attempt to influence a Senatorial election in Alabama.

Many of those regulations could apply to Twitter, Google advertising, Snapchat, Instagram, and others.",1513098454.0
stefantalpalaru,https://en.wikipedia.org/wiki/Computer_science,1513177489.0
skydivingdutch,"Man has opinion, more on this breaking story at 11.",1513152823.0
IJCQYR,Sounds like someone's about to have a little accident while using his smartphone... ;-),1513112191.0
cjrun,"That was great. He was hardcore in evidence and delivery effective. I might check out the book now. 

Thanks for posting!",1513094082.0
,"Hmmmm how about we as consumers regulate the market ourselves? Do we really want the government wipe our butts every time we take a dump? Are we as powerless as we think we are? Isn't there anything at all we can do, like stop using Facebook, Google, Amazon? Are we capable of coming up with our own solutions for the market? Just asking.",1513104738.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/infrasociology] [Scott Galloway Says Amazon, Apple, Facebook, And Google should be broken up](https://www.reddit.com/r/infrasociology/comments/7kkd70/scott_galloway_says_amazon_apple_facebook_and/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1513592273.0
SteeleDynamics,"Break up all the things, huh?

No Microsoft? No automobile companies? No banks? No pharma companies? No insurance companies? No FPGA companies? No Verizon? No Comcast? No Sinclair Broadcast Group?",1513138286.0
pynewbie,I said this in r/tech about Google and got downvoted to shit.,1513118762.0
RareMatter,Let’s break up all companies larger than 2 people,1513110434.0
ianwold,"Those four but not Microsoft? As a .NET developer, I feel good about that 😄",1513142828.0
omtnt,Sometimes I think Elon Musk is the only one who can save humanity,1513148612.0
hackingdreams,"...not without Microsoft (and maybe Oracle) also joining that group. Because *FUCK* going back to the Microsoft Monopoly. The DOJ even *tried* to break them up, and through bickering and sitting on hands never did...

In reality, we'll never bust up these companies. What's far better, and more likely implementable, is lobbying against the mergers that have been happening that have made these companies such unapproachable giants, and implementing sanctions whenever one of these companies oversteps their antitrust laws (sanctions like they do in Europe, which while still small, are infinitely better than what we do: fucking *nothing*).",1513149382.0
nemesit,"try it and they move completely to china or some other better country lol, why would they pay more taxes for nothing in return anyway?",1513096151.0
redsoxfantom,"(shamelessly stolen from a much funnier commenter): I didn't want to affect the outcome, so I didn't read this article",1513087256.0
sulumits-retsambew,"After skimming the article it's apparent to me that this is a somewhat of an apples to oranges comparison.  A machined learned index for instance as a b-tree replacement does not function with the same precision and accuracy as a b-tree, it is more akin to a probabilistic algorithm. The learned model aims to minimize the error (which is likely not be 0) while a b-tree is completely guaranteed to find or not find the record with predictable performance.

Probabilistic indexes may have their place in some cases of-course.
",1513031370.0
stackered,Direct link to pdf on arxiv: https://arxiv.org/pdf/1712.01208v1.pdf ,1513021964.0
Prcrstntr,"Silly question, but I thought that hashes were always super great. How can something be faster than a hash?",1513033747.0
aranciokov,"(Somewhat offtopic, related to how the website works, I think)

Something bad happens [here](https://imgur.com/a/GkdeH) D:",1513021238.0
,"So essentially, tailor made indexes are better than generic data structures....... who would have ever thought that was the case..........",1513031508.0
inf-groupoid,"The paper is titled “The Case for Learned Index Structures”, but the case seems to be very weak. They should have benchmarked these so-called “learned indices” against *real* data sets, like an ERP's database. Only if learned indices can handle (ACID, of course) transactions at the scale of an ERP system, then should DBMS implementors start paying attention.",1513088224.0
dannyvegas,Selective overfitting?,1513049232.0
umib0zu,"[This is not a new idea at all.](http://www.cs.utoronto.ca/~rsalakhu/papers/semantic_final.pdf) When you start learning about topology and the problem of taking high dimensional spaces equipped with a metric, and mapping them into low dimensional spaces that respect the metric, you realize this idea is not only not new, but is a really important motif in all of mathematics. The neural networks have an added bonus that they can map seemingly related objects to ""nearby"" indexes. The fun part is you really don't even need a neural network, as there are plenty of methods that exist to embed high dimensional spaces into low dimensional indexes equipped with a metric.",1513091311.0
Hollowprime,"Shouldn't that be obvious given the machine learning A.I. has beaten numerous times the classic chess A.I. whic h is based on B-tree structures (minimax,forward pruning etc) ?",1513020179.0
cerberus6320,"That sounds great, but if you have paralysis by analysis, you'll never be able to take your programs anywhere outside of academia or a private hobby.

That sort of applies to most fields. Don't be so blinded in the pursuit of perfection that you can't create anything. If you want to make something perfect, go out and try to make it and learn from your mistakes. If you can learn from others mistakes even better",1513007288.0
Blackbeard2016,"Ew, font vomit",1513008712.0
,*composed correctly,1513007202.0
obamabamarambo,"Well, considering that both of these things are just really Markov chains, i think it's best to say that maybe Markov invented page rank",1513008016.0
MaunaLoona,Exhaustive enumeration sure sounds better than brute force.,1513001225.0
Funktektronic,"Interesting project!  You know, with a little bit of CUDA/OpenMP4+/OpenACC code you could probably batch up your bit bashing routines and get some decent GPU performance on this problem.  The integer units in GPU hardware are not great, but there are a lot of them in a modern card.  It might be worth a try.",1512976735.0
wollae,Saw the title and thought this would be an article about the state of math in 31 years.,1513008763.0
khatvong,"This is neat. I wonder whether there is a faster approach using binary decision diagrams or a variant thereof. If the state transition were represented by a binary function, BDDs could allow for counting states without actually enumerating. The question would be, how to actually find all the fixed points of that function.",1513019483.0
ArlenM,"I recently took a class and we used;

https://swish.swi-prolog.org/

Free, and seemed to do the job fine.",1512952931.0
Ferdinand-Wu,"Are you a talent computer scientist in academia? Then we need your hand! Recently we have built an online paper commenting hub. Could you spare a little time to contribute and let's build it together for a better academia :)

**What's your way to pick an academic paper when facing a pile of literature??** By impact factor? By recommendations? Or…by feelings? Before trying, how do you know the paper in your hand trustworthy as your step stone? That’s why I was dreaming of a journal club where I can find comments on papers before reproducing or examining it. And I know some trials and experiments may take more than one or two months for a single round…

We are a small Danish team presenting a not-for-profit journal article hub “[Crinetic](https://www.crinetic.com)” which serves as a compass for academics, where you can find discussions and ratings of research articles, and more features fitting your needs. Just grab a “DOI” and input your comments, with simple steps and in minutes more readers will be benefited from your contribution. Of course, you are welcome to participate anonymously!

As a newly established database, we do need you to make it grow and prosper. If you have any feedback or would like to join the development, don't hesitate to PM here or on [Crinetic Facebook](https://www.facebook.com/crinetic/). We appreciate your participation, and of course, please help share this post and spread the word.

[Facebook](https://www.facebook.com/crinetic/) @ www.facebook.com/crinetic/

[Official Page](https://www.crinetic.com) @ www.crinetic.com",1512927226.0
youanden,"Typo on registration ""Email adress""",1512932293.0
Ferdinand-Wu,"After posting here, I have got many suggestions, thanks for your encouragement. It's a long-term project and we will put your ideas in our next version!

And here I would like to share some innovations Crinetic equips. While we are doing user investigation, time investment for commenting normally is the biggest obstacle. To solve this, we designed Crinetic with many innovations. We discarded the traditional commenting method as all commenting likewise sites do. We introduced ""praise"" and ""criticism"" labeled comments, thus users are encouraged to express concisely and in a bullet way. By this, readers may have a good shape of the paper at a glance. Of course if you wish, you are more than welcome to upload a ""supporting material"" to back up your comments in your own format.

For instance, **""I repeated X experiment exactly as described 10 times and could not reproduce these results""** as others mentioned here who are searching for a place for this kind of comments, it is a perfect example as a ""criticism"". It's short but it's from the effort you put a lot in the work. The concise comment acts as an alarm to academia, not a conclusive and sound judgement to the paper which needs concrete materials as evidences. Actually the short comment has benefited the world because people might try to ask and discuss what's the hidden parameters behind the paper before doing, that's the scenario we are dreaming of :)

With your help, we are together building Crinetic as a compass for academics, you can see it as a tool to assist your papers hunting. It shows paper's feedback from ""comments"" but readers would not just stop in checking comments. They can explore further by checking ""Suggesting readings"". It might be a difficult project but we are kicking it off and hoping people may get involved in it.

Have any thoughts about a paper, we are looking forward to you on [Crinetic](https://www.crinetic.com)!",1512988195.0
Wallblacksheep,"Is this like the _goodreads_ of academic papers?

https://en.wikipedia.org/wiki/Comparison_of_reference_management_software#Password_.22protection.22_and_network_versions",1513792670.0
vastlik,Stop using coinhive without users knowledge. Otherwise nice work.,1512936199.0
datafile4,"Emm, my Symantec detected Coinminer on this resource",1512981688.0
mcandre,https://copy.sh/brainfuck/,1512929149.0
FloatingBlimp,My professor went there! ,1512883656.0
jmite,SPJ looks like he just lost hide and seek!,1512799489.0
MyNameIsDan_,"Off topic but related: I had a course with Farzan for 2nd year computing theory. My worst experience in a course ever (average was 30%, got petitioned to the undergrad chair) and steered clear of her courses ever since. I even dropped it and took the accelerated variant of the course and did/learned better lol. Though I heard she was decent for software verification course. 

She’s well known in the faculty for her research but rather controversial when it comes to teaching. ",1512838070.0
DeusExCochina,I would have liked to see Rich Hickey (of Clojure) in this lineup.,1512842243.0
sailorcire,"> Zoe Paraskevopoulou


> works with Andrew Appel


Gee...I wonder what language she works in. 😒",1512849246.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1512749814.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1512748237.0
datinghell,"Max dimensions 7, huh?
>However, our most intriguing result is that d = 7 serves as
an unexpected dimensional threshold at which the quantitative
equivalence between the two quantum resources disappears.
A satisfying explanation of the origin of this phenomenon is
still missing: What is so special in dimension 7?
",1512759026.0
SiddharthG5,"If you are a novice computer science enthusiast, Crash course computer science by Carie Ann  playlist on youtube is the place to start... even for experienced people many videos are helpful.. It starts from scratch (early computing) then all the way through memory, CPU to AI machine learning and robots... Its accurate fast and simply awesome. 

https://www.youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo",1512738086.0
vijayant123,"    Hi, A good question here.

So, basically we have been restricting our process executions by memory space and run time. I think the non-availability of infinite run space causes it to be designed as a finite timed system.

What are the computations and problems we would be solving if we had an automaton with an infinite volatile memory. And was reliably powered constantly.",1512742849.0
mraheem,Linked list are annoying to understand -_- especially when your behind and your gonna know binary lists too,1512717985.0
ColdDemon388,Do you find formal education sufficient to learn programming? Or does it have any meaningful impact at all with the majority of your skills learned independently?,1512661602.0
,Is there a way to do or contribute to research in Robotics and AI without doing honors or masters?,1512726092.0
sparcxs,"This is a really good write up, nice find!",1512701354.0
MassiveDumpOfGenius,"Lovely writing. One thing, am I the only one seeing `?0?=1 ?0? + 0 ?1?` everywhere? I feel some of them are alpha and some of them are beta.",1512718454.0
bald_cyclist,The website is really difficult to navigate on mobile. Is this on purpose?,1512719773.0
anowlcalledjosh,"https://en.wikipedia.org/wiki/Betteridge%27s_law_of_headlines

As I see it, a blockchain is (hopefully) linear - long-lived or intentional forks are not normal. Git, however, uses forks extensively: one of its major selling points is how easy it is to make a new branch.",1512652202.0
fw5q3wf4r5g2,"Git is a Merkle Tree (or rather, DAG)

A blockchain is a Merkle Tree

A monkey is a primate

Therefore, Humans are monkeys",1512735660.0
VeganBigMac,"So, the conclusion is, if you use a very general definition that nobody uses, sure.",1512651896.0
wackyHair,Is mayonnaise a blockchain?,1512791679.0
orksliver,"If you are looking for a common pattern shared by these two things I suggest.
 https://martinfowler.com/eaaDev/EventSourcing.html",1513307639.0
AllTom,"I'm missing something… What does this accomplish?

It starts with ""Using this multiply machine."" Is the table described afterward a multiply machine, or is that something that was introduced earlier?",1512668955.0
Differenze,"This is actually related to what I am currently studying, we encounter this as ""cache blocking"" (to give you something else to Google after)

In the first example the cache remembers the blue 1 (from lhs) so we read that value once and then every value from the orange line once. So although we do 4 calculations with two numbers each we only access slow memory 5 times (instead of 8)

Hope this helps you along the way, I can explain further later today if you still have questions",1512648278.0
ChaosCon,Good old [Morton Z-ordering](https://en.wikipedia.org/wiki/Z-order_curve).,1512648831.0
Holdupaminute,"I must admit, this is way over my head and I struggle to understand it. What subfield of study is this? - and where can find more information on it, so one day it can not be way over my head?",1512694023.0
promach,"In the screenshot above, why ""(load#1 LHS1, load#2 RHS1)"" ? 

I suppose LHS1 and RHS1 are just the table indexes, and NOT the actual data that is going to be loaded into cache.",1512702060.0
promach,"I still do not get how this loop tiling mechanism works.  anyone ?
https://software.intel.com/en-us/articles/efficient-use-of-tiling ",1512635036.0
promach,"1) Could anyone point out how 'RRB' actually works in the screenshot above ?  Note: extracted from http://www-inst.eecs.berkeley.edu/~cs152/fa16/handouts/ps4-handout.pdf

2) For https://scihub22266oqcxt.onion.link/10.1145/143103.143141 or http://sci-hub.bz/10.1145/143103.143141 , why is rotating register file a hardware feature implementation of modulo scheduled loop ?

3) Could anyone explain ""Register allocation prior to modulo scheduling places unacceptable constraints on the schedule and, therefore, results in poor performance."" ?",1512625958.0
promach,"For rotating register file, why do we need two read ports and one write port ?",1518573099.0
Iskaral-Pust,"The Omega Tau podcast did a fantastic episode on analog computers a while back. Well worth a listen if you find any of this remotely interesting (episode 159):

http://omegataupodcast.net/download-archive/ or whatever podcast service you like.",1512604720.0
zsaleeba,"Strangely, my father literally did work on an analog computer. That was at the former [Aeronautical Research Laboratories](https://trove.nla.gov.au/people/616800?c=people) in Australia. The machine was used to simulate flight dynamics and it was pretty good for that purpose.",1512611646.0
gwern,"Ah, it's that time of year again.",1512603775.0
jaLissajous,"> Only experiments can confirm that a computer of this type would actually be feasible and that the accumulation of analog errors would not conspire against it. But if it did work, the result would be far beyond what today’s digital computers can do.

This is the real problem, one otherwise not acknowledged by the article.

The Analog systems I've heard of lack the facility for error-correction, so errors accumulate until the system breaks down. If they (or another team) have come up with a way to control, detect or correct errors in the analog representation of real numbers it would be a paradigm shift in mathematical computing.",1512622361.0
emilern,"A great read!

If we want to simulate an analog process, it only makes sense to use an analog computer to do so. Maybe in the future we can (again!).",1512599398.0
VermillionAzure,"Unfortunately, I do not have access. :(

But does the paper think about analog computing? I would guess that since neural networks are inspired by and are trying to mimic analog systems, making the processes ultimately analog vs. digital might be preferable, but I know nothing.",1512551717.0
simiananomaly,"Thanks, this seems very thorough, i didn't know there was so much diversity in the field. This reminds me of a paper by Traversa et al where they use memristor-based architecture to solve NP-complete problems in poly time. I could never understand how this was even possible...",1512557282.0
trcytony,Here is a profile story about the author: https://syncedreview.com/2017/12/06/neuromorphic-computing-yiran-chens-brainy-architecture-of-the-future/,1512857271.0
mailmygovNNBot,"**Write to your Government Representatives about Net neutrality**


(The brand new) [MailMyGov](https://www.mailmygov.com) was founded on the idea that a real letter is more effective then a cookie cutter email.
MailMyGov lets you send *real physical letters* to your government reps. We can help you find **all** your leaders:

* federal (White house, House of Representatives, Supreme Court, FCC & more)
* state (U.S. Senate, Governors, Treasurers, Attorney General, Controllers & more)
* county (Sheriffs, Assessors, District Attorney & more)
* and city representatives (Mayors, City Council & more)

...using just your address *and send a real snail mail letter without leaving your browser.*

**https://www.mailmygov.com**


**Other things you can do to help:**


You can visit these sites to obtain information on issues currently being debated in the United States:

* https://votesmart.org/
* https://www.govtrack.us/
* https://www.aclu.org/
* https://petitions.whitehouse.gov
* (suggest more sites here? msg u/mailmygov please with un-biased, non-partisan factual sources only!)

Donate to political advocacy

* [Set up your favorite political activist orgs as you charity on Amazon Smile](https://smile.amazon.com/)

**Other websites that help to find your government representatives:**

* http://www.house.gov/representatives/find/
* https://whoaremyrepresentatives.org/
* https://www.govtrack.us/
* https://resistbot.io/
* https://democracy.io/#!/  (will send an email on your behalf to your senators.)
* https://www.usa.gov/elected-officials
* https://www.senate.gov/general/contact_information/senators_cfm.cfm?OrderBy=state

Most importantly, ***PLEASE MAKE AN INFORMED VOTE DURING YOUR NEXT ELECTION***.

Please msg -/u/mailmygovNNbot for any concerns. Any feedback is appreciated!
	 
	 
",1512540550.0
Coloneljesus,Just say m = max - min + 1.,1512522841.0
luckygerbils,"So with some (albiet low and controllable) probability your result will have some extra values that weren't in the original array, right? And all duplicates are removed (so really it's a set). 

I feel like those qualifications should be mentioned at the beginning (especially since ""sorted"" is defined as allowing duplicates). 

It's interesting to think of cases where this might be useful despite those side effects. I wish the authors had gone into that more rather than the parallel algorithm (which doesn't seem that interesting is my opinion).",1512526480.0
AutoModerator,"Your submission has been automatically removed. In order to fight abuse, submission to YouTube videos must be approved by the /r/compsci moderator team in advance. Please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcompsci) with a link to the YouTube video you'd like to submit.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/compsci) if you have any questions or concerns.*",1512486126.0
07dosa,Eerie. It’s like 4-dimensional beings have invaded the earth and planted fractal trees to terraform the planet.,1512451944.0
spilk,this triggered my [Crystalline Entity](http://memory-alpha.wikia.com/wiki/Crystalline_Entity) PTSD,1512486778.0
pastermil,original post: https://www.reddit.com/r/perfectloops/comments/7e4ajj/n_10_aoc/,1512451371.0
bilboinlondon,"aww we had to implement such a tree in class with recursion, was quite fun... only in 2D though",1512468762.0
,[deleted],1512461366.0
Blokatt,"Oh hey, I made that! :D

[Here's the code if anyone's interested.](https://github.com/Blokatt/ProcessingStuff/tree/master/nEquals10)",1514644570.0
mayhempk1,That is cool and scary. ,1512473836.0
KBKarma,I suspect The Laundry might get involved if n gets any larger.,1512504923.0
TiramieTeitoku,At first I thought this is from r/surrealmemes,1512519625.0
kobj__,This really creeps me out.,1512455673.0
Aatch,"Wow there's a lot wrong with this.

Saying that C++ is a 35-year-old language isn't really accurate. Sure, the first version was released then, but it's been changed and updated over the years. C++11 was a big jump and C++ has been updated a reasonable amount since then.

The article never actually says why these ""old"" languages are bad. Probably because there isn't a good argument. At best you can say they aren't suitable for some tasks, but the article doesn't even do that!

I decided to read the PDF linked at the bottom and it's basically the same thing, but more words. My frustration with this topic (graphical programming languages) is that they never seem to properly consider why even modern languages are the way they are. Sure, momentum is one factor, it's hard to do something different. However, the fact that plain text is basically a universal format plays a big role.

The worst thing here, however, is that the author doesn't actually present an alternative to the status quo. Saying ""we could do X, Y, Z"" is one thing, but how would that work in practice?",1512446622.0
personalmountains,"I was thinking of a point-by-point rebuttal, but I'll just quote that:

> Since there has been such a dramatic leap in processor speed, storage size, and graphics chip technology, the tools we use to make computers perform for us, programming languages, should definitely be correspondingly different today compared to thirty years ago.

and

> The current objective of programming language designers is to perpetually introduce programming languages that originate from the days of mainframes and then add minor features on top of them.

Nothing in this article makes sense and it's not worth your time reading it.",1512446909.0
IndexingAtZero,This is probably actually the worst article I've ever read. ,1512447164.0
waterproofpatch,"What garbage. 

tl;dr author wants to write programs using symbols they used when writing answers for a math test and can't, and invents a straw man argument (which author never bothers to back up) as to why that is the case. No substance, no meaningful arguments, not worth reading. 

The title itself is clickbait at best, gratuitously dishonest at worst. Reeks of someone who was late getting a paper done and crapped this out at the last minute on some weird binge combination of caffeine and alcohol.

~ A guy who wasted 10 minutes reading it.",1512450311.0
Hougaiidesu,The article says Swift isn’t much different from Objective-C. As someone who has used both.... this is totally wrong.,1512451083.0
combinatorylogic,"> Advances provided by 2017 technology, such as VR, are now programmed with a 1980's programming language and this needs to be explained. 

Explaining: if your latency in VR exceeds the minimally comfortable 90Hz refresh rate, you'll throw out your breakfast. Therefore, code in C++ and stop whining. 

",1512466846.0
,"Meanwhile, I am happyily using whatever Unicode offers me when writing Racket - which could be described as the second-oldest-language in use (Bit of a stretch but it's a Lisp).

While I think the way we programm today may not be set in stone (consider how young our discipline is), reading this article did not quite give me the 'visionary'-vibes.",1512562655.0
,[deleted],1512428451.0
phthalochar,"I'm glad the discussion is about moving sexbots away from humanoid figures.
I think that's an interesting answer to the problem of correlating taboo sex acts with real people. ",1512425703.0
,[deleted],1512429998.0
ReginaldIII,"This isn't really the place for debugging and error message based questions. However, since it seems to be a simple error, here is a simple solution. 

Run:

    pip install numpy

on the command line, then run:

    jupyter notebook

If you use Anaconda instead of regular Python then use:

    conda install numpy",1512394146.0
inephable,"Since you apparently have it installed, try restarting your notebook.",1512397972.0
varunagrawal,"I'm guessing you're installing numpy in Python 2 by default and the Notebook is running Python 3. An easy way to check is run `pip -V` and see what python path it spits out. 

You should try `pip3 install numpy`.

If your system complains that pip3 cannot be found, an easier fix is to switch your python version in the notebook to Python 2.",1512398423.0
lordosthyvel,"It seems that you have not installed the numpy package required.
Check it out on this link
https://www.scipy.org/scipylib/download.html",1512394065.0
jonsterling,But where is the type theory in here?,1512392299.0
,"Syllabus looks good.

",1512336299.0
badmash108,"It is part of GaTech OMSCS program and is very well structured. Projects are tough. Content is good. Ada has a way of explaining everything with examples so it is worth a try. Its free, you can watch the videos, if you don't like the approach then go to our sources",1512367284.0
Theoneaxe,"Didn’t see processes with Unix forks and semaphores, otherwise its good.",1512292987.0
blindingspeed80,"Standard os stuff, except for weenix.",1512315891.0
,Not that good.,1512320167.0
CyAScott,"I know it depends on the professor. Our projects were difficult because we had empirically find things out about the OS like the average time for a context switch, the delay in time from reading from multiple pages vs one, etc. The other professor had similar projects but the code ran inside an OS simulator that made it easy to find that data.",1512316335.0
_0110111001101111_,"Semaphores, deadlocks and synchronisation are missing. Everything else is alright from what I can tell. ",1512317459.0
lgroeni,"Pretty standard, although I do take issue with having a separate lab course where you implement Weenix. Totally unnecessary in my opinion, and it precludes the main class from doing projects on advanced topics.

Personally, If I was writing the syllabus I would kill one of the file system programming assignments and replace it with something involving scheduling. I’d probably also kill the uthread/mthread pair in favor of a single thread assignment and maybe something involving virtual memory. Mine did an assignment on memory prefetch, for instance, but there are a ton of things you could do with manually mapping pages of memory.

But yeah, pretty typical OS course. The important material seems to be there, even if I’m not a fan of the structure and presentation. For anyone who isn’t going to touch kernel mode programming again it’s probably fine.",1512318023.0
claimred,[Author's eigenvectors example for graph isomorphism](https://encode.ru/threads/2739-Cryptographic-attacks-as-minimization-of-degree-4-polynomial-\(through-3-SAT-problem\)?p=55127&viewfull=1#post55127),1512253948.0
jarekduda,"Thanks, just was told about this thread. I was trying to find some exotic approaches - like using Grassmann numbers, or just integration question. I got some interesting perspectives, but didn't get close to answering P vs NP question (yet?).

Slides: https://www.dropbox.com/s/nwyxf44u38i42d8/pnpslides.pdf

Regarding graph isomorphism problem - looking at eigenspaces, we can transform it into a question if two sets of points (forming very regular highly dimensional polyhedron) differ only by rotation.

So this way we get geometrical interpretation, allowing to use tools like rotation invariants - there are known in 2D (Fourier) or 3D (spherical harmonics), but seem difficult to generalize to higher dimensions (?)

However, ellipsoids defined by quadratics are easy to test differing by rotation (testing characteristic polynomial) - the main missing question now is if they can well describe an arbitrary set: if ""wobbling"" ellipsoids with fixed some points, do their intersection adds some extra points?

More mathematically:  https://math.stackexchange.com/questions/2547251/analog-of-vandermonde-determinant-for-fitting-a-quadratic-form",1512388388.0
SirClueless,"> The total possible state space for a game is 9!

Plus a little for incomplete games. Minus a little because some games are complete in under 9 moves. Minus a little because some game states are reachable in multiple ways. Wouldn't it be more reasonable to estimate the state space being of size 3^(9)? (Empty, O, or X in each of 9 spaces).

> set of game-ending-moves: (5,6,7,8,9) => 5 [halting states]

There are hundreds of game-ending moves, no? I would say a ""game-ending move"" is something from the set of game actions, not from the set of move indices.

> We also assume:  
> The median number of moves in a game => 7

There are presumably many more games of length 9 than games of length 5. In fact, I would guess the median length of a game is, in fact, 9.

> If we can assume the computational complexity of a game has Big O of 2^N

Where is this coming from? No one in Tic-Tac-Toe is making a binary decision.

> Tic-tac-toe has a Game-Tree Complexity of '5'

We've already determined that there are ~9! possible completed game states. So the game-tree complexity should be on the order of 9!. Game-tree complexity is about the size of the entire tree, not just the depth of the game.

You've lost me for the remainder of the discussion. You seem to be dividing the size of the state space by an upper bound on the computational complexity, I do not know to what end.",1512294481.0
lkraider,"Problem: how to encode ""the only winning move is not to play"" using Sets.",1512249144.0
,[deleted],1512247750.0
NeverQuiteEnough,adds that pause when you disengage,1512202025.0
PM_ME_YOUR_ESOLANG,"The quintessential example of parallel but not concurrent is SIMD registers

Registers which can be loaded and operated simultaneously as a single register https://en.wikipedia.org/wiki/SIMD",1512164732.0
CyAScott,"The game of life can be done in parallel.  Each tick of the game requires several calculations, but the calculations can be done in any order.",1512191330.0
SirClueless,"One example is the Go runtime. The language uses goroutines and channels to allow for parallel computation that could in principle be concurrent. But if GOMAXPROCS=1 is set, then there will be only one OS thread running all of the user-level Go code. This was the default for a long while while they worked on the language and made it more stable and performant.

Another is event-driven environments like JavaScript (both in the browser and NodeJS), or Python's [Twisted](https://twistedmatrix.com/trac/). These are perhaps not strictly parallel in that they are explicitly single-threaded so you can do things like mutate global state safely so long as you don't make any asynchronous calls, but programs in these environments are often structured as parallel computations that make asynchronous IO calls.

Edit: Hmm, not sure why I'm getting downvoted. If anyone thinks these are bad examples I'd be curious to learn why.",1512169650.0
maybachsonbachs,Not infinite stream. A stream of unknown length. It isn't possible to pick a random natural number uniformly.,1512103534.0
foreverblues123,Reservoir sampling,1512136789.0
beefsack,"In the sample code, how does the loop terminate if the steam is infinite in size? It doesn't seem to explicitly terminate or yield.",1512128973.0
vagoon,"the special case for i = 0 is not necessary
",1512123222.0
Terr_,"I'm trying to think of a simpler way of describing this approach, and so far this is what I've got:

Suppose you have a set of choices, and at every moment someone can either add 10 more to the pile, or finish and ask you for random choice.

 You don't know how many times it might +10, but when it finishes you have to provide a random choice from everything you've seen.

So at each step beyond the first:

1. You have N items, and previously chose one of them as your candidate.
2. Now you have N+10 items. Pick a random number from  N+10 choices.
3. If the number is <N simply keep your old candidate
3. If the number is greater, subtract N and that is the index of the new candidate within your fresh batch of 10 items.

This means the chance of something from each new batch being chosen goes down depending on how many batches were already looked at. When you've lowered the batch size to 1, you can simplify out some steps and get what the article does.",1512152798.0
agumonkey,"Hey, recently been curious about re-reading recursion analysis (cost, master theorem, etc). I'd love an overview of the field if someone has this. Also is there any extension to graph traversal analysis ?",1512351139.0
Wallblacksheep,"Can anyone recommend any _pop_ CS books for someone going on a long vacation? I'm most interested in computational complexity and algorithms. _Grokking Algorithms_ is one I'm interested in, but something more on the complexity side.",1512524616.0
