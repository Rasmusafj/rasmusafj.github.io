author,comment,timestamp
OvidPerl,"I would use the ISO time with UTC. The unix number isn't portable because it's based on the Unix epoch, [but computing systems use a wide variety of epochs](https://en.wikipedia.org/wiki/Epoch_(reference_date\)#Computing), making the Unix epoch potentially less portable.

Further, many times when sifting through data, developers are staring at raw date strings. When you're trying to debug a problem with a report of sales since the beginning of 2017, is the date `1483227800` a problem or not? However, if you see `2016-12-31T11:43:20.000Z`, you can see a bug in your data.",1543592007.0
prite,"Storage: Number of units since epoch

Display: ISO 8601 in local locale

For reference, see how PostgreSQL does it.",1543597216.0
Rhino02ss,Do you need to worry about timezone or daylight savings time?,1543617923.0
mcandre,"ISO8601 is too flexible. Used a more fixed subformat like RFC3339.

UNIX epoch time can fuck right off, it doesn’t even have a way to explicitly label the active time zone, let alone the units. Really, we just assume UTC and nanosecond everywhere?",1543614119.0
Starmage21,"If you use the unix epoch time, conversion is simple to a standard date-time format:

Jan 1, 1970 12:00:00 AM + UnixTime (could be ms, but usually seconds)

&#x200B;

Also unix time allows you to math a little easier, since its a number field. Endtime - starttime = seconds of duration then parse for display

&#x200B;

this site is fun www.epochconverter.com",1543597799.0
not_stoic,Unix,1543591571.0
grauenwolf,I'm not sure that using MongoDB as the de facto query language standard is a good idea.,1543526191.0
welshfargo,You might find something useful here: http://www.databaseanswers.org/data_models/,1543517172.0
alazicza,"Can you be a bit more specific what it means (what is required). start from the CAP theorem and from there decide what is required. Which characteristics are required from there. There is a good illustration on the net which C A P properties are met by which databases. This will then tell you which dB to set up.

If you just want a quick and simple you can do SQL Server using AlwaysOn replication but read the CAP theorem as this will give you an idea on Synchronous vs Async replication (what are you sacrificing in cap here ) etc.

MySQL allows for distributed deployment but I think replication usually requires (or at least us) a 3rd component for replication. I think it was a little bit more work to set it up in in our case but cannot remember details now",1543467846.0
ToadingAround,"If you can post the assignment details here, while it would make it extremely easy for people to tell you how to do things (which would be terrible, the point is to learn) it'd make it a lot easier for us to point you in the right direction.

Also disclaimer, I have no idea what's going on. I'm just a web dev and this sounds like sharding to me (which if it is could be fairly simple), but I could be totally wrong.

EDIT: Something that is extremely relevant to this situation - when you're asking for help to solve a problem, always start with the problem, and make it the focus. Don't go too much into what you're trying to do, because it immediately makes it a case of ""how do I fix whatever i'm doing"", which could be off-point. Approach the problem first and foremost.",1543483032.0
imcguyver,You you go: https://github.com/kiwenlau/hadoop-cluster-docker,1543469202.0
alinroc,"You've got way too much code here that you're asking people to read through for free. Condense this down to a minimal example that reproduces the issue. You might even find the solution that way.

This much copy/pasted code in a ""real"" application is a code smell and whatever you're doing is probably the least-effective way to do it.",1543411209.0
liquidpele,Oh hell no,1543425390.0
ictatha,"This is would be more appropriate for /r/msaccess.

This is more of a Microsoft Access application question, not database related.",1543427693.0
grauenwolf,Number 3 was kicking my ass. Huge tables were being rebuilt with each deployment just because I used CONVERT instead of CAST. ,1543389280.0
svtr,"I'm sorry to say, but if your data is relational, as in you have a path that relates to categories, why not save it in a relational database, and have a 4 line sql query solve your problem?

I'm sorry, I really am, I try to be open minded, but ...... why do you want to force nosql on ""this is simple, right guys??"" things ?

&#x200B;

On NoSQL, you gonna have to tell which one, they all claim to be perfect, none of them are. On a document based one (im guessing here, guessing some more .... mongoDB?) You gonna have to have your ""it should be, cause it feels like it"" foreign key as a subdocument, that is not going to be to efficient, or you gonna have to do it as a ""it wants to be a foreign key"", key arrary, that you have to do lookups on the other collection.

If you try to do a select on the category collection, to then do lookups on a nested attribute on the ""other"" collection, oh boy. Index that.

Also, don't use a document based database for this, this is what relational databases are built for, what they excel at. This is so clearly relational data, that over this, I will beat to death, every damned fullstacky, with a stick of learning, it aint even funny.

&#x200B;

ps::  my non aggressive communication will commence tomorrow morning when i'm getting payed to be ""nice"" again.",1543368215.0
Xalem,Check your tables to see if the record count has gone WAY up.,1543362786.0
ComicOzzy,You can also try creating a new access file and importing everything to the new file. ,1543377658.0
edimaudo,It would depend on the data in your tables.,1543458319.0
liquidpele,"This ""article"" is like 1/10 of what it should be, and doesn't even explain the SQL it just dumps in there.  ",1543367123.0
TheEmperorTyrgils,"I think your answer lies [here](https://www.mysql.com/support/supportedplatforms/database.html)

MySql does not support any Mac OS under 10.13 which is high Sierra. You're either going to need a Boot Camp launch of Windows or a Linux distro to get that working, at least in an official capacity.

Hope that gives some explanation.",1543323306.0
wolf2600,"Are you required to use MySQL?  If not, I'd suggest PostgreSQL.

https://www.postgresql.org/download/macosx/

Working with large data sets, MySQL is not preferred as it only has the ability to do nested joins (which are horribly slow/inefficient when joining two large tables).",1543323551.0
lowercase00,Maybe try MariaDB,1543331668.0
LeftJoin79,"Tons of basic laptops on ebay for $100 that are way better. I know money is tight, but come on. Trash that thing.",1543333143.0
Alterr,You can run mysql in a docker container on your laptop. Plenty of tutorials on the internet and will run on all machines if you can install docker on it.,1543339020.0
mkingsbu,"Do you have an extra ~$5 a month?  You could setup a Digital Ocean droplet (virtual machine), put the database there and then SSH into it.  You can even use the GUI options like DBeaver with the SSH keys.",1543324961.0
ergo14,Postgresql is BSD licensed so it seems like a good fit.,1543264315.0
alinroc,"Any RDBMS will be up to the task. SQL Server Express Edition ($0) will probably be sufficient if you want to stick with MS tech, Postgres if you want to go open-source.",1543263065.0
jtwyrrpirate,sqlite: [https://docs.microsoft.com/en-us/ef/core/providers/sqlite/](https://docs.microsoft.com/en-us/ef/core/providers/sqlite/),1543261967.0
mkingsbu,"What OS are you deploying to?  My first recommendation would be SQLite. But if you need something else it might be possible to package something else, especially if you can deploy with say, a docker image.",1543288831.0
rose_anachronysm,"Maybe look into RocksDb, it ships as a part of Apachr Kafka and is ultra lightweight as I understand.",1543265044.0
israellopez,Firebird SQL is nice... it also has a server plus a client connection that you can use without a server.,1543264508.0
Golgothite,MySQL is usually the consensus choice as long as you don't want your clients to need a license.,1543262466.0
OvidPerl,"Prior to the creation of SQL in the 1970s, all databases where NoSQL. That's why we have SQL.

I work with a lot of clients who use NoSQL and most of them have _zero_ need for it. In one case it was so bad that I asked them why they made that choice and the senior programmer replied: ""because we misread the documentation.""

Or there was the client who was running at a maximum of about 40 transactions per second. They won a major contract that required 500 transactions per second and their technology director told them they needed to switch to NoSQL to get better performance. We got them to around 700 transactions per second in about three weeks ... using PostgreSQL. Most of their performance problems were simply a matter of technical debt and a poor use of their database. 

In short, trading SQL for NoSQL is trading a known set of problems for an unknown set of problems. I interview lots of programmers and I find that even excellent programmers often know very little about the care and feeding of databases. They assume that because they can write SQL, they can design a database. That's like saying you can design a car because you know how to drive.

Learn the basics of normalization. Learn how to configure your database. Learn how to tune queries. Learn when to denormalize (hint: only after all reasonable options are exhausted). Not only will you have valuable skills that many fellow developers don't have, you will be able to use many standard tools, crafted with decades of experience, without the blind skeet shooting that is often the NoSQL world.

**Don't use NoSQL**

... unless you have a clearly documented use case that your database can't handle. I recommend NoSQL solutions to clients, but _only_ after we have verified that their database can't handle it.

However, if you think you might need NoSQL in the future, make sure you design your app into clean layers so that your primary application does not know, or care, where it's fetching its data from. That will allow you to transition those parts of your application to NoSQL where needed.",1543250412.0
mkingsbu,I'd go with RDBMS wherever reads are at all important,1543240444.0
wolf2600,"I would use a standard relational database because it is the best option unless you have a use case where a relational database cannot be used.
",1543236722.0
Old13oy,"A RDBMS is great for comparing across records that have similar attributes while limiting the information that can be applied to a few pre-defined types, whereas NoSQL allows you to apply as much information as you want to any given record but gives you limited insight across multiple records. 

If you have a data model that you think allows you to have standard fields for every record, then it's probably better to go the SQL eoure.",1543240626.0
grauenwolf,"NoSQL is never the correct answer. (Unless the question is ""What technologies do I pick when coning some investors out of their money?"".)

There are special purpose databases such as Hadoop that make sense in very specific contexts. But if a relational database is a viable option, then that's what you should be picking. They won the debate about database design four decades ago for very good reasons. 

Most so-called NoSQL databases are just rehashes of those old, flawed designs that we discarded because they don't work anywhere near as well.

And in the few cases where NoSQL does have an interesting trick, chances are the relational database can do it too (or will be able to very shortly).",1543246802.0
boy_named_su,"What you currently have is called an Exclusive Arc Foreign Key. It's considered an [Anti-Pattern](https://www.oreilly.com/library/view/sql-antipatterns/9781680500073/f_0045.html)

What you want is called [the Party Model](https://www.ibm.com/support/knowledgecenter/en/SSWSR9_11.6.0/com.ibm.mdmhs.data.dict.doc/c_PartyDomainModel.html), which is a case of [Subtype/Supertype](https://searchsqlserver.techtarget.com/feature/Supertype-and-subtype-tables-in-SQL-Server) or [Table Inheritance](https://stackoverflow.com/questions/3579079/how-can-you-represent-inheritance-in-a-database)

You have an ""abstract"" type called a LegalParty. Individual is a concrete subtype of this. As is Orgnization (with subtypes being Company, Group, Government, etc). Your foreign key points to the supertype table

Use [Single Table Inheritance](https://martinfowler.com/eaaCatalog/singleTableInheritance.html) for performance, or [Class Table Inheritance](https://martinfowler.com/eaaCatalog/classTableInheritance.html) if you don't like `null`s

It's criminal that this isn't taught in every database course",1543089598.0
r3pr0b8,"> the attribute fk_submittedByID is associated to both the Person and Office tables

you will not be able to actually declare it as a FK to both tables

so it's not a FK

if you want it to be, and you should, create a supertype table for Person and Office, and FK it to that",1543095675.0
thebaziel,"I’ve come across codebases that do this. There are two columns, one the foreign key, and another a content type which tells it which table to check (in this case Office or Person). This is called a “Generic Foreign Key” in the Django ORM.

While saying it’s common practice at some places I’ve worked, I will say it’s really not my favorite to come across because it makes it harder to read how tables are connected (are those foreign keys to person and office the only things it can be connected to? Or just the only ones I've found examples of in the code base? How much looking so I have to do to be sure of that?), reason about code covering all the cases, and build complex queries.

I’m really interested to see what other people suggest instead.

edit: grammar",1543089645.0
mkingsbu,"Yes, this is the idea way of handling it so long as it meets the business requirements. Otherwise you'd have a column that was only populated.  The terminology for this is a subclass discriminator.",1543089687.0
changeupcharley,what statistics?,1543085396.0
jpers36,r/databasehelp,1543065047.0
cferranti,"Can you avoid the problem creating a data layer with authentication and permissions management like a rest API? In this way there is a complete separation of responsibility from the database management and its usage.
Is it a feasible way for you?",1543056468.0
wolf2600,"What do you mean by ""shared""?  External partners have the ability to login to the DB? Or the DB is replicated to multiple external sites?",1543069423.0
PandalfTheGimp,"Not enough information. If multiple users are wanting access, then it's as easy and providing each party access either as a group or individually. For what reason would a business partner be updating data within a database?",1543070658.0
cferranti,"Another easy solution could be maintaining a DB for each group/nation with attached a read replica and a ""non valid/untrusted/error"" communication way (a queue or an API). In scheduled time, a specific process read from each read replica, validate the data and copy into a master DB where only a central authority has access and where are stored all information with also audit log and ownerships.",1543082173.0
boy_named_su,"So, like any mobile app in the world? 

Many apps (an example would be LinkedIn) have a SQLite database. This is replicated to a central database server. ",1543086977.0
BinaryRockStar,"> ^ right there above im trying to figure out how to have multiple items in a inventory for a single player and if i want to look at another players same thing different items etc,

Here is how to insert multiple items for the same character:

    insert into Inventory(Inventory_ID,Player_ID,InventoryAmount,Item_ID)
    Values ('00000000-0000-0000-0000-0inventory01','00000000-0000-0000-0000-0000Player01','30','00000000-0000-0000-0000-000000Item04'),
    ('00000000-0000-0000-0000-0inventory01','00000000-0000-0000-0000-0000Player01','10','00000000-0000-0000-0000-000000Item05'),
    ('00000000-0000-0000-0000-0inventory01','00000000-0000-0000-0000-0000Player01','5','00000000-0000-0000-0000-000000Item06'),
    go

Now Player01 has three items- Item04, Item05 and Item06. Repeat the same thing with a different Player_ID to insert items for other players.

> The next thing i also am trying to do is get it so instead of saying Item04 it says, Shield instead if thats possible

When you perform your `select` statement to get the data you want, join to the `Item` table to get the item description.

    select Player.Name, Item.Name, Item.Description, Inventory.InventoryAmount
    from Inventory
        join Player on Player.Player_ID = Inventory.Player_ID
        join Item on Item.Item_ID = Inventory.Item_ID

What are the requirements of the task you are trying to achieve? Those fake GUIDs (""0000000-0000..."") are messy as hell. IDs should be `int` data type, as should InventoryAmount and the Item_Stamina, Item_Strength etc. columns in the Item table. The ID columns should also be `identity` so they automatically generate new IDs on insert rather than creating them by hand.",1543050660.0
The_0racle,You're welcome to ask questions but this subreddit isn't for homework.,1543028937.0
soulscore_work,"Hi, 

Enyoctap i think you are maybe looking for sql server express. It can run databases up to 10gb in 2008 R2 and newer. If you use odbc there is several people and guides out there to help you set that up. In most cases you can import your access into tables in sql server express.


https://www.microsoft.com/en-us/sql-server/sql-server-editions-express

ODBC Access:

https://dba.stackexchange.com/questions/143902/how-to-enable-odbc-connnections-in-new-sql-server-setup

Hope this helps. :)",1543000894.0
Shawnanonymous,It sounds like you are ready to graduate to MS SQL Server. I use it daily and can confirm that it is a great tool that is more capable than Access.,1543004592.0
computore,"You will have to ask yourself ""Do I want to do front end development or back end development?"". Access is the combination of both. When you start getting into larger data sets or more complicated front end logic Access starts to show it's downsides.  That is why you may feel like you are out growing it.
I am not saying you will have to choose front end or back end development and only do that to grow. Only that if you want to go beyond access you have to start splitting up the work and tackle one of them first.
If your role at work is an analyst, and that is where you want to be,  stay as close to the data as possible. Choose SQL, R or whatever data tool your company supports.",1543001316.0
newUIsucksball,Ask your manager and IT department for access to SQL server.,1543026776.0
Moe3615,If you're interested in no code hosted solutions you can check out tadabase.io,1543003179.0
ToBePacific,"Yeah, once you start to but up against the limitations of Access, it's a good time to strongly consider SQL Server. ",1543010395.0
pdp10,"There is no stronger Access. There are similar products like [Kexi](http://www.kexi-project.org/) or Filemaker, but that kind of database is limited in scale and capability, usually not cost effective, and typically legacy. Access also historically has a reputation for corrupting its databases with multiuser access.

The place to move up is to a real, client-server SQL database. PostgreSQL is fantastic and runs on your choice of platforms, but there's also certain cases to be made for MariaDB/MySQL, Microsoft SQL Server, [Firebird](https://firebirdsql.org/), even IBM DB2 and Oracle in some circumstances.
",1543441741.0
mkingsbu,"!remindme 

I'll take a look at it when I'm not as food fatigued from yesterday... ",1542982961.0
HildartheDorf,The default schema in postgres is 'public' not the database name.,1542962047.0
ramborocks,Tell your teacher to start understanding programming 1 means yes. Then stare deeply into his soul. Never blink. Don't give him reason to doubt you. Next day he returns you can an A from here on out. ,1542924499.0
ramborocks,"Too add a future date you can use dateadd(). I wouldnt even bother writing a case statement for the yes. Seems pointless but yeah goodlucj

",1542922061.0
Stychey,"Personally I would look at re-modeling the table to change the end date to a date. It keeps it consistent and useful at very little cost.

In terms of open records, there's a couple of solutions. With it being a late arriving dimension you can either decide on a default end date in the distant future or leave it null. Either way you should sent a standard and keep it consistent throughout the design. ",1542923828.0
ramborocks,"What's the error your getting or result it returns which is wrong? What's sql flavor are you on tsql? My sql?

One thing that might work is keep the subscriber date like you have on the create table syntax but add a new column called subscribeEnd date default getdate()+30... Just use date. No need for datetime here. 

Obviously this isn't best practice but for a simple class assignment it will work and... Sometimes you just gotta take a break and it will come to ya. ",1542943508.0
ramborocks,"Well the case statement for subscribed is solved correct?

We just gotta get this. You can do it... I wish wasn't on vacation and could help further. When I get stuck and need hardcore fast sql help I go to the pros at sql server central. Com. View some of the scripts they have or find a question someone posted similar to yours. They been around for years.

Goodluck and Goodnight. ",1542944742.0
ramborocks,"Try making it an int column not byte on the table. The case statement looks correct... 

In a real world example int probably is preferred over byte.  subscribed  could then be to identify a subscription type.. Like 0 for nothing current... 1 for emails, 2 for emails only about certain features.. You get  the idea. ",1542946380.0
ramborocks,"http://www.sqlservertutorial.net/sql-server-basics/sql-server-bit/

",1542947299.0
swenty,"Relational database are still the default choice, in that they provide reasonable tools for managing data in most situations. Unless you have solid evidence that you are bumping into the limitations, stick with a relational design.

To your question about rankings, yes, it's reasonable to pre-compute summary data such as overall ratings, ranks, item popularity, etc. A background task that runs periodically to update those numbers is often a reasonable solution.

Keeping a log table of every action (e.g. download) made provides the simplest source data to compute statistics over.

Retiring old transaction history isn't always necessary, it depends to a degree on how quickly your data set is growing. If you do find that keeping all of the rows around is causing the whole DB to grow more quickly than is manageable, look into storing periodic summaries. For example, you can store per day summary statistics for each item, instead of individual event records. That allows you to recompute statistics for a variety of longer periods using the daily summaries as the values to summarize over.",1542894267.0
DesolationRobot,"I do similar things all the time with in-game item sales. There are a lot of different ways to do it depending on how intense you need to be. Check out OPAP stuff if you want. 

I do it all in a relational database. The app logs out individual transactions and I have a daily script that aggregates actions by whatever measures I want to have preserved for all time. Then I archive or delete historical data. 

So I'll have a table that stores date, item, user age/gender/status, and a count of sales that day. 

It easily turns hundreds of thousands of rows into a couple thousand per day. ",1542942245.0
DataDecay,">Does anyone do anything like archive stored data that is no longer relevant to that time window for queries(eg past month/week), and keep another set of records of the archived data summed up? 

I cant speak to your specific use case, but I archive old data based on partition, and the partition is created based on time. This not only makes archival stupid simple I can query on partition to speed up my app.",1542857016.0
boy_named_su,"A terabyte disk costs like $50. Get two for redundancy, $100

",1542920292.0
skylartrap,I could use some help please,1542836561.0
eshultz,">0DD76742-2C90-450D-867A-F1AA33FA1FA6

Are you trying to insert this same value for the FK every time? Because every time you reload the Guild table, those keys are going to be different since you're using NEWID().

If that's not the issue and you're absolutely sure the FK value matches one of the PK values in Guild, then I would look at the constraint you declared next.",1542853510.0
Dirjel,"I'm on my phone right now, so no real code examples. 

I'd suggest you use a subquery in the values clause of your insert (select Guild_id from Guild_table where Guild_name = 'Hufflepuff'`

Also, you can wrap your code snippets in the back quote (`) and it makes it a lot easier to grok your post. ",1542897338.0
drunkadvice,"In general, it's poor form to use a guid as a primary key.  The randomness is great when you need a truly unique ID that could some from anywhere.  But +99.9% of the time, allowing the Clustered Index to be a sequential integer works just as well and performs better.",1542761018.0
soulfusion,"create table guild (guild_id int not null identity(1,1) primary key clustered, guild_name varchar(30) not null, guild_cap int not null, victory int null, defeats int null)  
  
insert into guild (guild_name, guild_cap, victory, defeats)
values ('Viper', 30, 0, 0), ('Striker', 30, 2,1), ('Newage', 30, 4, 5)  
  
if you make your id column an identity, you don't need to worry about creating new IDs for each insert.  
  
e: fixed identity declaration",1542732343.0
Catsler,"If it helps, you should check out www.mockaroo.com ",1542746282.0
DataDecay,Very good read thank you! Saved,1542716288.0
wolf2600,How are those merge joins coming?,1542720284.0
ww9_,I couldn't find an online tool to convert an SQL file from MySQL to SQLite so I wrote one. Please file an issue in GitHub if you have problems as I don't use Reddit often. Hope it helps someone.,1542690819.0
boy_named_su,"Use an ORM for simple CRUD. Use straight SQL, or a library like Jooq for complicated queries",1542663213.0
F_WRLCK,Database developer here. I won’t comment on views vs. not-views but I will say that ORMs generate some pretty crazy SQL and the thought of optimizing complex queries that have passed through the ORM meat grinder makes me shudder. ,1542689602.0
israellopez,"It depends.

Use the ORM and its automatic SQL generation for user-defined searches/queries/ plus the typical CRUD.

But Raw SQL once we need very specific optimizations.

But if we are making a very small tool, we would go to raw SQL and avoid an ORM altogether.",1542657637.0
grauenwolf,"Views.

Any decent ORM can read from them and complex query logic belongs in the database, not cramed into application code as inline SQL.",1542658162.0
KitchenDutchDyslexic,"Personally I write a mixture of raw pgsql `WITH x AS (...)` queries and python flask-sqlalchmy ORM `Model.query.filter(...)` queries. 

You will normally find me writing reports using raw sql. And when I work with CRUD screens I reach for the ORM. Unfortunately I'm also not afraid enough for the [`@hybrid_property`](https://docs.sqlalchemy.org/en/latest/orm/extensions/hybrid.html).",1542666305.0
colly_wolly,"I only know Djangos's ORM in any depth, but its not possible to do certain complex queries with it multiple conditions on joins for example.  I still use the raw SQL functionality to send teh SQL through  the ORM as it protects against SQL injection as far as I understand. ",1542710155.0
KitchenDutchDyslexic,Glad to see it mentions Liquibase. ,1542665354.0
mfigueiredo,"First you need to collect the data.Then analyze and make forecasts and make the order **leveling**.

There are a lot of softwares to address this problem, e.g. ERPs. But if want to develop your own solution I suggest you collect all orders and consumption - inventory and order management.

Then you will have data to analyse and make forecasts.
",1542654874.0
NotAnotherMoron2,"Access is actually a great database for the scale of project that you are contemplating.  Before you decide how to structure the database, first write down the exact math that you will use to estimate how much of a product to order.  That will tell you the values that you must either record or be able to calculate.  Then build a simple database that can record that information about products, orders, and sales.  From the data you record, you should be able to run a query (or actually a series of queries) that perform the math to estimate how much of each product you need to order.  ",1542772480.0
r3pr0b8,"no, and no

",1542638531.0
jdaytona,"Probably best to have a passengers table and then another table to link passengers to routes.
",1542638503.0
NotAnotherMoron2,"You need a design that takes into account a many-to-many relationship of routes to passengers.  Therefore, you need at least three normalized tables.

TableRoutes:  RouteID, DepStationID, DestStationID, DepDate, SeatsAvail

TableRoutePassengers:  RouteID, PassengerID

TablePasssengers:  PassengerID, Last, First, ...",1542771249.0
ToBePacific,"In a word: performance.

You could write stored procedures, triggers, and cursors to accomplish pretty much all of the business logic you need for your business. But depending on the size of your database, all of that will place a lot of unnecessary processing load on your database server.

By writing your application for client-side processing, you can ensure that when a user wants to rearrange how some data is presented on their end, they don't necessarily have to slow down the database for everyone else who might be using it at the same time.",1542554682.0
mycall,"Writing logic in SQL, which is not a Turing complete language, sometimes takes interesting coding gymnastics to do what is easier in other languages.  Also, SQL doesn't interact with hardware devices or other client interfaces easily.",1542598037.0
a5myth,I think the overhead for using stored procedures as opposed to just opening it and reading/writing is not worth the running cost long term as you scale up.,1542624702.0
Frumpagumpus,ur/web was written following a similar train of thought,1542569461.0
whales171,"Databases are the hardest thing to scale. If you have only X number of users and you know with 100% certainty that you will only ever have X number of users with Y amount of data, then go ahead and put everything on your database, but if you live in the real world then your technology stack needs to scale. ",1542579491.0
Dazocs,"No. Primary keys must be unique. In the situation that you are describing, the many side of the 1:M relationship would not have a unique primary key. If you need the branch\_id in the primary key, look into a composite key-- combining branch\_id with another column in the table to create the unique values needed for the primary key. ",1542410930.0
teiom,You want to make the fk the pk?,1542408858.0
PandalfTheGimp,"Is it a 1 to many relationship because a branch can have multiple locations with the same branch id? A primary key uniquely identifies a record within a table, so two records can't have the same value within a column that is the primary key.",1542410299.0
r3pr0b8,"if two tables have the same PK, why wouldn't you combine them into one table?",1542409887.0
Skinny-Puppy-Digit,"The diagram indicates a 1:M, but in reality its a 1:1.  Nothing wrong with a 1:1, its a physical model design decision.  

For instance I would do that when the physical usage of the table for update/insert is independent of the primary table, but usually only if the primary table is ""wide"" (lots of columns).

However, in the case you present, I don't see a need for it.",1542419922.0
r0ck0,"edit: fuck. OP deleted their question right around the time I posted this.  That's quite annoying for all the people taking the time to answer.

For anyone wondering, they were basically asking about using the same [ID from table A, in table B when they have a 1:1 relationship](https://i.redd.it/4ep69tj5ury11.png).  

**my answer:**

Sooner or later (assuming the system keeps growing) doing shortcuts like this will become quite regretful, and sometimes even become a limiting factor that makes you give up on adding some non-essential future functionality.

For this reason, I've also stopped using composite keys entirely, no matter how ""overkill"" it appears to have a unique UUID PK column on every table.

It just becomes a mess once more tables start referring to your linking table, and often linking tables grow to add more metadata about the relationship.  Sometimes you also realise that a pair of foreign records need to allow for more than one relationship... then your composite keys (and all the referring FKs) become a much worse mess.


So basically now I have on every table (including seeming simple linking tables) I have:

* A unique UUID PK column for most tables where users or automated systems etc will be adding new records.
* Otherwise in some cases where the number of records will be predictability limited (e.g. a ""country"" table, or fixed records defined in code that might have otherwise been an ENUM), I might use SMALLINT instead.

Not only is this the best long-term solution to mitigate future regrets... but just sticking to these rules means I'm no longer wasting time on these decisions, which are often just bike-shedding on silly micro-optimisations anyway.

And sticking to these rules also gives you more power to build all-encompassing things like auditing systems, loggers, cold-storage and stuff like that which deals with data from all tables and even systems that cover data from multiple separate projects.  With UUIDs you don't even need to *always* keep track of which table the ID was from (although you will 99% of the time with FKs), because you'll be able to figure it out later anyway.  Including when two tables merge into one or visa-versa.

Also UUIDs kick ass.  I keep discovering more and more things that they make super easy beyond the commonly known & discussed benefits.

",1542420451.0
release-object,"Traditional relational databases aren’t always a best-fit for social networks. This [blog post](https://www.facebook.com/notes/facebook-engineering/tao-the-power-of-the-graph/10151525983993920/), by the Facebook engineering teams, talks about their difficulties and eventually move to a [graph database](https://en.wikipedia.org/wiki/Graph_database).",1542407918.0
r0ck0,"I've spent a stupid amount of time bike shedding on BIGINT vs UUID PKs, public URLs, and even custom written generators for all this stuff for both in PHP + JS (and thrown most of it out now).  Literally 100s hours of this basic low-level shit. (largely ADD hyperfocus compelled probably)

Firstly, don't let performance concerns make decisions for you regarding your primary database schema (within reason).  These microoptimizations are usually always going to be better handled with caching anyway once you get to the point of needing to worry about performance.

I'm in the process of building very large long term projects (planned to be life-long), one similar to reddit + stack overflow.  So I'm certainly thinking about performance and a lot of caching/performance related stuff to allow me to host high-traffic sites from a minimal number of cheap Linode/DO/Vultr VPSes. I've been planning and researching this shit (especially too much time on this low-level PK / URL stuff) for about 5 years now.

My conclusions, with the #1 priority being most flexible and fewest future regrets...

* UUIDv4 for all PRIMARY KEYS - I'm never going to have to worry about kind of lock-in, conflicts, merging/splitting tables or even multisite/multiserver/multitechology projects in future expansion.  (only risk here is a faulty random number generator).  
* Also means I never need to worry about the PKs being publicly exposed, there's nothing meaningful/traceable in UUIDv4.... however I won't be exposing them anyway in many cases...
* Most records with an ID in a URL will use a separate ""public ID"" (not the db primary key).  This will be either a random or auto_incrementing INT or BIGINT, and converted to base36, maybe with something like https://hashids.org/ - so I get short URLs, but still get all the flexible of UUID PKs.
* Yes this means two columns on most tables.  If you're picking what to use for **both** PK+URL, then you're making compromises in some areas.
* When records are merged or split, the flexibility of using separate public IDs (not the PKs) makes this easy to solve - especially if you're concerned about SEO.  
* Musicbrains actually does something similar, but they do it the other way around (INT PKs, and public UUIDv4 URLs)
* This highly flexible future-proof approach covers every logistical/security/flexibility downside - the only downsides are in performance (in the early stages before you're caching stuff anyway)
* So on performance - once you get to the stage of worrying about performance (90% chance you never will sadly), you're going to be using redis, caching, CDNs and stuff like that.  You can use the smaller public INT/BIGINT IDs as your caching keys.
* I might consider some kind of NoSQL - but only as a secondary database, i.e. a caching layer - all ""master"" data will always be written to postgres for permanent storage.  
* Anyone who suggests starting a new project with **only** a nosql database likely doesn't know SQL very well, and they're writing fuckloads of shitty backend app code that should really be a few simple layered SQL VIEWs (which even I have been very guilty of doing over most of my career, even when using SQL databases).  
* Take the time to learn advanced usage of SQL VIEWs and maybe TRIGGERs too, and you'll never look back.  I only got around to do this stuff once I needed the performance from a machine learning system.  Those were some lessons that I now use in everyday webdev databases.

Anyway, after years of changing my mind about a lot of this stuff these are my final conclusions that have stopped me from spending more time re-thinking all this stuff again and again.  

tl;dr: Focus on data-integrity & flexibility with regard to your schema - because the performance stuff is better handled in other ways anyway.

Lemme know if you have any questions.  It's all my just opinions of course, but I've put a stupid amount of research, thought & testing into this over the last few years.  I've also built a few social networks over the years (my first one for my friends in about 2001) and learned from various schema and premature-optimization related regrets on bigger projects since then.

> I've heard reddit uses two tables for every entity.

I remember reading up on the ""thing"" table stuff when researching reddit's structure, I think they've moved away that quite a bit.  Not sure of the details of their current schema though.  But it was originally done because postgres was slow to add new columns.  This is no longer a problem, especially as of postgres v11 - which apparently even solves this on new NOT NULL columns.  These days, you definitely don't want to design your database like reddit originally did.

> Also, how does reddit generate the small alphanumeric string that uniquely 

Last time I checked they were just incrementing, and converted to base36, basically what I mentioned above for my public URL IDs.  So no, they're not (or portion of) a hash.

I've considered using hashing in some places such as filenames for uploads (and do have some projects still doing that) - but sooner or later I usually find some lock-in / conflict / security issue that makes me drop the idea of using hashes for stuff like filenames or URLs.",1542425302.0
TimIgoe,"Think about Facebook, everything works via a graph id, I would assume these are a form of pk for optimisation reasons, they don't hide away by having hashed entries etc",1542398239.0
boy_named_su," use a database that supports recursive common table expressions

here's your main table:

    create table relationships (
      from_person_id int references people(id),
     
      to_person_id int references people(id),
    
      primary key (from_person_id, to_person_id),
    
      check (from_person_id <> to_person_id)
    );


Read  Joe Celko's Trees and Hierarchies in SQL for Smarties, 2nd Edition
",1542408363.0
LeftJoin79,"Great post and discussion. I'm not interested in building this myself, but always interested in how things work / are approached.",1542466180.0
DesolationRobot,"First one is easy--it's just sorting by hours descending.

Third one is pretty easy. The hint I'd give is that you'll be using a left join between examine and attend and then filtering where attend is null (no match found). Then joining that back on Students to retrieve the data you care about.

For the second one you need the count of distinct lectures and then the count-per-studentID of the attend table where that count = total available.

You doing this in SQL?",1542400738.0
torstengrust,"Hi,

here's a DRC formula for the first query:

    { ‹l1› | ∃ t1,h1,p1 ‹l1,t1,h1,p1› ∊ Lectures ∧ ∀ l2,t2,h2,p2 ‹l2,t2,h2,p2› ∊ Lectures ∧ h2 ⩽ h1 }  

In words: lecture `l1` has the most hours (`h1`), if *all* other lectures `l2` have less or equal hours (`h2`).

Cheers,
  —T",1542476860.0
aamfk,Also oracle with 5 CALS is 40 grand. Sql server express is free ,1542402972.0
chris062689,"Instead of loading up a database administration tool that could potentially expose yourself to additional risk, try something like Metabase. 

I've used it in the past with success. 

Metabase is the easy, open source way for everyone in your company to ask questions and learn from data.

https://www.metabase.com/",1542427596.0
sHORTYWZ,"Is Oracle a requirement for this?

PHPMyAdmin is a full featured web interface if you can migrate to MySQL/MariaDB. I did some googling and it doesn't look like there's anything similar for Oracle, pre-built.",1542395306.0
mkingsbu,"Is your Oracle machine Windows or Linux based?  If its Linux based, you should set up SSH so they can connect directly to the database.  They can either run SQL*Plus through the terminal (or Putty if they are on Windows) or use a RDBMS manager like DBeaver with those credentials.",1542406183.0
ludicrust,"Can it be live, such as sqlfiddle.com?",1542406652.0
winkers,I sorta had to do something like this.  Ended up using a docker MySQL image which works if you don’t need shared multiuser sessions.  Created scripts to create and populate the database.  Had other scripts as examples of queries.  Plus everyone got to learn docker which was fun.  ,1542419326.0
WeaselWeaz,"Why not just let them remote into the server with limited permissions and load up the SQL client? I.e. Connect via VPN, connect via Remote Desktop, and then open us the client? Then they can see whatever parts of the database you've given their account permissions for. They can look at tables, views, write queries, etc.",1542397868.0
aamfk,Use SQL server and setup roles and permissions. You can't even properly secure most databases if you work in Active Directory environment ,1542402843.0
getoffmyfoot,"I’ve had to solve this problem, and played with full text search in sql server. 

...it’s rough. I fought with it for a long time and was never satisfied with its search behavior.
I eventually decided to try Solr and it worked fantastically for the use case you just described. Basically OOB full database text search. I did have to write some ETL but it was very basic and boilerplate since Solr has a rest API. 

For this reason I’d recommend using the right tool for the job and not try to fight sql server into doing this for you. I think you’re going to put a lot of heartache into something you won’t be satisfied with.

My 2 cents.",1542337770.0
mkingsbu,"Read the Wiki articles on normalization up through BCNF (https://en.wikipedia.org/wiki/Database_normalization) and it should become more clear what you need to do.

Basically the idea is that each table should contain information that no other table contains. So if your animals all share commonalities, you keep those in the ""animals"" table for example. Then you might have an attribute table and an animal_attribute table, forming a many-to-many relationship.  You'd use the pimary key of the animal table and the primary key of the attribute table as a composite foreign key in the animal_attribute table.",1542296180.0
goblando,"Couple things:  If you are looking to do this in an app space, try to use SQLite instead of mysql as it will be closer to most app dev environments.  If you are windows based, Sql Server Express would fit your needs as well.

As someone else posted about BCNF, I will say that it works for most scenarios, but how you use the data will make certain queries tricky.  That doesn't mean you don't do it, it just means you will need to do some more work.  For example:

**Users**: are their own entity, they stand in their own table.

**AnimalTypes**: are their own entity, they stand in their own table.  You use the table to describe specific traits or app logic related to the animal

**AnimalTraits**: Generic descriptions of Traits an AnimalType may have.  For instance, size (could be multiple versions ie. point, pounds), gender, age, conditions

**Trips**: a trip created by a user

**UserTrips**: a trip that a user took.

**Prey**: A unique animal linked to a specific trip.  Could have multiple user records associated (who spotted, who trapped, who killed).

**PreyDetails**: AnimalTrait values for a specific prey

The generic tables are used for programming logic in your app.  You will have one to many relationships in multiple places, and while your queries will look complicated, they are better in the long run.",1542297871.0
SlasherMcgurk,"Hi There, parametrise your query, on SQL Server this would be.. like..

    SELECT     N.*, P.* 
    FROM       Nurses N
    INNER JOIN Patients P ON N.ID = P.NursesID
    WHERE      N.ID = @NurseID

BTW don't use \*..

So just pass your Nurse in to the query and the query will filter down the results to just the patients for that nurse, in SQL Server you can pop this in to a Stored Procedure, PostGres will have something similar. There may be more technical ways to filter down which fields are visible to whom, you may be able to use column level security. If you are in a specific group then you can see these columns.. Otherwise you can apply crude security in your query,

Good luck!

&#x200B;

&#x200B;",1542268869.0
iBlag,"This is an application layer problem. Your database can give you mechanisms to implement this (eg: row level security, as somebody else already linked you to), or you can code your app entirely in SQL (please don’t), but fundamentally coding this in your database is going to be difficult.",1542268645.0
rbobby,"Maybe https://www.postgresql.org/docs/9.5/ddl-rowsecurity.html

Or do it in the application itself.

Either way it's going to be a pain.",1542268439.0
BornOnFeb2nd,"If they are going to be able to login **directly** to the database, this gets harder, and you need to think about the various bits of security mentioned.

If they can only access through some application you're also building (preferred), then do what Slasher suggests....  

They'll have a *magical* display button in the tool that'll look at who is logged in, and pass that to the query, limiting their view to only the folks assigned to them.

Don't forget about situations where a nurse is sick, has a backup, on lunch, etc, etc....  Someone who is supposed to be able to not being able to access patient data could be a **very** bad thing...",1542282196.0
coldincanada,In a hospital patient information can be a life or death situation. Tell the hospital you are not qualified to do this and to hire a consultant before someone’s grandmother dies because of bad advice from Reddit. ,1542296644.0
boy_named_su,"Pg supports row security, and column security. Meaning you can lock down individual cells if you want. Read the docs and post specific questions here",1542293805.0
DRdefective,"I think you should invest some time in trying to come up with something before asking the community to do it for you. If you have done something so far, you should add it to your post.",1542288161.0
DesolationRobot,"Yes, it's the same join logic you have above, just add the cards table.

    SELECT *
    FROM decks
    INNER JOIN userDeck ON decks.id = userDeck.deck_id
    INNER JOIN cards ON decks.id = cards.deck_id
    WHERE userDeck.user_id = 2

It's worth pointing out that this will return all cards from all decks associated with that user, not just the one deck.

You can alter that ""SELECT \*"" to return only the data you need from any/all of those three tables.",1542234569.0
wolf2600,So you're looking to return all the cards which belong to a specific deck?,1542234577.0
reallyserious,Any relational database will do. Try PostgreSQL. An RDBMS is the correct tool for 99.9% of database needs. Especially when you need to do analytics and reporting on it.,1542221763.0
DesolationRobot,"I don't see why that couldn't just be a basic relational database.

The nature of the data would dictate the schema somewhat. Do all objects have the same 30 traits and data for all of them?",1542221715.0
KitchenDutchDyslexic,"# PostgreSQL.

If you are up for the obscure but workable pgsql for example [PostgreSQL rename attribute in jsonb field](https://stackoverflow.com/questions/42308764/postgresql-rename-attribute-in-jsonb-field) and [How do I modify fields inside the new PostgreSQL JSON datatype?](https://stackoverflow.com/questions/18209625/how-do-i-modify-fields-inside-the-new-postgresql-json-datatype) from stackoverflow.

>  The timing at which the transactions took place is very important 

Might need to add [triggers](https://www.postgresql.org/docs/current/plpgsql-trigger.html) to track this info.

> [AWS friendliness is a big plus](https://aws.amazon.com/rds/postgresql/)

Now all this assumes when you talk about object data you talk about json like data.",1542222097.0
mkingsbu,What language are you using?  This *seems* less like a database question and more a question about what your language is capable of interfacing with.  I know in Python I'd highly recommend SQLAlchemy with an RDBMS backend for this.  500k objects isn't all that much. I have a database with 10x that for one of the tables and its performance is fine for reporting.,1542225447.0
da_chicken,"> I'll be storing a preformatted objects with set of properties that change over time, usually every few hours/days not seconds. Each object will have about 30 of these.

The main question here is: are these properties going to be present on all or nearly all objects, or are you going to have one object with 30 properties, the next with 30 completely different properties, then next with 30 properties unrelated to the first two, etc.  Do you have objects that you'd have to use multiple different classes to represent or you'd have to use dynamic property names to store?  If so, your data will have a hard time fitting into a relational database.

If you're going to be analyzing more or less the same 30 properties that are instances of objects of the same class, then what you're doing is exactly what relational databases were designed for.
",1542232137.0
kum0nryu,"I don’t know anything about their particular specs but this data dump looks like JSON, presumably served up from an API.

As far as database platform, if you’re going transactional or even analytic (below several, several hundred gigabytes of data), I’d recommend PostgreSQL. It’s very forgiving, very robust, and free. There’s a reason it’s used extensively in startups and why it was the database that formed the foundation of Amazon Redshift.

I also echo the sentiment that MongoDB and other noSQL databases aren’t great options outside of some special circumstances. They generally seem like they are more “buzz” than “best”",1542181613.0
merican_atheist,Mongo is never a good choice.,1542180360.0
recourse7,"Come on dude.  

There is like zero chance for someone giving you access to their production DBs.  

Its super easy to spin up a free tier aws instance and throw mysql on it with some dummy data.  

Then you can access it remotely.    If you need assistance in doing that I can give you a hand but prod data on my networks?  nah dude nah.



",1542154450.0
Frndlyy,Why don’t you just stand one up in AWS or Azure?,1542153450.0
xkillac4,"This is the easiest way, if you can get docker installed. https://www.techrepublic.com/article/how-to-deploy-and-use-a-mysql-docker-container/",1542155411.0
NSA_GOV,Use Adventure Works ,1542158730.0
wolf2600,"For your second table, I would name it Customers instead of Locations.

    Customers
    --------------
    CustomerID
    LocationID (nullable)
    UserLocation (nullable)

If the customer selects a location from the list, LocationID will be populated, otherwise they can enter their own location which will be stored in UserLocation.

",1542132800.0
WeaselWeaz,"If location if an address you would presumably have an address table. Address Lines 1-3 as text is common, lookups for State/Province and Country against their own tables so customers can pick from a list, and zip code.  You can have multiple addresses per customer\_id but not allow the wild west of every customer entering State or Country differently.",1542142964.0
alinroc,">my first Question is im getting a error for 4 certain drop tables

Rule #1 when asking for help with an error message: **provide the error message**

The problem is most likely (I haven't read through all your code) is that dropping the table will break foreign key constraints on dependent tables. So you'll need to either drop the constraints or the dependent table(s) first.

>second question is how do i do a join table Especially a 3 way join?

The same way you join two tables, but with one more table in the mix. Where exactly are you having difficulty constructing this?",1542132669.0
s13ecre13t,"Besides mentioned row level security, which is probably best for what you are looking for, there are other methods that also allow for the type of security you are asking. For example

# Schemas and Table inheritance 

create separate schemas for different users. main table, then create a child table. Assign users to child table. 
Reading from main table will read all records from all children, but children are independent of each other. 

    CREATE SCHEMA private;
    create table private.generic (id int, value text);

    create schema ClientCompanyA;
    create table ClientCompanyA.generic () inherits (private.generic);
    insert into ClientCompanyA.generic (id, value) values(1,'a')

    create schema ClientCompanyB;
    create table ClientCompanyB.generic () inherits (private.generic);
    insert into ClientCompanyB.generic (id, value) values(2,'b')

    select * from private.generic; -- 2 rows returned
    select * from ClientCompanyA.generic; -- row id 1 returned
    select * from ClientCompanyB.generic; -- row id 2 returned

Please note, each user you create will be assigned a specific schema, so as far as clientCompanyA or clientCompanyB is concerned, they are just reading their 'generic' table.

# Schema and Views

Alternative way is to create schemas with specific views, where the views filter data out. You will have to create view with 'CHECK OPTION' so that inserts / deletes / updates only happen on rows that are exposed in the view. 

the setup is similar as above, where different schemas have different filtered views pointing to one parent view. The major difference is that here a row could move across clients (when row's values change affecting if row passes view's where clause or not). 


This type of stuff is usually discussed under the name of 'multi-tenant' application development. 

",1542141507.0
zepplenzap,Quick google search for 'PostgreSQL row level security' turned up this: [https://www.postgresql.org/docs/9.5/ddl-rowsecurity.html](https://www.postgresql.org/docs/9.5/ddl-rowsecurity.html),1542126432.0
kevin3030,"MySQL changed the default authentication plugin and not all clients support it yet. That’s why you can connect from the shell but not php.

Here’s a conversation about the same issue. 
https://stackoverflow.com/questions/49083573/php-7-2-2-mysql-8-0-pdo-gives-authentication-method-unknown-to-the-client-ca

You need to create a new user and specify the plugin in the create statement. 

> CREATE USER username@localhost identified with mysql_native_password by 'password';",1542114394.0
grauenwolf,"MongoDB is never the right answer.  PostgreSQL is faster and more reliable even when using MongoDB's own benchmarks. 

Seriously, MongoDB is a case study on how to not make a NoSQL database.  (And an even better case study on how marketing trumps quality. )",1542076623.0
burnaftertweeting,">relatively long  \~300kb

*laughs in large BLOB*

Postgres all day. Full text index plus you can use actual SQL.",1542084517.0
pixelbaker,"If your queries are variable and the result is JSON, it seems like it would be a better idea to go with straight relational model, index appropriately for the most common queries, and throw an API on top of it.  It doesn't seem like you'd gain much performance (and may actually impact performance) by storing pre-formed JSON that may or may not be relevant in the response.",1542080891.0
grauenwolf,"Define ""query"".

If the database treats them as opaque files, indivisible black boxes, then you want to use a format optimized for compression. 

If the database needs to open them up and look at the contents, you want a format that can be indexed.",1542076326.0
mezza77,"Have a look at Couchbase - Much simpler to manage, has built in full text search, a SQL like language and Analytics. Its stores json docs, same as Mongo, but is so much easier to use.",1542120255.0
DesolationRobot,"You can have a persistent generated column where the value is just set to equal the value in the other column.

What I don't know is what happens when you want to stop writing to the old column and start writing to the new column.

You could implement the same with a trigger. So before insert set new.new\_col = new.old\_col. Then eventually when you start writing directly to new\_col you turn off the trigger.",1542061482.0
grauenwolf,https://stackoverflow.com/questions/2889871/how-do-i-escape-reserved-words-used-as-column-names-mysql-create-table,1542059452.0
boy_named_su,You must wait for another DBA to die,1542056700.0
ILiterallyLoveJV,"Step 1: live in a populous city to ensure higher rate of possible DBA positions.

Step 2: start looking for Junior DBA positions.

Step 3: work at said company until your junior status becomes mid or senior status (3-5 years).

Step 4: profit  


In all seriousness, it took years for me to finally get on with a team of DBAs to learn from.  Looking back, I didn't really need a team, but rather just the guidance.  Start small.  Document out your best practices, and then hit those goals.  Example:  start with backups, and have jobs that restore said backups to a dev server or something not too crucial to ensure the backups are valid and that your restores work.  
I'd also recommend getting involved in your online communities for tips, tricks, and answers when you get stuck.  Let me know if you'd like to know more.",1542055290.0
is_not_null,"It fell into my lap. We have a 12c production database and a handful of SQL Server databases. I took some classes from Oracle and learned on my own. I have a weak SQL background but enough to get by. It sounds like you are already doing 90% of what a regular dba does. Here are some other topics you might want to look into. Working with indexes and knowing how they work is a very good start.

* Database locks and how to resolve them.

* Patching.

* Backups. If nothing else, understand backups and make sure they are getting done. Make sure you can restore backups. This is arguably the most important part of being a DBA.

* Hardware monitoring (usually more of a sysadmin thing but it might be expected that you watch it too).

* Managing wait times and watching for sessions/processes that are causing long waits. Learning how to resolve issues causing long wait times.

* Account management.",1542045149.0
imcguyver,"I highly recommend picking up future proof skills, like linux administration, basic software engineering, python, java, cloud computing.  Those are examples of core skills that stand the test of time.

Oracle DBAs became a popular career path at a time when cloud computing was bleeding edge.  Monolithic db's like SQL Server & Oracle are being cannibalized by cloud computing.  Between lynda.com, coursera, udacity, MIT OpenCourseWare & others, the materials are out there to learn, just set aside time to start.",1542084678.0
getoffmyfoot,"I got a junior DBA role right out of school and learned under someone for a few years. With the experience you describe, you could step into junior dba work, so as others have mentioned it’s probably a large part living in an area that has the job market to support a junior dba. ",1542058060.0
liquidpele,"For Oracle, certifications may help...  it’s very very finicky and there are tons of options.   Anything else you can mostly learn on your own, just figure out how to normalize, when not to, data warehousing strategies, debug query plans, profile for issues using logs and 3rd party tools, resolve deadlocking, shard/partition, failover, and the ins and outs of functions/triggers.   If you need webscale, use mongo of course (iz joke).",1542071508.0
DataDecay,"DBA - Dont Bother Asking

But in all seriousness, it sounds like you'd have everything to fit into a jr dba role at the company I work for. Unfortunately when there are no jr dba positions it's hard to overcome lack of experience.

Set standards and best practices for fsl's, backup policies including LTR policies, restore, refresh procedures, audit, compliance, automation, jeez the list can go on.

Join a oracle database user group in your area.",1542074484.0
fxgx1,"Here is my insight... you already know enough since you can deploy and a database infrastructure and maintain it. I will suggest you do the following. If you primarily work with relational databases you should aim to add two more databases to your list. A MongoDB(noSql) and or big data tools such a Redis or Hadoop. And to finish top it up, never walk around with without knowing at least one programming language. Pick up Python or Java. ",1542119921.0
caeelrod,"I am currently a 1st year Jr. DBA, and you already have more DBA type experience than I had before I got the job. My advice would be to apply for every open DBA position, no matter the requested experience. While they might love to find a Sr DBA with tons of experience, a capable Jr DBA that could immediately take small tasks off the Sr DBAs at 1/2 the price to boot might be an enticing option depending on their situation. Be up front, convey confidence in being able to take on work immediately, and remind them that you'd be willing to accept a relatively small salary. The job posting for the job I eventually got was looking for 5-10 years of Administering Oracle and SQL Server...",1542334899.0
mkingsbu,"Make ""UniquePartID1"" the source part and ""UniqueSourcePardID2"" the destination part.  (E.g. source_part_id, destination_part_id).  Only ever put something in the ""source_part_id"" if it also has a relation to another part. That eliminates the possibility that AB = BA because they are no longer pairings of related parts, but the source part and then something it is related to.

I say ""source"" but you can use whatever word you want to describe it that basically means, ""This is the thing A for which I want to check if there is a related thing B."" Even if ""Thing A"" and ""Thing B"" are the same physical types of objects.",1542031410.0
metalbark,"*do I add both AB AND BA pairs*
&nbsp;no

&nbsp;

*or do I sort the query with MIN() / MAX() and put everything only in AB order in the table?*

&nbsp;What query ?  Results are ordered, tables aren't.  So what is the intent of the query ?

&nbsp;

*when PartC is UniquePartID_1 but also when it's UniquePartID_2*

WHERE id = UniqueID1 
 OR id = UniqueID2

edit: i suck at formatting



",1542031670.0
DesolationRobot,"You can do it in MySQL. You only need to store one row per pair, then at query time to, say, find all parts that are compatible with id = 1500 you'd do:

    select * from MatchStatus where (UniquePartId_1 = 1500 or UniquePartID_2 = 1500) and MatchStatus = 'confirmed interchangable'

Theoretically this will work no matter how many interchangeable parts there are. You could be careful to always store, say, the min part id in part type table as the ""parent"" part id, then put that number in UniquePartId\_1 field for every applicable match row.

However, if the primary purpose of the database is the relationships between these parts and/or the relationships between the parts and the make/model they pertain to you could look in to a graph DB.",1542043479.0
CekoDeko,"Don't worry about table size, you won't have speed issues as long as you set up indexes, which if you're using Django, you'll get automatically.

Which of these options describes if something is interchangeable?

* Part A is always interchangeable with part B
* Part A is only interchangeable with part B when it's used as this part type

If the first is true, look at asymmetric many to many relationships: [https://stackoverflow.com/questions/19837728/django-manytomany-relation-to-self-without-backward-relations](https://stackoverflow.com/questions/19837728/django-manytomany-relation-to-self-without-backward-relations)

If the second is true, look at custom through models **and** asymmetric many to many relationships: [https://docs.djangoproject.com/en/2.1/topics/db/models/#extra-fields-on-many-to-many-relationships](https://docs.djangoproject.com/en/2.1/topics/db/models/#extra-fields-on-many-to-many-relationships)

&#x200B;

If you would like one more level of indirection you could make a PartCompatibilityGroup table

&#x200B;

**Part**

id

name

&#x200B;

**PartCompatibilityGroup**

id

&#x200B;

**PartCompatibilityGroupPart**

compatibility\_group\_id

part\_id

&#x200B;

Then you'd just be adding each part once to the **PartCompatibilityGroupPart** table.  Again, I don't know what you're building, it may make sense to have a part belong to multiple compatibility groups too ¯\\\_(ツ)\_/¯",1542041114.0
alinroc,"Open the file in a plain text editor like Notepad++. If your zeroes are there, it’s excel being “helpful”. ",1541954872.0
da_chicken,"Excel auto formats anything that looks like a number as a number.  It will also usually format anything that looks like a date as a date.  It will make these changes silently, without confirmation, and it cannot be modified.  There's no way to modify this behavior except to change the column so that it's a formula for a literal text value, which makes it useless as an actual CSV file.

Try a plain text editor or something like CSVed.",1541984608.0
karlophonic,"It's excel. I've changed the file associations for .csv & .txt to notepad++ on my computer to avoid this situation. I deal with files that have columns with leading 0's on a regular basis. Excel is overly ""helpful"" and causes all kinds of problems.",1542032245.0
NotAnotherMoron2,Best advise ever is to NEVER allow data of any kind to even pass through Excel if you value it's integrety.,1542773205.0
Ay--_--ye,Mongo just released a beta tool. It looked pretty sick but I’m on mobile and don’t remember the name. ,1541974494.0
hackjob,I've used dbschema+Cassandra before but not sure if that tool handles the requirement you list.,1541991987.0
r3pr0b8,"    SELECT ...
      FROM ...
     WHERE price > 50
       AND MONTH(Deliverydate) = 6",1541846446.0
HappyNarwhale,"CMS - customer management system 
Try starting with that. If you don’t find what you need check back and let me know. ",1541811651.0
shippingmypants,Access ,1541816195.0
Mamertine,"As in you want a shell to populate or you want a data set to practice with?

Option 2 is to find a copy of Microsoft's example db ""adventure works""

Option 1 is likely to buy a product or do it in Excel.",1541811685.0
graboskyc,You probably want a CRM like Zoho CRM. I also just heard about this from a podcast and they have CRM templates: airtable.com,1541878720.0
BornOnFeb2nd,"If you're looking for something to keep customer names, address, notes, phone number, etc.....  and you want something free... What about using the Contacts list in something like Gmail?",1541892758.0
KitchenDutchDyslexic,">  software or online product that gives me a simple database. 

Like other poster mention why not a CMS? Might not be a ""simple"" database.

But you can deploy odoo on your own machine or use the online product. ",1541831663.0
welshfargo,http://www.databaseanswers.org/data_models/,1542406747.0
r3pr0b8,"yeah, it is,, except the diagram is messed up

according to the crow's foot line between Player_Log and Player, each player log can have many players

clearly, the opposite is intended, because Player_Log has a FK to Player

i would be real hesitant to use whatever software created this diagram",1541699183.0
norvares,"A few problems I would've with your current design:

1. PlayerLocation: you've to duplicate location entries  (Name, Coordinates) which should be masterdata. Maybe a Location table for the MasterData and a PlayerLocation table to map the player. In this case a timestamp would be nice to order them or a bit like ""is active"". If you only need to save the current Location, integrate the fields in the player Table.
2. Membership: subscribed should be an tinyint which you map later on in your application. It's quite more space efficiant.
3. Guild: you aren't able to check who and when lost/won in a guild with fixed values - problem if someone finds a bug and starts cheating.
4. Main cities/Story Quests/Dungeon/Side quests: I don't really get your fields based on the naming. I think you're able to merge most of them (Main Cities and Locations with a main city flag), (Quests, Dungeon, Side quests with an additional text table and a type difference) . Dungeons are maybe locations, Quests and Side Quests should be identical and so on ;)

I would suggest checking your datatypes, varchars are inperformant and expensive memorywise.

Try to reuse most of the tables, any table under 10 entries can be solved with a tinyint and an application mapping of values like (1 => Elven City, 2=> Human City and so on).

&#x200B;

Please don't take it too hard, I analyzed your schema based on performance and IO optimization - under a few hundred players you wouldn't have any problems with most schemas ;-)",1541700982.0
jonas_meng,"The fundamental change in graph dbms is the relation between tables now is considered as first class member, which essentially change the way u model and query data. By ''relations'' hrer I mean keys in the tables.

Of course, one can argue such thing can also be achieved on rdbms. It's true, rdbms can of course do all things. But it suffers at some aspects. Just like Mongdb allows flexible scheme, one can acomplish an app with rdbms without this flexible scheme, just need more effort on manamengment.

Same as graph dbms here, the most obvious improvement is that queries involving many heavy relation joins can be executed more efficiently.

That's my general opinion on graph dbms. Hope it helps.",1541700079.0
swenty,"I don't know the answer, but this is an excellent question. 

Those proposing and building database systems that don't adhere to the relational model really ought to explain clearly in what ways their theoretical models differ; not just the advantages, but also the limitations, especially with regards to unsupported query types and limitations on types of integrity constraint.",1541726593.0
cgfoss,"I think graph databases are a lot like network model databases.

https://en.wikipedia.org/wiki/Network_model
",1541738984.0
brantam,Graph database systems are navigational. They are modern examples of the network database systems that have existed for more than 50 years.,1541745074.0
welshfargo,https://blog.jooq.org/2016/07/05/say-no-to-venn-diagrams-when-explaining-joins/,1541700540.0
nnsol1tary,"Hi, for example SQL JOIN's explained: [https://www.reddit.com/r/programming/comments/1xlqeu/sql\_joins\_explained\_xpost\_rsql/](https://www.reddit.com/r/programming/comments/1xlqeu/sql_joins_explained_xpost_rsql/)",1541689105.0
NotAnotherMoron2,"Venn diagrams are good representations of join types, and ER diagrams are extremely valuable once you have a basic understanding of the basic concepts of how a normalized database is structured. ",1542773631.0
getoffmyfoot,I’d need better details to be able to provide an answer. What exactly are you imagining for logical replication? Are you referring to a particular vendors term when you say that? What technology are you using to back your data lake? How are you planning on storing your metadata? All of these questions are important in describing an answer.,1541635260.0
msiekkinen,"Citing mysql manual in 2018 and then linking to version 5.5?  Look at the [5.7](https://dev.mysql.com/doc/refman/5.7/en/query-cache-configuration.html) version

>   The query cache is deprecated as of MySQL 5.7.20, and is removed in MySQL 8.0. ",1541609694.0
hippocrat,"I know mostly Oracle...
Physical backup is a copy of the actual datafiles and transaction (redo) logs, byte for byte. Oracle has its RMan tool that makes physical backups to a proprietary format, but when you restore you should have physically identical datafiles. Cold is when the database is stopped and hot is when it is running. For Oracle, you have to put the database in ""backup mode"" which captures extra information to the redo logs to make sure a consistent restore can happen. Never heard anything being termed a ""warm"" physical backup.

You are correct that a logical backup is basically the SQL needed to rebuild the database and as the DB must be up to generate that SQL it is hot. Unlick mysql or postgres, Oracle uses a proprietary binary file format, but essentially its the same. 

With the logical backup, you need to be more aware of the data changing while the backup is happening. Typically each table will be consistent, but other tables will not. In oracle, you can specify a ""as-of"" time or change number for the export to ensure consistency across the entire export but will need to make sure you have enough UNDO space to cache the state while the export happens.",1541602721.0
norvares,"Could you paste a table schema or describe the type of the updates?

I'll presume the following dataset:

&#x200B;

&#x200B;

|ItemID|NewValue|
|:-|:-|
|1|500|
|586|1253|

&#x200B;

&#x200B;

I would suggest importing the Dates in a newly created staging table and then run a massupdate like:

&#x200B;

    UPDATE valueTable vt 
        set value = uv.new_value 
    FROM updateValueTable uv 
    WHERE vt.item_id = uv.item_id

&#x200B;",1541589444.0
WeaselWeaz,The abstract answer is yes. It depends on the format you were provided by the table structure. If this is a timesheet/time clock system I can't stress the CYA aspect here. Understand what is happening and have emails ordering you to make these changes backed up to your personal email/devices since this could be your manager attempting to commit fraud.,1541597900.0
rat9988,Can you just give us a small subset of your work so we can help you? You don't look like someone with strong programming skills just give us a snippet of your problem and we might be able to help you.,1541612821.0
welshfargo,https://apex.oracle.com/en/learn/,1541537121.0
BrotoriousNIG,"**Small things first**

Some consistency issues with your field name style, e.g.:

* `Capital_City` (underscored, capitalised words)
* `Noob_city`, `Login_history` (underscored, capitalised first word only)
* `item_name` (underscored, all lowercase)
* `ITEM_NAME` (underscored, all uppercase)
* `Item name` (spaced)
* `LocationName` (camelcase)
* `subscribed` (lowercase)

Be consistent, so you don't need to keep checking your schema during development to see how you named something.

**Logic issues**

*Guild - Player*

You've got `Player_ID` as a foreign key in `Guild`, which means each player can be in multiple guilds, but each guild can have only one player.

If you want a guild to have multiple players and a player to have only one guild, then `Guild_ID` needs to be an attribute of `Player`.

If you want a guild to have multiple players and a player to have multiple guilds, then you want a third table `PlayerGuild` with fields `Guild_ID` and `Player_ID`, both of which form a composite primary key and each of which are foreign keys to the `Guild` and `Player` primary keys respectively.  You can then have additional fields in there that hold attributes about the player's membership of the guild, e.g. date joined, rank, and so forth.

*Player Location - Player*

Similar issue.  `Player Location` has an attribute `Player_ID` as a foreign key, which means the location can have one player, and each player can have multiple locations.  Invert that relationship to have a player having one location and a location having multiple players.

You might also want to rethink this modelling of location, but I'm not sure what the model represents so I could be way off.  If it's literally a coordinate set, then you're not going to store a location entity for each possible coordinate, so you probably want to just put the coordinates as attributes of the player, e.g. `Player`.`x-coord`, `Player`.`y-coord`, `Player`.`z-coord`.

*Membership - Player*

Similar issue.  You've got `Membership`.`Player_ID` as a foreign key, meaning each membership can have one player and each player can have multiple memberships.  If `Player` is an account, then I can't think of a reason not to have the `Member` fields as attributes of `Player`.

**Improvements**

*Password*

You're storing this twice.  Why?  If you're storing the 'password' and 'retype password' box entries, that is not necessary.  The purpose of that is to check that the user is submitting what they want to submit, by giving them a second chance at typing it in case they made a typo the first time.  If they match, take the password and store it appropriately, which leads me to...

You're storing it in a VARCHAR(20), which suggests that you're storing in plaintext.  You absolutely shouldn't do this.  You need to hash the password or run it through a Password-Based Key Derivation Function (e.g. bcrypt) and store the result.  When the user logs in, you perform the same operation on the password that was supplied at login and compare it to what you have stored.  Therefore you want a VARCHAR of the length of the output of your hash or PBKDF.  For bcrypt, for example, this is 60 and it looks something like this: `$2y$10$SFdgFSyYgkedwYl8aJDUhez1jrsaEfxRD6mDF9keCIwJeE0mzjFq.`

*Email*

You've got this stored in a VARCHAR(60).  The maximum length of an email address is 254 characters.

*Login History*

I think if you're going to store login history you probably want something more like `Login_Time`, `Logout_Time`, `Player_ID`, `IP Address`, `Client_ID`.  Client ID might be some hash of the details of a system.  This is usually done to prevent account-sharing if that is undesirable to you.

*Multi-character Futureproofing*

The scope of the MMO might not be that each person has multiple characters, but you might want that in the future, so I would decouple the character concern from the account concern by having an `Account` table with the passwords and email addresses, and a `Character` table with the details of the character and a foreign key to `Account_ID` so each account can have multiple characters and each character can belong to just one account.",1541521627.0
Dazocs,"Are your foreign keys in the right places? For example, you have a 1:M relationship defined for guild to player, so why are you putting the player ID foreign key in the guild table rather than the guild ID foreign key in the player table?",1541521189.0
BrotoriousNIG,"Additionally, I have no idea what `Main citys` represents, so if you could clue us in to that, that would be grand.  Maybe give us some example rows.",1541522224.0
Nukken,Don't make PlayerID your FK if it's the actual player name. It makes it much harder to handle name changes. Use a record ID and a player name field.,1541535580.0
Sparkybear,"It's your decision, there's really no right answer. If you will ever want to go back and retroactively assign items not tied to an event, it's probably better to just use a null value, tho it's not really all that different. 

If you are searching the items table by event id, I think it's a little faster if it's a null, as that should reduce the need for an additional join to pull in the event date of a record for event-less items.

That said, if your event table is just a lookup table, tho, the DBMS should optimise it to be identical to using a null value. 

If the application connecting to the database requires the record to work in a specific way, then follow that. Otherwise, do what you're comfortable with.",1541493983.0
r0ck0,"Yeah generally this is a common use of NULL.

In my earlier days as a programmer I didn't really understand all the behaviour around NULL, so I kind of just tended to set everything NOT NULL, and avoid them in general.  

But now I use NULL all the time.  Most of my table columns allow (and default to NULL) - unless the field specifically needs to always contain a value.

But maybe for your use case it does make sense for there to be a ""no category"" category, which has the same functionality as other categories.  So it really depends on how you're treating these ""no category"" items in your application.

To use an analogy with physical stuff being stored in boxes... if the item was out on the floor (not in a box), that would be NULL.   But if you have a ""misc stuff"" box, and you put the item in there, that's still a named box.",1541503685.0
BigHipDoofus,"NULL means unknown, it does not mean 0 or none. If a data point hasn't been evaluated yet, or the evaluation is inconclusive, then NULL is the appropriate value. If the datapoint has been evaluated and you know there is no connection, an empty string, zero, or other value explicitly indicating that there is no connection is the appropriate value. 

If I understand your application structure correctly, you sometimes know that there is no seasonal event for a video game. In that case you should include a value for No Event in your event table and link to that field from your video game table.",1541515927.0
Prequalified,"Index fragmentation, disk space overhead because of indices, index usage stats. Would be helpful to know which indices are unused, which need to be rebuilt and which should be considered to be added. ",1541447762.0
Ipecactus,I would recommend planning on adding every relevant DMV,1541437388.0
iamdenisu,Index type and index algo used maybe? ,1541437782.0
VeronsFabulousBeard,"Alright, I have looked into this once more and I would like to ask if this is a good way to find out the chen notation: 

Let A,B,C be Entity types 

If I pick an Entity from A and one from B and thus I can one entity from C, I write a ""1"" as a chen notation for C

Example: If I pick one topic and one student, I can identify the prof (you cannot write 2 papers about the same thing as a student)

If I pick an entity from A and one from C and I cannot completely identify B, B gets an ""N""

Example: I pick one prof and one topic but I cannot yet identify the student 

and so on.

Is this a good way to go about?",1541417488.0
morningmotherlover,"Maybe this is what you're missing: when a relationship is not binary, it is an n relationship. I don't know what notation that would exactly but I'm used to either * or n.",1541438340.0
Vendredi46,Good*! I meant Good!,1541412127.0
amved,"I would further decompose Book_Table into, create a new table for author_mast (author_id) and publisher_mast (publisher_id). So your Book_Table will only have author_id and publisher_id, removing author name (because one author can write multiple books) and publisher name and publication (same as author) .

You don't need Order_id column in student table, also the overdue_balance (place this is order_table). ",1541438257.0
getoffmyfoot,I think your best ticket is to work through a couple of realistic problems and creating database designs for them.,1541358409.0
Fairwhetherfriend,Go to literally any of the Lynda or Skillshare type websites that are everywhere these days and find something that looks good.,1541321447.0
KnowledgeableNip,Oracle SQL Server Data Modeler is a free software you can play around with if you want to practice after your videos.,1541352633.0
KitchenDutchDyslexic,"Install open source programs like zabbix and odoo and have a look at their database design.

If you run a tool like http://schemaspy.org/ against these databases you will start to piece together how the software store its data.

I feel this approach gives your real world software, with real world dbs. ",1541364826.0
mjsalvilla,"Try finding the book ""Modern Database Management"" by Fred R. McFadden, Jeffrey A. Hoffer, and Mary B. Prescott.

That's the prescribed book by our professor when I was still in college. Kept it at home.

Fast forward, ten years after, working as a DBA now, and I still revisit and refresh database theories using that book.",1541403965.0
Stychey,"I would highly recommend looking into papers by Kimball Group. You can find The Data Warehouse 3rd Edition in a PDF format with a quick a Google (however you should buy a physical copy if you are in the industry).

The main problem I find is there are plenty of resources to tell you how to design databases but they tend to be cookie cutter models. Sometimes you need a database for a very specific purpose. I have found the book extremely useful when looking at alternative modelling.",1541330122.0
Th3MadScientist,Type your title into amazon. ,1541333929.0
OvidPerl,"Try [How to Fake a Database Design](https://www.youtube.com/watch?v=y1tcbhWLiUM). It explains the basics of database design, but pretty much skips over the terminology.",1541336995.0
mkingsbu,"If I'm understanding you correctly, you want to make it so that you have a limited set of choices for a text field?

Normally you'd make a lookup table that contains only those location as well as an ID then you'd put the ID in to the source table instead then join them together.  

It sounds like you have Cities, Players/Characters, and Teleports --- and then there are relationships between the three of these, is that right? If so, I'd design it with those tables above and then intermediary tables for many to many relationships and foreign keys in them for when the relationship can only have one value.

",1541263554.0
r3pr0b8,"> whenever the relation ""buy"" takes place, 1 Customer buys N things?

no... the one-to-many relationship means that a customer can buy multiple products, not necessarily all at the same time

",1541264078.0
wolf2600,"You're making a table of Items?

Then every Item will be a record (row) in the table.  And the columns will be the various attributes of that item.

    Items
    -------------
    Item_ID    varchar(10) PRIMARY KEY,
    Item_Name  varchar(30),
    Item_Strength    integer,
    Item_Stamina    integer,
    Item_Dexterity   integer

    Items
    ----------------------
    1    Dagger    2    4    6
    2    Sword    7    3    4
    3    Pistol  11    8    10
    4    Torch    2    4     9


To add a new item to the Items table:

    insert into Items
    values (5, 'Shotgun', 20, 11, 8);
",1541190796.0
mtVessel,"Relationships are between entities.  The cardinalities describe the *type* of relationship.  A relationship should always be read in both directions to ensure it correctly represents the underlying domain.  In your example, ""an instructor may teach many courses; many courses may be taught by one instructor.""  To be clear, the relationship between instructor and course is one to many.  The relationship between course and instructor is many to one.  It is one relationship, but it can be read either way.

Other forms of ERD notation do not show minimum cardinality (optionality).  Generally, the most important information is which is the ""one"" side, and which is the ""many"" side, so other notations just focus on that.

IE notation also indicates if the relationship is optional, which is probably the confusing bit.  If a relationship is mandatory, read it as ""must"".  If it's optional, read it as ""may"".  If the maximum cardinality is one, we say ""one, and only one"".  Ex: An instructor *may* teach many courses.  A course *must* have *one, and only one* instructor.  But don't be fooled, the relationship between the entities is still many to one in that direction, because many courses could be taught be the same instructor, even though any *individual* course must have one, and only one instructor.

It is rare for the many side to be mandatory (as it seems to be in the example you posted).  In this case we say, ""one or more"", as in, ""An instructor must teach one or more courses.""

Hope that helps.",1541135908.0
Old13oy,"I literally just had a field engineer from Neo4j in my class today. Their tech is REALLY cool, I think it stands to challenge the supremacy of a lot of existing tech.",1541123583.0
Milnternal,"You could do this with literally any database... I'm not entirely sure what you are even asking. In a relational database you would use a join between a data and a tags table and WHERE clause on the tags table, and on a document database you would simply filter on the tags property. You don't usually create 'html/javascript' websites to connect to databases, you would tend to use backend code (could use javascript for it I guess if you used node) to query the database then render the data into html. If you do use couchdb I believe it does have a build-in API so you could use ajax calls (probably from JQuery: [http://api.jquery.com/jquery.ajax/](http://api.jquery.com/jquery.ajax/)). And then you would simply use the find ([http://docs.couchdb.org/en/2.2.0/api/database/find.html](http://docs.couchdb.org/en/2.2.0/api/database/find.html)) or filter call to filter on your tags collection property: [http://docs.couchdb.org/en/2.2.0/ddocs/ddocs.html#filter-functions](http://docs.couchdb.org/en/2.2.0/ddocs/ddocs.html#filter-functions). There are plenty tutorials if you google couch db example app ([http://www.speqmath.com/tutorials/couchdb\_tasklist/index.html](http://www.speqmath.com/tutorials/couchdb_tasklist/index.html))",1541098387.0
akaitatsu,"> Since our last release, we’ve helped Comcast provide its users with great customer support...

Comcast is not known for their customer service. I kinda stopped there.",1541104857.0
mkingsbu,"Add a table for player, remove player_id from the guild (unless a guild can only have one player).  Make a guild_player many-to-many relationship table that just has thow two keys in it.

""Victory's"" should be victories, although usually I keep all of the columns and variables singular in my database for the sake of consistency since it is assumed that there will be more than one of anything and the row represents a single value of that thing.

If strength, stamina, and dexterity are all attributes of an item then that is fine. You could also put it in long form and do:
item, type, value
001, strength, 5
001, dexterity, 10

etc.

As it is, its fine so long as the key makes sense. If there are a lot of nulls (such as items that have no dexterity) it makes more sense to do it the way I mentioned above, but having

inventory_id, strength, dexterity, stamina

Is fine too so long as they are separate columns.",1541087764.0
DRdefective,Trello,1541075110.0
ILiterallyLoveJV,Is this on a production sever or a reporting server? What’s indexed (if any)? Have you checked your data execution plan? Have you tried creating some CTEs within a Stored Procedure to break this down with variable parameters? This is quite a complex query. One idea is to funnel data into a temp table and call on it that way.,1541026684.0
goblando,You are joining on the evaluation of functions meaning almost no indexes can be used.  I would create a temp table performing the function work upfront and then join on that.,1541026992.0
Th3MadScientist,"CTE is killing performance. The application of functions in your join conditions negates the column's SARGability, not sure about postgresql, but it looks like you also have sub queries in your select statement. All this screams piss poor performance.",1541031859.0
wolf2600,Buy it a nice dinner and a couple of drinks first.,1541020477.0
david622,"There's no sense in ""getting familiar"" with a database with no context. Try to find existing reports and look at their SQL, and then you'll have business context around the data in that report. 

Or, read through application code which calls the database and see how the data is being leveraged.

Just staring at data is meaningless. What's meaningful is to try and solve a specific problem and figure out what parts of the database are used to solve that problem.",1541027334.0
KitchenDutchDyslexic,http://schemaspy.org/,1541024016.0
R4bbidR4bb1t,Depending on the database I would try to reverse engineer an ERD. I would also look at preexisting views and functions to see what data they pull and effect. If you have an existing reporting system maybe open some reports look at what tables they use and data they pull. Look at the constraints on the tables. You could also look at any program that connects to the DB and see if it offers an option to see any underlying SQL for a give module/task. ,1541023936.0
NSA_GOV,"Does it connect to a front end? Pull up a record (Customer record, etc) and match tables and fields and values to what you see in the front end.",1541086729.0
alazicza,"IMHO this a typical scenario where a traditional approach with erd and 3nf first that may give you a bit lesser elegant solution. However these techniques are still valid and I am aware of other systems have been designed this way and work good.

However if I had no constrains I would go for REAL (Resource Events Actors Locations) as an accounting methodology. Wikipedia and accounting text books have pointers on this. I would implement this using CQRS pattern and Event Sourcing pattern. If of interest look at Greg Young and his talks or really simple and elegant sample (I think it is sitting on GitHub). His sample is a trivial inventory system.

I hope this helps.

Kind Regards

Alex",1541050632.0
welshfargo,Some BOM example data models can be found [here](http://www.databaseanswers.org/data_models/).,1541111954.0
boy_named_su,"You should get to know an ERP/MRP first. Maybe look at its table structure and code. 

You could look at https://xtuple.com/, https://ledgersmb.org/ or https://www.odoo.com/ 

Otherwise, you're going to need to understand some pre-existing data model patterns:

https://dba.stackexchange.com/questions/12991/ready-to-use-database-models-example/23831#23831

For BOM, you want a database that supports Recursive CTE, and you should understand the Adjacency List

For Inventory, you want to model it as a series of Movements between Locations. Inventory is a view, not a table",1541006433.0
alinroc,"Is your company in the business of building software? And specifically, this sort of software? If not, buy, don't build. Otherwise, you'll now have two jobs and spend **far** more time and money on building and maintaining this thing than you expect.

You aren't looking for a database, you're looking for an entire solution *which includes data storage* (a database). There are turn-key solutions out there for ERP, inventory management, job costing/BoM, etc. that range from web services that will scale with your business to massive on-premises solutions which will require a datacenter and hundreds of thousands of dollars in hardware.",1541076992.0
WhiteMos,"I've never heard the term ""range query"" before. 

In terms of query writing and performance, don't take two lines to write what you could do in one. Don't sub-query stuff you don't need to. Don't join tables you don't need. Don't drag in columns you don't need. This is all kind of ""code writing"" 101. Write the shortest, crispest code you can write that gets the job done as quickly as possible. 

Of course it all depends on the nature of the query. When I was newer with SQL I wrote queries sometimes that were 300-500 lines long (using unions and stuff). After about 8 months of experience I've been able to take that same query and get it down into about 30 lines, believe it or not. 

If your question is ""Are shorter queries better"" the answer to the question is: The shortest and best performing query that can get you the results you want is the best one. ",1541021857.0
doctorzoom,"Your question is pretty vague, so I'll give a pretty vague answer: yes.







Joking aside, I'll try to answer some things that might actually be your question. You could say a query is short because it doesn't take up much space on screen or you could say it's short because of how efficiently or quickly it runs. There's a tendency for queries that take up more space on screen to run more slowly. They usually touch on more tables and are doing more expensive operations (joins, aggregates, etc.) However it's possible to write pretty short queries that will run all day (""select * from some_really_gigantic_table"" might do the trick.)  Some mammoths on the screen might run pretty quickly from a human's perspective. 

I'm thinking you were probably reading about databases being used in an interactive application. People tend to write short queries in a transactional environment because the tables are optimized to return results quickly given just a handful of parameters. For example if you're wanting to populate some user information in a web application you'd see real short queries like ""select user_name from users where user_id = 2452349523740987"". In this type of database work I'd say yes, ""short"" queries are much more common than they are in analytics or reporting type of work.",1541023824.0
wolf2600,"    select * from orders
    where order_date between date'2018-01-01' and date'2018-01-31';

I guess this is a ""range query"", in that you're filtering on a range of dates.",1541025677.0
Isvara,"Why that name, though? It makes me shudder every time I read it. And they make it worse with the pictures!",1541002486.0
DataDecay,"Each different rdbms has different tools available, some are community, some are costly. In addition I see no mention of which rdbms so I'll use a open source database like mysql as a point of refrence.

1) How I would test if the problem was concerning the relational DB?

An RDBMs is generally architected over client-server (not always but usually) so you would have the following stack.

App --> os --> network --> os --> database
--> os --> network --> storage (the last three are if ur using things like SAN, Netapp, ect nfs).

Each part of the stack equally works in the pipeline and can equally share blame in performance degradation. Unfortunately going over troubleshooting each layer would be far too long for a post. But things too look out for can sometimes be app bottlenecks, network bottlenecks, database bottlenecks, storage bottlenecks, os bottlenecks. 

A quick way to check the database against the app is database retrieval of request and processing time. Throwing a tail on (again using mysql as an example) process list will watch for connections. You can time when you send the query to when the process is opened on the database, that is request time. Next watch how long it take for your app to process and watch the apps process time on the database, if the database time is low but app time is high, look closer at app. If visa versa look at the database.

2) What I would do if it was identified that the DB is slow compared to if I found it was actually very fast?

O boy this is a big one I'll keep it general. Check the queries:

1. Check the explain plans.
2. Based on explain plan look for poor use of bind variables, to may full table scans, poor indexing, ect.

Check the database resource internally and externally (meaning os), are there too many iops, are you swapping, are your transaction logs to small causing to many os writes, ect.

All of these finding conversely can lead to proving that the database is also performing at peak performance.

Again this is not an exhaustive list or approach every time I work on ""performance problems"" it's different and leads down equally interesting and different rabbit holes.",1540997869.0
cowp13,"1. There are couple of ways: check server resources history, if the query is read-only, run it directly against prod DB (maybe with a help of DBA).

2. look for the most common issue first: joins on unindexed columns, too many joins in one statement, too much data in temp tables, badly written SQL, so on

if the DB is fine, have the user try to reproduce the issue with you on the other end, maybe doing the same thing if you can. if your connection is fast but the end user's is slow then check the connection",1540999163.0
DesolationRobot,The DB will have a slow query log that tells you what queries go beyond a set threshold. A web app typically shouldn't have a query longer than a second. There are tools like Percona's pt-query-digest that automate it a little.,1540996320.0
drunkadvice,"The presented scenario sounds like Parameter Sniffing.  Here's a really really detailed write up on it.  http://www.sommarskog.se/query-plan-mysteries.html
",1541011747.0
da_chicken,"RAID 3 (byte striping with dedicated parity disk) is ancient and was abandoned for RAID 5 due to the disk failures that 3 caused and the complexity of the HBA it required.  If you're running RAID 3, it's going to be on a 20 year old server using old parallel SCSI interfaces.
",1540938832.0
swenty,"This isn't the right forum for this question, and even if it were, you haven't given enough information to be able to get much useful help, such as whether it's hardware or software RAID, what type of disks, what type of server, which operating system, what type of RAID controller, what error messages, etc., etc.

I suggest you collect as much information of that sort as you can, then post the question to https://serverfault.com, tagging it with ""RAID"" and the type of the controller and/or RAID software.

Alternatively you might be able to get support from the server or controller maker. Check also whether you have a support contract.

Often it is possible to recover RAID errors, but one needs to be careful not to make the situation worse by making incorrect configuration changes. Hopefully you also have current backups of the data in case that doesn't work out.
",1540947789.0
nsnow70,Not sure if you meant some other type of raid because I’ve never heard of RAID 3. If they simply added a disk to the server and didn’t add the drive to the array then maybe no damage is done. Otherwise I’d just rebuild and restore from a backup. ,1540937192.0
grauenwolf,"Looks like you are missing the many-to-many mapping tables.

* Users -> Projects
* Users -> Departments
* Users -> Jobs
* Projects -> Platforms
",1540833847.0
jpers36,"r/Databasehelp is a better sub for this sort of thing.

&#x200B;

Start diagramming your tables to see how they interact, and what interactions you're missing.  I get the feeling there are some fact/bridge tables that you'll find need to be added.",1540833054.0
merdincz,You have to take a look at [database normalization](https://www.guru99.com/database-normalization.html) ,1540837576.0
GirthBrooks,"Have you thought about using a tool like JIRA or are you looking specifically to create something from scratch?

As far as I can tell, JIRA would more than meet your needs.",1540850096.0
kombinat,Exactly 20 tables? What's the reason for that?,1540854326.0
kombinat,"Couple of ideas...

&#x200B;

Fine grained permissions:

Group, User\_Group

Roles, User\_Role, Group\_Role

Permission, Role\_Permission

&#x200B;

Customisable status workflow:

Status\_Type

Status\_Change (controlling what status can move to what status)",1540861528.0
welshfargo,http://www.databaseanswers.org/data_models/,1540926889.0
twistdafterdark,Maybe an ERP system is what you're after?,1540826309.0
KitchenDutchDyslexic,"Have you looked at running odoo on a local debian box?

We successfully run https://www.odoo.com/documentation/10.0/setup/install.html

Latest would probably be https://www.odoo.com/documentation/11.0/setup/install.html

Goodluck,

Otherwise a vps/localhost install of pgadmin4 is easy as well. 

But I would recommend postgres and fdw to build a base on.

Btw if your doing IT infrastructure monitoring zabbix might be a solution? Even use it to store IoT data with triggers on items.

meh this post becoming a random brain dump :/ ",1540826440.0
mabhatter,"Not to bash on ERP, but what you describe is more of a CRM.. Customer Relationship Manager. Like Salesforce. Where you can put all the contacts and leads and then have tools to keep up with tracking them. 

You could probably find a combination of open source or hosted projects that would get you there. Things like Wordpress and Drupal have “CRM-like” modules to add.. and there are providers that can host them for you. ",1540853310.0
mkingsbu,I just installed Tryton (http://www.tryton.org/) for a client who sort-of has similar requirements and they've been liking it so far.,1540826727.0
Actually_Saradomin,Just use something like hubspot or salesforce. Otherwise you'll spend half your time maintaining the software,1540897211.0
changeupcharley,"Why don't you just scrap all tbe bullshit theory and use MySQL? You can have everything you want for a fixed cost. Don't skimp on your skimping, it's not worth compromising your technology over at this ridiculously small amount of data.",1540709690.0
xkillac4,Have you looked at aurora serverless?,1540741756.0
quentech,"> have millions of rows potentially

You sure about that? Or are you prematurely solving scale problems you don't and might (likely) never have?

> using S3 to store the items

If you really only need key/value lookup, blob storage is fast, cheap, and reliable. Storage is a foundational cloud service - many other services rely on it, so it tends to be very solid.",1540749082.0
Solstiss,"This is a broad request, but what I can suggest is to get a Linux VM running with mariadb set up. You can use MySQL workbench or dbeaver or some other tool to work with the database and practice much of what you're looking for. You can find all manner of free data online to work with (kaggle is a good example) to leverage all these tools. 

Mariadb is easy to get a basic setup going and there are a bunch of tutorials that will get you started. 

Also if you are trying to do this with the command line, mariadb client (I am sure windows has some variant) and do it that way too. 

mysql -u <user> -h <hostname or ip> -p <password>",1540673194.0
welshfargo,http://sqlfiddle.com,1540676981.0
DataDecay,"Get a VM set up and install an open source RDBS (for now stay away from nosql unless your told that's part of the job), I would suggest any of the three mysql flavors.

By the time you get a database up you will have a basic understanding of connecting locally. Whatever you do, after that, DO NOT install and use workbench, these gui based tools are handicaps for beginners. Instead work on understanding the remote connectivity using scripts to query with connection strings and output data.

You want to understand the core mechanics of the database on a DBA level. I deal with too many developers who call themselves DBAs yet use unions and cartesian joins across the board and then complain about performance. Or neglect to gather stats, or index on ridiculous unused columns, or God forbid run loads in nologging ""because not doing that takes up space"".

A database is easy to use but hard to use properly, please don't take it lightly.",1540696993.0
postalot333,"Do those databases vary from client to client, with regard to structure?",1540657891.0
DataDecay,Importing and updating resulting in customer ID changing? I do not think I understand this question. The results of an update or import are heavily derived on business logic and database design structure. An update or import is not synonymous with actual data change,1540667282.0
bears-eat-beets,It might be unpopular here. But what about using a data lake? Where the customer can land raw files. Then you have a process that sucks up the data from the raw files and matches to a membership key (probably a compound key of client ID-Email) and then upserts/merges the data. ,1540673902.0
Catsler,"Don’t build anything. 

~~Buy access to a web app that does all this already.~~

Petition your managers/directors to get you a proper time and attendance system. 

If you start down the road of developing software, you’ll then have 2 jobs. ",1540648063.0
alinroc,"Like /u/Catsler  said, this is something you buy, not build. This is a solved problem, there is already software on the market that can do this.

Cloud storage? Make sure it's HIPAA compliant.",1540660300.0
welshfargo,https://wheniwork.com,1540754074.0
seetler,"nope, that's why they hire people.

&#x200B;",1540591117.0
mkingsbu,I've started using SQLAlchemy for this.  I basically use the source tables as the baseline then normalize it but it takes time!,1540599489.0
stickman393,"Pull the metadata. Look for patterns.

Identify the primary keys (or candidate keys) in each table.
Look for the same columns in other tables - this could identify relationships.

Categorize the tables into Transactional, lookup, logging, etc

ODBC available? Ingest the schema into Enterprise Architect and start putting together ERDs on a per-subject-area basis.. 

Identity the Business Domain Experts and talk to them.",1540607545.0
drunkadvice,If there aren't FK constraints applied to the database the algorithm doesn't have a chance at figuring it out.,1540597481.0
JanellieBean,Thank you for this!!!,1540601456.0
needs_must,Thanks! This looks awesome!,1540608392.0
grumpyyoshi,Seems like it's not available in the UK ,1540626884.0
lordkoba,"Default configuration on MariaDB is ACID and GridDB in theory is ACID by they are clearly not flushing.

> GridDB’s memory-first architecture that enables it to better manage data that stays in memory reducing the amount of I/O required

MariaDB can also not flush changes immediately for those benefits. This reeks of bad faith.",1540567900.0
reSAMpled,"Hopefully this doesn't feel off-topic. It's a Gremlin driver, which works on lots of graphDBs and other dbs as well",1540512135.0
DesolationRobot,"Regular SQLish tables?

You're talking ""how to know what data is no longer valid if I remove this data source?"" To avoid abandoned columns/tables?

Step #1 would be to make sure all data sources are using different database users to access the database. Then all your other logging becomes easier.

Percona has a tool called pt-schema-digest that can ""fingerprint"" queries. You could explore that.

You could take a really pro-active approach and only give each database user permissions to the exact columns and tables it needs. Then when you remove that source you have documentation right there as to what tables and columns are now suspect.",1540508999.0
mkingsbu,Perhaps I'm misunderstanding you but it seems like this would be a good use of Foreign Keys.  ,1540508694.0
ACarmon02,"Can someone help me interpret this.............

&#x200B;

 ;WITH A(A) AS (SELECT '' FROM (VALUES (0),(0),(0),(0),(0),(0),(0),(0),(0),(0),(0),(0),(0),(0),(0),(0)) T(C)),  
   B(N) AS (SELECT TOP(100) ROW\_NUMBER() OVER (ORDER BY (SELECT NULL)) N FROM A A,A B)  
SELECT B.N  
 FROM B  
 WHERE NOT EXISTS(SELECT \* FROM myTable X WHERE X.myCol = B.N) ",1540824600.0
wolf2600,"https://www.baeldung.com/kafka-exactly-once

""Exactly once processing""??  Sounds like it means a message will be delivered exactly once... it ensures a message isn't delivered multiple times (or not at all).


>  Due to various failures, messaging systems can’t guarantee message delivery between producer and consumer applications. Depending on how the client applications interact with such systems, the following message semantics are possible:

>  If a messaging system will never duplicate a message but might miss the occasional message, we call that at-most-once
  Or, if it will never miss a message but might duplicate the occasional message, we call it at-least-once
  But, if it always delivers all messages without duplication, that is exactly-once
",1540346544.0
timezone_bot,"11:59 pm PDT happens when this comment is 10 hours and 42 minutes old.

You can find the live countdown here: https://countle.com/ThfcflHrJ

---

I'm a bot, if you want to send feedback, please comment below or send a PM.",1540325768.0
ramborocks,"I was gonna skim through it and see how it looks... Missed the timer :(

I hope others give you some solid feedback. Good luck ",1540373169.0
ramborocks,"Maybe, it was just late at night and I couldn't sleep.  reddits great for that... ",1540476036.0
KitchenDutchDyslexic,https://couchdb.apache.org/,1540315014.0
WeaselWeaz,"It really depends on how it's being used, not enough info here. It could be a SharePoint Online library so that you can tag the documents with different columns of metadata. If it needs to be free and it isn't a ton of data then an Access database with row attachments could work.",1540315191.0
adoarns,"SQLite

    create table tagdata (recordid integer primary key, blobtype text, attachment blob);
    create table tags (tag text, dataid references tagdata(recordid) );

Then a little python utility that inserts into the database file.",1540385287.0
Oxford89,"You're most likely not going to find anything free considering tthe effort required to maintain live stats for every game. These aren't databases but I did a quick search and found two services. You could likely use their API to build your own database.

Live Baseball API | www.sportradar.us/

Fantasy Baseball Data API | www.fantasydata.com/",1540300673.0
extDASH,Nice! Thanks :),1540302009.0
nivenkos,"Buy a raspberry pi and arduino and build a sensor to generate data. Then work on sending that in regular events to an event queue to be processed etc.

Basically build your own pipeline. ",1540276319.0
boy_named_su,"Pentaho Data Integration is free, open source

And SafariBooksOnline has a free trial (which you can ""extend"" by using a different email address), and has a few books on it.",1540246240.0
svtr,"I'd honestly say that you can't practice ETL. In theory designing and implementing ETL processes is dead simple, interface to the left, interface to the right, extract load transform, done. 

In the real world, it usually is not that simple. In the real world, you have to technically specify those data interfaces, test the data delivery against the interface, alert for mismatches (which happen A FUCKING LOT), deal with edge cases, deal with the software lifecycles and so on and so on. 

The book theory is all good and fine, but its not the work you do when working on ETL processes.",1540255166.0
kum0nryu,"Build yourself a Postgres database (you can even host a “micro” Amazon RDS Postgres instance for a year for free) and then practice writing functions and small applications (in Python for instance) for loading and transforming data from openly accessible API’s, csv’s, and other formats you’ll encounter out there among business data infrastructures. Play with ETL and ELT so you know both methodologies and when/how to use them as tools of your trade.

Though this won’t prepare you for the specific nuances of each case, it might help you develop some techniques and will give you a nice sandbox.",1540273998.0
Solstiss,I believe some variants of talend are free. It's the ETL tool my company and several peers are starting to use. ,1540248587.0
msiekkinen,"Build a table with average temperatures for your city per day.   Then extract that to do averages per month in another table, then another for year. ",1540281514.0
Seemslikeagoodday,"Use at your own risk, http://libgen.io/ has most textbooks for ""free"". I did a quick search for SSIS and found SQL Server 2017 Integration Services Cookbook for example.",1540244434.0
mercyandgrace,"MS Data Tools is free with the development release of SQL Server, if Iam not mistaken.",1540258063.0
gnarfel,"The jet engine only supports local mdb files, there is no “ms access server”

The closest you can get is a file share but in practice that’s a bad idea ",1540235185.0
eshultz,Are you trying to model the menu or customer orders?,1540175993.0
Philosiphizor,Can you try to get the erd to represent 3rd normal form? Should remove your worries of duplication.,1540206412.0
kairos,"Wait, 18c?!

What happened to 13, 14, 15, 16 and 17?",1540211316.0
aaaqqq,"> **Will patches be provided for Oracle 18c XE?**
>
> No
>
> **Can I use Oracle 18c XE in production?**
> 
> Yes. However, Oracle Database Express Edition is not supported and does not receive any patches, including security patches
>
> **Can I log an SR against XE?**
>
> No

it's really only for folks who already use this database in production and want something like this for dev
",1540211388.0
forvarm,">Only one installation of Oracle Database XE can be performed on a single environment. This does not affect any existing installation or new installations of Oracle Database 18c Personal Edition, Oracle Database 18c Standard Edition 2, or Oracle Database 18c Enterprise Edition. To run more than one Oracle Database instance or install more than one copy of the database software, upgrade to Oracle Database 18c Personal Edition, Oracle Database 18c Standard Edition 2, or Oracle Database 18c Enterprise Edition.  


So what defines an environment? Can this be used in an OpenShift environment with multiple nodes?

&#x200B;",1540195032.0
whiskeydude,"I’d agree with your single table for all users idea, but I’d only have an ID mapping to another table like Positions or UserType that would define what each landlord/agency/tenant can see. You could also create another table with individual elements that you want to be able to show/hide for each ID (table named Elements maybe), and have a mapping table between the Positions/ UserType table and the Elements table.

Or if you want to keep it simpler than that, have your Positions/UserType table have a different column for each thing you want to enable/disable. Regardless of the direction you go, you need to figure out the best way to check the current users access rights. If it were me, I’d have these access rights in the database but manage most of my user flow through PHP links. Then again, I’m a backend dev so I’ll defer to others how to implement that type of logic.",1540113926.0
Philosiphizor,"I'm just wrapping up my database management course at uni. So I'm coming from more of a theoretical application versus actual experience. Anyways, have you constructed an entity relationship diagram (erd)? After the erd and getting data to at third relational form, I think you'll have a lot less tables. As far as restricting access to rows of data per user, I'm not sure. Restricting a unique user name to their user id can enable some sort of self verifying access control.  Permitting access to only the data (entered) by that user. ",1540124515.0
boy_named_su,"You need to read up on Table Inheritance and The Party Model

Individual and Organization are subclasses of LegalParty

A group is a subclass of Organization. So is a company

A LegalParty can play the role of Agent, Landlord, Tenant
",1540317044.0
welshfargo,http://graphdatamodeling.com/resources/rettigNormalizationPoster.pdf,1540144751.0
mkingsbu,"Honestly, the wiki pages on normalization are actually really good. I'd check those out first.",1540087075.0
turimbar1,"users shouldn't be directly interacting with tables - inserted values should be parsed and sniffed by stored procedures, and read values should be handled by views. Otherwise you are at serious risk of an sql insertion attack, which is even more dangerous if they know your table names...

Access to the data should be handled at the application level, unless they are working directly with the database (eg a dba or developer) - in which case assigning Role base permissions is far more scalable, as you can just assign and un-assign users to roles which have all of the necessary permissions.

database systems do have users/permissions on a queriable table - typically it is a system table. If you grant users access to the system tables to read permissions then that is a serious security risk just by itself.

I hope this database isn't storing anything important.",1540069245.0
ecrooks,"I do not know what other RDBMSes offer, but DB2 has a feature called Row and Column Access Control (also called RCAC or Fine-Grained Access Control) that sounds like it could meet your needs ",1540069390.0
alinroc,"> Is there a particular DBMS that excels at this functionality?

Oracle and SQL Server implement row-level security. In SQL Server, you define a function for each table which, in conjunction with the user's identity, will allow or deny access to a given row. There is a performance penalty for this.

I'm pretty sure SQL Server has column-level security implemented as well (without using a function).

Managing such granular levels of access can be a lot of administrator overhead though. Tread carefully and, as /u/turimbar1 noted, you may want to rethink how your users access the database in general.",1540084387.0
alinroc,"Multi-tenant with one database per tenant quickly becomes a management challenge. You will need tools and processes for handling this at scale. 

Source: I’m a dba for a system like you describe. ",1540053024.0
bewalsh,"I'm only intimately familiar with azure, but their resource management api is high quality. Check out azure sql, you can create resource 'elastic' pooling so your db list is billed all as one cost bucket. Doing so makes a long list of dbs pretty affordable and simple to manage for me.",1540069558.0
rN8EYUyb,"My comment is based on the assumption that your talking about analytic type workloads. If that’s incorrect on my part, just ignore. 

But if you are talking about analytic workloads check out Snowflake: https://www.snowflake.com/

It’s not really a good option for transactional workloads - small writes are expensive - but for “batchy” things, beefy query patterns, and scalability it’s pretty dope. 

Pricing is based usage for storage and compute. Starts at $2/hour - billed at 60 second min. - for compute and $40/TB/month for storage. There’s also various “editions” that have some more enterprise features and you can do capacity type licensing that’ll get your storage cost way down. 

Don’t need to worry about capacity planning since storage is all S3 or Azure Blob, depending on where you want to deploy. So you also get those durability guarantees. 

Compute is instantly resizeable. So no need to reprovison entire clusters like in Redshift if your current cluster can’t handle the load. Just resize and query again. And/or have multiple types of clusters for specific jobs.

Automatic resume and suspend so you don’t pay for compute if no one is using it. But if you do expect 24x7 usage - which it seems like that’s not Going to be your case - it’s going to be more expensive than similar products like BigQuery or Redshift. 

There’s also “time travel”, zero copy clones, data sharing, fail safe against data loss, etc., etc. 

I do a lot of work in the analytics space and we’ve done projects like yours standing up multi-tenant instances for clients in the B2B world where their clients are looking to supply analytics, reporting, whatever, as another revenue stream on their existing platform. 

Anyway hope that helps. ",1540612254.0
DesolationRobot,"If you're ""rewriting the array"" every time, you can re-write the individual rows or update all order values just as easily, no? Even if it was as drastic as ""delete from table where user = XXX"" and then ""insert into table ..."" every time. And even then, the update math isn't hard. You just watch for what position the user moves the photo to and what position they moved it from. If they moved it from a high index to a low index then update that photo to that new index and all photos *except that one* who have an index value of that or higher get incremented one. Reverse if they moved a low index photo to a high index photo. You could actually do that all in the database with a BEFORE UPDATE trigger that evaluated old.position vs new.position.

Second way is definitely a better way to store data.

Is there ever a chance that two users would have the same photo? Or would that be a common enough occurrence to matter? Your list-sort table could be a lookup table between users and photos tables.

Would you ever want to run analysis on user behavior? ""What is the most popular pinned photo?"" etc. Or bulk update photo data: ""all http urls are now https"" or whatever. Second data structure would both a lot easier.",1540018536.0
c3534l,The second one without a doubt. The first one is a classic how not to do databases.,1540024063.0
dangercraft,"Your first way is the way NOT to do it.  I won’t lecture you on why, but if you are interested you can look up database normalization forms and you’ll find that it violates the first form and why thats important.

As users before pointed out, the second way is the way to do it, but you would need to handle shifting all of the absolute positions every time they are changed. I would suggest that rather than using absolute positioning for the image url, that you have a reference to the id column of the image that comes before the it.  Then make a constraint check so that the position can only either be null (first image in group), or pointing to the id of another image in the group.  You would also want to set up an insert, update and delete trigger so that if an image is added with a position id and group combination that already exists, or if there is all of a sudden an hole in your sequence, you shift all of the position ids of the group to automagically accommodate the insertion, deletion or movement in the list.  This would make it very easy to change list orders and keep the logic wholly in the dbms.  Determining the group could be as easy as having another column with a foreign key to the user’s id whose picture that belongs to.  I also might add a trigger so that if an image is added, the position field is empty and its NOT the first image in a group, that the position field then point to the id of the image in the group that does not have another pointed to it. 

Of course this is assuming that either you aren’t storing the images in question or that its not a concern if multiple users store the same url.  In case you are storing the image, you could store the image hash with the rest of the image information in a separate table (uri etc.) and whenever an image is introduced to the system, you check to see if the hash of the new image is present in that table.  If it is not, add a new row, save the file (file system, S3, cds etc)and point to that row’s id as a foreign key instead of the url.  Otherwise, just point to the corresponding row.  Of course your url column would now have to be a foreign key pointing to the proper id in your images table.

Hope that helps. Kind regards.

Edit: spelling, grammar, clarification",1540036945.0
boy_named_su,"i prefer the second way, as it's relational, however you get an update problem

one way is to use decimals to represent the sort-position

say you add a new picture between 4 and 5, it becomes 4.1. You don't need to update the ones ""below"" it",1540011995.0
drunkadvice,app > server > db,1539998365.0
nlytnmnt,Connecting the two directly is not a good idea. I would recommend having an API service layer in between the database and the mobile application. ,1539999555.0
shanky35,"As others have mentioned, the better approach would be to create an API which would accept the requests from your app and make the db calls and sends the response back to the app. This 3-tier architecture would help you in the long run.",1540016968.0
gnubyter,https://github.com/lk-geimfari/mimesis,1540014194.0
inknownis,Put a bookmark. Thanks for sharing,1540015440.0
crookedkr,Would be nice if you could also choose different distributions. ,1540033643.0
adeguntoro,Let me bookmark this.,1540044591.0
stickman393,It sounds like you need a [Date Dimension](http://radacad.com/do-you-need-a-date-dimension),1539994181.0
kenfar,"This is a common requirement, and has been solved well many times for databases with many billions of rows.  A few things to look into:

   * Use dimensional modeling concepts to have a very small number of fact tables that then have keys from related dimensions.  If you can't handle versioned dimensions (which give the best quality time-series analysis) then try to denormalize dimension data into the fact tables.
   * Ideally partition fact_table by whatever low-cardinality columns you filter by often.  This will often be date, maybe team?, etc.
   * Generate daily aggregates for your main fact tables, say fact_player, fact_team, etc.  These aggregate tables basically look identical to the base fact table except that they only have 1 row per day for the rest of the key.  You can also build higher-level aggregates at the level of week, month, etc if you have enough data and need more speed.
   * Leverage parallelism in your database.
   * Try to query the aggregate rather than base fact table whenever you can - and just regroup at the higher-level (weekly, monthly, etc).  As stickman393 says - a date dimension helps a lot for this.

If you've just got millions of rows, and your aggregates are say 1% that size or less and you just need queries to finish in a couple of seconds it should be easy to just groupby & sum as needed.  If you need it a bit faster then speed up your tablescans by using partitions to bypass 90% of your data.  If you need it still faster and have highly selective queries only then might indexes be worth exploring.   And if you're using MySQL, well, it tends to suck at this kind of querying.",1539999633.0
My_POSH_Reddit_Acct,"What you outlined is, essentially, a staging area.  My question to you would be why not create the staging area as s single separate database on the same server as your data warehouse?

Extract the the data and load it into the staging database, then do your transforms to your fact and dimension tables.

Edit:  And make sure you add a date/time stamp in each of your staging tables - it you have to reload the DW to a point in time or just reload a single days' transactions - this in worth it's weight in gold.",1539971913.0
six0seven,"This is a kind of stage one data lake. If I had to do this on-premise with windows machines, then I would use separate servers for the database and this data lake, knowing you can buy cheaper drives for the lake machine and optimize the database machine for serving queries faster. 

Also it would be good discipline to build custom 'producers' to connect from your source systems to the data lake. This is the perfect three tier situation. I have huge problems with using schedulers as part of the database system itself. It's an amazing fact how well crontab runs, considering what people spend for other third party schedulers. When your lake is good, you're way better off for disaster recovery as well.   


I would highly recommend using sftp and plain CSV on your lake on server side encrypted drives unless you have a very specific requirement to encrypt the files themselves. Unless you have a serious key management system, encrypting source files is a recipe for serious headaches.   


I've done staging areas and ODS's before. Data Lakes are an evolution. Call it a data lake and learn more about them. You're better off in the long run. [https://insight.full360.com/what-is-the-role-of-a-structured-data-lake-in-dw-cbc8682cc815](https://insight.full360.com/what-is-the-role-of-a-structured-data-lake-in-dw-cbc8682cc815)

&#x200B;",1539990586.0
alinroc,Date tables are very useful. https://www.brentozar.com/archive/2014/12/simply-must-date-table-video/,1539960332.0
grauenwolf,"Date tables change everything for me. They make what was once hard, error prone, or tedious very simple. I now use them on nearly every project.",1539963476.0
ilikedbthings,"Its not a bad idea. In Dimensional Modeling for the Kimball Methodology (Data Warehousing), it is recommended to have a date dimension. I have no experience with FileMaker, but 60k records shouldn't be a lot.

https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/calendar-date-dimension/",1539959109.0
HereForCatharsis,"Probably not a lot of FM fans here, but I thought the question was broad enough to extend beyond the platform itself.  

I know the answer is probably subjective, but I'm always looking to improve my design knowledge.  ",1539957205.0
thejumpingmouse,https://en.wikipedia.org/wiki/List_of_hospitals_in_Romania,1539957375.0
jlrobins_ssc,"JSON with 'quite a lot of recurring recurring field names' sounds like it'd be suitable for traditional relational / columnar storage. Migrate to traditional table structures where the 'field names' would be stored just once, in the system catalogs. Could then have a JSONB column to hold the 'rest' of the document. A view could then integrate the two at SELECT time, essentially decompressing the common bits from relational storage into JSONB and then merging in the non-uniform bits to project a single integrated JSONB column.

All that said, it sounds like you're going to have to make trade offs for read convenience / random access versus overall write compression. 

What have you looked into in terms of NoSQL storage engines more specifically tuned for storing blobs of JSON? It wouldn't surprise me if those might try to optimize for the real common case of a large volume of highly similar documents, namely the degenerate case of using NoSQL to store highly uniform (and therefore could have been done with a formal schema) JSON.

Sounds like you want name -> document lookup with high / shared dictionary compression across documents, but just read-mostly, not read-only as in squashfs? What is your document count / uncompressed and compressed sizes / scope?",1539967957.0
a5myth,"How about using the BTRFS filesystem with COW switched off. It supports compression. And you're probably aware of the JSONB data type on PostgreSQL.


I think if you implement the right features of both it could work well.",1539947603.0
liquidpele,"If you don't need any json functionality awareness, then it seems like you really just need a file store and not a database. ",1539951630.0
dbaderf,If I were looking at it in Oracle I would use the context engine to index the text for keyword searches and just store the text in a clob with securefile high compression. I haven't looked to see if Postgres supports either of those options though.,1539958625.0
six0seven,"When we deal with something like this we use Vertica (which is sometimes interchangeable with Redshift) but we do one particularly important trick which is to convert JSON to a key/value pair. So with three columns minimal, you can have a document index, a field name and a field value. Generally we will add to that a raw value and a value type depending on how much work we want to do in the conversion.

Nevertheless, a columnar database will smartly compress the keys (field names) to the absolute minimum and of course with duplicate values you'll get compression as well. This makes a very big difference when you get into the tens of millions of rows, which you easily will. Vertica is not cheap but you only pay based on data size. Below 1TB raw data it's free.

Vertica works exactly like Postgres because it basically is, but with a completely different physical model.",1540356710.0
Milnternal,Yeahhh don't do that... if you want to record change history look into **Slowly Changing Dimension** (also sometimes referred to as 'history tables') pattern to do this properly,1539948032.0
anras,"I wouldn't do that - I would update. If I must keep a historical record of past job titles or names I would either use a built-in database feature for that purpose, such as SQL Server's temporal tables, or come up with my own solution for that, possibly involving triggers that insert to a historical table on update/insert/delete.",1539918841.0
Howisthefam,"This might help you:

[https://docs.microsoft.com/en-us/sql/relational-databases/tables/temporal-tables?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/relational-databases/tables/temporal-tables?view=sql-server-2017)",1539969295.0
welshfargo,Use a surrogate key (e.g. account_id). Don't use as a key any field that can change or be duplicated. What if you have more than one person with the same name?,1540050805.0
boy_named_su,"high cardinality: q,w,e,r,t,y,u,i,o,p,a,s,d,f,g,h,j,k,l,z,x,c,v,b,n,m  <- most (or all) values are different

low cardinality: true, false, true, true, false, false, true, true, false, true, true, false, false, true, true, false, true, true, false, false, true <- most (or all) values are the same

now think about creating indexes for these columns:

index 1 {a:11, b: 23, ...}

index 2 {true: 1, 3, 4, 7, 8, 10, 11, 14, ..., false, 2, 5, 6, 9, 12, 13, ...}

In which index will it be faster to find the row you're looking for?",1539886130.0
186282_4,"The number of distinct values divided by the number of rows will give you the cardinality. The higher the cardinality, the more useful an index on that column is. ",1539932265.0
wolf2600,It just describes the number of unique values in that column.,1539900769.0
fresh1010,What would be an example of Transactions Supported in Stored Procedures?,1539881117.0
karafili,Really nice,1539912163.0
kum0nryu,"Another vote here for Postgres. It’s powerful, free, more robust than SQL Server in my opinion (having years of experience with both), has good speed, and plays nicely with development environments.

",1539844035.0
quentech,"[PostgreSQL](https://www.postgresql.org/) or [SQLite](https://www.sqlite.org/index.html). Microsoft SQL Server is great, too, but anything other than the [Express version](https://www.microsoft.com/en-us/sql-server/sql-server-editions-express) is not free of course.",1539839712.0
r0ck0,"postgres.  Free + and more features + robustness compared to mysql.
",1539839716.0
colly_wolly,I have used MySQL for ages and its a decent enough database but has plenty of quirks. Those start to get annoying once you have spent enough time with it.  Personally I would go for Postgres as it's also free and has a far batter reputation than MySQL. ,1539846745.0
a5myth,"If standalone to accompany a local app use SQLite.


If a bit more site wide or across networks use PostgreSQL.


Both are are free and work really well with no real quirks regarding how they work, they are cross platform and have no cost or licensing issues.",1539851047.0
strongdoctor,"What other requirements do you have? It's really unclear to me where the DB would be, how important the data is etc.

 Check out SQLite if that would be enough.",1539849088.0
alinroc,"Is this database going to be local to the computer running the application and single-user, or do you need it on a network with multiple users?",1539864666.0
SUCKITNORMIES,Just want to say thank you all so much for recommending PostgreSQL. I would of never thought of using it at all. So glad I didn’t go with MySQL. It’s all thanks to you awesome dudes for knowing the good stuff. Thanks again!,1540058047.0
cboxsolutions,"Do you want to ensure that your data is consistently organized and remains easily accessible? CBox Solutions got a research department which creates the list based from your preferred market criteria specifically for your target market. We disburse more or less a quarter of million dollars yearly to maintain and develop our database. Because the clients' segmentation and targeting requirements are increasing, we establish our own in-house research team for us to advance our data repository. The bulk of our prospect data comes from our personal data collection activities and we authenticate each record in our database on a regular basis. In fact, we make two million phone calls and send out six million emails. We consider all of these conversations as a consequential opportunity to reconfirm and re-profile the information we have. Also, we highly prioritize data quality which leads the prospects inquire in us for database maintenance. Truly, our data will be the most relevant and updated list available, and you will get it as a part of your campaign. Visit us at [cboxsolutions.com](https://cboxsolutions.com) for more info. ",1541579838.0
DesolationRobot,"Unless you define some unusual or specific needs any RDBMS would be a logical place to start. The difference between MySQL and postgres and SQLlite won't mean much. 

If it's an online app I think Amazon Aurora has a free tier. That'll get you up and running and scalable and if all your data uses are SQL standard then migrating to another RDB is easy--or at least doable depending on what level of downtime the app can tolerate. ",1539843336.0
r3pr0b8,"> looking for some clarification as to whether my diagram would allow the database to function correctly.

it's not the diagram that allows correct database functioning, it's the design of the tables

you need two tables -- an incident table, and a location table

all of the things you need to determine are then very easily produced with sql",1539782952.0
mkingsbu,I don't see the diagram - did you forget to provide the link?,1539784598.0
ACarmon02,[Schema Diagram](https://i.redd.it/la04akay6rs11.jpg),1539785509.0
zepplenzap,"Don't use simple db, aws is trying to sunset that service, it is in maintenance mode only. DynamoDB is what you should be looking at.",1539678931.0
revnext2,Look at AWS AppSync,1539750385.0
beyphy,"> none other than me know any SQL. Thankfully, SQL is pretty easy to learn.

It depends on what you mean by SQL. Do you just know DML? (SELECT, INSERT, UPDATE, DELETE)? Or do you know DDL and DCL as well?

Even if SQL isn't that difficult, good data practices is what make database design more complicated. Good database design requires planning in advance. It's harder to fix problems as they come up on the fly. So if you're not familiar with how databases work, you should really familiarize yourself with the concepts if you're planning on creating one.

> For this, the back end of the database is hosted on a network drive, and each user enters data on his/her own front end. This seems like a poor hack at best.

That's actually a pretty common way to share an Access database. Access databases used in this way can support up to 250 users IIRC.

Given all of your requirements, PostgreSQL is probably your best option. PostgreSQL isn't just a database like Access. It's also a *database server*. As a database server, it's best utilized on a server OS, like Ubuntu Server. If you don't have one of these, or don't have experience setting one up, it may be best to just use a service like AWS or Azure.

Setting up a database can be very difficult. Especially when you plan things like database integrity, back ups, when you have to think about things like [ACID](https://en.wikipedia.org/wiki/ACID_(computer_science)), etc. It can be very difficult, which is why it's usually done by IT departments. It much harder to make and properly manage a good database server than it is to develop and hack a quick Access database.

If that's what you plan on doing, and you're seriously interested in learning everything, then more power to you. But it can take a ton of time and be very difficult, and there may be limited resources online. I essentially set something like this up for the first time a few months back. I struggled, for example, to setup SSL certificates on PostgreSQL in an Ubuntu Server. It's something I spent several hours trying to do but was not able to successfully implement. So I just ended up using an SSH tool (KiTTY) that allowed you to minimize to tray so that the connection is encrypted, but the window is out of sight.

If you're interested in good resources, Digital Ocean has great guides for setting up an Ubuntu Server, installing and setting up postgresql, using SSH keys, etc.",1539637920.0
Dr_Legacy,"As long as everyone can use Windows, I would not make this change just for the sake of getting off of Access.  

",1539648069.0
ComicOzzy,I'd vote 100% for Postgres in your situation. ,1539654133.0
six0seven,"Use RDS Postgres on AWS. You'll never worry about backups or performance or any sort of maintenance. If you have that much time, spend it getting to know how to navigate around AWS. The migration itself, in capable hands, would take a couple days. Consider outsourcing the gig. ",1539655262.0
gnubyter,Hire a consultant and let them do the work for you.,1539724885.0
doctorzoom,"Consider an AWS solution like Redshift, or other RDS offering.
Pros:

* Super easy to set up
* The DBA work gets handled for you
* Automatic back-ups/snapshotting
* Easy (and practically limitless) scaling
* Highly secure (you can control ingress/egress points and permissions per resource)
* Your entire DB won't live on a single computer waiting for coffee to get spilled on it.

Once you're on AWS, you also have stuff like S3 for unstructured data, plus a ton of analytics, reporting, compute, webhosting and other doodads that all play nice with each other.",1539661980.0
mbmw,"Checkout influx data ecosystem.
",1539571891.0
DataDecay,"Timeseries data works well in both rdbms and nosql you just need to pick one and stick with it. If your going to go rdbms then you need to normalize it, what you think is fine now might not always be fine.

If your going to go nosql I would personally suggest a aggregate of readings per hour. Essentially each doc is a device pulling in data for each hour. Depending on your level of precision you create a matrix that accounts for all time in the hour (for instance 60x60 for seconds).

After you have your design you need to think about your otlp process. I personally keep all deviations and equations in memory with the application. I use the database for raw data (input parameters) storage and I compute on that data in my programs. I do not compute based off anything in the database other than the raw input or design logic.",1539571244.0
FatLungs,"I would recommend  influxdb which is made for time series data:
https://www.influxdata.com
There is a nice client api in python for it :
https://github.com/influxdata/influxdb-python
I would also recommend something like grafana for a dashboard visualization of your data if you're so inclined:

https://grafana.com

I literally just finished a project using all the above and it works great!",1539573997.0
boy_named_su,"If it's historical data, and you're going to use it for analysis, then wide format is exactly what you want. 

A data warehouse / analytic database uses fact table, which represent an event or process that you want to analyze. Dimension tables are just an optimization to save space",1539709732.0
da_chicken,"Can one blood drive cover more than one testing center?  If not, then I believe that Blood Drive ID makes Testing Center ID redundant.  Or potentially vice-versa depending on what your entities mean.

The only other possibility I see is if a testing center can only have one tester, or only one tester on a specific date, or only one tester for a specific blood drive on a specific date, etc.  Or, on the flip side, if multiple testers can participate in a single donation.

Both of these seem somewhat unlikely, but it somewhat depends on your business rule definitions. If I were developing a database for an application to be sold, I would probably go with this definition just to support other possible configurations.  Indeed, you might need to get even more complex if a customer needs to treat simultaneous donations as though they were different donations.  You might want to use a concept of a donation set in that case.

Thinking about it more, I guess I'm not quite sure I see what each entity represents.  Is the ""tester"" the person or device administering the donation, or is it a person or device running a test on the donation at a later date?
",1539530916.0
r0ck0,"I just use regular postgres.

Either have a single `vote` table that covers all types of things people can vote on, or one per type, i.e. `comment_vote`etc

redis isn't really permanent storage, it's a RAM cache.",1539478799.0
gatorsya,"If it wasn't a typo, I want to correct that, it's 'relational' databases.",1539481306.0
aviral1701,"rational database, boy that would be something!

&#x200B;",1539493456.0
btburnett3,"At CenterEdge Software we've been using Couchbase extensively since 2012, during which time it has provided an ever increasing percentage of our data storage infrastructure. In fact, most of our new cloud-based products are using pure Couchbase as the database backend. I would definitely not consider it a prototype. The biggest barrier to entry for most people is moving your mindset from a relational database to a NoSQL document store, but if you're already familiar with MongoDB that should be easy. And yes, N1QL is incredible and IMO the best NoSQL query mechanism around, bar none.

I will admit that the documentation is normally more reference oriented, and less how-to oriented. However, there are free online training resources available [https://training.couchbase.com/online](https://training.couchbase.com/online) and a very helpful community on [gitter.im](https://gitter.im) and the forums.",1539610505.0
exile1362,"I'm a Principal Architect at Nuance Communications. We went through the MongoDB vs Couchbase comparison a number of years ago and chose Couchbase as our primary NoSQL store. We've been in production with it for over 6 years, host billions of documents, and have rarely had a problem. Ease of use, performance and the feature set have all been outstanding and continue to get better. Echoing btburnett3, N1QL is an amazing tool that takes the SQL concepts for relational databases and applies it to JSON-based document stores. This is no easy feat and I will admit that in the first release it had some rough edges as the article author described. Since then, it has evolved tremendously and I find it to be intuitive and easy to use.

In some ways, I think the author may have also been comparing apples to oranges. The cbq utility is not designed to be the primary interface to Couchbase. They have rich client SDKs in a wide variety of languages that are far better suited to the task. For instance, creating a document in Couchbase using a N1QL insert statement is silly when you can just do a simple ""put"" via any of the SDKs or via the REST API.

In my opinion, Couchbase is a far more ""enterprise grade"" offering than MongoDB. I would definitely recommend that you give it a shot.",1539704143.0
ccb621,"The ""article"" is marketing for consulting services company. That's not necessarily a bd thing, but take the advice with a grain of salt. Get more sources of data. Why did you stop on that one article, and post a question?",1539445627.0
8601FTW,"Just a prototype?? Not even close.

I worked with Couchbase at my last company for about 4 years. We managed over a thousand servers with well over a hundred buckets for a wide variety of uses. As for the statement in the article, I wouldn't say it is entirely accurate. We had problems here or there, but we were never ""constantly fighting against Couchbase"". For the most part, it ran smooth and very fast with some great tools and some pretty solid API and data management tools. Couchbase has some top notch people working there that are very helpful and it seemed they honestly cared about any problems that I reported.

As for MongoDB, we ditched it shortly after it caused a major site outage for some egregious technical error in their implementation.",1539720300.0
GirthBrooks,Depending on your use case it may also make more sense to store a link to the file rather than the file itself.,1539380353.0
wolf2600,Use the CLOB (character large object) datatype.,1539377019.0
swenty,Use the TEXT data type. Values can be any size up to 1GB.,1539420338.0
lgastako,"Something like this:

    import os
    import sys

    def load_file(fn):
        print ""would load file: "", fn
        with open(fn) as f:
            rawBytes = f .read()
        print ""Read"", len(rawBytes), ""bytes""  # do your psycopg2 insert here instead

    def load_from(path):
        for fn in os.listdir(path):
            if not fn.endswith("".txt""):
                continue
            load_file(os.path.join(path, fn))

    def main():
        for path in sys.argv[1:]:
            load_from(path)

    if __name__ == ""__main__"":
        main()
",1539376848.0
wolf2600,"Create a column in your database as a CLOB datatype (character large object).  Then you can store text files to the field.


edit:  Apparently postgres doesn't support CLOB... use TEXT instead.",1539377195.0
berzemus,Xamp is alive?,1539370396.0
DJDarkViper,"Spin up a MySQL one-click droplet on DigitalOcean https://www.digitalocean.com/products/one-click-apps/mysql/

$5/mo for unlimited connections, 1ghz cpu, and 25gb of SSD, and a full TB of bandwidth 

It honestly doesn't get much cheaper than that without some big compromise. 

Also I can vouch that the MySQL instances support JSON, as I'm currently using modern JSON functions on mine. ",1539402170.0
isakdev,"Relational database?  
PostgresSQL  
MSSQL",1539358051.0
YZHSQA,I meant specific online cloud service provider like aws,1539358098.0
Hargbarglin,">> Current MariaDB version doesn't support mySQL

What are you even saying here?",1539370313.0
YZHSQA,"Sure. I’ll let u know when I do it in the weekend.
Thanks again!",1539519034.0
alinroc,">They suspect that a member of staff may be stealing from the drinks stock room

May I suggest a simpler, more reliable, and more effective solution? Just two words:

**Security. Cameras.**

Should the restaurant track inventory? Yep. And ideally that inventory would be tied into a full front of house + back of house system that takes care of *everything* - POS terminals, sales, inventory, order management, non-food supply management, reporting, staff scheduling, the whole nine yards.

But you have a people problem first and foremost here, and the system you describe can be gamed. You may find out that ""someone"" is stealing but no hard proof of who it is. You'll most likely write it with security holes - allowing data to be fudged. People will make honest mistakes and then get accused of messing with inventory or sales numbers.

The camera doesn't lie. Be above-board about it, tell everyone that cameras are being installed for everyone's safety and security, don't say ""we think someone's stealing from us"", and make sure they know that access to the cameras is secured to only the owners and general manager of the restaurant.",1539346434.0
s13ecre13t,"Do you track breakage / spillage? 

Do you track customers ordering and then sending drinks back? 

How busy is the place? It is faster to pour quickly overpour, than risk under delivering or being slow and exact. 

---

If theft is a concern then best is to install a video camera. How will your report guard against watering down vodka?",1539354054.0
hawk3ye,What do you want to know?  I was the lead tech support rep for that product (and almost all the other Embarcadero Products) up until a few years ago when I quit.,1539365306.0
mkingsbu,Technically you could save it as a BLOB but it isn't ideal. Usually you use a file system for saving files.,1539307451.0
Phnyx,"Either save each document in a nosql database or, if it doesn't need to be a database and you just want to have something faster than CSVs, use pickle/feather/similar file format with compression and itemized datatypes for everything.",1539290246.0
JamesTweet,Here is a search page that I use for this sort of question. [http://www.oehive.org/proogle.htm](http://www.oehive.org/proogle.htm),1539286861.0
rbobby,"    SELECT public.""ands"".""id"", result
    FROM lineagetree

You're asking for the id column from the public.ands table... but the from is lineagetree. You need to select columns from the lineagetree table or join from the lineagetree table to the public.ands table.

You probably want:

    SELECT id, result
    FROM lineagetree

",1539211897.0
chaita12," Try this both 

1. With out selecting the columns ,use this - select \* from lineagetree - it must work.
2. Second option - Alias even the first union base table .""ands"" as some sp, then try retreiving out.

if both the tables have same columns sructure and nomenclature , then you must be able to retreive . 

Keep me posted if any of the both worked out.

&#x200B;",1539695973.0
francisco-reyes,"That is fine. A few points to consider. Is the original table insert only? if not, how will you track updates? You could have a trigger and a last_updated field. How about deletions?

How much data are we talking about? Which version of postgres? Another potential way would be to just have a slave and do the reporting from the slave. Specially if you are using Postgres 10 you could have parallel sequential scans. Another way to work on this is to have materialized views of summary data.

If you give us more info, we can provide more feedback. I think the main points are:

* How much data

* Insert only table?

* If not insert only, is it insert / update only and no deletes?

* Version of postgres

* Is this a time-series data? if so have you looked at https://www.timescale.com ?",1539203382.0
six0seven,"My recommendation is that you dump out your records to a text file and then store them in S3. If you're not pushing hundreds of thousands of records a day it is NOT worth it to run Redshift. Make them vertical bar delimited for ease of use and use Server Side encryption (really easy) in S3. You can then run periodic queries directly against S3 with Athena. When you decide Athena is too slow, then you can import them to RDS Postgres. When you decide that RDS is too slow,THEN use Redshift. But it should take million record aggregations for that to happen, at least.   


We've got a Billion Row Demo against a typical Redshift 3 node cluster that runs in 7 seconds. That's when you need that kind of power. Don't buy a Ferrari to get your groceries.",1539391024.0
sHORTYWZ,"The error is stating that the public.larger table does not exist, not that the column doesn't exist.",1539194415.0
Dreamafter,"Are you certain that permissions to select from public.Larger are granted to the role/user performing the query? Because I've just created similar tables to yours and run the same select and it worked for me. 

Is the public.Larger.IID different than public.Larger.ID? Are you certain that your query should work?",1539195700.0
s13ecre13t,"""public.larger""  vs  ""public.Larger""

if you are using capital letters or other weird things (like spaces) in your table/column names, then please quote them.",1539195898.0
alinroc,"Honestly, I'd do none of the above if you don't need the data in relational tables the instant it's collected.

Dump the data into a document database that's optimized for writes; you've got a pretty simple data structure you can define in a JSON format.

Then have a separate process that vacuums up that data and inserts it into normalized relational tables, and either deletes or marks as ""imported"" (so it's not imported again) when complete.

Three relational tables:

1. Exchange
  * ExchangeID
  * Exchange Name
2. Company
  * CompanyID
  * Ticker
  * Exchange.ExchangeID
  * Name
3. Prices
  * PriceID
  * Company.CompanyID
  * Price
  * Timestamp

`Exchange` will be updated rarely, `Company` less rarely. `Prices` will be write-heavy so design & index appropriately, not to mention setting up foreign key constraints.

In reality, hundreds of inserts per minute is pretty trivial even for MySQL if you've designed things (including doing a bulk import vs. 200 individual `insert` statements, for example) properly and spec'd sufficient hardware, so that intermediate document DB may not even be necessary (you haven't done a prototype yet, so you don't know if your system can handle this load). Creating lots of identical (in schema) tables on the fly is *not* designing properly.",1539179600.0
DesolationRobot,"""100s a minute"" isn't exactly taxing.

Will it maintain historical data or only the current exchange-ticker-price combo?

If the latter just have one table with a unique key on exchange+ticker and do ""replace into..."" Instead of ""insert into..."". You'd have to have a massive amount of exchanges and tickers before that became a problem. Like possibly higher than the number of exchange-tickers that exist in the world. 

If queries will always include an exchange id you could look into partitioning the table. It's effectively like making a new table for each exchange. But speed gains would likely be minimal.",1539184104.0
Seeteuf3l,"Adding Access form into a website or to Sharepoint doesn't seem to be very complicated at least.
https://support.office.com/en-us/article/build-an-access-database-to-share-on-the-web-cca08e35-8e51-45ce-9269-8942b0deab26

How to do it with MySQL and PHP:
https://www.taniarascia.com/create-a-simple-database-app-connecting-to-mysql-with-php/

Express and MongoDB:
https://zellwk.com/blog/crud-express-mongodb/",1539159158.0
mkingsbu,"I'm not sure what you mean when you say that session_id can be duplicated.  I'm assuming you mean that user1 can have session_id = 1and user2 can also have their own sesssion_id = 1.  If that is the case, then your design should be fine because you are correct, the composite key is session,user, and game _id and that would uniquely identify what you're seeking as far as I can tell.",1539100316.0
,[deleted],1539108090.0
mkingsbu,"It sounds like you're trying to recreate the filter functionality. Any reason you can't use a filter?

https://support.office.com/en-ie/article/apply-a-filter-to-view-select-records-in-an-access-database-2a493ded-e544-4144-9103-b9b1d1865147",1539092809.0
grauenwolf,"SQL Server Express

PostgreSQL ",1539056750.0
assface,"* Postgres
* MySQL v8
* SQLite
* Firebird
* VoltDB",1539062753.0
greenman,MariaDB has had CTEs since 10.2,1539075989.0
b0ggl3,"Try a property graph database instead, like Neo4j, they have been built for these types of queries",1539066191.0
Jejernig,"This is the exercise that I do every time I want to normalize a database. 

1) I open excel and put all my columns into the sheet.
2) I fill out a few rows with fake data. 
3) I find repeating data and then break them off into a different sheet (table). 

This kinda shows me what’s repeating and what can be normalized. ",1539051301.0
kelenfenrisson,"You have the entities sorted, but can you answer this ? :

- How do you know the payment  ""from"" account ?

- How do you know the payment ""to"" account ?

- How do you know the account balance ?


Try to write the requests and it should ring a bell.

Edit: clarification & markdown.",1539050775.0
yaseenramzan,"The process of producing a simpler and more reliable database structure is called normalization It is used to create a suitable set of relations for storing data.This process work through different stages known as normal forms. These stages are 

1. 1NF (First Normal Form)
2. 2NF (Second Normal Form)
3. 3NF (Third Normal Form)
4. 4NF (Fourth Normal Form)
5. 5NF (Fifth Normal Form)
6. BCNF (Boyce-Codd Normal Form)

If You have any problem related to database normalization read out this article

[https://www.queryhelp.com/2018/10/database-normalization-1nf-2nf-3nf-4nf-5nf-bcnf.html](https://www.queryhelp.com/2018/10/database-normalization-1nf-2nf-3nf-4nf-5nf-bcnf.html)",1539714175.0
welshfargo,"http://graphdatamodeling.com/resources/rettigNormalizationPoster.pdf
",1539107513.0
r0ck0,"Sounds like you're new to this, so regardless of what you use, there's going to be some learning involved.

Just learn postgres or mariadb.  

NoSQL has it's place... rarely... for a limited portion of data... but for like 95%+ of projects SQL is much more suitable as the one-and-only primary data store.  

The people that think nosql is suitable as the one-and-only database in common typical projects... mostly just never bothered to learn SQL properly to begin with.  Then once their data gets more complicated they realise it's a huge mess of application code that could have instead been done in a few simple SQL VIEWs/queries.  

I'm yet to ever hear of a decent example where data is completely non-relational and makes sense to use a nosql db as the one-and-only database on a project.",1539056325.0
popeus,"Nosql is comparatively quite slow for reading data when compared to a rdbms. Part of a url shortener is expanding urls when passed a code in the form of a shortened url. A regular rdbms will exceed at this.

A regular rdbms will also allow partitioning. Say you have a billion urls, you could partition them into a yearly partitions as it's unlikely years old urls will be requeried.

Regarding relationships. What if you allow users to create an account to keep track of their old urls? That's a relationship between a user table and the main url table. Don't discount relationships as 'old tech'.",1539036790.0
SearchAtlantis,"Your health information table will be a mess.

Consider: your weight, and BP are going to change some amount randomly at every measurement. OpHx, CD, Rx also are (relatively) constantly in flux. 

What range of values do these fields take? Are they only the most recent result? Will you keep history? Are they free text (vchar) fields?

That table alone addresses something like 40% of full blown EHR system.

My advice to you is to constrain, constrain, constrain, and document these constraints. Eg an idiopathic autoimmune disease may be a clinically irrelevant chronic condition. Similarly a chronic psychiatric disorder. 

Also consider who is using the information when you talk about these constraints and use cases.",1539016557.0
welshfargo,"Why do you have separate entities for Staff, Nurse, and Blood Tester? Aren't they all Staff?",1539027853.0
SearchAtlantis,"Also why do you have a blood type table? You should store that value in the donation table and donor table.

Single staff table as another poster mentioned. 

Are you sure you need to track inventory? Typically donation groups are going to transfer to an actual blood bank which would simplify a great deal on the inventory side.

One other comment: what scale use are we talking? While normalized tables are better from a scaling perspective (and [imho] somewhat integrity) the trade off is analytic complexity and in some cases rigidity. Get a clinician or MLS to make sure you've covered every blood type, and remember that while free text fields are nice from a flexibility perspective the inputs need to be validated by either the database or a frontend application.


Sample queries for your consideration:

What is our current inventory of each blood type?

How much of each type did we use (inventory --) last month?

What are our rejection numbers per drive or site?

What is the average unit donations per nurse? Are there any outliers?

How many of each blood type did we get for a given donation drive?

How many donations of each type for a given donation site in the last year?",1539088649.0
wolf2600,What do you want to be able to manage with it?  Just monitoring CPU/mem/disk space?  Or do you want to be able to run queries on the DB?,1539017273.0
alinroc,What RDBMS?,1539019826.0
hipster_dog,I've used [RemoDB SQL Client](https://play.google.com/store/apps/details?id=com.kriskast.remotedb) and it's pretty solid,1539088108.0
tpsykes,I use postgres/postgis. ,1538925959.0
alinroc,"All the major RDBMSs (SQL Server, MySQL, Postgres, Oracle) have GIS & spatial functions, either in the default install or as installable extensions. Use whichever you already have in your shop or you have experience with.",1538933015.0
quentech,"While not a durable store, Redis also has some GIS functionality, including querying by distance using GeoHash.",1538937971.0
chaz6,You might find Google's S2 [http://s2geometry.io/] library useful as it was designed specifically for doing this kind of query.,1538983437.0
mercyandgrace,"Are you familiar with joins? The first 1st question requires you to join the Pokemon and Trainer relations, then use the WHERE clause to filter for the results you need (Bug and Grass).",1538926186.0
YourSourcecode,"I'd recommend you to have a look at the definitions of the operations.
[Wikipedia](https://en.m.wikipedia.org/wiki/Relational_algebra) might be a good start.
You might want to have a deeper look st selection and projection. That really helped me in the finale database exam. ",1538940394.0
HildartheDorf,"What kind of plugin are we talking?

Ye Olde native plugins: sqlite.

Modern client-side plugins: You'll need to use something browser based like sessinStorage.

If it's a browser plugin that calls out to your webserver, then I would say whatever is best integrated with your hosting platform and that you know how to use. If in complete doubt, sqlite is a good choice, but not the best in anything particular.",1538817415.0
boy_named_su,Sqlite for sure. Most apps run it,1538804011.0
svtr,Load as in Amazon .... PipeDream DB would be perfect.,1538874779.0
jahayhurst,Check out CockroachDb.,1538762915.0
six0seven,"Honestly, this sounds like an artifact of the pricing model of Snowflake. In Vertica, you pay for the raw uncompressed loaded data, period. You don't pay for seats. You don't pay for K safety. You don't pay for storage the database decides it needs to serve more users or materialized views. I'd talk to sales engineers to get the details, but I don't see the value of this feature where multiple users couldn't just access views to which they are granted read-only access and that's the end of it. ",1540357626.0
mkingsbu,"That depends on what the relationship is.  If you have three subclasses for which there can only be one relation to the parent respectively (as in, you have an ANIMAL table and then DOG, CAT, and BIRD as subtables; a BIRD can be related to an AnimalID say in the ANIMAL table but only to one. In this case, there is no reason to have the BirdID in the Animal table because a BIRD can only have one AnimalID).  Forgive the clumsy example, while I have had my morning coffee, it is still kicking in.

Lets say on the other hand you have a many-to-many relationship.  So a APARTMENT and TENANT table.  Over time, an apartment may be rented to many people and a tenant can live in many apartments.  In this case, you'd have an intermediary table such as APARTMENT_TENENT that has both of the primary keys of each table, which are in turn both foreign keys to their respective tables.

In other words, yes, only the child table might know the connection.  So when you are querying it, you would either start there or join to that table to ensure that your query was taking into consideration that requirement.",1538748673.0
r3pr0b8,">  wouldn't only the child table know the connection, not the parent? 

interesting way of putting it, but you are absolutely right",1538750950.0
grauenwolf,"I see nothing wrong with that. As for improvement, I can't say. If the queries were already fast, they won't get any faster. But they may free up RAM/CPU on the database for other queries that can't be cached.",1538761222.0
mkingsbu,"I haven't had my coffee yet so this might not be what you're intending, but why not keep the data in the format you originally indicated and then make a view that is takes your row_number() as a CTE and then left join it to the user/product id.

So:

    with t1 as (
       select * , row_number()... blah blah blah)
    select t2.userid, t2.productid, t1.renew_n, t2.renewdate
    from whatever_that_table_is
    left join t1 on t1.userid = t2.userid and t1.productid = t2.productid",1538739709.0
DesolationRobot,"Add a primary key auto incrementing ID to your table. Then it becomes a normal ranking query.

Nth per user:

    select a.*, count(*) as purchase_number
    from subscription a
    inner join subscription b on a.userId = b.userId and b.renew_date <= a.renew_date
    group by a.id;

Nth per user/product:

    SELECT a.*, COUNT(*) AS purchase_number
    FROM joe.temp_purchases a
    INNER JOIN joe.temp_purchases b ON a.userId = b.userId AND a.prodId = b.prodId AND b.renew_date <= a.renew_date
    GROUP BY a.id;

Then you can use that in a subquery to, say, cherry-pick a user's Nth purchase.

You could also embed the purchase number as a column in the table and populate it with a trigger that would check on insert and set the value to the previous count plus 1. That solution is more fragile if records can be deleted from this table or otherwise removed from consideration (e.g. a refund).

EDIT: you *can* do it without the unique-per-row ID number by changing the group by to

`GROUP BY a.userId, a.prodId, a.renew_date` assuming that no two rows could share that combination. (You could enforce that with a unique index--but at that point why not just add an explicit ID number?)",1538759411.0
rbobby,"Meh. Suck it up buttercup. This is life in the corporate world. Sure it sucks, but some things you can fight and some things you can't... and this sure looks like a ""can't"". You need a strategy to minimize the the pain in the ass factors (i.e. those users are not going away, and if you make a big enough stink over seemingly trivial issues then **you** become the problem).

> calling/emailing our DBA

Time for an issue tracker. Requests need visibility and time for appropriate analysis (impact and cost). 

Once you can start putting costs on requests (changing the structure in such and such a way will cost $x) some requests will just disappear. The remaining requests would be ones your users (sure they're in another department... but they're still your users) actually need.

If there are sufficient requests being made then your manager/group/department may have a case for an additional hire. But without some hard details (mandays of effort within dept, mandays of effort outside of dept) it will be more difficult to get budget approval.

Good luck!",1538735708.0
dbaderf,"Not knowing the corporate dynamics I'll still venture to say that I would respond with, ""We don't have the resources to deal with changes requested by users outside of our application.""

What will happen when your customer requests changes that will break their infrastructure?",1538703610.0
wolf2600,"Their department didn't pay to build support it, I say you tell them that they can use it as it is or go build their own.  But that they may not make any schema changes.

Or if they do want to make changes, they must submit an official change request, have it review by your department, and any changes which will introduce regressions will be refused.",1538702552.0
mabhatter,"I’m kinda with your boss to tell them to pack up. Obviously the expensive corporate data warehouse didn’t work for them, but it’s not YOUR problem if the thing you made doesn’t work the same way. 

The more diplomatic way might be to document what they actually need and adjust your database or create indexes and views to accommodate them. But yeah, somebody found a “new toy” and they’re wasting your guy’s time. ",1538735947.0
honghai22081986,I think you do not need it,1538713871.0
SpeakerEnder1,Any advice would be appreciated.  I'm trying to setup a database that will take user input in the test point and observed text boxes and then display a percentage based on comparing them in the difference % box.  I have found this unusually difficult to do in Libreoffice Base.  It there a database program that makes real time math between fields easy to update and display?,1538694814.0
AaronPDX,"If you don't have a DB already and need both DB and UI, MS Access isn't a bad solution.

If you already have a DB solution and are just looking to build a UI, MS PowerApps is pretty simple drag-n-drop for making data access applications.",1538700067.0
assface,"Switch to Postgres:

`BEGIN TRANSACTION ISOLATION LEVEL SERIALIZABLE;`

`SELECT * FROM urls WHERE url = 'XXX';`

`INSERT INTO urls VALUES (...);`

`COMMIT;`",1538693875.0
thejumpingmouse,Look up isolation levels for nosql ,1538693133.0
wolf2600,"Your application should only have a single process that does the query and the insert.  The user input portion can be multithreaded, but all those threads should submit their requests to single process's queue.  That process will then handle the queue in order, checking on a single URL, inserting if it's available, then returning the result to the requesting thread before moving on to the next request.",1538694541.0
ToAskMoreQuestions,"Also, a unique key / constraint will keep this in check, even if you don’t have Postgres-level control of the threading. 

Mongo... just... ugh. You have a well-defined schema. Why?",1538701558.0
mich4elp,"One thing to consider also is if it really matters if this happens. The chances of two users shortening the same URL at the same time are low, and if they do, what happens? Assuming that when someone uses a shortened URL, it's basically just a lookup in your database, if you have short\_url\_1 -> long\_url and short\_url\_2 -> long\_url, things will still work.",1538708724.0
welshfargo,"WTF are you trying to reproduce something that already exists? Like tinyurl, bit.ly, etc. ? BTW, the database is irrelevant.",1538793641.0
OcotilloWells,"If you make the URL that is to be shortened a unique indexed field in an RDBMS, one user would ""win"", and the others would get not be able to get their shortened URL created.  Or you just check for this, if an error is thrown because it is a duplicate, then it is already there and just send the user the already created shortened URL, and they won't even know.  If you aren't worried about creating superfluous shortened URLs, that could also be the ""Check if it has already been done"" part of it (if that is going to be a required part of your user interface); it will either create one on the spot or give the user the  already existing shortened URL, though the users might be surprised to see that there is always one already created.  You might get some that will never get used, if users just want to see if one exists without actually creating one, but unless your application will be massive and users are doing a lot of checking it shouldn't be an issue.

I don't know your use case, but if you were to give different users different shortened URLs, it would allow you to track not only how often that URL is being used, but how often the shortened URL created by specific users is being used, which might be useful information to you.",1539039761.0
alinroc,"You're not looking for a database *per se*, you're looking for a project management and/or CRM system. These just happen to have databases behind them. But you shouldn't be shopping for ""a database"" - you're shopping for a system that solves your business problem. Don't think about the technical implementation details and operate in terms of ""this is the problem we need to solve.""

Unless your business is building these things (which it is not), buy, don't build. Basecamp, Salesforce, Zoho, maybe even something like SmartSheets or ServiceNow are worth a look.",1538681604.0
changeupcharley,"Tableau, Access, Excel, all your friend.",1538680593.0
six0seven,"OK. There's several players they didn't select. #1 in terms of in-memory realtime databases, they didn't list **VoltDB**. Volt kills. We cannot put enough data into Volt to slow it down, and there are still community editions available. #2 They didn't mention **Vertica**, a very mature database that scales way better than Redshift for multiuser applications. (Like when you need to have 1000s of concurrent users or way more than that, think Farmville). #3 They didn't mention **Levyx**, who is killing right now in the FPGA arena.

So yeah, it's almost impressive if you don't know the other dragonslayers in the market.",1539391522.0
codemagic,"A data warehouse (also a database by the way) is built for asking reporting-relevant questions from. As long as you are not building an operational data store to run day-to-day business out of, you are likely wanting to store information at some kind of summarized level. In the data modeling world we would refer to that as a dimensional model (talking about the traditional star schema dimensions). You have some number of facts  or measures (quantities, amounts, etc.) that you will slice & dice based on some number of dimensions. Knowing the set of use cases you can model your data to best answer those questions. The ETL needed is what you have to do to load & transform the data from its transactional source structure to the ideal reporting structure.",1538610474.0
KDavid12,"The data warehouse is created around a data model that enables analytical loads. You can create a data warehouse using a 3NF normalized design, but this is not the best alternative for typical requirements in BI environments. In other words, a data warehouse is built with the same SQL tables, relationships and constraints that are used to build transactional systems, but those components are interconnected in a different way. If you want to investigate more about these analytical data models, you can Google terms like  Kimball, Immon and logical data warehouse. Now, if you are going to use this information to support decision-making processes (analytical loads) you will need to clean, conform and verify this data before storing it into the data warehouse. In summary, with ETL processes you want to avoid the garbage in=garbage out law. ",1538606077.0
elyuma,"Better performance and less space. 
Most of the fact tables are key based related to a dimension. Instead having 1000 rows with a repeat word, you change it to a INT with a key number from a dimension where that word is only one row. 

Also sometime one dimension can be used with multiple fact tables. ",1538608624.0
boy_named_su,"Tables in an operational database, like in a CRM, are tall and narrow. You design them this way to avoid update anomalies.  

Tables in a data warehouse are tall and wide. You design them this way to ease analysis, ie not requiring any deep joins

Data warehouses often contain data from multiple sources,  which is one of the factors which necessitates  ETL",1538633184.0
superwormy,"I think we'd need more information here to give a really informed opinion. 

What type of database is it you're pulling data from? 

What is ""very large"". Is that 1000 rows? Or 100,000,000 rows? 

Is it relational data, or better represented as key-value? ",1538583014.0
mkingsbu,"I've used SQLite for this type of thing before and it worked fine.  Depends on the scale of the data, of course.  Normally I use Python to orchestrate everything. E.g. it pulled the data in to a memory database, did the manipulations, and then when it finished executing, database disappeared.  The big problem is that even pulling into memory, you are IO bound based on the read speed of the source database. So building the application out the first time, I'd suggest getting some faux data that has the same architecture or using a 'limit 1000' or top 1000 or something from the source tables so you aren't constantly waiting for a long query to run to figure out how you want the application to do all that it needs to before the memory db goes away.",1538580852.0
assface,"What performance issues are you hitting now?

You may be prematurely optimizing by going with an in-memory DBMS. Have you tried Postgres or MySQL?",1538587828.0
siscia,"Please wait!

I build a ""database"" that is a perfect fit for this.

Please have a look into https://RediSQL.com

It is a Redis modules that embed a SQLite database. You can have the database both in memory or written on disk.

It basically provides disposable databases, you can create them, make copy, destroy them and of course query them as easily as a Redis command.

From your use case you definitely don't need the PRO version and I promise it won't let you down.

I will leave you with the documentation here: http://redbeardlab.tech/rediSQL/references/

If necessary I will help you to set it up and with how to use at its full potential.

Cheers",1538589612.0
in3465,"Sorry wrong address 
Books are at ualearningg.jimdo.com/data-structures/",1538579390.0
mwdb,"You have a requirement, not mentioned in your post but mentioned in your leetcode link, to keep only the smallest ID among the duplicates. The first query does that, the second one does the reverse. Imagine what your joined data looks like as an imaginary table (without the where filter applied) :

    +-------+-------------+-------+-------------+
    | t1.ID | t1.Customer | t2.ID | t2.Customer |
    +-------+-------------+-------+-------------+
    |   1   |    john     |   1   |    john     |
    |   1   |    john     |   3   |    john     |
    |   3   |    john     |   1   |    john     |
    |   3   |    john     |   3   |    john     |
    |   2   |     bob     |   2   |    bob      |
    +-------+-------------+-------+-------------+

Now you apply the where clause ""t2.id > t1.id"" and you're left with only the following:

    +-------+-------------+-------+-------------+
    | t1.ID | t1.Customer | t2.ID | t2.Customer |
    +-------+-------------+-------+-------------+
    |   1   |    john     |   3   |    john     |
    +-------+-------------+-------+-------------+

So, we've zeroed in on the joined row to delete, but do you delete the data from t1 or t2? Remember, you have to **keep** the row with the **smallest** id, which means **deleting** the **larger** one. We can see above that t2 is the larger one (because we looked up by ""t2.id > t1.id"") So you should delete from t2, leaving your table as follows:

    +----+----------+
    | ID | Customer |
    +----+----------+
    |  1 |   john   |
    |  2 |    bob   |
    +----+----------+

Deleting from t1 gives you the wrong answer:

    +----+----------+
    | ID | Customer |
    +----+----------+
    |  2 |    bob   |
    |  3 |   john   |
    +----+----------+

&#x200B;",1538597197.0
msiekkinen,Is your example schema with a column called Customer supposed to be what you're referencing as Email in the queries?,1538541141.0
,[deleted],1538545017.0
wolf2600,I've never seen a projection used in a delete statement before.,1538570840.0
shooshrooms,"Attributes are the columns in a table. For example: first name, last name, DoB. These are all attributes of a relation, otherwise known as a table. 

Basically what they want is a table with column headers visible. ",1538536033.0
wurkns,"If you want a decentralised database, there is [solid](https://solid.inrupt.com/how-it-works) by [Tim Berners-Lee](https://medium.com/@timberners_lee/one-small-step-for-the-web-87f92217d085)",1538505921.0
wolf2600,"Except there is no legal purposes where the ""decentralized web"" (ie: blockchain) is superior to the current solution.  ",1538499985.0
chocotaco1981,"sounds like contract work / consulting is in your future, sounds like it fits your personality
",1538422976.0
mkingsbu,Sounds like consulting might be up your alley.  Shorter bursts of difficult tasks then onto the next project.,1538415846.0
jenkstom,Way too many things about MSSQL Server are annoying.,1538408735.0
colly_wolly,Why not use a relational database in the first place? ,1538408623.0
mkingsbu,"DBAs actually do less work with data than you might expect.  I did very light querying when I was a DBA; the SQL that I *did* use was used to control the database, not the data generally. File systems, tablespace areas, etc.  If you don't have any Linux experience, I'd highly recommend learning the basics of Bash and installing databases on those machines.  (Unless you want to specifically work on SQL Server only, in which case I'd recommend looking for SQL Server materials specifically).

If you can install Oracle, PGSql, and/or MySQL on an Ubuntu install, that will go a long way. Basically get to the point where you can do basic tasks in them. Login, create users, update passwords, make tables, etc.  They all have slight differences that are worth knowing in an interview too. E.g. You log into Oracle with SQLPlus and can use a single bash command that's something like:

    sqlplus / as sysdba

(Its been awhile since I've done Oracle DBA stuff).  

In general, DBAs are in dev ops, so you'll want to focus your studying on those types of tasks if you aren't familiar with them. It isn't a development job and while it helps to have that skill-set to some degree they aren't that closely related other than the sense they both utilize the same tools.  That said, you might have to do optimization and so it helps to know why a database might be slow - design, poor indexes, network bottlenecks, etc. So it helps to have built things before and know why they break or why something might not be working.

E.g. just the difference between inserting a single row vs. many rows can be complicated and require fairy detailed knowledge (https://stackoverflow.com/questions/12206600/how-to-speed-up-insertion-performance-in-postgresql)

So if a developer comes to you and they're working on a PGSql database you would be benefited by knowing that there are certain ways it handles bulk inserts that are better than others for certain tasks.",1538399637.0
HeyzeusHChrist,"I love that there is this much enthusiasm for dba work in a person just finishing school.  You need to get some perspective as to what dba work is actually like.  Tons of graduates in data science are doing ""machine learning"" and predictive analysis but while that sounds/looks/is cool, it's not the bread and butter of actual DBA work.  Learn linux, learn how to install and configure databases, learn the basics of SQL performance tuning (what is an index? why wouldn't we just index every column in the db?).  Learn how to think about organizational data size and movement.  Understand what DevOps is, learn some basic automation.  After you get all that, you can move on to the nosql and the other newer stuff, but I think that stuff is best understood after you've been using a traditional RDBMS for a while and understanding the limitations.

If you have the skills that /u/mkingsbu mentioned here, I'll hire you right away.  ",1538403153.0
MadRhetoric04,Thank you for all the comments on this thread. I’m already a DBA as I was pulled off a help desk to learn sql and function as a sys admin of sorts at a smaller company and I didn’t know I needed to learn some of this Linux stuff. I gots more resources to look up and learn now. Thanks!,1538414890.0
mkingsbu,"Well, the problem is that there isn't a concrete answer to the question as it ultimately boils down to: When it costs more (or when you reasonably anticipate the costs to be higher down the road) to be normalized then denormalized, then you should do one over the other.  That is going to vary wildly depending on the context!

For example, it could depend on scale.  If you're a single developer and the application is straightforward in design, denormalization can be done more easily than if its a complicated application that requires substantial changes in the architecture.

On the other hand, if you index the table appropriately, you might find that you don't need to denormalize.  In other cases, materialized views can solve many issues. That isn't technically denormalization as far as I'd consider it but you could think of it in that way I suppose.  The idea basically is that if you have a query that is frequently performed but the data aren't updated but once a day at 6AM, a materialized view is good because you can run it the query and cache it so that anyone who needs that result can quickly access it without actually fully executing the query at that point in time.",1538365104.0
wolf2600,It all depends on how you're using the data and which design would provide the data the users require at the best performance.,1538404960.0
oldlibmike,"You don't discuss the architecture of your warehouse. Are you considering a dimensional design? DataVault? Some hybrid-Inmon structure? To some degree most warehouses are normalized - usually to 2nd normal form for dimensions - rarely  beyond that. The differences are to what degree and whether you create flattened or snowflake dimensions. You are not trying to maximize concurrency and data integrity so much as you are trying to maximize reporting performance and more importantly, making querying somewhat simple. If you duplicate data, but populate it in the same job, you are probably fine. If you are creating facts/dimensions and then summarizing into aggregates and then more and more of the same, it could become a maintenance burden. With modern big data databases (Snowflake, Redshift, BigQuery, etc) you have other options. You can use VIEWS to implement your aggregates instead of duplicating/summarizing data for example and these large engines can handle it. I would worry less about the ""slowing down"" aspect of normalized tables and joins and worry more about adding complexity to your database design. Star Schema is still a pretty viable and simple way to present data for reporting. If you are using columnar storage engines, ""joining tables"" isn't much different than selecting rows of data since the data isn't stored in rows and has to be combined together in either case.",1538439977.0
HeyzeusHChrist,Kimball's books are a good place to start.  I think it's called the data warehouse toolkit,1538336845.0
acialjonny,"Ditto! I just came here to make essentially the same post.

I’m starting to manage a lot of data and have just began learning about macros and VBA for data management in excel. I’m looking at building a Product database, but don’t know where to start or what needs to be considered in the design and layout of the data. Figured this could be a good place to get some help",1538317587.0
DJ_Laaal,Here you go: [https://docs.microsoft.com/en-us/sql/samples/wide-world-importers-what-is?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/samples/wide-world-importers-what-is?view=sql-server-2017),1538272642.0
release-object,Have you tested 1? Millions of rows isn’t that much for modern dbs. There are some great fake data generators out there you could use to test at scale. Creating lots of tables will become a pain to manage. Recommend you avoid this. ,1538215994.0
release-object,"Assuming we’re talking about a [relational database](https://en.wikipedia.org/wiki/Relational_database_management_system) then these kinds of relationships are their party piece. 

Sorting the data will mostly likely be more expensive than retrieving it. 

As long as you have enough cores and RAM to service the numbers of requests you receive a well designed table and indexes will get the job done. 

**Edit**

Mean to add this point:

What happens when you release ‘Comments v2’ that requires an additional field? Do you want to update one table or thousands?",1538233400.0
mkingsbu,"I prefer to have a many-to-many relationship in this case myself (an intermediary table with a Comment_ID and a Topic_ID). That way if the thread or the comment are edited, you can keep versions of them instead of issuing an alter statement.  Doing an update is a lot slower than an insert (update is essentially a cascading delete statement followed by an insert so it should basically never be faster than a simple update).  If you wish to retain versions after a period of time for example, you can set the database to issue the delete statements during downtime.

If you then index the primary key columns, it should make the reading much faster.

You could also partition the table by subject area so that if you had say, 10 types of content areas, each one would be its own portion of the storage space. How you go about this would vary depending on the RDBMS you use.",1538219022.0
mohelgamal,"If you are to use a SQL solution, then go with a table for topics and one table for comments. Most Databases will not allow millions of tables but you can easily have millions of rows. 

However, A NoSQL database could be a better fit, something like mongo or a graph database like neo4j. Where you can have topic nodes, comment nodes, and even user nodes. And keep a good track of who wrote what and what comment goes to which topic. ",1538224642.0
wolf2600,"One question:  What is the DB you're using for this project?  Is it a traditional RDBMS (like Oracle, SQL Server, Postgres)?  Or is it a Cloud data store (Amazon AWS, Google Cloud)?

Traditional vs. Cloud would impact how you design your schema.",1538221153.0
Nukken,"Create a topic table with a creation date column. Then one comment table per year and relate it to the topicID and year in the topic creation date. Add 50 or so comment tables to cover the next 50 years. 

It's not fantastic, but i'll work unless you anticipate many millions of comments per year.",1538232467.0
rbobby,"Storing a tree structure (eg.. parent -> children -> grandchildren) in a relational database is tricky.

The book [Joe Celko's Trees and Hierarchies in SQL for Smarties](https://www.amazon.com/Hierarchies-Smarties-Kaufmann-Management-Systems/dp/0123877334) contains a variety of detailed solutions (plus good discussions of pros and cons of each).",1538214279.0
dbaderf,"I think that whatever notation you use will be accepted as long as it's applied correctly. If you focus on correctly applying concepts, the notation doesn't mean much.

Good Luck!",1538173975.0
welshfargo,No one outside academia uses Chen notation. Crows feet is much more widely used and is much cleaner and compact.,1538177184.0
JobsHelperBot,"*beep beep* Hi, I'm JobsHelperBot, your friendly neighborhood jobs helper bot! My job in life is to help you with your job search but I'm just 376.0 days old and I'm still learning, so please tell me if I screw up. *boop*

It looks like you're asking about interview advice. I'm ~72% sure of this; let me know if I'm wrong!

Have you checked out CollegeGrad, HuffPo, LiveCareer, etc.? They've got some great resources:

* https://collegegrad.com/jobsearch/mastering-the-interview/the-eight-types-of-interview-questions
* http://www.huffingtonpost.com/rana-campbell/10-ways-to-rock-your-next_b_5698793.html
* https://www.livecareer.com/quintessential/job-interview-tips
* http://people.com/celebrity/worst-job-interview-answers-askreddit-answers/
* https://www.thebalance.com/questions-to-ask-in-a-job-interview-2061205",1538175068.0
msiekkinen,Do you happen to have a link to the job description from the company you're applying to?  I think that would be useful in getting a feel for what kind of interview you're up against. ,1538172988.0
mkingsbu,I'm not aware of any Excel tool that would allow you to do this but a Python script could be built that would essentially loop through the various tables giving you the distinct values of each column in each table.  This would vary by the RDBMS used though. ,1538151242.0
welshfargo,"Sounds like SAP. If so, [this](http://www.system-overload.org/sap/tables.html) might help.",1538177361.0
nowrongturns,"I think your best bet is to query the information schema or system catalogue for tables with column names like the name of the column you think it is.

So what I would do is list out all the things the column would likely be called and then find equivalent names in German and try the different combinations.

Another method would be to import data from all the tables into excel worksheets/books or csv files and then write a script with whatever scripting language is handy to search for the value you are looking for in all the files. Of course you could do this manually as well by opening each file and searching through it but that can be very tedious.

",1538198734.0
APoetNamedNewman,"No.
 The luxury of sqlite is  that you don't need a server database at all. Just use a sqlite database on the users phone. That's why people suggested you use it for Android. 

If you then need to store some things off of the device then you will need to use a server database available to you.",1538139575.0
cgfoss,"Sqlite is its own,  as you say lightweight, database engine.  There is a difference between which client software is used to connect to various database services.  Oracle, mysql, and sqlserver are different engines and sqlite is in that same engine category.",1538139525.0
-IoI-,"SQLite is a database with a small footprint that's ideal to be packaged and run up with other applications as a database local entirely to the one application.

If you were using MySQL/MSSQL for Android development, it wouldn't be hosted on the phone and would only affect the performance positively as they need to be hosted on a dedicated server and therefore offset the processing required by the client device. 

For any uni project, SQLite will cover your needs fine. All you really get from a full server is scalability, programmability and reliable scheduling.",1538140579.0
goblando,"Well, you can kind of code inheritance with views, but that probably won't be the best performer.

First I would create a product table, it would contain basic information about a product that could be for sale, you want to keep this high level and try not to create extra columns.:

Prodid, Brand, product name, release date (for products that use the same name with different internals)

Next I would create an inventory table.  It would contain information about a product that you would know after it is in the warehouse:

InvId, ProdId, purchase date, serial number, cost, supplier, expiration date, lot number, etc.

Last I would create a category, metafield, extended properties system to be able to record more information about the product and inventory that can be normalized for a user interface.  How you store and implement this is up to you.  

In my job, I create a property table that has four columns: 

propid, proptypeid, propvalueid, propvaluetext

By default, most text is stored in a separate value table, but their are use cases where non-normalized text is required for a property.

I then create a property type table.  This table is where I store information to help build the UI.  I can add as many columns as I like here and can add more tables to create logic based on proptypeid and propid.  I use a heirarchy system here to be able to build logic for a property type that may be different based on another property type.  Basic columns are:

Proptypeid, name, parentproptypeid.

Next is the property value table where I store common text that is frequently reused to save space.  This isn't as important these days, but I am old school.  

Lastly, I create the product properties table that takes ProdId and propid.  Whenever I query this table, I use joins and isnull logic in the select to show either the text from the property table or the propertyvaluetext from the propertyvalues table.  I also like the add an integer rank column here so you can order the properties in importance for search logic.  This allows the property ""Total Network Ports"" to be more important to a server, and be less important for a laptop.

For your case, you can add another table: inventory properties table.  This is a place you could store specific properties of the piece in the warehouse.  For instance, the cars options, or any server add in cards.",1538134069.0
janakaontomatrix,On second thought you can think about category structure where each category has different set of properties. That will make things easier.,1538135642.0
chaz6,"The ""technical"" description of of this problem is [Entity-attribute-value](https://en.wikipedia.org/wiki/Entity%E2%80%93attribute%E2%80%93value_model) or EAV for short. It might help you when searching for answers.",1538138412.0
siscia,"This is just going to be extremely messy in whichever way you approach the problem.

&#x200B;

A couple of solution that comes in mind are:

1. Table ""product"" with all the common fields + another column to store JSON that you can use to put all the ""specific"" of a single element item.
2. Table ""product"" and another table ""key-value"" with a composite key => (product\_id, specific\_properties) and as value the value of property.

But both of them are messy and are going to be an issue, some way or the other, but if done correctly can go a pretty long way.

&#x200B;",1538119885.0
jcsf321,Hire someone that knows about databases design.   You really haven't provided enough information or used cases to help you.  ,1538118141.0
ericbanana,"If anyone is curious here's the Tea model

    using System;
    using System.Collections.Generic;
    using System.ComponentModel.DataAnnotations;
    using System.Linq;
    using System.Threading.Tasks;

    namespace ClevelandTeaRevival.Models
    {
    public class Tea
    {
        public int ID { get; set; }

        [Required]
        [RegularExpression(@""^[A-Z]+[a-zA-Z""""'\s-]*$"", 
    ErrorMessage = ""First letter must be capitalized. No non-letter 
    characters"")]
        [StringLength(30, ErrorMessage = ""Name cannot be longer 
    than 30 characters."")]
        public string Name { get; set; }

        [Required]
        [RegularExpression(@""^[A-Z]+[a-zA-Z""""'\s-]*$"", 
    ErrorMessage = ""First letter must be capitalized. No non-letter 
    characters"")]
        [StringLength(30, ErrorMessage = ""Category cannot be 
    longer than 30 characters."")]
        public string Category { get; set; }

        [Required]
        [StringLength(40, ErrorMessage = ""Description cannot be 
    longer than 40 characters."")]
        public string Description { get; set; }

        [DataType(DataType.Currency)]
        public decimal PricePerCup { get; set; }
        [DataType(DataType.Currency)]
        public decimal PricePerOz { get; set; }
        [DataType(DataType.Currency)]
        public decimal PricePerPot { get; set; }
        [DataType(DataType.Currency)]
        public decimal? PricePerLb { get; set; }
        [DataType(DataType.Currency)]
        public decimal? OtherPrice { get; set; }


        public int? Amount { get; set; }
    }
    }
",1538104292.0
jahayhurst,"What are the chances we get a sqlfmt golang binary also? Basically that works like go fmt or shfmt? (cause this is already pretty cool, but then editor extensions are super easy)",1538079448.0
welshfargo,Too much indenting. I last used 8 character indents about 25 years ago when the tab key always indented by 8.,1538177667.0
grauenwolf,"Downvote for twitter. Post the real article, not just an out of context picture.",1538076142.0
quentech,"> interface with a mainframe emulator a user is logged into

In what way, or for what purpose?

> Are there any tools for front end development that a VBA/SQL/MS Access centered group... would require minimal developer retraining and would still provide rapid solution development?

Ehh.. VBA & Access are a long ways from any kind of web tech. You're not going to be building any kind of web based front end remotely like what most VBA or Access apps offer without significant web specific development skills.

Even some simple stuff like plain reports will be hard to avoid HTML, CSS, some new language (VB.Net is very much different enough from VBA to be considered a ""new"" language to your group), a new platform framework with new ways of doing things like querying your SQL DB, and various web related concepts.",1537989630.0
Buckwheat469,Typescript and React with a NodeJS middleware/backend layer.,1538019456.0
rbobby,"For free... maybe PostGres with PostGis? GIS (geographic information system) stuff lets you query for data in a particular area (etc etc etc).

Also... 90% of them would be zeros makes me ask why store zero values? What is the difference between ""no data found"" and ""found a bunch that's all 0""?",1537929243.0
zimm0who0net,"1.6 trillion rows is going to be beyond the capabilities of standard databases. You’re going to need some sort of data warehouse. Something like a Netezza (or whatever IBM is calling it now). 

However I’m not sure this is well thought out. 0.5km square over the whole country is 15M rows every 5 minutes. You’re going to have a hell of a time even loading that much data in 5 minutes. 

Oh, and I’d highly recommend you stay away from spatial extensions. If you can do queries using a standard bounding box using integer or float columns you’ll be much better off. ",1537936320.0
GuyWithLag,"Having worked with geospatial datasets in a previous life, answer me these questions four:

* What is the total number of parameters that will be non-zero per 5-minute intervals
* Are these locations fixed or moving?
* Is the number of parameters fixed or fluctuating?
* Have you considered run-length coding each run?

What kind of queries will you run on these? have you considered non-SQL solutions for the querying part?",1537979696.0
_Zer0_Cool_,"Postgres

Edit: Postgres w/ TimeScaleDB and PostGis extensions? 

-- These are Time Series and Geospatial Postgres extensions, respectively.

Also, Postgres has geometric data types. Not sure if that matters for your use case.",1537929109.0
mypirateapp,"Only 2 words, timescaledb

* Built on top of postgresql: check
* Optimized for time series storage: check
* Auto partition into hypertables based on time slots so the amount of current accessible data at a time is limited: check",1537962868.0
quentech,"> 2 bytes of precision would be plenty

This would probably have to be custom implemented. I'm not aware of any SQL engines that support 16 bit floating point types.

> 1.6 trillion rows generated per year.

I'm going to assume you won't be doing custom types like 16 bit floats. You need x, y, DateTime, and param value. You're looking at at least 16 bytes per record, depending on what DateTime types are available (MS SQL e.g. you'd be at 18 bytes without a custom DT type).

At 16 bytes a row you'll be generating *at least* 25 TB of data each year.

Even if you have 16 bit floats and squeeze the DateTime into 32 bits (can be done with 5 minute resolution if you don't need more than a couple hundred year span of valid dates), that's still 10 bytes a row and > 15 TB a year.

Do **not** use SQLite for that.

> 90% of them would be zeros.

Sparse columns are available in MS SQL, maybe others, though with less than 25% of your row size being sparse columns you're not going to save all that much space.",1537991320.0
alinroc,"This question is lacking the context necessary to answer reasonably.

This is probably from a homework assignment. If so, post the whole question and then understand that the answer won't be handed to you, you'll be guided on how to work it out.

Hint: Everything stored in a current mass-market computer is in a digital format.",1537885072.0
wolf2600,"In general, database management will likely focus on managing traditional RDBMSs like MSSQL and Oracle.  Data Engineering, will likely focus on both the database (but it could be a NoSQL data store, like Hadoop), and the data integration (moving data from one system to another) aspect.",1537879904.0
imcguyver,"Move towards sw engineering. The data engineering industry is increasingly moving to sw engineering. Think python to plug tools together or scala for real time streaming.  DBAs are increasingly rare as their need is diminishing. DBs like snowflake.net handle optimization that was previously done by DBAs. DBs like SQL Server can connect directly to HDFS, diminishing the value of SQL Server.  IMHO, DBAs are going the way of the dinosaurs. DEs are being replaced by sw eng’s.",1538021129.0
getoffmyfoot,"I understand your plight but I find it unethical to do your assignment for you. I’d be willing to answer specific questions, if you have some. ",1537840168.0
talameetsbetty,Go get Len Silverston’s universal data modeling book #2 on Kindle. All of the answers are in there. ,1537840712.0
,[removed],1537826924.0
leandro,No.,1537805046.0
Fairwhetherfriend,"Nope. If you need a backup version of it for testing purposes, you can try an RMAN recovery on a test server - there are tons of walkthroughs and stuff on Google about how to do this. It's the option most certain to get you a perfect copy of the database. It'll generally require that you install the same version of Oracle on the new test server first.",1537826088.0
da_chicken,"I live in the US and I'm not sure where I could get live classroom training *in the US* for *any* RDBMS.  That's just not how it's done anymore.  I mean, the local college here has classes in generic SQL and relational databases, and you can still get some classes for Microsoft admin certifications, I think, but I think all of those are online or virtual classrooms with labs done with VMs.

Did your management give you any reasoning for such a requirement?  It's really not realistic anymore, especially for less common RDBMSes like DB2.
",1537667143.0
wolf2600,"What do you see from the explain plan?

When the large query is running, what does your system look like from a resource perspective?  Is it spiking CPU? memory? Disk?",1537560667.0
anras,"Saving hourly aggregations might be a good idea, assuming you don't need a finer grain than that. You could do it on a schedule rather than manually. Or you could even add to the hourly total on insert with a trigger. (You'd of course have to backfill all the existing data when you set that up.)",1537562558.0
ecrooks,"Does MYSQL have materialized query tables(MQTs)? If it were Db2, my answers would be MQTs, multidimensional clustering (MDC) and/or BLU(a combination of incredible compression, in-memory processing, and columnar organiztion).",1537589593.0
goblando,"What is your storage subsystem?  I do aggregates on tables 100 times that size that return in seconds.  Also, how many columns and what are the data types?  Does this table have huge text columns?  If so, I would copy the data you need to a new table to work with it there.  You can schedule that job to run hourly to populate new / changed data.",1537634126.0
DesolationRobot,"If you've partitioned the table it might be faster to sum within each partition and then union the results and sum them.

Check the explain for doing it both ways. 

Often aggregating across partitions in a table is slower than the same query on an unpartitioned table of the same size. I think the DB unions the partitions behind the scenes then does the query. If you know that you can pre-aggregate each partition and just sum the results you'll have to manually tell it to do that.  ",1537566893.0
BinaryRockStar,"If the data is relational, use a relational database until performance becomes an issue.",1537532706.0
grauenwolf,"As far as I'm concerned, the only time you should consider using MongoDB is when your customer is determined to use it and you can afford to extra hours its going to take to implement something.

For newbies, MongoDB might be faster at first. But as you gain experience you'll generally find that the lack of DDL/DML means you'll spend far more time over the long run trying to coax the database into doing what you want. One experienced developer I know puts a 5x time multiplier on any estimate that involves MongoDB to account for this.

",1537541706.0
TimIgoe,"In such a scenario you wouldn't want to use mongo, or most other no SQL data stored. You want the power of relationships so you are great staying within a traditional SQL database.",1537549989.0
,[deleted],1537571009.0
kormer,When you need webscale. Otherwise never. ,1537582725.0
wardmatthew,"Either use a relational database or be ok with doubling (in this situation) the data. Mongodb is ok with you doubling your data, if you are. It doesn't slow down quarries because you will only query one of your Scenario at a time. Thus when you have millions of rows in a RDB in both tables it's more expensive to look up the answer. However, all is not magical in Mongoland, if you want to update students and classes for the next semester you need to basically start over. You could have another field such as semester ID, but you will change every entry in the DB related to the query (in your case every entry in the database including all of this duplicates). 

&#x200B;

Generally I've best seen no-SQL or document db described in general is when you want to work from a program prospective. I want to treat my database like objects in my application. The easiest situation I've heard someone describe is pulling data from social networks (twitter, facebook, github, flickr) and storing them in their Couchbase DB. They don't have to continue to come up with a normalized scheme and ETL that can represent the data as the API dumps it to them. So when Orkut becomes popular again and they want to add it to the feed, it becomes much easier to store it native.

&#x200B;

Some years ago I heard a lot of people running to MongoDB because it was able to store different types of data like JSON, Binary Files and so on. However, most other database can do this too, but I've never heard of an elegant solution. With the advent of S3, a lot of these use cases are better served using something like that instead. I don't have first hand knowledge of that but I've heard it a lot. ",1537614069.0
bgraham626,"I would nest subjects inside students. It seems like subjects could have a lot of students, but students would only have a limited number of subjects.

Either query would still be simple.

Scenario 1: Grab a student and you got their subjects.
Scenario 2: find { Subject: 'Math'}, and that gets you a list of all students in that subject

I see a lot of people pretty against mongodb, but we use it at work in an enterprise size web app, and it's been great for us. Generally queries are super simple, and when they aren't the aggregate pipeline is pretty powerful",1537620948.0
boy_named_su,"When you get down to it, most relationships are really many-to-many relationships",1537474280.0
berry_lover96,"you guys seem like professionals! And better yet, conceptual design is up your alley. Maybe you can provide me some input? /u/boy_named_su /u/guytoronto 

[https://www.reddit.com/r/Database/comments/9iiy68/can\_anyone\_help\_me\_with\_this\_conceptual\_design/](https://www.reddit.com/r/Database/comments/9iiy68/can_anyone_help_me_with_this_conceptual_design/)",1537803678.0
hipster_dog,Try Metabase,1537438351.0
AaronPDX,Power BI?,1537440976.0
100shadesofcrazy,"Start making some tables!

Looks fine to me.",1537425881.0
mkingsbu,"Part of being a good analyst is noticing things ""between the lines"" so to speak. In other words, the professor may have intentionally been vague in the business rules because - not surprisingly - people who aren't used to data frequently assume things and don't say them.  So you need to ask about it.  Its almost like a meta-assignment.

What I would do is revise the business rules and send it to the professor including prescriptions and ask if that is a reasonable amendment.",1537445439.0
BadSpeiling,"So take this with a grain of salt, depends on your standards/lecturer/language, I also haven't had to touch a database in 2+years, doing this as revision, pls don't use if you don't understand, will only burn you in the long run, my solution would be:

FK=foreign key

person(KEY id, first, last)

physician(KEY FK person.id, title)            //pretty sure there is a better way to do this, look through your notes about inheritance

appointment (KEY id, FK person.id, FK physician.id, dateTime)

illness(KEY medName, name, degree).       //honestly no idea what the degree of a disease is?

diagnosis(FK appointment.id, FK illness.medName).     //This is also a many many relationship so maybe illness=medication

This is probably not the best way and I'm sure there are mistakes, and if you also need to do medication, it wood look very similar to the last 2 tables",1537451452.0
TPSreportsPro,Sounds like it.  You might be able to get a list of illnesses from a pharm source.  ,1537416355.0
GMarthe,"It looks fine. You have to only create the higher level Data base representations (which I can't recall the the exact name at the moment), right?

The bit at the end is precisely what it says, you have to deal with a many to many relationship. A medicine can be given at many different appointments,and in a given appointment, many medicines may be given. So that needs a special kind of relation to be created. 
",1537444654.0
IgneSapien,"Is that the full text of the assignment? What you've posted is just a description and while I can take a guess that you're being asked to design a database it doesn't actually ask, or tell, you to do that? ",1537447824.0
TPSreportsPro,The FDA has an api if that helps. Perhaps they have the data you need.  ,1537453302.0
Resquid,I don't really see a specific question here.,1537422254.0
kihonuser,Where are you hosting your app ?,1537527231.0
assface,Postgres ,1537438632.0
kevin3030,Why do you need to optimize it? How frequently is it executed?,1537401280.0
quicksj,Narrow your query down by the columns you want to return in your select. ,1537411639.0
qwertydog123,"Are you able to combine them into one query? E.g.

    SELECT * 
    FROM information_schema.key_column_usage AS kcu INNER JOIN     
     information_schema.referential_constraints AS rc ON (kcu.constraint_name = rc.constraint_name AND kcu.constraint_schema = rc.constraint_schema) 
    WHERE kcu.table_schema = 'joseph79_tiny' 
    AND ((kcu.table_name = 'options' AND rc.table_name = 'options') OR (kcu.table_name = 'links' AND rc.table_name = 'links'))

&#x200B;",1537420929.0
DesolationRobot,"If you're meticulous in your naming conventions the folder structure could just be artist, album, then with each file name tacknumber - title.mp3. then your app can just concatenate that file path from the song data. ",1537330996.0
koalillo,"I'm writing a kind of handbook about this kind of stuff and I just wrote my thoughts there:

[https://github.com/datacentric-group/datacentric/blob/master/database\_design/patterns\_antipatterns/blob\_or\_not.md](https://github.com/datacentric-group/datacentric/blob/master/database_design/patterns_antipatterns/blob_or_not.md)

(btw, I'm looking for collaborators :-p)",1537618398.0
quentech,"> This means I have to search around 2^128 elements to guarantee finding 2 elements that repeat

Do you really understand how large of a number 2^128 is?

If inserting one row takes one nanosecond (it won't, it will take much longer), then inserting 2^128 rows will take close to **10,000,000,000,000,000,000,000 years**.

That's about as big as the age of the universe multiplied by the number of people on Earth.",1537316697.0
jlrobins_ssc,"PostgreSQL + using [`COPY ... FROM STDIN`](https://www.postgresql.org/docs/10/static/sql-altertable.html) statements to do bulk inserts with minimal parsing overhead + multiple clients doing the inserting (in parallel, esp. if you're writing to SSD). If can allow for a crash (via just starting from scratch again), then adjust the table to be [`UNLOGGED`](https://www.postgresql.org/docs/10/static/sql-altertable.html) so that you can avoid 1/2 of the write traffic.",1537302239.0
popeus,What have you tried? Have you tried doing it in postgres? There is literally no harm in giving it a shot. ,1537299990.0
cachedrive,I recommend PostgreSQL 9.6+,1537300565.0
web_ml,"For no money, Microsoft Access if you have it would be perfect for what you are trying to do. You could use the google docs to gather the data and then you would have to load it into ms access manually. It would be a quick setup and easy to maintain. However it would only be able to be used where it is saved; meaning if you save it on a computer only someone accessing that computer would be able to use it. However if you put it on a network drive you can access it wherever the computer can access the network drive.",1537240141.0
BinaryRockStar,"I couldn't see myself using it, no. The databases I work with are mostly not backing stores for web applications, they are large ETL databases used to land data sets from our suppliers, clean them up and present them in a standard format for further processing downstream.

The questions that immediately spring to mind are 

1. Performance- how well does this system work with hundreds of millions of rows in a table? Does it have some way of intelligently applying indexes to non PK columns? How quickly could I insert or update a million rows from a CSV file?

2. Durability- is there a 100% durability guarantee? Will data ever be lost under any circumstances?

3. Downtime- how many nines of uptime do you guarantee? Having a critical line-of-business application not accessible due to maintenance of your servers is unacceptable.

4. Privacy- with the recent GDPR laws enacted in Europe, my company and tadabase would be liable for hefty fines if PII user data is stored without meeting the GDPR obligations of customer consent. Is there an option to host on-premise?

5. Maintenance- are backups of the database taken? Can the database be rolled back to point in time if, for example, a bulk data import goes wrong and nukes a table?

Having said that, it's a neat looking product that would take a lot of the boilerplate out of small web application development.",1537230442.0
alinroc,"1. Isn't this one of the target markets for things like MS Access, Power Query, etc., just web-based?
2. @ 0:42 - Spreadsheets *can* be linked together. It's not as powerful as proper FK constraints in a relational DB, but saying that spreadsheets ""can't"" be linked together is misleading.",1537233002.0
dibarra912,"I use Knack.com. Super powerful, and you can build complex applications with no code. Their API also allows for a ton of cool integrations. I also built iOS/Android apps with data from knack app",1537235934.0
dibarra912,"Very cool! Yea, I selectively use knack depending on projects. I have def run into bugs/issues, but since I’ve used them for so long, I kinda know where to draw line. AWS integrations would be awesome. In terms of knacks price/features, I have not come across anything better. Whatever project you’re up to, I’d love to hear about it! ",1537236751.0
colly_wolly,"The problem is even reasonbly experienced devs make a shit job of designing databases, (even though it isn't particularly difficult in my opinion). 

Do you really think handing over that responsibility to business focused people is going to be a good idea?  (Actually re-reding your question I am not sure who it is targeted at, non-techies or techies. Techies will likely just use a ""real"" database, no? These tools can be helpful, but as soon as you want to customise things a certain way then they tend to be quite limiting). 

Plus we had Access bundled with office for years, still very few people got beyond using Excel as a database. I'll admit Access had its limitations, but I still haven't seen anything match it in terms of being able to set up an application with minimal coding knowledge.",1537280286.0
lgastako,3000/minute is only 50/sec.  Any modern relational database can handle several orders of magnitude more than that on a single core.  Just use your favorite relational database.  Or Postgres if that's not already your favorite.,1537220979.0
DesolationRobot,"That should be an easy task for any RDBMS.

Relational vs non relational should be decided more on the actual nature of the data rather than the volume.",1537220982.0
grmpf101,"This kind of traffic is also easily handled with non-relational databases like ArangoDB (Yes, I work for team Arango)

To decide which data model is the right one for you, you have to know which queries you want to run against the data and if the data is highly structured or semi- to unstructured. The first is good for relational, the latter ones would rather tend to schemaless (non-relational). With PostGres support of JSONB format and for smallish dataset you could also use Postgres. If you have a larger dataset, JSONB in Postgres turned out to be not sooo fast in our open-source tests https://www.arangodb.com/2018/02/nosql-performance-benchmark-2018-mongodb-postgresql-orientdb-neo4j-arangodb/

Rule of thumb can be the following:
- Simple key lookups and simple data structure -> key/value
- Simple operations like aggregations and projections but more complex data structure -> document model
- Advanced operations including combinations of joins, filter, aggregations and other complex stuff (in short, if you need relations) -> relational model
- Deep searches of unknown depth or range searches against highly interconnected data points with filters and stuff -> graph model

The advantage of using the right model is natural mapping of your data into the datastore, leveraging optimizations for data format within the DB and you can compose queries exactly matching your data format... overall more time to work on your business logic.

ArangoDB is a native-multi-model database and supports all of the above models with one DB core and one query language. Might be worth having a look in general.",1537278388.0
therealcreamCHEESUS,"This article is dangerously lacking in information.

Please ignore the contents of the above article entirely and use this one for information about shrinking databases:
https://www.brentozar.com/archive/2017/12/whats-bad-shrinking-databases-dbcc-shrinkdatabase/",1537201034.0
wolf2600,"100k records in a DB is not very much for an enterprise production system.

    Users
    ------------
    UserID (PK)
    Name

    Friends
    ------------
    FrienderID (PK, FK)
    FriendeeID (PK, FK)
    RequestTimestamp
    AcceptTimestamp

Relational database should work fine.

A graph DB might be considered if you were doing a lot of analysis on the data as a whole (paths between users, user clusters, etc).  But if you're just using the DB as a datastore for a Facebook-like webapp, an RDBMS would be best.  (Where each request is just looking to return one user's friends)",1537188897.0
JavaJuke,"Graph databases are good for low cardinality result sets with a lot of joins and filterable relationships. They're great for traversing long paths but struggle when the subset of desired edges/relationships is large. They're a rather niche thing, which has a place in our modern database needs, but is over hyped because people don't understand their application. ",1537203193.0
woxorz,I usually work around this by using a temp table instead of a CTE. Because you can index temp tables you won’t experience the same performance hit.,1537216233.0
JavaJuke,Has anyone found a way to stop Postgres from materializing CTEs by default?,1537203391.0
cachedrive,"Yes Access is a database system.
If an employer is asking, they want to know if you have any SQL experience. Do you understand normalization, do you know how to see if something needs an index, do you understand how to look at a data model, etc etc etc. Based on your experience, I would hope you answer 'no' to working with databases.",1537184088.0
onemorepersonasking,Thanks for the information ,1537187951.0
alinroc,"I'm afraid this is impossible to answer without knowing how this ""sync"" is performed and how the system is structured.

Whoever set this thing up needs to work it out. You aren't going to get much from us here.",1536855043.0
mwdb,"Not an answer to your question, but are you sure charset ""utf8"" is what you want to use? ""utf8mb4"" is the ""real"" UTF-8 charset. More info: [https://medium.com/@adamhooper/in-mysql-never-use-utf8-use-utf8mb4-11761243e434](https://medium.com/@adamhooper/in-mysql-never-use-utf8-use-utf8mb4-11761243e434)",1536848900.0
DesolationRobot,"> Is there a way to quickly include the entire range of tables in the OTRS database for conversion? 

You can query information\_schema.tables. There's a column called ""table\_collation"". You can also query information\_schema.columns to find columns with a default character set and collation different than what you want.

You could then write a loop to run through both those lists and convert. You can do it in a stored procedure but you'll need to concatenate the SQL statement as a string, then use PREPARE... EXECUTE to run it as a SQL statement.",1536857669.0
kevin3030,"Select concat(‘alter table ‘, table_schema, ‘.’ , table_name, ‘ character_set utf8mb4 collation utf8mb4_unicdoe_ci;’) from information_schema.tables where table_schema = ‘yourschema’;


That should get you a list of all the alter statements. 

You can also use “pipes” on the command line to pass the commands back to another MySQL session. 

mysql -Nbe “your query” | mysql

Add user, host, password options as needed. From another shell window run “show processlist” to track progress.  


Are any of the tables bigger than the amount of free space in you temp directory?  If so you may have problems altering the table in place. 

Take a backup before you begin :)
",1536879414.0
eshultz,"If your app needs to write/modify the data, then a data warehouse is the wrong approach. You can still use a data warehouse, you just need your writes to be made upstream.

Can you be more specific about your new application and the 12 other apps?",1536819308.0
---_---_-,"OLAP model could work if the app is being used by internal teams such as finance/underwriting/marketing etc for their analysis and decision making. Since this would require only reads and could involve complex aggregations, DW model should work just fine.

But if there are any inserts/updates/deletes to the data - OLTP is the way to go. 

A datawarehouse is typically for analytics and gaining insights into the data for business. If this is not the intended purpose of your app, it's generally better to not go that way.",1536820374.0
Fairwhetherfriend,"Okay it sounds like there's some clarity issues on the terminology. I'll do what I can to clear it up, but you're going to have to speak to your boss about clarifying his request, too.

Here we go!

So you're right in saying that a ""schema"" in Oracle is basically the same as a ""database"" in MySQL. There are a few key differences - most importantly, a schema in Oracle is *also* a user. All users are schemas and all schemas are users. Side note: if you want a user that *doesn't* have a schema, well, that's not a thing. You just make a user as normal (which creates a schema), and then you just don't put anything into that schema.

A ""database"" in Oracle is what you would call an ""instance"" in MySQL. So, when your boss asks you to ""back up all the databases on this box asap"" is he using the Oracle definition of ""database"" or the MySQL definition? That's something you'll have to ask him. Because he's saying ""box,"" (which tends to refer to the actual server itself, upon which Oracle can run multiple databases), I suspect he means that he wants you to back up all the Oracle databases on the server, not just all the schemas in one database. But that's something you'll have to ask him - if you're working in a MySQL/SQLServer shop, he may also be confused about terminology.

If you're looking for a list of all the schemas on your box, that's a good way to do it. There's a huge list of DBA views that you'll want to familiarize yourself with - DBA_USERS is a good one to start with (it's can also produce the information you have listed above). Here are a few more you want to check out:

* DBA_TABLES - contains a list of every table in the database, where the column ""owner"" tells you to which schema the table belongs.
* DBA_OBJECTS - much like above, but it contains a list of every single object in the database, not just tables (though it includes those too)
* DBA_SESSIONS - this provides information about connected sessions

There are *hundreds* of these, and they're all outlined in the Oracle documentation, so that's something to check out.

Also, like the other commenter said, the split of 3 lists is just a SQLPlus formatting thing. You may want to get your hands on SQLDeveloper or TOAD or something like that - it'll make your life a lot easier for digging through this kind of information.",1536777466.0
throwaway2arguewith,"It's just the column headers repeating.
Run ""set pages 0"" before running the query to stop the repeats.",1536776399.0
HildartheDorf,"Ive been in this situation, although I'm a MSSQL dev thrown into managing our oracle box ""Because you know linux"" from using ubuntu a bit at home...

In Oracle, an MSSQL/MySQL 'database' is an oracle 'schema'. an MSSQL/MySQL Instance is an oracle 'database'. Unlike sensible database platforms, oracle users and schemas are a 1:1 relationship. Convention seems to be 'users' log in and have no objects in them. 'Schemas' contain objects but are never logged into during normal usage. Break this rule and madness will consume you, if Oracle hasn't already driven you insane.

RMAN is the backup tool you are after for backing up. Like every oracle tool, the arguments are insane, the errors make no sense, and most googling will give you either people giving minimal information and trying to sell you consulting services, or raw documentation that gives far too much detail.

Make sure you know how to restore your backups before calling the job a good one! I've had oracle professionals send us backups missing a critical file (the control file iirc?). Maybe this is normal and the control file is supposed to be backed up separately, I don't know, but it was certainly not helpful to me.

&#x200B;

So, looking at the output of your post, yes the FINANCE user/schema is probabally the 'database' you should be backing up.

&#x200B;

Sorry for the slight rant, but honestly, after using Oracle database for a while I've completely stopped complaining about MSSQL's oddities and quirks. Because at least it's not Oracle(tm).",1536787333.0
dbaderf,"You can use expdp to do a full backup of the entire database.  You have to login with sysdba privileges to do that.
Better is to use RMAN on the full database.  
If you want to right click stuff, you need to install and configure Enterprise Manager.
You're in a whole new world now.  You need to start reading many manuals.

",1536778751.0
welshfargo,"The Oracle Concepts manual is the best place to start. Also, register at otn.oracle.com for even more information, like this: https://docs.oracle.com/cd/E11882_01/nav/portal_4.htm
",1536781874.0
luart12,"These are Pages, use the command set pages 0 to set only one in Sqlplus.",1536812162.0
luart12,"For a end user they always use “database” but at Oracle level, the database is call instance, inside this instance you have several schemas, owners of the objects inside the database, so your boss is refering to all the objects in the database.

A simple tar or backup of the datafiles with the instance down will do the trick.

If you go more inside, you find out that the objects Lives on tablespaces.",1536812793.0
cachedrive,"So I tried to do a RMAN backup. It failed sadly because the database was in NOACRHIVELOG mode vs ARCHIVELOG mode.

    RMAN-00571: ===========================================================
    RMAN-00569: =============== ERROR MESSAGE STACK FOLLOWS ===============
    RMAN-00571: ===========================================================
    RMAN-03009: failure of backup command on ORA_DISK_1 channel at 09/13/2018 14:13:19
    ORA-19602: cannot backup or copy active file in NOARCHIVELOG mode

I then checked here to verify:

    SQL> SELECT LOG_MODE FROM SYS.V$DATABASE; 
    LOG_MODE 
    ------------ 
    NOARCHIVELOG

So my boss is trying to avoid doing shutdown of the database but it seems my only options are to shut it down to set up ARCHIVELOG mode in the pfile or just do a cold backup. When I try to alter database, I get the following:

    SQL> alter database archivelog;
    alter database archivelog
    *
    ERROR at line 1:
    ORA-01126: database must be mounted in this instance and not open in any
    instance

Is this my only options from this point if I want to use RMAN?",1536863569.0
InternetBowzer,I tried to compile the dankest memes. I’m sure you guys will pick out the hits and misses. Enjoy!,1536721949.0
MarkusWinand,Related: [The 3-Minute SQL Indexing Quiz That 60% Fail](https://use-the-index-luke.com/3-minute-test),1536729399.0
,[deleted],1536686296.0
msiekkinen,"This might be what you're looking for 

https://stackoverflow.com/questions/5031268/algorithm-to-find-all-latitude-longitude-locations-within-a-certain-distance-fro",1536688108.0
xkillac4,"In memory: kd-tree for lat/lon, lookup table for city

Database: if you don’t have millions of points, the quick and dirty way is a compound index on Lat/Lon. The “real” way is to use PostGIS or some other db/extension for spatial indexing",1536688383.0
zimm0who0net,"Are you trying to find items in a bounding box or in a radius query?  If you're doing simple ""point in bounding box"" queries, you can get by with a separate column for latitude and longitude and running a quick and dirty threshold on those columns.  Anything beyond that and you're going to be better off defining a geometry column, putting your points or polygons in there, applying a spatial index, and using that index in your queries.  

Also, if you're loading a bunch of bounding polygons (e.g. postal code boundaries, state boundaries, city boundaries, etc), you're well advised to run them through a geometry feature reducing algoritm before loading them.  Some of those polygons can have thousands of points at foot level accuracy when the feature is bounded by something like a river, lake or ocean.  You likely don't need anywhere near that accuracy if you're just looking for simple queries (e.g. give me all the points within 2 miles of this postal code), and reducing the complexity of these features will significantly increase the speed of your queries not to mention reduce the size of the database.

One other point (I could go on and on and on, but it would turn into a book).  Point in poly queries run quicker than thresholding in ST_DWithin.  So if you tend to run queries with set distances. (e.g. give me all the postal codes within 5 miles of this arbitrary point, or give me the roads within 1 mile of this arbitrary point, where the 1 and the 5 are pretty constant), you're well advised to pre-buffer your geometry column using ST_Buffer and storing that in another column (with a spatial index) and just do a ST_Intersects query.  
",1536693280.0
brantam,I suggest you should first choose an architect / technical lead for your project. You obviously need some advice. Having the right team is far more important for success than choosing some database software.,1536642050.0
BruisedGhost,"figure out what you need from a database and figure out which engine supports what you need.  Also, fair warning, most newer lambda and kappa architecture have VERY specific use cases,  there isnt really a one size fits all solution.  

For example: need join support?  Mongo and Druid are out. is ACID a must?  most distributed systems wont work.  ",1536639361.0
mkingsbu,That would be really complicated. I odn't know if you could effectively make something like that with the dizzying amount of possibilities.  Sometimes firms will build a product using several solutions and then go with the best one (say in an instance of a RDBMS vs. a NoSQL database).  There are different wayt to optimize each and sometimes you'll find that one is better than the other for your specific product.,1536638155.0
TPSreportsPro,"What others have said.  But besides you needing technical help, know that good, clean, easy to understand data can be ported to something else later.   We're moving to an Oracle based system now and some of our old data has issues that make me want to step in front of a train.  ",1536646399.0
ciscocollab,You'll also want to evaluate what kind or enterprise support you will need if things go wrong.,1536652000.0
cgfoss,"https://www.reddit.com/r/Database/comments/95mq7j/how_to_choose_a_database_in_2018/
",1536699768.0
ciscocollab,"Assuming your on linux, can you try typing `su - postgres` which will change the user to the postgres user. Then type `psql` and you should not be prompted with any password.",1536194266.0
kentnl,"DSL's aren't inherently bad, see for example regex.

It's just DSLs that attempt to simplify an inherently complex underlying mechanism incompletely, they get screwed by waterbed theory",1536142576.0
wolf2600,"It it was <= 100 records, you might consider doing it manually.  10k records??? Definitely automate it.

Look into learning Python and using the BeautifulSoup module to do the scraping and inserting of the records.


You'll spend far less time learning the tools and building the workflow than it would take to manually enter all that data.  And when you're done, you'll have learned additional skills.",1536095172.0
Mamertine,"Programmers are lazy. Make something to do the work for your, unless you like doing data entry...",1536092636.0
burnaftertweeting,Webscraping + spidering. Should take a few weeks your first time around. Six months is plenty of time.,1536127283.0
Titus_Lucretius_,"I'd be interested in this too, if anyone has sample code or resources.",1536095064.0
DRdefective,What do you want to test?,1536075658.0
jolome2,https://pgtap.org,1536087179.0
KitchenDutchDyslexic,"Draw me a diagram of your tables and columns. In mspaint or any [tool of your liking](https://www.quora.com/What-is-the-best-free-DB-schema-design-tool).

Once you have your data structure one can think which db to use or app(google sheets).

Your report, how should it be generated?
",1536160891.0
msiekkinen,"For tests of your application that might involve some database interation to retrieve data it's common to use a ""mock"" (fake) database connection that returns your golden test data set you define.   The tests then go on to make sure the out put is as expected for the known input.

This also means you can run your tests w/o an actual database resource setup and running.

Now there are going to be larger sections of tests where you want to make sure your queries are actually reading and writting to the underlying database as expected.  This generally falls under the continous integration testing paradigm.  

Here you would have a small mysql running somewhere along with other tech services in your stack (elastic search, mem cache, hadoop, etc, etc)  

These tests take longer but are more thorough across your technology stack.  You also need extra test copies of all these setup for your integration tests to run against.

You need to make sure things are sandboxed in such a way that test A isn't messing with the same resources test B is running at the same time.",1535999483.0
metachor,"I’ve never heard of any difference between a database management system (dbms) and a “database system.” They are the same thing.

I know it’s generally not helpful to a student to answer their homework questions for them (as it deprives them of a learning opportunity)... but this sounds like its either a trick question or the person asking doesn’t actually know about database use in the real world.",1535945517.0
alinroc,They may be looking for the data on disk (database) vs. the software that accesses that data and “does stuff” to it (database system or database management system (DBMS)). ,1535977340.0
N4t4ly4,Database system seems to be a general term. I've heard people call a dbms a database system and some people refer to Hadoop clusters as a database system. I've also heard businesses use the term database system to refer to all of their stored data.,1535945599.0
mohelgamal,"This may not be home work worthy answer but it may point you in the right direction


A database is just any software that functions to store and retrieve data. Several of the indexes file systems in use are actually database software used to organize files.


A database managment system, is a system that allows the user to interact with the database, acting as an intermediary between the user and the database. This usually implies the use of some form of a query language. 

For example MySQL is a DBMS, when you deploy a MySQL database you can choose one of several underlying databases, such as innodb or MariaDB. But regardless of what you choose. You, the user, will still use MySQL to store and retrieve data using exactly the same way regardless of what is the actual underlying database. 

For all practical purposes, when people say database they usually mean DBMS. 



",1535949929.0
boy_named_su,"You'd probably want to store the videos in a distributed file system, not database. Store the metadata / user info in a database

You could use AWS S3 to store the files, and AWS CloudFront to stream them

S3 is very cheap (pennies per GB). Pricing is here: https://aws.amazon.com/s3/pricing/

CloudFront pricing here: https://aws.amazon.com/cloudfront/pricing/

Depending on the number of users, one database might be fine. Two for redundancy

You can use AWS RDS to host say PostgreSQL database. Not super cheap. https://aws.amazon.com/rds/postgresql/pricing/
",1535928341.0
r0ck0,"To be honest, if you think questions as vague as these can actually be answered, you're probably way out of your depth on this.

Even if I was 100% building the system for you, I wouldn't be able to give useful answers to those questions, or at least ones that would make sense to you until the system had actually been running for a while.  And even then, there's never going to be any consistent cost or number of servers, assuming people actually use the site.  It's going to completely depend on how the system is built, and how much traffic you get.

>  instead of the company owning servers, it will partner with a bunch of host providers (nodes) or entrepreneurs to host a portion of the site. Each node will spend no more than $500/month on hosting costs.

I spend a lot of time planning and making online businesses, and I've actually put quite a bit of thought into exact same idea too.

I think it's going to be very hard to get traction on something like this.  Why would video creators pay for something they already get for free, and which already has like a billion users?  If it's not actually the creators paying, but some types of business investors, it sounds a bit like MLM, even though it's obviously not that.  Only clueless people are going to go in for something like this unless you already have a proven system making profits.

YouTube has already added twitch + patreon-like features to deal with some of the adpocalypse issues, and for anything you build it will be very easy to them to add something like that themselves and wipe you out overnight.

Anyway, sorry to be so negative, but I don't see you profiting from this, especially seeing you don't really seem to know much about the subject in general.  

If you want some technical info, look at how peertube works:

* https://joinpeertube.org/en/home/
* https://github.com/Chocobozzz/PeerTube

... so only keep in mind that ignoring competing with google, twitch etc... on the more specific technical model, you're also going to be competing with open source stuff like this.  It may be worth looking into if you can actually use that existing system and make money from hosting or ads or something rather than building your own.

Also try not to get sucked into all the crap you read on /r/Entrepreneur/ ... it's mostly dream-selling to clueless people who think they're going to be rich in 5-10 years from now.  Not that I'm immune myself, and most of my clients are the same... we think we have some idea that is going to make us millions, but 99% of the time we're all wrong.

I'm not giving up on that dream myself, I actually spend about 80% of my time working on personal online ventures.  But from all my past failures I'm slowly learning to become a better judge of what is and isn't likely to compete and succeed, and therefore spend time on.  

Like I mentioned earlier, I've actually thought about this video hosting idea already, and there's too many risks/downsides and also a huge amount of technical work compared to almost any kind of online system you could build.  Not even Google could get youtube to be profitable for a very long time, I'm not even sure if it is yet.

Lots of possibilities to make money with online businesses, but you're way out of your depth for something as technical as this, and so I am I...  even as someone who's been designing, programming and hosting database driven sites for 20 years.  I would never try to compete with something as technical as this against the likes of Google etc. You should focus on things you can at least fully understand yourself.  This isn't something that's going to compete and succeed against youtube/twitch/peertube etc with a few cheap freelancer coders and your business vision.

Good luck with any future ventures though.",1535941145.0
wolf2600,"Figure out how you would need to use the data, and that should help you determine the best way to design your schema.

If your webapp is going to make two separate calls to pull a users Liked Posts and Liked Comments (where the two sets of data will be used separately rather than lumped together), then you can either put them in the same table or two different tables.    It's up to you.",1535906275.0
stickman393,"*I don't want to make a table for ""Liked_Posts"" and ""Liked_Comments"" because of the repetition.*

What repetition? Seems me this is exactly what you need. You've got a Many:Many relationship between Users and Posts; and Users and Comments. Resolve each of them with a join table:

User_X_Comment and User_X_Post (or pick your preferred naming convention)",1535936271.0
gsvigruha,If you are using an object relation DB like PostgreSQL you can look at inheritance. Otherwise a messier solution is to have two columns in the like table (post_id and comment_id) and then have a constraint check that exactly one of them has to be null. Same for view.,1535944184.0
boy_named_su,"Read up on SQL Table Inheritance, and the answer will become apparent",1536008671.0
,If you want us to do your homework you need to offer your intial solution.,1535671627.0
BlueMagicMarker,"You need an associative entity. Think registration, the entity that sits between student and course. 


So you have student - registration - course

That's about all i can say without you adding this post to your works cited.",1535673454.0
welshfargo,"http://www.databaseanswers.org/data_models/
",1535671538.0
wolf2600,"    Courses 
    -id

    Syllabuses
    -id 
    -course_id FOREIGN KEY (course_id) REFERENCES Course(id)

    SyllabusItems
    -id 
    -syllabus_id FOREIGN KEY (syllabus_id) REFERENCES Syllabus(id)

    Classes
    -id 
    -course_id 
    -syllabus_item_id 
    -syllabus_id <--- redundant column 
    FOREIGN KEY (course_id, syllabus_id) REFERENCES Syllabus (course_id, id) 
    FOREIGN KEY (syllabus_item_id, syllabus_id) REFERENCES SyllabusItem (id, syllabus_id) 

First of all, why are you linking a syllabus item to a class?  Isn't the entire syllabus associated with the class?
In Classes, you should have either `syllabus_id` (if the entire syllabus relates to a class) or `syllabus_item_id` (if only certain syllabus line items relate to a class).  You should not have both in Classes.

    FOREIGN KEY (course_id, syllabus_id) REFERENCES Syllabus (course_id, id) 

This is incorrect.  course_id references Courses, not Syllabuses.  You have a FK relating to an FK relating to a PK.  Don't do that.  The relation should be FK to PK. 

    FOREIGN KEY (syllabus_item_id, syllabus_id) REFERENCES SyllabusItem (id, syllabus_id)

Same with this one.  `syllabus_id` has an FK relation to the Syllabuses table, not to the SyllabusItems table.  But since the relation is already defined (between Syllabus and SyllabusItem) it should not be included here in the Classes table.



Correct:


    Courses 
    -id (PK)

    Syllabuses
    -id (PK)
    -course_id  REFERENCES Courses(id)

    SyllabusItems
    -id (PK)
    -syllabus_id REFERENCES Syllabuses(id)

    Classes
    -id (PK)
    -course_id REFERENCES Courses(id) 
    -syllabus_id REFERENCES Syllabuses (id)


And actually, is a Syllabus specific to a Course, or to a Class Section of a Course?  Make sure the relation is to the correct entity... If the Syllabus is changed each Class section, then the Syllabus FK should exist on Classes, if the Syllabus for a Course stays the same each semester, then the FK relation should be on Courses.  But again, don't have a relation to both.  It's one or the other.",1535573321.0
SelectCompare,"I don't see this approach very useful. The SyllabusItem already had a foreign key to the Syllabus, and adding it to the Class complicates CRUD operations on it. 
What if you want to move a SyllabusItem to a different Syllabus? You'll have to update all Classes.",1535564493.0
afex,Are you really posting your homework for others to solve for you?,1535531759.0
kelenfenrisson,"To split this un-normalized data, you first need to answer these questions : 

- What do you want to do with it ? Bonus points if you can write a list of requests you'll want data for.

- How much entries will contain each entity ? Bonus points if you can establish a ""filling-rate""

- Which Relational Database Manager will you use ? 

- How many people/services will access it ?


Then, you isolate entities. What I see from your spreadsheet : 

- Movies : One movie can be engraved on many DVD

- MovieGenre

- MovieRating : Only if it's PEGI Rating, if it's user, it's another story ...

- DVD

- Price : Only if the price is defined by a chart

- PriceCategory : If a price category can have multiple associated prices

- Distributor : One distributor can release many DVD

- Member

- Staff

- StaffRole : One role can be held by many Staff

- Address : Member and Distributor need addresses and many members cans share the same address


Finally, many-to-many relations between entities will generate tables : 

- Rentals 

Here are the tables i made from it (pk is primary key, fk is foreign key). I had to make assumptions (Rating is PEGI like, Price is fixed by a chart):

- Rating { title(pk), description }

- MovieGenre { title(pk)}

- Movie { title(pk), genre(fk), rating(fk) }

- Address { id(pk), street, suburb, city, state, country }

- Distributor { name(pk), address_id(fk), email }

- Member { id(pk), firstname, lastname, birthdate, joindate, telephone, email, address_id(fk) }

- StaffRole { title(pk) }

- Staff { id(pk), firstname, lastname, role_title(fk) }

- PriceCategory { title(pk) }

- Price { title(pk), value, pricecategory_id(fk) }

- DVD { id(pk), movie(fk), price(fk), distributor_name(fk), release_date, purchase_date }

- Rental { member_id(pk), dvd_id(pk), rental_date(pk), return_date, staff_id(fk) }


It's not perfect, but should do the job for a small business.",1535527237.0
welshfargo,http://graphdatamodeling.com/resources/rettigNormalizationPoster.pdf,1535553247.0
NotImplemented,"I'm just guessing, but I think the problem is that you used the wrong abbreviation ""GB"" for gigabyte in the parameter definition instead of the correct one ""G"".

See here:
https://dba.stackexchange.com/questions/128544/does-innodb-buffer-pool-size-accept-gb-values",1535484715.0
jynus,"Stupid suggestion, but (just in case) do you restart fully the server after applying the change? Dynamic buffer pool resize only started on 10.2.2+

Otherwise, run mysqld --print-defaults in case it is reading from a config file you may not be intending to.",1535487040.0
grauenwolf,"No.

When your first sentence is ""My goal is to create a X using Y"" then you probably aren't thinking clearly. 

Unless your goal is just to learn how to use Y, your first question should be ""What should I use to build X?"" or maybe ""Is Y an appropriate tool for X?"". But putting Y as the goal itself means you're already throwing away possibly better answers for dealing with X.",1535481503.0
NotImplemented,"> user creates a post, the amount of rows created is equal to the amount of followers the user has. For example, if the user has 50 followers 50 rows will be created containing the contents of the post

Why do you create one row per follower? Isn't the ""content"" of a post the same for each of the user's followers? Or do you change the value of ""content"" depending on the interest of each specific follower?

> Since I am using cassandra, to maintain the prosperity and effeciency of the database I have to avoid the usage of allow filtering to do that I am using a secondary index on the viewer column. When the user opens up the newsfeed this query is ran by the server SELECT * FROM content WHERE viewer = ? and then all the posts created by the users that the user is following are retrieved and displayed for the user in the ui

Do you have a specific reason for chosing a column-based DBMS over a row-based approach? Do you regularly access columns for all rows of the table or is filtering more typical?

From what you have written, I can't see any real advantage of a column-based approach. A typical row-based DBMS sounds more appropriate, at least for the simple use case you described, and simply does not have these problems/limitations when filtering rows.",1535482935.0
teiom,"I would try it with 3 tables 
- history (company, position, start date, end date)
- paystubs
- call sheet

With SQL server you can calculate the days when you have the start and end date. You don't want to cramp a fixed number into the table. 
You could also add vacation and sick days.",1535472957.0
wolf2600,"When designing a schema, forget about most of the normalization rules and start by figuring out how you plan to use the data.  Do you want to produce a report?  Query the data with a web site and display it in the browser?  Figure out the pieces of data you'll want to retrieve, then work backwards and figure out the best way of storing that data so it's accessible in the format you want.

From there, jump to the front of the data flow process and determine how your ingestion of data will proceed.. what format is the incoming data, etc.  Once you have the input data format and the final output data format, you can figure out how the data needs to be transformed from the input to the output.",1535481071.0
seoakshay,"we are a leading data provider in India, our data consists of information related to Hospitals & Nursing Homes (All Types) (All India) Data 2018. The data consists information about 2,626 Hospitals & Nursing Homes Database in which we cover anufacturers, dealers, distributors, traders, suppliers, importers, exporters, etc. data in excel format.",1535447102.0
popeus,"They've got there by using domain knowledge (knowledge of the dataset and problem) to describe the functional dependencies within the relation schema. From there you can normalise to 3rd normal form from the functional dependencies.

Also, practice. 
",1535441314.0
ergosumdre,Could you also rename the Sub ID table to address then move the street column to the address table? ,1535457220.0
teiom,"It seems to be setup like this:

1.) A member can only have one position
2.) A position can have multiple members 
Same with suburb

2.) A member can have multiple bookings
3.) A trip can have multiple bookings
4.) A venue can have multiple trips


Hope this helps

",1535472661.0
JavaJuke,You are able to pay someone to help with your access data base. ,1535416850.0
WeaselWeaz,Yes.,1535420807.0
diffcalculus,"Only in Bitcoin, unfortunately. I know, it sucks, but those be the data base rules.

You could pay me to alter the database to accept you paying for database help by other currency, tho. I only accept ethereum",1535422850.0
DRdefective,I would go the way you suggested. That's what I thought of.,1535413568.0
DesolationRobot,"If ALL entities will ALWAYS have values for ALL genes then it doesn't actually gain you anything to have two tables with a relationship. You might as well have a column for every gene containing the value for every entity. This would make error-checking tools built into the database (e.g. non-null constraint) to be used to make sure you've actually populated a value for every gene.

>but the number of genes may increase over time.

Would pre-existing entities get values populated for the newly added genes? If not then keeping them all in a single table isn't warranted.

If you did want to have separate entity/gene tables you would call it a many-to-many relationship and you'd need three tables. One for entities, one for genes, and one for the entity/gene/value intersection. This allow you to have newly added entities with values for newly added genes but not require backfilling or keeping null values for older entities.

As I understand it, this is essentially what graph databases were created to do. You could do this in a graph db if you wanted. Each entity would be a node, each gene would be a node, and each value would be an edge. (I'm not a graph db expert.)",1535414874.0
arlejit,"Theres a couple ways i believe. Im nowhere near an expert on the performance of these approaches though. 

1) Case statements. 
     Case when machine type = A then previous month +12
      Case when machine type =B then previous month +24

2) if/else statement
     If machine type =A then prev month +12
     Else prev month +12

3) if and only if IIF
     IIF(machineA, +12, +24)


One thing to keep in mind is also scalability. What type of code would be more easy for you to manipulate say if you added 5 more machines over the course of some time period or if you added different increments of mainetnenace follow ups. 

I wish i had the time to type out the full code for you but these were a few approaches i could think of for MS Access. 
",1535377342.0
seoakshay,"99DataCD is engaged in offering a comprehensive range of data related to Architecture and Interior Designers All India Data. The data consists of information about 2,737 data providers which go as manufacturers, dealers, distributors, traders, suppliers, importers, exporters, etc. data in excel format.",1535360839.0
r3pr0b8,"> Does a data model represent _one_ table or a _group_ of tables? 

the latter

> Is each data model different for every schema in a data warehouse? 

more than likely, although there may be exceptions for mirrors, archives, etc.

> Does a data model show how all tables are related to each other? 

you betcha... that's its single most important feature

> When someone asks me to ""design a data model for data warehousing,"" what do they mean by that? 

a deeper question than you may want a short answer to

google ""differences betweeen OLTP and OLAP""  and ""star and snowflake schemas""

> What makes a data model ""good?""

how closely it represents reality",1535319776.0
welshfargo,http://www.databaseanswers.org/data_models/,1535321120.0
seoakshay,"we are a leading data provider in India, our data consists of information related to Madhya Pradesh and Chhatisgarh all States Database. The data consists information about 4,108 existing companies in which we cover anufacturers, dealers, distributors, traders, suppliers, importers, exporters, etc. data in excel format. ",1535187031.0
seoakshay,"99DataCD is engaged in offering a comprehensive range of data related to Jaipur, Udaipur, Jodhpur, Kota, Alwar & Rest Rajasthan (All Trades) Business and Industrial Data. The data consists of information about 10,093 Companies which go as manufacturers, dealers, distributors, traders, suppliers, importers, exporters, etc. data in excel format.",1535175496.0
kevin3030,"What’s in mysql.gtid_slave_pos_table?
https://mariadb.com/kb/en/mysqlgtid_slave_pos-table/

I’ve read articles about MySQL crash safe replication and it described cases where changes were committed to tables, but the current position of the relay log was not updated before it crashed. That sort of sounds like what’s happening here.   The good news is that it’s not going forward in time!  But yes, annoying to fix. 

MariaDB and MySQL have diverged here and don’t know what else to suggest - I would if it were MySQL. 

Good Luck. ",1535204452.0
seoakshay,"we, 99data provides a qualitative range of business and industry data of India. These are of great use for different industries and will prove very useful for Industrial Professionals such as Manufacturers, Exporters, Importers, Distributors, Dealers, Traders, Suppliers, Consultants and Service Providers who are willing to uplift their business through a network of professionals.",1535110110.0
thancock14,"I've yet to be in a scenario in which I needed to magically scale up two more nodes in my cluster. I still can't understand taking on the overhead of managing my virtual hardware through docker and kubernetes. I've always had time to request a new vm and add it to my cluster. So I don't understand the appeal.

Can someone explain to me why the overhead is worth it?  of yaml and performing your own security patches and upgrades to frameworks and other maintenance you must do to a docker image?",1535125417.0
seoakshay,"99DataCD is engaged in offering a comprehensive range of data related to Delhi / NCR (All Trades) (Delhi, Noida, Greater Noida, Ghaziabad, Gurgaon, Sahibabad, Faridabad & Manesar)etc. All Trades related to Business and Industrial database. The data consists of information about 45,000 above companies which go as manufacturers, dealers, distributors, traders, suppliers, importers, exporters, etc. data in excel format.",1535104958.0
r0ck0,"> MySQL or SQL Server

Not sure if you have any choice, but with postgres you can use foreign data wrappers to connect directly to external databases and just SELECT the data into new tables as if you were just transferring  within a single db.

* https://wiki.postgresql.org/wiki/Foreign_data_wrappers
* https://github.com/ibarwick/firebird_fdw/

",1535043754.0
hcdiercks,"Hi, my name is Holm Christian Diercks. I am contacting you from IBExpert in Orlando, Florida.

IBExpert is specialized in Firebird db development. We do have had project that require replication from Firebird to oracle/mssql...

please contact us at [contact@ibexpert.com](mailto:contact@ibexpert.com) to find out more. In December we are also having a bootcamp in orlando that teaches the replication technology...

you can find more info here: [http://www.ibexpert.net/ibe/index.php?n=Main.BootcampUSA](http://www.ibexpert.net/ibe/index.php?n=Main.BootcampUSA)",1538681633.0
TinyLebowski,Great write-up. I would've liked to know more about the *cost* of these indexes in terms of space. I suppose it grows exponentially in relation to the number of columns in the index?,1535011495.0
,[deleted],1534835646.0
wvgbishop,"I think you need another foreign key field in transaction. A transaction, by definition, must have two entities. While the primary key could be personaccountid, there should be two foreign keys in transaction that describe the direction of the flow of money (ie buyerid and sellerid). ",1534828150.0
r0ck0,"I've worked on a bunch of accounting + CC systems, and built my own accounting system that I use for my own stuff and contracting business etc.  Some thoughts, and unexpected issues I've discovered over the years that you should consider...

* Is there enough difference between a regular bank account and credit card to warrant separate tables?  They're kind of the same thing, aside from logistical technicalities.  I find using a single table means you're doing less redundant code on building reports (which are very similar) and linking to similar things etc.  Also means if you add other types of accounts that don't neatly fit into either type, you're not adding a 3rd, 4th table etc... e.g. paypal accounts, superannuation, and other things that can hold balances and/or have transactions.
* And when it comes to visa/mastercard debit cards... they're basically also a bank account at the same time.
* Keep in mind business/partnership/joint accounts, trusts etc too.  ""Person"" seems very limited.  You might only 
* Likewise, `Transaction` and `Transfer` are the same thing if you go for a single account table.
* Are you actually storing credit card numbers?  Unless you plan on building a payment gateway service and competing with paypal/braintree/stripe etc... you probably shouldn't be.
* Is there a reason to retain old password hashes?  It's additional risk if the hashes are leaked, now your attacker has even more password hashes for each of your users instead of just latest one.
* I'd skip the AccountBalance table.  Banks only report dates, not timestamps, and you can have multiple balances on the same day.. and the order won't always be what you expect.    
* I store the bank-reported balance in a column with the transactions. I also calculate the balances in SQL myself and compare with the bank balances to confirm I'm not missing or duplicating transactions - they should match most of the time but not always...  
* banks have two different balances usually, and they're not always the same... pre-auth, cheque settlement and stuff like that (available balance -vs- current balance)
* May not really need to store your own balance calculations - and the data will be wrong if you discover bugs or oversights in how you calculated them.  If the user wants to balances in your system, do they want to see what the bank reports (and available vs current), or how you've calculated them?  Sometimes those could be 3 different numbers.
* What's the `PersonAccount` table for?

* Keep in mind in general (with any system) that you might want to change things in regards to user logins, and whether they can access stuff from other accounts / orgs etc?  Also can't assume that a person is only in one org, and orgs may have multiple users acting of behalf of some of all of their data.  Better to structure this in a flexible way, and SELECT from VIEWs if you don't need the flexibility yet.
* Even on simple stuff that doesn't involve companies/orgs... there's other possiblities like people dying and giving others access etc.
* Location table... smallest address unit seems to be zip code... what happens if a company moves office down the street, or next door?  Or different room in same building? (but still needs to be treated as multiple locations, but only for some of the crossover time).  The `Location.Name` field could cover that... but don't assume your users (or anyone else) will make sane decisions there.  Even the London Underground has separate stations in separate locations with the exact same name.
* You might need a BankBranch table if you need to differentiate separate branches in the same bank company.  Keep the location point above in mind here too.
* Banks are companies... so do you need a separate `Bank` table?  What happens if the bank itself becomes a user in the system (or a client of one of your users)?  Bank fees are transactions, just like other transactions, so makes sense for them to be linked through to a `Company` record with some JOINs like any other type of transaction with any kind of company... and again things like paypal + superannuation etc can get a bit fuzzy on what you classify them as (bank or company)... so just making all companies a `Company` record simplifies these potential future edge cases too.
* This doesn't matter as much... but not all ""organisations"" are companies, even though that's the table they'll be in.  But you might find down the track that even less literal ""company"" entities are getting stored in that table... like joint accounts, partnerships, trusts etc.
* Also on banks, and the single account table stuff I mentioned above... credit cards are linked to bank/company too... another reason for a single `Account` table (you'll find more reasons later on too).
* Are you a big fan of creating lots of SQL VIEWs (rather than doing joins in app code/ORMs) for stuff like this?  If you aren't yet... I highly recommend getting into doing as many SQL VIEWs as you can.  It makes these types of systems a million times easier to build, maintain & debug.  My current personal accounting system (I'm the only user) has 24 tables, and 26 views.  And more generally almost all of my projects now have more VIEWs than regular TABLEs.
* For the most part, try to avoid making design decisions on important tables and stuff like this based on performance.  There's exceptions on this of course... but in 20 years of programming most of the regrets I've made in designing things were because I wasted time (and effort with over-complexity and compromises) because I was unnecessarily worrying about micro-optimisations that never would have been a problem in the end anyway.  Still learning this lesson repeatedly all the time.  
* Related to the point above, I'm using UUIDs for everything now... even in situations where it seems stupid to.  As the systems get more complicated, this proves even more to be the right decision.  I now wish I'd used them on my personal accounting system when I built it a few years back... even though I'm the only user and it's only on a single server... there's other benefits.

Lemme know if you have any questions.  I've learnt from quite a few mistakes & assumptions with these types of things over the years.",1534862451.0
Caedro,Seems reasonable at first glance.  Why break out company if you are only keeping one attribute though?  Why not just throw company name in the location table?,1534827146.0
smomnipotent,"Great job! I have a few suggestions though:

1. Rename primary keys to make them less ambiguous (e.g., [PersonAccount.ID](https://PersonAccount.ID) \-> PersonAccount.person\_account\_id). This also allows you to write join-queries with the USING-syntax (e.g., `SELECT` [`a.Email`](https://a.email)`, b.Password FROM Person AS a JOIN PersonPassword AS b USING (person_id);`)
2. Use natural keys where possible (e.g., [CreditCard.ID](https://CreditCard.ID) is redundant if CreditCard.Number is unique)
3. Use compound keys where possible (e.g,. [PersonAccount.ID](https://PersonAccount.ID) is redundant if (PersonAccount.PersonID ,PersonAccount.AccountID) is unique)

I recommend reading SQL Antipatterns by Bill Karwin, it's an excellent book!",1534851009.0
emaklokiperlazio,whats software did you use to create diagram like this?,1534848849.0
austrologi,You better not be storing that password in plaintext,1534856420.0
jDave1984,I thought about that but thought maybe a trigger after each new transaction would be better,1534849188.0
jDave1984,Might want to add some more metadata letter on :),1534849285.0
jon23d,"I would keep track of the balance, but inside the transactions table. Why is a credit card separate from an account? Why are passwords their own table (is this a user password?)",1534856023.0
welshfargo,http://www.databaseanswers.org/data_models/,1534879130.0
RedneckT,"I’m no expert by any means, but why use the name ID every single time? Wouldn’t it be clearer to change it to something like PersonTableID or something like that so that they’re not reused all the time?",1534856078.0
Felidor,"Look for an analyst or report developer position. Get some experience, and then you can move to database development when the opportunities come. ",1534739796.0
patrickmurphyphoto,You could make a portfolio of erd designs amd writeups. I got a job at my university after school and design databases for business intelligence and research,1534729717.0
Mamertine,"IMO you have 2 options when you graduate.  First is to try to get recruited by a larger company. As you say you've got a good GPA, go to job fairs at our university to follow this one.  Your plan B will be to contract after you graduate. This is how I got into the field. Reach out to a contracting/placement firm. Pay will be similar to if you got recruited, but you'll get no benefits.  Be open to contracting for short periods or contract to hire positions.  These positions exist, but seldom get posted as these places usually have a steady stream of people applying.",1534731318.0
lazerath,"It's a tricky field to break into, but stay the course and you will eventually get there. When you graduate, in my eyes, it is most important to get a few years of experience using your degree, regardless of the specific role.  If your interest is in working with data and you find a role using it, take it, but know that it's uncommon to get a role right out of school in data because it's a specialty.  Employers are typically looking for experience and past results.  That said, take a look at the Stack Overflow 2018 Developer Survey and you will see SQL is extremely pervasive.  It's legitimate to learn full stack development or start in another corner of IT because so many roles have some component of working with data. If you gravitate to working with data as you learn the ropes of IT, you will typically get more and more opportunities and exposure in data. Your coworkers will see that you are passionate, interested and good at it, and funnel the work your way.  Then, you just keep your eyes out for opportunities that build on and leverage your experience.  You will find yourself in a DBA, data engineer, data architecture role eventually assuming you keep angling your career that way.  I have hired more than a few folks who have just become known as the 'data guy' at their company despite it not being their formal role and it makes them qualified and desirable for those elusive data focused roles. ",1534732034.0
yawaramin,"This one is a hidden gem:

> SQLiteStudio is a SQLite database manager with the following features:

> Portable - no need to install or uninstall. Just download, unpack and run.

> Intuitive interface,

> Powerful, yet light and fast,

> All SQLite3 and SQLite2 features wrapped within simple GUI,

> Cross-platform - runs on Windows 9x/2k/XP/2003/Vista/7, Linux, MacOS X and should work on other Unixes (not tested yet).

> Exporting to various formats (SQL statements, CSV, HTML, XML, PDF, JSON),

> Importing data from various formats (CSV, custom text files [regular expressions]),

> Numerous small additions, like formatting code, history of queries executed in editor windows, on the-fly syntax checking, and more,

> Unicode support,

> Skinnable (interface can look native for Windows 9x/XP, KDE, GTK, Mac OS X, or draw widgets to fit for other environments, WindowMaker, etc),

> Configurable colors, fonts and shortcuts.

> Open source and free - Released under GPLv3 license.",1534707647.0
thancock14,"Another one I use is [https://sqlitebrowser.org](https://sqlitebrowser.org)
",1534725892.0
blither,Good unions are hard to come by. They usually are my (data) types.,1534664662.0
stedun,Perhaps that SQL Server is hosted in a right-to-work state. ,1534690596.0
boy_named_su,"I guess that would be a ""left"" join",1534797972.0
torstengrust,"    -- fight for your rights, ...
    R ⋈ (S ∪ T)",1534711822.0
wasp_killer4,PHP is very easy to integrate with mysql. You could setup a server with apache/PHP along with a mysql database? ,1534625742.0
EnsomTre,"I have a very similar query and would be super interested to know if there's a non-programmer (!) friendly solution.   
I help with management of a growing poster archive at my workplace - the dream is to have some kind of interface that staff at box office computers can use to search the database without editing it. Not long ago I came across a mobile app called Memento Database, which worked perfectly - this might work well for your family recipes idea? - though the desktop version of the software cannot sync with google-sheets (which is where we currently have the data).  


(Apologies for piggybacking the question... and for not having a helpful answer!)",1534703354.0
MagicWishMonkey,"What would you gain from switching? Seems like your setup already works, why change?",1534545325.0
cgfoss,"It sounds like you have quite a few data lakes that the other parts of the business access.  ETL (or ELT) is likely a good focus point, especially since your input data is so varied.

You may want to look at Spark and PySpark in particular.",1535476223.0
BinaryRockStar,"Without knowing much about Entity Framework, why is `Team.TeamId` not annotated with `[Key]`?

The background here is that columns in a DB can be defined as `IDENTITY` which means when performing an `INSERT`, the DB will generate a value for the column, typically a linearly increasing integer.

In certain situations like bulk importing old data you might want to tell the DB ""I know you normally generate values for identity column `Team.TeamId`, but just for now I want to tell you explicitly what the values are for the coming `INSERT` statements"". This is what happens when you `SET IDENTITY_INSERT ON` and `OFF`.

In your case, you have told the DB that a given column is `IDENTITY` so it should generate a value on `INSERT`, but then you have provided a value for that column in the `INSERT` statement. The DB rightly complains that you are giving it conflicting information and tells you how to instruct it to not generate a value for that column, if that's what you want.",1534530985.0
meznaric,"Yeah, go for timescaledb - time series database built on top of postgres. You can do everything you can in postgres and you don’t have to learn bunch of new ideas to make it work like with influx + you have less complexity in your stack if you can put everything in 1 db.",1534494670.0
kentnl,"Depending on task, [RRDTool](https://oss.oetiker.ch/rrdtool/) can be helpful. That's what's behind monitoring services like nagios, munin, etc.

But it can be a bit of a head-spinner to use sometimes.",1534499945.0
xkillac4,How much data are we talking about? What sort of queries will you be doing? What’s your budget?,1534509517.0
boy_named_su,"If you want to store *and analyze* your time series data, you could also look at http://druid.io",1534528481.0
eshultz,I have no experience with DB2 but can you increase virtual memory (swap/pagefile)?,1534563575.0
mfraune,"still facing the problems? if yes, try to force bufferpools sizes for this database or even all database during their startup phase (no matter what the configured bufferpool sizes really are). set the db2 registry variable DB2\_OVERRIDE\_BPF like

**db2set DB2\_OVERRIDE\_BPF=1000**

after issuing this command you have to recycle your db2 instance (db2stop/db2start). Do not forget to delete this setting once all bufferpool are set to a correct value.",1536159049.0
pixelbaker,"What you’ll be building is an integrated reporting database, which is still an excellent step forward. They should not expect someone without in-depth knowledge of data warehousing to build anything that resembles an actual DW. If they want a true DW, they should hire someone to architect it while having you serve as the internal data expert. This allows them to achieve their business goal while you build your DW chops to maintain it for them and expand it further. 


Source: Am a DW Architect/Consultant. I see these projects flounder and fall without proper support every day. Happy to answer more questions.",1534459777.0
didit7,"I agree with pixelbaker.  but if you still need to build it yourself, I think you need to start by gathering report requirement and map it to Microstrategy specific data model and start populating / ETL data from the source to that data model.  The problem is Microstrategy requires specific data model to achieve its roll up and down capability.  if you start from the bottom up, you will end up having to reshape the data when you start the reporting phase of the build.  The issue with this approach is it is very report specific and may not have the extensibility of a properly designed DW, but this is probably the best approach provided the time you are allocated.  Properly built DW takes a lot of time to design and build.",1534470732.0
boy_named_su,Agile Data Warehouse Design book is good too,1534459314.0
jrbless,"From the description you gave, it sounds like it's currently designed more as a data warehouse than a transactional database.  Data warehouse designs are good for reporting, but tend to be slow for transactions (writing data).

Moving the columns for ""payment method X"" to their own table is not normalizing the data.  What it is doing, however, is splitting it up by business needs.  You will need a separate table for each of the ""payment method X"" types. If you split them all out, you have a few ways of relating the data back together.

1. The main purchases table gets separate columns for each purchase type. Most of them will be NULLs (presuming that each purchase has one payment method). You'll have to add a new column for each purchase type.  This is not a good design.
2. The main purchases table just has the common information in it.  Each ""payment method"" table has a column that points at the purchases table.

Option #2 is a much better design.  Each payment is aware of the purchase it was for, and adding additional payment types does not affect the purchases table at all.",1534453022.0
wolf2600,"Sounds like your DB schema was designed by someone who has no experience working with databases and just threw everything into a single table.

Start from scratch, build a new (correct) schema, then create SQL scripts to pull data from your current table and insert into the new tables.",1534476796.0
seoakshay,"we, 99datacd provides a variety of Database in different sectors in which we cover Manufacturers, exporters, importers, distributors, dealers, suppliers, traders, etc. all professional business details. we are offering you a data related to Trophies, awards and Momentos data in which we have 551 existing companies and our data speaks our accuracy.
",1534418535.0
didit7,"\- SQL is pretty similar, but Netezza may have different function to accomplish the same thing compared to SQL Server.    All in all Netezza functions are not as rich as SQL Server.

\- No trigger in netezza.

\- CTE work the same way as SQL Server.

\- Netezza SP is horrendous.

\- Dynamic query in SP works, but netezza sp is bad.",1534401894.0
SelectCompare,"While the syntax if the language is important, I think that even more important is to remember that Netezza is a share-nothing architecture and certain concepts appropriate for SQL Server may not be right for Netezza. Look out for partitioning keys in your tables and ensure that you move the least possible amount of data between nodes of the appliance.",1535296292.0
alinroc,"If you can query the database from PowerShell (should be doable, easiest with MSSQL but there are .NET libraries for all the major databases), you can use Doug Finke's `ImportExcel` module to export from a data table to an Excel file, with conditional formatting. See [this Stack Overflow post](https://stackoverflow.com/q/46679139/1324345) for how to do the conditional formatting.

Edit: Even if you can't query the database directly from PowerShell, if you can get the data into a CSV format, you can import the CSV into PowerShell with `import-csv` and the export it to Excel.",1534344698.0
Dreamafter,"Excel does have a way to directly access a database and apply a query to it which you can then manipulate through a pivot table. It's accessed through Data > Connections and then Add and select your particular database connection file/format (ex. ODBC) and set it up for your environment. From there, if you wanted conditional formatting that would be done through normal Excel methodology.",1534345471.0
LeSpatula,You could probably use C# and use the Interop API to create the Excels.,1534352529.0
alinroc,"> The primary part of this is there are joins in the code that are not necessary. For example displaying a list of offers for sale, we are fetching the offers by joining a bunch of data that is not actually displayed until you click on the offer, and see all the details of it. This is just one example, but there are a number of similar issues in the code. This is starting to affect our ability to scale.

Your problem here isn't that you're using a relational DB, it's that you're doing work in the database that isn't necessary. Change your application to only call for those offers *when the user requests them*. Or if you need a faster response time, don't pull them on the initial page load but with a separate async call from the page itself once the page finishes loading.

Also, make sure your tables are indexed appropriately. At a minimum, fields used for joins should be indexed. And verify that you're using appropriate data types. The wrong data types will kill performance. Parameterize your queries and/or use stored procedures so that the the server can keep compiled plans cached in memory and not have to redo all that work on every ad-hoc query it gets.

>There are a number of cases in the code where a query is simply re-used, so it is returning much more data than is needed, rather than creating a new narrower query. We would be able to fix things a fair bit by just restructuring all our queries for sure.

OK, so why aren't you doing this yet?

> I am thinking that if a Document DB makes sense, then this is likely the time to acquire some NoSQL talent do it.

I'm not seeing how rewriting your entire data model, storage model, and then your data access layer is going to benefit you yet. You've already identified some pretty big places where you can optimize your current database and data access methods and I'm sure you'll find more as you dig in further.

The best part is, you can make these changes incrementally, one sprint at a time, and see those performance improvements over time. Your users will notice that the system keeps getting better and better, month after month.

Or you can put everything on hold and rewrite your data layers from the ground up.

Don't throw the baby out with the bathwater - fix what you already know is broken and then re-evaluate. You've got highly relational data here so staying with an RDBMS is probably the most appropriate solution. An RDBMS isn't technical debt - poor usage of an RDBMS is. Will a NoSQL database ""scale better""? Impossible to say from the information provided here. For that matter, we don't even know if your current database server is sized appropriately. You could be running it on a single core with 4 GB of RAM and 5400 RPM disks in a non-RAID configuration, in which case it doesn't stand a chance no matter what you do.

Perhaps your application would benefit from a caching layer for some of this data before you rewrite everything.",1534301804.0
wolf2600,"If your data CAN be represented in a relational schema, it SHOULD use a relational DB.
",1534301463.0
cryonine,"I'd say most workloads can be done extremely well in relational databases. While NoSQL is very tempting, it often comes with tons of regret years down the road. With an overview of your schema, it's shouting from the mountain tops that it wants to be in a relational database. Even if you're not doing the joins in a NoSQL database, you're still loading all the data. Documents also have size limits, so you could reach a point where frequent customers documents are gigantic.

I'd focus on someone that is knowledgeable in SQL to optimize your queries and code.",1534302382.0
Brillegeit,"Switching from a relational database to a document database for performance reasons is 99% of the time a bad move, fixing the performance of your relational database is the answer all the way up to a scale thousands of times larger than what you describe.

You say your database is highly normalized. Then how about denormalizing it a bit? Views or perhaps persistent views can be used if you have purists that doesn't like that option.

Are 99% of your queries using indexes? Does your data model cache data to reduce repeated queries? Does any common queries take longer time to execute than some arbitrary number like 1ms? Find out why, fix and move on to the 2nd most common query until the server load is corrected.",1534349427.0
mohelgamal,"It seems you may benefit from a graph database. These can be helpful way beyond the typical social network thing. In a sense they are like a document database where you can define relations between documents. 

This allow you to get both the benefits of normalizing your data and having a flexible schema. 

Also depending on your use case they may be ideal as you can start from one point, the user, then hop the relations to get the node details. 

Take a look at Neo4j tutorials and see if that would help you. ",1534307397.0
jenkstom,"If you are on Windows you can use the dbase ODBC driver. You could probably configure access to use the ODBC driver, although it should be able to read the dbf files already. It's possible there is file corruption and you need to fix it.

On Linux there's a package called dbview for reading, or there are dbf/dbase modules for many languages. For instance there are python-dbf and python-dbfread packages on Ubuntu if you are handy with Python. There appear to be ruby and java packages as well. There are packages to load into postgresql (cl-pgloader, pgdbf, pgloader) and mysql (dbf2mysql).",1534264032.0
da_chicken,"There's not really any configuration with MS Access or LibreOffice Calc/Base.  If they're not showing all your data, I would guess that your file was either a newer version or uses something that these tools don't support.  You could try setting up an ODBC connection.  Windows Data Access Components that ship with Windows include Microsoft's ODBC drivers that are supposed to support dBase.  You could set up an ODBC connection to the database file and use that.  However, I would expect that Access would use the exact same method to connect to the file, so it may not work.  You could try connecting with .Net through PowerShell.  You could alternately install Python and use one of the Python .dbf modules.

Frankly, though, I'd be inclined to just pay the $50 or whatever it is to get your data exported so you can work on it.
",1534270357.0
cowp13,https://www.sqlite.org/faq.html#q5,1534281909.0
ciscocollab,"Apologies if this comes across as a more Python-centric question however I am more interested in the Connect Commit, Cursor Close and Connect Close concepts which seems to be across most DBMS.",1534213367.0
jarodsun,I mean the stability of the database system,1534211563.0
imcguyver,"UML is great for OOP.  UML is not a standard in data engineering.  You'll see it in consulting.  You won't see it at a big tech company.  ER models and diagrams are useful.

Question 4 is missing a lot of requirements.  Can members own 0 toys?  Can members own 2 or more toys?  Are toys shared between members?  Can toys share the same name?  Can 2 or more toys be in one picture?  Can 2 or more tags be in one picture?  Can records be updated?  Can tags be removed?  If toys have a dateFrom and dateTo, why don't members?

Tables:

    members (id, fname, lname, email, country, date_created, date_updated)
    toys (id, name, age, date_created, date_updated)
    locations (id, name, date_created, date_updated)
    photos (id, date_from, date_to, created_by, date_created, date_updated)
    tags (id, photo_id, type, link) [type=location, toy, member]  [link=<FK to member toy, location, etc>]

[edit] I see ur using natural keys as PKs.  Use id's instead.  You want ur PKs to be unique, preferably all ints so they can be indexed.",1534188126.0
noteG,"The problem with using UML for DB is that it's not really created for modeling databases, it's created for modeling systems. A class and an entity type is not the same thing. Because of this the community is pretty scattered and people are using the language in an adhoc way that might differ from place to place. There is a high probability your teachers are not as well versed in UML as you would assume they are. This also makes it hard to give advice, not knowing the UML form of your education. If your school is using a specific defined profile of UML you should find assistance in those documents.

Based on my view of UML, it looks like you might need to do some changes in the diagrams and also of the way you are thinking of certain elements of UML.

I would check out [https://www.youtube.com/watch?v=F5UJkENKc50](https://www.youtube.com/watch?v=F5UJkENKc50) to learn more about UML from the creators.(There is a part 2 as well). Another resource is the UML userguide written by the 3 amigos in the videoclip.

Again, not knowing how you use UML for DB modeling(which it is not meant for) it is hard to say what your teacher deems correct. But for example:

Your pub-diagram is saying that Pubs(usually UML would notate as Pub, as it is showing a static view, instances of pub could still be several) can only have one meal. Remember in UML multiplicity is opposite end of ER-diagrams. In general i would say association classes are often better to be reified, meaning created as a real class. Associations are showing a static view, when instantiated an association is a link between some specific objects in UML. I would also be careful with the 1..\* as it in UML terms say that an instance of a class is not legal to exist if an instance of the class its associated with is not connected with it through a link.

What level course is this? Look at what notation style school is using and try to understand the elements of UML, its not that many and when you understand it things start to make sense. I am however thinking in it from a more traditional systems perspective. 

Read up about unary associations and see how that might apply to links between different locations.",1534159454.0
Hertweck,"A simple place to put the data to start with could be something like http://academictorrents.com 

At least you could get the data out there and then see what people would do with it while you work to build out a usable interface.",1534158659.0
firoxer,"Maybe this will work: [https://www.metabase.com/](https://www.metabase.com/)

It's basically a human-friendly analytics interface to your database. You just download it (it's open source), connect it to your data, create visualizations with the provided builder or with SQL and then compose these visualizations into dashboards and share them. It's simple to get up and running and there's \_tons\_ of functionality.",1534178212.0
seoakshay,"99DataCD is engaged in offering a comprehensive range of data related to Warehouses, Logistics, Cold Storages and Transporters All India Business and Industrial Data. The data consists of information about 1322 Companies which go as manufacturers, dealers, distributors, traders, suppliers, importers, exporters, etc. data in excel format.",1534141750.0
changeupcharley,Tab completion.,1534137989.0
leobacard,I haven't had a chance to try yet I'm interested in Maria DB striving for Oracle PL/SQL compatibility. Someone mentioned it in the comments along with other features similar to what the Oracle database FRA provides.,1536104022.0
Hixon11,### [IntelliJ IDEA](https://www.jetbrains.com/idea/),1534008083.0
cooldug000,RedGate,1534026717.0
boy_named_su,"PostgreSQL  

Pentaho Data Integration  

Bash  

RapidMiner  

SAS JMP",1534179691.0
Task_wizard,Upvote for visibility,1534005457.0
welshfargo,"I have been designing databases for decades and I have never seen Chen notation used outside of academia. It's useless for all but the most trivial data models. You really should learn ""crow's feet"" notation, or IDEF1X if you do government work. Almost all data modeling tools support both. You should also consider UML.",1534171108.0
seoakshay,"99DataCD is engaged in offering a comprehensive range of data related to Logistic, Cargo and Transporters All India Business and Industrial Service Providers data. The data consists of information about 8,000 Companies which go as manufacturers, dealers, distributors, traders, suppliers, importers, exporters, etc. data in excel format.",1533990563.0
seoakshay,"99DataCD is engaged in offering a comprehensive range of data related to Ahmedabad, Baroda, Surat, Rajkot, Jamnagar & Rest of Gujarat  Directory. The data consists of information about 46,572 Companies which go as manufacturers, dealers, distributors, traders, suppliers, importers, exporters, etc. data in excel format.",1533972225.0
seoakshay,"99DataCD is engaged in offering a comprehensive range of data related to Ahmedabad, Baroda, Surat, Rajkot, Jamnagar & Rest of Gujarat  Directory. The data consists of information about 46,572 Companies which go as manufacturers, dealers, distributors, traders, suppliers, importers, exporters, etc. data in excel format.",1533970513.0
wolf2600,"How ""small scale"" is your project?  

Amazon and Google have free tier versions of their cloud DB products:


https://aws.amazon.com/free/

https://cloud.google.com/free/

But it looks like if you want a relational DB, the only option is the 12-month free trial of Amazon's RDS.  The always-free offerings from both AWS and Google are non-relational datastores.",1533902683.0
arti_work,"What scale is your project? Azure pricing is pretty complicated, if you were looking at $732/mo that indicates to me you selected a general purpose managed instance or something. 

Compare that to a basic tier elastic pool with 50 eDTU, that runs about $75/mo. 

If you're willing to step away from SQL Server (I see elsewhere in the thread you want to avoid nosql) then you could use a MySQL droplet on Digital Ocean for as little as $5/mo. 

Really just depends on the scale and features you're looking for.",1533916522.0
stump82,AWS RDS has some free tiers for most db engines. If cost is a concern then definitely consider an open source db when you do upgrade to a paid tier.,1533917291.0
fourzerofour,Both [DigitalOcean](https://m.do.co/c/0c7906b274a4) andd [Vultr](https://www.vultr.com/?ref=6812472) (affiliate links) have reasonable pricing you could install SQL on. The pricing is much easier to figure out than AWS too.,1533918016.0
boy_named_su,"you could rent a virtual private server w 2 GB RAM and SSD for like $5 / month (better spec than AWS free)

https://lowendbox.com/

PostgreSQL takes about 3 minutes to install and get running",1533920110.0
cowp13,i use compose.io. The pricing scheme is little different than AWS or Azure but it works for me.,1533920883.0
jlrobins_ssc,"Well, if any large company will be able to migrate away from and/or reduce their Oracle exposure over time, I'd put my money on Amazon.

And if Amazon were to blog about all of their migration steps to then become a reference point for a near-infinite number of smaller companies to reference as ""Here's how Amazon migrated out from under Oracle, so we can too!,"" then I'd be rather quite afraid, Larry. Quotes like this, Larry, should only serve to throw down the gauntlet. No one has won through pissing Amazon off yet, and, gee, no-one is going to be rooting for Oracle's side here.",1533844479.0
kittydoses,Laughs in Aurora...,1533840454.0
stump82,I wonder what they actually use Oracle for. Most of AWS including dynamo has originated from Amazon's internal development for their own use. If they just use Oracle for a few legacy applications then this is still true but somewhat watered down. ,1533874835.0
grauenwolf,"Aurora... does that offer anything in the query optimizer that PostgreSQL doesn't? 

A major selling point of something like Oracle or SQL Server is its ability to generate good execution plans for really complex queries. 

Baseline PostgreSQL is making progress, but it still has a long way to go.",1533847483.0
changeupcharley,Yes.,1533840440.0
alinroc,"For the size of what you're doing, any relational database will do.",1533758272.0
boy_named_su,SQLite is probably fine,1533760037.0
takito_isumotu,"You definitely better use a relational one, which one is up to you. I highly recommend Postgres or MySQL they are both fine for your case, plus if you project will become bigger and more complicated, these databases gonna handle it",1534194025.0
wolf2600,"What's the daily volume of records in the CSV?

I think it should be a simple matter to produce a script which connects to the DB and inserts the records either daily, hourly, or even in near-realtime as they are created.

What data is contained in the CSV?  Is it the currently remaining balance?  So when users query the DB to get their balance, the balance is only updated once per day?",1533740952.0
Ay--_--ye,"When you say DBA..

Yeah this is definitely not how you should do this. You should be querying the tables directly, but if the implementation is as bad as you say, I'd be afriad to see the database itself. ",1533769164.0
r3pr0b8,TL;DR   postgresql ,1533749645.0
artsrc,"https://www.seas.upenn.edu/~zives/03f/cis550/codd.pdf

People choosing between relational databases and other alternatives would be much better served reading Codd's paper than this.

If the Author understood the terminology of the relational model as used by its creators they would not write things like this:

> Likewise, highly relational data is still better stored in a relational or graph database

",1533772947.0
impossibletogetagf,"Really enjoyed your post. Also checked out your others and ""scaling for newbs"" was super educational as well. Since PostgreSQL is your favorite, if you ever write anything about that in specific, I'd love to read it. I was building an app using all sorts of polyglot persistence, but then decided screw it I can just do this all in PostgreSQL...",1533743975.0
FairCom_Support,"CompMath,

c-treeACE doesn't support MERGE/UPSERT.

It looks like it can be broken into separate DELETE and INSERT statements:

*DELETE FROM testactuals WHERE NOT EXISTS ( SELECT address FROM  m\_edws m WHERE m.address = testactuals.address)*

*INSERT INTO testactuals (address) SELECT m.address FROM m\_edws m LEFT OUTER JOIN testactuals a ON b.address=a.address WHERE a.address IS NULL*

You can find additional info at [http://www.faircom.com/developers](http://www.faircom.com/developers).

Thank you for using c-treeACE!

FairCom Support",1533765129.0
a_s_clark,"I'd probably use a Clustered Columnstore index in SQL Server. It's designed for this sort of requirement:

https://docs.microsoft.com/en-us/sql/relational-databases/indexes/columnstore-indexes-described?view=sql-server-2014",1533672418.0
welshfargo,A columnar database?,1533668297.0
lerchmo,Easiest = bigquery OR amazon athena. if you want to do it on your laptop for free https://clickhouse.yandex/ is VERY fast and should be able to handle the data no problem.,1533709430.0
KitchenDutchDyslexic,"> The only solution I can think of is creating index on each of the columns on a relational database like Postgres. This works but having an index on all the columns is making me think twice about this.

Wondered about indexing every column and stumbled on this 
[Indexing all the things in Postgres](https://www.citusdata.com/blog/2017/10/11/index-all-the-things-in-postgres/) article.

Why does indexing all your columns to optimize your query make you question your method?",1533702120.0
r4start,Have you checked Clickhouse https://clickhouse.yandex/ ?,1533713111.0
ppafford,Maybe https://aws.amazon.com/redshift/ ,1533686667.0
cowp13,"Excel and PowerBI might be a good solution. You can get more help here: /r/BusinessIntelligence
",1533655365.0
da_chicken,"TLDR: LibreOffice is [still] getting rid of Java.  LibreOffice Base uses HSQLDB as the RDBMS, which is Java-based.  They're switching to Firebird, which is C++.  They're adding a bunch of tools to help with the migration, including non-Java-based tools that can migrate an HSQLDB to Firebird so they can actually remove HSQLDB from the project and not lose backward compatibility.

Firebird is experimental in 6.1, and will be officially released as the default format in 6.2.  People should prepare to migrate, or otherwise prepare an external HSQLDB server, because HSQLDB is not long for LibreOffice.
",1533665102.0
p00m4573r,I think that you linked the wrong image.,1533606182.0
welshfargo,ArangoDB has a Slack channel that you might wish to visit to ask questions.,1533685643.0
welshfargo,"https://db-engines.com/en/system/ArangoDB%3BNeo4j
",1533768412.0
a_s_clark,Huh. I've never heard of *any* of those.,1533403370.0
dlindema,"Of the tools you listed, I don’t think many of the regulars to this subreddit would consider them “Databases”. That isn’t to say that the tools listed can’t be “a place to store and access data”, but the opinions and discussions here are usually around things like PostgreSQL, MySQL, Mongo, Redis, Neo4j, DynamoDB, Cassandra, etc. These tools are things that are usually interacted with by a computer program to store and retrieve information. But not by a regular person using a computer.

All that said, I love airtable. :)


",1533404607.0
andresonjohn_49,"It depends on what kind of data you have. I use Infanywhere, but I have more then 10 000 recods inside. For smaller project I prefer Airtable.",1541491871.0
DasWood,"Why do you need an online only database? Mysql or one of it's variants or Postgresql. Pick one, read the docs, read a couple books and go. If you learn either Mysql or postgresql there's tonnes of online platforms that host those or you can host it yourself and jobs for these aren't going away or rely on a single vendor which might just go poof one day. Learn what's ubiquitous. ",1533403916.0
cryonine,What are you trying to do? Most of the things you mentioned aren’t databases. Probably none of them actually. ,1533405277.0
wolf2600,Choosing a database solution should be done after you've gathered all your design requirements.  You should know exactly what you're building and why... THEN you can start looking for a database which satisfies your clearly defined requirements.,1533414286.0
Verolee,"Can I add to this question? https://www.reddit.com/r/Database/comments/93qv2q/daas_vs_no_code_databases_can_someone_clarify/?st=JKG2ZTKA&sh=8fd545ea 
",1533427283.0
seoakshay,"99DataCD is engaged in offering a comprehensive range of data related to All Types Of Machinery, Tools and Automation. The data consists of information of 73,000 companies which goes as manufacturers, dealers, distributors, traders, suppliers, importers, exporters, etc. data in excel format.",1533287894.0
boy_named_su,but why?,1533225266.0
ryati,"Be careful about getting too polymorphic. You don't want to have a table that becomes a key-value pair (if you are doing relational). 


So think of mods for spells and items. This may be fine or it may not be. If the mods on spells and items are mostly similar, then it's ok to have one table. If the mods for items are fundamentally different than the mods for spells, then make it two different tables. By fundamentally different I mean, perhaps item mods always have an adjustment to weapon stats and spells don't. 

You might be able to learn a bit form this article by the WOW people. They had some normalization issues. https://worldofwarcraft.com/en-us/news/21881587/dev-watercooler-world-of-warcraft-classic",1533154640.0
SQLSavant," * I generally shy away from generic column names like 'level', 'name', 'description', etc. because they can butt-up against keywords/reserved words in some RDBMS and are often highlighted in IDEs, which can make it annoying when looking at someone else's code. 

 * Will you be storing character information in this? Most of these tables would be what I'd describe as lookup tables and less as dynamic tables. 

 * The RACIAL_TRAITS and RACIAL_TRAITS_OPTIONS table duplicates the NAME, SNIPPET, and DESCRIPTION columns. Since it looks like RACIAL_TRAITS_OPTIONS already stores this, I would probably only store the RACIAL_TRAITS_OPTIONS.ID in RACIAL_TRAITS. 

 * I would split the ITEMS table to include your 'base' columns in a BASE_ITEMS table.

 * Don't use 'ID' as the name for your primary key fields for columns. When you have a table that references multiple other tables, you'll need to rename the foreign key columns in that table. ALWAYS keep your foreign key column the same name as the primary key column it's referencing, it becomes a headache for anyone writing queries against those tables if you don't. So for example, do SPELLS.SPELL_ID, RACES.RACE_ID, etc. instead.

 * Personal preference, but I like to distinguish in the column names between natural keys and auto-increment/sequence primary key columns. If SPELLS.SPELL_ID is being generated off of a sequence I would name is SPELLS.SPELL_SEQ instead. 

From an ERD Standpoint:

 * Bold any column names that are NOT NULL.

 * I put an asterisk at the end of a column name if it has a default value.

 * I would include the data type of what the columns will be.
 
 * Study up on Crow's Foot Notation and begin using it to distinguish relationships between tables instead of just the generic connection you're currently using. ",1533154856.0
towniedev,"I might add a join table between race and racial_trait, if there is any duplication.

Also might a generic modifier table, with key, relationship, modified_value... useful for speed, daze etc.


Also, a current state table might be worthwhile to aggregate the game state ",1533152080.0
danwork,"You are right that there are many ways to break up this info that would be ""correct."" In general, you should think about how you will pull the data out and what combinations of queries you will need to do so. Then you can maybe merge or mix data together to support those queries. For instance: You could merge the subraces into the races table, and just add a column called parent\_race, where a parent\_race of Null would mean a master race.  If you are going to be pulling out both races and subraces at the same time, it might make it easier to have them together in the same table. If every query you make to the table ends up joining the tables back together, then go ahead and make then join permanent by merging the tables. The more you break up the data, the more effort and query-time if will take to regather it up again, so there is a balance. The punchline is, let your usage patterns tell you what is the best format. If you wanna geek out and are interested in General-Case rules for how to efficiently store relational data, check out stuff about data normalization: [https://en.wikipedia.org/wiki/Database\_normalization](https://en.wikipedia.org/wiki/Database_normalization)",1533155211.0
KitchenDutchDyslexic,"How did you create the image? Share the file, instead of just the ERM diagram. Maybe some SQL Create statements?

Do you have some data for your tables? Maybe in csv, spreadsheets?
Or how would you start to capture actions in the database?


",1533693929.0
SQLSavant,"> Can anyone point out some good resources for the different strategies in database design for a case like this?

Relational database design is the same regardless of what data you're trying to store. I'd recommend picking up a book on it, as it's not going to change based on your specific use-case. (Recommendation for Beginners: [Database Design for Mere Mortals](https://www.amazon.com/Database-Design-Mere-Mortals-Hands/dp/0321884493/ref=sr_1_1?ie=UTF8&qid=1533198193&sr=8-1&keywords=database+design+for+mere+mortals))

> I'm having a little trouble deciding what's an attribute. Should Product makes have it's own table or is it an attribute. Should year and price be part of the product table or should they be attributes. I guess I'm not too clear on the definition of attributes in this case.

I think you're jumping ahead of the [normalization](https://en.wikipedia.org/wiki/Database_normalization) process. My suggestion would be to collect all of your data points, prior to trying to force them into a particular table. The book I've recommended, along with any other book on database design or any that teaches the forms of normalization will show you this process.

> Should Product makes have it's own table or is it an attribute. 

That being said, this is more relevant to the business case of the database solution as a whole. If PRODUCTS is the central table in your database, then I would probably create a table MAKERS and MAKER_MODELS to further safeguard on the data integrity. 

If PRODUCTS wasn't a central part of the database, and it was merely being used as more of an item inventory for a larger business case, then you may be safe to get away with having make and model in the PRODUCTS table. 

Part of good database design is having the experience to know how your data is going to be query/used. When creating your logical design for your database, you need to constantly write psuedo-queries on your tables to see what those queries are going to look like. If you have to write complicated queries for core data of whatever application/etc. is going to use it - then that probably means your design is flawed. 

",1533198310.0
Felidor,Are you using MS SQL Server? You could set up an AG with a readable secondary. ,1533138287.0
asshelmet,"Use log shipping and set an interval that makes sense to apply the logs to your secondary read-only database.  You can apply the logs every hour or once per day, for example.

The alternative is to do differential backup & restore.  No need to do a full backup every time.  The incremental changes are likely to be much smaller than a full backup.",1533139969.0
popeus,"You've picked exactly what I would do. Store health /time at last calculation and then just recalculate as needed in the app and update the db. This means the db is for persistence, app is for logic.",1532929475.0
stump82,"You might want to consider a column store database. It sounds like your data will be mostly redundant with updated timestamps. 
Postgres can do this with extensions if you don't want a dedicated column store db. ",1533917574.0
newsagg,"Uh, Firebird forums? 

Getting advice from reddit is like getting head from a drain pipe. ",1532908518.0
SQL_Stupid,"A relational database is probably perfect for this scenario.

What database (vendor, year, edition? i.e. SQL Server Standard 2014) are you using?

Are you currently using rowstore table architecture?

What is the total amount of storage being utilized for all of these databases?

It sounds like you may need to add indexes to your data, and perhaps optimize your storage in two ways - using Columnstore and not Rowstore if that is the case, and also optimizing physical storage. Optimizing physical storage could mean better architecture for files and filegroups for a 'traditional' SQL engine like SQL Server or PostgreSQL (and using performant storage media like SSDs or hybrid disks, or a dedicated storage appliance), or tuning your distribution and sharding for distributed systems like AWS Redshift or Azure SQL Data Warehouse.

For some useful homework, look up how to pull an execution plan for your query, for the type of database you are running on. Posting that may help.

For indexing suggestions, based on what I know so far, here are my suggestions... You should probably have a clustered primary key on Line ID OR a Clustered Columnstore Index, a single-column index on Type, and either a single-column lookup index or a covering index (with Debit/Credit and Amount columns either indexed or included) on ID.

I don't think a NoSQL database would make this any simpler/easier. ",1532842841.0
mabhatter,"Why are you using the “WITH” rather than a nestled select? That forces it to be treated like a separate query with a full table scan that gets thrown out and rebuilt every single run. Then you’re hit again to build a full JOIN over the whole table. 

Having it be a subquery will allow the Query optimizer to figure out you’re asking for the SAME data twice and do it’s own temporary index thing. Those temp indexes stay in memory when you call a routine like this multiple times. You get a penalty on the first run, but then users run multiple times thru so they get improved performance times 2-x at least for a bit til it gets kicked out of memory. ",1532875675.0
impossibletogetagf,PostgreSQL,1532762137.0
Dawn_John,"Try a free version of SQL Server by Microsoft which uses a variation of SQL called T-SQL, but most of the basic SQL commands remains the same.",1532760475.0
FoCo_SQL,"Oracle, SQL Server, postgreSQL, and MySQL are the biggest database flavors out there. I would recommend to verify you are living where you want to live. If you plan on moving, look at job postings out there and see what the demand is. It's 60% SQL Server, 20% Oracle, 10% MySQL, and 10% other locally for what I see in jobs. My employer used SQL Server too, so it made sense for me to choose that. So check the demand where you want to be and see what your employer uses if you have no preference. ",1532779780.0
a_s_clark,"I'd suggest SQL Server as a good one to start learning, as setting up the free version is relatively painless, and its tooling is quite user friendly. On the upside, even though there are different dialects of SQL in different products, a very large amount of core SQL skill is transferrable between platforms, so whichever you end up learning, it's not wasted if you need to use a different product.",1532785331.0
wolf2600,"Start off with ANSI SQL... that's the generic/universal SQL language core that all(?) RDBMSs support.  Then each RDBMS (SQL Server, Oracle, MySQL, PostgreSQL, etc) will have their own unique features which go beyond the basic ANSI SQL.

If you're looking for a database to install and play with, PostgreSQL would be my suggestion.


SQL is the query language used for relational database management systems, it's not the database application itself.",1532791593.0
grauenwolf,"This is one of my favorite tools for performance tuning SQL Server.

Use SET STATISTICS IO (Transact-SQL) and SET STATISTICS TIME (Transact-SQL), then paste the messages into the window.",1532714461.0
doublehyphen,"It is not only the complexity which matters but also the constant. Since the children are stored in large lists you can read these lists sequentially which means fewer random read accesses to memory/disk and more sequential reads. Random read are slower because you can't make as much use of the CPU cache and the disk cache.

Additionally a binary tree will waste much more disk space on pointers since there are more edges in a binary tree.

EDIT: I can't think of any database which uses binary trees. They are just not a good data structure for databases compared to for example hash tables and B-trees (your k-ary trees)",1532708699.0
blitzkrieg4,"The problem, at least on modern cpus, is paging. You might know that there's such a thing as ""virtual"" memory. Whenever you think of memory in software, pointers, text area, etc, you're thinking of virtual memory address. These memory addresses are resolved to ""real"" memory addresses by the hardware. They do this by looking up into a ""table"" called the ""page table"" that maps virtual addresses to their real counterparts. 

Now, not every addressable byte is stored in this table. That would mean there would be more memory devoted to the lookup table than to using the memory. Even storing every word addressable in the table would mean you only get to use 33% of your memory. Instead, the table is addressable by 4k offsets. Each of these 4k regions is called a page.

So long as your BST fits within that 4k page, you would be fine relative to k-aray trees. The problem happens once you start addressing memory outside of that 4k range. Now the OS and hardware have to find a ""real"" 4k page out there to map to this new ""virtual"" address that you just tried to access. This is called a page fault.

Page faults are very expensive operations. With a binary search tree that grows organically, you would have your nodes pointing to children that are in different pages. That could mean with an n-deep traversal of the tree could incur n page faults. One way to avoid this is to use a k-ary search tree and make your node size a multiple of the hardware page size. In database systems these are called ""database pages"" and are always a multiple of the hardware page size. For example, postgres runs with 8k pages by default, and usually runs on 4k page machines (Intel).

Now, when you traverse down your k-ary search tree that is aligned to page size, you *know* that you will incur a page fault for every step in the traversal. This might seem like a bad thing, but you have to keep in mind that the k-ary search tree is much shallower than your BST. I remember reading somewhere that a vast majority of database trees are less than 3 levels deep. 

The other answers are also considerations, it's just that this is the overarching consideration when designing database (and other) systems.


Edit: In database B-Trees, the nodes are not stored in a list, but in a data-structure more akin to an array. That means that they will without a doubt take up _more_ space than a BST. However this a small price to pay for having page-aligned data structures. ",1532711143.0
grumpyyoshi,Really interesting question but just wondering what type of role you applied for? ,1532783793.0
alinroc,"You can do the initial design and scale it out later on something more appropriate. You have to be careful to not lock away too much of your logic and data-related rules in Access-proprietary code (modules, forms, etc.). And be prepared to make some changes when you do the eventual migration because Access is different from even its closest relative, SQL Server.

Standard rules apply - choose your data types properly, normalize your data, apply constraints & indexes, etc.",1532625854.0
epidemiologist,"If you are thinking of a database that goes beyond personal use, I doubt you will find many people on here (if any) who would suggest MS Access. If you are just prototyping, it's not a bad tool to use. If you are talking about an enterprise solution, I would suggest talking to your IT department before you start creating anything. 

That being said, MS Access is designed to allow you to migrate to an MS SQL Server backend. It's not always the smoothest transition (depending on how you designed it), but there are built-in tools to help you with it. If you design things with scaling in mind, it will go a lot easier (don't lock yourself into the front-end - for example, don't hard-code your drop down list values into the form field, pull them from a table). 

I have used Excel and Access as backends for some simple preliminary data analysis, and I workaround that I have used was to split things over multiple files when the 2GB limit became an issue. It's not ideal, but there are times when it was a quick solution for an issue that wasn't worth setting up something more complex. For example, I put the 2016 data in one file, the 2017 data in one file and the 2018 in another, then linked those files into the front end. It worked for me in my particular situation, but that isn't always the case. ",1532626631.0
boy_named_su,You can use access as the front end to any odbc database back end. Access isn't a great back end. I'd recommend postgresql or ms sql server as the back end ,1532661747.0
Burge_AU,If you haven’t already get the ‘Optimizing Oracle Performance’ book by Millsap & Holt as well. Read that first and then the Method-R Guide. Both very valuable resources when it comes to Oracle performance optimization.,1532622836.0
cowp13,I'd add compose.io to the list.,1532695281.0
alinroc,"1. Yes, it's ridiculous.
2. Your management has to make the business case for getting you proper computing resources to perform your jobs. Yes, it'll cost money. No, I don't care. It's the cost of doing business.
3. Spec out what you *need* to get the work done (if you're doing calculations, you probably need CPU over RAM, but spec something out that can be expanded at least to the licensing limits of the RDBMS you're using). Understand why you need RAID - because it's not a backup, it's for availability and/or performance (depending on the configuration). Make sure this server is in IT's backup rotation and all other backup practices are being followed.",1532618818.0
,[deleted],1532652328.0
wolf2600,"> They have talked about moving to Google Cloud Services,

No no no no no no!!!  Do not use shared hosting for anything resource intensive like data analytics. ",1532624410.0
merican_atheist,Not what this sub is about.,1532569694.0
eshultz,">let us know what you think. 🚀

I think you should advertise elsewhere. Just because your paid service has a database behind it, doesn't make it relevant to this sub, unless we're discussing technical details.

Edit - wow holy shit check out the user page for OP. 

#MEGA ULTRA SPAM MODE ENGAGED",1532581247.0
NotImplemented,"You should create a ""Coaches"" and a ""Player"" table with a unique id for each player and coach. Then you add a player_id and coach_id column to your ratings table that references these ids. This way, you prevent redundancy (storing player information like their name multiple times) and you can store different ratings about the players for each coach.

Example:

    Coaches(ID, name, ...)

    Players(ID, name, ...)

    Ratings(player_ID -> Players.ID, coach_ID -> Coaches.ID, rating_fitness, rating_strength, ...)

Hope that helps! :)",1532540070.0
swenty,"It would be easier to offer useful suggestions if you stated explicitly both what is your goal and in what way your current solution fails to meet the goal. 19GB isn't a particularly large data set. Are you anticipating scaling up the number of users massively? Or is it that your report queries don't execute sufficiently quickly? Or you want a solution that can hold many years worth of historical data? Also, what kind of reports are you running?

The advantages of storing data in a relational database like MySQL are (1) support for multiple simultaneous updates, (2) declarative data integrity guarantees, (3) flexible query language that can join tables, group rows, and compute arbitrary functions. 

If those aren't features that you are taking advantage of, you could probably build more efficient data storage and reporting methods customized to your application. For example, storing the incoming data as flat text files would still allow you to perform computations on individual records or groups of records using any number of general purpose computation tools (python, perl, etc.). It looks like your data records in raw text are around 40 bytes of data per record. For 121 million rows that would be around 5GB. Quite likely that data is highly compressible. Storing lines of text is more efficient that storing database rows, but it still is less compact than a binary representation would be for example. Also there's a lot of repetition between rows. If you compressed old data files, using gzip or bzip2, the total amount of storage would probably be significantly less. It's easy to decompress files on the fly for analysis as needed.

If most of the reports you are computing are based on the entire historical data set, it might make more sense to load only summary data. For example, you might primarily be looking at daily averages, in which case you could compute those averages once per day and leave the rest of the raw data in compressed files.

Or, if you are only always looking at reports for individual users (rather than across users), it might make more sense to split the data into files by user.

The way to optimize storage is to build a tool that is specific to the task at hand. But in order to do that you'll need clarity on what the specific task is. If flexibility is your primary goal, you may not have many options for optimizing. Optimization and flexibility are usually aims that are in opposition to each other.",1532520422.0
,[deleted],1532515843.0
KruppiABC123,"I think your idea with storing just the changed data sounds good. 50% data reduction is a lot.

You could think about dropping old values if they are not longer needed, or are you keeping them forever? Then maybe you can aggregate to bigger resolution. Maybe storing just one average value per hour.

Other options would be table compression and table partitioning which should result in lower size or better performance. With a decent machine you should be able to have at least several billion rows before you have problems

Sharding is the expensive option, because you have to pay for more machines. We have currently stored our timestamp data in one table per customer. This gives us good performance for each customers, and if the table grows too much you can move it later to another machine.",1532519240.0
killthebaddies,"This sounds like a perfect use case for Google Bigquery. It's a database as a service that will comfortably move up to petabyte scale without any need for tuning, managing or anything. You can partition on a date and it's very happy to just keep taking data and you can do aggregates over immense volumes. It's also incredibly cheap to run. It's a true no effort big data solution. ",1532532528.0
reallyserious,"Set default values in the columns. This way the value doesn't need to be stored but will behave like it is there. You can use your most frequent value as default value. 

Have you looked into the compression features in MySQL? ",1532518241.0
wolf2600,"What is the granularity of the data that you really need to have available?

Do you need data points every 5 minutes historically?  For the current day/week/month it might make sense to keep that level of detail, but for older data, you could find the hourly average temps and just store one record per hour per user.  That would pare your data down to 1/12 its current size.",1532520418.0
dessmond,"If you have the opportunity, compress on MySQL level and leave everything else as is. I would oppose strongly of deleting data. You may use it later in some form unbeknownst to you now.  

https://dev.mysql.com/doc/refman/8.0/en/innodb-compression.html",1532534458.0
killthebaddies,"So it can do petabyte scale, but I think once you hit a couple of hundred million rows it's worth using and you get advantages from it. The querying charges are based on the amount of data in the column as its columnar. There aren't any indexes as such, but columns are all compressed. Row filters (unless you're filtering on a partition) don't restrict the amount of data being queried for charge purposes. Each query will charge you the full amount of data that exists in that column. Because of the compression though, a column with low cardinality will have a small amount of data.

When you price it up it always seems to come out cheaper than what you expect. ",1532536128.0
iblaine_reddit,">I'm trying to figure out the best way to slim this data out while still being able to run reports on it.

If space is an issue and this is an Innodb table, then convert it to MyISAM.  Using Innodb for what is essentially a log file is not a good use of Innodb.

>Maybe the answer is to leave the data as it is and investigate something like sharding?

Sharding is a useful when used correctly.  The example table you provided doesn't have much going on with it.  I supposed you could partition by date range for the purpose of decreasing the cost to read/write recent data.

How you use data has a big impact on optimization.  For example, social networking sites calculate users as new, active, inactive, by day.  A new user has never logged in, an active user visited the site today and yesterday, and an inactive user did not visit the site yesterday.  Even with billions of users, the cost to calculate these metrics is very low because the data footprint is relatively small.  Store last_logged_in_date by user and you've got all the data you need.  Anything else can be moved out of the database or deleted.
",1532844111.0
DesolationRobot,"You also need to ask yourself if you need to know what a particular user's status was on a particular date. 

If not, the common solution is to roll up current stats into aggregate tables at whatever interval works. E.g daily. You won't know what user X's favorite dingus was on July 1st, but you'll know which dinguses were most popular on that date and how that popularity has changed over time. 

That tends to be the more useful information. You need to know player X's current attributes because the app needs to serve accordingly. But you don't necessarily need to know their historical stuff individually. 

An aggregate table will always be smaller than raw data. The tradeoff is that you have to roll up by whatever grouping factors you want at the time you aggregate (e.g. age, gender, locale) and all other groupings are lost instantly because the source data is mutable.",1532325873.0
mercyandgrace,">if a user changed their favourite dingus on June 2nd then one record must be replaced by two).

You only need to add 1 record with this approach, however the valid_to attribute needs to be updated as well. 


Given the two scenarios I would choose this one.",1532316396.0
johnnotjohn,"It depends.

How wide are the rows? How many repeated rows are there?

If it's a 'fast change' (i.e. changes more often from record to record) than the space savings might not be worth it, if it's a 'slow change' it's probably worth it.

1,000 items squashed into 2 is good, 1,000 items squashed into  999, why bother.

Other methods:

1) Use a timestamprange (I don't think redshift supports this currently, but worth mentioning) \[2018-06-01, 2018-06-02 23:59:59) 

|12345|\[2018-06-01, 2018-06-02 23:59:59)|25|
|:-|:-|:-|
|12345|\[2018-06-03, infinity)|53|

2) Don't store the 'valid\_to' field

You can always find the valid value by getting the first value < the current time you're looking for.

select dingus from table a where valid\_from <= '$timestamp' limit 1;

The efficiency here depends on your access method.",1532354836.0
newsagg,tl;dr:  Avoid UUID if possible and make sure you use the right encoding if you need them. ,1532276164.0
grauenwolf,"Do you have a small project (or money to burn) and want the best tools? Use SQL Server.

Otherwise go with PostgreSQL. 

For the overwhelmingly vast majority of projects there is no reason to consider anything else. And for the few cases where other technology is more appropriate, you can bolt it on when you reach the scale where these stop working efficiently.

*******

That's not to say you don't have to think about *how* you store your data. I'll use SQL Server as an example because I know it better. 

* Normal, a.k.a. rowstore, is the default.
* Denormalized, a.k.a. NoSQL, data using rowsrtore tables. Possibly with XML/JSON columns
* Columnstore when you are writing ad hock queries and its impossible to create all the possible indexes. (Min table size: 10 million rows. Don't bother on anything smaller.)
* Memory Optimized Tables: High performance, lock-free tables without foreign key contraints or large object storage. 
* Memory Optimized Tables, RAM only: caching tables
* Full Text Indexes: Any kind of textual searches. I even used it once for type-ahead address lookups like you see in google maps
* Spatial Indexes: Dealing with maps, like searching for locations within 5 miles of a given point.
* Graph: Graph-style data and searches

PostgreSQL has equivalents for most of these and a few extra tricks of its own.

***

**Modern relational databases are incredibly flexible and can be used in most situations. So start there unless you have very specific needs that are provably unsuitable.**",1532103120.0
mohelgamal,"I have a good system for that. There are three general types of databases you want to look into SQL, NoSQL, graph databases. Also you want to figure out how big your database will need to be. 

To figure out which type you need, imagine that you have to write the data down on paper instead of a computer. Think what would be an easy way to do it on paper. 

If you start drawing tables on sheets of paper, use SQL. Example would be a user list where each user has a name, address, phone number. These are also called relational databases because you could link the tables. 

If you start dividing your data into little folders containing a variety of  documents or items that changes from item to item. Use a NoSQL database. Example would be amazon product list where each item has a different set of properties than the next. 


If you start plotting circles and squares and draw lines between them. Use a graph database. Example would be if you are trying to write a family tree or who is friends with who. 

Then you wanna think about size 

For SQL: if just a few million lines in each table with simple data , use MySQL. If bigger than that or the data is very complicated, use PostgreSQL

For NoSQL: start with mongo which is easiest  to use and can scale endlessly, but if you need advanced feature like Acid compliant or you are want a managed service try amazon DynamoDB

For graph : start with Neo4j which is super simple unless you know you are going to get super big, then look into Janus graph

I am not aware of any books that cover all three types but there is a cheap book called “seven NoSQL in a week” that cover the NoSQL category, I think it is like $2 of kindle 

There is also one thing you need to keep in mind, you are not limited to one database per project,  apps can use more than database. For example you can have your User data in MySQL and your document data in mongo and you can mingle the data at the application level. What I like to do is separate the data retrieval functions into a separate module so that if I decide to switch the database I am using, I would just import a different module with the same function name. ",1532089964.0
SzejkM8,"As a rule of thumb I say if you don't know what database you need, use Postgres. ",1532114762.0
a_s_clark,"""Seven databases in seven weeks"" is a good introduction to different kinds of databases. Personally, I'd always default to using a relational database until some specific need becomes apparent that requires a different choice - relational systems are far more flexible in how you can use the data than other models, and not all your use cases are necessarily apparent upfront. I've yet to meet a scenario that an rdbms genuinely can't handle well with enough thought given to the data model.",1532098928.0
wolframhempel,"I know your troubles and wrote a blog post on that topic called ""How to choose a database in 2018"": [https://arcentry.com/blog/choosing-a-database-in-2018/](https://arcentry.com/blog/choosing-a-database-in-2018/)",1533744384.0
welshfargo,"You might also find [this webinar](https://pages.awscloud.com/Purpose-Built-Databases-Right-Tool-for-the-Right-Job_0703-DAT.html?&trk=em_a131L000005jgrOQAQ&trkCampaign=July_0703-DAT&sc_channel=em&sc_campaign=GLB-WBNR-AWS-OTT-2018-07-00-DG-A&sc_medium=em_100041&sc_outcome=PaaS_Digital_Marketing&sc_geo=NAMER&sc_country=mult&sc_content=Webinar&mkt_tok=eyJpIjoiWkdKaVpXVTVOVE15WVRCbCIsInQiOiJpc0U3WDV6d1lYOXFQSW9UYnVcL1pNWCtaTDhRZzg0YjhGVVdpZ2tEYThuZlJMTTZnUmppRmYrMHRyXC8rajRYYVEzK3pQd2o5QnM0SUNGcHNQd2c5RkdwOUFacFd4RWtDQTltN012VnByckdZT1psKzhHUDV6VnZINnVhQWtWTGFGV1A1XC9WdjNDWnJNZHNIM2xaNkh5OEE9PSJ9) worth attending.
",1532107287.0
DesolationRobot,"The classic solution is a users table, a tasks table, and an actions table (your history table). Sounds like you have that covered. 

Are you saying there's metadata specific to an instance of a type of action that's not the same for all action types? Can you give an example?

If there is you could have a JSON field in the actions table that stores non-relational data. Your app would have to be able to parse it.",1532074426.0
jevans102,Agree with first commenter. Nothing sounds inherently wrong. You just need to query against the ticket number sorted by date created (desc). Maybe you need a long free field comments column?,1532078943.0
boy_named_su,"You probably want two tables. One that stores data about the thing, and one that stores prices (and the date)

I'd recommend MS Access (or LibreOffice Base) as the front-end (GUI) with SQLIte or PostgreSQL as the backend (engine)",1532020467.0
cowp13,why not excel?,1532021563.0
aodel,"Hit me up means contact me, like make a comment!",1531954972.0
Felidor,"As a sql server dba, I handle most of my automation with stored procedures that get executed from the SQL server agent (backups, index maintenance, alerts, and many more). Powershell and SSIS are also common tools to get some automation done. ",1531952830.0
xkillac4,"As a time-crunched developer at a startup, I just use Amazon RDS w/ slave/hot spare replication.",1531963827.0
aaaqqq,maybe using a time series database would be a better option than a document database,1531910901.0
dankestcompsci,Hey guys! I am a student at Rensselaer Polytechnic Institute (RPI). Me and my friend made this NoSQL database this weekend. I thought we could share it to get some feedback from the community. Feel free to leave it a star and use it on any of your open source projects :).,1531874784.0
,[removed],1531882740.0
r3pr0b8,"B is functionally dependent on A when it's not possible for a B to exist without its A 

example -- rock bands make appearances, so it's a one-to-many relationship

appearances are functionally dependent on the band, because you cannot have an appearance of a band that doesn't exist

so if you delete the band, you have to also delete all the appearances

and if you want to add an appearance, you have to make sure that the band exists first",1531850934.0
jpers36,"You're not aliasing your columns in your subqueries.

EDIT: Specifically, date(l.created\_at).  Additionally, you're not selecting updated\_at in your subquery to include is as a column in whatever.",1531832387.0
wolf2600,"    +-------------+------------------+------+-----+---------+----------------+ 
    | id                | int(10) unsigned | NO     | PRI   | NULL      | auto_increment | 
    | job_id          | int(10) unsigned | NO    | MUL   | NULL      |                       |
    | timecard_id   | int(10) unsigned | NO     | MUL | NULL      |                        | 
    | event_id       | int(10) unsigned | NO     | MUL | NULL      |                        | 
    | duration       | int(11)     | YES    |        | NULL     |                 | 
    | gap              | int(11)     | NO       |        | 0         |                 | 
    | created_at     | timestamp        | YES     |        | NULL      |                      | 
    | updated_at   | timestamp         | YES     |        | NULL      |                      |
    +-------------+------------------+------+-----+---------+----------------+

    SELECT TIME_TO_SEC(SUM(TIMEDIFF(updated_at, created_at))) AS total 
    FROM
     (SELECT DISTINCT date(l.created_at), 
        (SELECT MIN(created_at) FROM logs WHERE updated_at BETWEEN l.created_at AND l.updated_at) AS TIME_ENTER, 
        (SELECT MAX(updated_at) FROM logs WHERE created_at BETWEEN l.created_at AND l.updated_at) AS TIME_EXIT 
     FROM logs l) AS whatever 
     WHERE updated_at <> CONVERT(updated_at USING ASCII) 
     GROUP BY date(created_at);


What is your WHERE clause supposed to be doing?  Where update_at is not the ASCII version of updated_at??


What data are you looking for from the query?  You want the number of seconds between the minimum created_at and maximum updated_at timestamps?  What is the goal of the DISTINCT DATE(created_at)?

Can you describe what your query is trying to return?
",1531837720.0
ofb,"Simple answer: No. Don't split. Lots of downsides.

Complex answer: Yes, in very large applications, to have clearer separation of concerns between [microservices](https://en.wikipedia.org/wiki/Microservices). Keeping everything in one db will encourage you to share resources between microservices. Don't do this. Keep in mind microservices can be a ton of overhead and the benefits often only start to pay off in very large applications or application ecosystems.

Some downsides of splitting:

- Permissions can become a real PITA. This is the big one. (in certain cases they will simplify)
- You now need to fully qualify table names all the time. Don't forget! The default db is now a liability and source of bugs.
- Most ORMs break or get kludgey across databases.

edit: I'm not aware of any performance differences between splitting or not splitting.",1531771111.0
stump82,Yes and no.  Look up sharding or partitioning. You will gain some benefits in very large databases in terms of index searching and lock contention but the downsides of admin and management might outweigh those benefits.,1531948448.0
Keehebert,"As a variant try next step:

[https://support.microsoft.com/en-us/help/193952/how-to-troubleshoot-to-resolve-suspected-corruption-in-visual-foxpro](https://support.microsoft.com/en-us/help/193952/how-to-troubleshoot-to-resolve-suspected-corruption-in-visual-foxpro)

[https://download.cnet.com/Recuva/3000-2242\_4-10753287.html](https://download.cnet.com/Recuva/3000-2242_4-10753287.html)

[https://onlinefile.repair/en/dbf.html](https://onlinefile.repair/en/dbf.html)

Copy a valid memo file over the corrupt memo file to access the database again. 

NOTE: The original memo field information will be lost unless the valid memo file was an exact backup of the corrupt file. All memo field backup files have an extension of .TBK. 

The corrupt memo file will have an .FPT extension, such as OLD.FPT. In the following example, assume NEW.FPT is a valid memo file. To bypass the error message and access the database again, type the following command at the MS- DOS command prompt:",1532187413.0
drunkadvice,"Redgate. I don't use their data generation program, but all of their other tools are amazing.",1531560567.0
ubernorm,"Is this to generate test data for performance testing?  Have you heard of TPC?   I think the zip files contain data too?  (It's been years but I vaguely recall that it did) 

[http://www.tpc.org/tpc\_documents\_current\_versions/current\_specifications.asp](http://www.tpc.org/tpc_documents_current_versions/current_specifications.asp)",1531866491.0
welshfargo,"I just googled ""data generator"" and got a whole list of them.",1531591831.0
boy_named_su,"For PostgreSQL, you can use the [COMMENT ON](https://www.postgresql.org/docs/current/static/sql-comment.html) statement to add comments to tables and foreign keys:

    comment on table $table_name is 'this table is a list of cats';

    comment on constraint $foreign_key_constraint_name on $table_name is 'foreign key to cat food table';

If you want something fancier, you could try Pentaho Metadata Editor:

https://help.pentaho.com/Documentation/8.0/Products/Metadata_Editor",1531501104.0
jpers36,Oracle SQL Developer's modeling tool is 99% compatible with SQL Server.,1531504867.0
welshfargo,"You should not store a person's age, you should store their date of birth. Their age can always be calculated accurately at runtime.",1531493123.0
NorthNorthSide,"I feel you should touch on the disadvantage of an index. There is no penalty for reading (you gain performance, as you said) but there is a penalty on insert, update and delete because the database engine has to write the data and index too",1531505210.0
JohnWillkinson,"There are many variants to help you. One of it SQL Server Management Studio.

Another one is mssql repair, I applied it only in emergency cases, I hope [guide](https://sql.recoverytoolbox.com/repair-sql/) will help you, if not, see this instrument. I found it immediately in Google, probably it is the best in this sphere. https://blog.sqlauthority.com/2013/07/19/sql-server-database-in-restoring-state-for-long-time/

Start SQL Server Management Studio

Click the “Start” button. Move the mouse cursor to the “All Programs” menu, then find and click “Microsoft SQL Server” to open a list of SQL Server programs.

Click “SQL Server Management Studio” to open the Connect to Server dialog box.

Select a server on your network. Set the “Authentication” pull-down list to “Windows Authentication” and click the “Connect” button.",1531873183.0
BrayanWatson,"There is a method below, third-party repair utility for visual foxpro tables. Another way to fix dbf might Google search query for special instruments - there you may find this one guide and others. [https://www.fixtoolbox.com/dbffix.html](https://www.fixtoolbox.com/dbffix.html) or [http://www.dbf.repair/](http://www.dbf.repair/) Try opening the .dbf file in Excel or other program that supports opening the .dbf format. Once opened in Excel you should be able to view the file, delete any ""bad data"", and resave to a new name (with the .dbf extension). Also possible to create a new table with the structure of the old table, append records from the old to the new. May salvage records until the corruption is encountered.",1531866559.0
TheChadSatoshi,would you know if they will also release it for Linux? it looks like my repository is not updated with this version yet.,1531336871.0
boy_named_su,"Yes you should use the smallest appropriate type. Make sure you can guarantee it'll fit long-term though, as it's not fun to change later ",1531163839.0
anras,"It depends how much time you want to spend optimizing. If you're using say mysql (you didn't mention which database), a tinyint vs. an int won't make much difference for a modest number of rows, but once your tables grow larger you can save a lot of space. Also consider foreign keys in other tables referencing this one.  
  
As far as how much you would save, check your documentation where data types are described. Again assuming mysql, if you used a tinyint vs. an int, you would save 3 bytes per row for this one column, as an int is 4 bytes while a tinyint is only 1. Not much of a difference but once you get into millions or billions of rows it could add up. Also if you index this column, the column will appear in the index many times as well.  
  
What I generally do is consider each column and use the least big, but bigger than I'll ever need type. At the same time, don't sweat the small stuff. If you're spending an hour wavering on whether you need an int vs. a bigint, just go with the bigint.",1531164109.0
DesolationRobot,"enum and tinyint are both 1 byte and allow up to 255 distinctions. So that's the same. If you're going to have < 255 shipping methods then either is fine.

* enum is nice because the metadata is right there in the table. Human-readable.
* tinyint with a lookup table is nice because you can store \_more\_ metadata about each shipping type.
* tinyint with lacking documentation or documentation that's in a place other than the database itself is annoying because few people will know what shipping\_type = 25 means.

So don't do that last one. But whichever of the first two you do is up to you.",1531164277.0
wolf2600,"Using an integer is fine for this.  Ideally, the value should coincide with a record in your SHIPPING_METHOD dimension table which will have the method's name and description.

",1531165409.0
Matthew-bevan,"Have you ever wanted a Database hack access or penetration testing service and you had contacted many hackers to render this service but cannot guarantee a safe exploit for your school systems or company system. Fear no more, when it comes to network exploit of any type of database including social media, Emails, cell phone interception. We have all hack material and tools to penetrate each system with the necessary certifications ,Dont be ripped off. Contact us at - paynedlucas@gmail.com",1531446319.0
AaronPDX,"Dunno what technology you're going to be using but I work in SQL so here's what I would ask someone if they were interviewing with me:

- Left, right, full joins
- Primary/natural/foerign keys
- Clustered/non-clustered indexes and when to use them
- normal forms and what they say about db design
- 1:1, 1:*, *:* relationships and how to handle them in your db design
- mean, mode, median, what they are and what they tell you/when to use one vs another
- Various graph types and when they're appropriate to use (pie chart, bar graphs, line graphs, area charts, etc) For bonus points rant about why pie charts suck and a bar chart should be used instead, there's some good info online about it.
- prob some stuff about query tuning, how to read query plans, what key performance bottlenecks to look for, and how to pick out where you're maybe missing an index somewhere
- Likely some shit about SSIS and how to create/manage/run packages
- Probably some basic developer questions like what're for/while loops, if statements, what's a class, what's a struct, interfaces,  inheritance, polymorphism, big O notation, etc.  But those might technically go a bit beyond the official job description, just useful stuff to have.

I am a data engineer and it's a weird position where they sorta want a developer + data analyst + DBA so being a jack of all trades, and able to pick up new knowledge as needed, is pretty key. 

When I interviewed for my position, I was frankly under-qualified, but thankfully they were looking for someone who would grow into the position as it was a whole new thing for them as well.  The question that actually tripped me up the most was, ""What's the odds of rolling any given side on a six sided die?""  I said, ""1 in 6?"", but they wanted a specific % chance, and I choke under pressure so I couldn't think of how to actually math it even though I can usually do that kind of thing without batting an eye.  But the guy interviewing me (who is now my boss) is sorta notorious for trying to throw people off balance in interviews.  He's turned out to be a nice guy aside from that though.",1531069793.0
r3pr0b8,"looks reasonable... 

why a separate notification date and time? are you thinking of ~day~ and time, e.g. every Tuesday at 10:00?

",1530901700.0
MarkusWinand,"> but if anyone has read it and liked it

Well, I *wrote* it and liked it ;)

First of all, there is a free version of the book available at https://use-the-index-luke.com/ — that's not something pirated, it's my very own site. I offer the content in several forms. Go there, check it out.

Other than that: The book uses the Oracle database as the primary database for demonstrations. The way the book covers several databases is that it focuses on the concepts, and uses one database to demo it. Differences to other databases are also show, when required.",1530884569.0
welshfargo,Check out Tom Kyte and Richard Niemiec.,1530903346.0
Karter705,"It's been a while, but I really enjoyed [The Art of SQL](https://www.amazon.com/Art-SQL-Stephane-Faroult/dp/0596008945/ref=sr_1_1?ie=UTF8&qid=1530911482&sr=8-1&keywords=the+art+of+sql)

It's also not DBMS specific, but I used to do a lot of work in Oracle and found it very insightful. It goes into a lot of detail on how the the optimizer and the DBMS engine work under the hood, which I found helps me 'think' about queries more like the DBMS does. It also covers overall strategies for designing your database for performance.",1530911608.0
wolf2600,https://smile.amazon.com/SQL-Tuning-Generating-Optimal-Execution/dp/0596005733/,1530899610.0
grauenwolf,"The 'big hammer' solution is to set the transaction mode to ""serialized"". I use this when I must ensure everything is right and can't figure out all of the possible race conditions. 

This can kill performance because it literally limits you to one person at a time writing to the target table.

Here is a primer I wrote on the topic: https://www.infoq.com/articles/Isolation-Levels",1530804326.0
doublehyphen,One solution is to store counters in another table and always update the coutner before inserting or deleting a row.,1530803957.0
grauenwolf,"> One way I can think of is to lock the table before an insert, but that would open the table to other deadlocking issues. 

Are you sure?

If your transaction only touches this one table and no others, you shouldn't have deadlocking issues. Usually deadlocks occur when two or more transactions request multiple tables in a different order.",1530804494.0
thejumpingmouse,Would it be feasible to just keep time for each insert then pick the earliest 25? Or are you limited on other things that keeps the max at 25?,1530803723.0
xkillac4,"It is database dependent. If you're using a solution like timescaledb, everything is taken care of for you, more or less. Others, you will have to use the databases unique capabilities to develop your own scheme.

For another example, here is amazons guidance on using dynamodb for timeseries data: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-time-series.html",1530788780.0
reallyserious,"\> Should I divide the table structure into one table per day, or one per user, or one per location?

In an RDBMS you would do this with one table that is partitioned on some condition e.g. timestamp.

But there is too little information to recommend anything really. Is it sensor data from a lot of sensors or do all data originate from the same source? How much data per row/entry?",1530789522.0
rose_anachronysm,"I would suggest reading a book first or hiring someone else to develop it with experiende in the technology. I do consulting with Cassandra and you wouldn't believe the number of failed clusters we see because a RDBMS DBA attempted building a Cassandra cluster. 

There are serious fundamental difference which have impact on scalability and performance.

Just noticed it will be Cassandra. I'd advise speed reading the new addition of Cassandra the definitive guide. 

Key points:
*Use replication factor with a multiple of 3
*Use SSDs
*Use NetworkTopolgyStrategy
*Don't ffs normalize the data
*Be wary of large partitions
*Don't use Allow Filtering, Secondary Indexes or Materialised Views
* Beware tombstones

I work at a managed service provider for Cassandra, if they can afford it get them to get a managed service or support contract. It'll save them from doing it later down the track when they have build an unfixable monstosity.",1530819381.0
wolf2600,"Why do you think you need a NoSQL DB?  If the data you're ingesting is structured, you should use a relational DBMS.  Capacity and scaling are separate issues from the DB structure.  A million records per day is easily doable with an RDBMS.",1530791113.0
newsagg,You should reject the job and admit your unqualified,1530790706.0
codemagic,There’s a reason why some technologies endure and aren’t consumed by the buzzwords of the day. Consistency is one of them,1530804279.0
AiexReddit,"Can't help with your request, but here's the subreddit you're looking for:

https://www.reddit.com/r/datasets/",1530673176.0
jujijoog,"[https://footballcsv.github.io/](https://footballcsv.github.io/)

if you havent already found it",1530721585.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/datasets] [Could anyone help me out here? Thanks!](https://www.reddit.com/r/datasets/comments/8vycfv/could_anyone_help_me_out_here_thanks/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1530674307.0
grauenwolf,"I would create a SQL Server Integration Services (SSIS) package that pulled all of the data into a single server.

Another option is to use SQL Server's Linked Servers feature. This allows one query to span multiple database servers.

Both assume you are using SQL Server. Other database products have equivalent features.",1530633993.0
asshelmet,I would prefer to send the detailed data to a staging area in your central data warehouse.  Offload the aggregation and transformation to the BI load process.  This also depends somewhat on the refresh rate that your users expect.  A lot of places are satisfied with BI reports that are based on data that is current as of the day before.,1530636381.0
cowp13,"there are pros and cons with each of the approaches between having a data warehouse vs pulling directly from the 4 sources every time you run the report

**data warehouse** \- add complication but it will be faster to pull report especially if you have multiple large tables and need to join them. everything is consolidated and centrally located. 

**pulling ad-hoc from each location** \- simpler than data warehouse but depending on how much data needed to process/join, this can be slow.",1530697946.0
ecrooks,"You need to set Logarchmeth1 to a disk location that is different than your active log path. Preferably a different filesystem. You need to then manage retention of archive log files using either scripts or AUTO_DEL_REC_OBJ

After the change to logarchmeth1, an offline backup is required. There is no way around this. Get in the habit of setting it every time you build a database, before it becomes production.

https://datageek.blog/2011/05/04/managing-db2-transaction-log-files/
https://datageek.blog/2014/11/04/db2-basics-backups-of-data-and-configuration/",1530586373.0
WeaselWeaz,Why a custom database? There are existing CRM products that would do this and more. ,1530619774.0
crookedkr,"Not exactly sure what you mean about database structure. You will want a relational database management system (RDBMS) rather than a document store or graph db (this is probably quite obvious). In which case you will use tables to organize your data.

Really though, just use an existing CRM system (there are countless to choose from).
",1530614612.0
jahayhurst,I've referred to parts of https://use-the-index-luke.com/ from time to time (in the online version). I've not been through the whole book though.,1530569642.0
leandro,"This book is garbage.  The relational model definition it gives (p. 90) is just plain wrong.

What you want may be too much for a single book.  For the concepts, I would get Chris(topher) J Date’s An introduction to database systems; then I would get something about PostgreSQL for performance tuning.",1530557933.0
welshfargo,Check out Joe Celko's books on relational databases and SQL.,1530563746.0
FoCo_SQL,"/u/jahayhurst made a great suggestion, that site is fantastic for indexes from all systems.

I would recommend to read [Kimball, then move onto some light Inmon](https://www.amazon.com/Kimballs-Data-Warehouse-Toolkit-Classics/dp/1118875184/ref=pd_lpo_sbs_14_t_2?_encoding=UTF8&psc=1&refRID=TXBDRD15C22PB6DS3YPY).

That really only relates to data warehouses, there's still a lot of other design theory out there, but I think those books may be a good consideration. ",1530620291.0
boy_named_su,"For database concepts, you can't beat Christopher Date

For SQL the language, you can't beat Joe Celko

My favourite modeling books (real world business apps) are [Enterprise Model Patterns by Hay](https://www.amazon.com/Enterprise-Model-Patterns-Describing-Version/dp/1935504053) and [Data Model Resource Book by Silverston](https://www.amazon.com/Data-Model-Resource-Book-Vol/dp/0471380237)

For data warehousing my favourite is [Agile Data Warehouse Design](https://www.amazon.com/Agile-Data-Warehouse-Design-Collaborative/dp/0956817203)

Performance-wise, you probably want to pick a db brand and find the appropriate book, or just put a bunch of SSDs in RAID 10 and not worry about it ;)",1530723833.0
gram3000,"I enjoyed this one recently enough https://pragprog.com/book/bksqla/sql-antipatterns

It talks about patterns to avoid if possible with alternatives to use instead.

It started me off reading more database books.",1530568144.0
leogodin217,"It's been years since I worked with MySQL, but I remember replication being very simple. You should be able to find several tutorials.",1530453018.0
The48thAmerican,"You'll get much more comprehensive information and tutorials by googling ""MySQL replication""",1530495855.0
WeaselWeaz,"> (less 5% fulfilment fee)

Uh, what?",1530388701.0
littlecaesarspizza,Can I listen in on the interview? Would love to get some insight from a pro db admin as they review your structure!,1530344627.0
wolf2600,"What is the product in question?  If the company is trying to looking to interview people who use their product, it would help if the product name is provided BEFORE we're required to provide our facebook/linkedin account.",1530358219.0
DataDecay,"Am I missing something in the link, this seems broad, not all databases are the same especially for environment reviews.",1530460585.0
reallyserious,"The points he lists should not be a surprised for anyone that works seriously with databases. 

It's the obvious disadvantages you choose to have when choosing a schema-less solution. There is no free lunch. The whole SQL vs NoSQL debate is more about when you want to pay. With SQL you pay up front and get guarantees on your data. Kids these days don't like that. So they pay the price of having inconsistent data later on. ",1530183502.0
artsrc,"When I buy a product I often focus more on the negative or neutral reviews.

Then you read from people who don't like the iPad because it was a poor umbrella.

That is how I feel about this article.

Sure he had a bad experience, but seriously there are some people who just ...",1530188347.0
bojanderson,"Disclaimer never used MongoDB or NoSQL, but have read about them a lot.

I feel like this is a typical lifecycle for applications. Starting off when large scale change and full design 
 will happen often then the schema-less flexibility that NoSQL offers will be very useful.

However as an application matures, the company institutionalizes, the size of the data grows exponentially and the team grows and the application is less prone to massive redesigns of how data is stored or what new fields are used SQL will offer more and more benefits.

Even for massive organizations NoSQL can still be off huge benefit and many large companies uses NoSQL, but I suspect over time as they grow for many use cases SQL becomes more tempting.",1530187809.0
ibishvintilli,"You are overthinking it. MySQL is of course a DBMS, the data that it has, let's say a customer table, is saved in the disk as a file. If the database is only the place where the data is stored than this would be the file. But they go always together, don't say to anyone that MySQL is not a database.",1530137457.0
johnfrazer783,"'The Database' has several meanings none of which is 'The Actual One': It's a piece of software that helps you organize, store and retrieve data; as such, it's also a brand; it's a piece of software that you can address via sockets, TCP/IP or otherwise; it's a single file (in the case of SQLite) or more commonly a collection of files that may or may not reside in a single location on your HD (or a remote server, as the case may be); it's a collection of data that you only think about in terms of getting an answer for that question of yours; it's also, for some RDBMSes, a self-contained, named logical storage 'location' ('database' is what this is called in PostgreSQL; MySQL calls this a 'schema'), a 'partition' of the grand total of data that is managed by one particular physical installation of the RDBMS.

Being a PostgreSQL user myself, I'm using `psql` day in, day out in the terminal. If I didn't knew better, I'd probably say that 'my DB is `psql`', which is correct in the sense that a full 100% of my data input and output are funneled through that tool.",1530194313.0
mohelgamal,"Think about it this way. a database is like a car, it has several large subsystems in order to function. to the user, the car consist of steering wheel, tires, chairs, doors etc, but there is also an engine somewhere. the engine is important, without the engine there won't be a car. but you can have two engine options for the same car. 

each database has a storage engine and management system. the storage engine is responsible for dividing the data into manageable chunks and allocating them to the disk (usually as files or pages). when you query the data, the storage engine knows where to find what you want. in MySQl, you can have storage engines like Mariadb or Innodb. but this won't matter to how you use it, you can use it the mysql way (as in using the SQL language and have the limitations of the mysql system). google for example offer a database hosted in the cloud called CLOUD SQL. when you subscribe to this service, you are given a choice of using it as a MySQL or a PostgreSQL. these databases in reality have little in common with how a regular MySQL database running on a server would store its info but to you the user, you can use it exactly the same. ",1530163957.0
fern4lvarez,Probably is somethink like innodb for MySQL what you're looking for.,1530139562.0
Old13oy,"Two things: domains and atomicity.

A domain can be defined as a group of related things that all fit under the same column heading. In your example, Transportation Method might be one such domain - the things in it would be car, bicycle, bus, train, etc, whatever the values in the form would be. Your 7 fields are probably your different domains.

Next is atomicity. Every domain needs to be broken down into its smallest possible parts. For example, in the Transportation Method domain, just saying ""car"" might not be specific enough - you might need to specify coupe, truck, 4 door sedan, etc. Whatever the values are, they need to be unique and not overlap with any other value. Once you figure out your domains, figure out your list of values, and make sure they're all atomized into the smallest possible group that you want to deal with.

Note that for fields where there's an unpredictable number of non-standardized values, like name, you may not be able to break it down much at all. That's okay.

Once you have your domains and your atomized values, you can create a small table for every domain that holds all the possible values for that domain. That table should have an ID # for every atomized value in the table. If the values for the domain are infinite or not standardized, skip this step.

Finally, once you've done all this, create a table to hold your form submissions, with one column per field that they'll be entering. For the domains with standardized values, those columns should be populated with the ID #'s from your atomized tables. Non-standardized fields can be put into the table itself.

Hope this helps.

Edit: feel free to bug me with questions too, I'm trying to do more work on web DB's and I could use the practice",1530134966.0
mohelgamal,"This looks like a job for a NoSQL database, like mongo. This would allow each entry to have its unique structure 

Or it could fit neatly on a graph database like neo 4j depending on how you want to query it. ",1530160992.0
wolf2600,"Create a table with a concatenated primary key of both MODE and TYPE.

Then your data would look like:

    MODE (PK)        TYPE (PK)
    ----------------------------------
    BICYCLE          MOUNTAIN
    BICYCLE          ROAD
    BICYCLE          ELECTRIC
    CAR              UNLEADED
    CAR              DIESEL
    CAR              ELECTRIC
    BUS              PUBLIC
    BUS              PRIVATE

.

    select mode from myTable where type like upper('%electric%');

",1530134788.0
alpacahq,"We wrote a post how we created the Go plugin from start to finish in three sections: Installing MarketStore, understanding MarketStore’s plugin structure, creating the Go plugin., and installing the Go plugin.

Btw, MarketStore is a database server written in Go that helps users handle large amounts of financial data. Inside of MarketStore, there are Go plugins that allow users to gather important financial and crypto data from third party sources.",1530128250.0
sunshinelov1n,"SELECT *

FROM [db] 

WHERE [attribute] LIKE 'st%'",1530130605.0
boy_named_su,You could look at PostgreSQL Text Search. It has prefix searching,1530115450.0
serkef-,What flavor? How many columns / rows? ,1530114730.0
Photizo,"In your where clause, something similar to ""like '%st%'""",1530115410.0
sikian,Is this a one-time search or something that's going to run somewhat frequently?,1530123060.0
Turkey_Slap,Look into using the CHARINDEX() or  PATINDEX() functions. I’m on a mobile device or I’d write out an example for you. Hope it helps!  ,1530129993.0
jlrobins_ssc,"*Young database (CrateDB) implements hash-joins, film at 11*.

Vast tracts of blogspace dedicated to showing big rowcounts and an O(M+N) algorithm is much better than a O(M*N) algorithm.

",1530032547.0
littlecaesarspizza,I appreciated this post. I’m a newbie when it comes to databases and this was a great way to jump into the deep end and learn a bunch of new words and retrieval methods!,1530086980.0
EnuffIsEnough,"I am sorry but this is a common knowledge in the database systems community. Hash joins are way faster than nested loops join. The most recent and widely cited paper about the choice of a faster join algorithm appeared in VLDB (Very Large Databases, a premier database conference) in 2013. If you are interested here's the link: [http://www.vldb.org/pvldb/vol7/p85-balkesen.pdf](http://www.vldb.org/pvldb/vol7/p85-balkesen.pdf)",1530033436.0
DataDecay,"So with mysql 8 one of the key features is data dictionaries. Back before 5.7, system tables were in MyISAM. 5.7 was the first version to label these system tables as deprecated with the promise of data dictionaries in mysql 8. These data dictionaries are protected with dd_table_access_check (this can be skipped in debug). Many many people screwed up system tables in mysql in the past which is why Oracle did this. Those ""errors"" are pretty much saying mysql rejected statements to those tables that are not allowed; aka your bacon was saved. You should be free to go about your business.


See below for a list of DD tables.

https://dev.mysql.com/doc/refman/8.0/en/system-database.html#system-database-data-dictionary-tables",1530153020.0
ecrooks,"You cannot. You must run db2 reduce max. You can monitor the asynchronous extent movement operation to see the space being returned: https://datageek.blog/2016/07/12/monitoring-extent-movement-progress/

I think the idea is that most databases steadily grow in size, and never need reduction. Small amounts of empty space within a tablespace are reused automatically. This process is only needed for situation where the space will not be reused.",1530014952.0
Ay--_--ye,I admittedly know nothing about cratedb but this kind of comes across as 'we fixed a bug in our system that caused some joins to run 23k times slower than expected'. ,1529972094.0
mohelgamal,"I am not an expert but that doesn’t seem much different from dbaas like cloud SQL or 
Aws dynamodb  where you just need to start an instance and it will go on its own ",1529852147.0
One_Standard_Deviant,"I'm not a DB admin myself, but I professionally research software and databases as an industry analyst. I was at the Oracle Open World event last fall when they announced their autonomous database, and I have a pretty good view of the company's vision for the product.

Oracle's stance, which they stated clearly, is that the autonomous database is not supposed to replace the DBA role in any way. It is, instead, supposed to lift much of the burden of everyday admin tasks so that DBAs can focus on more strategic, ""human"" tasks such as project planning. Since DBAs are typically backlogged with maintenance and admin tasks, it is common in many organizations for database maintenance to be neglected. Things such as patches are not always implemented as soon as they become available, leading to security risks. Planned downtime for database maintenance is notoriously difficult for DBAs to negotiate in business settings -- users do not want their database down -- so the idea is that an automated database would be able to self-implement common maintenance without any downtime. 

Oracle's autonomous database automatically handles things like patching, provisioning of scale-out clusters, testing and change management of applications and workloads, monitoring and resource management, and correction of failures and errors. The idea here is that it is helping DBAs spend less time doing the ""grunt work"" of keeping the database running on a day-to-day basis so that they can focus on more important things.

What are the more important things that DBAs will focus on, if they're using an autonomous database? That remains to be seen, but the idea is that the DBA will become more involved in shaping IT strategy, and spend less time simply ""keeping the lights on"" -- e.g. keeping the database up and running. 

Some organizations may interpret an autonomous database as an opportunity to cut back on DBA roles, but that view is short-sighted. The autonomous database should be viewed by most organizations as a way to assist DBAs, rather than eliminate them. It is true the DBA's role will evolve if they are using an autonomous database, but that role should not be eliminated.

Oracle's autonomous database is not a sudden, radical capability; databases have been adding in automatic features gradually over the last 10 or so years. Cloud databases in particular, such as Amazon's managed database services, have become highly automated. Oracle's new database is just the continuation of a trend to automate database optimization and maintenance. 

So with this trend, the role of the DBA will continue to gradually change. I don't think Oracle's database specifically is any more threat to admin jobs than any other modern database or managed service. Smart organizations will recognize database automation as a way to put their DBAs to more important, forward-looking tasks, especially since those DBAs will need to spend less time keeping the databases up and running on a daily basis. ",1529866063.0
DataDecay,"As many have already pointed out, the autonomous has to do with the Oracle cloud. Oracle 18c itself isent anything too different from the past releases, it's not as though 18c is out of box autonomous. There will always be customers and clients that won't go Oracle cloud just like not everyone is rushing to azure or AWS.


However if your a DBA and your employer moves to the cloud get ready to shift to a much more functional DBA role.",1530049969.0
jynus,What kind of introduction is this that doesn't even mention non-official tools such as [`pt-online-schema-change`](https://www.percona.com/doc/percona-toolkit/LATEST/pt-online-schema-change.html) and [`gh-ost`](https://github.com/github/gh-ost) ?,1529875885.0
jdizzle4,"I found teamtreehouse good for a few subjects. When I was starting out I used it to brush up on PHP, learn sass, and for learning wordpress development (I had to for a job prospect). It wasn't the only resource I used, but it did help.",1529769931.0
WeaselWeaz,Are these intentionally bad?,1529710236.0
mcstafford,"They're boring as hell until they slow down, or stop working. That's when the excitement begins.",1529711483.0
cowp13,"After 15 years of traditional database, I started working on a pretty big project with nosql. So far it's been nothing but headaches. It just isn't the right tool for the job. I'm sure there are some projects out there where nosql is a better fit than relational db for but I haven't come across it. 

The main problems are 1) schemaless. this can get out of control 2) joins...a lot easier with relational db 3) drivers for nosql arent available for some tools

Instead of asking why nosql is a good option, I'd ask why shouldn't you use relational DB.",1529609155.0
exoticpenguin,"Don't worry about speed as much, since this is turn based, which should give your CPU some time to perform its needed tasks. Unless you're building a behemoth like Civilization, your server should be perfectly fine.

I would highly recommend using just a regular RDBMS. Unless you have a specific reason to use NoSQL, there is no need for it, and you lose out on a lot of useful features from a relational database.

If you're worried about saving state, you can easily make a records table in your DB, and have it keep track of what actions were performed by who, and what the attributes and items used were.

Ideally, you should break things down to their smallest category. For instance, a table for users, one for items, one for matches, perhaps a join table that sets an item to have specific attributes (that refers to an item\_attributes table), and so forth.

The records table mentioned above could even be as simple as:

    CREATE TABLE match_actions(
        id PRIMARY KEY,
        match_id BIGINT REFERENCES matches(id) NOT NULL,
        match_step BIGINT NOT NULL,
        actor_id BIGINT REFERENCES users(id),
        receiver_type VARCHAR(255) NOT NULL,
        receiver_id BIGINT NOT NULL,
        attributes JSON,
        UNIQUE (match_id, match_step)
    );

Here I would set receiver\_type to some type of string or enum, so you can refer to either users or non-playable items/characters. In place of using NoSQL, you can simply use a JSON type to manage any kind of attributes or other state you want to keep track of. The example above was based off Postgres, and from the top of my head. Do not simply copy and paste without changing to fit your needs :)",1529617911.0
The_0racle,Blizzard uses Oracle DB for WoW.,1529609658.0
approx-,"MySQL is probably your best bet.  Free, but it works well.",1529610102.0
,[removed],1529581918.0
erikpurne,losing*,1529578618.0
supra621,"SQLDBM has a beta that lets you break things down into subject areas. Not sure if that’s what you’re looking for, but it is free to use at the moment.",1529537675.0
PandalfTheGimp,"I want to say Red-Gate has a software called dependency tracker that gives a visual on dependencies between objects, so that could help determine what you may break by removing certain objects from the current database.",1529544953.0
phpnode,Save yourself a lot of pain and frustration and just use postgres. ,1529576923.0
alinroc,"Stop using the `sa` account.  **YESTERDAY**. It's generally advised that this account be disabled on all SQL Server instances. You are essentially running as `root`. With the credentials stored in the application. In plain text. This is like 4 layers of bad stacked on top of one another.

So...set up a regular login on SQL Server, grant it only the permissions that it requires, validate that the credentials work correctly (via SSMS) and you don't have any firewalls blocking traffic, and then try again.",1529502882.0
reallyserious,"I love a lot about it but I hate the fact that it's written in the wrong order. I.e you write SELECT first when your IDE doesn't know from where you select so it can't give you any hints.

If it was FROM ... JOIN ... SELECT ... then it could give you suggestions on what columns exists to select.

Also, the syntax for recursive CTEs is far from intuitive. ",1529483647.0
DJDarkViper,"Love: Schemes design
Hate: I sometimes really wish SQL was designed as a general purpose programming language so that basic things like conditionals and loops were super simple and straightforward ",1529480130.0
twenty__2,"I love is so unique. Set oriented programming, powerful and with unique functionalities in a sort of declarative way. The way you have to think and structure your programs are something else.",1529514590.0
release-object,"Hate: Deep nesting. I’ve seen queries with 5+ layers of sub queries. 🤮. It also annoys me when a CTE/sub query pulls a million rows that the main query doesn’t use. 

Love: Data! 
It still amazes me that we have tools capable of crunching  a gazillion records in no time at all. ",1529527285.0
insulanian,"I love that there are no layers, like in web development. It's just data and you. You can focus on solving the business problem instead of jumping through the layers. ",1529527895.0
mikes2123,I love taking a really horribly-written query and tuning the crap out of it. Seeing those results come back in 1/100th of the time makes it all worthwhile.,1529533959.0
jamietwells," As a web developer with a keen interest in writing performant queries, here is what I hate: http://blog.cleancoder.com/uncle-bob/2017/12/03/BobbyTables.html",1529526435.0
redszitall,I hate when there is no data governance in place.  I also hate the way the tool formats your statement when creating views.  ,1529526427.0
mohelgamal,"I find that the databases are the easiest part of the app. The trick is to avoid learning all the ORM layer Magic until you know how to manage the database directly or you will be confused as heck. There is three families of databases 

SQL family (relational databases) : many members all respond to one language called SQL, you can learn that from w3schools or tutorials point. They are for data that fit on tables

NoSQL family: there is more diversity here, but easiest to use is mongo so start there. They are for storing documents where the structure of each item is different or flexible and each item is more or less independent 


Graph databases: they are a special type of NoSQL for highly connected data. Start with Neo4J official website tutorials. They make it so easy to understand 


Then there is all kinda branches of the above, some are purely in memory, some are fully managed (like you start an instance on aws and you don’t need to do any configuring other than little stuff) some try to straddle between different types 
",1529465697.0
burnaftertweeting,I'm working on some mysql videos right now. What questions do you have?,1529447840.0
,do u wanna learn heavy things about databases or just a few things like queries ...?,1529452787.0
petercooper,"Keep an eye on https://dbweekly.com/ each week to see what new sorts of databases are coming out, overall industry trends, etc. Aimed very much at programmers coming to databases.",1529455106.0
welshfargo,https://lagunita.stanford.edu/courses/Home/Databases/Engineering/about,1529601076.0
carlsonivan,"Database Design for Mere Mortals, Third Edition, by Michael J. Hernandez",1529788997.0
techcraver,"Disclosure: I work for Panoply and am hoping to add value here.

An analytical data warehouse is another form of database. 

The good thing about analytical data warehouses is that they consumes data from various analytical databases. See our data [warehouse guide](https://panoply.io/data-warehouse-guide/) that talks about different databases and their role within the data warehouse.",1531423354.0
israellopez,"I think... you dont need a database.

You just need excel.Because if you were doing this in Excel, you would just have a Data tab with all of the Students Sent (-) and Received (+). Then a pivot table for each given school, and if you add up all the + / - numbers, you should come up with a balance per school (# of spots).

Beyond that, a database is great for things like

* Allows you to define ""What is a Student/School/Exchange"" and extend that over time.
* It ensures that data is kept clean (School names are stored once, dates are only dates, not text etc,.)
* Enables multiple users to input/query data
* Complex language to manipulate data, but gives you really great results

Usually problem solving for these kinds of ""business logic"" flows like this.

Try and Model in Excel. Need more horse power? Try MS Access. Dont like MS Access? try Filemaker. Need moar? Try a Relational Database (MS SQL, MySQL, PgSQL) .

But honestly, I'd just try Excel first.

If you feel like you need that multi-user and shared information, you could try Google Sheets, Airtable or even Smartsheets.  All of those work like excel, and could give you what you need.",1529442896.0
carlsonivan,Try Airtable.com,1529789125.0
alinroc,And how much are *you* making for spamming referrals for these interviews across Reddit?,1529372323.0
rose_anachronysm,"If you are using standard build yeah. That is replication factor of 3 on a dc of 3 nodes that are rack aware.

If so each rack will contain a replica of the data. So you'll have two nodes with 100% of the data on two individual racks and 2 nodes with 50% of the data on the same rack.

Which is why you add 3 nodes. So each node on each rack contains 50% of the data.
",1529355160.0
The_0racle,"It depends on how the data is segregated across the nodes, your high availability requirements, and growth trends. Without knowing more about the setup it's difficult to answer that question.

EDIT: It sounds like they're wanting 3 node silos with 1 coordinator and 2 replicas. That's just a guess.",1529353419.0
186282_4,"It's not true, but it is a strong recommendation. We have a 4-node cluster in prod, with RF = 3. It's not ideal, but it's what management decided to pay for.

If you can maintain multiples of 3, it's really the best way.",1529362900.0
techstuffguy,"What type is result? It sounds like it should be an enum: https://dev.mysql.com/doc/refman/8.0/en/enum.html

Enums index fine. How big is the table?

Not sure about resources. Just google stuff.",1529284599.0
DesolationRobot,"It depends on how your queries are structured. 

If you always query on job_id and result then you can have a compound index probably with result first. If you always query on job_id and sometimes on result have a compound index with job_id first. 

This is a good chance to learn how to interpret the EXPLAIN results. ",1529295960.0
colly_wolly,"If you want to know about indexing this site is excellent. 

https://use-the-index-luke.com/",1529315359.0
eson83,What about partitioning over the result column ?,1529330534.0
,[deleted],1529287270.0
DasWood,a single table  with 4 fields? ,1529227432.0
gtderEvan,"Another thought is to have a nightly ETL into a 'data warehouse' of sorts. We'd lose more granular time-based data than daily, but that may be just fine.",1529085414.0
da_chicken,"> I figured out that if there is a single wrong variable in the configuration file, running mysqld --initialize will leave the database in an inconsistent state.

Yeah, that's definitely something that should be complained about.  At the very least, there should be something like a --hardinitialize flag to recover from this sort of half complete startup.  It would be nice if the system would scan the configuration file and do some bare validation before it did anything to the file system, too.

> I wonder why the temporary password is so complex; It could easily have been something that one could easily remember without decreasing security, it's temporary after all.

I'm glad they do.  The surprising proportion of people won't change default passwords.  Now you can't blame Oracle because they chose a weak default.

> A quick look in the MySQL 8.0 release notes told me that the PASSWORD() function is removed in 8.0. Why???? I don't know how one in MySQL 8.0 is supposed to generate passwords compatible with old installations of MySQL. One could of course start an old MySQL or MariaDB version, execute the password() function and copy the result.  
> [...]  
> (Update:: I later discovered that the right way would have been to use: FLUSH PRIVILEGES;  ALTER USER' root'@'localhost' identified by 'test'  ; I however dislike this syntax as it has the password in clear text which is easy to grab and the command can't be used to easily update the mysql.user table. One must also disable the --skip-grant mode to do use this)

The point is that *you shouldn't be directly modifying the mysql.user table at all*.  It's a bad way to do user management, and Oracle is wise to move away from it.  It should be treated like a read-only system catalog table.  You should be using CREATE USER and ALTER USER, which is how essentially every other RDBMS does it.  You do that so you can cleanly separate privileges and cleanly identify when a user is trying to modify server security.  The system administrative tasks of creating users and modifying their properties shouldn't be assigned by granting INSERT and UPDATE permissions to an arbitrary table.  It's the same reason we don't modify table schema by modifying system catalog tables with INSERT and UPDATE and instead use CREATE and ALTER statements.

I'm also confused why you think this is plain text that can easily be grabbed:

    ALTER USER 'root'@'localhost' identified by 'test';

But this is somehow *not*:

    update mysql.user set plugin=""mysql_native_password"",authentication_string=password(""test"") where user=""root"";

The `PASSWORD()` function gets executed by the query engine, not the client. The entire query text is passed in the clear in both instances.  [The doc](https://dev.mysql.com/doc/refman/5.7/en/encryption-functions.html#function_password) even says:

> Caution: Under some circumstances, statements that invoke PASSWORD() may be recorded in server logs or on the client side in a history file such as ~/.mysql_history, which means that cleartext passwords may be read by anyone having read access to that information.

If you really don't want to pass a plain text password to the engine, then you should use the [`IDENTIFIED WITH auth_plugin AS 'hash_string'` syntax](https://dev.mysql.com/doc/refman/8.0/en/alter-user.html) and actually do the hashing on the client side.
",1529068662.0
mcstafford,"* Requires registration
* PDF",1529005489.0
mcandre,"Never use MySQL, use PostgreSQL.",1528985666.0
wolf2600,"2 would be a good option.  Basically build a duplicate DB on new hardware with a fast network connection between the new and old systems.  Then just do an INSERT INTO SELECT FROM and insert the data into the new DB, then delete from the old DB.


Also check with legal.... depending on what your data is and your business industry, you might be legally required to retain the data and keep it available for audit purposes for a certain period of time.",1529004102.0
mohelgamal,It may be easier to tell us the tables and what columns they have instead of exposing your data.  We shouldn’t need access to the data itself ,1528977201.0
supra621,"I have Access installed, can take a look this evening after work. ",1528977085.0
wolf2600,Don't send the data file.  Post an ERD.,1528977712.0
wolf2600,Could you list the requirements and what you have for the schema so far?  ,1529006117.0
digital_af,"I could try it, but don't have Access installed. Is Open Office capable of Access files these days? Or could you draw an ER-Model of your database?",1528954710.0
knnseh,"The choice to draw the lines connecting entities (or tables) is based on the question: 

What new information can you get out of that new line. 

In your example, perhaps patients should be connected with doctors with a M:N relationship (1 doc to many patients, 1 patient to many doc), thereby creating a concatenated (a table with both primary keys of doctor and patient) entity that would have a meaningful name like doctor_patient_records. 

In other words, the line would exist only if there’s new, desirable information to be obtained. 

Another example. Say school has students and student has parents. Would saying parents have kids in school tell us any new information (I.e parent having a line to school)? Probably not. 

Cheers ",1528865629.0
Fienix,"If you want to store which doctor helped a certain patient, you would need a fourth table, probably called Visits, that has foreign keys to patient and doctor tables.",1528866021.0
wolf2600,"Think about the data and what it represents.  Don't focus too much on diagram requirements or following ""the standard"".  You're building an ERD to model your data for your situation.  Build the model that works best for you, not the one which conforms to standards.  When building a schema, start with the requirements:  how will the data be used: what reports, application queries, etc will be run against the DB?  Once you have these usage requirements, you can work backwards to determine how the data needs to be stored in order to be able to fulfill the usage requirements.  Once you have your reporting tables, you can continue to work backwards to design the ETL flows which will take your raw starting data and transform it into the refined data you'll use for your query use cases.  Don't get hung up on the textbook theory, keep your mind set on the practical real-world usage.

There is no inheritance of relations like that unless you explicitly make one.  Will there be a PK to FK relation between Doctor and Patient when you create your DB schema?

",1528895332.0
alinroc,Do you know for certain that Access is what's slow? Or could it be the server on the other end of your driver that's processing the query slowly? Or maybe it's the network? What are you doing with the data when you query it \- just showing it in a table view in Access?,1528826244.0
,"What is data source you're pulling from? That's pretty important. I imagine a linked table would be faster if you're currently copying everything to local, but it has the possibility to lock up your data source.

EDIT: OK, you're *queries* are slow, not your loading. So I assume you are using a linked table? Maybe you could try making a snapshot copy to local every day or so and running your queries on that.",1528826356.0
brantam,"The best solution would be to *avoid* having to extract and load 100000 rows. Instead of processing the data in a client-side application, why not put your processing inside a DBMS and just return the end result?",1528827236.0
dsn0wman,">Is there a better platform than Access to do this task.

Probably. Although I kind of wonder why you would want to read 100,000 rows. I mean your query returns 100,000 rows, and then what? Your going to look at them all or something. Doesn't make any sense.",1528829944.0
WxGeo,oh ya? Then what’s it mean hot shot,1528832748.0
WxGeo,postgresql.. ,1528826068.0
DrewHuber,"[Photoshop](https://onlinefilerepair.com/en/dbf-repair-online.html) stores the files it is working on in a temporary folder on your computer. You can probably recovery dbf, although it might require a little digging to do so.  
 • Open ""My Computer"" from your desktop. Double-click your hard drive.  
 • Open ""Documents and Settings.""  
 • Open the folder labeled with your user name.  
 • Open ""Local Settings"" followed by ""Temp.""  
 • Scroll down to the files beginning with ""[Photoshop](http://www.ebay.com/sch/625/i.html?_nkw=photoshop)"" (""PS"" for older [Photoshop](http://www.ebay.com/sch/625/i.html?_nkw=photoshop) CS builds). This will be followed by a series of letters and numbers.  
 • Double-click each ""Photoshop"" (""PS"") temp file to open them in Photoshop CS. This is the only way to identify which temp file contains the project you wish to recover. Luckily, because when Photoshop shuts properly it deletes its temp files, there should be no more than three or four.  
 • Select ""Save As"" from the ""File"" menu when you find the temp file you want. This will make the file permanent.",1529881092.0
anras,"It's too bad they can't be updated automatically, and lack incremental refresh. :(  
  
Edit: Query rewrite would be nice too.",1528762763.0
R4bbidR4bb1t,Couldn't you use ASP.Net either webforms or MVC and the Entity Framework to write to both XML and the database? ,1528729692.0
CarefullyCurious,"Do you have any columns in the raw data table which you can reference, for example a unique key?

If so then just create a separate mapping table, link to this as a fk, and add any data you like. If you want to also filter this by customer, create a separate customer table too and add the customer pk as a fk to the mapping table. This incidentally also allows you to do many-to-many mappings if you like.

If not then you could use a hash function on the data in the main table and use this as the fk, but this will of course be MUCH slower.",1528702348.0
wolf2600,"So what you're saying is the source that provides the customer data is starting to provide columns which might be unique (the column name) to only a single customer record?  And other customer records might have their own unique columns as well?

The first question is how these additional columns are used in your database?  When considering database design decisions, always start from the end-use, the goal of what you need to get out of the DB, figure out how to provide the refined data, then work backwards to figure out how to produce that refined data from the raw source data you're ingesting.

If you don't have clear requirements on how the data will eventually be used, you can't make an informed decision on how to transform/store the data.

Is a user going to pull the details of a specific customer and want to see a list of all the one-to-one mapping values?  Or is there one specific mapping that will want to be seen for all customers?",1528720841.0
grauenwolf,"> 1) The obvious way is to use joins but suppose we have 10 mappings in the future. That implies joins with 10 tables. I can't afford to do that in db.

Why? Tables are cheap.",1528702788.0
grauenwolf,"> 3) I maintain one table for all customers with columns CUSTOMER_ID,COLUMN_NAME,COLUMN_MEMBER,COLUMN_MEMBER_MAPPING. This solution is the best I can think of rn 

That's by far the worse solution. 


1. It is highly error prone. People will screw up the SQL when trying to use that table.
2. You defeat basic database protections such as foreign key constraints. Which means you need to manually add integrity checks that are periodically run.
3. You lose the ability to properly index the table. This, combined with [2], makes the database work harder.
",1528703134.0
grauenwolf,"> 2) … but that would increase a lot of space which is not currently an issue(tbf I don't like this approach).

No it doesn't. Generally speaking, mostly empty columns are cheap. You are probably wasting far more space by using the incorrect data type somewhere than you would be adding a few extra FK columns.

Don't micro-optimize for space at the cost of your data model.",1528703271.0
MarkusWinand,"My response from that time:

https://use-the-index-luke.com/blog/2016-07-29/on-ubers-choice-of-databases",1528696367.0
mosaicorange,"Why would you share a blog post that’s 2 years old, especially when both MySQL and Postgres have had major version changes ",1528669860.0
swenty,"I would look at using logical replication to copy the prod data to analytics DB, and not have a separate replica DB. Pushing down queries over a FDW to execute optimally remotely is a work in progress in Postgres 11. Until that is much further along, many (most?) queries will execute more slowly over FDW than when run on the home stored data.",1528668731.0
welshfargo,Does the replica have any indexes? Are the statistics current?,1528663873.0
glennhunter,"This method how to repair access database may help if your database is already corrupt or in case neither article nor mentioned links can't help you. [Here](https://www.databasejournal.com/features/msaccess/article.php/1494111/How-to-Recover-a-Table-Deleted-from-an-Access-Database.htm) you may find solutions that are probably more effective than others.

Before doing anything else, make a copy of the corrupted mdb file while Access is NOT running, and without overwriting any earlier backups. This lets you try different approaches and sequences if necessary. Next, try the built-in repair utility. This very simple solution may work with corrupted indexes, and might even get rid of a corrupted object:

• In Access 2010, click Compact and Repair Database on the Database Tools ribbon.

• In Access 2007, click the Office button (top left), then Manage.

• In Access 95 - 2003, choose Database Utilities from the Tools menu.

If this does not work, [follow the steps](http://www.access.repair/) for the symptoms of your corruption below. 

[https://www.restoretools.com/accessrestore.html](https://www.restoretools.com/accessrestore.html)

[http://allenbrowne.com/ser-47.html](http://allenbrowne.com/ser-47.html)",1529170215.0
welshfargo,http://www.databaseanswers.org/data_models/,1528569414.0
apc0243,"Not sure what ""hosting visualizations"" means because R has a couple different plotting libraries. ggplot2 tends to be the default used to generate most plots, but they're static and tend to be stored in image formats. I suppose you could generate one and then send the image or maybe open up a stream, but i'm not sure if that would allow you to interact with it. If you did, it'd likely be very slow. 

If you can sidestep the IIS then Shiny is a very lightweight R visualization server that you write in R and is pretty robust. 

But outside of that, I'm not sure what R offers, it's not meant to be a webscaling visualization platform by my knowledge. 

Hopefully someone else can give better input. Maybe reach out to r/visualization or /r/Rlanguage for specific info, doesn't really seem like a DB question. ",1528473998.0
ilimanjf,"Definitely look into shiny. R Studio made this product for this very purpose. They offer a free server that you can install and manage yourself, and they also offer a managed cloud service. The free service does not support authentication out of the box (last time I checked)  so be mindful of this, anyone on the network can access the visualization. The managed service offers it, but requires a monthly/annual subscription of sorts to get this feature. If you are dealing with proprietary/sensitive information please consult a security engineer  to ensure the server configurations are hardened. ",1528508499.0
mcsoftc,I would be glad. Sending PM.. ,1528416205.0
StatisticalOutliar,Sending pm,1528427425.0
Caphiped,"Been reading into many SQL blogs and material lately. If this can help me further with acquiring SQL knowledge, I'd like a free copy. PM incoming.",1528432375.0
crashdmx,"Hi,  I'm interested !!!",1528472463.0
Great_Wall,"I rate it 5 stars.

I have no email.

Gibs me anyway.",1528415819.0
Dolphinmx,"is difficult to recommend something without knowing anything about the schema and how is used, how many tables, if they are exactly the same.

Sorry the answer is not useful but is not easy to comment on something that there's not many details.",1528432480.0
DesolationRobot,"""Users"" is one table. ""Jobs"" is another table. And ""start/stop"" or ""events"" or whatever you want to call it is another table.

Your ""events"" table is starting, pausing, unpausing, and finishing a job. So that table is user_id, job_id, action (start, pause, unpause, finish), and timestamp. Now you can see how many elapsed days it takes to finish a job (finish action vs start action) and how many total manhours it takes, etc.

You will have to be conscientious about inserting pause and unpause events--like at the beginning and end of a workday.

Also user_id+job_id must be unique. Every time they start a new job--even if it's the same type of job they've done before--it gets a new row and id from the jobs table.",1528399867.0
r3klaw,"It's not specifically made for this kind of thing, but I believe Plex allows you to tag audio metadata and search it via a web gui.",1528384485.0
grauenwolf,"I used SQL Server and MapDotNet. Just downloaded the flood map GIS data from the government and imported it into SQL Server. Then attached MapDotNet for server-side rendering and a JavaScript framework for the browser. 

Unfortunately that was several years ago and I don't recall where I got the flood maps. They were in a standardized GIS format, so they would work with any geospatial database.",1528303357.0
gschizas,"I wonder how well this article would go in /r/programming, or /r/java or /r/dotnet or /r/python, or even worse, in /r/webdev 🙂",1528301762.0
HeWhoWritesCode,"Best db design I worked with was where the employer got a postgraduate(read cheap labour) for the specific field/industry to sit down with our ""dba"" and design our schema without writing any code first.

While I believe if we contacted someone in the field with more experience, we might have gotten a better design. We could not afford a expert.

Also I like to keep my db design in python sqlalchemy models, because it really support advance relationships and datatypes. 

So normally the first thing I do when I inherit a db is to code-generation these models. And from there modify the models and diff the old and new db using liquibase.

This allows me to save my models and sql in version control easily and doing a diff on py code is a lot easier then doing a diff on sql code imo.  ",1528300022.0
grupwn,"I always try to use a DB-first approach when possible for a number of reasons, but my favorite reason is that some of the development tools I use support code scaffolding off of the data model, which cuts a ton of boilerplate code out of my workload and allows me to tweak it to my needs and then focus on business logic.

I also enjoy developing data models in a DB much more than I do building java/python/etc. classes for it.  If I can just build my DB model and have a bunch of those classes built for me, it's a win in my book.",1528384027.0
mohelgamal,"I think your best bet would be to use survey monkey which would allow you build a form the student can submit the info in and it would gather it in excel sheets for the teacher to review. 

Another solution would be spreadsheets shared over google docs or Microsoft cloud. Where each student would fill in their info in an online file viewable by the teacher. it is not a pretty solution but it is one not requiring coding skills.  I believe Microsoft access has a mechanism to have you create an online form the students can submit to populate the database. 

You can also look into App Store to see if someone created an app for that purpose


I am assuming this is being done on a small scale (10-100 student at a time). Although survey monkey can handle bigger loads 
 

If you are talking about something bigger, then you may want to hire an IT guy or a firm that can handle “full stack web developement” to build you a web app. This would consist of a “backend” server, usually on a hosting service. And a “front end” which is a website the students can access on their phone, each with their unique log ins and you would have teacher view that can see the students. 


You can probably learn the necessary skills to build a rudimentary web app with a backend yourself in 3-6 month if you are willing to put the time and effort in. You would need to learn something like a LAMP stack 

Keep in mind survey monkey or open google docs is not HIPAA compliant so you can’t have patient info their. ",1528274516.0
mohelgamal,"Glad to help, I am actually a surgeon with a hobby for IT stuff so I understand what you need. 

Full stack web development just means someone capable of doing both the front end (website or app) and the back end (the sever that will receive and store the information submitted). 

Give Microsoft access a look, I think it has the capability to have you create intake forms going into the database. This would likely be the only solution that doesn’t require you to write code on your own.


If you want to design something yourself then here is hundreds of possible ways to do it. All will require a good degree of knowledge of programming. It would be a fun learning experience. Where to start is a long story. But easiest intro to programming would be python for the back end with a database like mongo dB with html/JavaScript for the website 

",1528318393.0
ebauman,RedGate sell a solution for this: https://www.red-gate.com/products/sql-development/sql-compare/index,1528214643.0
mcsoftc,"Visual studio 2015 have a comparison and sync tool for databases, works pretty well. I use it all the time ",1528212838.0
alinroc,"It's built into SSDT, which you should already have in Visual Studio. [http://www.cjsommer.com/2016\-06\-23\-comparing\-databases\-using\-ssdt/](http://www.cjsommer.com/2016-06-23-comparing-databases-using-ssdt/)",1528212943.0
boy_named_su,http://www.liquibase.org/documentation/diff.html,1528214930.0
ppafford,https://sqitch.org,1528250832.0
diwanharsh,Redgate has a tool for doing exactly this.,1528267609.0
DesolationRobot,"I haven't seen numbers on it, but I wouldn't be surprised if the exodus to cloud databases has reduced the number of DBA jobs has gone down drastically.

Do you want to be a DBA long term? This job you're taking about seems like it would be a better stepping stone to an analyst position rather than a DBA position. ",1528161103.0
bengalfan,"An even better stepping stone is working in the network admin field. The DBA is a mesh of network and software, I’ve seen more people jump from a network position into DBA. ",1528174519.0
dsn0wman,"Sysadmin or Developer leads to DBA more than anything else. I would run screaming from any job that wants you to know Access. Either the position is below your technical skill, or worse, you work for a company actually storing valuable information in Access or using Access as some kind of wonky front end interface.",1528208699.0
thechosenblerd,"The way to start would be to create an E/R Model and figure out how you want all of your tables to interact,",1528143838.0
EverydayExploring,"My work team uses Airtable for a moderately complex database, and it covers about 95% of what we want in a database management system - I'd certainly recommend it for the project you described. It isn't going to be able to do everything full-featured RDBMS's like MySQL or Postgres do, but with the tradeoff being a far simpler process to build a working database. Airtable lets you create/modify/delete tables and fields, set up entity relationships, and add excel style formulas with a simple interface. The free version even has version tracking on each record for the past two-weeks. Still better (IMO) is that Airtable has a fantastic API with amazing documentation generated for your database (with lots of examples), in case you plan to connect the database to some external application. 

As for the case you describe, the Airtable way of handling that would be:

* one table for contact source of truth, called it *contact* , each record representing a unique individual.

* add *contact_ID* as a foreign key on another table, call it *tblB*, with a field on *tblB* where the field type is ""Link to another table"".

* join fields from *contact* to *tblB* with a field where the field type is ""Lookup"" on *tblB*.

Edit: formatting",1528145386.0
Cagurtay,Are you sure this is the correct place to ask? Maybe you should try /r/datasets/,1528115790.0
klasius,"You have to build some ETL program to merge the data.
You can do it on T-SQL, use the SSIS platform or write it on your favorite language.",1528384505.0
jmkni,Firebase?,1527792798.0
notqualifiedforthis,"PostgreSQL, MySQL, and SQL Server Express",1527792021.0
carlsonivan,Airtable.com,1527926772.0
codemaster,"Access is compatible, yes, but I would only use that when you are prototyping.

For something user-facing, please do not go with Access.

My last experience with Access, it is best for single-user applications as it has a lock file and is slower than any traditional RDBMS.",1527734212.0
dessmond,"Please no Access. Are you out of your mind? :-) Access has its own sql dialect, is not acid compliant, and  scales up to one user.",1527747873.0
baptizedbycobalt,"I’m going to go with not Access.

I love PostgreSQL, and it’s a hell of a lot better than Access.

That being said, I wouldn’t discount MySQL if you’re just starting out. It’s very well supported, and there’s a ton of documentation and online resources available. I find it a bit easier to work with.",1527740583.0
merlinm,postgres!,1527737563.0
chocotaco1981,"every time you use MS Access, God kills a kitten.

please, think of the kittens.
",1527768732.0
mkingsbu,Pgsql 100%. ,1527736892.0
Caprikhan,"Thank you all so much for your answers and advice, I hereby solemnly swear to not mention Access as a multi-user tool on this sub again ;) 

And I'll probably be back sooner than later with more newbie questions... Or to show off my first big DB achievement (one can dream)!",1527769492.0
welshfargo,You might find [this site](http://www.databaseanswers.org/data_models/) useful for working out your database design.,1527777739.0
GFandango,skateboard wheel vs Ferrari :),1527922640.0
boy_named_su,"Both. Use MS Access as a front-end for a real database (Postgres)

Access *is* useful for making rapid GUIs",1528305992.0
mkingsbu,LibreOffice Base (a free equivalent of MS Access) can do that.  ,1527702591.0
hokie47,"I would look at zoho CRM, but if you are looking at a basic database I would take a look at MySQL. How big is this company you are working for or is this for a school project? I ask because this is not a small task at all, and usually companies will spend big $ on getting something like this right. This is not a task a company should take lightly.",1527702826.0
codemagic,"I don't think it's inventory you are monitoring, but employee vacation time and some mysterious 'items' that get replenished(at different rates), issued to each employee. If you want to call these items inventory that's fine, but I would call them assets if these items are not going to be sold to a customer. I would recommend setting up a database in either MySQL or Access, and create some tables like Employee and Asset. You have a relationship between these Assets and Employees, which can be handled using an association table that tracks the intersection of Asset to Employee, where you can also put a start & end date to show the duration the Assets are good for. Reporting on this can be handled a variety of ways, but if you have a good relational backend you have a range of choices on reporting.",1527711716.0
slouch,Knack,1527729495.0
bumbelbumbel,Airtable,1527738835.0
NoNotTheDuo,"The GUID in ROWGUID stands for Globally Unique Identifier.  It's usage is described in this link: https://forums.asp.net/t/1248405.aspx

Basically it's a unique identifier across tables/databases/globally.  Therefore, it makes sense that trying to join on this column between two tables will result in no rows returned.",1527700879.0
drizzt001,"Just out of interest, which RDBMS are you using? Is it SQL Server, by any chance?",1527701785.0
newsagg,"I'm convinced that GUID is created in products to help the NSA keep track of 3rd party intelligence assets. It's very common feature in all enterprise software but it never has a defined use, the documentation just kind of glosses over it, and 99% of the time using it makes things more difficult to maintain and debug for no benefit that you couldn't get by a generic lookup table where the values can be made actually impossible to collide and can mean something. ",1527712620.0
spitfiredd,Tl;dr. No analysis provided.,1527688330.0
grauenwolf,"Well that was a waste of time. I'm not convinced the author has ever used SQL Server or PostgreSQL. 

",1527699499.0
grauenwolf,"> Moving on, PostgreSQL has **indexable functions feature** will boost the performance of the database to another level. 

What is that? I can't find anything on it so I don't know if that's a real feature, a clumsy way of saying ""you can index a calculated column"", or something he imagined.
",1527700505.0
grauenwolf,"> As SQL Server is a Microsoft’s product, it only runs on Windows and developers having Mac or Linux cannot work on it. 

Uh, what? Seriously? Microsoft has been talking non-stop about SQL Server on Linux for the last couple of years.",1527700589.0
grauenwolf,"> Not only that, PostgreSQL has modules or extensions support and you can do a lot of things that SQL server is incapable of.

Apparently the author doesn't know about .NET extensions for SQL Server either.",1527700705.0
info_dev,"In the one production application I've built on Postgres, the answer was sadly 'no' :\(

Had been wanting to use it for a while, but it quickly started to have issues with queries inexplicably timing out: [https://dba.stackexchange.com/questions/195023/postgres\-occasionally\-slow\-to\-query](https://dba.stackexchange.com/questions/195023/postgres-occasionally-slow-to-query)

I've run similar loads on MySQL and SQL Server in the past with no issues, so, until I can explain and fix that issue, it's a show\-stopper for me.",1527699094.0
grauenwolf,"> Partitioning is also important from the scalability point of view. As the application scales higher, the database will become large and if it is not split, it can become larger and accessing data will take a lot of time.

No, that's not how this works. There are many important reasons for portioning such as managing data archival and moving rarely used data to slower drives. But performance isn't one of them. Simply dividing your data across two tables on the same drive is no faster than putting it in one big table.

> As for SQL Server, there is a proper partitioning feature but you have to buy the feature as an add-on whereas, in PostgreSQL, you get it for a lesser price and with more efficiency.

You can't discount a feature just because it costs money.",1527699662.0
grauenwolf,"> SQL Server has underdeveloped concurrency and you are sure to get various locked, blocked, and deadlocked reports in the log. This causes mismanagement of data and the processes of the applications get very slow. In this comparison, PostgreSQL has a better concurrency management system and there is less chance of deadlock situations due to its optimization MVCC feature.

1. SQL Server has had row level versioning since 2005. While not MVCC, it does address the same ""writes blocking reads"" issue. (There are significant performance trade-offs with both MVCC and row versioning that you need to be aware of.)

2. SQL Server allows you to turn off row level versioning when the trade-offs aren't in your favor.

3. SQL Server does have MVCC. It is used in the ""in-memory tables"", which can offer amazing performance for very specific access patterns.

4. SQL Server offers read-uncommitted mode. While I usually don't recommend it, it allows you to ignore locks when appropriate. (PostgreSQL only offers read-committed or better.)
",1527700184.0
grauenwolf,"> The technologies are updating faster than ever. In such a scenario, SQL Server’s approach of releasing a new version after a few years is outdated. PostgreSQL releases updated version regularly and they keep up with the trend for offering faster performance.

Having more frequent releases doesn't make your code faster than a more established product. Why would you think that? 

",1527700362.0
grauenwolf,"> JSON and JavaScript are ruling the web world and PostgreSQL has support for JSON. You can sync the client, server, and database properly but SQL Server is still stuck on XML. 

SQL Server has JSON support as well. ",1527700619.0
klasius,"Worst arcicle ever. As people are writing, all arguments in post are wrong.",1527780709.0
getoffmyfoot,"My opinion is Oracle is headed for a nose dive. Oracle’s bread and butter is big enterprises that are slow to change. But they DO change, and when they do, more and more will go to the cloud. Between their pricing practices and licensing entrapment stunts they pull, and not being a strong player in the cloud, I really think their days are numbered.",1527688223.0
grauenwolf,"Honestly, I find it hard to predict where the market will go because I can't predict how stupid my clients are going to be.

For example, one potential client wanted an estimate for a million dollar project to rewrite everything to use an 8-node Hadoop cluster because SQL Server was starting to struggle under their workload.

Turns out their SQL Server was running on an 8-core box, no more powerful than an old desktop I picked up from BestBuy about a decade ago. If they put it on a proper server and gave it some RAM to use, it would have lasted them a very long time.

Other clients insist on using their database as a glorified key-value store and won't touch any feature that isn't exposed via the ORM. So no full text search, no spatial indexes, no temporal tables, no windowing functions, etc. etc. 

They will, however, pull back massive object graphs that include every column from every related table. No, I can't ""just throw some indexes on it"". ",1527698437.0
grauenwolf,"Another issue is the stalking horse, PostgreSQL.

While SQL Server is picking up awesome features like JSON, graph DB, and column store, PostgreSQL is working hard to catch up on performance. It's still got a long ways to go, but each iteration puts more and more companies into the ""PostgreSQL is fast enough"" category.

Honestly, the only thing keeping me from making PostgreSQL my default is that I don't have a good deployment story for it like I do with SQL Server and SSDT.",1527698576.0
grauenwolf,"That's hardly a reliable source. According to their methodology, ""MySQL is a horrible database"" counts as a point but ""SQL Server Columnstore is awesome"" doesn't.",1527697936.0
hughk,I don't like Oracle that much as a company or a product but it takes a lot to displace big databases. We use SQL Server only where it is bound to an external product.,1527716015.0
,[deleted],1527692091.0
popeus,"I would combine the measurement and timestamp table into a single table. This table would have a pk, a date time stamp of the measurement and the two measurement columns. This table would also have FK relationships to station and sensor type. # of rows won't be an issue for any relational db. ",1527544314.0
Plentix_ICO,Im planning to do this also.. Thanks for advices.,1527560752.0
swenty,">It feels very odd to me to create so much tables, am I right doing so?

No, that's not the right way to do it. Instead add sensor_id and station_id as columns in the measurement table. All the measurements would go into that table.

It could be argued whether the measurements at different rates should go into different tables. I think maybe I would do a separate table for each rate, only because the amount of data collected will vary by several orders of magnitude. But you could also make an argument for adding rate as a column in the measurements table.",1527560825.0
DasWood,checksum?,1527578724.0
mkingsbu,I'd probably use PGSql to be honest and leave the syncing requirements to the application.  Especially if retrieval is important; my understanding is the NoSQL databases have great performance writing but not so hot performance reading. I might be misunderstanding what your requirements are though,1527464608.0
mosaicorange,"Have you looked at [OrientDB](https://orientdb.com)?      
What about [MySQL](https://www.mysql.com) and it’s new Document Store?      
",1527454263.0
boy_named_su,Check out symmetricDS for cross-brand db  syncing,1527535169.0
Plentix_ICO,Nice article.. Very interesting,1527487377.0
LeftJoin79,"To eliminate data redundancy (redundancy causes you to have to keep two copies of the same data up to date). If it's something like a contacts table and the person can have multiple email addresses but the address can only belong to one person, then that is many to one and could sit in a multi-valued attribute, and could hang off a related table. If it's something like a company table and a companies attorney table, a company could hire many attorneys, but each of those attorneys could work for many companies. That is a many to one relationship. Stuffing it in a 2 table structure table would cause redundancy.",1527422133.0
nonoisfirst,A multivalue attribute is inherently a 1:M relationship. ,1527427284.0
popeus,"Look up a ""type 2 slowly changing dimension"". It's a data warehousing technique but can be used in 3nf databases as well. In effect you just date range every row in your attribute table and each time a change is required it changes the current rows end date from infinity to the current second. The next row then starts at +1 second.

Regarding the historixal querying, it's best to do this separate to your app db, it's a trivial task for a data warehouse schema but difficult to be pefromant in an app db. ",1527293164.0
starnakel,"[http://www.aosabook.org/en/500L/an\-archaeology\-inspired\-database.html](http://www.aosabook.org/en/500L/an-archaeology-inspired-database.html)

For inspiration :\)",1527324828.0
QuirkySpiceBush,"Is this an app for your own personal use, or will there be multiple users, say via a website?",1527300551.0
ErroneousFrog,Take a look at the Data Vault approach for designing and building a long term data store. It will probably be a better fit for your requirements (of which I'm making several assumptions) than using type-2 SCDs.,1527322445.0
ramborocks,"Create a changes table to store data. Create a trigger to on insert update delete place on the changes table. Make sure to have a datetime column on the changes table to track the insert date.

Now when u need to query historical data you'll use changes... And go off the insert date?

I've only been doing sql for two years... I wanted to post my thoughts before reading others. I'm interested in seeing what others would do. Goodluck! ",1527985292.0
InvitedAdvert,"Son, wait for PostgreSQL 11, which has much better partitioning support. If you have db2 luw, it already has time travel queries with plenty of examples out there.",1527293096.0
jevans102,"I'm no expert, but the foundation of the database of an enterprise application has this concept. I'm not too familiar with how the application efficiently uses them, but essentially all tables (chunks of multiple related 1t1 data points) have an effective date.

The way I see yours working has an effective date (or year/quarter combo if you dont care about dates) for every attribute set. Changing even one element creates a whole new row with a new effective date with all the same data except the one data change.

From there, you'd just need to implement logic to get the greatest effective date before today to get the ""current"" row. All rows before that row are past data. All effective dates after today (or whatever your target date is) are historical records.

I'll PM you the app name if you'd like. There's a ton of documentation about the high level workings of what I glossed over.",1527301760.0
DBAFromTheCold,How long have you worked for Couchbase? ,1527287425.0
thenickdude,"Yes. At the very least this will reduce the amount of data that needs to be transferred between your database and your application, which can be significant for large SELECTs. If future columns are added to the table, you'll end up selecting those too with a SELECT *, so the query would get slower over time for no good reason.

In the best case, the columns you need may be satisfied by a covering index, so the database will not even have to find and read the row on disk (the query can be satisfied using just the content of the index). This can be a massive speedup.

That being said, if you want 28 out of 30 columns, I'd just go ahead and ""SELECT *"".",1527242822.0
r3pr0b8,"> Is it best practice to query only columns I need? 

yes

> It's definitely a lot of work in cases where I need 28/30 of the columns.

there are ways to help with this process",1527239408.0
messyjesse_,"Yes it's best practice to only query columns you need, and not just for the performance or ""creep"" considerations \(for a growing table\), but also for readability. It's going to be a lot easier to understand what you were trying to accomplish when you can refer to a tailored list of columns.

SELECT \* can be helpful in testing, but that's the only place where I implement them, and even then I think it's less preferable than just adding some of the columns used for logical processing \(like anything involved in a JOIN or a WHERE\) on top of the columns you wish to display.",1527254089.0
liquidpele,"Yes, but people seem to obsess over it too... unless you are dealing with a professional production system where you need  every performance gain you can get it doesn't really matter... but it is good practice.  ",1527254605.0
,"What Db do you use? In T-SQL I know you you can join system tables and use dynamic sql to automatically give you a list of all the columns in a table (you don't have to do this yourself each time, just search stackoverflow and copy the solution).",1527248895.0
diwanharsh,"It's always best practice to select the only columns you need.

I can explain at least one reason.

Let's say you have a non clustered index in the column in the where clause.

If you use select * , SQL Server nerds to go to clustered index or table to get all the other columns.

If you have only selected columns and those columns are covered by index then server may get all the data it needs in the index itself.

This can save a lot of work in case of a frequently run in query.

So it's almost always better to use only those columns you need.",1527251496.0
catchergg,"Leaving performance aside.

Explicitly stating columns is the way to go. Otherwise, adding columns to a table can lead  to unexpected behavior from your application.

Consider the following SQL:
Select * From Person.
... 
Somewhere in your code there's this:
Int id = resultSet. getInt(1)
String personName = resultSet.getString(2)
String personLastName = resultSet.getString(3)

After a while, you've altered the table and added the column MEDIAN_NAME just before LAST_NAME and after FIRST_NAME.

The code above will now get median name instead of last name.

The correct way was to define:
Select ID, FIRST_NAME, LAST_NAME From Person;

That way the change in the database above won't affect you at all. ",1527286263.0
merlinm,"It's very simple.  Use SELECT a,b,c when you want columns a,b,c, and use SELECT * when you want *all* the columns.  There's a simple test; imagine adding column 'd' to the table...if you have to go and add it to the query, you should generally (not always) be using SELECT *.  If not, you should be using explicit columns.",1527249784.0
markwusinich,"For development/exploring you can use select *; However for production you are always better off explicitly listing the columns.

One of the only advantages of using select * would be that if new columns are added or old columns removed, then your program would be flexible enough to manage that, but unless you are just dumping the data to an output, you probably want the query to handle this change as early as possible. ",1527256815.0
mkingsbu,"It depends on the scenario.  I use select * a lot for munging, which is one of my primary responsibilities. But if I were setting up a performant service, I'd use exactly what I needed for sure.",1527265051.0
vrsuresh,"https://en.wikipedia.org/wiki/Time_series_database#List_of_time_series_databases

How is this better than KDB? ",1527206334.0
DJ_Laaal,"I'm interested in trying this out and I wanted to do some basic tests by using the docker image first. 

Issue1: As per the pull command sample on your docker repo page, running that command gives an error:

 docker pull alpacamarkets/marketstore
Using default tag: latest
Error response from daemon: manifest for alpacamarkets/marketstore:latest not found

Looks like your docker image is missing the ""default"" or ""latest"" tag. Or esle, update the docker pull command to include the specific tag meant for the latest version.

Issue#2: I looked up the tags on your image and found v2.1.2 as the last modified one (as of today, June-7th-2018). Decided to use it:-

docker pull alpacamarkets/marketstore:v2.1.1
v2.1.1: Pulling from alpacamarkets/marketstore
image operating system ""linux"" cannot be used on this platform

Note:: I've docker for windows installed on a Win10 Pro PC, with Docker set to use Windows Containers. Does your docker image not support Windows containers? Might want to make it clear on your GitHub repo page.

I'll give it a shot again later with Unix Containers and see how it goes.",1528405047.0
JoelBenett," I don't have enough experience in searching for solutions or articles on the Internet. But I had to use some search engines like Google, Yahoo and Bing and what I dug up there... [http://www.dbf.repair/](http://www.dbf.repair/) \- as I found out from contacts of this resource, group of programmers created it, but I'm afraid of doing it immediately, because it is new thing for me. What opinion do you have about this masterpiece? Can I use it for my case? If someone applied it, share your thoughts how it works and how much it might be effective repair dbf file.  


Unfortunately, I didn't find any resolutions from Microsoft \- I can't realize why such huge corporation doesn't care about safety of data of their customers like me, like you. This question for me now doesn't have an answer. If someone owns this information, inform me, please.  


I wasted several valuable hours of my spare time and I've no idea who or what could return me these very hours. Pats, I hope this time or next time you'll be more sympathetic to problems of strangers on your community. ",1527541512.0
billyh1919,"1\) Restore the file from a valid backup copy.  
2\) In case of no backup,  take a look here: [how to fix dbf files](http://www.dbf.repair/)",1527523186.0
Daniel_0926,Define real-time database,1527089328.0
assface,"Postgres with [LISTEN/NOTIFY](https://www.postgresql.org/docs/current/static/libpq-notify.html)

Next question.",1527091333.0
jenkstom,"echo ""my data"" >>my_file
",1527100358.0
brantam,"Both Microsoft SQL Server and Oracle have streaming data support.
Also worth a look:

www.osisoft.com

www.abinitio.com/en/system/continuous-flows
",1527104925.0
craigkerstiens,"It would be very helpful if you could better elaborate what exactly you're looking for. Realtime means a lot of different things to different people. Some mean a streaming system like Kafka, others are talking about aggregation over a lot of data for customer reporting. For the latter we have a number of people that use Citus that way \([https://www.citusdata.com/blog/2017/12/27/real\-time\-analytics\-dashboards\-with\-citus/](https://www.citusdata.com/blog/2017/12/27/real-time-analytics-dashboards-with-citus/)\)",1527458810.0
hi117,Also hard realtime or soft realtime?,1527089870.0
real_redditer,"As a solution, I created a new login and then linked that to the new dB login I created.
And voila! Worked! 

Just incase anyone encounters this error.",1527099991.0
macdull123,"
tidb",1527635703.0
vai_singh,"Hi I read your post. And would like to hear more about the situation. For normal advertising data, we also use MySQL, and it should not be an issue for your use case too. Important thing is to make sure you have your data is received, validated, and processed efficiently. And query tables are light. We can talk on Skype, and maybe we can share few pointers here and there. ",1526960209.0
redhatb2b123,"super blog nice thank you for sharing ........................

Website: [https://redhatb2b.com/](https://redhatb2b.com/technology-users-list/sage-users-email-list/)",1527074355.0
Oxford89,This is hilarious.,1526935412.0
drunkadvice,"I'm thinking, this can't be serious, this can't be serious...  Then I saw block chain and I knew for certain I was seeing the future.  Please take my money so I can be a first adopter in this technology.",1526938778.0
Hertweck,"Huh... Well, points for creativity for sure",1526945055.0
cgfoss,"why am I reminded of this?  https://www.youtube.com/watch?v=b2F-DItXtZs
",1526998509.0
simtel20,"Congratulations!  You are either about to become a distributed systems nerd, you are about to buy more hardware, or you are going to learn about database tuning.

Without you telling us more about your query patterns, your indexes, and your pain points, here are some approaches:

1. You are probably *not* going to need to become a distributed systems nerd.  You want fast consistent queries, so you don't need to go to the real-time streaming approach.  

2.  No solution is horizontally scaleable unless you build your application to work within its operating model.

3. Anything can hold 200gb of data, but the types of queries you're running determine what databases can make your application run efficiently.

I don't run any medium sized postgres DBs, but I would first suggest that you look at the most recent postgres releases to see if they will address your issues.

Second I'd look at analyzing your queries and your indexes to look for poor performers and optimizing for them.

Only if those utterly fail to bring you back to a performant baseline would I suggest that you look at the sql-ish databases that may be able to help you if you're willing to understand how they work:

1. [cockroachdb](https://www.cockroachlabs.com/)
1. [clickhouse](https://clickhouse.yandex/) if you're doing column-oriented queries
1. [foundationdb](https://github.com/apple/foundationdb).  Before apple bought the tech, close-sourced it, then open-sourced it, there was a [sql-layer](https://github.com/jaytaylor/sql-layer).

There may be other options too.

Here's the caveat: you're asking for a solution but you're not actually describing the problem you think you're having, just ""slow""l.  I say this because with the amount of data you're dealing with I honestly don't think these very large-data solutions are going to be a drop-in fix for what really ails your databases (and I don't know what the real problem is).  Each of these come with their own strong and weak points, but it is really rare that if you tap postgres for all the performance it can give you, that anything is going to be both faster and easier to run.

But there you go, use those as a starting point if you must.  And remember that if this is really a business that needs these things to work, you may just want to spend a little coin and get one of the postgres maintainers' consulting companies to come in and help you before you start throwing man-months at a new database.",1526905008.0
jtwyrrpirate,Since you’re already on Postgres you might want to have a look at PostgresXL,1526904154.0
jinqueeny,"Given the fact that you are already using Postgres, I would strongly suggest you gave CockroachDB a try. However, if you would like to even consider the MySQL protocol, I strongly recommend TiDB (https://github.com/pingcap/tidb). TiDB is an open source distributed scalable hybrid transactional and analytical processing (HTAP) database. There are many use cases with large amounts of data. Here is one use case with close to 100 nodes handling dozens of TBs of data. For your reference: https://www.pingcap.com/blog/Use-Case-TiDB-in-Mobike/


Disclaimer: I work at TiDB.",1527154472.0
ppafford,Have you looked at AWS Aurora? https://aws.amazon.com/rds/aurora/,1526904237.0
pdp10,"I agree that the first two steps are going to be (1) tune queries and everything else, and (2) throw hardware at the problem.

What's the database server hardware? Operating system and exact/kernel version? Most crucially, what's the memory size and the exact I/O setup?

Do you have a development version of this database set up so you can try changes without affecting production?
",1527293871.0
markwusinich,"These are just rules of thumb, each database/application pair requires its own considerations.

For databases that track and monitor transactions, FK are good.

For your reporting database, you can go ahead and do the joins to have human readable values. 

The thing I would pay most attention to are: 1, at the very least have a place to store documentation. Very little of your system will be documented (sad, but true) unless it is required by government regulations, but for those portions where someone does create documentation, give them a central place with common sense format/folder structure.

2, build tests.",1526880364.0
taeylorswift,I would advise not joining on purchase information. Join on an identity column or anither candidate key that can provide uniqueness to each record,1526883216.0
taeylorswift,Doing the aforementioned advice will create indexes and speed up your querying,1526883260.0
weekend_golf,"Another thing to consider is scale. If you have 10k orders, your reports will run fine. If you have 100MM orders those joins might give you issues. This is a good reason for some denormalization. Additionally, keep in mind that there is a cost for FKs which is incurred on insert/update. ",1526883432.0
valcroft,"The good books on database design would indeed be something I'm interested in coming from this thread.

That being said, I see no problem with you using foreign keys instead. UUIDs would be what I'll use instead of integers. 

Some indexing would be needed if you think you'll always need to access the purchase data, which would then still allow you to use the foreign keys.

And then when you run into performance issues... that might be the time you'll consider denormalizing. Depends on the complexity of your code I suppose. 10 tables aren't really that many.",1526903377.0
welshfargo,For reporting purposes use views to simplify the queries for the reporter. Use materalized views if performance is an issue (assuming that you already have appropriate indexes defined).,1526912563.0
pcsag,B2B Database Providers - B2Bdatapartners provides high quality b2b marketing data to businesses across the world. We cater the best b2b data list for better campaigns and ROI.,1526873369.0
alinroc,So you want to re-implement Amazon's & Azure's database services? That's what their portals more or less do.,1526829543.0
cowp13,Datagrip might be what you are looking for but it doesn't do mongodb. No tools will do both traditional and nosql as far as I know.,1526830220.0
_Zer0_Cool_,"DataGrip plus Docker. 

(For accessing and maintaining, respectively).

DG supports most popular DBs and with Docker you can get any type of DB as a pre-configired image.

DataGrip supports these DBs...

- Postgres
- MySQL / MariaDB
- SQLite
- Oracle
- MongoDB (via plugins)
- Derby
- Sybase
- Azure
- Redshift 
- IBM DB2
- H2
- Exasol

And has other plugin features for

- Docker
- Terminal / bash
- Python
- R
- JavaScript

...And many other features 


Edit: DG also has quite a few DB management tools built in. You can do stuff like auto-generate schema DDL code or copy an auto-formatted connection string to use with another programming language. ",1526841519.0
s13ecre13t,"There are other concerns

**Wouldn't your nosql still support indexes?**

If your non sql data storage engine supports indexes, then just double check how those work. String searches might be indexed and this could help you deal with products that share same prefix. For example the tshort '0375585024' could be prefix to variations, like '0375585024-xls' or '0375585024-black'. So in your no-sql an index could help with starts with type of string searches. 


**What does your product ID mean?**

Are your products IDs made up (as you in control them) or are they something that is natural (for example: UPC or EAN codes)? Very often how product id is derived can steer you into a specific design. 

Some people prefer natural ids, as they don't obscure the data, and across all other references in other places it is more obvious what the id means.

Other people dislike natural ids, as they can lock you down. Especially if what you though is natural id: isn't.",1526775638.0
IHeartPi-E-,Lol this is not free. They still charge $15.....,1526744124.0
DesolationRobot,">I am not sure if I should be using MySQL or some NoSQL variant

This depends more on what you want to do with the data after you store it. Either will store it just fine.

>combine them by timestamp key-value

As in you're doing some logic on the retrieved data before you insert it into the database, or you want to be able to go back and join data from similar time ranges together after you've logged it?

> how I would automate api calls and throw the returned jsons from multiple sources into the DB?

cron?",1526662617.0
wolf2600,"DB selection depends both on what your data is and how you plan on using it.  

The JSON data:  Do you plan on parsing it and entering the parsed values into the DB, or are you going to store the JSON as\-is?

What do you plan on doing with the data?  Will it be processed/transformed?  Queried \(how and for what purpose\)? etc....",1526664376.0
mabhatter,"Check DNS. 

Being horribly cliche, but maybe the hostnames got swapped somehow. With so many similar names in oracle clusters maybe they crossed paths. A bunch of the bits that make it go are all virtual names dynamically assigned. 

If its not a cluster, check all the TNS entries (or the other ways) still match up. I’ve spent a day fighting with one entry on a copied connection that was just “name2” instead of “name2.net” when both are valid but the connection didn’t like it because it didn’t exactly match. ",1526706744.0
spellsword,For clarity this is oracle,1526657430.0
IAMNOTACANOPENER,Wait so you're not sure if the database you've been working in is the same database you're connected to now?,1526666127.0
simtel20,Oh my I haven't seen most of those words in 8+ years.  Do you have any filesystem snapshots that could potentially help you?  And do you have support for the DB?,1526665647.0
jtwyrrpirate,"Try running Postgres on your Plex server, and if that doesn’t work out you can move to RDS without changing much.",1526604689.0
crookedkr,"While postgres is a fine choice and frequently my goto for something like this I might go with sqlite. You'll be up in no time, backups are a breeze, and there is a ton of tutorials/online help.",1526636952.0
getoffmyfoot,"I agree with postgres. It has all the features you describe you need. It’s free and simple to use, and has great tooling.",1526605396.0
Cabelitz,Search a bit about Heroku. Free online database that might suit you.,1526606328.0
welshfargo,Just get [LibreOffice](https://www.libreoffice.org/discover/base/). Your needs are not complex enough for PostgreSQL or similar solutions.,1526651312.0
Himecchi,"Really not sure why you wouldn't just make it id = 5? May not necessarily match up number to words, but it could save you a lot of pain in the future. You're really asking for trouble down the line in so, so many possible ways.",1526589818.0
thenickdude,"Add a new column that controls sort order, and sort by that column where this information will be displayed.

If you do end up changing the primary key, definitely avoid using float to forestall any accuracy problems. Use the decimal type. ",1526590625.0
WeaselWeaz,"As stated by others, add a SortOrder column to serve that purpose. You wouldn't use a primary key specifically for the issue you're seeing, you would break other tables referencing the correct Category value. With a SortOrder column you could change the sorting whenever you want without causing issues.",1526604723.0
supernoma350,"I would keep the id column as it is, and just make it category id of 5.  If you need to sort them then just add a sort_order column and populate that 1-5 in the order you want them sorted.  ",1526590878.0
eshultz,"The ID itself shouldn't have any inherent semantic meaning. It's just an identifier. It will be much easier if you just set it as 5, rather than changing the datatype.",1526605298.0
somethingintelligent,"I would not recommend updating the current values - it’s best practice to leave existing data as it is. You might not know if the values are hard coded in database objects or applications. 

As already suggested, adding a new row with an ID of 5 should resolve it. Alternatively, add a nullable sub category ID column if you want to expand the data set.",1526592082.0
roachman14,Have you considered the issue of floating point accuracy? See this website for more details: https://0.30000000000000004.com/,1526595461.0
mothzilla,"This sounds like a very bad idea. Why not add a field called ""order"" or something like that which represents the intended use?",1526595698.0
msiekkinen,You have a lot of answers to your real question so I'll be that guy and ask why are you running 5.1? Legacy system you have no control over?,1526606345.0
mgdmw,"As others have said, let it be 5 because while you see the underlying values nobody else ever should. It’s meaningless outside the database level.

In fact, I recommend making keys be GUIDs instead of auto-incrementing integers because it saves a world of pain when merging databases and again, because the actual values should not matter to any user.",1526606366.0
NickPadgett,"Possibly - depends on how it's being used further down the lifecycle. 


I'm not a database expert, but some programming languages expect integers, and will truncate the remainder (or round/floor). Even if the code is built to handle doubles/floats, comparison of decimals in code is often done incorrectly and will create issues. 

Better to leave it as-is, in my opinion.",1526589513.0
boy_named_su,"Unique Keys, or Unique Constraints can cover multiple columns

A Primary Key is a Unique Key that cannot be null",1526581889.0
space_sounds,Have you looked at using a composite key? would save some data from not having to use the id column as well.,1526581656.0
alinroc,"`ALTER TABLE ADD CONSTRAINT Unique_Poster_Post UNIQUE (user_id,post_id)`

At this point, it may as well be your primary key and you can drop the `ID` column if that's all it's there for.",1526586700.0
liquidpele,...  did you even TRY google?,1526591639.0
JacobMoorgan,"There is a method below, how to repair dbf file. Another way to fix dbf might Google search query for special instruments - there you may find this one guide and others.
https://www.fixtoolbox.com/dbffix.html
http://www.dbf.repair/
Try opening the .dbf file in Excel or other program that supports opening the .dbf format.
Once opened in Excel you should be able to view the file, delete any ""bad data"", and resave to a new name (with the .dbf extension).
Also possible to create a new table with the structure of the old table, append records from the old to the new. May salvage records until the corruption is encountered.",1527547563.0
Sparkybear,"You don't need a database for that. Look at the top comment, you can query through the Discord API directly, without going through your bot:

https://stackoverflow.com/questions/47454876/get-total-number-of-members-in-discord-using-php",1526513906.0
_Zer0_Cool_,"Sometimes you SHOULD piss off the rest of your team lol.

TL;DR There are a lot of good reasons to put logic in the database and a lot of valid reasons not to. 

Like any other humans, Devs are opinionated and reasons are often based on superstition or social convention. The choice to put logic in the DB should depend on the situation and context.

**PROS:**
*Encapsulation -- Like anything else, encapsulation is good in SQL. Treat Stored Procedures, views, etc.. as data API's. 
It's pretty dumbfounding that devs that follow best practices in every other way avoid encapsulation in SQL. 

*Performance -- A well written query will pretty much always outperform an ORM. ORM's are often crutches for those who don't feel comfortable with set-bases logic or have little SQL skill. Additionally, writing a Stored Procedure avoids data shipping, network latency, and round trip calls to the DB (http://database-programmer.blogspot.com/2010/12/cost-of-round-trips-to-server.html)

*Centralized Code base -- The DB is it's own code repo. Likewise, the code in DB's can be reused in any application that touches the DB across an enterprise. This follows the ""Thick DB"" paradigm.

*Longevity -- Databases will ALWAYS outlive applications that are built upon them. Often the Data IS the business and apps just serve as a way to collect that data. 

*Data Activities --  Behind the curtain of the DB, there is a whole world of data. SP's serve not only to support applications, but to serve up long term supported for a companies data structure. If you step into that world you realize quickly that the rabbit hole goes pretty deep and that applications are the tip of the iceberg. The data is often the true lifeblood of an enterprise. 

NOTE: I am a full time DB developer, I know a few languages (Python, Java, JS, R) but my current job involves 90% SP's that never see the light of day. Most devs in my organization have no idea what goes on in the DB. Their collective head explodes when I show them. Like... ""Wow, I thought you guys just wrote quick SQL that supported our web apps"". Nay, I'm supporting a codebase to manage corporate data that will last 15 years.

**CONS:**
*Vender Lock-in -- Oracle/SQL Server, yikes! Expensive. What if you want to switch DB's? Truth be told this only matters on a small scale operation. Wholesale data migrations from one DB to another are quite rare (even then, leave it to companies like EnterpriseDB to help with that). In enterprises at least, the database will probably outlast the tenure of your employment and will almost certainly outlive your application by ten-fold.  Writing logic in the DB makes sense when the data will last for 20 years when an app has to be rewritten every 2 years to meet modern web standards. In this way, DB logic allows web app languages the flexibility they need to ""keep with the times"".

*Cost -- With commercial vendors like SQL Server etc.. you are paying for CPU cycles. Putting EVERYTHING in the DB is a bad idea from a cost standpoint. Also, stuff like calls to web-API's should be used sparingly only be seasoned devs to avoid the DB hanging on pending requests.  

*Limited OOP programming capabilities -- SQL is **always** better at handling data, but not good at modeling UI. DB's like Oracle and Postgres are OORDBM's (object oriented RDBMS) w/ PL/SQL and PL/pgSQL, but these OO features are not up to snuff compared to external languages and may not necessarily be able to match to objects in external programs.

Note: You can now write SP's in a variety of languages. JS, R, JAVA, Python, Ruby, Bash, Perl etc.. in PostgreSQL and a subset of those in Sql Server, Oracle, and MySQL (future). 

P.S. There are more pro's/con's I'm sure. This is just off the top of my head. ",1526476903.0
codemagic,"There are few use cases that demand stored procedures, but the place where it makes good sense is in recursive queries such as a hierarchy or dependent object traversal. ",1526444688.0
firagabird,"Fellow backend (PHP) programmer here that deals with the database (MySQL 5.6). I've long shied away from stored procs and general programming in the DB layer, but it's always been instinctive rather than rational. I have that same off-putting feeling when I think about using a function in SQL rather than the application codebase.

That said, I do make heavy use of foreign keys (FKs), views, and triggers. These make absolute sense within the context of an ACID-compliant relational database. The value of FKs should be self-explanatory. Views are more or less essential for abstracting complex queries with many JOINs between normalized tables in a performant way.

Triggers are basically mini-functions that (at least in my DB models) preserve ACID compliance in cases where a simple FK and ON UPDATE/DELETE cascade isn't enough. I find them especially useful for aggregation tables, which I use as high performance aggregate views.",1526447683.0
kavisiegel,"If you were going to query the database with the result of a query from the database, stored procedures are your thing

On my projects that use them, I write .sql files that get committed with CREATE OR REPLACE FUNCTION so anyone can update the function in code and other developers just run the sql files. Make sure to use comments in the sql file. Add a comment in the backend code that tells future developers where to find the .sql files and what the stored procedure does ",1526447092.0
beliasas,"I'm working with a large team on huge projects, and using SQL Server on a daily basis. Views and stored procedures is what we do for a living. Practically zero logic is defined on the web server, except to authenticate the user, securely pass the parameters to the SQL Server, or provide a custom logic. Everyone is comfortable with it. It is especially useful when you have to deal with different platforms (Web and Windows applications), and have to reuse the same logic in them.
I would never want to go back to the regular ""client > web server > database"" way of doing things. Using web server as a thin layer,  and handling logic in database is simply amazing.
So, to summarize this, views and stored procedures will be incredibly useful tools under your belt, just make sure that parameters are properly handled (SQL injections, etc.), and avoid RBAR (row by agonizing row) and similar concepts for better performance. Think SQL!",1527238028.0
DasWood,A stored procedure would allow you to return a result set that a database admin would be able to tweak as necessary. So it has it's benefit. But you probably don't need it for small databases.,1526453774.0
pcsag,"best contact database provider company — With years of professional marketing experience to back my claim, I can say that the following database companies have always stood the test of time when it comes to effectively generating B2B leads",1526437916.0
dbaderf,"A starting point for your research.  https://en.wikipedia.org/wiki/Time_series_database
",1526434842.0
simtel20,"Graph database: This data is not a graph.

SQL database: Depending on how frequently the data changes, you could use a sql database this way.  If ""current state"" matters, maybe you could use triggers to implement temporal tables/data.

Otherwise, if the data over time is the most important thing, look at time series databases as /u/dbaderf mentioned.

Also, if you're just writing reports, and you're not trying to operate at a huge scale or anything, you may want to just think about using hdf5+your language of choice to read and write the raw data and create your reports.",1526475248.0
mohelgamal,"I think you need to clarify the nature of data first to be able to define which model works best. 

In the generalist if terms here is how I think about it. Imagine you have to do this by hand without computers: 

- if you would put the data in tables, go for a relational database such as MySQL (for up to millions of entities) or Postgres (for billions of them/ complex queries). 
Example of this type is an item list where each item has a name, an, id, a price , a quantity. 

- if you would write separate sheet/ folder for each item with different fields from each other then try NoSql database usually the easiest is mongo but if you will deal with lots of data look at amazon dynamo. 
Example would be technical specs of different type of products where the specs of a tv is different from the specs of a microwave so you can’t fit it in a table 

- if you get a big sheet of paper and start putting items as little circles and connect them to each other with lines. You need a graph database almost prominently Neo4j 
Example for that would be a family tree or social network where the focus of the database is how each item relates to other items. This can also be used to define a picture of which user buy which item kinda thing 


In all these databases you can time stamp each entry with one or more time stamp fields and almost all databases allow query by the time stamp so you can count the entries, etc 

If your changes are expected to be very fast and mostly focused on the time, you can try time series database 


If you care to elaborate the nature of your use case we can give you better advice 

",1526477998.0
alinroc,"For that sort of thing, you're probably best off looking for a company that does ""DBA as a service"" type work. Staff augmentation, DBAs for companies that don't need someone 40 hours a week, quick hit consulting/troubleshooting things, stuff like that.

I'm sure there are quite a few companies that do this, but two that come to mind which may be in that realm are DB Best and RDX.",1526407629.0
nivenkos,"Zapier were hiring a few, bloody hard though.

Gitlab also hire remotely IIRC. ",1526412906.0
assface,"1. Read the latest proceedings of VLDB and SIGMOD.
2. Find a paper that interests you the most.
3. Implement their techniques on your own. Figure out what works and doesn't work.
4. Create an improvement on that paper.
5. Write your thesis.",1526383830.0
falafel_eater,"The field of Databases is generally concerned with techniques for storing, compressing and retrieving various forms of data, as well as performing queries over it. Additional considerations relate to performance, scalability and fault-tolerance/recovery from errors.  
""Big data"" is a term with no clear definition and is not, by itself, a field of study. *Data science* might be what you're looking for.  

As with almost all active fields of study, there are countless research topics.  
Speak to your thesis advisor and ask for help on where to start. Although /u/assface brought very useful advice, remember that you should not just arbitrarily pick whatever paper from conference proceedings and make that your thesis. Thesis advisors exist for a reason, and they will help you choose a line of research that should be probably consistent with your current ability.  
It also helps (read: borderline crucial) to work on a topic your advisor is interested in or is knowledgeable about. 

So add step 0: ""Speak to your advisor"" and then follow the other steps listed.",1526398895.0
wolf2600,"If you don't have a question/topic in mind, why did you choose ""big data"" or ""database""?

Usually your master's thesis is based on a topic you've been studying and which you have familiarity with.  If you don't even know the topics in the field, why are you writing a thesis on the subject?  The purpose of the thesis is a culmination of the years you've spent researching a certain topic.",1526397142.0
Dazocs,"There no reason to split the beer types into separate tables.  Have one table for all beers, and then have one of the columns in the table contain a value to distinguish the domestics from the imports.

For example:

create table beers (beerid integer, beername varchar(xx), beertype varchar(xx))

In the beertype column, you would place a code to determine if it is an import or domestic. Then if you wanted to see the imported beer, your query would look like this: select beername from beers where beertype='import code'. Of course, you would devise your own code to represent the imports. 

I hope that helps.",1526334755.0
HawkeyeGK,"Off the top of my head

tblBeer (BeerID, BeerName, fBeerStyleID, fBreweryID, TastingNotes)
tblBeerStyle (BeerStyleID, BeerStyleName, BeerStyleDescription)
tblBrewery (BreweryID, Country, City)

and if you want to get fancy and model a many to many indicating which beers are available in which states

tblState (StateID, StateName)
tblDistribution (DistributionID, fBeerID, fStateID)",1526355774.0
Daywalker24007,"You would want those as attributes for your beer/product table

What is your high level goal? ",1526334383.0
yourdeadbeatmom,What are you building on?,1526347501.0
bears-eat-beets,"I would also reccomending setting up something like this in a ledger style to track inventory. 

It's really easy to have and ""onHandQuanity"" column on the tblBeer and then to update it as you add/remove stock. But it's a trap. Because you will never be able to answer basic questions like ""How long have I had beer x in stock"" or ""what was my most popular beer last Thursday?""

To avoid this, you'll need to create a table (called tblInventoryMovementLedger, or something of the sort). besides primary key and auditing columns, it will need beerID, eventDate, movementTypeID (FK to a table with movements such as restock, sale, gift, damage, adjustment, etc.), quantity (positive for stock and additions to inventory, negative for sale, damage, etc.), TotalValue (either the wholesale cost of resupplying that quanity or the cost of the transaction, positive for credit, negative for debit, zero for adjustments/damage, etc.) ",1526364824.0
codemagic,One owner’s import is another owner’s domestic depending on the location of the store. Domestic/local can be a derived attribute which can be determined based on the beer country of origin and the store country. ,1526397268.0
welshfargo,"You might get some ideas from here:
http://www.databaseanswers.org/data_models/
",1526398451.0
cowp13,"I came from 15 years of database development (mostly mssql). few years ago I decided to want to learn python. Now I used it for everything....from web to data to whatever. I wanted to love golang but comparing to python, feel it's still many years behind.

I don't know if python makes your life easier being a database developer. I'd say it make life easier for everyone. Language is a pleasure to work with and there are libraries for everything.",1526292775.0
da_chicken,"I currently use SQL Server exclusively, so I tend to use PowerShell and SSIS.  I'm sure I could find uses for Python, but so far it's just not been something I've seen a need for.
",1526299718.0
gnubyter,"I started using python because of MySQL and visual foxpro. I learned that I could use an ORM for projects that required their own databases, and it would allow me to switch to another database later almost effortlessly.

Python made my database management easier because all the coding was quick; there is no low level stuff to trip over. On top of that, python has so many tool kits that if I need to pivot or add functionality I can do so on a whims notice. For example, add a webstack, or add a GUI front end, or create an excel sheet, or create a pdf - things along those lines.

If you’re a really good developer looking to make sense over the data, there’s also so much machine learning and deep learning that it’s hard to argue that any other language can compete(maybe R).",1526297228.0
codemagic,"The idea of making my prototype work repeatable and more easily transferred into the DevOps way of doing things is what got me into using Python in my database work. The PETL package is a great swiss army knife of data manipulation, which I use in a variety of ways; excel import to DB, data cleansing, extract from DB to a dataframe for Pandas, and a quick and dirty profiling tool. the pandas-profiling package is mostly aimed at Data Scientists but I like running it to save me from running a battery of SQL it would take to get the same insights. Python can help you get unstructured data scraped out of images and pdfs using the textract package. Can’t recommend Python enough for database development. ",1526306282.0
Phnyx,Check out the dbt library for python. It's a great add-on for managing table dependencies for ETL tasks and more.,1526312820.0
ryati,"I am actually the opposite of you. I started off as a python developer and have switched to database development.

I would recomend just playing around with python3 in general to get a strong footing with the language. Python2 looks like the sun is finally setting, so focus on python3. After you get comfortable there are a few ways you can go.

If you want to look at ETL type tasks, check out [airflow](https://airflow.apache.org/).

If you want to try some python data science, look into [jupyter notebook](http://jupyter.org/) combined with numpy. You can to a lot with these tools.

There are many more great tools, so don't be afraid to look around.",1526314346.0
bewalsh,"Python is amazing. So many outstanding free tools: pandas, scikit, sql alchemy, scrapy, beautifulsoup. I mean shit even the jupyter notebook ide is amazing.

But in an mssql role: good luck getting your corporate team to adopt it.",1526348160.0
mkingsbu,"I'm a database ""analyst"" now, although I primarily build databases. Our organization mostly works with SQL Server, though I have leeway to use whatever technologies I want so long as the people who use the data can access it. I use Python for all of the ETL processes that I build. I find that it is easier and more robust than attempting to try to use some kind of procedural SQL language. Especially since there are so many modules that Python can use to directly talk to a database.  

I also recently used it to create a Powershell compiler of sorts. Basically, I needed something to talk to my Outlook 2016 instance, so I have Python build and then execute a PowerShell command and return the results in a JSON object.  

If you're at all creative, you can definitely see a great number of uses for Python!",1526936918.0
mkingsbu,"psycopg2, as in the Python module?

Check out this StackOverflow answer: [https://stackoverflow.com/a/10147451/4637721](https://stackoverflow.com/a/10147451/4637721)

Basically you can prepare a single statement and then use parameterization.  psycopg2 has what is, to me, a weird performance issue, thus the recommendation in the linked answer to mogrify a giant insert statement and then execute that.

the ""for x in tup"" can be replaced by opening the csv file and skipping the first line if it has the header",1526272261.0
swordstaind,"you could do it with 4 tables one for customer, one for states (this could also just be a data filed on the link table) one for product and a link table that tie the other three together.  the three main tables would be your standard info with id fields then in the linking table you would keep the three id's for the values to be excluded i.e. user id 1 state id AL / 1 product id 30",1526247564.0
kellermaverick,"One question, then one of two approaches: 

Do your excluded states always fall into identical groups? In other words, if you sell snow shovels, ice scrapers, and snowblowers, you may always exclude FL, GA, AL, MS, LA, SC, NM, AZ. That makes for a cleaner process. 

1) If that's true, you can a group / area / region table and assign a group to each state in your state table. Then, on your product table, add a column for excluded / included state group (or use an approach similar to (2) below but with a shorter ExclusionGroups table and joins by product ID and group.

States
_____
id  group
AL 1
IL 2
...
WV 3

2) If the states excluded are not always consistent, you can build a lookup table with a PK that combines the product ID and excluded state for products with state exclusions. You could then left outer join by state and product ID and select rows from your left table where both are null values. 

ExlcusionStates
_________________
product ID ExclState
0001          FL
0001          GA
0001          AL
0002          CA
0002          WA
0002          OR
....
0999          MN
0999          WI
0999          MI
",1526275289.0
grauenwolf,"> My concern is that this method could become slow when having a lot of users and songs, and that there will be a lot of queries because every time a user clicks on a song it will have to check if entry exist and every time a song will be clicked it will need to sum the number of views.

That's going to be stupid fast. 

By that I mean so ridiculously fast the database's optimizer isn't even going to think about how to best fetch the data, it's just going to do it.

> very time a song will be clicked it will need to sum the number of views.

    SELECT SUM(views) FROM SongViews WHERE SongKey = 1


Is this fast or slow?

If you have and index that sorts by SongKey, it will be really fast. Possibly even stupid fast depending on the number of users. Databases love doing this kind of thing.


Want to make it even faster? You can create an ""indexed view"" or ""materialized view"" (same thing) from the above query. This will make updates slightly slower and reads stupid fast and cheap.

",1526275025.0
mfigueiredo,"If you want to implement the number of views add total_views in the song and every time it's played, increment it.",1526216575.0
notqualifiedforthis,"I would have a table for recording views but skip the check for user and song.  Use a time stamp field for tracking when they “viewed” or played the song and you’ll be able to report on things like daily plays, average plays, plays in last X days, etc. This is a pretty basic table and if you use indexes it should be fine for awhile. ",1526224063.0
jpers36,"Additional to the other recommendations, you could store the view table as you suggest, also store the view count on the song table, update the view count on the song table on a regular basis by calculating from view table, and report from the song table.  Downside is that the reported value can lag the truth.",1526257249.0
BinaryRockStar,"    User
    ----
    UserID, int, PK
    Username, varchar(100)
    ...

    Tag
    ---
    TagID, int, PK
    Text, varchar(max)

    UserTag
    -------
    UserID, int, PK, FK -> User.UserID
    TagID, int, PK, FK -> Tag.TagID

    Post
    ----
    PostID, int, PK
    Title, varchar(max)
    Body, varchar(max)
    ...

    PostTag
    -------
    PostID, int, PK, FK -> Post.PostID
    TagID, int, PK, FK -> Tag.TagID


Get all posts for a user based on the posts tags and the users tags:

    select p.*
    from User u
    inner join UserTag ut on ut.UserID = u.UserID
    inner join PostTag pt on pt.TagID = ut.TagID
    inner join Post p on p.PostID = pt.PostID

You could reduce the number of joins with some smart denormalisation but that would be premature optimisation unless you're talking about millions of posts or tags.",1526202119.0
in_n0x,"Your task is to write about someone else's experiences? 

Do your own homework.",1526196483.0
bla4free,"The easiest solution is to simply cut down the size of your script. Essentially, cut it up into two or three parts.",1526083954.0
msiekkinen,"The insert query is over 4,194,304 bytes in string length.   See https://dev.mysql.com/doc/refman/5.7/en/packet-too-large.html

Options:

Increase max_allowed_packets

    set session max_allowed_packet=20000000;

Insert in smaller chunks


Even if bumping the upper limit it's doing just that.   You probably want some kind of sanity limit for bytes sent over the wire as well as records in a single transaction. 

Note: the set session only applies to the current client connection.  If you want to make it stick for every connection you'll have to up that variable as a global and want to persist it in your startup config file for when your DB restarts.   From what you described that might not be an option so session might be the way you have to go every time. ",1526094906.0
alinroc,"Importing that amount of data via `INSERT` statements is probably the least efficient way to move data between databases and that amount of traffic will probably send up a red flag if you're successful.

You should do a bulk import instead. If you're using MySQL, [use mysqlimport](https://stackoverflow.com/a/2811128/1324345)",1526119683.0
r3pr0b8,"upside: easy to store any kind of data

downside: ridiculously difficult to assemble all pieces of information for an entity in anything resembling an efficient query",1526087635.0
anras,Here’s some old asktom discussion - https://asktom.oracle.com/pls/asktom/f?p=100:11:0::::P11_QUESTION_ID:10678084117056,1526096346.0
AQuietMan,https://www.red-gate.com/simple-talk/opinion/opinion-pieces/bad-carma/,1526175508.0
bleoag,"It is a good reporting tool but NOT a good database. Usually, DOMO pulls data from a database like SQL Server.  I wouldn't recommend using it as the DB",1526092213.0
chowmeined,"Domo is not a product built on a strong data analytics foundation. We use it at work and I have a number of issues with it.

* It does not handle data relationships well. No concept of dimensions.
* Even basic things require breaking out ""beast mode"" formulas.
* The formula editor breaks easily, and doesn't provide any information other than ""Invalid formula"".
* The ""cards"" (reports) break intermittently with little to no explanation other than ""An error occurred"". Sometimes they start working again, sometimes they don't.
* Their pre-built reports don't actually link to your data. They are all built on sample data, which makes a nice demo, but if you want to actually use them you have to re-wire them to your own data sets.
* The web interface is sluggish, as is data processing. Especially their ETL and SQL data transformation processes.
* It is quite expensive for what it does, and the support is quick to refer you to professional services (even more $$$).

On the plus side it is pretty. And it has a lot of built-in SaaS integrations (of varying usefulness, but many decent).

Have you looked at SiSense? Extremely fast and really good relationship handling with their elasticubes.",1526097809.0
Paratwa,"Domo is okay sorta as a tool, I like it solely because its competition for Tableau which helps Tableau grow better / stronger. But given a choice between the two I’d use tableau almost every time, then maybe SSRS before Domo. ",1526099000.0
coake,You should use https://www.trackvia.com,1526162499.0
DesolationRobot,"I run piles of stored procedures on schedules. I have each log out to a log table when they started and then update that row with a completion timestamp and completion status. Some have error-handling built in so they log out the explicit error. It's a nice way to do it if the reason a SP may fail is 1) out of your control and 2) may indicate something else in the source data that needs to be fixed.

So log table is a good solution.

But have you tried purposefully forcing the error? Put a dummy line first thing in your stored procedure that does an illegal insert (same unique key twice, inserting to a table that doesn't exist, etc). Or revoke the procedure definer's insert permissions. Something where you can see if you run it directly it causes an error. How does your backend report that to you?

",1525974775.0
grauenwolf,"I wouldn't worry about it. You'll get an error message if the operation fails.

But if you are concerned that it has to be 100% verified, open a new connection and read the newly inserted/updated records. ",1525965062.0
anras,"A few millions of records is small. I have more than that in my Oracle database that I use for learning/experiments that runs on a [NUC](https://www.cdw.com/product/Intel-Next-Unit-of-Computing-Kit-NUC6CAYH-mini-PC-Celeron-J3455-1.5-GHz/4480723?cm_cat=GoogleBase&cm_ite=4480723&cm_pla=NA-NA-INT_DT&cm_ven=acquirgy&ef_id=VX9xYwAAAN5ra6hK:20180510133637:s&gclid=Cj0KCQjw28_XBRDhARIsAEk21FjoefM-nJ5BO0EmKylvIQ8S1Qe3eqGeD3KG8hL-aQpUsnSHTsfHWGQaAnOfEALw_wcB&s_kwcid=AL!4223!3!198551352378!!!g!394403552047!) at home.  
  
As far as your question, that's really more of an application architecture question. You can do all those things you're talking about with a client/server architecture, a 3-tier architecture, or whatever.",1525959534.0
mvelasco93,"1kk iw not that much. What matters the most is the engines optimization for your case.  Postgres, Maria db,  mysql should work fine. How fast a query will execute depends not only on the database but also the way the query is and how indexes were made. ",1525961030.0
Buzzard,"I would double check the method you are using to check table size and make sure it includes the [TOAST data](https://wiki.postgresql.org/wiki/TOAST).

e.g.  https://www.postgresql.org/docs/current/static/disk-usage.html

Edit: PostgreSQL also has a [JSONB data type](https://www.postgresql.org/docs/9.5/static/datatype-json.html) for efficient storing/indexing/querying of JSON data which might be useful.  SQLite does have a [extension to help](https://www.sqlite.org/json1.html) with working with JSON too.",1525910960.0
grauenwolf,"Do you have compression turned on? If so, JSON is very easily compressible since most of the size comes from structural overhead.

If you want the most efficient storage, don't use JSON. Normalize your arrays so that it can store numbers as numbers.

If you must use JSON (which I doubt given your description) then you can compress it before you put it into the database. Though that won't be as efficient as table-level compression.",1525907880.0
swenty,"If this is a single user application and the only data you are storing are images, I'm not sure what benefit you get from using a database. Wouldn't it be simpler and at least as efficient to store each image as a binary file in the file system?

Depending on what type of images these are you might be able to get a significant degree of compression using a standard lossless image compression library such as PNG.",1525917475.0
da_chicken,"> these json arrays have 400 KByte each in my memory

What?  Do you have an example?

Also, how many records were you testing?",1525907740.0
DJDarkViper,"Doesn't PostgreSQL offload TEXT, Blob, and Json fields to other areas of the disk and stores the disk location reference instead? like mysql?

>The internal representation of a MySQL table has a maximum row size limit of 65,535 bytes, even if the storage engine is capable of supporting larger rows. BLOB and TEXT columns only contribute 9 to 12 bytes toward the row size limit because their contents are stored separately from the rest of the row.[https://dev.mysql.com/doc/refman/5.7/en/column\-count\-limit.html](https://dev.mysql.com/doc/refman/5.7/en/column-count-limit.html)

Or am I thinking of something totally different here? ",1525909018.0
Tostino,What methods were you using to measure the size?,1525913700.0
AQuietMan,"> For SQLite I used sqlite3_analyzer and for postgresql I executed this query https://wiki.postgresql.org/wiki/Disk_Usage

Which query on that page did you use?",1525950432.0
doublehyphen,PostgreSQL automatically tries to compress large values. This is probably what you are seeing.,1525954972.0
Rob_Royce,Could the page size have anything to do with it? PostgreSQL’s is 8kb (up to 32kb) whereas SQLite is 4kb (up to 64kb)  by default. Did you adjust these values at all? ,1525975168.0
jenkstom,I think you are asking the wrong question in the wrong sub. Maybe look at the iot subs or the cloud subs?,1525875369.0
ecrooks,"SQL0574N is not about LOBs... 

SQL0574N

DEFAULT value or IDENTITY attribute value is not valid for column column-name in table table-name. Reason code: reason-code.

https://www.ibm.com/support/knowledgecenter/en/SSEPGG_10.1.0/com.ibm.db2.luw.messages.sql.doc/doc/msql00574n.html


",1525866980.0
grauenwolf,"> UI ends up being coupled to the id of the record in a way that feels unnecessary.

Then don't do that. Add a new column (real or virtual) that indicates that this record has special rules.

This will be especially important later on when you discover that you need two custom records and that -2 has slightly different handling than -1. (Maybe one is taxable and the other not.)",1525801014.0
wolf2600,"What we typically do at work is if we receive an order where a product ID doesn't have a  matching record in the product table, we'll create a stubbing record in the product table using that product ID, but all other fields in the record will be ""N/A"" or ""0"" or null, depending on the field.  That way if someone runs a report on orders and joins to the products table, the line item won't be excluded from the report.",1525802145.0
swenty,"I don't know that it's always bad practice, but it seems like a kludge. If I were designing a new system and I could come up with a design that doesn't require a kludge, I'd say that was a superior design, as it would require fewer of the kinds of accommodations that you listed. Sometimes you don't have that freedom to start over, which is a big reason that legacy systems are so confusing and difficult to work with. Kludges piled on top of kludges. In this case is the fix as simple as allowing null values in the foreign key to the inventory table? That seems like an unambiguous solution that nonetheless allows the application the flexibility to render the results appropriately.

If you can't do that I'd suggest adding a trigger rule which prevents deletion of the placeholder record.",1525804571.0
grauenwolf,"> So, my initial way of handling this, was to create an Inventory record with a primary key of -1 to use in the PartsUsed relationship.

I have done that before and not regretted it. Just set the ""IsSystemRecord"" column to 1/true and update the deletion logic so that no one can delete system records. (You may have to add a database trigger as a backup to protect this record.)

****

Alternately I have left the column null to indicate that there is no matching inventory record.

***

Which is best depends on how the rest of the system is setup. Generally speaking, the null option is hard to add unless you planned for it from the beginning.",1525800801.0
grauenwolf,">  I don't want to create a new Inventory record for these PartsUsed since they will never be used again. 

Records are cheap. Go ahead and add the record, setting the ""SingleUse"" to 1/true. 

Then update your views and queries, where applicable, to ignore any single use record.",1525800640.0
jtwyrrpirate,Postgres or PostgresXL if you really need clustering,1525798271.0
boy_named_su,"Postgresql  has decent search. It probably supports more data types than any other system. Multi-master cluster support is weak, but multi-slave works fine. It supports JSONB which lets you store semi-structured data, and is indexible",1525824187.0
alinroc,"Postgres, Oracle & SQL Server on the RDBMS side all meet these criteria.

The last point is kind of moot with a ""schema on read"" database (all of which are NoSQL, but not all NoSQL DBs are schema-on-read).

Aside from the clustering requirement, this list of requirements is so vague as to be satisfied with nearly any database.",1525831385.0
b0ggl3,"You might want to try a graph database, like Neo4j",1525800703.0
Shamanmuni,"Well, I'd need more information to give a better answer but I think if the main thing you need to do is search according to multiple criteria then Elasticsearch is a great choice.

In Elasticsearch you work with JSON documents that let you have many different document types and evolve them. Adding fileds is free but changing the types of fields requires you to reindex the data, so beware.

ES has been designed from the get go to be used in a cluster,   It simply scales horizontally and that's the reason for it's name. You have a REST API so with just HTTP you can use it.

ES is not light on hardware requirements but if you operate in a cluster and need powerful search capabilities then I think it's the best choice for you.",1525830100.0
washtafel,"Have you ever heard of datomic? Not sure about the cluster requirement, but it supports every other point you mentioned.",1525942123.0
mvelasco93,Postgres is good at handling large quantities of data but 1kk of entries for me is not that big. MariaDB or MySQL can handle that pretty good too. How fast the search will be also depends on the indexing you make and how you optimize queries.,1525713097.0
alinroc,">with the ability to limit what certain users can search/see

Where are you implementing this security? At the database level (based on the credentials used by the user to authenticate to the database service itself), or the application (in which case, the database isn't a factor)?

Pretty much any RDBMS will handle this without difficulty, contingent upon some of your other design decisions.",1525713861.0
liquidpele,"protip:  before you decide, make sure you understand the difference between wildcard searching (like and ilike) and full-text searching.  For full-text, you'll want to understand ""stemming"" and how it ranks relevance (e.g. is one term found 20 times or 5 terms found once better?).  Cheers.  ",1525721325.0
Fresan_,"Raima Database Manager can easily handle the amount of data, and I think it would be a bit more affordable than many of the larger RDBMS.",1525767505.0
welshfargo,[Amazon](https://aws.amazon.com/products/databases/),1525789660.0
myringotomy,"Postgres can do everything you want. You can even use row level permissions to limit visibility into the data. It has really good full text indexing too.

",1525910420.0
,[deleted],1525713752.0
grauenwolf,"> FoundationDB does not support long-running read/write transactions, currently defined as those lasting over five seconds. The system employs multiversion concurrency control and maintains conflict information for a five second period. A transaction that is kept open longer will not be able to commit.

OMG. Really?

This is supposed to allow for highly scalable applications and yet it can't deal with any update that takes longer than 5 seconds?

Granted, that 15 second query might just be missing an index that will turn it into a 15 millisecond query. But if we're talking end of month reconciliation and batch processing, 15 seconds would seem like a dream.

https://apple.github.io/foundationdb/anti-features.html#long-running-read-write-transactions",1525548295.0
grauenwolf,"> The FoundationDB core exposes a robust and powerful API but includes no separate query language. FoundationDB empowers developers to employ a broad range of data models and use the query languages best suited to their applications, implemented as layers.

https://apple.github.io/foundationdb/anti-features.html#query-languages

WTF? Seriously?

They are actually saying that having a ""query language"" is an anti-pattern?
",1525548150.0
grauenwolf,"Thank you amirouche for leading me to actually look into FoundationDB. I think its a bunch of crap that you would do well to avoid, but had you not mentioned it I wouldn't have gotten off my lazy ass to read the documentation.",1525548409.0
grauenwolf,"Why foundationdb?

Looking at the website (https://www.foundationdb.org/) there is not a single verifiable claim beyond ""this is a database"". 

Consider  this line:

> All data is safely stored, distributed, and replicated in the Key-Value Store component.

Well no shit. Unless you are using a IASM database from the 1970's (or older versions of MySQL), of course your data is going to be stored as key-value pairs. That's part of the reason why we call it a ""primary key"". The value, of course, being the rest of the row.

There are exceptions to this of course. For example, a ""heap"" or ""column store"" table. But those are only used for very specific situations.

****

Now consider these two claims:

> FoundationDB is easy to install, grow, and manage. It has a distributed architecture that gracefully scales out, and handles faults while acting like a single ACID database.

> FoundationDB provides amazing performance on commodity hardware, allowing you to support very heavy loads at low cost.

What that says to me is ""this database runs so poorly on a single machine with non-trivial RAM and CPU capacity that we jumped right to a distributed model"".

Is that the case for FoundationDB? I have no idea. But it was certainly the case for MySQL and MongoDB.

***

Maybe some people need this system. But for most people the answer is ""learn how to write SQL"" and ""stop putting the database on commodity machines that are less powerful than my laptop"".",1525547919.0
grauenwolf,"Fixed schema? What, are you using an IASM database from the 1970's?

There is nothing ""fixed"" about a database schema since the intention of SQL and the `ALTER TABLE ADD columnName type` command.

For a non-retarded database (sorry MySQL fans) this command is basically instantaneous regardless of your database size.

Yea sure, there are other schema modification commands that do take a non-trivial amount of time. But your ""schemaless"" database will require you to hand-write data migration scripts that do essentially the same thing, just with more code and fewer safeguards.

Do I use a schema-less data model? Sure. In a typical project I have maybe 75 to 100 tables. And one table, just one, will store an XML or JSON blob because that's the right option for that piece of data.",1525547471.0
goblando,I would research programs that create flat files for you or that translate the edi x12 to edit xml.  That should make the job way easier.,1525555008.0
goblando,There can be a lot of information in those files.  Will you have one trading partner or multiple?,1525546030.0
wolf2600,"Make `Name` non-nullable and put an index on it.

Also, you don't need 256 characters for an IP address.  Reduce your datatypes to realistic sizes based on the data they will contain.",1525477301.0
da_chicken,Your table has 15 million records and has no key?!,1525485288.0
HiTierChris,"I've created a new column called ID int AUTO INCREMENT PRIMARY KEY.

Querying by email now takes even longer...",1525518976.0
welshfargo,"Any table that has a foreign key to another table is dependent on that table.

https://www.youtube.com/watch?v=wFOJnRxqrp8",1525467434.0
s13ecre13t,"1) how big is the row:  Calculate how much space is taken on average per row. This is easy for some types (char, int, date), for others (VarChar) and can be really hard (Text / Blob / Image types).

2) how much index:  Next check which columns you index, and how many indexes you have. Calculate how much index space is taken by a new row indexed.

This should give you something along the lines of:

* a row in average takes 1024 bytes of data space
* a row in average takes 200 bytes of index space

A total space per row is around 1.2kb . In a gigabyte I can fit a million of such rows.

You then might want to keep some spare space for future mitigations:

* adding columns (with non null default) can force full table re-creation, so you need to reserve free space of same amount as the table you are updating.
* running update on all records will recreate full table, so you need to reserve free space of same amount as the table you are updating.
* long running transactions / sessions can get you tons of dirty records that need to be preserved until transaction is commited / rolledback. 
* slow / big queries can make db rely on temp storage. 
* certain dbs (ms-sql) have transaction logs in full setup. This means every update / delete / insert stays in the log. This log can be cleaned with proper backups (or disabling db logging through simple recovery mode). 
* internally, db file is like it's own file system. It will fragment up. You need to run db maintenance to ensure that fragmentation is minimal. This is postgress vacuum, and ms-sql db reindex on clustered indexes.

So out of a million rows assuming 1.2kb per row, with various contingencies, you probably can do 80% of that with good maintenance. Reporting , long transactions, bad maintenance, updates without where clauses, all these things will limit what you can do way down.",1525417756.0
DasWood,"This depends completely on what your records are. 

You can have a database that is a single table that is a single row with a single entry that is 1GB in size. Set the max_allowed_packet to 4GB and insert DVD iso into that blob column. 


You will really need to benchmark how quickly the database grows yourself. Even identical schemas with different content (eg, popular cms/sales platforms for different sites/stores) will grow at different rates. Even then things can be done to demolish large sizes like performing db maintenance and truncating log tables.",1525414580.0
space_sounds,"Difficult to say, it's possible to do the maths if you're not changing your database schema, I would start here https://dev.mysql.com/doc/refman/5.5/en/storage-requirements.html   ",1525417038.0
duplicateBadger,"So you delete the user, but you keep the log entries related to that user? Wouldn't that mean you could no longer attribute those actions to a specific user anymore?

Why not keep the user when an account gets deleted or deactivated, but set that user to inactive (new column)? That way you could limit the access of deleted users, but not lose consistency in your DB, with Log entries pointing to User_ID's that no longer exist?

Or am I not understanding correctly?",1525389938.0
svtr,"Honestly, it will take you a couple of years to actually become an oracle dba. You really do not have to map out 10 different things you want to become an expert in. I'd advice you to for now concentrate on actually becoming an oracle dba *with some sql server support. As far as timelines are concerned you got the next 2-3 years covered with that already.

I say this, because I am a DBA, and I know how long it took me to know what I know today, while still learning new things every other week. The rabbit hole is a lot deeper than you might think.",1525381779.0
tdl-jturner,"You sounds like you are definitely going down the right path. A lot of Oracle DBAs start out as developers and thus having a bit of coding is extremely helpful. Personally, I found the Oracle certs to be very valuable and a very good way to focus your learning to get going. They won't get you a job but with experience they help set you above. Mostly they are valuable as they force you to learn the technology. As for a second database, I'd urge you to start learning Cassandra. A lot of major companies have been moving projects from Oracle to Cassandra and having the two skillsets would be quite valuable.

Good Luck!",1525382251.0
welshfargo,"Also learn about ETL, including the ""hot"" topic, streaming systems like Apache Kafka and Spark. Batch ETL is not enough these days.",1525448288.0
pdp10,"My last Oracle & SQL Server DBA moved on to work on very large MySQL databases for an established enterprise.

I'd strongly suggest PostgreSQL, which happens to have features very congruent to Oracle, including [the stored procedure language](https://en.wikipedia.org/wiki/PL/pgSQL).

AWS implements several kinds of databases as RDS (services) for general-purpose use, a couple of NoSQL types, and Aurora (built on MySQL) and Redshift (built on PostgreSQL) for specific purposes.
",1525908926.0
Papafynn,What kind of income level are you looking at once you get your Oracle certifications?,1525392754.0
sfvito,\+,1525626451.0
r3pr0b8,"please do not ever use the word ""loop"" when talking about sql

> doing this call would provide me with a lot of duplicate information

so, change the call",1525273839.0
technical_guy,"Here are some general rules for you:

Network operations are slow

client---network----web server---network----dbserver

Minimize your network traffic, which means do as much as possible in your dbserver using temp tables if necessary or stored procedures if necessary. You want the dbserver to return the minimal amount of data back to the web server and also have the web server send the minimal amount of data back to the client (often a phone app or a web site or application). 

When you join on an index it is perfectly efficient. There is nothing wrong with joins and they are the best mechanism to return the minimal amount of data. It is ok and sometimes necessary to create temp tables and indexes on temp tables on the fly and then join those tables to return the result set. As long as all this is done inside the db server. 

So back to your question - where do you want to loop. In a stored procedure inside the dbserver - thats ok. Inside a back-end language like PHP to filter data you got back from the dbserver over the internet - thats not ok. 

Ok, here is an exception. When you use cacheing and have small tables, it can be advantageous to cache this data on the web server and when you are going to do a db operation, look in the cache first and use that instead. We do this to dodge to slow network operation. 

Finally one argument is, my db is on the same server as my web server - this makes no difference. You still want to follow the rules and assume one day your db may be on a different server on the network. ",1525298767.0
heyda,"Could a sub-query do what you are looking for? Query the database once and get the information you care about, that said if the duplicate data isn't a lot of data (more than several mB) or isn't queried a lot of times, having duplicate data isn't inherently terrible.  

    SELECT a.1, a.2, a.3, b.1,b.2, sub.1, sub.2, sub.3
    FROM a
    INNER JOIN b ON a.id = b.id
    INNER JOIN (SELECT DISTINCT d.id AS id, d.1, c.1, c3
    FROM d
    INNER JOIN c ON d.id = c.id
    INNER JOIN e ON d.id = e.id) sub ON a.id = sub.id

",1525275048.0
alinroc,"`JOIN`s can be optimized by the database engine, assuming proper indexing.

>The other options is to grab information from table a and b and then do a loop to get data from d and e.

Can you pull your data from A & B, then pull another data set from D & E, and merge them in the calling application? What is the nature of the ""duplicate"" data? Is it sales vs. sellers, where you'll have multiple sales for the same seller and you don't want the seller's information duplicated on each sales record? If that's what you're trying to optimize away...don't.

I tell my developers that I'd rather they make 2 hits to the DB for 50 results each than 100 individual hits. Operating in batches is much more efficient - that round trip from the app server to the DB server is not insignificant.",1525275293.0
DasWood,"What database are you using? 

Using joins is a good thing. Way better than subqueries. When you have your where clause the optimizer should automagically just skip items, which will make your queries pretty fast.

Since you only where clause is on b, you might want to make that your primary to try to help out the optimizer. Also, including what specific fields you want to return isn't a bad idea. Otherwise you're going to have a.id,b.id,c.id,d.id. . .  according to your query you have there. Depending on the database you're using it might rearrange this automatically but you would need to test it. Essentially you only want a join to happen when your where is true.",1525498849.0
mcandre,JOINS are inherently expensive,1525289702.0
pcsag,"Binaryclues provides powerful Database Contacts list and database plans to help your business grow. Contact enrichment, lead generation, financial compliance, and more.",1525205393.0
DesolationRobot,">I've looked at MariaDB, mySQL, PostgreSQL. I can't tell if there is a real difference

There are, but your needs sound so ordinary that any of them should do just fine.

Those are all free and well documented. Honestly, I'd pick the one you're most interested in learning and having experience with. Probably Postgres if you're concerned with how it'd look on a resume. But with the level of application you describe, they should be functionally equivalent.",1525205307.0
DJDarkViper,"MariaDB is a drop in replacement for MySQL. There’s subtle differences but Maria is a fork of My by the original authors of My, once Oracle fully bought My.  
The big thing to know is that it’s got a relationship like LibreOffice to OpenOffice. Libre is still a fork of Open, but where Libre is allowed to soak in whatever features they want from Open’s upstream, but not vice versa. 
Maria is able to bring in any upstream work from My, so it can remain on parity features and compatibility wise. 

That said, if your work involves some GIS or JSON, Maria isn’t on my list of choices at the moment. 

I prefer to simply work with MySQL. It’s a chameleon of use cases. InnoDB (its default database engine) is excellent for write heavy AND read heavy concurrency, supports indexed fields  (and with the release of 8.0, descending indexes!), foreign keys, geospatial indexing, JSON generated field indexing, the works, and it is crazy fast when you got your table properly fixed up with the right fields and indexes.  You can pretty comfortably shove tens of millions of records into a single table still have lightning fast lookups that take 0.04ms to retrieve.  Combine this with a MEMORY engine table or two for those low write & high read scenarios, and you don’t even need to spin up a redis for most all aside the highest trafficked applications. 

Basically: it’s one of THE the best all rounders. It’s the Ryu of Street Fighter. Most everyone can pick it up and kick ass with it without much issue. 

PostgreSQL I would use if I planned to use RSBMS for “Big Data” (high hundreds of millions of records), and Postgres is always on the cusp of indexing and features for a free database engine. They were the first to introduce a proper JSON field data type that could rival the performance of NoSQL databases in an RDB world. 

It’s the Ken of Street Fighter. A little less used overall and you could argue for weeks over which one is technically better (Ryu or Ken) but they balance each other out mostly. 

——

So here’s an aside: it’ll be hosted on Windows, and you’ll be using C# to access it.  I have to ask why you haven’t considered SQL Server Express? ",1525206072.0
Quadman,"SQL Server is not open source but it is free (express version). It works with C# and .net really well and if you are already set on windows you can interact very easily with it through PowerShell.

If you don't really know how to compare features of the database engines you are considering then it doesn't really matter which one you pick. You can learn it as you go and if you realize something you absolutely need is missing then you can just pick another one.",1525258355.0
doublehyphen,"I am not a .NET guy but PostgreSQL seems to be pretty popular and have good support in the .NET ecosystem.

But as others have pointed out your requirements are very conventional so almost any SQL database will do.",1525263122.0
dsn0wman,"You want to go down a well worn path so things work as good as they can. The general combination of OS and DB's that were made to work together are SQL Server/Windows, MySQL/Linux, and PostgreSQL/Linux.   
  
You can run MySQL or PostgreSQL on Windows if you want, but that makes you more of a corner case. I know people do it, and it is supported, but it doesn't put you in the best case scenario.",1525270143.0
brantam,"Since you are using C# I suggest you use SQL Server. It integrates best with .NET apps and is very well supported by third-party tools and open source code for .NET developers.

Of the products you mentioned PostgreSQL seems like the obvious choice. It has a much more comprehensive and more powerful implementation of core SQL features and syntax than does MySQL/Maria.
",1525241395.0
HelWerther,"Wait, the Help Desk software stores data on Google Sheets? Or you people updates the sheets manually? Cause in this case, you could just access the database created by both writing some code in python (or any language you want).

All this proccess seems a little off (and confusing) to me. I think the best solution would be just change the CRM and Help Desk Softwares. Or just write it yourself, which I think would be easier than all this proccess.

Sorry if I understood it wrong, but I think your main issue is the process.",1525196514.0
newsagg,This is a terrible idea without more information or requirements.  ,1525214044.0
_Zer0_Cool_,"""NoSQL is the most sought-after technology that development companies are using for storing and managing data in the back-end.""

That is not even close to true... 

At most NoSQL stands for a small sliver of the pie and is almost never used without accompanying relational stores and alot of integration work in between.",1525216682.0
stillalive75,"I've never heard of any but I'm sure they'd be a hoot!

I have always wanted a place to ask some really abstract questions and run things by people before fleshing out some take design, so I'm interested to see if anybody knows if any.",1525182805.0
alinroc,"What is this database going to do? How are you going to get data into and out of it?

You need more than a database. You need an application of some sort that uses a database for storage. How that application (platform, language, delivery method, etc.) will be developed depends upon the needs of the business and the skills the developer has.

To get *any* valuable recommendations, you need to flesh out your requirements.",1525115948.0
welshfargo,http://www.databaseanswers.org/data_models/,1525129091.0
neglectron,Sounds like you could probably just use Microsoft Access for this.,1525140911.0
petepete,"I'd write a script to do it, it's straightforward in Ruby with pg. Basically, something like this should work.

    \copy (SELECT encode(file, 'base64') FROM blobs LIMIT 1) TO '/tmp/file.b64';

    base64 -D /tmp/file.b64 > yourfile.jpg",1525089262.0
da_chicken,"It's easy enough to do with a script.  I use PowerShell with SQL Server:

    $SqlServer = 'MyServerName'
    $SqlDatabase = 'MyDatabase'
    $SqlConnectionString = 'Data Source={0};Initial Catalog={1};Integrated Security=SSPI' -f $SqlServer, $SqlDatabase
    
    $SqlQuery = ""select FileName, FileData from myTable;""
    
    # Define the path pattern for the output files.  {0} will get filled in with the filename.
    $OutputFileFullNamePattern = 'C:\Path\To\Output\{0}'
    
    $SqlConnection = New-Object -TypeName System.Data.SqlClient.SqlConnection -ArgumentList $SqlConnectionString
    $SqlCommand = $SqlConnection.CreateCommand()
    $SqlCommand.CommandText = $SqlQuery
    
    $SqlConnection.Open()
    $SqlDataReader = $SqlCommand.ExecuteReader()
    while ($SqlDataReader.Read()) {
        # Set in the file name 
        $OutputFileFullName = $OutputFileFullNamePattern -f $SqlDataReader['FileName']
    
        # Save the data to the file
        [System.IO.File]::WriteAllBytes($OutputFileFullName,$SqlDataReader['FileData'])
    }
    
    $SqlConnection.Close()
    $SqlConnection.Dispose()

I imagine it would be pretty easy to do the same thing with Python and PostgreSQL.",1525098857.0
Mamertine,"Having a DB that stores data about ETL or similar load processes is fairly normal (even advisable).

I'm not fully following along with your plan. Why split them into 2 dbs? 70 tables is a small DB. If one DB will be reading/updating the same table, just leave them together.",1525046756.0
mabhatter,"You can have a LOT of tables in even a PC backed database... like don’t even worry til you get to maybe 300-500. 

The bigger issue is read and write performance, but Database Locks are the biggest problem as you add users in ways you didn’t expect. Writing tight code that limits user locks and complex queries as much as possible is the biggest thing you should be concerned about. 

This is where you’re getting into DBA-land with tracing queries, building indexes, tuning your RDBMS, etc. ",1525048681.0
quentech,The number of tables in your database doesn't have anything to do with performance,1525048466.0
mabhatter,"Well there are dozens of combinations of websites and databases available. You “don’t know” what you don’t know. 

So what Database do you have available on your website? Is it hosted by a third party? Is the website handmade or an open source package? You gotta do some homework. 

The simplest way, If it’s just one table on a homemade website, you could probably just find tools in your scripting language for SQLite. You would have to manually export your table from Access into an SQL-type format as plain text and then do some massaging to import it into the SQLite. But once that’s done most scripting languages have libraries to work with SQLite tables. ",1525063280.0
wolf2600,We can't see the LucidChart link without creating an account.  Could you post a screenshot instead?,1525005443.0
MagicWishMonkey,"Couldn't you just take a snapshot of the database (including user accounts and all that), and restore against the new database server? It should be pretty straightforward. 

Reminds me how much I hate it when devs over-engineer shit when no one is there to stop them. I am dealing with one team of devs right now who insisted on spending weeks adding cloudflare support to an application that *maybe* has 10-15 active users per day. Ugh. ",1524864774.0
ppafford,"Might find more responses here r/postgresql 

I’ve never used PostgreSQL-XL but I’d think it’s still PostgreSQL, a normal dump and restore should work as I think XL is more about load balancing and high availability to my understanding ",1524919595.0
KitchenDutchDyslexic,"Wonder how it compare to [PostgresSQL Foreign data wrappers](https://wiki.postgresql.org/wiki/Foreign_data_wrappers). 

Only wrapper missing seems to be a Kafka one. But a quick google returned some github projects.",1524806511.0
alinroc,"Microsoft makes available sample databases to show off every possible feature of SQL Server.

WideWorldImportersDW may work for you. https://github.com/Microsoft/sql-server-samples/tree/master/samples/databases/wide-world-importers/wwi-dw-ssdt

Read the README and other docs carefully before you set it up.",1524661765.0
ryati,"I don't think it's really going to be that much data, esp if you normalize your tables. A ""loggedIn"" table could have UserId, ServerId and datetime. Updating this every minute could generate a good amount of rows, but nothing it couldn't handle. Any RDBSM should work fine for this.

What are you writing this in?",1524602400.0
swordstaind,"you could do it with a user id column, a bit column(for identifying if logged in) and two datetime columns, one for first time seen logged in one for last time seen.  Then update the second column for every continuous minute logged in.  Once they are not seen by the bot for x amount of time flag them logged out and when they log back in again start a new record.  

Then when you query the db you can do a check for all bit flags of 1 to see who's logged on  and then use the datetime fields to see when they first logged in and when the last time the bot checked their status.

If you want to get fancy you can move the record to a history table once the code determines the user is no longer logged on and only keep active log in in the first table.  This would do away with the need for the bit column and make query time more responsive over time as you wouldn't have your historical records cluttering up the main table.",1524604142.0
DesolationRobot,"Do you have access to login/logout events? Having data logging based on a triggering event is better than a recurring poll.

But even if you don't, I wouldn't store one row per minute per user who is online, I'd have it keep logged in-logged out timestamps. So if it finds a user online it inserts a new record with an in time--unless that user already has a record with an in time but no out time. Then it does nothing. Then it runs through all the users who have an in time but no out time and sees if they're still online. If they aren't, it updates their out time to now.

Then in the future you can see who was online at any given time by checking in and out times. And it'll have the same granularity but far fewer rows (assuming the average session is > 1 minute).

But unless you're logging millions of users that's really not that much data.

>Should I just have it delete records after a while?

Yeah, you can if data size starts to become a problem and the benefit of keeping those records is low. You could also archive them or aggregate them.",1524611413.0
drunkadvice,"It had been said that 500k records isn't a lot.  Which is true (I have a 500gb table in production).  I'm more curious about your approach to check every minute and saving the results. Does your state change that frequently?  Are you able to check if you're online, and assume you are online until you are offline?  If your status only changes 10 times per day, you will only have 10 rows of data, and be able to infer almost the same information. 

   10:00 signed in

   10:30 signed out

Is much better than 29 rows saying ""yep, still signed in"".  Can both methods tell when were you signed in until?  Sometime between 29-30 when you signed out.

  Some times there are reasons for redundant messages.   I can't think of one right now.",1524621659.0
jlrobins_ssc,"You only really need to store the time ranges that you were logged in, using either a time-range column like postgres’s timestamptz data type, or if more primitive, then a pair of culumns to hold (start timestamp, end timestamp).

Sample every minute. If you’re logged in, and there’s an open record in the db, then extend it. If there wasn’t, then extend it. If not logged in and there’s an open record, then close it. Finally, if not logged in and no open record, then do nothing. 

So, basically you’re just recording the contiguous time ranges in which you’re logged in. Then periods of time that you’re in the same state mean no new records.",1524611446.0
duplicateBadger,"This isnt a ""massive"" number of rows by any stretch, especially with computers nowadays that have 4, 8  GB of RAM... But even a cheap VPS with 1 GB of memory, that could likely store tens of millions of rows that were structured that simply (user_id, last activity timestamp)",1524613230.0
jorisber,a database is used to store many rows so i shouldnt worry about capacity (unless you have very limited storage space),1524649843.0
Zardotab,"Since the original link is dead, a brief description can also be found at:

https://stackoverflow.com/questions/66385/dynamic-database-schema#46202802

As far as the ""is it relational?"" debate, I see it as a matter of virtualization. It's virtually a ""rectangular"" table (or at least you can interpret it that way). Thus, it's ""virtually relational"".

I interpret the quote that starts with ""One possible way to alleviate this is to make the number of columns open-ended per record..."" as the number of explicitly-filled columns (those with non-null values), not the number of virtual columns. The query decides the number of (virtual) columns, not the table.

As far as practical use, it can be for rapid prototyping, and for rapid application development (RAD). I realize RAD is not the best practice in many cases, but has its niches.",1524424976.0
Zardotab,"This was intended to link to an archived Reddit discussion, and NOT the original article:

https://www.reddit.com/r/programming/comments/a2rli/we_have_dynamic_languages_so_why_not_dynamic/

Since the original link is dead, a brief description can also be found at:

https://stackoverflow.com/questions/66385/dynamic-database-schema#46202802

As far as the ""is it relational?"" debate, I see it as a matter of virtualization. It's virtually a ""rectangular"" table (or at least you can interpret it that way). Thus, it's ""virtually relational"".

I interpret the quote that starts with ""One possible way to alleviate this is to make the number of columns open-ended per record..."" as the number of explicitly-filled columns (those with non-null values), not the number of virtual columns.

As far as the practical use, it can be for rapid prototyping, and for rapid application development (RAD). I realize RAD is not the best practice in many cases, but it has its niches.",1524422781.0
hicksyfern,"The question is quite broad but I will do my best. I'm by no means a database expert, but I do enough to get by as a full stack developer (my current job has react native as the front end too as it happens).

My day job is building a social network, or rather migrating one from a single dB laravel application to a more scalable architecture, so this is where my bias comes from.

My previous job was at a fintech company. The types of data and how it is interrelated is far more interesting in the social network, so my recommendation would be...

Design a database (or databases) for a social network. Consider its application at scale. You will hit concepts like normalisation, indexing to improve query times (trading off insert times), denormalization, graphs, trees, transactions, etc.

What parts can you get your database to do, Vs your application? Which bits need to happen immediately vs ""at some time in the near future"". This goes more towards general backend architecture but you can start off with a simple relational database structurw and go from there.

Basically, try to build Facebook. It's interesting!",1524415780.0
wolf2600,"I'd start out by finding a data set somewhere.  Ideally something un-normalized (a giant CSV file would be good).  Model the data (define attribute names and datatypes), create your schema (tables and relations), then write some ETL scripts to pull the raw data, transform/sterilize it, and insert it into the tables.

Then you could create some reports which would analyze/display the data.",1524450174.0
HawkeyeGK,"There are two important things you need to learn here.

1. How do you normalize a database.  Non-DB devs are notorious for creating poorly normalized databases and having all the problems that come from that.  Grab a book or an online tutorial on how to normalize a database.  Understand that.  You'll get questions on it in the interview.

2. Understand how to access a DB naively from code.  No abstractions like Hibernate.  Understand how to access code using a native SQL command and how to invoke a stored procedure and get data back.

In order to practice this, I'd pick some real world thing that you'd like to model that you know well.  Pick something like your school, the classes, the students, the classes the students take, what buildings they are in, etc.  That will teach you the basics of data modeling and normalization.  Then create a simple UI that edits the data in each of those tables.  Then, create some pages that edit data in multiple tables at once.  

In your interview, be up front.  They'll be able to tell you're inexperienced, and that's ok coming right out of school  Tell them what you know, and don't be afraid to tell them you don't know that either, but this is how you would go about figuring it out.",1524453371.0
BobbyMcWho,https://blog.discordapp.com/how-discord-stores-billions-of-messages-7fa6ec7ee4c7,1524255270.0
mkingsbu,"The reason that Cassandra or HBase work for something like this is they get insane amounts of posts per second.  Cassandra can cope with way more writes than a relational database. The downside is that reading can be a lot slower.  What NoSQL databases will do is create what we might consider to be materialized views where user messages might be partitioned into multiple tables simultaneously because writes are so much faster.   That way, reads can still be done without doing full tablescans, which would generally be the default. Deleting can also be a pain because records aren't deleted, rather a tombstone is placed ontop of the record so that eventually when tables reach a certain size, they're merged and then tombstoned rows are deleted.

Some NoSQL database might be worth it though I'd personally start out with relational and see if it can work but I'm admittedly not in a space where I have to worry about concurrent users so don't take that as gospel.",1524250177.0
ppafford,PostgreSQL for either Relational and you can use JSONB for noSQL option,1524258224.0
assface,"Do you have 100k users now? No? Then stick with Postgres.

What does MapReduce have to do with this? You don't want a column store either, since your workload is insert-heavy.",1524253859.0
kinesivan,Postgres,1524257844.0
ryati,"Is this a current problem you have? Or a potential future one? If this is for a potential future problem, don't get caught bikeshedding. Pick a respectable DB that is well documented and stick with it until the chat does start slowing down.

I don't want to encourage technical debt, but having a small scale app with a few users that works is much better than a small scale app with no users because its too complicated to setup.",1524256517.0
rose_anachronysm,"This is like Cassandras go to example. Cassandra originated from Facebooks messenger app. 

You are literally just looking for key value pairs, Cassandra is a key value pair database. And your search path will be based on who is talking to who then grabbing the last x messages. Which for Cassandra you'd use a compound partitioning key of user uuid with the uuid of the group / user they are talking to. Add a clustering key with time and return the last x results.

Why you should use Cassandra:
-It's highly scalable and highly available.
- For this type of workloads its insanely fast (at my work we look after clusters with < 0.0001 seconds of latency.
- It's designed for write heavy workloads.

Dont use Kafka/Redis/RabbitMQ, these are used to move around data usually for analytica and later use. You don't want your chat message to ""eventually get there"".

Full disclosure I work for a provider of managed Cassandra. Feel free to hit me up if you want some tips on implementing it.",1524264785.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/node] [Best database for a chat application?](https://www.reddit.com/r/node/comments/8dq3yv/best_database_for_a_chat_application/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1524251154.0
DesolationRobot,"As part of the design, does the chat need to be stored in the cloud database? e.g. for asynchronous messages or syncing chat history between two devices? Because that question could determine a lot.",1524261012.0
anuruddhak,"This is a very timely question for me personally so thanks to the OP and all the responders.

I am designing a system - a little different to chat - but one that will need to store a BLOB (JSON) for about a minute.

I expect 500,000 concurrent users.

We have picked Redis for this since we have seen that (in our limited tests) that we can push the BLOB with a Long key into Redis very very fast and also read out of Redis just as fast.

We did not try Cassandra as we just don’t have the skills and quite frankly did not have the time to learn it (perhaps we should).

I am interested in everyone’s thoughts and experience about this approach.

Unlike the OP’s requirement, our use of Redis is simply to “store by key” and then about a dozen times “retrieve by key”.

Please help / comment / share!",1524272292.0
4_teh_lulz,"As others have said, go with Postgres or Amazon Aurora if you want easy scalability. As your chat application scales it will become clearer if you need to make a change there.

",1524319179.0
kenfar,"The chat services on hbase probably aren't using map-reduce, they're probably just looking up a specific row - and treating the data more like a key-value pair.  Though I suppose they could store additional data in it, and also access it in a variety of other ways for analysis, etc.

It would be helpful if you added more info - like how quickly will you need to scale to 100k simultaneous users, and how much skill & money do you have to support this *now*.   Cassandra, for example is a bear to manage and doesn't handle schema migrations well at all.   If your team is small and doesn't have much skill in these products you might find the cheapest solution is to just use a managed service for now (say, postgres on rds), then refactor & upgrade as necessary to support that 100k users in two years or whenever.",1524323320.0
dj_swift8,Aerospike can also proove to be helpful for this use case.,1524326930.0
cachedrive,"I would consider PostgreSQL \- it's a perfect RDBMS for me and can also handle NoSQL better than some NoSQL databases. That being said, Cassandra is also a possible option as well.",1524598502.0
duplicateBadger,"I'm here a couple days later ALSO considering a chat app. 

I was thinking MySQL or Postgres, but this is all making me wonder if perhaps Elastic might be better? (It's the only other key/value store I know!). Especially after reading the Discord challenges - things i never would have thought of - conversations stored in different index fragments, etc.

Could Elastic be of help here? Or is that the wrong tool for the job?",1524614335.0
techno_gold_pack,"If you are planning facebook or watsapp scale chat then you need scale-out distributed system like Cassandra, Spanner or Mongo, recently new systems like Cockroach and YugaByte provide distributed-acid properties too.",1524766836.0
r_karthik_007,"I was one of the folks in the core team that built HBase and was responsible for running Facebook messages on top of it. Now a founder of YugaByte DB - about 6 members of the original team that put FB messages in production on top of HBase working on YugaByte, which is built specifically to solve issues such as the one you are talking about - of course this is just my perspective :).

> I am having trouble understanding how chat is modeled as a MapReduce job, as it seems much more suited to simple relational model (like 2 tables) or document collections/column store to me.

You are 100% correct in this. Chat should not be a map-reduce based architecture because of its realtime nature. My rule of thumb is if its got to be served to an end user, it cannot be an MR job.

> I would also imagine chat favors availability over consistency (but then why Facebook). Any suggestions?

At Facebook, I was one of the engineers that had originally worked on Cassandra (even before it was open-sourced) - for messages search, where eventual consistency was acceptable because it is rarely read. When it came time to store messages, we switched to HBase specifically for strong consistency and a highly scalable design. But it is operationally hard, whereas Cassandra is very easy to operate. This gap (operationally easy, scalable and strongly consistent) is what prompted us to start building a new DB -YugaByte.

In fact, here is a blog post we had written about creating a scalable chat application on YugaByte (you can easily see parallels to Cassandra/Redis schema as we are API compatible with these). Here is post describes the blue print for such an application here including a possible schema layout:
[Building Scalable Cloud Services - An Instant Messaging App!](https://blog.yugabyte.com/building-scalable-cloud-services-an-instant-messaging-app-97cd52fbc121)",1524845912.0
dbbeginner,"I was going to write a snarky reply referring you to a high end Oracle installation on SuperCluster hardware, but really, without more information it's impossible to answer.

SQLite would probably support 2-10 simultaneous users. Maybe even 20. Is that an appropriate answer for the use case you're imagining?

:)",1524247234.0
HildartheDorf,"Whatever you are most comfortable with.

SQLite is trivial to maintain, then when you outgrow that, Postgres \(open source\) or MSSQL \(Paid, with a free tier \(express\)\) would be my recommendations.",1524248209.0
DGTexan,"Seems pretty cool, actually",1524163123.0
Minsan,Wasn't this the database that Apple bought? Wondering why they did it though.,1524201183.0
jlrobins_ssc,"Reads like major catchup to PostgreSQL features, aside from nice-to-haves ""Query Optimizer Takes Data Buffering into Account"" and ""INVISIBLE Indexes.""

Smells like a hole crap-ton of features dropping in at once, though. I'd be scared of the new bug surface area.",1524155618.0
Haxonek,I don't understand what the point of this is? It sounds like you're just trying to use big words to get people to think this is important and give you money for your ~~scam~~ cryptocurrency,1524116383.0
rydan,If they are using something other than a blockchain they have completely failed at their task.,1524047948.0
cudenlynx,Isn't the blockchain the database? The blockchain is where the ledger and transactional record is stored. ,1524072621.0
BillNyeThePiousGuy,"Bluzelle is working on this. They are the only DB solution I have seen so far, but haven't released their prpduct yet. I think are getting close. They claim performance and price will be better than current solutions. The CTO has answered a few of my questions in r/bluzelle. Check it out.",1524639620.0
yingyaoxie,I also created a new subreddit r/DecentralizedDatabase/ so we can discuss more about this there,1524040596.0
Huwaweiwaweiwa,Wrong sub? Will you be creating a database out of the comments here?,1523983224.0
CrimsonSmear,"I'm about 50 lbs overweight, but I wear it well, and my avoidance of sunlight has people guessing my age at a decade younger than what it actually is.",1524004076.0
cachedrive,select \* from myFatAss;,1524599333.0
,[deleted],1523983344.0
alinroc,">I recently got a .mdf and .ldf as a back up

Sounds like SQL Server.

But those almost certainly aren't a ""backup"". SQL Server has a [well-defined backup process for databases](https://docs.microsoft.com/en-us/sql/relational-databases/backup-restore/create-a-full-database-backup-sql-server?view=sql-server-2017) and it does not produce two files with those extensions by default. What those file extensions *are* is the main data file (mdf) and the transaction log (ldf).

The only way these files *might* be valid for you to use is if they stopped their SQL Server instance or detached the database, then copied them. If they copied them while it was running, they're no good.

/u/whisperedzen is correct that you need an installation of SQL Server (the same version as the vendor used or newer - you can't go backwards. And until SQL Server 2016 SP1, you'll also want to match editions - if they were using Enterprise Edition and used EE-exclusive features, you won't be able to properly use the database on Standard Edition). But you can't just ""import"" the files. You need to **attach** the files to the running instance, either via the SSMS GUI or via SQL. See https://dba.stackexchange.com/a/30441/35474

Even if they did stop SQL Server or properly detach the database, they did it wrong IMHO. A full backup would be a much more appropriate way to give you your data.

**TL;DR:** Call your vendor, tell them they gave you a bad copy of your database and ask for a **true backup** of your database. But there's still gotchas.",1523929164.0
whisperedzen,"Well... given the extentions of the files that looks like a sql server database.    
You will have to:
1- Install sql server in your machine.    
2- Create an empty instance of the server.
3- Import the database.    
",1523918265.0
shiv2003,You can get some help from here: [https://www.kerneldatarecovery.com/blog/how\-to\-restore\-master\-database\-in\-sql\-server/](https://www.kerneldatarecovery.com/blog/how-to-restore-master-database-in-sql-server/),1525412428.0
wolf2600,This is in /r/database?,1523909794.0
,[deleted],1523895807.0
r3pr0b8,"OP, is this your article?

it's pretty good by i would like to ask the author a question",1523889587.0
anras,Deferring constraints is nice but not supported by all databases...,1523891497.0
Daywalker24007,Doesnt sound to be anything too rigorous. How will the data be accessed?  Thinking MySQL?,1523791293.0
MeGustaDerp,TIL that business instances of FB or corporate facebook exist...,1523652200.0
da_chicken,"I don't think I agree on EAV tables.  Sure, I agree, they absolutely suck and are horrible to work with and do cause problems with missing records creating invalid data or problems with data types or problems with performance, etc.  You should avoid EAVs except under duress.

However, I wouldn't say that they should *never* be used.  I've worked on several systems which allow the customer to define their own fields and often define their own screens.  These systems use EAVs to allow the user to store arbitrary data in their systems.  If you really need to query specific screen repeatedly, it's very easy to build a view that has a query which correctly formats the data.

They're still usually better than doing something *even more irritating* like packing everything into an XML or JSON string and then saving that structured data into a single field.  That is *really* miserable.
",1523634660.0
,[deleted],1523634901.0
SzejkM8,"Too bad it's Enterprise-only feature. Such feature is what differentiates Cockroach from others and could bring a lot of audience.
",1523566087.0
randomfrequency,"""An experienced developer would rewrite to use subqueries""

Wow.

A lot of databases out there do not have support for materialized subqueries, so no they would've left the join asis.",1523584940.0
tannhauser45,Easy query != good query ,1523586566.0
AandreFergusan,Could be amazing to see Liberdy making a revolution in the data advertisement industry ,1523442164.0
yaldithcol,Hmm did i got it right? Are they planning to create a marketplace for data? something like Fiverr? and then if someone decides to buy my data he needs to pay for it?,1523442365.0
kunalgrover05,Does advertising need to be a solution for the future? I dislike how it's okay to sell your personal data to use a service (Google/Facebook). Isn't there a different way for the world to run?,1523470068.0
elyuma,"1. Dynamics columns can brake your PoweBI report. 

2. To do this I think you will have to use the * on your select statement. Not good for performance. 

3. You can do this using SQL SP and something to trigger that SP. ",1523314393.0
CorrelatedSubQueries,"> The views' columns could change with each day.

Why is this?  If you keep renaming columns, it's going to break whatever tool you're using for your presentation layer whether it's SSRS, PowerBI, Tableau, etc.

> An Idea I had was to write some script that would run nightly, loop through all tables and sum up all the data and generate the reporting views needed.

Look into using the SQL Agent which will allow you to schedule a process that has one or more steps to be executed.  So for example, create stored procedures that do logical groupings of work and then have each step of your SQL Agent job call those stored procedures.",1523368770.0
bennyman32,But how will adding a few tables and mapping relationships between them within power BI break power bi. Do you have to do a lot of complex calculations on the dataset. ,1523584249.0
HawkeyeGK,"Don't create one big de-normalized table that changes all the time.

Create a [dimensional model](https://data-warehouses.net/glossary/dimensionalmodel.html).",1524453596.0
welshfargo,Use [indexed views](https://docs.microsoft.com/en-us/sql/relational-databases/views/views).,1523367994.0
Prinamwill,"Bring the latest technology and best feature eSoftTools Software which gives you a best software that most trusted EDB to PST Converter software have great efficiency to recovery corrupted edb file data into multiple formats like PST, EML, MSG, EMLX, and HTML file. EDB to PST Recovery Software supports also to split the oversize PST File by size up to 5GB during conversion Software supports all MS Outlook versions included- 97, 2000, 2002, 2003, 2007, 2010, 2013 and 2016.

get more info:- http://www.esofttools.com/exchange-edb-to-pst-converter.html",1523278572.0
r_karthik_007,"Didnt read in detail, but going by your benchmarks is the main benefit performance? In other words, what limitations of existing systems does this overcome (or feature set does it improve).",1523296217.0
,[deleted],1523028153.0
Actually_Saradomin,I’d use typeorm on the node side. You need to decide which side’s orm gets to be the authority on the schema if you’re working from the same db.,1523041248.0
dobby12,"Can someone provide an elementary example of the difference between the two? 

Is it basically just the DW handling the transformations instead of the ETL tool? ",1522953894.0
imcguyver,This is a shill post.  And it's about ELT vs ETL.  Nothing new here.,1524018076.0
wolf2600,"Unless you're encountering a problem that is unsolvable with your current solution, don't worry about trying to keep up with all the new stuff.",1522875361.0
whisperedzen,It might be an awesome piece of software.... but it is sooo hard to sell with that name.,1522898162.0
ram-foss,"Here is the list of [open source key value store](https://www.findbestopensource.com/tagged/key-value-store) You can choose Redis, Apache Ignite, Memcache",1522913034.0
bidwreck511,"It depends on the job, but for most software development jobs I've had the focus has been on being able to solve problems in the high-level languages (Python, Java, etc.) and using SQL mostly for basic CRUD stuff.

If you are specifically looking for data analyst then SQL would be more important of course, but it's still only used for extracting your starting data sets while the bulk of your analysis effort is spent with an analysis language/platform (Python, R, Power BI, etc)

SQL is important, but you won't be using it to actually solve complex problems.",1522856670.0
yaxis50,Wtf is this ?,1522636222.0
assface,"Fun Fact: *Horizontica* was the original name of VoltDB when it was being built at Vertica.

Source: http://hstore.cs.brown.edu/documentation/faq/#q2",1522621765.0
tannhauser45,March 2016?,1522634057.0
fozzie33,"Too much redundancy... Have you normalized it yet? 

You'll need an intersection table between fights and events, then fights is really an intersection between fighter and fighter. ",1522460412.0
jetset314,"Did not have a chance to go through your entire schema, but here are a couple of thoughts.  

I would also do a normalized table on the fights.  For UFC they are normally numbered, right? So you could have fight, fight_id, and then you could have a composite key on fighter_id and fight id.  Could even break it down further by having a matchups tbl, where you join the fighters and the fights together.",1522460364.0
coconut2015,1.0 was just released.,1523039990.0
fozzie33,"First option would be a nightmare to query. 

I'd create a table for all colors of ink and a code for each.

Then a table of all Pantone colors and a code/ID, 

Then an intersection table with ID of ink and ID of Pantone and percentage of ink needed for it. 

",1522341245.0
collinstevens,"An older, but longer version of the same talk. https://www.youtube.com/watch?v=8mKpfutwD0U",1522298695.0
pcsag,"Business Database Services - BinaryClues Database Services, Sales & Marketing Contacts Providers.",1522234351.0
jpers36,Try /r/DatabaseHelp,1522162867.0
aka-rider,I think you need something like https://www.tableau.com/,1522325420.0
unajodienda,"I don't think you have a clue about what Databases are, which is okay, but I have a few comments.

You said ""multiple"" databases, why?

What you're describing can be done on one Database, they are specifically built to handle a lot of things you describe. So with that in mind, can you answer what made you conclude that you even need one in the first place?",1522154023.0
pierredewet,"I recently used the Manning book, PostGIS in action (http://www.postgis.us/) when I was doing my MSc and found it super useful. To be honest, I only used a small subset of the PostGIS features but what I did use was really well explained. I’d recommend. (Incidentally, I also used datagrip as Jetbrains gives students free licences, but the SQL IDE was the easiest part of the learning experience so using others would probably work as well)",1522091138.0
AQuietMan,I admit I'm biased against any database professional who uses the word *modelization*.,1521994485.0
bedknobsandbroomstix,"Good points, although I think it's overly harsh to declare UUIDs as anti-pattern, there are definite use cases for them.  Plus, I wish more discussion on alternatives was provided on all the cases.",1521989348.0
einhverfr,"I also have worked in projects where Non-1NF designs were good and useful but you would never use a string to do this since PostgreSQL has good array support and GIN indexes.  I have worked on projects where we stored arrays of tuples in columns in order to optimize access.

However, you have fairly serious issues with anomilies when you step into this territory so it is not a general tool for a general situation but a specific tool for very specific cases, and often these end up TOASTed in PostgreSQL which poses additional performance gotchas too.  Understanding when to use non-1NF designs is an important skill when working on large databases but it is not a general tool for general systems.",1522498438.0
wigi1,eg https://www.abisource.com/,1521977519.0
GuyWithLag,Your question does not make sense. Are you a human or a Markov model?,1521981733.0
KitchenDutchDyslexic,"Not 100% sure if it fit your requirement but [firebird](https://www.firebirdsql.org) has a manual zip you should be able to run without admin permissions and connect to it using oodb.

Might still need the jdbc drivers for fb so that might invalidate this option for you. 

But then again can you connect to another db from oodb without jdbc?",1521966244.0
swiz0r,Pgadmin?,1521906608.0
,[deleted],1521924280.0
gram3000,"I'm looking for one too on Ubuntu. So far I'm using Jetbrains Datagrip and it's only ok. 


Navicat have a version for Postgres too  though I haven't used it.",1521904036.0
KitchenDutchDyslexic,Some will shun the wine route but have a look at [HeidiSQL](https://appdb.winehq.org/appview.php?iAppId=3326).,1521960217.0
guttermonkey,"I'm honestly not up to date on Node.JS at all, but I'm betting it has nothing to do with your database.  I don't mind trying to assist, but I'll need to know more:

* Which ORM are you using?
* Which DB are you using?

I can take a peek at the documentation and see if I can make heads or tails of it. ",1521836635.0
sh4dowc,"I think it's about your sql code, ""req"" variable",1522700310.0
KitchenDutchDyslexic,">  Sure I could handle that on the application level and only look for keys in the json that are in the form, however what about the redundant data from old fields? One approach that comes to my mind is to access all submissions of the given form and update the json value accordingly.

Yes I believe a cleanup event will need to take place against your old submissions if the schema changes. But I feel it is by design. 

If my jnr dev drops a field on my main schema I don't want it to retrofit my data. Only once I want to explicitly drop the field will I need to run a application inside or next to the db.

Personally I have been using [json-editor] to define my forms. It calls a restapi that insert the json into a jsonb column. If it loads data that is not part of the schema it hides it or you can change/program the behavior. It also removes data if not part of the schema. So next time a user comes around and changes something the form cleans up itself.

Also for most reports you can select your data out of the jsonb colum in a very rational way. Build a dynamic select from your schema to only display fields using the `->` operator.

---

I did not have answers so did a quick google to refresh my mind. This [stackoverflow] based around 9.5 helped me; and the current [JSON Functions and Operators] documentation on how to manipulate json inside of pgsql.

Please feel free to correct me if I understood something incorrectly. 

[json-editor]: https://github.com/jdorn/json-editor
[stackoverflow]: https://stackoverflow.com/a/23500670
[JSON Functions and Operators]: https://www.postgresql.org/docs/current/static/functions-json.html",1521781924.0
NotImplemented,http://sqlitebrowser.org/,1521649223.0
spitfiredd,"DBeaver is what Use, it’ll connect to most databases including SQLite.",1521733717.0
santino79,"I've used Sqlite Studio, has all of the features you want there: https://sqlitestudio.pl/",1521805346.0
klasius,http://squirrel-sql.sourceforge.net/ for example.,1521635330.0
mdbgc,MySQL and MongoDB - check out their careers page and look for desired skills and experience. https://ca-commercial.com/careers/careers-data-scientists,1521585193.0
punpunpun,"Now that they've been exposed, I'm betting NO SEQUEL.",1521605891.0
SzejkM8,I'm not really believing benchmarks that are made by the companies involved.,1521576473.0
jaspeed76,"There are protection levels for the project. The typical setup I see is to use Encrypt Sensitive with Password option, which then lets the credentials get encrypted upon deployment. Then when your scheduler runs, it will use the package/project password to decrypt. If it is your account that executes the job, you can use Encrypt Sensitive with User Key.",1521470514.0
klasius,"Assuming we are talking about SQL Server and SSIS, you can store the credentials into a .dtsConfig configuration file. The package protection level should be configured as Do not save sensitive data, to prevent misclick executions.
",1521539197.0
Ay--_--ye,I don't have an answer for you but honestly you should only be using sqlite for prototyping and maybe your dev environment. You should definitely use postgres or some other database when you deploy the app.,1521471259.0
Naeuvaseh,"I am a full stack developer with a strong background in database design and data modeling. After looking at the linked article, I am a bit confused at what exactly the author is trying to convey... They were using a lot of buzz words for agile methodology for programming practices.  I personally don't think that you can necessarily take an agile approach to designing a relationak data model because it requires that all of the business requirements are established in order to define even the most basic of structures.

I'm a database purist in which your application is only as good the database is. If you have a poor, unoptimized design, your application is going to be just as shitty. A database is the foundation if any good application (if you're taking the relational database route). 

For noSQL solutions, agile could work because you're not worried about tightly coupling referential integrity with the data.

Either way, never heard of an approach where you should be agile with your data model.

If you're curious about how I go about starting from scratch when designing a new backend, PM me and I'd be happy to walk you through what I do.",1521429799.0
Margo-Klein,"I have experience as a DBA. I tend to be more agnostic in my approach, provided the database is planned well. And that seems to be a risk with this approach as it is laid out on that site. Just because an iterative approach works with software development does not mean it works with everything. (Sorry Agile supporters.)

If you were going to follow this approach then the only time I would use it is for small niche databases.  Or for those that are project specific (that is, they will not be used for long term historical data.)  In no way should it be used for enterprise databases, [data warehouses](https://panoply.io/data-warehouse-guide/the-difference-between-a-database-and-a-data-warehouse/) or mission critical systems.  

Why? Because the risk of data corruption and data loss are large. 'Refactoring' a database is not to be done on a regular basis. And the larger the data sets grow, the more time it will take to, in essence, redesign the database. Production systems tend to have limited offline time, the less risk the better. 

And anyone using such an approach should be extremely rigid with their backup and recovery processes.  All it would take is a slip of the finger or a bad network day and you could find yourself with an unusable production database. ",1521734728.0
autotldr,"This is the best tl;dr I could make, [original](https://www.nytimes.com/2018/03/18/us/cambridge-analytica-facebook-privacy-data.html?smid=re-share) reduced by 89%. (I'm a bot)
*****
> There were also questions from technology experts and others about Facebook&#039;s reaction to the news reports by The Times and The Observer, especially its decision to suspend the account of Christopher Wylie, a data expert who oversaw Cambridge Analytica&#039;s data harvesting - but also spoke out about it to the two news organizations.

> Now Facebook said on Sunday, Mr. Wylie is refusing to cooperate with the company until the suspension is lifted - a move the social network is not willing to make because of his role in the data harvesting.

> The two top Congressional Democrats leading inquiries into Russian interference in the 2016 election - Senator Mark Warner of Virginia and Representative Adam Schiff of California - called for investigations of the Facebook data leak.


*****
[**Extended Summary**](http://np.reddit.com/r/autotldr/comments/85j6uu/facebooks_role_in_data_misuse_sets_off_storms_on/) | [FAQ](http://np.reddit.com/r/autotldr/comments/31b9fm/faq_autotldr_bot/ ""Version 2.00, ~299033 tl;drs so far."") | [Feedback](http://np.reddit.com/message/compose?to=%23autotldr ""PM's and comments are monitored, constructive feedback is welcome."") | *Top* *keywords*: **Facebook**^#1 **data**^#2 **Cambridge**^#3 **Analytica**^#4 **question**^#5",1521463606.0
Naeuvaseh,"You're correct regarding a composite primary key not allowing NULL values, but I'm a bit confused with your suggestion to correct the design flaw. I hope you're not suggesting using something like an auto incrementing attribute in conjunction with the composite primary key. If you are, that would defeat the purpose of having a composite primary key that includes first name and last name to unique identify a record.",1521412700.0
grauenwolf,"> Where PictureID and UserID are the primary keys,

This means you can have no more than one comment per user per picture. 

If that's what you want, cool.

If you want multiple comments by the same user on a single picture, that won't work and you'll need a CommentID column as your primary key.",1521394908.0
wolf2600,"The other wolf is correct.  As an alternative to a commentID, you could also add a ""posted_ts"" timestamp value to the PK and store it to the fractional second.... conceivably you wouldn't have the same user commenting on the same picture multiple times during the same hundredth of a second. (yyyy-mm-dd hh:mm:ss.ff)

And with things like picture comments, you'd want to store the timestamp anyway, so when you display the comments, you could show them in chronological order of when they were posted.",1521396488.0
jlrobins_ssc,"Here's an implementation in PostgreSQL I'm gifting you. It assumes that you will be reading weighted random elements far more often than you either add new elements, update weights of prior ones, or remove elements. The strategy is to make things very read-optimized by having a trigger-maintained side-table to the one holding (product name, unnormalized weight). The side table holds a normalized percentage chance in the form of a [low-number, high number) range , where low and high are between 0.0 and 1.0. This corresponds with the return result of PostgreSQL's `random()` function, and probes into this side-table are index-assistable.

Extending to multiple kinds of products, as well as setting up a hosted Heroku installation is an exercise left to the reader or my paid consulting services.

-----
    \set ON_ERROR_STOP 1

    begin;

        -- Here's the model for the original request
        -- data: a unique name ('product'), and its
        -- unnormalized priority weight.
        create table weighted_product
        (
            product text not null primary key,
            weight integer not null check (weight > 0)
        );

        -- Trigger maintained table of normalized
        -- ranges within (0.0,1.0) representing the
        -- 'chance' of choosing a given product.
        --
        --
        -- PostgreSQL has extremely convenient range
        -- types for this sort of thing, which become
        -- doubly-nice when combined with an index
        -- which both prevents any overlapping ranges
        -- in addition to being used when probing for
        -- the product whose range contains a given
        -- point.
        create table calculated_product_range
        (
            product text primary key
                references weighted_product(product),

            computed_range numrange,

            -- Builds index as well as guarantees no overlapping ranges.
            -- Does not guarantee that (0.0, 1.0] is completely covered,
            -- (could be proven at the end of the update trigger easy enough,
            --   however).
            exclude using gist (computed_range with &&)
        );

        -- Statement trigger function to auto-rebuild calculated_product_range
        -- upon any changes to weighted_product, namely inserts
        -- or deletions, but also upon changes to weight.

        create function trigger_rebuild_calculated() returns trigger
        language plpgsql as
        $$
            declare
                total_weight numeric := sum(weight) from weighted_product;
                current_low numeric := 0.0;
                current_high numeric := 0.0;
                current_length numeric;
                row record;
            begin

                -- Out with the old ...
                delete from calculated_product_range;

                -- Visit each row in the base table
                -- and determine it's normalized percentage
                -- range. Then store into materialization table.
                for row in select product, weight
                    from weighted_product
                    order by weight, product
                loop
                    current_length := row.weight / total_weight;

                    current_low := current_high;
                    current_high := current_low + current_length;

                    insert into calculated_product_range
                        (product, computed_range)
                    values
                        (row.product, numrange(current_low, current_high, '[)'));

                end loop;

                return null;
            end;
        $$;

        -- Hook up statement trigger to entirely
        -- rebuild calculated_product_range contents
        -- after any change to weighted_product.
        create trigger auto_recalc_computed_range
            after insert or update or delete
            on weighted_product
            for each statement
            execute procedure trigger_rebuild_calculated();

        insert into weighted_product(product, weight)
        values
            ('a', 1),
            ('b', 2),
            ('c', 3),
            ('d', 4)
        ;

        -- calculated_product_range will have been populated
        -- with one call to the after statement trigger after
        -- that insert.

        select * from calculated_product_range
        order by lower(computed_range);

        -- And now a function to pick a random product
        -- based on the weights. Does so by picking
        -- a random number in range (0.0, 1.0) then
        -- probing calculated_product_range for the
        -- row whose range contains that value. This probe
        -- will use the GIST index over the numeric range
        -- type so will be very scalable.
        create function pick_a_product() returns weighted_product
        language plpgsql as
        $$
            declare
                random_val numeric := random()::numeric;
                chosen text;
                result weighted_product;
            begin

                chosen := product from calculated_product_range
                    where random_val <@ computed_range
                ;

                if chosen is null
                then
                    raise exception 'Did not find one!';
                end if;

                select *
                    from weighted_product
                    where product = chosen
                into result;

                return result;
            end;
        $$;

        select product from pick_a_product();
        select product from pick_a_product();
        select product from pick_a_product();

        update weighted_product
                set weight = 17
                where product = 'a';

        -- calculated_product_range got reubilt, with
        -- 'a' now dominating most of the spread.
        select * from calculated_product_range
        order by lower(computed_range);


        select product from pick_a_product();
        select product from pick_a_product();

    rollback;

The results:
-------
    psql -f weighted_product.sql 

    BEGIN
    CREATE TABLE
    CREATE TABLE
    CREATE FUNCTION
    CREATE TRIGGER
    INSERT 0 4
     product |                 computed_range                  
    ---------+-------------------------------------------------
     a       | [0.0,0.10000000000000000000)
     b       | [0.10000000000000000000,0.30000000000000000000)
     c       | [0.30000000000000000000,0.60000000000000000000)
     d       | [0.60000000000000000000,1.00000000000000000000)
    (4 rows)

    CREATE FUNCTION
     product 
    ---------
     d
    (1 row)

     product 
    ---------
     b
    (1 row)

     product 
    ---------
     c
    (1 row)

    UPDATE 1
     product |                 computed_range                  
    ---------+-------------------------------------------------
     b       | [0.0,0.07692307692307692308)
     c       | [0.07692307692307692308,0.19230769230769230770)
     d       | [0.19230769230769230770,0.34615384615384615385)
     a       | [0.34615384615384615385,1.00000000000000000000)
    (4 rows)

     product 
    ---------
     a
    (1 row)

     product 
    ---------
     a
    (1 row)

    ROLLBACK

",1521299777.0
3XBecks,"https://drive.google.com/file/d/0B8SdvjuqNAUjQ3BVaWJkS1NxYkFCbDNIYUtad01udEREUEUw/view?usp=sharing

Here's the link, I know you have to download the Access doc. I promise I'm not doing anything malicious. I'm just an idiot trying to do a good job",1521168165.0
ccb621,"What evidence do you have that many apps use many different databases?

The applications I have worked on have used 1-2 types of databases—typically PostgreSQL and, maybe, MongoDB. We have multiple *servers* to handle sustained demand, but that's it.

I can't think of many reasons an application needs multiple *types* of databases. You'll have to share some examples if you want help explaining the decision-making.",1521160034.0
wolf2600,"I've seen setups where there will be a smaller DB (usually just 1-2 tables) which contains application configuration settings/parameters, then another larger database to handle the actual application data.

But never more than that.",1521166896.0
alinroc,"Different purposes/different types of data are best served by different databases. I wouldn't put accounting data (ledgers, etc.) in a document database and I wouldn't put large quantities of unstructured data into a relational DB.

Sometimes you need to ingest large amounts of data quickly, then process it for longer-term storage and analysis asynchronously. That'll require at least two different types of databases/data stores.",1521207420.0
mercyandgrace,"Throwing away the idea of normalization or any notion of modelling, you could get away with using a single table, with columns: Date, Company, Category, Number. If you want something a little more nuanced, you should have seperate tables for each of the columns listed, with foreign keys relating them all together.",1521148966.0
DesolationRobot,"Table of companies (one row per) with an ID and all the info relevant to that company. Table of categories, also one row per. Then a table of company-categories with timestamp, company ID, category ID, and value. 

Insert into company-categories once a week and into companies only when you add a new company and categories only when you add a new category. ",1521175308.0
jon23d,"You are effectively describing sharding. In the exact scanario you described, however, to me it seems unnecessary, given that many database systems support geographic coordinates. Also, even with millions of stores, I doubt this would be needed.",1521122058.0
alinroc,"You're severely overcomplicating this.

1. As /u/jon23d pointed out, many RDBMSs support GIS data and functions, either natively (SQL Server has it native) or via extensions (PostGIS for PostgreSQL, for example). Record the store's GPS coordinates right alongside address, phone number, etc.

2. When the user searches, use an API (Google has one, I'm sure there are others) or lookup table (for ZIP codes - USPS publishes data you can load into your own table with ZIP codes and the coordinates of their center(ish) point) to get the coordinates of the location entered by the user.

3. Do a search on your store table, including a calculation of distance between the user's location and the stored location of each store. Again, the RDBMS will have a function to do this calculation for you. Order by that distance descending.

Sharding this out into multiple tables seems incredibly wasteful to me.",1521124380.0
da_chicken,"Pretty straightforward. Is there a reason you don't do this:

    psql  -c ""\\copy (select $sel from $table where created_at >= '$day' and created_at < '$next_day') TO STDOUT WITH CSV HEADER"" | gzip > $table-$day.csv.gz

AFAIK that should work, and would save a step and a lot of unnecessary IO.

Also not super keen on the use of string concatenation, though it would be a bit of work to correct while maintaining the functionality.
",1521140256.0
mike8675309,"Big query recently added a number of DDL statements that you may want to investigate as they provide more control over the table that is created at runtime including the ability to specify partitioning and the column to use for it.
https://cloud.google.com/bigquery/docs/data-definition-language",1522850911.0
liquidpele,"Learn everything on this site :)

https://use-the-index-luke.com/",1521077629.0
jlrobins_ssc,"Transactions, constraints, triggers as offered by a more feature-rich database than Access, such as either the free version of SQL Server or PostgreSQL.

Then figure out how to use Access as a front-end to said database.

Also, find problem domains and then try to model their needs in SQL. Rinse, wash, repeat.",1521060964.0
R4bbidR4bb1t,"Summer studies curiculumn includes.
1. Beer
2. music
3. Parties
4. your choice of companions. 


",1521135620.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/programming] [Tick or Tock? Keeping Time and Order in Distributed Databases](https://www.reddit.com/r/programming/comments/84eof7/tick_or_tock_keeping_time_and_order_in/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1521044781.0
supra621,"I haven’t tried this for PostgreSQL yet. Does it work well?

I’ve had issues with other software cooperating nicely with it in the past.",1520975012.0
SociableIntrovert,I use this for MariaDB. It's definitely one of my favorite clients. ,1520983466.0
DesolationRobot,I've been spending more time in SQLYog as of late. Which means I haven't tried Heidi for I'm sure a good number of versions now. Any recent developments that make it worth trying again?,1520998033.0
rigamaroo138,"I'm on a data migration team and Dbvisualizer has been a great pick for a jdbc-friendly DB client. Solid feature set (I do have a pretty lengthy feature request though). I bounce between Oracle, Postgres, and SQL Server connections daily with no issue. We use the premium version - fyi. 

Edit - I've never tried HeidiSQL. Looks decent enough. In the free SQL client market space, I'm curious what it offers compared to SQuirreL SQL.",1520998300.0
myringotomy,These days I mostly stick with datagrip.  Not the best in everything but pretty good at most things.,1521019867.0
johnmudd,"Great product. I've donated.

Installs smoothly on Linux via Wine. But... does not include libpq.dll so I can't open my Postgres database. I use it for MySQL though.",1521033369.0
HeWhoWritesCode,+1 whenever I need to recommend a quick db client for friends and family.,1521325662.0
Tim_WIV,"I've been thinking of switching from HeidSQL to something more stable.

Every version I've tried in multiple years crashes and deadlock constantly.",1522308734.0
pcsag,Database marketing is using customer data to deliver more personalized and effective marketing.,1520965365.0
Resquid,"Eh, it's not really *my* data so it's whatever. Just a bunch of insurance claims and medical information. Blah blah.",1520908673.0
kevin3030,"Whatever you do, make sure your resume/CV are stored separately, offsite. 

Is there a context to your questions?  Are those just questions to get us thinking about our HA/DR plans?

Half the answers are - depends on business requirements, risk tolerance and budget. ",1520908878.0
supra621,Data centers tend to be redundant throughout more than just one building. A gas leak would be a rare occurrence since servers generate enough heat by themselves.,1520938061.0
r3pr0b8,"teh google is broken for you?

there are literally thousands of results when you search for **how to  create, edit, add, remove values inside the Database**

here's one -- https://www.essentialsql.com/introduction-sql-server-data-modification-statements/",1520879909.0
DRdefective,Reverse your capitalization. Lol jk. I would start with the w3 schools tutorial. Use the try me parts,1520879801.0
awo,Try pgexercises.com. Learn by doing tutorial for sql. ,1520914122.0
cowp13,"> Is there a course online

youtube, udemy, so many more",1521042790.0
Psps22,Info about storing time series in the DB: http://siridb.net/blog/how-we-store-time-series-in-siridb/,1520852591.0
eduard93,"> Batch up your queries

True enough, but irrelevant to database design. 

> If you can anticipate a join, try to avoid it 

That's a bad idea in a transactional DBMS. Denormalizing the data is possible and sometimes needed for analytical DBMS, but your example is clearly transactional. Also indices on joins remove the problems with speed.

> Add new columns frugally and don't split your data for no reason 

Again, normalization exist for a reason - it works and allows easier management of big data models. ",1520778873.0
mkingsbu,"I think this advice is missing some important context because the examples are not particularly conducive to the type of work you do.

For example, I've spent hours trying to match the names of people in a ""name"" field for which it was not clear if a middle name should exist or for other such peculiarities such as suffixes or prefixes.  In response to another post you've mentioned you should do things like this if there's no chance in the future you'll do something... there is never a zero chance.  Sure, maybe your query runs 10 seconds faster... but somewhere 10 years down the road now a developer is spending 2 days cleaning up 10 billion rows of messy data. Now, admittedly you *might* be able to get away with a single names column since the rules for them are modestly straightforward but I'd suggest thinking really hard about whether or not your performance gain is worth that level of ambiguity.

In another part, you mention that if you eliminate the join, you will have faster inserts.  In your model, you related people to many cars.  In reality, the cardinality should be many to many. In other words, you have a car_person table that only has the two IDs in it.  The query to insert into this would be simply:

    insert into car_person (car_id, personid) values (%s, %s)

I'm unaware of a query that would be more optimized than that.

As far as the inverse of this, querying the data, obviously indexing is something you mentioned.  The country column would definitely be a candidate for partitioning.  Your query would basically treat the country as a single table so the scan would only be as long as it takes to retrieve a set that is already defined.  That still may be slower than a single row scan although I highly doubt doing a full table scan of a full text column would be...

Now, I'm well aware that there are benefits to denormalized data.  I denormalize data all the time, particularly into a wide format for statisticians to analyze.  So I don't think that your best practices are wrong, I think that in the dichotomy I presented at the top, it's that you would probably want to come up with better models.  I'd probably suggest doing an A,B comparison on speed too. Actually build out the model and give how many ms and % faster several queries are between the two data models. There are a lot of people in my boat where normalization is way more important than any other consideration and they would be wise to disregard your advice.  Putting the level of context of providing actual queries and data would go a long way towards making people like me who start to read the article eventually stumble upon the very specific subset of people who this would actually (probably) be good advice for. 

In other words, someone who does similar types of jobs that you do and would read this and think, 'wow, this guy nailed it.' And another guy (ergo myself) who reads it is thinking, 'I know this guy wants speed but goodness, I'm feeling anxious even thinking about trying to munge this guys data'. In which case, you would have only served to reinforce those who already know to do what you do.

At least that's my take on it, I hope the comments are helpful. You obviously know what you are doing but it isn't clear you are conveying that accurately!",1520783805.0
elcric_krej,"This is a shameless plugin of my article, mainly because I'm hoping to get some feedback on it.

It's more centered around various well-known and less-well-known best practices, or at least what I'd consider best practices, when working with large amounts of data and wishing to reduce size and decrease query time.",1520776756.0
r3pr0b8,"TL;DR

you maybe looking for **supertype and subtype tables**",1520688538.0
codemagic,"Just a quick note on subtypes; they are a logical data model construct to be able to reason out the differences between subtypes. When it comes to physicalizing the model there is an aggregate that occurs to “resolve” the super/subtype relationship. You either roll up or roll down, and you’ll need to decide what works best for your implementation. Good luck ",1520711051.0
welshfargo,"I don't see the point of the company_role table. You brought it up in the context of having to track different types of asset. I would contend that the asset type implicitly defines the company's ""role"".",1520706004.0
databaseprovidersonl,https://www.databaseprovidersonline.com/hni-database-ahmedabad/,1520622397.0
da_chicken,"> Case in point, Microsoft did not add Window Functions to SQL Server until SQL 2012.

This is incorrect. Microsoft began adding window functions in SQL Server 2005, which included ROW_NUMBER, RANK, NTILE and DENSE_RANK.",1520589197.0
grauenwolf,"> In MariaDB, a non-recursive CTE is basically considered to be a query-local VIEW whose syntax is more readable than nested FROM (SELECT …). 

As in an actual view? Not a temp-table like PostgreSQL? 

",1520611338.0
dessmond,"I booked and next day want to book for a friend   
",1520534664.0
liquidpele,This is a project for school isn't it.  ,1520532323.0
space_sounds,What if a user wants to book multiple events?,1520536147.0
lukaseder,"Thanks a lot for posting this. I'm Lukas from the company behind jOOQ (and behind this translator). This is work in progress, so your feedback is very welcome!",1520528047.0
DesolationRobot,Easiest possible solution is a hosted database. You can get an AWS instance up and running in a few minutes. ,1520498118.0
mvelasco93,Try Heroku,1520527083.0
ash286,"There were way too many really bad, poorly written mistakes in that article.
Also, the huge amount of unrelated images didn't add to the content.

It's a shame, because this sort of thing really devalues the message about consistency and it's importance.",1520523345.0
SzejkM8,CockroachDB should be mentioned at least.,1520531352.0
mwdb,"I think we need more info about your data and how it is queried. You mention SELECT * but what are the WHERE clauses like, and the data/cardinalities of the columns in the where clause? Is it time-series data? Could you benefit from partitioning or GIST indexes? Expression indexes? Are you CPU-bound or I/O bound? Can parallel querying work for you? Might clustering help?  
  
500k rows isn't really that much. I used to manage a Postgres data warehouse (ca. 2007-2010, so probably running on a weaker machine), where I had one summary table (lots of joins etc. from base tables) of a few million rows and query time was consistently <1 sec. 

You could ditch Postgres and buy an expensive column-store db like Infobright, or a massively-parallel db like Impala, or Oracle with its oodles of data warehousing features, or you could use a specialized NoSQL database that might be more suitable for your use case (like a time series db if relevant)...but really it should be doable with postgres for a 500k table.

Also, offset isn't always the best approach to pagination - https://use-the-index-luke.com/no-offset",1520465214.0
xkillac4,"Look into elasticsearch. It is designed for exactly this application. Do a PoC, you will be amazed at how responsive it is. ",1520489715.0
klasius,"If the only ""where"" of the cache table query is the pagination you should add a new column in the table with some autonumeric based on the way you want to paginate the table.
The query would be like:
select * from cache_table where mycolumn between 50 and 100

I think every other index in this table is useless, and only degradates the performance of the refresh process.

In order to achieve better performance refreshing the cache table I think we need more data about the 30join query, the execution plan for example.

",1521538781.0
r3pr0b8,"you're at university, taking a university level course, and **you're asking reddit**????

is google broken for you?",1520351300.0
r3pr0b8,"inherit is a poor word here

a foreign key **references** a primary key -- and you get to decide which rules then govern what you can do to either the parent or the child in the relationship

for example, you can disallow deleting a parent row if there are any child rows which reference it, and you can disallow adding a child row unless the referenced parent row actually exists

in terms of weak or strong, if the FK in the child is not (part of) its own PK, then that's a weak relationship, and it implies a child row could exist (with a PK) without taking part in the relationship (NULL FK)",1520280469.0
crookedkr,"I'm not sure that this is a real question that they are looking for technical reasons for or just a prompt to get you to show your position. Perhaps it's to see if you are married to one technology or another (""oh postgres is the best I would never choose MySQL!"") or maybe it's to see if you know the licencing aspects, MySQL is now owned by Oracle, you can use Maria which is the fork that remains free under the GNU GPL. 

If they are digging for performance or tuning differences I would claim that tuning can be very sensitive to data set and workload and that until you set it up and start adjusting knobs you really don't know how fast you can get one or the other. 

If they think that there is some huge difference without a specific test then I would be skeptical of the shop in the same way that I'm skeptical of shops that require new devs to have X years in language Y.",1520267734.0
mkingsbu,"They're probably not looking for a particular answer. They're probably looking for your thought process. ""Well, it depends. Do we have any data in either system? I would look to determine if what we are developing had any particular requirement for a database engine. E.g. I was working on a project with Python and GDAX and it turns out their API has a built in engine for MongoDB.  That doesn't apply to either of these engines but if we were working on a project that already had something developed, go with that one. Also, do we have any other developers who are better at one that the other? If so... probably that one.""

This shows my junior status but I'd answer this question by basically saying I'd default to PGSql because it's the one that I have the most experience with and then add the caveats I mentioned above. Basically, most RDBMS engines have a really similar set of features. The syntax they have might be a little different and they way they are maintained are a little different but since maintenance isn't an issue you'd be agnostic about that particular aspect.",1520269414.0
grauenwolf,"Though MySQL continues to improve under Oracle's leadership, I still see no reason to use it for new projects when you have superior products such as PostgreSQL and SQL Server available. It can't match PostgreSQL for SQL compliance and feature sets or SQL Server for tooling and multi-threaded performance. 

Case in point, MySQL still doesn't support check constraints, an essential element in robust database design. Instead you have to merely hope that your application developers never make a mistake.",1520270213.0
jlrobins_ssc,When your operating environment / sysadmins already support MySQL and your development shop is already well versed in it?,1520266111.0
liquidpele,"MySQL: Great if you've never set up and built a database berfore due to: easier install and initial setup, nicer docs (imho), easier administration (mostly due to not setting up sequences separately), etc.  

PostgreSQL:  If you know what a trigger does and how to properly index a table, then you should use this.  You won't hit a wall with it. ",1520292656.0
elcric_krej,"The only real difference I've seen between mysql and postgresql are:

a) Postgres has better support for complex queires (e.g. more advanced cursor queries, array data type, composite data types, JSON/XML operations)

b) Mariadb actually has an easy to use r+w clustering tool (in the form of Galera cluster)

c) Mariadb has 3 storage ""main"" storage engines in the from of Tokudb, Innodb and the CS engine (though that one is very incomptaible with it's sql syntax and concepts... it's argaubly a whole new database)

Performance wise you could have a 5 days discussion and get nowhere, basically it's very dependant on the machines, the data type, the query size, the r/w ratio and 1001 other things. There's no ""clear winner"".

I find Mariadb to be easier to setup optimally, with more of what I'd consider ""sane defaults"" being included as actual defaults. But that's when you get into heavily subjective matters.

I'd also argue that the monster which is the postgresql codebase is basically unbreachable to all but the craziest of humans, mariadb is based on an old codebase but... somewhat managable, and written in a much modern language in the form of semi-modern C++. But, again, that's subjective.

Almost all of the time you could use either and do your job. If you really ""need"" to use one of them, what you ,probably, actually need a different type of database (high guy who implemented a large scale object store in postgresql and guy who implemented a dynamically generating date partitiong based schema instead of using a column store).",1520277063.0
leandro,"You should never use MySQL.  Period.

MySQL is just badly engineered: 31 Feb used to validate, for instance, and critical, data loss bugs went unfixed for several years and major releases.  Seems faster with trivial, irrelevant tests but is much less efficient with any real life load.

PostgreSQL is much more solid, efficient, has a better, more standards-compliant SQL implementation, and progresses faster too.",1520273868.0
Dolphinmx,"Like all answers, it depends.

Both databases offer similar and different features that it depends on the project requirements.

From a cost perspective it depends which will cost more, licensing/support cost, training developers/sysadmins.

Without knowing the details is hard to recommend, you can recommend one based on your personal experience but that doesn't mean is the best option for the problem.",1520292831.0
larsga,"MS Access sounds like just the thing for something like this. It does sound like Excel is not enough for your needs. Finding all the cases like you describe is easily done with SQL.

What you call database is usually called a table in database terminology. Every table has a unique key (from one or more fields). Your unique numerical ID is a perfect key. You can use columns to refer from other tables to this key. That's how you build a database of interlinked tables.

Building something like this should be doable for someone who isn't technical but does have plenty of time.",1520099938.0
crookedkr,"Whatever you do is going to be a couple hours a night for a week or two and if you are going to put time in to learn, I would suggest just about anything other than access. Postgres can be a little much to set up if you are new to it. I would probably suggest Maria and even though sqlite isn't really the right tool here it would still be better than access.",1520102473.0
sea_of_FIRE,"Also, FWIW (not much I suspect) I do have a bit of programming knowledge in Python.  Nothing much, I can write very simple web scrapers and simple scripts to extract data from csv files and stuff like that.  Wasn't sure if that was helpful or relevant here so left that out of OP, but thought I'd share.",1520103464.0
redditfanster,"take a look at this book. it will get you started with database concepts and how to interact with one using a no cost, very capable, and cross-platform (available for use in windows, linux, mac os, and even some mobile phone operating systems) product called sqlite. 

for more info about sqlite, go to: http://sqlite.org/index.html

the book info:

Getting Started with SQL: A Hands-On Approach for Beginners
by Thomas Nield
O'Reilly
ISBN-10: 1491938617

here's a portion of the book's description at amazon's web site:

...
You will quickly master the fundamentals of SQL and learn how to create your own databases.

Author Thomas Nield provides exercises throughout the book to help you practice your newfound SQL skills at home, without having to use a database server environment. Not only will you learn how to use key SQL statements to find and manipulate your data, but you’ll also discover how to efficiently design and manage databases to meet your needs.

You’ll also learn how to:

Explore relational databases, including lightweight and centralized models
Use SQLite and SQLiteStudio to create lightweight databases in minutes
Query and transform data in meaningful ways by using SELECT, WHERE, GROUP BY, and ORDER BY
Join tables to get a more complete view of your business data
Build your own tables and centralized databases by using normalized design principles
Manage data by learning how to INSERT, DELETE, and UPDATE records",1520145229.0
Grundy9999,"As a fellow lawyer who has a database background and now find myself making custom Access databases for my law firm, I agree that Access is a good tool for what you propose.   Access is relatively easy to learn, as it was designed primarily as a desktop tool for end users, but you can do some pretty sophisticated things with it if you want to invest the time and use the coding language associated with it (VBA, short for ""Visual Basic for Applications"").  For example, you can reach out into the internet and import information from data sources on a periodic schedule, or download and store links to pdf files, etc.   

If you want to share some more specifics about your end goal here, I can share some thoughts regarding the basic design you may want to consider.  I can already see that you are going to want to have several interrelated tables - parties, cases, and judges, perhaps more.  Thinking through the way that these entities will relate to each other, in connection with the information you are tracking, is an important exercise early on.",1520100974.0
Pwdr_Mnky,"The book Database Design for Mere Mortals helped me a lot when I was starting out, it's not a bad place to start if you want to learn how all this shit works. There's a lot of terminology and tricky concepts but once you've got a handle on those the basics are pretty simple. Easy to fuck up too though, so try and figure out the difference between good and bad database design (You really need some help with that bit, it's a fine line sometimes). 

Draw.io is a pretty cool online tool for flowcharts that can help you to design a database. And by all means post your designs here and they can be critiqued. 

MS Access is tailor made for your needs, so go with that.",1520102572.0
welshfargo,https://www.reddit.com/r/MSAccess/,1520108450.0
DROP_TABLE_UPVOTES,"I'm going to interpret ""working as an academic"" as you are working for a university.  Do you have any IT resources available from your institution?  At our school we have a lot of services/systems available for researchers to use and all they have to do is ask.",1520177755.0
AQuietMan,"I got my start with litigation support databases and financial management software for law firms. I did presentations on litigation support database design and implementation at Bar Association conventions for several years.

I guarantee that you'll regret designing and implementing this yourself.",1523706825.0
dstrait,"Are you using a relational database like Oracle or SQL Server?

It is strange to me that the data is so non-normalized.",1520037326.0
xkillac4,Prototype and benchmark. Only way you'll know. Shouldn't take too long to build and measure. ,1520037028.0
not_so_humble,"Pretty sure your DBA wants to create a separate table to hold those codes with a link back to the main table not duplicate 700 million rows up to 25 times just to eliminate columns.

I agree that it should have been done that way from the get go.  There's a reason normalization is a thing.  However, he now needs to show the benefits over what you have.  Prototype and benchmark like everyone else said.  I'm thinking it will be faster for analytics like pulling everyone who had code: XYZ in any position and maybe a bit slower for getting an individuals record for a specific instance.  But by slower I mean only milliseconds slower.",1520046955.0
mihirjpatel,"I agree with the DBA.  I assume you have a development cycle so the new data structure would go through performance tests.  I understand you are using SQL server.  You can always use views to display the data in the denormalized form as you may have it now.  

",1520045529.0
stickman393,The DBA should thank his or her lucky stars that the diag codes aren't crammed into one 200 char field,1520037152.0
wolf2600,"Your new DBA is right with his instinct to get rid of the 25 columns, but wrong in his approach to doing so.  

Create a second table with 3 columns and a concatenated PK of:

`CLAIM_ID, CLAIM_LINE_ITM_ID, DIAG_CODE`

Then if you have a single claim line item with multiple codes, you'd have multiple records in this new table.

Then during queries, just join to the new table to get a list of DIAG_CODES associated with a single claim_id and claim_line_item_id.

Do you have an example of how this data is being queried/used?  That will determine the best way to design the schema.... For example, will having 25 identical lines (with different diag_codes) be the ideal format for your data to be returned?  Or do you query on a single claim and just want a list of the diag_codes?  Or some other report format?  

Figure out desired output format of the data, then work backwards to design your schema to best support the output.
",1520048722.0
smokinggun46,"**Is this statement fundamentally sound:**  Our current tables are denormalized as an OLAP system.  When we do our analysis, queries are either satisfied by ONE table or a complex web of tables and aggregates.  Our new DBA is used to normalized OLTP systems.  

And to be fair to him: our OLAP tables have been getting slow AF and weren't properly indexed nor partitioned.  He was offering a solution based on his experience.  ",1520085401.0
dstrait,"You've already got an example. 

A group of several fields with the same name but appended with numbers are a dead giveaway for non-normality in any data model I've looked at. Usually there are only a few, maybe three to five repeated fields. I think that twenty-five is a record, for me. Among the reasons that this is bad are:
1.  It doesn't work well with the way that SQL Server creates and uses indexes
2. It makes writing queries nuts. I presume that any particular diag code (ex: pneumonia ) could wind up in any one of those 25 fields, and be in different fields for different claims. That means the query has to search 25 different diag fields in the WHERE clause, which makes the query a hassle to write, makes it easy to forget one of those fields where wrong the query, thereby causing a bug and makes it hard (maybe even impossible) for the query optimizer to use an index even if the index exists.

Back to your actual question: without knowing the size of the fields in the record (or three King of hardware you have), it's hard to say for certain, but going from one row with 25 repeated fields (as well as a bunch of other fields) to 25 repeated rows (with fewer fields) is very likely to drive up the overall size (measured in bytes) of the table because you will be repeating all of those other fields in each row. 

Bigger tables usually mean longer running queries, all other things being equal. Doubly so, if the query optimizer can't use any indexes. 

I'd also want to know how you are accessing the data. Are you looking for all of the data on a claim? Are you looking for all of the data on a particular diagnosis? Maybe a particular recipient or provider, which will cover many claims? 

I'm also curious as to why a DBA got into the picture. Are there current problems that need to be solved or is this a general review for best practices? Is there sure in production? Is it good enough, if not perfect?

My shoot-from-the-hip idea would be to look at breaking the diag fields out into a child table and then joining to it, which is what I would expect any DBA to recommend. The child table would have the a field for the primary key of the claims table, a field for the dish code and a field to indicate the sequence of the diagnosis for a claim.

Alternatively and/or additionally: 
The column store feature might be able to help you. 
The data compression feature might help. 
Analysis services might help. 

No matter what, be sure to test before and after to measure what is better. One experimental result is worth ten hypotheses.

(Is there an acronym or short hand for saying ""Sorry for the formatting, I'm on a phone""?)",1520086843.0
mkingsbu,"Unfortunately, a lot of claims data are setup like that.  I 100% agree with your DBA.  SQL Server has a row over() function that will let you represent your data vertically like:

Person1: Diag1: 111.11
Person1: Diag2: 123.25
Person1: Diag3: 153.00
Person2: Diag1: 324.93
Person3: Diag1: 500.01

Etc.

Then all you need to do to get the data like how it is currently arranged is to pivot on the second column up to the maximum number of diag codes there can be.  I just built something very similar for a similar dataset with around 120 million rows and it completes on a machine with mediocre resources in seconds.",1520048032.0
whisperedzen,"What I don't understand is the motivation for the change. Are you experiencing bad performance, using to much disk or something?    
Because if you are not, then I see no reason to do this, even if it could theoretically help in some cases.     
The is just no way to justify the cost in time and the added risk of migrating the data to fix something that wasn't broken.    
There are technical reasons to normalize or not, and we can't give a real answer without knowing the details of how you are storing the data and how the applications interact with your database. What I fear is that the DBA has no real reason to do this except than to prove something since he is new.",1520049457.0
Sharks_With_Knives,"Cool project! I'm not sure on this, but I recall usually seeing the foreign keys listed at the bottom of the table's fields, rather than immediately following the PK. 

You could also look into crows feet notation for signifying if a relationship between 2 tables is 1 to 1, 1 to many, or many to many. And in the event of many to many, create a bridge table.

Have you written SQL create table statements yet to reflect this model? That is another cool thing you can do with this.

Goodluck!",1519975480.0
dreb87,What software did you use for this?,1520356569.0
CowboyProgrammer,"I personally name the Primary Keys as ""ID""",1519990642.0
ben_it,"I'm not sure what your question is.
Are you asking if your data model will meet your requirements?
If so, it looks OK to me, but I don't know your complete requirements.

What do you mean by ""make a base of datum""?",1519944958.0
Jbusiness,"After looking at your schema it's kinda hard to tell what's going on relative to what you've posted above.  If possible you should try to make a simpler flat model first and see if that works for what you are trying to do.  With 500k records per year I wouldn't worry about performance/optimization in a mySQL OLAP application.  From your description it doesn't sound like this is an OLTP DB.
",1519959593.0
mtVessel,"Le service de traduction que vous avez utilisé (Google?) a fait un mauvais travail et vos mots sont corrompus. Par exemple, la façon dont le mot «authority» est utilisé n'a pas de sens en anglais. Ironiquement, je pense que le mot recherché est ""instance"", qui est le même en anglais et en français.

Cependant, si je comprends bien votre intention, je ne vois pas comment vous capturez la présence dans une instance spécifique d'une leçon, puisque les élèves semblent être liés à la classe elle-même, pas à l'instance.

(Traduit avec Google Translate - s'il vous plaît pardonnez toute grammaire ou utilisation incorrecte)",1519960369.0
strohfliiip,I think what you need is a self join but i did not umderstand the question 100%.,1519849046.0
mkingsbu,"Select space1, space2 from table2 where space1 = 'C' or space2 = 'C'",1519854670.0
mkingsbu,"Or if you wanted them all in the same column:

select space1 from table2 where space2 = 'C'
union
select space2 from table2 where space1 = 'C'",1519854734.0
svtr,"the implementation as far as having queries against this performing decently will depend on the DBMS you are using. 

on a relational database, the basic thing is, you create a table : 

    ID    ParentID
    1      NULL
    2      NULL
    3      1
    4      1
    5      3

Something like that. And on MSSQL for example, you can write a recursive CTE to query trough the hierachy. On MSSQL you could also use a HierachID datatype, which provides basicly the same thing, but gives you system functions that help dealing with hierachies. To give you a more useful answer i'd need to know more about what you are trying to do, and on what system you are trying to do it.

If you are talking about graphs where you have n:m relations in a very redundant way, I can give you a proc i've written to deal with shit like that to transform a graph to a flat tree, to be able to ""work"" with it, but well.... its not an easy read (it's a contains more comment than code kind of thing).... its really damn fast thou.",1519858919.0
crilen,"XAMPP stands for Apache + MariaDB + PHP + Perl and it is a development environment.

I'm assuming you are using MariaDB or MySQL for your database then.

You can use a MySQL admin software to import and export. Such as phpmyadmin or mysql-front or MySQL administrator.",1519804327.0
welshfargo,"It's not video, but [this site](https://use-the-index-luke.com/) has much of what you want.",1519766632.0
mandru,"Hello.

 This one is pretty good

https://youtu.be/Z4hKomnGHFA",1519799915.0
grmpf101,Product specific and not a video but maybe helpful as an overview https://www.arangodb.com/2018/02/indexes-types-arangodb-part-1/ ,1519987712.0
wolf2600,"Our finance users run reconciliation reports between the source system and the data warehouse to ensure all data has flowed into the DW.  Basically they're just aggregate DB queries, like:

    select extract(month from trans_date), prod_line, sum(order_amount) from table
    where trans_date between date'2017-07-01' and date'2017-12-31' 
    group by trans_date, prod_line
    order by prod_ln;


Then do a similar query on the source application and compare the values to see if they match.",1519739680.0
mcstafford,"If you're using MySQL, then give [pt-table-checksum](https://www.percona.com/doc/percona-toolkit/LATEST/pt-table-checksum.html) a look.",1519745732.0
ClemsonLaxer,"On our company's ""Data Governance/Data Quality"" team, we'd often have data controls that ran on a scheduled basis.

Data controls being ETL processes that do these comparisons (we used IBM Datastage) and captures exceptions. In this case the exceptions were rows that failed the reconciliation.

We'd store these values along with key identifiers in a data mart that had a dashboard on top of it for users to look at. ",1519766863.0
BinaryRockStar,"So you have Oracle DB installed on a server and can't access it from another machine using SQLPlus, is that right? If you run SQLPlus on the server itself, can you connect that way? Is the server software starting up correctly? Check the system logs, Oracle logs etc. Letting us know the OSes involved would help as well.",1519635939.0
bedknobsandbroomstix,"Just a few comments, could be nit picky, but I've always felt that making table names plural is never really a good idea.  It makes it harder to follow the primary key name sometimes.  You have a teams and a teamid, but also a people and a personid.  It's not very consistent, additionally you don't apply this everywhere, like pitchoutcome.  

Also not a huge fan of breaking dates out into component parts, can make it a pita to work with in the future.

Additionally, every table should generally have a datecreated, datelastmodified fields, and you should figure  out how you are going to handle deletes.  Just straight out delete the row, or do you want a history and should think about tomb stoning?",1519534364.0
armastevs,Having a separate table for the pitchers hand seems like overkill.  Why not have just one field that is R or L,1519526560.0
crilen,"The dates table scares me. Don't do that.

Franchises has OriginalCityId, but no CityId, why are you confusing your indexes for no reason?

Make the PK in its own table just ""id"".

""IsBatter"" and ""IsPitcher"" should be one field

Teams are connected to sports through seasons? That's confusing.",1519545077.0
goblando,"So, I don't see a way to track players moving between other teams.  I am guessing you are trying to track baseball data at the pitch level.  So, you are moving from a pitch outwards.  

Pitch (Game, inning, sequential number, pitcher, hitter, pitch style, pitch hand, batter hand, outcome)

Game (season, location, starttime, endtime, home team, visiting team, gameconclusion, hometeamscore, visitingteamscore)

Player (Id, name, etc)

Season (Id, start date, enddate, name)

Team (id, franchise, name, etc)

TeamhomeLocation (teamid, start date, enddate, locationid)

TeamPlayers (teamid, playerid, start date, enddate, position, status)

That is the way I would handle this problem.  For the end date, I like to use a static expdate of 1/1/2100 instead of null so the query is written with more consitency.",1519579724.0
jDave1984,"I'm building a sport statistical database, but starting with Baseball. The goal is track each game pitch-by-pitch. When I first tried modeling it, I had a very large Pitches table. I've since tried to normalize as much as I could",1519521649.0
drazde,"Marginal question..., what do you use for entity relationship?
Is it opensource? ",1519553949.0
Oxford89,"What is the point of having a dates table? Just replace the DateID field in every FK instance with an actual date. You can use built in functions to get day, month, year, etc without a separate tablespace.",1519592192.0
wolf2600,"Are you learning SQL the query language or SQL Server the DBMS?  My suggestion is to learn a DBMS like MySQL or even Oracle first, before MS SQL Server.  



This was how I initially learned SQL:

https://smile.amazon.com/Oracle-Database-Fundamentals-Guide-1Z0-061/dp/0071820280/ref=sr_1_1?ie=UTF8&qid=1519524014&sr=8-1&keywords=oracle+sql+fundamentals


",1519523913.0
PM_ME_YOUR_GLIMMER,"I am not an expert in SQL, but I am the only one in my database class that knew it before going in. Also the teacher tends to pick me to answear questions, and my classmates come to me as well for help. The way I taught myself was using W3 schools website for syntax, and using code academy to actually learn how to perform SQL statements. Also if you have any question I would love to help if I can. ",1519535311.0
InternetBowzer,"I wrote about this often asked question. Check it out: https://www.mlakartechtalk.com/how-learn-sql-server-free-starter-pack/

You’ll get the theory from lectures and textbook. The practical usage you must learn by doing. Good luck!",1519518395.0
NormalAvrgDudeGuy,I'm not sure if there are better alternatives but check out [ Sql zoo]( https://sqlzoo.net),1519548113.0
stebrepar,">Any Free Resources to Practice SQL outside of class?

Maybe check out [SQLite](https://www.sqlite.org) if you don't want to have to install and maintain any of the big SQL database applications.",1519579220.0
welshfargo,"Learn to do ""reasonableness checks"". By that I mean figure out what a reasonable answer should look like. SQL is deceptive - its easy to learn the basics, but set logic can trip up the unwary. You can get answers easily, but how do you know that the answer is correct? You run other queries to validate the answer. Accountants do this - they call it ""cross-footing"". Have fun.",1519608695.0
o_edo,Exercise and course for free : http://www.studybyyourself.com/seminar/sql/course/?lang=eng.,1519713020.0
webwoo,"In addition to all, I advise you to read this old but very useful book ""High-Performance MySQL""
https://books.google.ru/books/about/High_Performance_MySQL.html?id=D0b_Xg3UeXEC&source=kp_cover&redir_esc=y",1522855410.0
crookedkr,"The term is ""universal table"". Its the case where all the normalized tables are joined. This turns queries into scans without joins. There is some research around this but they aren't widely used because updates suck and the space the table takes up is enormous.

Edit: you could also just be looking for the term ""denormalized"" or maybe ""first normal form""",1519428958.0
onewheelonly,"I can't see a reason why I would ever make a table for this. The few times I've needed something like this I have used a view with CROSS JOINs.

If you're talking about a name for this way of combining the data, I would call it a Cartesian Product.",1519492703.0
eshultz,"Typically those 3 entities would exist in their own tables, and the table you have there is known as a junction table. Usually a table like that represents *something* - facts or events. There's not much use in having every possible combination of values persisted in the database, when you could get the same behavior using a cross join on the tables.

    SELECT client.name, country.name, product.code
    FROM client
    CROSS JOIN country
    CROSS JOIN product",1519428715.0
Optrode,That's what views are for.,1522165711.0
johnfrazer783,"That's the Table That Is The Reason Why We Have Relational Databases, also known TTITRWWHRD for short.",1519432670.0
changeupcharley,How much will you pay me to do your homework for you?,1519376259.0
welshfargo,"I am not an expert on branding, but I think you should rethink the name of your product. Just imagine a sales pitch to senior management of a potential customer.",1519499934.0
onex0907,"In a relational database management system (RDBMS) such as MySQL,  you would have (at the most basic level) each sheet become an entity/table (search normalization to find out how you really want the data to be structured).  The excel columns that you match on would likely become the Primary and Foreign keys of your tables.  

You would not use formulas such as VLookup or Index/Match but rather use SQL to create a join (if you're not familiar search SQL Joins).  I'm on mobile right now but will be home in a few hours.  If you want to provide your spreadsheets I can show you what your tables might look like.  ",1519354583.0
gullevek,"""UTF-8’s dominance has been partially driven by “adopted words” from foreign languages, but more likely the main factor has been its support for emojis.""

Seriously? What kind of strange point of view is this. This is a truly strange blog.",1519377632.0
chickeeper,Had a developer run into this and never looked it up before. Just took it as the beginning.  So I dug a bit deeper into the [Gregorian calendar](https://en.wikipedia.org/wiki/Gregorian_calendar)  I have been a DBA for 15 years and never looked this up before.  I am ashamed.,1519250182.0
welshfargo,"It's been a while since I used PD, but I seem to recall that you can manage sub-models and version control in the model repository. You can also link models, so that might be an approach. Check the docs.",1519251945.0
NoNotTheDuo,"There's no rule that a data warehouse has to be internal only.  Probably what you've found so far is referencing internal facing dashboards because that's the biggest use-case for a data warehouse.  At the end of the day, a data warehouse is just a different representation of the same data as your OLTP system, so as long as you can securely surface dashboards/widgets/reports to your customers from a database, you'll be fine to use a warehouse.",1519246879.0
Phnyx,"Replicate your live database to a cloud based instance on AWS, Azure, etc - ideally into an analytics database. Either do a nightly update with easy ETL or sync them with a data management system every few minutes if you the data to be more up to date. This also allows you to scale out easily in the future.

On this you can use various BI tools like PowerBI, Tableau or Looker to create reports that can be embedded into webpages or shared with your customers if they get a special account (only works with b2b customers - don't know your situation).

 If you have just a handful of different reports variations, copy your base reports and change the parameters directly (like 10 different suppliers each with their own filter). If you have more you need to find a way to pass on their IDs as filters to the report without it being visible on the webpage.

Depending on how much data goes into the reports and how often viewer needs to interact with filters, a ""import"" is often faster for the user than a ""direct query"" report.",1519241490.0
welshfargo,"I have worked in BI/analytics for 20 years and I have never heard of this ""rule"" about data warehouses being for internal reporting only. A star schema is just a data model - it can support any reporting interface that you choose. Just don't go crazy with superwide fact tables, and use partitioning where it makes sense.",1519313072.0
a_s_clark,"I'd suggest looking into RPO and RTO. The point of any backup strategy is to be able to recover in a specific amount of time with the possibility of a specific amount of data loss. The various backup options available are just tools to meet those objectives, and are usually used in combinations.
Then you can compare the options available in each to meet those goals.",1519199099.0
Bakuwoman,"I have done a few BIGINT projects, and I prefer making a new table then “swapping” the tables. That trigger to bomb inserts, updates, and deletes is not the way to do it in my mind. If your application is still trying to interact with the table missing data is the last thing you need. The other thing about his documented process was it seems very manual. I build a configuration schema to generate and execute the scripts for this process, and I would never try to do any production code changes depending on a development server. Having said that, it is interesting to see other approaches people take.",1519126588.0
msiekkinen,"I was involved in a pretty large mysql conversion of int to bigint.  We were running out of int IDs even though unsigned it was largely a java shop where everything app side was signed anyway.  

There was more engineering effort converting ints to longs and running into things like oh shit this old code base was using an int as an array index and you cant do that with longs in java....

Database side wasn't really that hard since we already had a series of replicas, some not really being used.    Had the luxery of doing an alter on an unused slave, letting it catch up, then do a master/salve promotion/failover.

We had enough hardware these expansions weren't really much of a concern for memory/disk/backup space required.  ",1519148371.0
BiggRigg,I've known great DBAs with English degrees and terrible ones with masters in computer science.   Your diploma only matters for your first job/internships.   From there your work resume is the only thing you need.    ,1519072894.0
darcMadder,I don't think there is a specific degree for DBA as long as you know your shit.,1519071484.0
MDiesel26,"Best way to become one is to try it out and work with it. SQL Server, MySQL, Oracle are big names in database software if you want to jump in they all have free/developer versions.",1519074245.0
dripxlife,[deleted],1519086800.0
186282_4,"I've got a high school diploma, and now I have 20 years of experience as a DBA. 

What platform have you chosen to start with?",1519091165.0
wolf2600,"...said nobody ever.


[just kidding]",1519073074.0
a_s_clark,"What you studied for a degree doesnt really matter when it comes to a career choice like that (some employers may care, but in my experience most don't).
I'm a DBA and i studied computing science for my degree. Some of the best DBAs I've worked with didn't do anything computing related at all when they studied. 
Its more important that you have an interest in the field, and are willing to put in the time and effort to learn, practice, ask questions and hone your craft.
I'm a SQL Server guy myself, and I can tell you that there are lots of forums and communities with great people to learn from out there.
Good luck!
",1519078445.0
InternetBowzer,Of course! Take some classes in RDBMS and spin up a VM lab on your computer and start working with evaluations. You. An gain practical experience by reading some books on amazon or oreilly   ,1519084478.0
liquidpele,More like “how to keep mssql running”,1518986658.0
DuplicatesBot,"Here is a list of threads in other subreddits about the same content:

|Title|Subreddit|Author|Time|Karma|
|---|---|---|---|---|
|[How to Suck at Database Administration](https://www.reddit.com/r/programming/comments/7y769v/how_to_suck_at_database_administration/)|/r/programming|/u/grauenwolf|2018-02-17 23:17:17|0|


----

 I am a bot [FAQ](https://www.reddit.com/r/DuplicatesBot/wiki/index)-[Code](https://github.com/PokestarFan/DuplicateBot)-[Bugs](https://www.reddit.com/r/DuplicatesBot/comments/6ypgmx/bugs_and_problems/)-[Suggestions](https://www.reddit.com/r/DuplicatesBot/comments/6ypg85/suggestion_for_duplicatesbot/)-[Block user (op only)](https://www.reddit.com/message/compose/?to=DuplicatesBotBlocker&subject=remove%20user&message=grauenwolf)-[Block from subreddit (mods only)](https://www.reddit.com/message/compose/?to=DuplicatesBotBlocker&subject=remove%20subreddit&message=Database)

Now you can remove the comment by replying delete! (op only) ",1518915571.0
jjcampillo,Definitely I suck at my job 😂😂,1518951842.0
taylorwmj,Puppies poster. Yes seriously. That's a thing. Not its official name but it's the de facto standard poster for showing data normalization. ,1518914670.0
welshfargo,"http://www.bkent.net/Doc/simple5.htm

http://www.informationqualitysolutions.com/FreeStuff/rettigNormalizationPoster.pdf
",1518919186.0
brantam,"It would be easier and wiser to demonstrate that 3NF is irrelevant and logically speaking can be considered obsolete.
Read Carlo Zaniolo's paper A New Normal Form for the Design
of Relational Database Schemata.",1519017995.0
,"I have a project due as well today and came here to try to get help! That puppy poster is super helpful. 

I wonder if we are working on the same project haha ",1519133605.0
welshfargo,"If I have understood you correctly, you would have Type in the Car table as a foreign key to the Type/Airbag table.

http://www.informationqualitysolutions.com/FreeStuff/rettigNormalizationPoster.pdf",1518812620.0
AQuietMan,"What functional dependencies and multivalued dependencies did you derive from those requirements? 

In your diagram, you seem to have included a *lot* of attributes that aren't in the requirements. What textbook taught you that's part of normalization?

In your diagram, you seem to have included attributes that are not in the requirements, but are based on your understanding about how some (all?) universities work. Is that actually part of this exercise? Because if it is, you've left out some things about how a university works (in the USA).",1518874133.0
assface,Postgres. Next question. ,1518762587.0
seetler,mysql because i'm trashy,1518803280.0
francisco-reyes,"As others have mentioned Postgres is a great choice. In addition, there are a number of addons (for example Citus Data) and derived databases (GreenPlum, Postgres-XL, Redshift) which you could potentially move on to if your data truly grows to a large scale (Petabytes) while been able to use the knowledge you acquired from the stock Postgres.

Additionally Postgres has done pretty good strides on improving parallelization and supporting larger data sets (for example BRIN indexes).",1518795080.0
Ay--_--ye,MongoDB supports sharding. Does postgres support sharding? No. Case closed. ,1518816537.0
sHORTYWZ,"This is pretty standard.... just how the warranty on your car is voided if you start ripping out random pieces, the warranty on your ERP software is voided if you start poking holes in it.

If you have the resources or ability, I would suggest a second installation of the software/environment as a 'test' set for your queries. Even then, however, you have no idea what potential long-term issues could creep up with your queries.

I've dabbed through the tables for Peoplesoft WFM and while I can manage to hobble some data together, I still have absolutely *NO IDEA* what 3/4 of the tables in the database do... I wouldn't dare try to make changes without having a complete 100% understanding of every single interacting data point.",1518743637.0
Paratwa,Check and see if they have any API you can hit to insert/update the data.,1518748326.0
eshultz,"It sounds shitty, but you have to keep in mind that you probably will need their support at some point in the future. You should get clarification from the vendor on whether modifying the data via SQL will void your support agreement or anything like that. My guess is that it will.",1518741880.0
MissGenericUsername,"I'm a systems accountant who's skill level has surpassed that of our first tier support, and spend the majority of my time datafixing in SQL for a quite buggy ERP that frequently corrupts data. I think what it comes down to is being completely confident you understand what your updates are doing and more specifically if those records you are updating have a relationship to another table you have missed. This is where your 'fix' will make things worse if you haven't identified all relationships. Something like a free text field on the user interface is easily updated in SQL with minimal risk, like a comment or note field, but something like a customer ID can exist in many tables for the one transaction.",1518761099.0
wolf2600,You should have no problem using a standard relational SQL DB for this application.,1518697425.0
dessmond,"Individual request NoSQL, aggregated data SQL. It is generally smarter to focus on requirements on how to get data out than on how to insert. You've got that covered with the aggregation bit in the middle.  (I'm not familiar with any other specific requirement, only based on your post)",1518677280.0
grauenwolf,"## SQL

CSV.

Step 1 is to have each web server dump that data into CSV files. Nothing is faster or able to handle larger amounts of data.

Step 2 would be to bulk load them into staging tables in a database. I would use SQL Server because I know it best, but any relational database server (i.e. not Access or SQLite) can handle it the load you're talking about.

Step 3, run your aggregations against the staging tables to populate the reporting tables.

",1518707345.0
xkillac4,"For the first, look into elasticsearch, redshift, couchbase, timescaledb, influxdb. 

For the second, Postgres, redshift, bigquery.

But if you truly don't need the data until the following day, I would definitely take the users advice below about dumping events to Kafka and doing a batch process to aggregate. ",1518696510.0
grauenwolf,"## NoSQL

CSV.

Step 1 is to have each web server dump that data into CSV files. Nothing is faster or able to handle larger amounts of data.

Step 2 would be to aggregate the CSV files using a big data tool such as Hadoop or Splunk. 

Step 3, is to take the output of step 2 and populate the reporting tables. I would use SQL Server because I know it best, but any relational database server can handle it at this stage. 
",1518707428.0
utkacc1,Remindme! 1 week,1518708614.0
Mer1in,"This is easily achieved in access, I do something very similar on a daily basis at work.

Simply create the form or report the way you want it to look with all the bells and whistles. 

You could have a button to open the form, that opens a pre-form asking what things to show, and then toggle each item on or off with Me.textbox1.visible =true.

Or you could have the toggle switches built into the report header/footer (set to display on screen only), and then refresh/requery the report after selections are made.

Lots of ways to achieve this.",1518706311.0
denny10,"Take a look how to fix dbf error codes here:
https://www.fixtoolbox.com/dbffix.html

",1519045407.0
bretkinley,Entity Relationship Diagram is a type of structural diagram for use in database design. Learn the key features that makes er diagrams unique. Visit to avail our free er diagram tool..! ,1518611320.0
river-wind,"MSA=Metropolotan Statistical Area?  Where is MSA coming from in the dataset?

You are getting the four input columns in as what, a CSV file, and are hoping to design a database table for it?  Or is the data already in an existing database and you need a SQL statement to calculate the numeric columns and get the right output format? ",1518610643.0
grauenwolf,The easiest way to do it is create a Calendar table listing every date and corresponding week. Then the problem becomes a simple join and group by.,1518671712.0
alinroc,"For two people running a very small business, a Pi will work fine with any DBMS that runs on Linux and is available as a package for your distribution.

But you haven't addressed the need for a user interface (application) to access and manipulate that data. That's a whole other kettle of fish.

But this is almost certainly a solved problem. There are already solutions for managing this sort of business, doing inventory, scheduling, invoicing, etc. You can probably get something web-based that's turn-key and have them up and running in about 45 minutes. Maybe even something you can install on your Pi and run locally, if not an affordable online service.",1518563737.0
BiggRigg," I'm sure postgres or sqlite would work, those would be my first choice.  Informix db has a raspberry pi build but  I personally don't like that rdbms.",1518566361.0
wolf2600,"I don't know about the new Pi3, but with older versions it was VERY common to get memory card corruption and have to reimage.  I would advise you to use a standard computer w/ SATA or SSD drives (or cloud storage) if you're storing a business database.  On a Pi, it's very likely you'll eventually lose the data.",1518572441.0
ericbrow,"What I don't see mentioned here, and from your other questions I want to make sure you understand, is that Microsoft Access needs Windows to run, and both have a cost that will total about 10x the cost of a Pi.  From what I've seen about the version of Windows 10 that will run in the Pi, I'm not sure Access will even work on it.",1518584368.0
welshfargo,LibreOffice will run on Linux and includes a database.,1518638985.0
eshultz,"My gut tells me they should be calculated on the fly, but without knowing more about your data I can't say for sure.

I would say calculate on the fly until it becomes a performance problem. Just realize that if you calculate on insert instead, you're just moving that overhead from one operation to another, and then using additional disk space to store it. The right choice really depends on whether your application is write heavy or read heavy. ",1518547970.0
morningmotherlover,"Depends on which solution will cost you the most CPU/storage. If it never changes, calculate and store once , retrieve forever. If it constantly changes, restoring might cost you a lot of disk io or storage.",1518550086.0
wolf2600,"On the fly.

There's no need to store a value in the database if you can just as easily do `select (end_ts - start_ts) as ""duration"" from table;` and get the same result.",1518551349.0
alinroc,"**Textbook answer:** They should be calculated on the fly. Don't expend storage space on something that can be calculated. Also, if either end of the timespan changes, you *also* have to update the calculated value if you store it (not to mention writing it when inserting data in the first place). If an developer (application or otherwise) forgets to perform the update, things will get out of sync.

**More nuanced answer:** It depends. You could do any of the following:

1. Putting a trigger on the table. But in a write-heavy workload, that could be costly.
2. Creating a computed column on the table. This is a virtual column which will be calculated each time it's queried. In a read-heavy workload, this could slow your queries down as it's extra math
3. Create a persisted computed column on the table. This takes care of your ""recalculating on every read"" but it's extra work on writes.
4. Do the math on querying the table, manually. Probably only in the queries that need it. Similar to the computed columns.
4. Do the math in the application when needed. No extra work for the database, no extra space.",1518563235.0
larsga,"You haven't required the IDs in different tables to match, so now you're getting all possible combinations of rows. So you need to add a WHERE where you say dokument.x = osoba.y and so on.",1518523638.0
Joisp,"I want to post the solution:
http://sqlfiddle.com/#!7/0cc5d/3
",1518596802.0
d_r0ck,"I'd probably add a customer type dimension (person, business, whatever) and add an fkey to your customer table on CustomerTypeID (the ID) from your new dimension. 

Or at the least just add a denormalized CustomerType attribute. ",1518538769.0
g2petter,"It all depends on your business needs, really.

One approach is to have three columns: `firstName`, `lastName` and `businessName`, then add a function like `GetCustomerName` which returns the correct representation of the name depending on whether it's a person or a business.",1518530013.0
crilen,"One table for Business
Another table for People

have a name attribute on each model that gathers the first and last name, or the company name depending on the model

Polymorphic relationship to the other associated models

",1518549416.0
thenickdude,"An inverted index is exactly like the index in the back of a book. Instead of doing a linear scan through the entire book and reading every page to find the topic you're looking for, you can just check the much shorter index (the index is even sorted alphabetically so you don't have to scan through the whole thing), and the index gives you a nice compact list of page numbers that contain your topic. ",1518517141.0
ChrissMaacc,"My somewhat novice experience would use a table for all the assets (ID, name, purchase) and another table for each softwareAssets and hardwareAssets. They contain their own id and a foreign key constraint to the original asset table, along with context specific attributes.

Let me know if you want more detail!",1518455469.0
r3pr0b8,"yes you can track them in separate tables and yet still have a table for commonalities -- google ""subtypes and supertypes""",1518452631.0
mkingsbu,Sounds like you could do an asset table and make software and hardware subtypes since I'm assuming they'll have different attributes. ,1518459816.0
yiyux,"Maybe this could help
http://www.databaseanswers.org/data_models/inventory_of_it_assets/index.htm
 ",1518455544.0
burnaftertweeting,"I would track them separately, and have a separate table for common details. It might make sense to have an assets table, a licenses table and a warranties table as well. My reasoning is that there is a high chance you will need to expand on any one of these later on. You can optimize and condense later on but its far easier in my experience to merge things together than it is to pull them apart. In general, if there is any doubt I lean towards over-normalizing rather than under-normalizing.

It can also be helpful to build with NoSQL or a mock API first, so you work out the kinks before creating the DB.",1518499799.0
wolf2600,"One of the primary questions that determines how you design your DB schema is ""how will I be using the data?"".  Figure out your use cases, then work backward to decide how to structure the data in your database.

Are you going to have an application querying the DB?  Running reports on the data?  What are the data outputs you want/need?  Will you need a list of all assets together?  A list of just software/just hardware?  Individual record details?  Aggregate data on some specific category?  Figure out your use cases first, that will show you how your DB should be organized.",1518463495.0
OkTear,I have gotten it to 1nf but am struggling to get it down to 3nf as I don't have much experience with data normalization. Any help is appreciated!,1518305270.0
Mamertine,"For most applications, a relational DB is best. If you're going to have billions of records in the DB or adding hundreds of thousands a day, consider NoSQL.",1518293367.0
welshfargo,Also consider that PostgreSQL is both an excellent RDBMS and can handle JSON objects.,1518320272.0
ash286,"There is no right and wrong answer, but if your data is structured, you would be better off using a relational store, like almost any SQL database.


If you plan to store and analyze/interact with things like semi-structured data (JSON objects for instance) or documents, consider a proper database for that based on what you want to accomplish.

""NoSQL"" is a really broad term that talks about a variety of different structures, like Key-Value stores, graph databases, document stores, etc.",1518343623.0
grmpf101,"The advantage of schemaless databases is also in development speed. If your data model is not yet completely fix and you are still working on it (add, remove things), then a schemaless database can make your life easier.

You could have a look at ArangoDB which is schemaless and supports joins and graph traversals natively. If you like you can also nest your objects in there but you don't have to. Maybe worth a look for our use case.

(Yes, I work for team ArangoDB)",1518359072.0
mabhatter,"That’s all relational data, use a normal RDMS. It’s not really even a choice.. unless you like making crazy difficult programming work. 

Where you might use NoSQL products would be something like document stores for when event descriptions become super complex and you don’t want to write thousands of templates (birthday, protest, board meeting, flashmob, parade, coffee date, etc) with hundreds of fields for descriptions when an individual event only uses a few. 

The bigger place NoSQL is used though is capturing metadata.  When you start capturing date/location when users are entering, changing, creating, viewing the application. That’s dozens or even hundreds of pieces of data per user-per event from web server logs, router logs, ad services, etc that’s so vast and changing you just need to grab it all and then use big data tools on it later. ",1518355643.0
WhipTheLlama,"You can design a SQL or NoSQL data model for any app. At a small scale it won't really matter which you choose.

You don't know much about NoSQL, so either use this as an opportunity to learn it or stick with what you know. If you choose NoSQL you should probably use a document store like Couchbase or MongoDB. ",1518361341.0
nonoisfirst,"Personally, I'm a huge FileMaker fan, but the software isn't free. ",1518229552.0
crilen,Laravel is a good place to start for something like this. I've replaced lots of access applications with Laravel setups.,1518225179.0
alinroc,"Do it in stages.

First move your data out of the hell that is MS Access and into SQL Server (Express Edition will likely be fine, at least for now). When you do this, don't just run the stupid ""Import from Access"" wizard - build out your tables the right way with property data types, FK constraints, primary keys, etc. Then import the data into those tables. Make as much of your data processing into Stored Procedures. Build a front-end to that using Access.

Make sure you're backing up those databases properly before you flip the switch to make it the production system, and have a separate instance for testing/development so you don't pooch the system when you make your first change to the database.

*Then* look into building a web front-end. Can't get a lot more specific than that because we don't know anything about you or the tools/platforms you have available.",1518266000.0
,[deleted],1518224757.0
PavilionWI,"Seriously - if the end product is to be on premise you can't get more ""cutting edge"" then MS Access. 

If you're looking to build a web side application that's another conversation. But, in all seriousness - I've never found anything better than Access for on premise application development.

It can handle high user counts and high data volumes, security all of it - as long as you use SQL Server for the backend database. 

For the frontend user interface Access has it all you need.

May I ask why you're thinking of moving away from Access? It may be the current application is not built very well and so you're not aware of what Access can do if it's used properly.",1518567940.0
mkingsbu,Just about all you CAN do is ask the client for feedback. Unless you have a lot of domain specific knowledge. I will usually prepare a few samples of the various types of errors I see. That way I might be able to find a ride to follow to clean up. Sometimes data just needs to be discarded unfortunately ,1518132413.0
dbaderf,"You're on the right track.  Collect all exceptions into a table so that they can be analyzed.  As long as you're never throwing anything a way, you're on the right track
",1518139743.0
jokes_for_nerds,"Usually by cussing out whatever fly-by-night outfit set it up that way

Then by reviewing whatever awful documentation they left behind

Then making notes in between the margins of said documentation

And then by improving on those notes",1518160455.0
mkingsbu,"The thing that helped me the most was building stuff. One of the first projects I did was scraping a website I help moderate. I sat down and wrote out a list of things I wanted to store and grouped them into atomic levels and them built it. I screwed up a few places and had to redo some of it but thinking ""okay, a person can be related to many threads and a thread can have many people posting in it. Therefore I need an intermediary table with a composite primary key"". After doing a few similar projects, it is pretty evident to my anytime I have to build --- or access -- data because I have a good feel for what the range of options are. ",1518151571.0
40866892,"i want to achieve this too. Does anyone have a few articles that promote good practices one can start one? 

I'm just another dev trying to gain deeper knowledge rather than dip my feet in the shallow waters that only lets me do things but not really understand them.",1518473038.0
alinroc,"SQL Server Express edition is free and will handle this.

But don't put images and other documents in your database. Put them on the filesystem and reference the path in your database.

It sounds like you're conflating ""database"" and ""database-driven application"" which is really easy to do if you're accustomed to working with Access. You're looking for both a database to store this data in *and* an application that you'll interact with which stores the data in that database. You may end up writing the application yourself.",1518105230.0
echops,Postgres or MySQL will do the job,1518103467.0
faggatron0,"If you want a pretty front-end, then use MS Access or LibreOffice Base. You don't have to use Access's back end DB. You could use SQLite if you want to be nice and lightweight",1518114053.0
drazde,"Firebird SQL, opensource, simply to install and use, SQL standard, multi platform; and more... ",1518121447.0
wolf2600,Good read about factors to consider when planning a multinational database deployment.,1518096256.0
BinaryRockStar,"Say you have the classic one to many FK relationship of an Order and its many OrderItems:

    Order
    -----
    OrderID, int identity(1,1), PK
    Status, int
    CustomerID, int, FK -> Customer.CustomerID
    StoreID, int, FK -> Store.StoreID
    OrderDate, datetime
    ...

    OrderItem
    ---------
    OrderItemID, int identity(1, 1), PK
    OrderID, int, FK -> Order.OrderID
    ProductID, int, FK -> Product.ProductID
    Quantity, decimal
    ...

and say you have a website that accesses this database with a standard shopping cart sort of model. When the user of the website adds a bunch of products to their cart then finalises the order, the web server will add a single row to `Order`

    INSERT INTO
    ORDER (Status, CustomerID, StoreID, OrderDate)
    VALUES (1, 2345, 8367, CURRENT_TIMESTAMP())

Notice we don't explicitly set the `OrderID`, because it is an identity column, meaning the DB server will pick a unique and always increasing integer as the `OrderID`. Next the web server will insert the rows for the items in the order- it knows the `ProductID`s and quantities because it saved them as the user was adding them to their cart:

    declare @OrderID as int = SCOPE_IDENTITY()

    INSERT INTO OrderItem (OrderID, ProductID, Quantity)
    VALUES (@OrderID, 74389, 5)
    
    INSERT INTO OrderItem (OrderID, ProductID, Quantity)
    VALUES (@OrderID, 573, 1)
    
    INSERT INTO OrderItem (OrderID, ProductID, Quantity)
    VALUES (@OrderID, 7164, 2)
    
The web server is calling `SCOPE_IDENTITY()` to retrieve the automatically generated identity column value for `OrderID`, then using it to insert three rows into `OrderItem`.",1518038103.0
MDiesel26,Would appreciate this as well. I just wing it when a connection string is needed.,1517853359.0
jahayhurst,"* ODBC - Open Database Connectivity

A standard, started by Microsoft, made as a standard protocol for communicating between database X and language Y. You can find libraries that support ODBC in most languages. Once you've got that, you can code to that module and use the same application against MySQL, MSSQL, Oracle.... w/e as long as it supports ODBC.

Against MySQL it's slower than direct libs, but it's also standard so that helps? IDK about other languages.

* JDBC - not sure, but probably a Java specific ODBC.

I'd suspect that Java created something generic, then generalized to ODBC, and hope that Java didn't see ODBC and just create their own to be special. I don't write Java tho, sooo.....

* OCI - Oracle Call Interface

I've only seen this as Open Containers Initiative, a push for standardization among containers. Apparently it's some Oracle specific language? Oracle Call Interface? IDK, probably Oracle specific database driver, etc.

* OLE DB - Microsoft Office driver.

OLE was/is a ?driver...? within Microsoft Office, allowing access to a backend object. OLE DB primarily is used to access databases inside Office - or OpenOffice or w/e as long as it supports OLE.

Use ODBC, you may take a performance hit per query but you can also trade out your database at a later date a lot easier if you want.",1517857510.0
Whohangs,You might want to look into either liquibase or flyway.,1517852158.0
sMarvOnReddit,"google database migration/versioning tools and pick whats best for you. Other than that, I cant really help.  
Modern PHP frameworks like Laravel comes with DB versioning system, so you could also consider rewrite your app in Laravel or start new projects with it.  

Or you could write simple PHP scripts that alters the tables and apply some version controll to those scripts",1517852573.0
Xymanek,Take a look at [Phinx](https://phinx.org/),1517929437.0
mcstafford,> Firebird is a relational database offering many ANSI SQL standard features - - [About Firebird](https://www.firebirdsql.org/en/about-firebird/),1517770976.0
msiekkinen,"They talk about it being open source, as far as licensing goes how is that different than mysql?   People like persona and maria have made their own ports from stock community mysql.  

",1517783642.0
camelrow,How does Firebird compare to MySQL or SQL Server Enterprise (outside of the size restriction)?,1517810729.0
whisperedzen,"T-SQL is indeed less capable, although I find it easier to read (mainly because I have written more T-SQL than PL/SQL).    
    It has it's quirks, and sometimes it gets super annoying. The lack of a reasonable way to parse text to datetimes for example is terrible. And the bizarre overload of EXEC is just stupid: ""EXEC @VAR"" means something different than ""EXEC (@VAR)"".    
    
It gets frustrating.",1517720088.0
grauenwolf,"SQL Server is awesome. T-SQL hasn't improved in 20 years, a fact I find to be really frustrating.",1517729295.0
lukaseder,"Two things are dearly dearly missing in T-SQL:

- Indexed FOR loops
- Implicit cursor FOR loops

I cannot imagine how one can live without these and be efficient. And I'm not asking for much, having worked with much more modern languages, including e.g. Scala.",1517768558.0
StatisticalOutliar,You using SSRS? to deploy your report to from Visual Studio? ,1517612735.0
jamietwells,"Ok, here's how to do it:
Write a stored procedure to return what you need in the database. This can be done through SQL server management studio. There are plenty of guides online. Look up some guides on SQL on w3schools (they have nice simple examples).

 Then in visual studio you can connect to the database using the server window. Then you can use a SqlConnection and pass it the connection string. Then you can use that connection string to execute the stored procedure by following one of many guides online. Then get the data out by using either foreach loops over every row or LINQ preferably.

 Can't go into too much detail as I'm on my phone but this should be all the stuff you'll have to Google to get this done. ",1517644998.0
BinaryRockStar,"- Your cross reference tables (group_user, user_match, etc.) don't need their own ID field. In fact that would allow duplicates. Better off making the primary key the two IDs e.g. user_id and group_id for group_user.

- Also usernames should be longer

- If you're salting your password hashes those need to be stored against the user table as well.

- User Agent String should be longer as well, they can get quite long.

- You spelled language wrong on the user table.

Other than those nitpicks, solid design.",1517571533.0
Pritz,"I have being working with SQL for 17 years... 
Maybe, I forgot what I studied in college.

But my first question, when I read this is... What?",1517503624.0
b0ggl3,"Have a look at https://en.wikipedia.org/wiki/Functional_dependency and try to figure it from there. Learning about FD and candidate keys is pretty standard in courses about RA. Often learning RA is considered a prerequisite before learning SQL in universities. I think it's useful to understand these theoretical underpinnings, even though you may not actively use this for a long time questions about candidate keys can come up when trying to build up a data model in the real world. Also FD is used in languages like haskell for selecting type class instances, so the skill is not only useful in databases.",1517506106.0
harusaur,Thank you for your replies! I think I found the solution! ,1517512697.0
AQuietMan,"Every database textbook I've ever seen includes at least one algorithm for pencil-and-paper solutions to these kinds of problems. What textbook are you using?

1. AB is the only candidate key. 

2. ABD is the only candidate key. 

[A useful Google search](https://www.google.com/search?client=ubuntu&channel=fs&q=site%3Astackoverflow.com+determine+key+from+functional+dependencies&ie=utf-8&oe=utf-8)",1517529620.0
psychokitty,"I suspect there is a good deal of methodology, that we are not seeing, which is used to arrive at the answer. Googling ""Consider a relation R"" let me to this: https://www.csee.umbc.edu/~pmundur/courses/CMSC461-06/hw4_answerkey.txt
Which might provide some clues that you can use to find your answer.",1517507319.0
,/r/DatabaseHelp,1517503101.0
TheAfterPipe,Looks like gibberish to me. Are there any ideas in the class notes/textbook that introduce this type of notation?,1517505281.0
bewalsh,"This is almost certainly to enable many to many relationships, even if it's currently disallowed. The real evaluation you need is checking constraints on the table. If there's no enforcement on his 1:1 intention then it's not.

Given your example the real world objects have a many : many relationship. A person can own a house, and another house. They can sell that house to somebody else. I worry that the example doesn't fully communicate the app's use case and logic tho.",1517497626.0
r3pr0b8,"> The author claims there is value in having this relationship table because there are cases where it severs as a place holder that can be filled later.

no, this is garbage, and i'll show you why with your own example

> If a row in the person-house table has both a person and a house id, then the person has that house. If there is just a person in the table then the person wants that house.

if there's only a person id, then the house id must be NULL

therefore you can't see which house the person wants

i.e. garbage reasoning",1517498215.0
grauenwolf,"> Is there any value in it being used in the case where its 1 to 1?

Yes, but it is really rare. The ONLY time I do this is when I am not allowed to alter table A or table B, but I need a one-to-one relationship between them. 

This comes up when tables A and B are ""owned"" by a 3rd party application and thus can't be altered, but I'm allowed to add new tables to the database.",1517534398.0
Caedro,Go read about associative tables.,1517517225.0
dwhite21787,HTTPS://Data.gov,1517489595.0
mickkelo,http://www.databasetestdata.com/,1517489613.0
r3pr0b8,"here ya go --

    CREATE TABLE stooges
    ( id    INTEGER     NOT NULL PRIMARY KEY
    , name  VARCHAR(13) NOT NULL 
    , dob   DATE
    );
    INSERT INTO stooges VALUES  ( 1, 'CURLY'    , '1903-10-22' );
    INSERT INTO stooges VALUES  ( 2, 'LARRY'    , '1902-10-05' );
    INSERT INTO stooges VALUES  ( 3, 'MOE'      , '1897-06-19' );
    INSERT INTO stooges VALUES  ( 4, 'SHEMP'    , '1895-03-17' );
    INSERT INTO stooges VALUES  ( 5, 'JOE'      , '1907-08-12' );
    INSERT INTO stooges VALUES  ( 6, 'CURLY JOE', '1909-07-12' );
    ;",1517498896.0
Frederic_Bourdin,Thanks my friends this helps ::),1517495350.0
mercyandgrace,"Pentaho Mondrian. I've never used it myself. Pentaho offers a whole BI application suite that is free/open source.

https://community.hds.com/docs/DOC-1009853-mondrian",1517453347.0
GhaliaFellag,"Usually is applied SQL Management Studio it is easy accessible over Net.  
But in case it can't solve the issue and many others methods and instruments, IT professionals advise   
https://www.mssqltips.com/sqlservertutorial/110/how-to-restore-a-sql-server-backup/  
https://www.sqlserverrepairtoolbox.com/  ",1517438926.0
,"Software evaluation report doesn't mean anything to me in particular and I'm not aware of any ""standard"" for this.

So hopefully you've been given or come up with some requirements for your project or usage, and then you could review how each chosen product fits those.",1517155420.0
msiekkinen,"I'd say refer to your curriculum, ask the teacher for what format, what metrics they're wanting measured. 

Outside of academia if I was evaluating a software product you'd have some business need that you're trying to address.  That's first and foremost.  Of the candidates you then want to look at performance metrics.   There are basic stock stress testing tools for things like IOPs, Queries per second for databases.  

But you'd want to look more specifically at your use cases and access patters and storage requirements.

Then again in the real world the consideration is going to be dollar cost :)  Is it open source or are there licensing fees?  If there are fees what's the per unit break down, is it per instance, per core being run on, per user?    Are you buying a support contract?  What is the service level agreement for support response time?  What kind of support is offered?  Is it just query tuning suggestions or will they escalate actually putting features or fixing bugs in the product for you? Stuff starts to get a lot less sexy when you're out of school and have to deal with that stuff.  ",1517163284.0
doublehyphen,"1000 updates per minute is not a lot so virtually all databases can handle that load comfortably.

In general I recommend that people pick PostgreSQL unless they have very specific requirements. PostgreSQL is developer friendly, flexible, and scales reasonably. By using a traditional SQL database like PostgreSQL you can easily create custom reports and adapt to business needs.",1517140985.0
grmpf101,"True, Postgres is a great database, no doubt. You can also have a look at ArangoDB which is a transactional multi-model NoSQL database for documents, key/value and graphs. Also fast and scalable. If you choose RocksDB as your storage engine, you can write even faster thanks to only document level locks for consistency.",1517213507.0
eshultz,"Just delete the tables, no worries. How many tables was it? As long as you weren't trying to create tables with the same name as existing tables in master, you should be fine. If that was the case your script would have had some error messages. And obv you wouldn't want to delete the tables that already existed.",1517127795.0
XVar,"Sounds like you're describing a [Systems Analyst](https://en.m.wikipedia.org/wiki/Systems_analyst), I've only had one dev job where we had them but they were extremely useful for designing functional specs.",1517045831.0
bloor68,"Solutions Architect ?

",1517043494.0
r3pr0b8,Data Architect,1517047519.0
grauenwolf,"Where I used to work, that was the job of the Business Analyst. 

A good Business Analyst knows SQL and can even write his own database tables in SQL. (Though I would clean them up and add things he doesn't think about like constraints.)",1517072537.0
pierredewet,Yup this is normally the role of the Systems Analyst. (Or some variation on title thereof) the role is useful because it allows the experts to get on with doing what they’re good at. ,1517052297.0
hicksyfern,"A developer? Or, shouldn’t the database designer be able to speak with the end user. I mean, we’re all human.",1517042366.0
the_one_true_bool,"A primary key just uniquely identifies each row in a particular table. Primary keys need to be unique between rows.

Let's say you have a Users table which has FirstName and LastName


    [Users]

    FirstName              LastName
    -------------------------------------    
    John                   Doe


This is an issue because what if you have another user who also has the name John Doe? How would you uniquely identify the two records

    FirstName              LastName
    -------------------------------------
    John                   Doe    
    John                   Doe    


You might have other tables that need to reference these records. For example, maybe you have an Addresses table that stores the users' addresses. There is no way to say which John Doe belongs to which address, so you need to uniquely identify, like:    

    UserID    FirstName              LastName
    -----------------------------------------
    1         John                   Doe    
    2         John                   Doe    


Now each records is keyed. Let's take a look at a simplified Addresses table now

    [Addresses]

    AddressID    UserID    StreetAddress
    ------------------------------------------------    
    1            1         123 Main Street
    2            2         456 6th Avenue


So now each Address record has its own unique primary key, but the UserID tells us that UserID 1 (the first John Doe) lives at 123 Main Street, and UserID 2 (the second John Doe) loves at 456 6th Avenue. The Addresses table has a *foreign key* to the Users table.

Note, it's entirely possible for people to have multiple addresses and with the above table structure that is easy to represent. For example, maybe John Doe with UserID 1 owns multiple properties, that's easy to handle...


    AddressID    UserID    StreetAddress
    ------------------------------------------------    
    1            1         123 Main Street
    2            2         456 6th Avenue
    3            1         555 Beach Road


We can see that the first John Doe has two addresses, 123 Main Street and 555 Beach Road.    
",1516999984.0
thejumpingmouse,"Primary key is the unique identifier for each record.

Think of things like an employee number. No employee shares number. If you look up that number you will always get the same, single employee. ",1516999353.0
GunnerMcGrath,"If every US citizen is a row in your table, their social security number is the primary key. It's the one field (or occasionally the set of fields) that is guaranteed to be unique in the table so you can be sure you're referencing the correct record.  It also prevents you from having to reference a row by saying ""give me the row where the first name is John, the middle name is David, the last name is Jones, the sex is Male, the birthdate is 1/1/1980...""",1517007705.0
wolf2600,"The primary key for a table is one or more columns from that table, which uniquely identifies a single record in the table... this means that for every record, the combination of values for the PK column(s) is unique within the table.

If you try to insert a new record where a record already exists in the table with the same values for the PK column(s), the insert will fail with a ""Unique constraint"" error.

Tables can also have no PK columns, which means it is possible for duplicated records to exist in the table. ",1517012741.0
mcandre,"Data is often indexed by different attribute keys for quicker access. The identifier key is the most efficient way to select any particular row’s data, and is therefore the “primary key”.",1517033735.0
amitkhonde,"Suppose you are a family of 5. So the name is the primary key. Because:
1. You identify each other by name. And in database data is identified by primary key.
2. There can not be two family members with same name. Because then you would get 2 people if wanted to call one. Same is the case with primary key. It is always unique.
3. You can not just not name someone. Because then you will not be able to call them. Same is the case with primary key.",1517040339.0
thajunk,"Redis is much better suited for this particular task.
Look into AWS elasticache.",1516937491.0
rydan,It looks like an easy win was to set innodb_flush_log_at_trx_commit to 2.  This cut IOPS by nearly 90% and now a t2.medium is more than enough to handle it.  I had done  this on Rackspace but when looking in Aurora I didn't see the option.  But that's because it is at the cluster level.,1516947826.0
faggatron0,Use syslog like everyone else,1516997631.0
JoaozinhoLobo,"Such method as say my friends is the most reliable, but sometimes you have to repair sql server by more ways that are powerful, see more about it there  

https://www.techwalla.com/articles/how-to-repair-sql-server-management-studio  

https://www.mssqltips.com/sqlservertutorial/112/recovering-a-database-that-is-in-the-restoring-state/  

https://sql.recoverytoolbox.com/  
",1517337193.0
drunkadvice,"1.  Make yourself visible.  In my environment, management from the CIO down has a truly open door policy (sometimes they're literally closed).  They have a whiteboard with office hours that you can sign up for.  If no one is scheduled, they'll write in a few names and schedule 1-1's to get a feel for the company.  Obviously, not all companies are like this.  But go stop by their office and brag about how much faster something is going because of something you did.  Be that guy.

2. It's about career choice.  Did those greybeards WANT to be senior management?  Did they step out of their comfort zone to learn other aspects of the business, leadership, and resource management (the meatware kind)?  Did they try management and decide they'd rather stick with the code?

3.  Classic response I get from my Sr DBA when I ask is ""It Depends.""  Of course there are best/worst practices.  Believing there is a wrong or right way isn't good for any field including DBAs.

4.  Trends are what they are.  I'd be an idiot to say the role isn't shifting.  It's like that anywhere in tech.  I remember a job posting for Yahoo for someone good at organizing web pages because, at the time, Yahoo was literally a nearly static Yellow Pages of websites.  WebMaster's or Full Stack Dev's aren't a thing anymore.  But the role didn't go away, it became more specialized.  Pick something to be an expert on and focus on that.  For SQL specifically, it is not going to go away.  I look at COBOL as a precedent.  It's something that you laugh at, would probably never learn, and definitely not start a new project on.  But major major companies rely on it every day, and that code needs maintained.  SQL has embedded itself in almost everything.  There will be a demand for a DBA skill set or sub skill set for decades to come.",1516758459.0
cgfoss,"What you've described as enterprise DBA I call operations DBA, and I agree it's on the boring side.   I have plans to move that portion of my group to something like Amazon RDS.   After that I can define the SLA and mostly refocus.

Since you have a development background you may want to look for roles as a development DBA, where I've found those roles to be much more involved with design and data modeling.

If an organization says they value data, but don't see their DBAs there is a conflict, and possibly a toxic environment.

One aspect of data management that I've seen development DBAs become really good at is master data management (MDM).",1516806550.0
gc04,"I agree with your premise. The ""old school DBA"" is going to be automated and/or offshored out of a job very soon with everything moving to in the cloud and managed services. Only very sensitive (government) databases will be handled in-house a few years from now. 

BI/Data Engineering sort of stuff is a safer career path in my estimation. I started off as a pure DBA, but I am taking on as much ETL and analytical stuff as I can because I want to move in that direction before I become a dinosaur. 

Regarding Sr. DBA vs IT Manager/CIO... some people are happy to live in the code and don't want the extra responsibility of being a manager. A DBA with more than 10 years experience is making plenty of money and might not want all the extra work that comes with the extra salary of being a manager. 

I am on the fence about trying to get myself on the management track... I have my MS degree and could get the PMP with a few months of study (I have the experience in management through running my own business), but I am actually pretty happy working a 40 hour week and being done with work when I leave the building. ",1516802576.0
burnaftertweeting,"you can use mysql / mariadb  + mysql workbench. everything you're talking about can be done via terminal, but mysql workbench has a simple GUI for viewing the performance of various queries.

https://dev.mysql.com/doc/workbench/en/wb-performance-explain.html
",1516734601.0
lgastako,You could use a virtualization solution like vagrant + virtualbox to create as many VMs as you want and install whatever databases you want in them.,1516748915.0
welshfargo,"https://bitnami.com/stacks/database


http://www.oracle.com/technetwork/community/developer-vm/index.html",1516739630.0
gc04,"All oracle products are available for free educational use on OTN. You just need to register for an account. 

If you just want a basic database, Oracle XE installs super quickly and easily. You can also install the full suite with all of the monitoring software as well. ",1516803541.0
dandy1crown,pretty neat cant wait to see it in action,1516720790.0
picnicshirt,Hopefully everyone was able to pick up some while the price was down. It was a great opportunity.,1516720276.0
BerskyN,I think it will go higher,1516720725.0
willtron_,">In either case, this encrypts data on the disk, preventing reading of it by somebody with physical access to the disk. But aren't the encrpytion keys available on the disk anyway? Or, if not, that would preclude the OS or DB starting without manual intervention - not ideal if eg running a live website from the database?

For something not in the cloud, the DEK (Database Encryption Key) is protected by a certificate (stored in the master database). That certificate is then protected by the DMK (Database Master Key). That DMK is in turn protected by the Service Master Key, which is in turn protected by Windows DPAPI. There's a whole hierarchy to TDE encryption.

The encryption keys themselves are kept in the database. Now, to do a restore of an encrypted database, all you need as well as the Certificate used to create that DEK so that key can be opened. This can be achieved 2 ways - Have an environment with SQL Server running as the same service account (and I believe the same service name, etc...) so DPAPI can recreate the same SMK, which can then be used to open the DMK, which can be used to open the Certificate, which can be used to open the DEK.

Or... You can backup the certificate and encrypt it *with a password*. In this case, you can restore the certificate to any other instance as long as you have the password to that certificate's backup. Once the certificate is restored, you can restore any database with a DEK that was protected by that certificate.

If someone gets the .mdf and .ldf they can't attach them without the certificate (and its password). If someone gets the .bak they can't restore it without the certificate (and its password).

Now, for the cloud.... It looks like every instance has a built in certificate.

https://docs.microsoft.com/en-us/sql/relational-databases/security/encryption/transparent-data-encryption-azure-sql

When you backup a database to restore elsewhere, it looks like an unencrypted BACPAC is created in Azure. 

Or, you can Bring Your Own Key (BYOK) in which case it's more like on-premise TDE where you are responsible for backing up that key. If you lose that key (like if you lost the certificate) then you won't be able to restore a backup of the encrypted database. 

But the same still holds on Azure... if someone got a hold of the data files or the backup file, they wouldn't be able to attach or restore without that server certificate. Looks like you just need to be careful with the unencrypted BACPACs. ",1516643166.0
,[deleted],1516636533.0
chock-a-block,"The ""right way"" to do this is to store the key in a smart card.   A bad guy can siphon data all day.  It's going to take years to decrypt without the smart card token.

You are screwed in a cloud until the cloud provider comes up with some kind of compliant solution.

",1516679164.0
swenty,So the main reason to partition is to increase query performance for very large tables when most queries can be resolved within a single partition?,1516562383.0
DasWood,"Partitioning seems cool. However, suppose you have an application which relies on a database. Are you able to add partitions without modifying the application? Is this invisible to the application?",1516758233.0
newsagg,tl;dr: All the big players use Casandra extensively.,1516581974.0
The48thAmerican,"Postgres is pretty late to the game here, and with some major restrictions other RDBMSs don't have.  

I'd call no primary keys and no ON CONFLICT more than just ""slight annoyances"", especially when you're dealing with volumes of data that justify partitioning.",1516474008.0
leandro,"From https://www.postgresql.org/docs/10/static/sql-createtable.html ‘Partitioned tables do not support UNIQUE, PRIMARY KEY, EXCLUDE, or FOREIGN KEY constraints; however, you can define these constraints on individual partitions.’

So yes, one can (& must) use keys.",1516550058.0
mik3w,"In the article the author means this definition of collation:

""to bring together different pieces of written information so that the similarities and differences can be seen""

I don't think you can change the table collation as such in Excel, or at least on the version I'm using there doesn't appear to be an option to.

You can however change the file origin type when using the text import wizard, so if you type the UTF-8 ""á"" into a document and save it with ""ANSI / Windows-1252"" encoding, it becomes ""Ã¡"". You can then opt to import it as UTF-8 so it will become ""á"" again (although this character encoding was not what the author was talking about).",1516307881.0
kevin3030,"ProxySQL (which you already found) is the closest I know of.  There is a feature request to change the logging output to support a LogStash/Splunk format.  Though that would be a “heavy” solution for what you’re talking about.  

I don’t have the link handy, but I did find someone who paired MySQL proxy and some Lua scripts to load queries into ELK/Kibana. But I still think trying to extend ProxySQL to your needs is your best bet. 

And I’m curious, by response, what are you looking for?  Result set? Number of rows?  Error code?

",1516462414.0
The_0racle,What best practices were you violating? How bad is it? I've always found that admitting my mistakes then focusing on how to resolve the issue are the best openers in tense situations. Then wrap it up with what changes you'll make to ensure it doesn't happen again and take questions.,1516255805.0
mabhatter,"YMMV, Usually things like this are more about access controls and software doing what it claims. They might care about sloppy planning if it opens the software up to programming mistakes to correct it. 

I hear this and think about the programs properly keeping users in their accounts with permissions, so they don’t subvert the process to do tasks. That the actual business logic (facts, figures, maths, etc) is documented... accounting entry adds up, quality entry is documented and repeatable, etc. 

I’ve gone thru lots of SOX audits and they aren’t looking at “code” or “databases” they are looking at change logs to see if there are too many errors, that all the changes made have valid reasons and testing. Is the development under control, or cowboy in the Wild West. Because that can suddenly cost a company millions of dollars if things didn’t do what you said they did and customers file claims.",1516263443.0
iainaqa,"Just be up-front and honest. I've also inherited a mess. It sounds like you are doing what you can to rectify the situation. They will probably appreciate your candor.

Lots of companies have messy systems, so you may not be in as bad a position as you fear. The fact that you recognize it and are doing something about it counts for a lot.",1516276112.0
wolf2600,"Do you have the data of the firm specializations stored somewhere (Excel, database, text file)?  

Or are you asking how to go about finding what each firm specializes in so it can be recorded?",1516106891.0
eshultz,"As someone who's written probably a million lines of SQL, I can tell you that writing join conditions that involve two or more columns on each side is a big pain in the ass and also makes it easy for a new SQL developer to screw up a join and not realize it. Especially if a lot of the tables involved in the query have composite keys. It gets old real fast.",1515916953.0
mcstafford,"Unique compound keys are useful, but from an administrative perspective I strongly prefer incremental PKs.",1515893866.0
alazicza,"You asked about *ever* not *usually* and I think this has sent people in the spin :-).

I think that most of the techniques that we are using to end up with 3NF normalized design come from the times when db storage was premium and more complex workload on db server querying the data was preferred to storage used by database (rather have one column less (synthetic primary key) and have extra effort in joining on natural Keys). And this is my guess. 

Fast forward now from ‘80s to now the volumes have gone up exponentially, we are now more frequently building distributed systems, often abandoning 3NF for a sake of performance. CAP theorem makes things even more interesting on the database level...

At any rate, what I have seen (on a sample of systems I worked ), it appears that a synthetic keys often are used in conjunction with demoralization for sake of performance. This generally gave better query performance (simpler work for the database when joining the data). 

However this usually would came with trade offs with more complex integration when often interfacing (batch and real time) would be more complex in translating natural keys into synthetic.

Kind Regards

Alex",1515902935.0
grauenwolf,"I love using compound primary keys. They often allow me to completely bypass joins on intermediate tables. 

They also act as a unique index, which would otherwise have to be tracked separately.

But some ORMs are easily broken by them so you can't always use them.",1515906226.0
mabhatter,"I’ve worked with old enough stuff that I really hate the modern UUID methods. When you don’t have the tool that built the database the UUIDs don’t mean anything.  I also hate when other DB products want everything in “strings” (double hate points for everything being VARCHAR)  Even ID numbers or things that are exclusively numeric fields... squished into strings. 

Those completely strip all the MEANING out of the DB. Without the vendor tool that built the DB you can’t just look at the table descriptions and determine the relations.. it’s all UUID and VARCHAR fields and the actual data is 6-10 fields into the table. ",1515939637.0
wolf2600,"    ORDER_ITEMS
    ------------------------
    ORDER_ITEM_ID (PK)
    ORDER_ID
    LINE_ITEM_NUMBER

or

    ORDER_ITEMS
    -------------------
    ORDER_ID (PK)
    LINE_ITEM_NUMBER (PK)

Which makes more sense (and is the universally-accepted standard for a sales order schema)?  There's no point in having an ORDER_ITEM_ID column.  You're not going to be joining or querying on the ORDER_ITEM_ID column... it's simply there to ensure that your application which inserts records always inserts unique records... which the second example would do as well, if your application code is written properly.",1515933312.0
iainabc,"I can give you an example of something I worked with. We had a search engine which tracked which words were in which documents. The Words table and Documents table were both parents of the WordsInDocument table, which kept track of which words existed in which documents.

The only columns of WordsInDocument were word_id and document_id and they form a natural PK together, but each is an FK of a parent table, so you only use one column for joins: word_id for joining to Words and document_id for joining to Documents.

So, I would say ""yes"". If each of the columns forming the PK are all FKs to other tables, then that's a good time to use a compound PK.

This was a very crude search engine, of course! There's little reason to build one yourself, these days.",1515966409.0
pierredewet,"I find them useful for teaching and when thinking about design, but I almost always use ID columns instead for live systems as they’re easier to work with in terms of the actual SQL you’ll be reading and writing  6 months from now.  ",1515936686.0
d03boy,"It's entirely dependent on the specific use case, data, table size, key, etc. In general, I think it's safer to have a auto id unique id for several reasons. You can always add the composite key afterward.",1515947306.0
s13ecre13t,"The 'id' column is an issue if you have 'natural' key or not. Natural key is a key that comes from data, and doesn't have to be generated (as is case with sequential IDs, or UUID).  For example: many login systems use user name or email address as the primary key. 

Once you know what is the natural key, you can know if it is spread across multiple columns. 

Using a facebook as example, I will have a user's table, with email as PK. I will also have a post's table, with some ID as PK. Finally, I want also to implement likes. My likes table will have PK that is compound of user's PK + post ID:

 - USER table
* PK is email (natural key)
* other columns: birthdate, name, school, job, last login, etc

 - POSTS table
* PK is a new ID (db set to generate a key)
* other columns: post date, author, etc

 - LIKES table
* PK is compound across two FK: USERS.email + POSTS.id
* other columns: like date, like type (like/love/wow/haha,etc)

---

Compound keys are not common. They are mostly found where you have two tables and you need M*N type support.
",1515955754.0
alazicza,"Yes! Don’t you read Dilber ;-)

I will have word with my iPhone...",1516206823.0
MagicWishMonkey,"How is an id column meaningless? It's a unique identifier for a record, it's probably the most meaningful column of the entire table. ",1515892599.0
stormnet,"If understand this correctly, aren’t you going to run into situations where the same primary keys will be used so that you’ll have collisions. 

Let’s take a basic example of a school schedule. There are chances of having the same: student, teacher, class room and course. So using a compound PK comprised of all the FPKs will yield collisions. 

I don’t think of the ID column as useless. It would infact stop such cases from happening. ",1515891670.0
mkingsbu,I was thinking about getting this bundle but I know packt doesn't have a perfect reputation. Anybody get this and verfiy it's worth it?,1515855024.0
codemagic,"Thanks for the heads-up, even if most of this is not usable or review I couldn’t pass up an offer to pay what I want for that much material",1515877777.0
b0ggl3,"Building a serious, general-purpose database is a massive undertaking so you may want to figure how and why you want to do that and optimize for your requirements and your workload in order to minimize the effort. Perhaps have a look at existing, open source graph databases, like Neo4j. If you are particularly interested in having a declarative query language for graph databases, there are many free resources available from opencypher.org (grammar, parser, open source implementations). You could try to implement Cypher atop a simple in-memory engine or even an existing, underlying db. Please reach out if you're interested in that.",1515795217.0
welshfargo,[Apache Tinkerpop](https://tinkerpop.apache.org/gremlin.html),1515793203.0
dorian_here,"JanusGraph is a popular open-source GraphDB, you can take a look at it. It works with YugaByte DB, see here: https://docs.yugabyte.com/develop/ecosystem-integrations/janusgraph/

I would start it up, load the data (done in the tutorial) and then inspect the tables/schema in YugaByte to understand how it lays out the data and what the graph queries might be doing. That might give some clues as to what it does.",1516250940.0
welshfargo,https://aws.amazon.com/neptune/,1515806603.0
r3pr0b8,"attributes have no order, so you'll be fine 

what matters are whether you have the right attributes on each entity, and the relationships between entities

by the way, shouldn't the prescription entity have an attribute which identifies which medicine it's for, along with the amount? ",1515781739.0
welshfargo,"As others have pointed out, the order doesn't matter, but it's good to have a consistent style. For example, primary key(s) come first, then any foreign keys, then the other attributes with timestamps at the end. That's my personal preference. YMMV",1515793378.0
Tostino,Postgres does.,1515764290.0
Ashtar_Squirrel,"The oracle UDTs are horrible too, and unsupported in some of the drivers, so you have to do workarounds to even read the data!",1515863833.0
johnfrazer783,"PostGreSQL does support `domain`s which are essentially basic types + domain constraints, so you can easily build e.g. `single_character` (text that is exactly 1 character long) or `natural_number` (int that is 1 or greater), and I do that a lot. Those have a slight flaw tho in that these UDTs do not allow to build arrays from, so as soon as you say `array_agg( x )` that will fail if `x` is e.g. `single_character`; this has been confirmed to me as a lucuna in PG; since `single_character` is a subset of `text`, there's no good reason not to allow `single_character[]` when `text[]` is legal. 

There are two workarounds: either do `array_agg( x::text )` (which has the flaw that you both loose strictness in the result, and if `x` happens to be of any other type, that expression will magically succeed and do the wrong thing), or (my preferred solution) when you need a domain type in a table, announce the base type (like `text`) and add a `check` constraint (just write `check( value::single_character = value )` and you're good). Of course, this is not quite as strict as declaring the right datatype from end to end.

So there you have it; I'm quite fond of PostGreSQL and its type system, but there's some room for improvements, still. Another peeve of mine would be implicit coercion to text in some places, but that's OT I guess.

The takeaway from this is that when UDTs fail you, you can escape to other mechanisms, bothersome as that might be. In your particular case, maybe you can call a named function in a check constraint? Another thought would be to construct views that collect rows that check for constraint violations; those might also come handy when sanity checks genuinely exceed what can or should be done with types and foreign keys. ",1516366140.0
DasWood,sqlserver is pretty useless in general.,1515814078.0
,"I just wanted to point out that GraphQL doesn't really count as a database... I'll continue reading the article now. 

Edit: to clarify, GraphQL is an abstraction. It has types (scalars and user-defined) and a query language. But it is not a database. It just sits on top of one, typically. It's kind of an alternative to REST. 

Things it lacks: aggregate functions like sum(), wildcard selects (however wildcards are antithesis to GraphQL), joins (although you can resolve a 'join'), and you don't insert/update/delete you use mutations which are basically functions. So you'll have to make newUser(), removeUser(), etc. 

I think GraphQL and Rest are best used together. Depending on your application.",1515771621.0
r3pr0b8,"do you care about an address, and will you maintain it in the database, even if no coworker or customer lives there?  if not, it's an attribute, not an entity

the only orgs which care about addresses as entities are, for example, postal services and municipalities, who need to have data on addresses regardless of who lives there

same argument for phone numbers",1515613370.0
alinroc,">Im new to databases and trying to build an erm for a Techsupport company.

Why? There are dozens if not hundreds of products and services that are almost certainly capable of handling this. Why throw time and money, with an inexperienced developer, at building a solution to something that has a turnkey solution you can drop a credit card on and start using next week?",1515609657.0
welshfargo,"If a person can have multiple addresses, or you have to keep address history, then I would treat address as a distinct entity. Phone numbers are less clear, since people typically have few phone numbers, but the same principle could apply, depending on the business rules.",1515632372.0
faggatron0,"Yes, an address should probably be an entity",1515703068.0
wolf2600,"    SELECT history_client, count(*)
    FROM job_history
    GROUP BY history_client
    ORDER BY count(*) desc
    LIMIT 1;



> Does it make any difference whether or not I put Count(history_id) or just Count(*)

no",1515362944.0
welshfargo,"SELECT **history_client**, max(count(history_id))

FROM job_history

GROUP BY history_client;",1515361922.0
sugarshoehorn,"*As Cryptocurrency goes hand in glove with the Blockchain, Ties.Network too has a Crypto token for faster transactions amidst the portal.* 

Need more information about tokens.",1515360096.0
horseeating,Future is here!,1515363161.0
picnicshirt,niiiiiiiiiiiiiiiice,1515364779.0
BerskyN,It's great that users can trade with ETH currency also. ,1515365133.0
wolf2600,"I'd start by modeling your data... standardizing column names, datatypes, and relations.  Then you can start applying that model to your actual schema.",1515328682.0
gram3000,"I haven't this read this yet but might give some recommendations?

 ""Refactoring Databases: Evolutionary Database Design""  http://amzn.eu/3ciB0qm",1515313946.0
metalbark,"I see 2 routes: 

&nbsp;

1.  Redesign from scratch.

2.  Cleanup the database in sections/modules.

&nbsp;

It is difficult to know the best choice with the only the information given, but here are some things to consider:  

&nbsp;

Get Help.   Who else there knows about the procs and the views upon views (ugh..  I've seen this way too much) ?  If there is another sql programmer, that would be ideal.  If not, then it is going to be business that knows about the procedures.  You will be come besties :) 
 The help is going to be your second pair of eyes and your person to learn from about the business processes.  Schedule meetings every week or 2 with this person.   Make docs and requirements.  Everybody learns, yay.

&nbsp;

Use a CVS.   Make a git project on a file server,  make a Gitlab server, host on Github, etc..  Make a repository and make yourself follow a strict process to check your work in.  Once done: things are now documented for the future, you have a way of going back if a change messed something up, you have a log of accomplishments,etc.   The helpfulness of a repo cannot be overstated.  If you do a redesign, make an empty revision 0.1 and start with db creation scripts.   If you cleanup whats there, get a tool that will help you generate sql for all objects, put in the repo and commit release 1.0.

&nbsp;

Books about coding are fine/ok, but I learn better from doing as opposed to seeing others on paper.  In addition, what you really want to learn is the code in the database right now, and equally important the business logic behind it.  I think you would be better served by reading about project management and time management instead.  

&nbsp;

Good luck, it honestly sounds like fun.",1515338381.0
FoolofGod,"My thoughts as a developer that has seen good, medium, and terrible databases.

Step 1: Stop the Madness! Decide on some naming conventions for column and table names. As much as possible make sure there is basically a single point of approval for adding executing DDL so you can enforce/encourage these conventions. With views/stored procedures, decide if, generally speaking, in your ideal world, you would allow these. If yes, allow it but make sure thing are properly documented. If no, put a stop to them. If a user wants a view, tell them to implement it as a where clause or function somewhere in the application.",1515341586.0
H2o2woo,"I would personally : 1. Add a side schema based on a new robust data model with proper tables/columns relying on a simple naming convention 
1.bis. Enforce consistency with proper relationships (PK and FK)
2. Copy & transpose data from the crappy tables to the new ones
3. Add an extra layer of views on top of the new data model that mimics the crappy  table structure 
4. Update the existing business logic to go through the new layer of views (hence using the clean data model)
5. Correct/update the logic itself (SP, UDF, ...) to leverage the new data structure directly without the extra layer of views (but through a « decoupling » layer of views anyway (think of future changes))

This allows you to easily check for data consistency between both data models (being side-by-side) before being able to run and directly compare the crappy logic to the updates one (same inputs should produce same output...quicker).

...and dont Forget to document your new structure and logic ! ;)

CVS will also help you to track changes over the process and may support validation evidences rationals if required (Ps: I used to work in the pharma industry IT with FDA regulation ;)",1515447450.0
svtr,">Perhaps writing a lot of tSQLt tests to cover existing functionality and then refactoring from there

HAH! Good one. Start by getting all that into a version control, thats a start. Writing tests for non maintainable legacy crap.... well you can try, and if get there, kudos. Honestly, bad on a legacy level, starts with redoing the clustered indexes, primary keys and hence foreign keys, to stop having implicit type converstions on simple joins. Then you go into ""missing foreign keys"", then you go into wrong data types, then you go into actual index design....

The rabbit hole is very deep and there is some real trough to ""burn it with fire, and start from scratch"". Patching in a decent backend data structure to something really broken, is very damn hard.",1515459801.0
pauloxnet,@speckz thanks for sharing my article!,1515524116.0
getoffmyfoot,"Because it is free, it allows you to rethink scale. Got a bursty read workload? Script out for a standby node to come online, sync to master, help with the read requests, and then go away.

Using Mariadb also lets you think about using different platforms, languages besides Microsoft. For instance, using python against sql server is like squeezing lemons into your eyes, but mariadb is no big deal.

Another thing you can explore - storage engines per table. You can opt for simpler, higher-performance engines for certain tables where you know it’s practical.

Security model is subpar to sql server.

Just a friendly opinion - take it or leave it - id learn Postgres instead if I were you. I think you will find it a bit more familiar than mariadb.",1515298738.0
Lucrums,"MariaDB is an open offshoot of MySQL after Oracle bought the latter and the person who principally developed MySQL decided he’d made a mistake selling it to Oracle as I recall.

What does it help you with? Well it’s free and open source I believe and therefore you can look into what it does better. The other major RDBMS you might want to look into that’s free is PostgreSQL.

Oh and both are cross platform something Microsoft are only just adding now.",1515266780.0
amaxen,">In 2010 India started scanning personal details like names, addresses, dates of birth, mobile numbers, and more, along with all 10 fingerprints and iris scans of its 1.3 billion citizens, into a centralized government database called Aadhaar to create a voluntary identity system. On Wednesday this database was reportedly breached.
>The Tribune, a local Indian newspaper, published a report claiming its reporters paid Rs. 500 (approximately $8) to a person who said his name was Anil Kumar, and who they contacted through WhatsApp. Kumar was able to create a username and password that gave them access to the demographic information of nearly 1.2 billion Indians who have currently enrolled in Aadhaar, simply by entering a person’s unique 12-digit Aadhaar number. Regional officers working with the Unique Identification Authority of India (UIDAI), the government agency responsible for Aadhaar, told the Tribune the access was “illegal,” and a “major national security breach.”
A second report, published on Thursday by the Quint, an Indian news website, revealed that anyone can create an administrator account that lets them access the Aadhaar database as long as they’re invited by an existing administrator.


Pssst.  Hey buddy?  Want to buy an admin login for 8$?
",1515182127.0
iainaqa,"You can see that only one table has all the FKs. Look at those columns and follow the relationships.

For each relationship between two tables X and Y, think how many Xs can a Y have? How many Ys can an X have? If you can answer those questions, you know if the relationship is one-to-one, one-to-many, many-to-one or many-to-many. I hope that helps.",1515164988.0
sirpeanutbutter4991,"Hi guys I am the original author of this blog post. If you have any general questions on GridDB or how its geometry data types work, feel free to comment or PM me.",1515177431.0
traingoboom,Well you would have to have a way for the seats to not be purchased twice. So maybe a ticket number that is combination of date/time and seat. Then making this column not be able to have duplicates. I am purposefully not giving you the correct terminology for this as that is kind of your assignment. ,1515087915.0
pierredewet,Will the requirement difference  need to allow for seat reservations for multiple bookings? (And therefore probably adjacent seats?),1515102954.0
kinesivan,"I've been researching non-stop for the past few days on what the best practice is for using keys in tables. Only found old, controversial articles. This article helped, though. Thanks.",1515224376.0
veeja299,"You ERD seems fine to me. 
Few recommendations on minor enhancements.

#1: in the Listing_Price table.  
I suppose the latest (price_change_date) indicates the active price row.  If not I would recommend to add an indicator to show the active price row for a specific listing.

#2:  Company_Target

 -My understanding is that a Company can also subscribe to branches that is not owned/children of the subscribing company.
That means a company can also unsubscribe from tracking other targets. 

So having columns to keep track Subscription start date and potential end date or probably an indicator to indicate the active target rows would help as well.




",1515611479.0
welshfargo,"https://www.pgmodeler.com.br/

https://wiki.postgresql.org/wiki/GUI_Database_Design_Tools

",1515001094.0
OneDayIWilll,I started using http://www.vertabelo.com personally. I’m open to other alternatives though,1516342887.0
stickman393,"I recommend this:
[https://dataedo.com/](https://dataedo.com/)",1514833543.0
CrossWired,"Kafka is a streaming layer, think always flowing messaging queues.  Can be used for all kind of real tome status messages.

Redis is a key value store, think caching look ups from yoir database to save long look ups

Cassandra is a distributed NoSQL database which is higjly resilient and usefil when you dont have relational data, when duplication of data is ok.

Hadoop is one of the original NoSQL databases, it has its own file system where you can simply drop files, and figure out the layout at query time, no need to worry about those detials up front.  It is distributed and uses a map reduce fuction to split the work up, this is probably what is being talked abiut when they say BigData, this can get BIG
",1514770422.0
kenchikka,"I'm gonna go on the extremely unpopular opinion that these technologies are amazing but they're not necessary for probably 80-90% of the businesses out there. There are too many companies looking for IoT specialists and ""big data"" experts while their whole company, process and logs fit in a 5 GB RMDBS.

Don't fall for the ""meme"" of using these technologies if you're not thinking about a major application in a major company who will deal with possibly terabytes of raw data, possibly being collected live. Use cases are simply gathering TONS of data from your users in a NoSQL database (Cassandra) so later you can use some machine learning and see how to fine-tune the experience of your user based on his behavior in your app/website.",1514822055.0
assface,"> I've seen these names floating around the net a bit

Isn't Hadoop over 10 years old now?",1514759751.0
eshultz,"Personal preference, but there's no need to prefix tables with tbl_

A view or a proc sure, give it a prefix, but since tables are generally by far the most common objects in a database, it just gets annoying.",1514683332.0
grauenwolf,"Don't name your primary column `Id`. That's going to be a real pain in the ass later. Always use `GameRosterId` or better yet `GameRosterKey`. \*

Likewise, your foreign keys should almost always have the exact same name as your primary key. This will make your life much easier down the road.

*****

\*: The reason `Key` is a better name than `Id` is that people can't agree if it is spelled `Id` or `ID`. Looking at your diagram, you can't even decide which is better and use both.",1514692541.0
iblaine_reddit,"I recommend snake_case or CamelCase but not both.


I prefer to use Id as the PK for tables and assume tablename_id is an FK.


I can't figure out the purpose for the table baserunner. If the grain is baserunner per pitch then that'll be a huge table. ",1514694405.0
grauenwolf,"I suspect you are working backwards.

1. Start with a list of all of the questions that you want to ask. 
2. List the data that you need in order to answer those questions
3. Determine the cardinality (e.g. one team has many players. one player has one team OR one player has one team per season)
4. Layout your tables to match
5. OPTIONAL Consider normalizing tables to remove redunant information

Without knowing #1, I can't say whether or not your design is correct.",1514692409.0
solarjetman,"It is close.  Quibbles:

1. I'd prefer more stylistic and grammatical consistency.  As others have mentioned, don't mix snake case and camel case; I prefer snake case for databases. Your tables seem to have a mix of abbreviations/acronyms and words, e.g. ""RBI"" vs ""Homeruns""; I prefer not abbreviating.  Also, most of your table names are singular, so I'd rename ""pitches"" to ""pitch"", ""PitchTypes"" to ""pitch_type"", etc.  Those are just my preferences, but whatever you do, do it throughout.

2. GameRoster is an awkward table.  It seems to be more of a rollup of statistics than a simple store of which players were available for each game.  I suspect your ""pitch"" table contains all this information and you can run aggregate queries against it to determine how many singles a player had in a game.  That way you don't need to maintain it in two places.  If you really want to maintain a rollup of this information in its own table, it probably belongs as a separate ""fact"" table, whose values are generated by querying the base data. For application data it's best to stay normalized if possible to as to avoid storing contradictory information.  Even ""position"" can be deduced by looking at pitch, since it stores what opposing player played during each at-bat.  (Can't players change positions over the course of a game?)

3. Your game-event tables (at_bat and pitch) should probably have some column indicating the order in which they occurred within the game.  You don't necessarily want to rely on auto-incrementing primary keys to do this job, because your rows won't necessarily enter the table in the order in which the physical events took place.

4. Inning is probably a superfluous table.  You don't need the rolled up data, and you can just put a ""T9"" on at_bat and pitch to indicate that they took place in the top of the 9th.

5. For extremely small tables in which all the rows are hand-inserted, such as ""pitch types"" or ""result"", I tend to prefer human readable ""code"" keys e.g. ""HOME_RUN"" to numerical ids. It's still good to maintain the FK to a table of possible values, but it's a pain to have to join to it every time you are poking at data.

6. Have any baseball teams actually been disbanded since WW2?

7. I think the various columns on the pitch table that contain player ids should be moved to a child table, i.e. ""pitch_participant"", which would have pitch_id, position, and player_id columns.  It would have unique indexes on pitch_id and position (since you can't have 2 shortstops) and also on pitch_id and player_id (since the same player can't play 2 positions on one pitch).  The latter constraint is a pain to enforce on the pitch table (you'd need a gnarly check constraint) and also, to index all your foreign keys on pitch would require a ton of indexes.  It could hold batters, fielders, baserunners, and pitchers.",1514697735.0
sunbeamclouds,"i'd create PlayerID, TeamID, SeasonID to the tbl_season table, teamDisplay to the tbl_team, it looks like ID is used duplicatively as person ID and teamID in teamID and playerID (tbl_team and tbl_player) or what is the id field? i'm getting very confused...",1514711438.0
pierredewet,"As others have mentioned, choose and perhaps document a  particular nomenclature for your schema and then stick to it.
I tend to use:
- Title case singular for table names*
- lower case,  underscored, pretty descriptive for field names
- all caps for reserved SQL
- no prefixes except for views etc
- table name + _id for PK

Some other rules for compound keys, naming standards for indexes etc bu you get the idea. The key here (ahaha) is consistency more than anything. As long as you, and most likely someone else, can understand the schema after a  year and quickly get to grips with it then you’re doing ok. (At least in that front)

*(There’s some contention about singular vs plural table names. Choose one type per project and stick to it)

Using consistency as the above allows you to write (and more importantly, read) better SQL as you can clearly see what object you’re referring to in the script.
",1514722549.0
chickeeper,Between table player and pitches i would make a junction table player pitches.   I would add an enum to say position. It would allow for more growth and clarity,1514689562.0
kfranken,"There seem to be a lot a repeated columns (positions), that’s not normal form (c.f. Codd, etc).  Certain queries , like who was on the field (especially if they play a couple positions) would be impossible. ",1514690636.0
welshfargo,Many RDBMS systems come with an XML/JSON package or API. Check your vendor's docs.,1514595849.0
faggatron0,"you on windows or unix?

csvkit can read databases and convert to json. or jq can take existing text files and convert to json",1514592765.0
eduard93,"Maybe your ERP has features to import odbc/jdbc sources? CSV?
",1514659744.0
msiekkinen,Is there a bingo card?  To see how many people at work read this and all of a sudden say we need to start using one of them?,1514479031.0
codemagic,I expect everything that is AWS hosted to be brought up at work as the next thing everyone needs to start using,1514484361.0
mallencincy,"New Db's have almost replaced the new Java Framework of the Day as the top ""We need to change from using X to Y, it's the best thing since sliced bread"" ",1514485809.0
msiekkinen,I've never heard of it.  Either way you're going to get interviewed just like anyone else that doesn't have XYZ cert for what ever job.  If you have no industry experience *maybe* it might prevent your resume being thrown in the trash showing you were trying to do something to curate your skills.   But it's going to be entry level and only for places willing to put in training you'll need.,1514423764.0
wolf2600,"Certificates like this with no relevant work experience can lead to VERY entry-level positions if you're knowledgeable in the areas the job requires.  That means you'll need to be building databases, reading books/articles, and trying things out on your own.


If your career goal is to be a DBA, it looks like a solid investment.  It's a reputable school, and the courses appear to be fairly technical (ie: not just a business-type ""overview"" of the technologies).


",1514401370.0
getoffmyfoot,I’ve hired quite a few database folks right out of school. I wouldn’t give you any additional consideration if you had said certificate. My meager advice is to skip it.,1514437786.0
globalreutersinc,excellent marketing database provider company,1514379046.0
xkillac4,Start with postgresql and a Python/Flask todo app based on a tutorial. Go from there. ,1514240341.0
The_0racle,"Is this homework, job, or hobby? You're jumping in before you've properly scoped the project. It's admirable to want to start immediately but doing so almost always causes problems.

Some things you need to know before choosing the individual pieces of your stack (specifically DB):

 * Is your infrastructure already in place? (servers, networking, dns/domain, code deployment pipeline, etc)
 * Is your data set relational? (nosql dbs vs relational dbs)
 * Do you require security? (for example storing usernames and passwords in the db)
 * How long will this database persist in a production state?
 * How big will the initial data set be and how much do you expect it to grow over the lifetime of the project? (This determines if you need a highly scalable database or not)
 * Is there business logic for the project that you intend to live in the database? (imo it's much easier to put business logic into postgres, oracle, and sqlserver than the others)
 * What is your license budget? (the free databases are easy and typically work well for small hobby projects or short term apps)
 * How critical is the application? (is this a 0 downtime application that will require redundancy?)
 * What is your time frame? (Some databases can be installed in seconds like maria/mysql while others take a bit of time like Oracle)

I'm forgetting half of the project scoping checklist but this is a good start. Answer each of these questions then start looking up the pros and cons of each major database. If you're genuinely new to databases and aren't familiar with the concepts of nosql vs relational then it's usually a safe bet to just go with relational (but only you can answer that!). If you're not already informed of the difference then just think of it as: Do you want a single huge table (nosql) or will you require multiple tables for things like one to many relationships and data dictionaries (relational)?

Also, a piece of advice (Which you probably already know) is that you *probably* shouldn't store your images inside the database. Depending on how often you expect the images/data set to change you may need to even consider a content delivery network/procedure so you can easily automate the changes.",1514242890.0
r3pr0b8,"an employee can belong to more than one department? 

multiple customers can request the same loan?",1514051276.0
EvenFodness,"Otherwise, there are following effective articles and materials right there if you can’t repair foxpro database:  

https://support.microsoft.com/en-us/help/193952/how-to-troubleshoot-to-resolve-suspected-corruption-in-visual-foxpro  
http://fox.wikis.com/wc.dll?Wiki~TableCorruptionRepairTools  
https://dbf.recoverytoolbox.com/  ",1514118928.0
Data_Geek,"Got data, the NZ Analytics Packages and the testData std mentioned in the admins guide from the external sources, and loaded with the pre-canned scripts that loads into INZA.NZA db that specific data.

If anyone is interested I can post screen shots of it all working.",1514224113.0
itsmejvk,"I am trying test out ""GROW_DECTREE"" any help appreciated. ",1514568100.0
alazicza,"I think this may be a positive news for you. Firstly, (and I have less experience there than you), you may find that your job will become easier with transition with Chef to Docker. 

The reason is that most of middleware or databases has some quirks around a run time environment (clustering, ha, protocols, etc, etc). If you are building automation yourself (using 3rd party tool) you will have to figure it out for yourself vs having that problem resolved by the publisher of the Docker image.

From what you have written above, sounds like you have quite broad skills in terms various dbs or other middleware. While you may have another learning curve ahead of you it sounds that you will pick up another skill set rather than someone that last 10 years worked only as a dba and nothing else.

Secondly, Docker is becoming an industry norm and sooner you learn better for your skill sets. And that applies for all of us. You will probably have better chance in environment that is fully in rather then where there is only one or two experiments and going back to the previous ways.

Wrt the running a database on a Docker one thing I do not know well enough about the sizes of your databases, type of the infrastructure you are using etc. I am not sure how much you have control over storage, NUMA allocations, network, memory, etc. In VM world you have some control there but for the max performance you are probably looking at bespoke configured servers with high IOPs, NVMe, perhaps log tailing in memory, low memory latency etc. This would not be a general purpose servers to run average Docker containers. 

Good luck. It may not be easy but sounds like a good news...",1514007307.0
GuyWithLag,"AFAIK Docker won't play with SQL Server at all due to the Windows requirement. The question is, how do you feed your persistent store into the DBs? Docker is great when you need a new instance, pronto, but you need some persistent storage to serve...",1513959845.0
grmpf101,Love these guys :) ,1513959426.0
edimaudo,try dbvisualizer,1513911572.0
cwaydt,I used to use Aginity Workbench and had no issues,1513931452.0
rodtoberfest,Check out Toad Data Point - connects to everything you listed above. ,1527388560.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/sql] [Download Netezza Simulator, Netezza SQL Training Material](https://www.reddit.com/r/SQL/comments/7lbben/download_netezza_simulator_netezza_sql_training/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1513880769.0
,"Unless things have changed radically, Netezza (now Pure Data, I think) is underpinned by PostgreSQL’s parser and language definition.  Much of the SQL syntax will be common.  Indexes are a no-go, though - the implementation is very different under the covers. 

I believe that there is a NZ demo available, but it demands some grunt so you will need a lot of RAM and a powerful CPU. ",1513883231.0
andredp,"The schema is simply a structure of entities and its relations (materialised in tables). I'd go as far as saying that one schema can live in multiple different databases (imagine you implement foreign key triggers to force integrity between the tables from different databases).

In MySQL a database should be the sandbox around the tables inside it. Basically what you set when you do:

    use <databasename>;",1513872886.0
newsagg,"Database to me means the SQL service, tables and their data.

Schema means the way the relationship rules and the way SQL service enforces the relationship between those tables using those rules.

You can add ""views"" which enforces a sub-schema that is compatible with the main schema. 

If you have two incompatible schemas on the same ""database"", basically what you really have is an undocumented super-schema. ",1513877017.0
anras,"I'm not convinced you have much reason for ruling out Python. You want to write scripts that run on multiple platforms and work with multiple databases - Python fits the bill.  
  
If I really had to choose between Bash and PowerShell, I'd choose Bash, just because I prefer a more Unixy environment. It's just a preference.",1513805107.0
iblaine_reddit,"Use bash.  Bash is very extensible.  Powershell is clumsy, has strange syntax and should be avoided if possible.",1513843503.0
romeo_pentium,"I'd choose Python. Bash is very unforgiving. It's very hard to do graceful error handling in Bash.

Also, the idea of working with databases in Bash is weird to me. Bash is ok for loading DDL into a database or taking a backup, but the moment you want to do any sort of query you want something like Python.",1513866306.0
cgfoss,"I use both and like them both.  I've been working in Unix and Windows for decades.

Powershell is superior.  My reason is philosophical.  

Bash sees everything as text, you use text streams to pipe between different applications and parse text to make decisions.  Its mature and available on Windows, Unix, and Mac.

Powershell does everything bash does, plus it can treat data as an object and understand context (in come cases).  Powershell is available on Windows and Unix (not sure about Mac).

One example.  I have some powershell scripts that provide integration between sqlserver, Jira, and a source code repository.  We ported this setup to work with postgresql on linux instead of sqlserver in less than one week.  A few years ago I did try to write the integration with bash and it didn't end well; there was just too much parsing to understand what it was operating upon versus powershell understanding context already.  Error handling in powershell was much cleaner as well.

I'm a big fan of using the best tool for the job.
",1513973833.0
dsn0wman,"As a DBA I find you don't always get to decide what software is installed on a production server.  
  
This makes bash/ksh and python very valuable, as I have yet to come across a server that doesn't have those on it. Of course I don't work with SQL Server or any databases on Windows.",1513813444.0
readitlikeitdidit,"PowerShell or Bash if you want to write something quick production scripts without much testing. But, make sure you have the command line interfaces available to access databases via bash or PowerShell scripts.. Python has a good number of dB APIs for database programming but may sometimes lack in completeness of the library or API. If you want etl like functionality I would recommend using specific etl tools instead of having to code etl workflows in bash /PowerShell or python. Python has a few handy etl packages. ",1513832645.0
mcandre,"Title answers its own question. PowerShell is technically available for Linux now, but no one is arguing that PowerShell scripts are very portable. If you had to pick one, bash is much easier to run, not only on all UNIX derivatives, but in Windows as well with various cygwin-style environments.

If you want something more portable and robust than bash, switch to Ruby, Go, Java, and so on. Those languages are Windows/Linux/macOS portable and much less likely to feature subtle shell flaws, especially if you use libs instead of external processes.",1513875768.0
skewerbr,"You could deploy any database using containers (Docker) to isolate projects and environments (development / testing / prototyping...). That is like VMs but much lighter in memory usage and startup time.

Most popular databases have a ready to use image for Docker. It is also possible to persist data in the host filesystem (good for production) or clean up on exit (good for automated tests).
",1513733157.0
bobbypriambodo,Are you looking for [SQLite](https://www.sqlite.org/onefile.html)?,1513737368.0
Tafkas,"My suggestion is to use the same database you will use in production. It might be tempting to use SQLite locally because it is the easiest to set-up-. This will bite you later when you realize that your production database behaves differently than your local one.

And don't get me wrong. I love SQLite and use it in many projects. ",1513785873.0
_Zer0_Cool_,"I have multiple DB instances running in Docker containers (oracle, Postgres, mssql, mongo, mariadb) on my Mac. 

All of these images together take less disk space than the smallest comparable VM.

With a little Linux command like work you can have complete Dev environment for node.js, Python, etc.

It's super easy. Docker does have a small learning curve, but if you have any Linux background you shouldn't have too much trouble with the slight configuration items of docker. Way easier than standing up a DB and Dev instance on Linux the traditional way.

Easy as ""docker pull <DB image>"" and then the run command specifying the ports etc. And you can save containers and backup images.

It's the best thing that ever happened to database development.",1513880082.0
bretkinley,[sql database model tool](https://sqldbm.com/) you can get it online.,1513830075.0
imcguyver,What advantage does Citus have over snowflake db?,1513722111.0
,It's the secret sauce in the webscale recipe!,1513749196.0
Naeuvaseh,"You're right: your data model is wrong. You can't have a M:M relationship between two tables. Instead, you have to create an associative entity that relates the two tables that have a M:M relationship. That associative entity will have at least one PK from each the parent tables as a FK in order to create that relationship correctly. Ideally you can use those FKs as a compound PK for that associative entity. 

But I have a bigger question for you: Why does your design have to separate Location and Venue? Can't your Venue table have a location attribute (i.e. normalized address attributes)? If you removed that Location table and moved the related attributes into your Venue table, that M:M relationship would go away and you'd be golden.",1513686360.0
eshultz,"I just skimmed through what you wrote, sorry if I missed anything. There's a lot of ""academic"" wordiness here, and I never took a course on set theory.

But isn't what you are talking about colloquially referred to as ""business keys""? I.e. the collection of tangible values that defines a unique entity? In most databases we go further to assign an arbitrary integer (PK) to identify this entity (though you absolutely could just use the collection of values that defines the business key as the unique primary key) - the integer primary key is really for ease of use in joins, lookups etc.

Again sorry if I missed the point.",1513668925.0
,"Verbosity plays well in academia but succinctness wins in business.  If you can't tell me what your problem is and why it matters in under 100 words, then you've got larger theoretical issues you either don't need to worry about or are simply making things complicated for yourself.  If you try to explain your idea multiple ways, you need to simplify it and realize it's already likely been covered.

 ",1513744005.0
mallencincy,"Yuck, full outer joins",1513647123.0
r3pr0b8,"> In an Oracle DB is left join the same as left outer join?

yes, and in all other databases as well

the word OUTER is optional ",1513648197.0
swenty,"Your boss is misinformed. Left join is a SQL abbreviation that means the same thing as left outer join. That's different from an inner join, and different again from a full outer join.",1513646874.0
TinyLebowski,"Great read. I learned a lot. 

IMHO it deserves a better title though. At first I mistook it for a /r/learnprogramming question, and started trying to answer the question. Of course I face-palmed hard when I realized it was a rhetorical question, asked by someone who knows way more about the subject than I do. ",1513625085.0
nerga,Facebook uses multiple databases. I believe they are mainly MySQL. They also have Cassandra db which is a distributed nosql. ,1513625154.0
welshfargo,https://www.reddit.com/r/MSAccess/,1513481632.0
dlyk,"Also, maybe take a look at sqlite.",1513507167.0
popeus,"If you wrote the proc (which I think you did ""UDP""?), try adding SET NOCOUNT ON at the start of the proc.

These sorts of errors occur sometimes when a procedure returns a result set prior to the one you intended. ",1513471802.0
fullmarke,no one can help me here?,1513952456.0
fullmarke,"someone help me please! r/Database, you're my hope.",1517439230.0
Fairwhetherfriend,"I'm not that clear on what you're actually looking for here. Are you looking for a tool that would output, say, a .CSV and are wondering if datapump would do that?",1513376053.0
alazicza,"IMHO there is a difference between a specific piece technology (such as CLOB) and what it is being used for and has it been used for the correct reason and in the correct way.

Essentially, if implemented correctly, the medical records can be stored in the traditional relational database model with individual pieces of information stored in tables and columns that you can navigate and report using standard database tools.

The alternative way would be to use some form of document database, where the entire piece of information (e.g. patient charts, episode of care, clinical documents, etc) are stored as a single piece of data stored in the CLOB. The plus side of this approach is that this may give faster, more elegant and more flexible system. the downside would be that it adds complexity when querying the data and more possibilities for the errors.

However, both approaches are pretty much valid. It looks like that in your case, your current EMR is poorly implemented. I think you should evaluate an alternative EMR. The one you choose may use one of the two approaches above, but the most important is that you choose the one that meets your needs.

HTH",1513275063.0
kenfar,"I'd suggest, in general, only resorting to a CLOB when you have no other good option, and need minimal functionality for the data.     And by functionality I mean ways of accessing and interacting with the data.   CLOBs severely limit your functionality.

For example, perhaps you are loading data into structured columns, but then have an audit requirement to keep the original source at hand, but are actually doing almost nothing with it.  That might be fine for a CLOB.",1513275015.0
mazerrackham,"So the whole table is just a single clob field, or all fields are clobs? I could see MAYBE staging input data in a table like that and then sanitizing it and storing it in the actual table, but honestly it just sounds like horrible, lazy design that is never going to scale.",1513275107.0
swenty,"It is a *painful* limitation of Oracle that there is no easily manipulable, convenient and efficient data type appropriate for descriptive textual fields with flexible length. 4000 bytes is often insufficient to store longer descriptions, narratives, etc. But for fields that are usually under the limit CLOBs are inefficient and inconvenient. It boggles my mind how many systems have had to implement kludgy workarounds for this basic limitation of what's supposed to be an enterprise class database.

PostgreSQL allows varchars up to 1GB. MSSQL allows up to 2GB (with varchar(max)). Even MySQL allows up to 64kB. And poor man Oracle is limited to 4k bytes.
",1513277409.0
AmodaWren,I really appreciate everyone's responses. It's helping support and flesh out my understanding. There is a time and place for CLOBs but they probably were a poor choice in this case. - Thanks! ,1513279356.0
oetker,nice.,1513251222.0
W00ster,"Very limited in scope - the query clearly eliminates all those who has been ""nice"" in other languages and tell me that Santa will only visit English speaking children!  
  
You need an in-list with all the various language versions of ""nice""!  
",1513261522.0
mezmerizedeyes,"I hope this is an ad hoc query.  If I find this in an app, you go straight to the naughty list!",1513296608.0
jamietwells,Excellent. I'll be sharing this at work tomorrow. ,1513283766.0
abw,"> and ResidualDate = 2017-09-15

The date is being interpreted as a numerical expression:

2017 - 9 - 15 = 1993

You should quote it instead, like so:

>  and ResidualDate = ""2017-09-15""

",1513240628.0
whte_rbt,select custno where guide = 'A123' and date > '9-1-17',1513231438.0
SQLDoug,"Like /u/abw said above, you need to account for the fact that there may or may not be a value in the EndDate column (and there is no ResidualDate column, thanks /u/miracle173).

My experience is in SQL Server, so if you are using something different, there may be different syntax. 

I set this up on my own server to test it out and verify that it's doing what I expect. Here is the end result, and below I will talk about how I did it.

    SELECT TOP 1 CustNo
    FROM #Customers 
    WHERE GUID = 'B107' 
        AND (EndDate >= '2017-06-17'
            OR EndDate IS NULL)
    ORDER BY ISNULL(EndDate, GETDATE()) ASC

I created a temp table with your test data like this:

    CREATE TABLE #Customers ([CustNo] INT, [GUID] CHAR(4), [Address] VARCHAR(50), [EndDate] DATE)
    
    INSERT INTO #Customers (CustNo, GUID, Address, EndDate)
    VALUES
    (1234 ,     'A123',      '123 Any St.',      '2017-11-01'),
    (1256 ,     'A123',      '234 Easy Dr.',      NULL),
    (1267 ,     'B107',      '456 Other Av.',     '2017-06-18'),
    (1344 ,     'B107',      '232 This Way',      NULL)
    
    SELECT * FROM #Customers

Then I added some more values to test in cases where a customer moved a few times.

    INSERT INTO #Customers (CustNo, GUID, Address, EndDate)
    VALUES
    (1001 ,     'B107',      '721 Electric Ave.',      '2011-01-01'),
    (1100 ,     'B107',      '200 Main St.',     '2016-06-10'),
    (1212 ,     'A123',      '000 Fake St.',      '2016-07-01'),
    (1256 ,     'C545',      '987 Dre Dr.',      NULL)
    
    SELECT * FROM #Customers

Here is my first shot at a solution:

    SELECT CustNo
    FROM #Customers 
    WHERE GUID = 'B107' 
        AND (EndDate >= '2017-06-17'
            OR EndDate IS NULL)

This accounts for the NULL dates and gets the right value, but it also gets another value. So, I need to sort the result and use TOP 1 to only get the perfect value for that date. 

    SELECT TOP 1 CustNo
    FROM #Customers 
    WHERE GUID = 'B107' 
        AND (EndDate >= '2017-06-17'
            OR EndDate IS NULL)
    ORDER BY EndDate ASC

But wait, this is returning the wrong value for that date! Am I using ASC when I should use DESC? No, but the NULL EndDate is sorted as if it were the oldest value, when in reality we want it to be treated as the newest value. There is probably another way to do this, but my first though was to replace NULL with today's date, just in the ORDER BY clause, and it worked. 

    SELECT TOP 1 CustNo
    FROM #Customers 
    WHERE GUID = 'B107' 
        AND (EndDate >= '2017-06-17'
            OR EndDate IS NULL)
    ORDER BY ISNULL(EndDate, GETDATE()) ASC

Not that pretty, but it does work for the cases I thought of. You need to think about whether a date of '2017-06-18' should return the 1267 value or the 1344 value though, and adjust accordingly.

And don't forget to run this if you use my test table.

    DROP TABLE #Customers",1513279647.0
alinroc,"NoSQL is not a replacement for MySQL. NoSQL isn't even a single thing, it's a catchall name for ""databases that aren't relational in nature.""

**If your data is relational, use a relational database.** Most data is relational and some organizations that have tried to use NoSQL data stores have been changing over to an RDBMS and discovering all the benefits that have been in existence there for 3+ decades.",1513202785.0
leobacard,"Through ROLES?

For your application;
CREATE ROLE IDENTIFIED BY 'password';
GRANT INSERT,UPDATE,DELETE,SELECT ON schema.object to role?
then set the role

For the vendor:
CREATE ROLE
GRANT SELECT
etc.

The vendor and users never get the password for the protected role.

Really, though. Users should not be giving their credentials to anyone. And third parties that do business logic on their own are always subject to issues if something changes or is interpreted incorrectly.",1513207319.0
puppy_by,"If I understood it right, you can just create separate account for your 3rd parties with only read access to required data, so they 1)can read whatever they needed 2) cannot change anything in the database.",1513195465.0
CarefullyCurious,"Having used both Oracle and application level accounts, in my own experience I think you are much  better off just having ONE Oracle account on the database for your application, and then controlling specific access from the front-end.

If you want to implement more fine-grained access control you can also create views on the data which filters on (for example) user id, joined to a table with user id + allowed data access mappings. I.e. it is quite easy to just show what the user is allowed to see - even within a table- with this approach.

I would not recommend using Oracle users/schemas to actually represent application users...",1513203382.0
CarefullyCurious,"Oh and if you are just worried about app data being changed, clearly you just need a new schema with read only privileges. Also never create schemas with ""select any table"", etc. always grant privileges explicitly.",1513203577.0
,"https://docs.oracle.com/cd/E11882_01/network.112/e10835/sqlnet.htm#CIHJDJII

sqlnet.ora invited nodes. audit your connections and review the IP's that everytings coming from, and only let the client IP's connect.

WARNING: you can ruin your own day if you do this wrong.",1513224030.0
jmarquez1974,"How to find jobs currently running or history about the jobs?
Alex Lima / November 26, 2008
In 10g one can find the jobs that are currently running by querying the following view

SELECT job_name, session_id, running_instance, elapsed_time, cpu_used FROM dba_scheduler_running_jobs;

Also one can use the following view to find the history details of job that has run.

SELECT job_name, log_date, status, actual_start_date, run_duration, cpu_used FROM dba_scheduler_job_run_details;

To find the jobs that haven’t succeeded
SELECT job_name, log_date, status, actual_start_date, run_duration, cpu_used FROM dba_scheduler_job_run_details where status ‘SUCCEEDED’;",1513185653.0
itizen,"What OS are you using? Your webhosting solution is overkill (I'd contact their support re that error). Also, get rid of it. Get this: https://blogs.technet.microsoft.com/dataplatforminsider/2016/03/31/microsoft-sql-server-developer-edition-is-now-free/
Install it on your machine and start there ;)
Your post sounds a little bit as you don't know where to start (Don't take this the wrong way, we all had to start somewhere ;)). Install youself a local copy of MS SQL. Have a look a round inside, create a database, limit memory utilisation, connect to your database in your own network from another computer or from a VM on your machine. Create some code that utilises a database and connect it to your database. This will keep you busy for the next few days and makes you realise what you need and what is important. Then look at next steps. What else can I do with this? Clusters, read replicas, etc. etc. etc. Have fun ;).",1513154876.0
cudenlynx,Depending on where you hosted it could be behind a firewall so you won't be able to connect directly from your PC.  Some hosting providers allow for VPN access.  You really should contact your hosting support.,1513177843.0
wallyflops,"Not to skirt your question, but I wouldn't bother with hosting. Simply download and install it on your local PC.
IF you're UK Based or around my timezone happy to help you out one evening on skype or similar.",1513206546.0
thenickdude,"On the Host Department example MSSQL control panel, they list an ""external server"" when you select your database, which is the address you should be connecting to (from your own control panel):

http://wpmssql3.worldispnetwork.com:9001/Default.aspx?pid=SpaceMsSql2014&mid=91&ctl=edit_item&ItemID=2780&SpaceID=5",1513188695.0
xsolarwindx,Why are you playing with Microsoft stuff? Perhaps start with PostgreSQL.,1513191411.0
,[deleted],1513133151.0
,[deleted],1513114617.0
grauenwolf,"Define ""like Access"".

Are you looking for just a storage engine? Or do you want a full tool with table designers, data editors, forms, report writers, etc. 
",1513133678.0
AltReality,"Openoffice Base  
MySQL running locally...  
I think there's a google docs database platform as well  ",1513116385.0
faggatron0,"You can use MS Access as the ""front-end"" only, and not its shitty JET database engine. It can connect to other database engines. 

PostgreSQL can run on cheap low-end hardware

What is the actual problem you are facing?",1513193007.0
xkillac4,You can spin up just about any open source database on your laptop...,1513131144.0
ovigia,Kexi,1513146786.0
jenkstom,"Some details are pretty much necessary. You can install various types of RDBMS (relational database management systems) on your computer. But that's just data storage. Usually for presentation, data entry, reporting and so on you will need the client half of the client-server model.

A fairly easy and universal way to get started would be with something like postgresql and Django. That would make it easy for you to create ways to create, read, update and delete database records using the built-in admin interface. Here are [some tutorials](https://www.digitalocean.com/community/tutorial_series/django-development) from our good friends at Digital Ocean.

There are a *lot* of other solutions, but we don't really know what your problem is so we can only guess.",1516303166.0
edimaudo,You can check out filemaker.,1513113025.0
pierredewet,"I’m not sure what you’re looking for (nice gui to work with, small footprint, simple implementation or install etc) but you could try https://www.sqlite.org/ as that’s relatively light with a small footprint and you can get some pretty good clients for it.",1513073849.0
sky5walk,And use http://sqlitebrowser.org,1513086131.0
grmpf101,www.arangodb.com is easy to install and work with https://docs.arangodb.com/3.2/Manual/GettingStarted/,1513272037.0
grupwn,[LibreOffice Base](https://www.libreoffice.org/discover/base/),1513385604.0
gc04,postgresql? ,1513094153.0
jenkstom,"I feel your pain. The whole world of 4GL database scripting systems (DB4, Clipper, FoxPro and even Paradox) were a great way to solve some problems very rapidly. That paradigm is mostly gone, and I'm not entirely sure why. I guess nobody wanted to learn a domain-specific language every few years when the next market leader came out. That and the DB4 scripting language never adapted very well to GUI environments.

These days you should expect to solve database problems in a generalized high level language like C#, Python, Ruby, etc, and use a framework or library for that language to make database programs. With object-oriented languages it really isn't bad. You get a database cursor object and move around. With C#'s LINQ you even get some very high level capabilities. But understanding LINQ is a journey to a whole new world if you aren't familiar with things like collections, generics, lambdas, etc.

Once you get up to speed you can do some nice things in .net using c# (or even Visual Basic, if you swing that way) and Visual Studio 2017 Community (free and extremely capable). A lot of ""business developers"" use this to solve problems much like dbase did in the 90's.

The closest thing to the old ""RAD"" concept that I'm aware of is Lazarus, an open source Delphi clone. It uses Pascal for the programming language, but you have drag-and-drop GUI building. .net never went quite to that level with their database programming tools. You can do the same things, there's just more semantic noise.

Everything is moving to the web these days, so something like [Django](https://www.digitalocean.com/community/tutorial_series/django-development) or Ruby on Rails might be where you want to start. As long as you don't get fancy with the front-end and just use simple forms you can go very far very fast with these.

To me the best RDBMS to program with is PostgreSQL. The SQL scripting language was done right, unlike most other RDBMS's that I'm familiar with (MSSQL, DB2, MySQL, a few others). Plus you can use other languages such as Python to run scripts directly on the server. Python's motto is ""batteries included"", so it's a pretty good thing to learn. (Django is a python framework, so that matches up nicely).

No idea if any of this helps. It's just been my experience since I dealt with some of the things you are talking about.

Edit: If you want something with a small footprint you would want a compiled language that uses a very lightweight database system such as sqlite. Sqlite can usually be implemented as a single DLL or even embedded into an executable. .net has a whole framework that has to be installed (which already is on Windows) and python would require python to be installed.",1516304030.0
Brytlyt,"Thanks Mark, we at Brytlyt are really excited to smash the benchmarking. ",1512990638.0
popeus,"Download the adventure works db, make a etl to load a sales fact table and a sales forecast fact table.

If you're unfamiliar with fact/dimension tables, look up Kimball and buy his text book.

Once you've done that, build some reports off the tables you've built. Actual vs forecast sales etc. ",1512930404.0
DaddyFit77,Perhaps setup a VM Server on your personal PC and start looking at ASP .NET using Visual Studio building sites that would connect to a SQL Database? Would have a lot of room in moving forward with various skills....throw in some Crystal Reports programming as well....good luck!!,1512905346.0
mabhatter,Where can you find non-trivial databases to work with. The ERP and MRP systems at my work run several HUNDRED tables each. Non-trivial queries and reports take using 6-12 tables on a regular basis. In addition to that one product is probably ten years old and the other thirty yo. So they have lots of crufty data and bad design decisions along the way that you have to write extra SQL to get around.. REAL world problems and not perfectly 4NF sample code. ,1512921675.0
grauenwolf,Want to play with large data sets? You can download the entire Stack Overflow database. ,1512962026.0
tmhandamh,Try using some of the free coding sites out there. Codecademy for instance. I would also try googling advanced SQL query examples and see if you can follow the code. I always learn best by seeing actual code that is/was used,1512905858.0
jenkstom,"Seems like every non-profit could benefit from one or more database systems or integrations. Membership management, volunteer hour logging, fundraiser tracking, whatever. And they love free IT help.",1516304263.0
leandro,None.,1512855299.0
r3pr0b8,wut? ,1512851568.0
eshultz,That's pretty neat for ad hoc visualization without doing a report. I didn't know about replicate().,1512890002.0
ThrowawayDB121,"And maybe an example will make it more clear:

I have an Excel Database of let's say (to keep it simple) 3 columns.

Field1; Field 2; Field 3

Primary Key1; Name1; Age1
Primary Key 2; Name2; Age 2

In my template I'll have two fields Name, Age. So when I selected Primary Key 1 - Name1 and Age1 will appear in the template and when I select Primary Key 2 - Name2 and Age2 will appear in the template.",1512803764.0
faggatron0,"See PostgreSQL's `COMMENT ON` clause

https://www.postgresql.org/docs/current/static/sql-comment.html

There's also Pentaho Metadata Editor: https://help.pentaho.com/Documentation/7.0/0N0/110/000

",1512673161.0
r3pr0b8,google INFORMATION_SCHEMA ,1512662872.0
framm100,"       SELECT
        date_trunc(‘month’,datetime)::date as month,
        sum(amount) as revenue
        FROM orders
        GROUP BY 1

""date_trunc(‘month’,datetime)::date as month"" and ""GROUP BY 1"" just hurts my SQL loving eyes. ",1512676470.0
,"What kind of performance are you looking to maximize -- ie, what are you doing with your data?",1512708455.0
kevin3030,"Is data store cheaper per GB than MySQL?  I’m not familiar with GCP pricing. 

Have you looked at compression?  That will reduce your disk usage at the expense of CPU.  Increased CPU may be cheaper than increased disk use.  In my case, I have some insert only archive tables that I compressed, saving roughly 50% disk. 

Partitioning.  Have you considered that?  You have a great use case for partitioning by day/week/month, and migrating those partitions off your main MySQL database to a near line store. 

If you keep all the data in MySQL, you’ll require fewer app changes to retrieve old data. 

Those are my thoughts now while on mobile. ",1512611292.0
ryanakron,"I have a similar project except we store the individual records in elasticsearch and ETL robust summaries of the data into MySQL. In our business case, it is unlikely anyone would need transaction-level data after about a week so the summaries work very well long term. Elastic stores for 30 days, and each night we shard the previous day's transactions into compressed csv files and send them to a GCP VM running minio, an offsite ftp, and nearline. 
If customers want old record-level data, we just give them the compressed csv files for the time they want and let them figure it out.

Also, I think you can save a little money if you are willing to admin your own databases and use Compute Engine.
",1512617396.0
r3pr0b8,"relationally correct?  bofadem!

as long as each attribute value can be reliably determined for a given value of the entity key, you're fine

also, i disagree with this, i think you have it backwards --

>  If, for example, I ask ""Which contacts use this particular phone number?"", the first schema is much easier to query. 

you're suggesting that a bunch of ORs is easier to code?  not really, not even if you simplify the syntax with an IN list --

    SELECT contact_key
      FROM contacts
     WHERE '800-456-7890' IN ( home_phone
                             , work_phone 
                             , mobile_phone 
                             , etc
                             )",1512555402.0
flipstables,"I believe somewhere in my brain, I'm remembering that the more ""relationally correct"" is the one where all the columns are non-nullable--some sort of purity argument that probably traces back to Codd.

Practically speaking, your first schema is one that I've seen and implemented the most often since the number of phone numbers per contact is generally small and unvarying.  The second form would certainly be more useful if there were many phone numbers per contact and/or the number of phone numbers per contact varied a lot.",1512586545.0
joelparkerhenderson,"IMHO the phone numbers a best stored in their own table, assuming all else is equal. Why? Because you want to aim for the business logic in the real world. When you use this heuristic, then you create a more valuable schema for growth and change. 

Here's a real world example: 1) each person can have zero to many phone numbers, 2) a phone numbers can be of many different types, 3) there are plenty of real world cases of adding new types over time. Case in point: our team built a healthcare app. We found that a typical western doctor may have all these phone numbers: mobile, pager, fax, home, main work, direct work, answering service. Some doctors also have vacation home phone, city apartment phone, administrative assistant phone, and satellite phone.

You asked about EAV. Storing the phone numbers in their own table is *not* EAV. Why not? Because the business logic and real world shows more meaning that just a text string of digits. Examples: whether the phone number is mobile or in a fixed location, whether the phone is answered by the person/proxy/machine, whether a call placed to the phone actually succeeds, whether there are meaning subparts such as country code or area code or extension, etc. 

A reason it might feel like EAV to you is because you're already thinking of storing the phone numbers in the user table, which is tricking you into thinking of them as simple text strings rather than a business model of how to call someone.

Your example of the invoice is the reverse: in your business logic and the real world, your invoice tracks your two GST numbers, and exclusively these. If your invoice had other kinds of business logic such as tracking which items are sold, which is truly one to many, then you would use a relation to a separate table.

For more about how to think about planning like this, I believe you may enjoy reading about Domain Driven Design (DDD). Authors  discusses these kinds of issues in depth. DDD shows why it's important to understand the business logic and real world first and foremost, and how to use that understanding to guide implementations such as OOP, DBs, etc.",1512717607.0
,"2 is not EAV because you don't store different types in the same column

2 is almost correct

A phone number is a phone number and should be stored in its own table. A home phone number is a role that a phone number plays with respect to a party

604-555-1212 could be used as your home and work number. Two people could share the same number. A number can exist without being used by anyone

So, you need a table for people (parties, really), phone numbers, and contact_methods, tying a person to a phone number with its role

person -< contact_methods >- phone numbers

where contact methods is person_id, phone_id, role (Home, Work, Fax etc)",1512590101.0
kevin3030,"They won’t provide a translation from the old numbers to the new number?  That’s frustrating.

It seems like your bigger problem is determining the mapping from old to new numbers.  I suggest looking into Fuzzy Matching / Approximate String Matching.  There are libraries out there for several languages.   That will help you match the descriptions. 

Once you have the mapping of numbers, then it’s just normal update statements, or changing your app to use these new numbers.  ",1512568359.0
adamnmcc,"Copy the table, create a new column with the new product id, 

send it to them for verification, then run an update.. or if speed is of the essence, drop and recreate using the new table.. ",1512572150.0
reallyserious,"The database to use on a mobile client is sqlite. It's great. 

One database that holds everything is orders of magnitudes better than many databases. 

I'm not sure what you mean by store by object etc. If you have products and sellers you'll have a product table and a vendor table with a many to many relationship table in between.",1512512881.0
vira28,You can take a look at firebase. I would use only one db. ,1512517350.0
DesolationRobot,"Each table represents a noun. Either people/places/things _or_ relationships between them.

So one table is customers, one table is stores, and one table is customer-stores that logs the relationship that each customer has to one or more stores. Purchases could be another table that would key to customers _and_ stores so you know that _this_ person made _that_ purchase in _that_ store.

Choosing a cloud-based option (e.g. Amazon RDS/Aurora) lets you start out now for basically free then grow dynamically as you get more users. Something to be said for that.",1512519986.0
burnaftertweeting,"MySQL. The client should be directly responsible for any hosting charges, and you should charge hourly cut into 15 minute chunks for any maintenance or modification related to the database or application. Also, you should know that the databases are going to be virtually useless to the client without some kind of application built on top. A simple API and front end should be fine. It's very poor practice to give an end user direct access to a raw database they couldn't manage themselves.",1512452376.0
grauenwolf,"NoSQL isn't just experimental, it's also really, really old technology. As in that's how we did things before the invention of relational databases. 

My roommate works on bank software that's been in use for decades. And it's all based on NoSQL style design patterns. They don't call it ""NoSQL"" of course, but that's what it is. 

And it sucks. Unless they need to read the data in exactly the same way it was written, performance is horrible. They would love to switch to a ""modern"" database like SQL Server or Oracle, but it would entail rewriting all of their software.

So no, the use of relational databases isn't ""old school"". It's still the cutting edge of database technology and continues to get better every year.

MongoDB, on the other hand, is just a revival of old, inferior ideas combined with a remarkably good advertising campaign. ",1512328051.0
,"To be fair, they're probably tired of [people that think they must use NoSQL for everything](https://www.youtube.com/watch?v=b2F-DItXtZs).",1512328592.0
grauenwolf,"Here's a fun fact. 

MongoDB's original storage system bad. Really bad. As in it used a global write lock that killed performance in write-heavy scenarios and it still managed to lose data. So bad that Mongo bought WiredTiger to use it as a replacement. 

WiredTiger is a relational database storage engine. I'm not exaggerating here. Prior to being bought by Mongo, they bragged about that fact in their marketing material.

So yea, newer versions of MongoDB literally sit on top of a relational database in order to get decent performance without losing data. 

",1512328480.0
KrevanSerKay,"This is a HUGE topic of discussion. But here are a few notes:

1) First thing's first. I think you're thinking about this totally wrong. Instead of trying to debunk their point and looking for reasons why you're right. Start by looking at why they think what they do. Some people really are just old and stuck in their ways. Others are experienced and well-informed.

2) I totally agree with /u/grauenwolf. It's true that you don't need an RDB for everything, but they're damn good for most thing. They're tried and true tech, that are constantly getting new, cutting-edge improvements. 

3) NoSQL is a broad category of things, since it's defined by what it's ""not"". So document-stores, key-value stores, graph databases etc are all NoSQL. They're all great at different things.

4) There are some generalizations about when to use what, but they're sometimes pretty shallow. What /u/DesolationRobot said is helpful, but there's more to it than that. Things like ACID compliance and referential integrity are absolutely necessary for financial data, as an example.

[This presentation](https://www.youtube.com/watch?v=KWOSGVtHWqA) does a great job presenting an argument for each type of database system. Some key takeaways:

A) For small datasets, simple query sets, and loose requirements, you can use ANYTHING and be happy. You can track customer data with an excel spreadsheet if you really wanted to. You could have code that writes data to a csv, and other code that parses the entire document. 

B) For bigger datasets, specific types of queries, and stricter requirements (transaction data you definitely don't want to lose for example) you should use a specialized tool

C) Gone are the days when you have to worry about only using one database system. Use multiple in production, each specialized for different tasks!

5) It happens that, as far as NoSQL goes, MongoDB is kind of meh. If you want an easy example, ElasticSearch is insanely good at what it does. It achieves performance by loosening ACID compliance, opting instead of eventual consistency. It also strongly encourages data denormalization, and to get the best performance, you should craft your schema around what you'll want to know about your data later on. Essentially, you're trading ACIDity, real-time accuracy, and the vast majority of ad-hoc transactional queries that are possible for doing a handful of search tasks REALLY REALLY well. 

WITH THAT IN MIND, even Elastic themselves tells you not to use ES as your 'source of truth'. Keep your data in a data warehouse or a data lake or your RDB, then pipe data into your ES index. If something is weird, you can always drop it, redesign the schema, model your data differently, and reindex it all. ",1512338901.0
inermae,"Use NoSQL if your data isn't really that important, and speed of retrieving 1 thing quickly is more important than asking for the absolute truth about something that is complex.  Or if asking lots and lots of questions quickly and being sort of right is important.

Use a relational database if your data is important, if the questions you ask of your data are complex -- and the answers need to be specific and absolutely true.

Real world examples:

If you track people's cat preferences and you offer a free service, and your revenue model is selling their information.  Yeah, tracking their 50 word posts about how much they like their cats in a NoSQL database may make sense if your weird cat site has millions of users.

If you do literally anything that solves any real-world business need, use a relational database.  Things get complex quickly, and you can't have a ""sort of "" correct answer to questions.  Banking, insurance, medical, etc.  It's complicated, and queries can be really ugly -- and they need to be right 100% of the time.  I have been absolutely shocked at how difficult it is to write relatively mundane queries in NoSQL databases.  I have not had even one project in 20+ years of business software development where I would have chosen that technology.

",1512338067.0
DesolationRobot,"People have argued but not really addressed your question. 

Use SQL when the data has a predictable structure or if it needs to be analyzed in aggregate. Use noSQL if the incoming data is unpredictable and/or you will only need to retrieve the data in the same way you stored it (e.g. as a stand-alone document). 

There are, of course, ways to do most all tasks in most all DB setups, but that's what they're optimized for.

So for the vast majority of use cases, the default should be SQL and a noSQL use case has to justify itself. ",1512334882.0
InternetBowzer,Use a relational database by default. Especially if you don’t understand well the technology. I would not recommend escaping the relational model without good thought and some knowledge. ,1512338037.0
gosh,"If you don't know exactly why a NoSql database is better for the task you are developing for. Then don't use it.

Relational databases are almost always better managing the data. They are just harder to build applications for.",1512342635.0
alazicza,"Short answer is it depends on the problem you are trying to solve. 

Unless you go to the specifics of the problem, performance, availability, data storing and access patterns, you would not be able to resolve the argument and you will just waste your time. 

On the projects that I work atm We have found a need to use both at the same time.

Most of NoSQL databases (you are really talking of a strongly consistent relational databases vs. Non relational databases - as you can use SQL variant to query most of new NoSQL implementations nowadays) have been born as people had a need to solve problem that they could not achieve say scaling MySQL or Oracle or any traditional relational DB. 

So hence my point it - depends on the problem you are trying to solve and the laws of gravity (e.g. CAP Theorem and how it applies to the database) your architecture style (SOA, EDA, CRUD, DD, ES, etc).

Btw most of the architectures  / designed and used by the NoSQL databases have been used by mainframes (codasyl,  vsam, isam ) and mini computers years before IBM published the paper on relational databases. And these dbs are still running many mainframe environments for years. 

So that comment about NoSQL being experimental made me laugh :-).

If this topic is just a religious argument, rather like any other problem solving process, maybe you are working with a wrong team?

Good luck!",1512343017.0
Shawnanonymous,"NoSQL is certainly a useful approach for some databases, whereas relational works better for other instances. Generally, I like relational databases if I want to structure the data. Both have their place.",1512353877.0
bitparity,"You use ""NoSQL"" when the data is inherently unstructured and in so much flux that it would cost time and resources to renormalize the database before you can enter new data in.

View NoSQL another way: Data in XML or hell, in a hand-typed comma-separated Microsoft Word document, would be examples of data best suited for NoSQL.  In situations like this, especially with massive amounts of volume, it might be faster to search and/or tag temporary attributes than it would be to create a dedicated field which may or may not be used again.

Relational is not for everything, and the people who think it is, says more about the data they work with, than all the data that is out there.  ",1512364400.0
d03boy,God have mercy on your soul,1512246439.0
manny9166,"few tips
-understand relational database
-understand different types of sql joins
-know the difference between keys..primary,foreign etc
-on youtube search for Microsoft Access playlists

hopefully that's enough to pass your interview good luck! :)",1512246153.0
Xalem,"Having created several Access databases with significant VBA code behind the scenes, I can suggest what you will be doing, and thus what you need to know/understand.  


You can do so much design work using the Access application directly. ( Make sure you know what a relational database is and how tables are joined together using keys.)      Forms, reports and the SQL queries that tie them together can be built using the designers by drag and drop.  So, familiarize yourself with the Access application.  I would suggest you avoid macros and stick with VBA code.  While designing a form, you can have the form designer assign functionality to buttons and controls that then creates VBA code. Lots of code that is created in Access is connected to the user interface, so knowing the events and how to respond to them is key.  Form_Open, Form_Load, Form_OnCurrent, mycontrol_afterUpdate,  

A key thing to know is the difference between bound and unbound controls.
While it is possible to create records in forms without any code (bound forms and bound controls), I have learned that for data integrity, using an unbound form allows for more control over data validation.  In this case, a record is only created when your code decides that it all requirements are met (validation) and then the code builds the record using either the DAO or ADO library(you may have code in the DB that uses the DAO or ADO api) .  You can do so much already in SQL queries, but when you really need control of the data, sometimes you have to walk through the data one record at a time.

Stepping through database records in Acces VBA is not like using entities or POCO objects in Linq to Entities (sadly).  You have to understand what it means to step through the records in a recordset.  (ADO and DAO recordsets are very similar),   This is very imperative programming, likely using Do Loops.  Over the years I have built up several modules and class modules designed to avoid the boilerplate. 

When I was young and foolish, I often put business logic ""behind the form"", placing (for example) the code that handled creating a new student in the event that handled the ""create new student"" button click.  A wiser approach is to move business logic to separate modules and have the  button_click event first do validation and then call the separate  module. 


   

If you don't know SQL, play with Access to see how it creates queries and switch to SQL view to see how the designer takes your design view of a query and turns it into SQL code.  

I expect that if their database has some problems, it is very likely records with nulls.  If it hasn't been done, go through the data, pick a table, and sort records on different key columns.  Records that have a NULL in that column will sort to the top of the table.  Don't be surprised to find a few records with blanks in important fields.  Blank information and NULLS in records cause any number of buggy data headaches.  (I find most validation code is just checking to see that all fields are filled in).   

Access is powerful and allows organizations to rapidly build a database, without even resorting to writing code.  But it takes additional effort to think through all the ways people can mess up that data and lock down the database so that mission critical data is protected and you don't have incomplete records and bad data.

As you work with this company's data . . . do be careful.  understand what a front end and a back end are and maybe, make a copy of the back end just for development and testing.  
",1512248433.0
rbobby,"Lots of good advice from other folks here.

Top priority should be ensuring that the application is under source control (https://stackoverflow.com/questions/187506/how-do-you-use-version-control-with-access-development). As the sole developer it might seem like overkill... but it's not, it's really really not. Being able to easily rollback to a previous version because of a nasty bug is pretty invaluable (and you only find out when things have gone to hell).
",1512249965.0
leandro,"Get Date, Chris(topher) J’s An Introduction to Database Systems.  Ðat will have you understandiŋ, as contrasted to simply operating, an SQL system.",1512315463.0
,Jesus Christ. It takes years to be even mediocre at databases. Let a volunteer w experience actually help the organization ,1512339337.0
soranacirstea,Easily convert EDB to PST with the help of EDB to PST tool. It is capable enough to recover/convert large sized corrupt EDB file and perform EDB to PST export. EDB converter tool runs on all Windows OS versions up to Win 8. Get more Details:- www.softmagnat.com/edb-to-pst-converter.html,1512365468.0
jessicalee1401,"EDB to PST Converter software is powerful utility that effectively repair crashed exchange server database and restore deleted data like Inbox, Outbox, Sent Items, Deleted Items, Draft, Journals, Tasks, Calendars, Notes, Contacts etc. It is capable to convert corrupted edb database into outlook pst format.It will help you to come out from the worst situation like Exchange corruption and easily recover EDB to PST to make enclosed data readable.It is compatible with Exchange Server 2016, 2013 , 2010 , 2007 , 2003 , 2000, 5.5, 5.0.

To download visit: http://www.edbtopst-converter.com",1512628946.0
xkillac4,"Who would you say your target audience is?

We have a MySQL database with about 50GB of data, and a Redshift cluster with 4TB. Shortly, we will be moving 40GB from the MySQL instance into PGSQL. 

Redshift --> analytics; Postgres --> bulk updates plus PK selects; MySQL --> combo PK selects and joins/filtering

Which project application would be best for  Citusdb?",1512181980.0
operatornormal,"There is no cloud, there is only other peoples computers. When taken into extreme, you have a p2p database like http://katiska.org/classified_ads/doc-latest/classTclWrapper.html#getDbRecordLabel where p2p nodes running sqlite are glued together and the DB api is exposed via TCL. Result is scalable, fault-tolerant and extremely easy for programmer. ",1512423438.0
