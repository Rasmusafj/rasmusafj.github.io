author,comment,timestamp
Cartesian_Currents,When your training data is 100% white people,1543553320.0
HugoWagner,what the fuck is this nightmare? did you use a gan to intentionally generate faces that morph into weird nightmares?,1543549385.0
Sjeiken,They exist now,1543537456.0
Krunkworx,Thanks. I hate it. ,1543546143.0
M0rph84,There is any more info about it? Who did it? Any article or technical explanation??? Made me very uncomfortable btw!!,1543540699.0
Nike_Zoldyck,"Genetic theory, plastic surgery and probability suggest that there is a good chance for them to exist even though they are faces generated using Gans. If the training is not proper, it is possible to replicate a training sample rather than generate a new one as well. So you can never be 100% certain that they don't exist. ",1543566530.0
obsoletelearner,Imagine someone going after a girl in one of those pictures and realise that all his life he was just trying to chase an illusion created by a machine.. damn ,1543565592.0
dmurthy,This reminds me of a Michael Jackson video. ,1543570251.0
alfred_dent,All of them has a problem with teeth.  Teeth looks like a snake. Researches need to create additional loss for teeth generation 😬,1543576934.0
cummingpowder,"Let me introduce you.  This man here....is the ""Michael Jordan or AI"".",1543525717.0
,[deleted],1543544211.0
rusikg,"screenshot of your code, seriously?",1543519106.0
ditox123,"Hi, people, I have to develop a neural network that doesn't use libraries like Tensorflow or Keras for a project. 

The problem is that my code does not work correctly, I get error values that all converge to 1 

I don't know if it's because I don't use quadratic error or I haven't added bias to my network. If anyone understands anything about it, here is a piece of the code just below",1543518746.0
ditox123,"I'm pretty desperate, but most of all what would interest me would be how to organize backpropagation and the output error",1543519343.0
supernihui,"Just use more layer.No one knows how to determine that.if you can,you can be a stanford professor",1543515547.0
drakesword514,"Unbiased dataset.
If the size of dataset is too small, augment it. (you have very less images for each class) 

Non zero weight initializations. (the framework takes care of this if you are using one) 
Non linear activations (tanh, relu) after each of your cnn layers. 

Have a hold out set against which to validate your model performance. Plot graphs to check for underfitting, overfitting, accordingly change learning rate, introduce regularization.
",1543506807.0
Laboratory_one,"It is recommended by Andrew Ng to reference the architecture of other networks which perform well. For your case, a network like VGG16 will point you in the right direction. 

The reasoning for this is because these networks have performed well with benchmarking dataset, imagenet, and have been developed through rigorous testing and experimentation. As it turns out, for similar uses cases, image classification, network architectures can be shared. 

In fact, this is led to transfer learning. This is the process of using a network such as VGG16, training on an dataset, imagenet, and retraining it to classify your dataset instead. 

Performance tends to be very high when doing this, and training time is very low. If you’d like to see how this is done, check my post. http://labone.tech/transfer-learning/. I use transfer learning to classify  spongebob squarepants characters. 

If you don’t want to use transfer learning or reference an existing network, there are other methods you can try. Consider:

- dropout layers
- gridsearch for hyperparameters
- isolating where the performance issue may be
- visualizing with tensorboard or similar to better understand performance over time ",1543516777.0
uofT_B,You should use more layers and then regularize the weights using regularizaion methods/dropouts. ,1543529031.0
Only_Chakra,"This might sound silly, but try to visualize each layer, and see what features are being learned by the CNN. Training a CNN is basically finding the correct convolutional filters. ",1543803598.0
tpinetz,"Internally you have a graph. It will just run all the operation in a feedforward path and then backprobagate to calculate the gradient with respect to each parameter / node in the graph determined by train\_gen and train\_disc. Depending on how they are defined it might calculate the gradient twice with respect to the discriminator, if two different loss functions are used and they are not connected in the graph / e.g. uses different random seeds for the generator to calculate the gradient with respect to the discriminator..",1543484845.0
DerMax0102,"Unfortunately I don't specifically know about the math in pooling layers. Maybe some general knowledge about NNs may help though.

&#x200B;

In NNs you compute the output based on the inputs in several steps by using layers. Each layers gets values as input X and outputs other values y. A layer has weights W and a bias b.

&#x200B;

For fully connected layers the output of a layer is simply a(W\*X+b), where a is an acitvation function (e.g. sigmoid). Note that X is a vector and W is a matrix in this case. W\*X a matrix-vector product. Here you can see, that the weights of layer determine how it's input affect the output. In contrast to that, the bias will be added regardless of the input of that layer.

&#x200B;

As we often want to process images in DL, we want to exploit the spatial structure of an image. This can be done with convolutional layers (kernels). Here our input are matrices or even tensors (which you can imagine as a stack of matrices). A convolutional layer has a much smaller set of weights and biases. These are applied to a small rectangle on the image and create an output ""pixel"" (or a stack of pixels) for this rectangle. This rectangle is now shifted over the whole input matrix to create a new output matrix with roughly the same size. The covolutional layers can find small patterns and features within the rectangle they observe.

&#x200B;

As our desired output (e.g. a set of classes) is usually much smaller than the input, we need to reduce the size of the processed matrix. This also helps to detect global features in contrast to the local features in the first layers. For that we can use pooling layers. A pooling layer divides it's input into rectangles of several pixels and composes each rectangle to a single pixel.

&#x200B;

Using pooling and convolutional layers by turns, hence, allows you to first extract small and local features, and then iteratively reduce the resolution of an image to extract more global features. At the end of such networks, fully connected layers are used mostly.

&#x200B;

To train a network, you need to optimize all it's parameters (weights and biases). This can be done with stochastic gradient descent as well as with many other optimizers. Important is, that all optimizers use the gradient of the network. Backpropagation is an efficient method to compute this gradient layer by layer.

&#x200B;

I hope this was somehow useful for you",1543478195.0
henrietteyoungmc,"Look after the Stanford course on cnns. If you search for the course name on YouTube, you can also listen to some lectures. I am sure you can answer your questions on your own.",1543480600.0
Cartesian_Currents,Thanks!,1543459343.0
happy_pirate,Learning new things from every post. :),1543414990.0
Cunic,"The high level idea is to compute the derivative of computations done at each timestep with respect to the inputs.. which in turn relies upon the previous timestep. So you do have to do the whole computation from input to each timestep and sum the contributions, doing this for all timesteps prior to actually changing the weights:

[Deriving BPTT](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/)",1543417564.0
Sjeiken,please go fuck yourself.,1543345545.0
Cunic,"LDA is popular since it's unsupervised, not requiring pre-assigned topics. I think most people would agree that a supervised method would solve the problem better.. but the data are harder to come by. LSTM's could do a good job given labels, so maybe document-level topic modeling could be a good fit since those labels are easier to come by, but word-level topic modeling is a challenge.

There could be room for a merger of an LSTM language model and LDA, though, but I don't yet understand the benefit for topic modeling.",1543419403.0
Laboratory_one,"I would attempt to do this using Neural Style Transfer. Normally, this is used for images. A content image and a style image is feed into a neural network. The network applies the style of the style image onto the content image.

&#x200B;

It is described in [this](https://medium.com/tensorflow/neural-style-transfer-creating-art-with-deep-learning-using-tf-keras-and-eager-execution-7d541ac31398) blog post, which references the original paper.

&#x200B;

You can definitely adapt this method for music using LSTMs. You'd need to implement a way to encode song data into a sequence. Additionally, may want to find a trained LSTM based network to bootstrap your model from.

&#x200B;

I've attempted [Neural Style Transfer](http://labone.tech/anime-generative-model-part-3) using images before. Since it's an unsupervised generative method, results will vary. A lot of tuning is required.",1543340997.0
transpostmeta,"I theory, this could work. You would need to define some set of parameters that constitute the mixing of the track. The network would output these, the song would be automatically mixed, and then you would have to compare the produced song with the reference song. 

This will be the problem, though. You will have to define a loss function - a function that tells the network in a differentiable way how close of far it got from the reference track. I'm not sure we could write a mathematical function comparing things like this.

Maybe if you had a lot of reference tracks you wanted to sound like, you could use a GAN or some other generative method.",1543331948.0
CzoKc,"That's not exactly how PhDs work. Deep Learning is broad category you can't do a PhD on it. You could be doing a PhD in improving theoretical aspects of Deep Learning, applying Deep Learning to new problems, optimizing the algorithms to train faster or classify faster and hundreds of different directions in research. ",1543339516.0
Pedrhok,"Doing a master in deeplearning here but my courses are more computer science and then I apply it for my master project...
But I would be glad to talk about it ",1543338747.0
S_T47,"Doing a PhD in computer vision and mixed reality, however deep learning is a common technique employed in the vision subject area so I have to have knowledge in it... however deep learning is a set of techniques. A PhD close to it would be either neuroscience (AI) or some other computer science theoretical based subject. However most PhDs are application based which will require a student to learn and utilise the SOTA techniques like machine learning/deep learning (now reinforcement learning too!)",1543348667.0
charl3sworth,I am technically doing  Computer Games PhD but I am working 100% within Deep Learning. ,1543363022.0
Tokazama,"From your comments, I'm assuming that you're looking for a program to apply to. As others have stated, it would probably be more useful to look at deep learning as a technique within man fields for this. I'd advise you to first identify the broad area that you wish to apply deep learning to (e.g., pure mathematical principles of deep learning, unique applications of deep learning in non-mathematical fields, computational execution of deep learning, etc). Once you do that find somewhere that will 1) teach you what you need to know to do your deep learning research, 2) surround you with good resources for advancing your research, and 3) won't get in the way of you actually executing your research.",1543405720.0
ratchet20095511,"I don’t believe this is a thing. Maybe applied computational mathematics would be the under pinning of the deep learning algorithms.

However, just as all the “Masters in Data Science” have just been popping up. I don’t think a PhD in deep learning is a thing, yet.",1543335192.0
Schnei1811,Doing a PhD in applying Deep Learning for computer vision and auditory data to a novel domain of ecological problems. It's a really exciting place to be! Conferences have this really enthusiastic buzz about them,1543360187.0
saransh661,That's how it's usually done where we have custom datasets. Matlab has an automated tool for it as well. It's Faster RCNN btw,1543335390.0
ggghash,Intel has built in math capabilities that Tensorflow installed with conda can leverage for improved training  speed. ,1543315759.0
crossCounter23,"The i9-9900k is 100% the best cpu for gaming

Source: my 9900k I’ve oc to 6 GHz single core, paired with 2080ti 

But at the same time yes, the additional gpu pci e lanes are limited. However, atm it depending on how well you weigh gaming performance vs deep learning performance. 

If gaming is more important to you grab the 9900k.

If otherwise then yes you may want to grab a REAL i9 to benefit from the added pci e express lanes and another gpu. 

It’s your choice. ",1543324979.0
PIIP_LAW," 

Tensorflow is an open source software library adopting the Apache 2.0 open source license. Is it safe to assume that you will be completely protected from patent attacks when using Tensorflow software?

In this article, we analyze the Apache 2.0 license and take a deep look into the relationship between open source software and patent rights.",1543274957.0
practicalutilitarian,This [LSTM](https://github.com/zachmaurer/lie-detector) uses CMUs deception corpus to train a lie detector for voice recordings.,1543245829.0
slaenig,"Micro expression detection

[https://medium.com/@101/how-to-detect-lies-microexpressions-b17ae1b1181e](https://medium.com/@101/how-to-detect-lies-microexpressions-b17ae1b1181e)",1543254139.0
Hiant,Regardless of the coding merit hasn't this 'science' been thoroughly debunked?,1543282272.0
mr_meeesix,"Define what is loss, so that we can have a clear picture on whether loss going down is good or not. Ideally we would want the loss to go down not increase.",1543233426.0
zenogantner,"What is your learning rate? Maybe it is too low.

Is the loss you report on the training data, or on a held-out (validation) set?

What is your batch size? I.e., how many complete passes over the training data did you do?

What is your key metric you want to be good at? Accuracy? Do you also watch this metric during training?",1543245310.0
snapitreditt,"Just 10images...!??? Or Batch size of 10images...!?
Note that DRAM inside GPU will be your bigger problem.
Feature maps are the ones that occupy most space while training...",1543238429.0
Hiant,"Network recs darknet, yolo, Unet. This is going to cost a shitload on aws btw. I know some researchers that did a similar project and easily burned through thousands",1543243260.0
Geeks_sid,"mention details about what is the size of data used, also write if you're using 2D FCN or 3D FCN, how many training samples you have, and what the architecture of model is.",1543216946.0
Manto1,"This may not be the answer you're looming for, but for example in deep reinforcement learning where agents learn to play video games, such as Atari, the CNNs used often contain just a few layers of convolutions. For example OpenAI baselines implementations of deep RL algorithms use 3 or less conv layers.",1543238432.0
LoveOfProfit,That one corner was absolutely invisible to it. Yikes.,1543184216.0
Klhnikov,"Code here : https://github.com/EParisot/Patate

It basically use single camera input to feed a small CNN on raspberry pi

(3 conv2D layers - 2 FC layers and 2 parrallels outputs (direction and speed) )",1543184266.0
1991viet,Github repo: [https://github.com/1991viet/QuickDraw](https://github.com/1991viet/QuickDraw),1543142654.0
anti-gif-bot,"[mp4 link](https://g.redditmedia.com/hNhWvpnwqQ3gIF7wM2iIg6k3ryIAiskRtG7xSlKVOcU.gif?fm=mp4&mp4-fragmented=false&s=14604386bfb8d743b79642f339dc4377)

---
This mp4 version is 98.67% smaller than the gif (1.12 MB vs 84.31 MB).  


---
*Beep, I'm a bot.* [FAQ](https://np.reddit.com/r/anti_gif_bot/wiki/index) | [author](https://np.reddit.com/message/compose?to=MrWasdennnoch) | [source](https://github.com/wasdennnoch/reddit-anti-gif-bot) | v1.1.2",1543142649.0
snapitreditt,"Ossum! Amazing!

How do you map those figures to real objects? Simple classification?",1543229618.0
Britefury,"In your `forward` method, change `return F.log_softmax(x)` to `return x`. `nn.CrossEntropyLoss` works on logits (values that are put into softmax or sigmoid), not on log probabilities.",1543068165.0
ai_is_matrix_mult,What happens when you train it? That's the ultimate test ;),1543100330.0
milos_popovic,"Hi, I wouldn't recommend starting with a FC network as this will introduce problems that are completely separate from what you're trying to learn. CNN vs FC networks and issues with FC networks on images is something that should be tackled in isolation in a simpler supervised learning setting. What I assume you are trying to learn here is reinforcement learning and it's own problems and issues, and work your way up from approximate Q learning to DQN solving one issue at a time. So I would propose doing just that, start with a CNN and Q learning without all the bells and whistles of a DQN, and work your way up from there.

Deep learning bootcamp is a good RL crash course that does what you want (ie. works up from tabular Q learning to DQN fixing issue by issue) in the first few lectures:
https://www.youtube.com/playlist?list=PLPfj7W0fIrmy3MfjPFbpy7jFGDmvspgHE",1543069335.0
ggghash,"I've been coding my second Deep Q network too, and I've found this series helpful. Machine Learning with Phil https://youtu.be/RfNxXlO6BiA",1543084060.0
pahtrel,Never heard of anything like this unless they mean something like what kind of models can run on mobile devices(scale by number of devices?). Was this question from an engineer or someone more business focused?,1543045736.0
bpooqd,I have a question: how would you make a blockchain/ICO scam built around the deeplearning hype train? Ahh nevermind.,1543060951.0
kingcooked,"DeepBrain Chain Website: [https://www.deepbrainchain.org/register.html](https://www.deepbrainchain.org/register.html)

[Slack](https://join.slack.com/t/deepbrainchainhq/shared_invite/enQtNDQyNjIzOTY1NzE4LWJkMjg3ZjMzODhkZDY2MmQ3YmE4MTA1Y2I2MTI3YTgzYThmYTY1NTQwNjFjMzVmNTJjNjNjYjMxZTU4ZmU0YjU) Channel  


You can also join our Telegram AI training net channel for help setting up:  
t(dot)me/DeepBrainChainAITrainingNetMiner ",1542991801.0
anr1312,"You should head over to MATLAB answers for a more definitive answer.

https://www.mathworks.com/matlabcentral/answers/index

It looks like you should be able to do most of those augmentations using the imageDataAugmenter.

https://www.mathworks.com/help/deeplearning/ref/imagedataaugmenter.html

",1542978045.0
49913122,"if you want to use your own set of images with your choice of output, you could use the transfer learning function",1542981396.0
BruinBoy815,LUCKY!!! This conference is awesome! I thought it passed though ,1542920359.0
BruinBoy815,Its M4!!!! These people are the top notch!!! ,1542920399.0
0mbre,Here is a model example [https://aifiddle.io/models/cloud/63cd3505-3ef4-4957-899b-792f817ddf1d](https://aifiddle.io/models/cloud/63cd3505-3ef4-4957-899b-792f817ddf1d). (will load dataset + weights so might take some time),1542902262.0
Abjury,So what would be the use case for this? ,1542899465.0
s_vaichu, very futuristic stuff....I really appreciate ur effort to keep it P2P and political-free.....if it is open source I definitely looking forward to contributing,1542954137.0
hardwaredev,"Is it possible to use a tagged photo in town instead of video?  Like drop all the pictures from Instagram for a town into this map and do data science?

Here is the github for anyone looking https://github.com/grasslandnetwork",1542973078.0
alexchauncy,what's the meaning in practicial scene,1542890163.0
ganLover,"Check the final activation function of your network.

&#x200B;

If its a binary classification  use sigmoid and for multi label classification use softmax. 

  ",1542854172.0
Laboratory_one,Could you please elaborate?,1542823455.0
lpiloto,"If you look here at the [original implementation](https://github.com/pjreddie/darknet/blob/f6d861736038da22c9eb0739dca84003c5a5e275/src/yolo_layer.c#L88), it looks like they get the width and height by running through an exponential which avoids the issue of taking the square root of a negative.

",1542838796.0
,[deleted],1542824032.0
pimp4robots,"Main advantage of dilated convolution is that it allows you to have large receptive field without reducing spatial  dimension of the feature maps or using large kernels.  Generally pooling layers are used for reducing the feature map size which results in loss of some information and using dilated convs can prevent that.

What I have seen people do is use 2/3 pooling layers and then use dilated convs instead of adding more pooling layers. ",1542818747.0
hamZmad,"Just to add to the above comment, it helps keep down the computation as well. You basically get a larger receptive field at low cost. You can get more detailed information from some of the representative papers that have used it - DeepLab v3, DeepLab v3+.",1542867588.0
PIIP_LAW,"Hello everyone,

&#x200B;

In the field of deep learning, it is difficult to discuss innovation through open source without considering the impact of Google's “TensorFlow” open source software. Many researchers and engineers use the TensorFlow framework for their implementations of deep learning and as a basis for novel research.

&#x200B;

To a certain degree, the patent, at least in the software field, seems to have become a relic of the past.

&#x200B;

However, the fact that Google is registering key patents related to deep learning, despite its promotion of open source software, is a controversial topic in the open source community.

&#x200B;

Our column will cover this issue, plz check and stay tuned!",1542776477.0
srallaba,Why do the graph nodes form a sequence?,1542746930.0
_spicyramen,"Oh no, another ML article which is a summary of a MOCC class. How original!",1542760066.0
zjf1301,I’m very inclined to agree with your assessment. There is an incredible amount of content of varying quality for beginners but that vein of content narrows very rapidly after getting past that junction.,1542790211.0
fkxfkx,"“Break into AI” - what a laugh :)
What are you- 12?
",1542758983.0
IntelligentVaporeon,"I couldn't care less about the name itself, however the way this was handled makes me furious. Researchers, among all people, are expected to make actions based on logic. And they did initially, by investigating this issue the proper way: they asked the people interested in NIPS and found that [the majority of people want the name to stay as is](https://nips.cc/Conferences/2018/News). They made all data available and presented the findings in numerous tables and barplots, categorized by many variables.

And now, because of a loud minority, they ignored the democratic and scientifically-extracted decision and changed the name. ",1542748092.0
LoveOfProfit,"I didn't see why it mattered before and why anyone cared, but equally I don't care now either. Slightly different name, cool. Let's move on to actually interesting problems. If the name change helps some people focus on those more interesting problems, then it was a good change.",1542743022.0
iuseptt,"NIPS is just a Neutral abbreviation,  I don't see any kind of injustice in the name itself",1542757895.0
duskybomb,Add me,1542701480.0
dlrlgugu,Any plans after yolo3?,1542694681.0
dhurba87,Interesting idea. Count me in.,1542698413.0
surajpaib,Count me in too. I have a slight modification I am interested in making to the Yolo V3. Maybe add a pose detection bit as well. In reference to the ObjectNet3D dataset. http://cvgl.stanford.edu/projects/objectnet3d/,1542698931.0
flabbychicken,Add me too. ,1542699529.0
rusikg,+1,1542700319.0
Nitin2103,I am interested. Count me in,1542702699.0
07_Neo,Count me in,1542703002.0
bracciodiferro,\+1,1542705497.0
karanbangia14,me too,1542705523.0
Sabtainahmad,Count me in too,1542706243.0
mihir-sam,I am currently learning YOLO v2 from youtube. I would love to join. Thank you. ,1542706995.0
sravankm0015,I'm interested,1542707234.0
SlenderShady99,Add me too,1542708403.0
turtle_13,Can I join? Idk what yolov3 is?,1542716552.0
,[deleted],1542716629.0
ank_itsharma,I am in. ,1542716963.0
niki_niki123,"Please leave your skype id, then I can add you on the skype.",1542717432.0
sohaib_01,I am very new to all of this so maybe I can join too? Skype id: sohaib.arshid101,1542721141.0
bhashithe,Add me too pls?,1542725341.0
PsychologicalPanic,Add me too pls. It sounds very interesting!! Skype id : shourya97,1542725509.0
navan00,Yeah interested,1542725973.0
duskybomb,iharshitjoshi,1542726300.0
ikaaye,shivamwe,1542726524.0
therealkenkaniff,"> 2176 GPUS

hahaha.
haha.
ha.",1542685372.0
_bnjmn,Start by learning coding.,1542667265.0
cpbotha,"Also, until recently (pre RTX) NVIDIA made sure that fp16 hardware support was disabled on consumer GTX cards, also on many of the Titan cards.",1542647106.0
merton1111,"The tools out of the box work better on 32-bit. If you want, you can start tweaking them to make them work. Wasn't worth my time. 

For big data, running more efficient algorithm is more important than precise, since you have more data that you can consume. But for most application, data is often limited, so waiting the extra time isn't such a big deal. ",1542640921.0
arachnivore,"People have known for a while now that DL systems can work well even when the numeric precision of the weights is as low as 8-bits (fixed-point).

Google's original Tensor Processor operated on matrices of 8-bit integers. The ""tensor cores"" in Nvidia's GPUs operate matrices of 16-bit floating point numbers. Intel's AI processors use a proprietary low-precision numeric representation called ""flex-point"" I believe.

I don't know how much academics use low-precision methods, but I'm pretty sure they're widely used in the private sector. The reason ""why"" low-precision formats work so well might be too poorly understood to justify using them for research for research not directly related to numeric format.

The conventional wisdom seems to be that you want to train networks with medium-precision weights (32 or 16-bit floating points) then reduce the weights to low precision (16-bit floating or 8-bit fixed point) for inference.

Jeff Hawkins, the guy working on Hierarchical Temporal Memory, claims that biological synapses are so noisy that it makes little sense to represent them with more than 2 or 3 bits. They simply aren't precise devices.",1542642379.0
cameldrv,"For inference, 16 bit usually causes no difficulties.  For training though, more attention has to be paid.  Gradient \* LR is necessarily going to be of a lower magnitude than the value of a weight.  With bfloat16 for example, if the gradient is less than as much as 1/127 of the magnitude of the weight, the gradient won't cause the weight to change at all, even if this gradient is applied thousands of times.

Natural neural systems may be low precision, but they're also stochastic.  If you make accumulation stochastic, low precision should work OK for training, i.e. subtracting 1/254 has a 50% chance of reducing the magnitude of the number.",1542657127.0
corysama,Lower precision during training makes convergence more difficult.,1542656770.0
ph03n1x333,"As others already mentioned, more FP precision will generally lead to better convergence during training. 

I can only add that, intuitively, think of scaling large values in your input data for deep learning increases the importance of decimal precision. Also, tanh and sigmoid activation functions will output -1 to 1, 0 to 1 respectively putting most information to the right of the decimal point.

From a hardware standpoint, you will see performance gains in terms of TFLOPs for float32 in both GeForce and Tesla series GPUs. This is a  table I made based on benchmarks found online (my apologies, I did not note the sources and couldn't find a FP16 benchmark for the K80) : [https://imgur.com/dPCCxgo](https://imgur.com/dPCCxgo)",1542712702.0
robot_parlan,"It is not much of a tutorial, but I implemented two common encoder-decoder architectures (fcn-8 and U-net) in PyTorch; you can find it here:

[https://github.com/parlstrand/ml\_playground/tree/master/computer\_vision/semantic\_segmentation](https://github.com/parlstrand/ml_playground/tree/master/computer_vision/semantic_segmentation)

Hope it helps.",1542649434.0
bellari,Try searching “teacher forcing” and you will have better luck.,1542600503.0
secularshepherd,"I don’t really think that this is a ML problem at its core. If you’re making a on-the-fly mock-up UI, why wouldn’t you just use buttons and drag-and-drop? As opposed to getting a training set of doodles. I get that it’s a proof of concept, so ideally, you would have a classifier that can classify N ways, but Im not sold on the idea.",1542587119.0
Abhishek_Advance,Awesome ,1542588261.0
MungoSoft,"For the right compensation, would you be open to mounting a (free) camera on your car, that takes pictures at regular intervals and anonymously shares them with self-driving car companies? 

Just some random info:

-> In some country (like switzerland) it's not really legal to do so due to privacy laws. ",1542544993.0
Enthusiast_new,"After you develop your code, run it on a small sample of data that will take at most an hour. Check the results to see if everything is going well. If everything is working fine, run it for entire dataset. ",1542538361.0
bpooqd,"its pretty much the same as with any other software system. What is helpful is to set the seed to some known value (in tensorflow tf.set\_random\_seed, numpy also has some special function to compare arrays (against test fixtures). Look into pytest/pytest-cov as a runner.",1542537046.0
whore_plains,I just had some Pynchon flashbacks. ,1542514953.0
Phnyx,"Try training a model on this for 2h with a 100% usage in summer weather, then check the temperatures of your GPU.

While it's a good laptop for the price, using a laptop for heavy duty tasks can be dangerous due to the limited cooling system. You can either blow your fan out or your system will just throttle down is performance to stay under a certain temperature.",1542524994.0
400_Bad_Request,"Steer clear from RTX from the time being, it's having issues and it doesn't provide significant benefits for its price as compared to the previous generations. Id suggest the 1080ti. An alternative is cloud computing as suggested by the previous comment, it's much cheaper and you'll get better performance for your models. Secondly your performance for deep learning depends on the amount of data you feed to it, if you're using it only for small datasets then it's an overkill imo. ",1542476580.0
jorgemf,"Everytime I read someone saying go with the cloud I remember when I spent 1500$ in 3 days. So if you go with the cloud make your numbers first.

PS my dataset was big and I was using 4 or 6 machines. ",1542504066.0
sudoankit,"http://timdettmers.com/2018/11/05/which-gpu-for-deep-learning/

I follow this great guide. 

TL,DR;
RTX 2070 is the best/£. ",1542481291.0
CzoKc,Unless you plan to do training 24/7 on your own graphics card it might not be worth to buy one. Why not use a cloud alternative? 1 year of GCP might end up costing a lot less. ,1542562972.0
fkxfkx,"Why bother?

Use the money and time to pay for online resources such as google, AWS or databrick, etc

Let them worry about the details and get cracking on the algorithms, data and code.

Hardware is a masturbation fantasy and an enormous time sink.



",1542476197.0
tkchris93,I feel like getting accepted to these conferences is becoming more like a lottery than anything. Get an unlucky draw on reviewers and you're screwed,1542442449.0
aditya_arun,"Looks like there were 7100+ submissions.

&#x200B;

ref:- [https://twitter.com/shaohua0116/status/1063790457195360257](https://twitter.com/shaohua0116/status/1063790457195360257)",1542464147.0
znihilist,"Hmm, the effect of Stemming/Lemmatization with Arabic text data could be a very interesting concept to look into considering how most words come from a limited three letters root.",1542580248.0
thisismyfavoritename,Wasn't the algorithm released more than one year ago?,1542433155.0
Hiant,Also I always find it funny that when people talk about these techniques that do a head to head result of some dataset horseracing them as if these things are plug and play and not highly dependent on the type of error in a particular data,1542454519.0
Gnonpi,"It's interesting, but that's not deep learning",1542446426.0
wavefield,"5.4, I think it's broken",1542462208.0
400_Bad_Request,"What's the dataset you used?
",1542462426.0
AlphaPulsarRed,GDPR,1542431635.0
bluecamel17,I went with Threadripper 2 for the 64 PCI lanes.,1542536667.0
ph03n1x333,"The major CPU considerations when building a DL workstation are (in order of importance to me):

1. Number of PCIe lanes - you will want at least 40 if you plan on ever running 2x GPUs
2. Number of Cores 1 (Data prep)  This happens in the CPU and - as already mentioned - you will want enough speed / cores to pre-process your datasets. I run a single Spark worker for most of my data prep and note that as of Spark 2.4 images are now supported as data sources ([https://issues.apache.org/jira/browse/SPARK-22666](https://issues.apache.org/jira/browse/SPARK-22666))
3. Number of Cores 2 (Containers) - If you think you may run other applications/containers on the same box then more cores+RAM will be needed. For example, I run NiFi for my data ingestion pipeline, TF serving, Jenkins, Zeppelin+Spark, as well as quite a few others. 
4. Overclocking - Most gaming motherboards used for DL workstations make it easy to overclock. Why not take advantage of this :)

In my opinion, even though Intel MKL is fairly well-supported in DL with the major frameworks (Caffe2, CNTK, MATLAB, MXNet, TensorFlow, and even Theano) CUDA/CuDNN is by far the major framework for hardware accelerated deep learning. 

On the AMD side, I haven't seriously looked into hardware acceleration, however, there is some more information available on the ROCm/MIOpen Github page: [https://rocm.github.io/install.html](https://rocm.github.io/install.html)

For my build, I went with an i7-6850k ([https://ark.intel.com/products/94188/Intel-Core-i7-6850K-Processor-15M-Cache-up-to-3-80-GHz-](https://ark.intel.com/products/94188/Intel-Core-i7-6850K-Processor-15M-Cache-up-to-3-80-GHz-)) since it met the requirements above and was the best value in my price range.",1542714544.0
Phnyx,"This depends on how much you will use single-core processes. This is often the case in preprocessing (tokenization, label encoding, etc).

Many tasks in Python run on just one core unless you rewrite everything for multiprocessing. For this, Intel is clearly better.

If, on the other hand, you will also run regular machine learning algorithms like tree-based models, having more CPU cores will be helpful. Here, AMD will give you more for the money.

Personally, I like AMD's underdog image but would still prefer Intel for machine learning as they have more related software and also offer Intel Optane memory (which is good considering the current RAM prices).",1542400256.0
_spicyramen,"For Tensorflow there is only support for Nvidia GPU, which framework are you using?",1542401751.0
parihar_atul_21,Great ,1542444129.0
parihar_atul_21,"Thank you very much Karthik for sharing this
",1542439650.0
ph03n1x333,"Looks like a good build. My only suggestion is that you may want more RAM as you are likely going to run more and more containerized applications (Ex: Nifi for data ingestion, Spark for local parallel processing on larger datasets, TF serving, Jenkins, whatever). 128GB would be ideal IMO if you can afford it.",1542378432.0
merton1111,"Why not cloud? It's not like you are going to run that thing 24h/7 at 100%. With cloud you can scale both up and down. Especially with a $5k budget. 

No maintenance... no need to manage software... no need to share with coworkers... no need to manage internet connection...",1542389253.0
bluecamel17,"Newegg has the HP EX920 1TB on sale for $165.  That's a better deal, IMO.",1542536900.0
tpinetz,This one explains what you need to know: [http://deeplearning.stanford.edu/tutorial/](http://deeplearning.stanford.edu/tutorial/). If you need more starting knowledge look for a basic calculus course.,1542377498.0
thisismyfavoritename,"Are you referring to computing gradients (derivatives)?

If so, grab any calculus book, ideally one tha also covers multivariate calculus and Jacobians.",1542375709.0
nevides,Nice article,1542367516.0
thisismyfavoritename,"Good article, especially for pointing out the Bayesian prior interpretation.

Just to be clear, I think you should specifiy that the NN objective function at the beginning is for a binary classification problem!",1542375614.0
zeroows,"Good job, also for others who are interested the 2nd course here talks more in depth 

https://www.deeplearning.ai/",1542453468.0
,[deleted],1542364761.0
gecko_from_geico,"Datacamp.com has some good material, Also check out Andrew Ng",1542358113.0
CzoKc,"Here is a relevant Kaggle: [https://www.kaggle.com/itoeiji/deep-reinforcement-learning-on-stock-data#](https://www.kaggle.com/itoeiji/deep-reinforcement-learning-on-stock-data#)  


But I wouldn't think DRL would perform as well as other regression methods. Regular Deep Learning will most likely be much more advantageous. 

&#x200B;

Also if you plan on actually using it, keep in mind that there is a paradox in predicting stock market price with everybody using ML. When a ML technique gets published in predicting stockmarket its also and the day it stops working.",1542326333.0
p-morais,RL is about solving sequential decision problems. Price prediction doesn’t strike me as that kind of problem so it may not even make sense as an RL problem,1542343694.0
cody2007_2,"Partly depends on what you're doing and if you want to get new or used parts.

If you are doing something like a tree search for batch creation, you may want to prioritize getting more cards, but if you want to do something like object recognition you might want to get fewer, but higher throughput cards.

At this point, in case it's not obvious, you probably want to stick to NVidia's offerings.

The CPU and RAM aren't as important. But again--it depends on your use case. If you're doing object recognition your batch generation is simple. If you need to do some type of rendering or physics engine, obviously your requirements will be more.",1542319998.0
ItachiUchiha8045,Go with laptop as it will be hassle free once the environment is setup it will become more handy as compared to google as your have to run code and download the large dataset everytime,1542301472.0
_spicyramen,Colab now offers TPU,1542300544.0
_spicyramen,"I would be careful of getting Hardware which may get obsolete very fast. One of the advantages of Colab or other cloud infrastructure is that they continually evolve and DL is moving very fast, Hardware's available in Cloud offerings like Colab or DL virtual images may be updated more easily and could be more cost effective be in the long run. But all depends on your use case ofc.",1542301545.0
new__vision,"[https://www.semanticscholar.org/](https://www.semanticscholar.org/) is an AI-powered ArXiV indexer. I like that it shows ""highly influential"" citation counts, [here's an example](https://www.semanticscholar.org/search?year[0]=2013&year[1]=2018&venue[0]=ArXiv&q=deep%20learning&sort=relevance&fos=computer-science). It's been helpful when searching for relevant recent papers about a specific topic.",1542308043.0
Catalanist,very relevant question. Commenting just so I can further visibilize the problem. I am in a similar situation as you.,1542304464.0
connerxyz,ArXiV sanity preserver.,1542333656.0
antiquemule,"I'm dabbling in deep learning too. The most obvious thing for us to do is to post interesting looking papers on here with some initial impressions and ask for feedback. Kind of like an on-line journal club. Putting in the hard work of building our own intuition of ""what's hot and what's not"" is a good investment, IMHO. The insight may be pretty poor at first, but we'll learn :-).",1542320639.0
cy1994,"It depends on the size of the dataset. If you're finetuning and have only a few thousand images, keep a larger fraction for training a few 100 or 1000 for test. If you have 100k images totally then the test set can have a few thousand images. The test set needs to have enough variety to represent your dataset as a whole, similarly for val.

Source: [deeplearning.ai](https://deeplearning.ai) videos",1542290865.0
cody2007_2,"I disagree. I think tensorflow is already at the right level of abstraction--abstracting it any more I think risks hiding and obscuring what is actually going on. And this can make troubleshooting impossible for users because they may not necessarily know how all the pieces are put together.

&#x200B;

In any circumstance where it was for some reason preferred not to use tf, I'd actually recommend moving in the other direction--working with cuDNN directly. I think there's a better argument as having that as an introductory approach because it really requires a solid understanding of how gradients must be passed around and the problems some loss functions can have wrt gradient magnitudes.",1542319045.0
warriorpush,This looks good. Your speak clearly and at a good place. I'll definitely follow!,1542262693.0
LoveOfProfit,Ai for everyone... If they pay us,1542259547.0
HugoWagner,Just learn keras. 95% of the time you'll be able to do what you want to do in a business setting with keras and it's way easier to write. Then when tensorflow 2.0 comes out Lear. That if you feel like it would be useful,1542224947.0
a_gj,"Just learn PyTorch or Keras, in my opinion amongts the best frameworks available out there!",1542232073.0
p-morais,"I would vote for Pytorch. Since Pytorch has define-by-run execution you can just insert breakpoints, timestamps etc directly in the code where you define the network, which means you can easily insert arbitrary profiling code between layers etc. ",1542232864.0
bobycv06fpm,"All the following free ML courses are associated with some form of Image or Text data. 

[https://www.coursera.org/courses?query=free%20courses%20machine%20learning](https://www.coursera.org/courses?query=free%20courses%20machine%20learning)

You may review them at your free time.",1542252850.0
saig22,"[https://gluebenchmark.com/tasks](https://gluebenchmark.com/tasks)

Sentences are already normalized and tokenized",1542381698.0
shl45454,check kaggle dot com,1542217088.0
brarbirender,Kaggle,1542222458.0
toanchitran,"Google those website below:

&#x200B;

UCI Machine Learning Repository

Kaggle

Quandl

Data.gov

data.world",1542252708.0
ljferguson94,"Thank you, all! These sites were EXACTLY what I was looking for. 

Keep making them GPUs thinking deeply so we don't have to! ",1542252973.0
ph03n1x333,"Congratulations - no small feat...

Take a look at some of the examples that come with Keras: [https://github.com/keras-team/keras/tree/master/examples](https://github.com/keras-team/keras/tree/master/examples) \- they are all well-defined and are ""clean"" data.

&#x200B;

Specifically, examples that use CNN/RNN layers such as cifar10\_resnet.py.

&#x200B;

All the best!",1542319538.0
duyth,"cool stuff. just curious how this is done in real time?  
Do you load both fornite dance clips (via OBS)  & capture players (via webcam) then run them both through the DNN at the same time? Is it resource consuming ?  
",1542257080.0
indrid_colder,Amazing.  Personally have never seen or heard of a demo of anything remotely approaching general intelligence.   Or even constrained to a tight domain.  Must be top secret.,1542217492.0
PK_thundr,I'll believe it when I see it. We're still figuring out how to protect against single pixel attacks but apparenly AGI is around the corner?,1542223415.0
WhichPressure,"Ilya Sutskever only summarized achievements in DL for the last 6 years and pointed that this progress prophesies near term AGI. Either he knows something more than we, or he just hunts for the investors :D",1542236886.0
SrData,One of the most critical parts for training Neural Networks is the memory of your GPU. With 4GB you will be struggling to train a decent model. I’d say that 6GB should be the minimum to call it mid-range. There are very decent and cheap GTX 1060 with 6GB of memory. I hope it can help. ,1542174989.0
Laboratory_one,"I’d suggest that you opt for an SSD over HDD. Reads from an HDD drive will bottleneck you for sure. You won’t be able to maximize GPU performance. 

Additionally, I don’t think you need the optical disc drive. I haven’t used one for a very long time. A USB stick would go further IMO. 

Definitely get more RAM. Even though you mentioned you’d upgrade later, it’s worth it to have as much as possible early in your ML experiments. 

I like your plan of upgrading the CPU and GPU later. If you can, try to get a better CPU first though. You don’t want that bottlenecking either. 

I say this because theirs some effort involved to get the most out of your GPU for machine learning, and you said you’re a beginner. 

TLDR; SSD > HDD. No need for an optical drive. At least 16 GB of ram. CPU is still important. ",1542176264.0
ph03n1x333,"I know this topic is highly subjective & situational but I was able to piece together the build below and I've been very happy with it:

&#x200B;

2x EVGA GTX 1070Ti GPUs

Intel Core i7-6850K

ROG STRIX X99 Motherboard

32GB Corsair LPX 32GB DRAM 3000MHz / DDR4 (late,r I upgraded to 128MB)

1x Samsung 860 EVO 1TB  SSD

2x Dell 2TB 7200 RPM magnetic drives (Had these laying around - good for ""warm"" big datasets)

EVGA SuperNOVA 850 G3 Power Supply

CORSAIR HYDRO SERIES H60 AIO Liquid CPU Cooler

Ubuntu 16.04 (headless)

&#x200B;

I’m happy with my two 1070Ti GPUs. I’m getting acceptable performance on medium-sized image datasets and large embeddings training CNN or LSTM layers. Additionally, when I use the Horovod optimizer / OpenMPI, I have observed close to linear performance increases when using the second GPU. GeForce series cards do not use SMI to share memory between GPUs for deep learning and instead use the PCIe topology. Tesla series cards with NVLINK would presumably have this capability. I’m sure this is slowing me down somewhat but it is difficult to say how much. I want to reiterate that I stand by 2x 1070 Tis as a superb configuration at this price tier. 

The i7-6850K supports factory overclocking and 40 PCIe lanes (very important for a deep leaning configuration) I frequently have sustained 100% usage on all 6 cores overclocked to 4k GHz without any problems. I’m sure that I’m missing out on some MKL hardware acceleration but as far as I’m concerned NVidia is the only game in town anyway.

I’d give the ROG STRIX X99 4 out of 5 stars for my use case. It supports the 32 PCIe lanes I need to drive my two GPUs (+8 to spare). Overclocking CPU/RAM was easy and the RGB lighting is beautiful. Only two issues so far. One is that the wireless drivers in Ubuntu 16.04 were a little flakey for me however with my setup can easily plug into wired ethernet so it isn't an issue. Two, there is no built-in graphics adapter so if I ever run a GUI in I’ll have to sacrifice some VRAM.

Turns out the 850W power supply is overkill. The GPUs have a power cap at 180W and even during sustained usage don’t come anywhere near this. While I like having the extra power available in case I load it up with a bunch of drives, it really isn’t necessary for me.

Lastly, the white LED on the H60 liquid cooling system is WAY too bright. It bothered me so much that I ended up putting a piece of electrical tape over it. Additionally, the outer plastic brace on the developed a crack in it. As far as I can tell the brace is decorative but I’m up to 2 cosmetic annoyances from this cooler. Although my CPU temperature peaks at only around 65C, I wouldn’t recommend this cooler. (edits: typos/grammar)",1542284232.0
ph03n1x333,One other thought to the point of more CPU cores + more RAM. I find myself running quite a few containers at any given time. Applications like NiFi for data ingestion+ pipelineing and Spark for parallel processing & writing to data to read-optimized formats.,1542319203.0
GayColangelo,Have you tried using Colab? It won't work for applications but it'll be good enough for most problems ,1542183440.0
TomBob42369,Looks like a nice list of parts. Wouldn’t really call it mid range. More of a low end pc build. A mid range I would say is 800-1200 USD but thats just me.  If you can wait I would save up more money for a better build. ,1542173927.0
bevice,you could also think about using an amd processor. they are really cost efficient,1542180151.0
Hiant,"I'd try and push and get the biggest memory wise gpu you can afford. It will speed up your training immensely. If you need to cut back, the optical drive is useless for deep learning( too slow) you could go with an amd processor. Also I noticed that you are using a micro atx Mobo. If you are thinking of upgrading to a bigger gpu card in the future make sure it would fit in the case. A lot of the newer ones are double sized",1542205950.0
clfkenny,Perhaps these videos would be more enlightening if you explained the actual algorithm in greater depth rather than plugging data into libraries and calling it a tutorial.,1542162726.0
aihaihara,is there any example using adaboost with neural network?,1542615960.0
HaohanWang,"There are quite a few methods for that general purpose.

* The most intuitive one will be domain adversarial training: [https://arxiv.org/abs/1505.07818](https://arxiv.org/abs/1505.07818) which requires you to have an extra speaker identity classifier and the **labels of speaker identity**, so that you can perform an adversarial game to push the representations to be speaker invariant.
* I have two other papers that follow the same goal, but with different approaches and applications:
   * [https://arxiv.org/abs/1609.05244](https://arxiv.org/abs/1609.05244) for video (multimodal) sentiment analysis
   * [https://arxiv.org/abs/1803.07276](https://arxiv.org/abs/1803.07276) for biomedical application
* Recently, there is a paper that is likely to appear on ICLR 2019 (and maybe as oral) on this topic, where they free the need of **labels of speaker identity** during training data. The essential goal is to automatically help the neural network learn representation independent from other statistics you don't like. However, you need to design the statistics. In the paper ([https://openreview.net/forum?id=rJEjjoR9K7](https://openreview.net/forum?id=rJEjjoR9K7)), they design the statistics to be superficial statistics of images. If you want to try this, you may need to design the statistics also.
   * In fact, one of my friends and I are working on designing the statistics for speaker identification these days, I hope we can get there soon.",1542157509.0
therealkenkaniff,"I'm sure there are more easily digestible resources out there, but here is the original paper on ""Deconvolutional Networks"" (Deconvnets) - https://arxiv.org/abs/1311.2901",1542171417.0
fz-29,"For your point number 2, check this out: [http://yosinski.com/deepvis](http://yosinski.com/deepvis)",1542171657.0
bunnnnnnnyx,"Thank you for posting this! Been working on NLP project and was wondering what is the purpose of embedding layer. Sadly, the keras documentation did not help me. This is a great article. ",1542155140.0
HamSlayer-,"Thanks for sharing! I'm in a similar situation. I'm interested and want to get started, mostly for fun. For the experts of this sub, how valid/up to date is this playlist? ",1542114083.0
_bnjmn,"I am currently an engineering undergrad that got interested in Machine Learning and Neural Networks.

At first it was frustrating because I don't where to start learning and I'm having a hard time understanding the technicalities of 
implementing neural networks (though, I can understand the concept).

But gladly I found this awesome playlist. I only have about a month of Python experience (I have a Java background), and I was able to follow along with these videos and I am proud to say that I have implemented my own neural network trained with my own pre-processed data and incorporated the model in my own project.

So, I'm sharing this amazing find with my fellow deeplearning enthusiasts. Enjoy!

Thanks deeplizard for this playlist.",1542110662.0
polandtown,"Python user here, I've installed Visual Studio (I believe) and am still being prompted during the Cuda installation that I don't have it? What am I missing here..",1542082791.0
notsorealsanta,Do you have a Nvidia card?,1542093588.0
LewisJin,darknet doesnt support cuda9.0 ,1542084337.0
chiraqe,how many images did you use for training?,1542079890.0
affinitive2,Using YOLO and the darknet architecture!,1542000911.0
punkehazardo,"I'm not sure if I understand your question correctly.

Faster R-CNN firstly gets RoIs from it's RPN (region proposal network) and performs classification only on these regions. So i would assume there would be one bounding box per RoI, not per category.",1542009306.0
angaria37,"implement alexnet, residual nets and YOLO by darknet",1541930240.0
ggghash,If you have the time build a web scraper for something you are interested and gather your own data. Then do analysis of that. Build and train a neural net on you own collected data. I've found that managing collection and processing of data is a crucial part of deep learning that is neglected by many courses.,1541960066.0
400_Bad_Request,"Style Transfer, Everybody can dance if you're into iy",1541940302.0
OkinawanSnorkel,I personally like GANs by Ian Goodfellow if you're up for it!  I also enjoyed reinforcement learning techniques like DQN and Actor Critic to play games like Pong and Space Invaders (Mnih and others).,1541927552.0
bluefourier,"I do not see how is that going to make you better in deep learning. It will make you better in development, but you are probably already good at that.

What I would do would be to pick a good library, develop some familiarity with its SDK and then devote most of the time in understanding what deep learning is really good for and how to use it to solve a problem.

",1541928598.0
xGQ6YXJaSpGUCUAg,"It might be because you are using a tanh activation function at the output of your network. Try to use no activation function for the last layer (linear).

If it doesn't work provide your source code.",1541931842.0
akukaja,I would be interested in your code. Could you share it with us?,1541935050.0
400_Bad_Request,Try using the square of the error as your loss function. Could be the problem ,1541941138.0
extracoffeeplease,"This is all of arxiv, but most of the papers are DL I suppose. Fun!",1541951540.0
venkuJeZima,Looks promising. That is your work? ,1541933165.0
bluefourier,This is called [Named Entity Recognition](https://en.m.wikipedia.org/wiki/Named-entity_recognition) and there are classic statistical but also machine learning ways of approaching it as a problem.,1541972618.0
DevTechRetopall,Uses neural networks and genetic algorithms. ,1541861655.0
hsankhla,"nice, did you made it on unity? I have always wanted to do this, is there some thing like tutorials for genetic algo that i can follow to do it?  ",1541873968.0
rhumsta,Dude great job! I'm just getting into AI and deep learning myself. Your intro helps fuel my passion and drive in this field!,1541870755.0
angelinux74,Great presentation! Are your slides available?,1541883026.0
Samawrabi31,This is awesome!,1541890546.0
Only_fish_No_SauerK,"Hey, maybe you can follow some image in-painting algorithms.",1541824707.0
gluchi,First you need a dataset and try to label the phases.,1541850280.0
gluchi,Image by image label it to its respective phases. What are the different phases in the microstructures?,1541851665.0
ak96,"Hey!
You are into Microstructure Informatics. What's your background? 
I am just curious as I have completed my Bachelor's in Materials Science recently and am playing around with Deep Learning on Udacity and other MOOCs. I want to make a career in Materials Informatics.",1542277864.0
moazim1993,https://www.deeplearningbook.org/,1541806209.0
lepuma,Where can I find the paper?,1541856331.0
SamStringTheory,Yes,1541777665.0
Cunic,"I have been investigating this as well and it seems to help in some cases. It's sort of a hot research topic, too, so keep on the lookout for more implementations. Check out papers from ICML 2018, there are 4-5 on related topics!",1541789722.0
homaralex,Is overfitting a problem?,1541735071.0
Cejan781,"Sorry meant to define ""better\*\*"" as an improvement in the MCC.",1541712388.0
chewxy,"your definition needs work: if it has the same initial weights and biases, and the same meta-stuff then it is the same neural network. 

your second question instantly breaks that assumption - different CNN architectures would have different numbers of weight matrices. Therefore your first question assumptions do not work anymore..

Perhaps a more interesting question is to train 2 same CNNs with images that have exactly two objects and two labels. Each CNN is trained to a different label. Will the performance be uniform? Perhaps. Good question to ask",1541719170.0
Jesper89,"Well that’s three very different applications for CNNs and in order to achieve those, the architecure of your network would have to be very different. However, you would still be able to employ the excact same CNN for feature extractions but I do not think that make any grounds for comparing them across classification, detection and segmentation. ",1541715698.0
Hiant,It'll look like some deep dream shit where everything is a peacock eyeball,1541715627.0
homaralex,Could you share your dataset? The task sounds interesting and I decided to give it a try myself,1541709625.0
barnett9,Do you think song is indicative of album cover in a meaningful way? My guess is that your GAN will not converge before overfitting occurs.,1541710009.0
moazim1993,"You can’t run it on the test set, then you learn the distribution information of that test set. You would use the training set and use that Eigen vector to transform the test set.",1541650236.0
pimp4robots,"I would recommend running on whole training set. I am not entirely sure but you can see it as a preprocessing step like mean substraction, which is always computed on entire training set",1541650262.0
tkchris93,"If you're using sklearn, use PCA().fit_transform() for your training set and .transform() for your test set",1541651624.0
Assasin_Milo,"A principal component analysis is a transformation of your data, it doesn't need a training set.  It is a tool to reduce the dimensionality of your original data while ideally losing as little information as possible. ",1541654037.0
IdeasRealizer,[Muvilab on Github](https://github.com/ale152/muvilab),1541652773.0
plshelpthedog,"I suggest CVAT: [https://github.com/opencv/cvat](https://github.com/opencv/cvat) 

Supports polygon annotation and motion interpolation.

Demo: [https://youtu.be/6h7HxGL6Ct4?t=93](https://youtu.be/6h7HxGL6Ct4?t=93)",1541659902.0
drakesword514,And then there is amazon that uses deep learning to filter their job application resumes. ,1541620429.0
arkrish,"Currently deep learning is used primarily in image and voice recognition tasks. Uber uses them to enhance its map detail by looking at the street signs. Google has a lot of work on text to voice conversion. Similar is in the case of Apple. I don’t know of any case which doesn’t have voice and images/video which need deep learning. Salesforce uses NLP techniques quite a bit but I’m not sure to classify it as deep learning. 

There is a much larger set of scenarios where statistical learning is in use. So if you go by pure numbers, statistical learning may be prevalent. ",1541618953.0
MonstarGaming,"> I have seen conflicting articles that mention traditional algorithms still prevail and wondering what are the use cases outside the Big N

This is because deep learning is quite computationally heavy compared to most other traditional ML algorithms. When it comes to scaling your application you will ALWAYS opt to use the algorithm that provides the quickest run time if all other things are equal (in this case, error). Also, customers/upper management do not like the feel of a black box. Unless you spend a ton of time mapping out each node you're not going to have a good idea as to why your network is giving as a certain prediction or which feature is weighted heaviest. That question is almost always something you'll want to answer because being able to predict something is really only half of the product. You often times need to be able to explain why your model is predicting the way that it is.

&#x200B;

I know its mentioned that deep learning is used quite a bit for image processing but other algorithms have been proven to work just as well if not better in some cases (which leads us back to why would we wait longer for the result if we can wait half the time with a different algo?). For instance K nearest neighbor performed better than LeNet on the image data for the postal service. Also, SVM's have proven to perform very well on imagery because its ability to map out nonlinear decision boundaries (Neural network's strong suit) is very good when using the kernel trick.",1541635407.0
easylifeforme,what news feed is this? ,1541624316.0
Phnyx,Clickbait,1541520878.0
arcticwolffox,Congratulations and good luck!,1541524906.0
nps_rsc,I am as well. Lets be study buddies. Starts this Friday.,1541527218.0
blitzheart,"How do i join? I wanteeeeeed to learn but don't know where to start.

I guess this is the best way to learn, yes?",1541567447.0
Runpdxbridges,"Hi All,

I am in too! I would like to join your study groups or be study buddies. ",1541641366.0
shsshs1,"Check that the one hot encoding is not in a separate Array. So basically sth like X=[24,0,0,0,1] 
Then: input_shape=(X.shape[1],)

May i ask what school expects you to do machine learning?",1541520490.0
pcidev,"I like ""how mind works"" by Steven pinker. It's not nunc about algorithm but will give a good understanding of neuroscience",1541668306.0
ippasodimetaponto,"For a first approach : goodfellow, deep learining.

For a more mathematics approach : Haykyn, Neural Networks and Learning Machine.

For some other reason :Antony, Neural Network Learning, Theoretical Foundation. ",1541492981.0
fkxfkx,"There is no real connection between deep learning neural networks design and actual brain neuron features and function.

Think of CNN as a cartoon model meant to aid explanations of how the algorithms work much the same way various incorrect cartoon models in physics attempt to explain gravity or photon electron interactions.

IOW no point in spending time on it.",1541508715.0
Hiant,I love the book machine learning for text by Aggarwal,1541475943.0
rylaco,their is a course on NLP using Deep Learning by Stanford on YouTube. ,1541481656.0
michaas94,"You might want to take a look at A. Karpathy PhD thesis: [https://cs.stanford.edu/people/karpathy/main.pdf](https://cs.stanford.edu/people/karpathy/main.pdf) . In particular, chapter 3 may be a good baseline for your task. Of course, you can go for more modern text or image encoding techniques, but I think this is a good beginning.",1541523294.0
ThatsALovelyShirt,What are you matching them by? ,1541466768.0
aziz_22,"I think it is probably due to the nature of the problem you are solving.
Images are better solved using CNN.
Did the authors in the paper use lstm in the experiments (sorry  I just read the abstract)",1541452632.0
henrietteyoungmc,"I guess you want to read about “gradient descent algorithms” in backpropagation .

I read “neural networks and deep learning” by Nielsen, when I started. Otherwise just use google or scholar.google.
Backpropagation is pretty old so you will find plenty.

",1541416030.0
hinduismtw,"Yes, you are correct. This ""automatic feature engineering"" is the *breakthrough* that eliminated hand engineering of features.

This is a major idea that is one of the reasons for the success of deep learning (CNNs more specifically). ",1541442504.0
Krokodeale,"The course provided by Udacity is excellent.

[https://eu.udacity.com/course/deep-learning--ud730](https://eu.udacity.com/course/deep-learning--ud730)",1541410532.0
hodorhodor12,What level math do you know? Do you know linear algebra?,1541458077.0
ninimben,"I don't know exactly but my guess is that if they had found a way to reliably automatically encode data for processing from visual inputs like that they would have mentioned it somewhere. More likely humans manually encoded the games.

AlphaGo Zero actually trains by playing games against itself and is even better than the original AlphaGo which made headlines: https://deepmind.com/blog/alphago-zero-learning-scratch/",1541386628.0
CzoKc,Object detection and topic modeling as features on a classifier?,1541369401.0
king_ricky_ricardo,Try http://www.arxiv-sanity.com ,1541446969.0
quanqinle,"omg,there is no comment here!",1542636371.0
late_to_ml,"True labels as the name suggests, is the actual label of the data. 

When a label is assigned randomly to the data (does not necessarily match with ground truth) it would be a random label. Now, coming to why would anyone randomly label the data, here is one interesting experiment with random labels to show the capacity of the deep learning models and how they generalize well despite this huge capacity. 

[Understanding deep learning requires rethinking generalization](https://arxiv.org/pdf/1611.03530.pdf)

Noisy labels may be more common than random labeling",1541423503.0
hodorhodor12,Seems completely not worth the time to do. The gpu is weak compared to cards used for deep learning and there are no libraries for it. ,1541308384.0
-TrustyDwarf-,not deep enough.,1541294009.0
Cunic,"Forgive my ignorance, but what is an example of a ""(practically) infinite input""?

Typically if you have crazy high-dimensional input you still know the dimensionality and have multiple layers projecting your input into smaller and smaller dimensions.

Or you can do unsupervised dimensionality reduction before feeding into a neural network.",1541253943.0
Cunic,"One approach is to try processing your data to match your problem. You would be comparing predicted values at one timestep with the true values at a later timestep. For example, at timestep 10, predict a value, compare this value with the value observed 5 steps in the future (timestep 15). Then you process timestep 11, predict a value, compare it with the value observed at timestep 16. Then to optimize the model, you minimize the difference between these predicted values and the true values: [(\hat{y}_10, y_15), (\hat{y}_11, y_16)].

Another approach may be to try to generate many steps at once in a sort of language-translation model: predict the next value, use that predicted value as the input to the next step, and repeat. Once you compute a few steps, compare all of those steps to the actually-observed value at those steps and minimize. This is another approach to time series forecasting.",1541254425.0
nickbuch,"When is this sub going to retire the “Intro to Machine Learning” tutorials and instead direct them to /r/learnprogramming or something? 


Honest thought, not shitpost. ",1541255569.0
scottyler89,"RNNs should be quite good with a periodic time series - I'll note though, that people are moving towards using GRUs over LSTMs because they perform similarly well, but have fewer parameters to train, so as a result they train more quickly and require less ram.",1541194949.0
BruinBoy815,Rnn and LSTM are the best neural network for predicting ts. YES,1541194512.0
scienceistoohard,"The reason is that the required number of Fourier coefficients grows exponentially in the number of variables in the function that you're approximating, whereas a neural network doesn't necessarily have to scale exponentially.

Suppose you had a function f(x), and you could approximate it to sufficient accuracy with a Fourier series that has 1000 coefficients. Great. Now suppose that you want to approximate f(x,y) the same way: now, instead of 1000 fourier coefficients, you need 1000\^2 = 1,000,000 coefficients. And if you want to approximate f(x,y,z) then you need 1000\^3 . And so on.

&#x200B;

So, if you're trying to do image classification (or whatever), then your function has thousands of variables, and it becomes impossible to store enough Fourier coefficients to get an accurate result. This is what is called the ""curse of dimensionality"". 

&#x200B;

The distinguishing feature of neural networks is that, for certain applications, they provide sufficiently-accurate approximations for high-dimensional functions. This isn't true for all applications, it's worth noting; whether or not a neural network works well depends on the kind of function you're approximating and on the level of accuracy that you need. Machine learning applications are notable in that they have a (comparatively) small number of correlations between variables that you need to capture, and you can get good results with very poor accuracy in your function approximation.",1541172766.0
ippasodimetaponto,"I think your is a good question.  Observe that Fourier Series, even the generalized ones , from <\\ R\^n \\to R \\> , are particular neural network of depth 1  (i.e with only one hidden layer)  and with activation function $y=sin(t)$ .

This is the reason why Universal theorem doesn't give an answer to your question: sin(x) is a good non-linear function which satisfies the hypothesis. 

&#x200B;",1541165816.0
p-morais,"https://arxiv.org/pdf/1703.02660.pdf

Look at the section on RBF policies. Not quite the same as you’re suggesting, but it’s a similar vein.",1541177322.0
PinkyPonk10,"I'm seeing questions like this crop up more and more.

Personally I question that an image, as normally presented to a neural network is basically as series of floating point numbers one for each channel of each pixel in the image (so essentially BMP format) takes up say 2mb, whereas a fourier transform based representation of the image  (I. E a jpeg) takes up a tenth of the size.

If the information in the image can be represented by the jpeg version then why are we not running neural networks entirely in the frequency domain?

",1541187875.0
wehnsdaefflae,"I don't understand what you mean. Can please be a bit more detailed? For example, maybe you only talk about supervised learning, but neutral networks can also do unsupervised learning. Also you might consider only rational data. Neutral networks can also deal with nominal data. But that's all just an assumption...

Maybe you can give an example where Fourier transformations can be used for regression?",1541138629.0
Nater5000,"Simply put: Fourier series are built from existing, already known functions, where as Neural Networks *learn* these functions.

Basically, you can't construct a Fourier series from a bunch of data, which is exactly what a Neural Network attempts to do.  For a Fourier series, the function already exists and you sample it to determine how to approximate it using sine waves.  So it is approximating a function, but we need to already have access to it for this to be applicable.  For a Neural Network, you give it a bunch of examples (inputs and corresponding outputs), and it builds a function that attempts to approximate a function that models the data.

An interesting take on this, however, would be to use linear regression with sine waves as basis functions to model data.  This would give you a series of sine waves that tend to model the data, but of course, this isn't a Fourier transformation.",1541162741.0
iDemandEuph0ria,"Can you provide more information? Why do you think this has something to do with using multiple GPUs? Did it work fine with just 1 GPU?  Has this ever happened before?

There can be 2 sides to this problem:
1) model: Is this a problem when, say you are using some other architecture/model/task? I haven't used CycleGANs before but do they by any chance have a ridiculously large number of parameters? Cuda out of memory could have caused it? (Though it usually just stops execution if that was the case). I would check for memory leaks etc in case you have made some changes to the model. Sometimes ppl accumulate loss in itself and not loss.item().. This stores gradients and could lead to memory overflow. 
2) hardware: check to see if you have the latest drivers. Are you running on AWS or something (that could completely rule this problem out).

Let's say this isn't a problem when using a single GPU. The only thing I can think of, ruling out any bug in the code, is the model/data parallelization is extremely inefficient. ",1541170097.0
hodorhodor12,You need to provide way more information. When does it freeze? What is last output on screen. Does it freeze with a single gpu? Freeze with other models? Etc. ,1541458221.0
Hiant,Are you plugging the monitor into the  gpu? ,1541476049.0
400_Bad_Request,"Kaggle datasets
Google dataset search",1541133490.0
philophysis1,"I think you’d need to be more specific. There is not one answer to this.
Deep learning can be applied to many domains: video, images, sound, music, dna, ...

If you have a specific idea in mind you will have to do internet searches to see what you can get for free. It may be that you have to collect a lot of data by yourself, which is painful, but an important part of many applications. Look at it from another angle:

If there is free data out there, your application may not be that unique. The dataset defines your application. If your dataset is unique and was painful to collect, your application has a potential to become something unique as well. There is not really a need for another dog detector ...",1541138190.0
1ab94ee,"If you want to more specific data, and the data unavailable in popular source like Kaggle or Google Dataset Search, you can use web scraping to gather data from site pages.",1541146815.0
king_ricky_ricardo,https://medium.com/datadriveninvestor/the-50-best-public-datasets-for-machine-learning-d80e9f030279,1541186229.0
WhichPressure,"It's called ""model zoo"". Here you can find models and pretrained weights for tensorflow: 
https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md

For YOLO architecture you can find here:
https://pjreddie.com/darknet/yolo/

For any famous architecture it's possible to find pretrained weights.",1541103044.0
ai_is_matrix_mult,"For team PyTorch the model_zoo is built into torch.utils (see here):
 https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/torchvision_models.py",1541115667.0
suki907,http://tensorflow.org/hub/,1541123537.0
thisismyfavoritename,"There is no notion of 'datetime' in RNNs, merely timesteps.

Meaning if you have a new record for every time step, you are ok.

The analogy would be sentences, where the ""time"" dimension is implicit (the sequence of words).",1541090764.0
roomylee,Have anything in mind that you think is awesome and would fit in this list? Please feel free to make [*pull requests*](https://github.com/roomylee/awesome-relation-extraction/pulls).,1541089715.0
Womblr,"The only real benefit is the pre installed software and whatnot which in reality would take you an hour or two. So if that's worth the extra money to not have to deal with it then go ahead, otherwise it seems unnecessary ",1541086190.0
vtcsengineer,"Building will be far superior.  Here's one possible option but you could easily spend more or less on just about every component just depends on your specific use case:

https://pcpartpicker.com/list/74DRMZ


I've been reading that a number of 2080 Ti's have been breaking recently, that along with the supply chain issues and the absurd cost/performance ratio I'd stick with 1080 Ti's.",1541113972.0
Hiant,I personally went prebuilt but instead focused on companies selling gaming PC s. I liked letsbld.com but also check out iBuyPower and cyberpowerpc. The gaming approach gets you what you need without getting gouged for wanting something with two high powered gpus,1541495678.0
gimel12,"I have built by myself my deep learning workstations, after a few month there was a lot of problem when updating and make it run it smoothly, so I decide to buy a plug and play solution and from all those companies I would recommend Bizon or Puget, I personally bought from Bizon since they offer a great warranty on the hardware and they have develop their own stack app which make easier for upgrade and run all the frameworks that I use so I don’t need to worry to update my Nvidia drivers, Cudnn or Cuda. ",1541086310.0
qwasz123,"Project malmo, depending on the observer, typically provides you blocks around agent. I've trained an agent to shoot a bow and arrow with project malmo. I'm now looking to train models with the raw input of pixels. To do that, I've decided to use Serpent AI.


So, it depends on what you're looking to implement. An agent to play minecraft completely would be too complicated with our current implementations of machine learning. To have an agent walk around avoiding blocks, you need to know how you want to feed input.",1541088684.0
oathbreakerkeeper,RemimdMe! 8 days,1540935506.0
baahalex,"No clue, but you could crawl amazon, build a dataset and post it here",1540920162.0
shorty_luky99,"For my home setup, i just use the nvidia runtime for docker and use the tensorflow image with gpu support. This works like a charm on my ubuntu, only have to install nvidia graphics driver, which is 2 commands in ubuntu. On my fedora (dual boot) however i could find no support for the nvidia runtime, and since i didnt want to spend my time installing the whole cuda tooling and co, i just use ubuntu for DL.

Didnt try to install the cuda tools on fedora though, so if you want to do it the hard, it might work

Edit: onky sounds funny, but i meant only",1540899188.0
Razmyar,"Yes, you can. 

**""But Fedora is enough good for DL too?""** what do you mean exactly by good enough? I have used both Fedora and Ubuntu for deep learning and have not noticed a significant difference between them. I believe more people use Ubuntu because there are more resources. Anyhow, as mentioned in the previous comments, use a docker. My suggestion is to spend some time to build your own image, so you can customize it based on your own needs. It will help you in a long run.

&#x200B;

**""Cuda tools support only Fedora 27.""** Nvidia tends to support the platforms which they consider tested and stable. Fedore 27 life cycle will end around one month after the release of Fedora 29th. So I assume they will shift to Fedora 28 soon. ",1540915955.0
velos,"Hmm.. NVidia + Linux... usually not guaranteed smooth ride (if it can go at all) be it Ubuntu or Fedora... I dont know, I might be wrong, am interested to find out how to use NVidia GPU for deep learning in Linux  too",1540914196.0
,This seems more like an advertisement for Coursera and involves a lot more than just deep learning. Please don't do this.,1540961544.0
nvidiaturing,Its an advertisement for Coursera ?,1541084689.0
BruinBoy815,Is there a link with the breakdown of various categories?,1540877358.0
BruinBoy815,Dude this is really cool,1540877026.0
lushdogg,What DL technologies are being used to drive this?,1540920176.0
PunKeel,Where did you find the data to train it?,1540929059.0
faz445,"If you want the new model to detect all 25 classes and the data quantity for the new classes is on par with the original 20 classes, and you have time and computing resources, retraining from scratch with stochastic sampling  will give the best results.

&#x200B;

If you use transfer learning by just fine tuning the old model on the new classes, yes catastrophic forgetting will be a concern. (Transfer learning w/ fine tuning is a good option if you want to build a classifier that just detects the 5 new classes and if they have limited training data compared to the original 20.)

&#x200B;

&#x200B;",1540859090.0
ai_is_matrix_mult,"Or better yet, PyTorch :)",1541115772.0
moazim1993,Now that’s doing the important work,1540844224.0
,The hero we need,1540848932.0
,Is that you elon-chan?,1540848942.0
bostaf,I'm dead.,1540843535.0
wehnsdaefflae,"""This project applies an implementation of Image Inpainting for *Irregular Holes*""",1540852449.0
adikhad,This is the greatest invention in the history of mankind!,1540869410.0
E-3_A-0H2_D-0_D-2,"Every day, we stray further from God,,,",1540878330.0
baahalex,Deep Hentai is here. Rejoice!,1540911718.0
niblince,Innovation ,1540888851.0
FloreTheFlaus,Can it work with other vids/images?,1541021488.0
selelee,anime was a mistake,1540846610.0
Hiant,"Classic 

[https://www.aclweb.org/anthology/D14-1181](https://www.aclweb.org/anthology/D14-1181)

&#x200B;

&#x200B;",1540850841.0
Hiant,There isn't enough data on the person you are calling. Yes you could have millions of telemarketing recordings but nothing generalizes to everyone. The best telemarketers have their own personal style that gets them success and they can pivot in the middle of a call into another tactic. A robot is only going to get one shot to be right and it's likely to be some shitty average of successful calls that plain doesn't work,1540842296.0
HaohanWang,"I think on the topic about deep learning for medical usage, one of the most important thing is to make sure that what your model learns is the genuine signals, instead of some false signals that result in high prediction accuracy. 

I have a Quora answer on this: [https://qr.ae/TUhGkK](https://qr.ae/TUhGkK) , from which if I need to summarize briefly: 

* To understand the importance of confounding factors: [https://arxiv.org/pdf/1807.00431.pdf](https://arxiv.org/pdf/1807.00431.pdf) 
* A follow-up: [https://www.biorxiv.org/content/biorxiv/early/2018/10/13/442442.full.pdf](https://www.biorxiv.org/content/biorxiv/early/2018/10/13/442442.full.pdf) 

The second one is my own paper, but honestly, I have many papers and I only recommend the ones I'm proud of to others. 

 ",1540832267.0
booooooooooradley,"Check out the proceedings from MICCAI and ISMRM, two conferences that are aimed at ml in medical imaging.

Medical image segmentation is a big topic (tumors, lesions, fractures, strokes). If you haven't read about the UNet yet, I'd highly recommend reading it and 1000+ variations of it.

[https://arxiv.org/abs/1505.04597](https://arxiv.org/abs/1505.04597)

&#x200B;

At the last miccai (august 2018), there was a segmentation challenge with winners receiving gpus from nvidia. Almost every participant used some combination of a unet. You can download the data and play around with it.

[http://medicaldecathlon.com/](http://medicaldecathlon.com/)

&#x200B;

Also, get familiar with the DICOM file format. Pydicom is your friend.",1540920223.0
edxsocial,"While I can't help specifically in regards to your medical image analysis question, you might be interested in the Deep Learning program from IBM on edX:

[https://www.edx.org/professional-certificate/ibm-deep-learning](https://www.edx.org/professional-certificate/ibm-deep-learning)

&#x200B;

Hope that helps,

Josh from edX",1540919496.0
,[removed],1540800461.0
invincible_guy,"Maybe this will help to some extends, https://ajinkyat.github.io/gsoc/mistakes/",1540803430.0
cheviethai123,"I means where is your loss function here and why do you use lstm. And you have to know CNN only return feature vector represent your datapoint and LSTM take the vector to learn with many gates to avoiding gradient vanishing and return output vector as the type you want. Maybe one-to-one, one-to-many, many-to-many depend on what you need. After that, you can adding softmax layer to classify or any classifier such as SVM,... to identify the output vector. So it's all depends on your problem and what your dataset is about.

Here is the example of text detection that combine these two method that's trending lately

[http://s3.51cto.com/wyfs02/M01/A5/CD/wKioL1nDcBCBjAWJAABZpCDMq8c604.jpg](http://s3.51cto.com/wyfs02/M01/A5/CD/wKioL1nDcBCBjAWJAABZpCDMq8c604.jpg)",1540807465.0
_theFaust,"[David Silver’s Reinforcement Learning Course](https://github.com/dalmia/David-Silver-Reinforcement-learning) 

GitHub link has links to YouTube videos, lecture slides, and most importantly, assignments (Jupyter notebooks to complete)",1540757690.0
Laboratory_one,"I’m taking this one right now from Coursera: [Practical RL](https://www.coursera.org/learn/practical-rl?) 

I like the assignments but the videos dont cover go deep enough into theory for me. I’d still recommend it. ",1540790462.0
400_Bad_Request,You could try the YouTube course by deeplizard,1540747559.0
sanchit2843,I think move 37 by siraj raval is best course i have seen yet. ,1540745103.0
Manto1,Deep RL bootcamp lectures on youtube https://youtu.be/qaMdN6LS9rA,1540747968.0
r2m2,Might wanna add .DS_Store to your gitignore...,1540793720.0
oDaftDank,"I have tons of 1070ti and 1080ti GPU's that I was trying to offer for the same purposes on here and was basically, oddly, discouraged? Maybe I could somehow use my resources via your org? I was trying to donate the time. I have over 125 GPU's

https://redd.it/9pylbt",1540741309.0
E-3_A-0H2_D-0_D-2,Thanks for this!,1540705527.0
MungoSoft,"cool stuff.

could you add [https://github.com/autonomio/talos](https://github.com/autonomio/talos) as a standard package ? For now I just reinstall it each time.",1540734057.0
daguito81,"Awesome thing to do guys. I have a 1070 GPU because of gaming and doing a couple tutorials and dabbling in a bit of DL I see the giant difference between training with GPUs and CPUs.

Kudos",1540719485.0
pastaking,How much disk is on each server? Ssd?,1540730044.0
LevKusanagi,Thanks! Pretty awesome.,1540732783.0
extracoffeeplease,"Thanks! Anyone managed to get tensorboard running in jupyter lab? I can run it separately, but then I can't access it..",1540735715.0
nvidiaturing,Awesome! I have few 1070 Tis. Can I help you guys?,1541085300.0
caseyscottmckay,"Do you have machine readable data for cannabis flowers? 

If no, get data. 

If yes, throw a common machine learning algorithm at the data--e.g., logistic regression, RNN, or CNN--evaluate the results, calibrate your model, rinse, and repeat.",1540672771.0
Echsu,You might want to look at https://www.kaggle.com/c/plant-seedlings-classification . It is a Kaggle challenge with the objective to identify seedlings of different plants - so quite similar to your problem. There are plenty of kernels to get you started. I personally found that just re-training the Xception net in Keras.applications did the job pretty nicely.,1540720480.0
Hiant,This is a very easy problem you just need the data. Why don't you start looking at projects on medium,1540741914.0
StartingOffSmall,Check out Comma.ai's paper: Learning a driving simulator,1540673079.0
anilmaddala,"If you really want to learn, building your own project is the best way. Before that understand the concepts of DL. Cs231n and videos by Siraj Raval on YouTube are great place for this. To get more technical you can refer to deeplearning.ai and fast.ai. Pick a framework like Tensorflow or Pytorch to build your project.",1540612953.0
Mathriddle,"[**RSVP Link**](http://bit.ly/Deep_Learning_Lounge)

[**Biocom**](https://www.google.com/url?q=https%3A%2F%2Fwww.biocom.org%2Fs%2F&sa=D&sntz=1&usg=AFQjCNFbQhuSccnpeoyGyLPStHHccR3YEg)**,** [**The Machine Learning Society**](https://www.innovation-labs.co/community/machine-learning-society) and [**CO Network**](https://www.innovation-labs.co/co-innovation-labs) invite you to our 3rd Annual Big Data Summit. This one-day event is an annual convergence of industry executives, bio-technologists and data scientists dedicated to deploying emerging technologies within the life science industry. Through a blend of illuminating keynotes, dynamic panel discussions, and interactive use-case studies from life science organizations, this year’s Summit will focus specifically on the disruptive and transformative role of AI and Deep Learning.  This conference is aimed at giving attendees the knowledge, tools and solutions critical to Big Data success within our industry.

This years gathering will focus on bringing together thought-leaders at the forefront of innovations in artificial intelligence, blockchain technology, computational drug discovery, genomics and robotics. Join us on November 6th, in San Diego to experience the future with the people responsible for creating it.",1540591530.0
practicalutilitarian,Where? When? and What?,1540593906.0
Cunic,My first thought is that you need citations. Too many unjustified claims to be trustworthy.,1540582033.0
VectorD,Show us the code man,1540529504.0
Oblivious-Man,Log your gradient outputs and take a look to see if it's the issue.,1540532157.0
mtanti,When this happens to me I usually solve it by using smaller minibatch sizes. What are you trying to do with the neural network?,1540567276.0
adamits,Can we see your parameter initialization?,1540611944.0
autotldr,"This is the best tl;dr I could make, [original](https://blog.insightdatascience.com/generating-custom-photo-realistic-faces-using-ai-d170b1b59255) reduced by 94%. (I'm a bot)
*****
> Conditional generatorsConditional generators, represented by conditional GAN, AC-GAN, and Stack-GAN, are models that jointly learn images with feature labels during training time, enabling the image generation to be conditioned on custom features.

> To solve this problem, the key innovation of our TL-GAN model is to train a separate feature extractor model y=F(x) using an existing labelled image dataset, and then couple the trained GAN generator G with the feature extractor network F. Once this is done, we can predict the feature labels y pred of the synthetic images x gen using the trained feature extractor network, and thus establish the link between z and y through synthetic images as x gen=G(z) and y pred=F(x gen).

> Classification: Choose a pre-trained feature extractor model, or train your own feature extractor network using a labelled dataset.


*****
[**Extended Summary**](http://np.reddit.com/r/autotldr/comments/9rlzxf/generating_custom_photorealistic_faces_using_ai/) | [FAQ](http://np.reddit.com/r/autotldr/comments/31b9fm/faq_autotldr_bot/ ""Version 2.02, ~360058 tl;drs so far."") | [Feedback](http://np.reddit.com/message/compose?to=%23autotldr ""PM's and comments are monitored, constructive feedback is welcome."") | *Top* *keywords*: **feature**^#1 **image**^#2 **model**^#3 **generate**^#4 **latent**^#5",1540569626.0
MungoSoft,"DL is a subpart of ML.

What is your background ? It seems that you need some knowledge in linear algebra, calculus and coding to follow thoses courses.",1540486968.0
Cunic,"Are you in a rush? If not, I would recommend ML first to understand the problems DL solves and also where that fits into the larger scheme of ML.",1540582099.0
theoldcircinus,"You might have a bigger vocabulary if you use a bigger corpus. Consequently, even if you keep the same validation set, the task is harder, and the perplexities are not comparable",1540480480.0
Statistical_Incline,https://machinelearningmastery.com/encoder-decoder-models-text-summarization-keras/,1540474588.0
Chocolate_Pickle,"May I ask what you're trying to achieve here?

Because there's a _lot_ of ML papers out there, both published and in pre-pub locations like ArXiv. I think you need to be a bit more clear with your intent.",1540449924.0
donutloop,Feel free to add refs to papers,1540448487.0
Kaleidophon,That's going to be one long list??,1540449878.0
donutloop,I have read some papers they are pretty generic,1540450199.0
Hiant,So all buzzwords? ,1540445780.0
This_Is_The_End,Use Keras API of Tensorflow. Access the weights by a property of the model object. Reshape and display. ,1540359483.0
E-3_A-0H2_D-0_D-2,"For Keras, make a forward pass till the layer where your target kernel is. Access the weights of that kernel using the Keras API, reshape it, and display it using matplotlib.",1540360905.0
noitq,"I wroted a simple network before, I extracted the filters in testing phase by using this step:

1st: get the filter name:  **encoder\_kernel** = graph.get\_tensor\_by\_name(""conv\_encoder/kernel:0"") 

2nd: run with the tensorflow session:  result, encode\_result, kernel\_result, mse\_result = sess.run(\[model, encoder, **encoder\_kernel**, mse\], feed\_dict={x: batch\_x}) 

&#x200B;

Refer to my repo in github: [https://github.com/noitq/conv-encoder/blob/master/main.py](https://github.com/noitq/conv-encoder/blob/master/main.py) in test() function

&#x200B;",1540361425.0
_vb__,"Although I have never done it in Python based frameworks, but is it possible for you to access the middle layer(s) of the CNN ?
In MATLAB there is an option to access the components of the CNN, after we have 'built' (not necessarily trained) the network.",1540354981.0
Razmyar,"To answer this question, we need to understand that deep learning field is rapidly improving. That suggests the currently available libraries/frameworks will go under a lot of adjustments. Consider Tensorflow as an example: Tensorflow version 1.0.0 was released on February 11, 2017. The current stable version is 1.11.0, which, has been improved so much since their first version. Tensorflow version 2.0 (not released yet) is expected to be a significant milestone, with a focus on ease of use. The same story applied to the other available libraries. I believe we will have more libraries to be available in future and, users will choose a framework according to their needs. So I think it is more important to learn/update ourselves with the new deep learning concepts/theories rather than focusing on the frameworks. 

&#x200B;",1540394709.0
Hiant,Pytorch maybe h20,1540445855.0
Hiant,Where's the last two trends,1540445941.0
SamStringTheory,"That's normal. It means you are very close to a local minima and your weights are fluctuating around that minima. To get even closer, use a schedule for your learning rate in which you decrease the learning rate over time. If you are not already, use a sophisticated optimizer such as Adam or Rmsprop which automatically decrease the learning rate over time.",1540264901.0
KorChris,How about using gradient clipping to overcome it? ,1540277469.0
adamits,Are you training in batches?,1541962463.0
glamdring001,"This one works as of **November 2018**! **NTA2D6W**

https://www.paperspace.com/&R=NTA2D6W",1541350330.0
zenxx3,it works: [https://www.paperspace.com/&R=JVR1JCI](https://www.paperspace.com/&R=JVR1JCI),1541583009.0
itdoesntmatter13,"Tensorpad. 

It would be cool if you could mark the free ones with an asterik or something. Or list the amount of free GPU time allowed for trial. ",1540289034.0
onclick360,Is your lacture are free or charge ?? I am impressed with your way of teaching ..,1540261077.0
Razmyar,"Yes and no. You will be fine using ur machine for the [fast.ai](http://fast.ai/) projects. Make sure to invest on a good Nvidia graphic card. Also, upgrade ur ram to at least 16 GB. I finished [fast.ai](http://fast.ai/) and the Udacity self driving car courses with this kind of system (with a 1070 and a 1060 Nvidia card) with no problem. Make sure to learn batch\_generators and multi\_processing techniques. Also, most deep learning frameworks provide APIs to pre-process ur data on a GPU instead if a cpu. Learning those will improve ur training time a lot. 

Having said that, upgrading ur system will improve the overall learning process, specially when dealing with large models. The question you will need to ask is how significant that improvement will be. Start learning and save money  to upgrade your system in future.

&#x200B;

A suggestion: Use Linux instead of Windows on your machine since it requires less resources.",1540218290.0
dt_magic,[https://www.youtube.com/watch?time\_continue=316&v=7I0Qt7GALVk](https://www.youtube.com/watch?time_continue=316&v=7I0Qt7GALVk),1540212252.0
realRishabhSagar,"Tip from personal experience, don’t get bogged down in the details of the lessons.  These classes are best tackled in multiple passes.  If you are struggling with any part, put a pin on it and come back to it after you have had more exposure.

Focus on coding things on your own rather than listening passively and trying to ‘learn by listening ‘. It is deceptive how much difference there is between watching and doing stuff.

Also, have your own ideas or projects that you’d want to apply newly learned skills, that way you’ll retain more of the knowledge.",1540222400.0
Jazoom,"You might get a better answer by asking this on the Fast.ai forum. I believe that's what it's there for.

Edit: but do search first. I'd be very very surprised if this hasn't already been asked multiple times.",1540211697.0
XSpring,"Not an expert in the area but i encountered this situation before. This situation is known as text similarity with low resource. You can try googling with these keywords to get some papers. Common approach is based on translation or transfer learning. A simple solution is to translate those text to English, and train a model on those translated sentences. Another baseline is to use multi lingual word representation (fasttext website), and compute the cosine of the two sentence representation with word average approach.",1540209609.0
velos,have you tried shuffling the data before training?maybe the missing portion is being allocated for testing? ,1540182713.0
Hiant,Did you do any augmentation? You need that. You should also make the training set as balanced class wise as you can or it won't generalize well,1540446441.0
soco,Fast.Ai,1540159989.0
marrabld,Getting on a project where you can start applying it will be the best education you can get,1540173011.0
schrodingershit,CS231n Stanford,1540168126.0
chewxy,"Consider the formula `y = log(x+1)`. As an AST, it looks something like this:

    log(x+1)
         |
        x+1
         |
      +--+--+
      |     |
      +     +
      x     1


The problem of course is `log(x+1)` yields wrong answers at very small values of x. Ideally you'd want to replace the entire thing with an AST that is just `(x -> (log1p(x))`. But the expression as written was `log(x+1)`.

The graph solution helps because you now have manipulable objects. You can delete nodes from the graph. And replace them with the correct/optimized nodes, BEFORE running the network. And since you have to parse the input functions anyway to a graph form, might as well keep it there. Only when running do you convert it into flat formats 

Doing it in a stack of closures (say, a SECD machine style) would only serve to complicated things.

Functions are graphs are functions. 

(FWIW, many deep learning libraries, [gorgonia](https://gorgonia.org/gorgonia) included, does compile to a flat form. If you're interested in the compiled form, [here's the code](https://github.com/gorgonia/gorgonia/blob/master/compile.go) )

More resources (that I wrote): [Deep Learning From Scratch](https://blog.chewxy.com/2017/04/26/deep-learning-from-scratch-in-golang/)",1540105925.0
thisismyfavoritename,Because the chain rule can naturally be expressed as a graph. That's even how I was taught in my calculus class.,1540096597.0
totallynotAGI,"Not all of them are, [thinc](https://github.com/explosion/thinc) apparently uses just higher order functions.

But I'd say, efficient *state-free* composition of both outputs and derivatives wasn't really a solved problem until perhaps recently: [https://arxiv.org/abs/1804.00746](https://arxiv.org/abs/1804.00746)

Using computational graphs just seems like an easier thing to do, considering most of the deep learning frameworks are done in some kind of object oriented language. Handling all this complexity + making it functional just makes it all that more difficult.

&#x200B;

But I do agree, functional approach seems much more viable! There's some new [papers](https://arxiv.org/pdf/1803.10228.pdf) talking about it and it's something I'm exploring in this [repo](https://github.com/bgavran/Compositional_Deep_Learning). ",1540157569.0
Hiant,"No, you're using the function that you would plug an augmentation function into but you need to write this function.  All your code is doing is sampling the original data ",1540093488.0
E-3_A-0H2_D-0_D-2,"I'd recommend modifying your train and validation data distribution such that the train data contains a mix of both augmented and original images, but your validation data doesn't have any augmented images. What you're essentially doing is 'bleeding' information about the augmentations to the validation data.",1540107068.0
Hiant,"It's nice that you are donating all this equipment and time to letting someone use your gpus but I would caution before spending a ton of time and resources getting this ready figure out what someone is trying to do. Writing deep learning code for multiple gpus is different and hard to do. If someone is just doing kaggle is
 hard to believe they need all that compute power. Most of those competitions are won in the technique not the computer cycles and some would argue (I'm on the fence) that winning kaggles has little to do with real world problems and more a function of stacking boosted models",1540446894.0
vtcsengineer,"This setup is completely impractical because you'll be hamstringing individual GPU performance, will need to use at minimum 3-4 jerry-rigged power supplies, a custom enclosure and for a whole host of other reasons but...


There are motherboards that can directly support that many GPUs via riser adapters which were designed for crypto mining such as the Asus B250, Asrock H110 Pro BTC+ or Biostar TB250-BTC Pro.  However the bandwidth available per card would be limited to a single PCI-E 2.0 1x lane which will seriously impact deep learning performance. However none of those motherboards support more than 32Gb of RAM and the LGA1151 platform as a whole is limited to 64Gb.


The only motherboard chipsets and processors that support 128Gb of RAM would be an X79/X99/X299/C422/C602/C612/C621 chipset with an LGA2011-v3/LGA2066/LGA3647 based Intel processor or an X399/SR5690 chipset with an sTR4/G34 based AMD processor.  But you won't find a board with more than 4-5 PCI-E 3.0 16x slots which means you'd need to use several incredibly expensive PCI-E expansion switches along with extenders/risers to hook up that many cards at a reasonable bandwidth which will still limit performance just not by quite as much.

http://cyclone.com/products/expansion_backplanes/

https://onestopsystems.com/product/expansion-backplane-5-pcie-x16-slots-457

",1540092219.0
bpooqd,"The most sensible option right now is with the ASUS X99-WS with 4 GPUs this is consumer-grade hardware, if you want to have more GPUs it is much more cost effective to just build more of the same configuration and connect via 10GbE. To go to 6, 8 or even 12 GPU configurations requires dual or quad CPU configurations since otherwise you have not enough PCIe lanes and won't be utilizing the GPUs at full performance. Which automatically means server-grade hardware which is \*\*much\*\* more expensive.

Anyhow the only brand I know who offer directly what you are asking is Supermicro, for example: [https://www.supermicro.com/products/motherboard/Xeon/C620/X11DPX-T.cfm](https://www.supermicro.com/products/motherboard/Xeon/C620/X11DPX-T.cfm)

Should be noted that even they only offer max 8 GPU 4U servers.",1540114641.0
SynbiosVyse,"It's not practical to have a single motherboard with that many cards. The enterprise grade hardware, full height rack servers only hold 4 GPUs. Remember, the GPUs are double width, plus take extreme amount of power and need x16 bandwidth for Max perf.

You need multiple chassis, and a good link between them. 1 Gbps is not going to cut it.

If you want to donate GPUs, I can recommend a registered nonprofit that would greatly appreciate it.
",1540520298.0
Jazoom,It's just a bunch of affiliate links.,1540072208.0
,[deleted],1540053637.0
JodofGod,"The primary point of preprocessing  time series is to convert non stationary time series into one that is stationary.

Non stationary essentially would mean that

1) Mean is a function of time (i,e trend shifts)

2) Variance is a function of time (i.e multi modal periodicity)

There are many good reasons to treat these properties which you can find explained here ""[Forecasting: Principles and Practice](https://otexts.org/fpp2/)"" 

&#x200B;

For preprocessing treatments you might want to try:

2) Mean depends on time : differencing (X^(t) \-X^(t-1))

2) Variance depends on time: Power transforms most commonly log, you can also use box-cox transform for this as well",1540092331.0
Rezo-Acken,Need more context. Is it a single time series problem with no covariates ? In which case neural network usually perform worse than simple methods like Theta or Arima. To make a NN barely catch up in forecast you have usually to at least detrend and deseasonalize the series.,1540095262.0
kailashahirwar12,Transformation of your time-series can be a good idea. Try different transformation techniques. ,1540062020.0
adowaconan,"That means your test array has a shape that does not match to your training array. Or the array is not in the desired shape. Check the array row by row, please.",1540023770.0
leanXORmean_stack,"At first glance, really like how you documented your process thinking into this. Very simple. I like it. ",1539961698.0
Nater5000,"I'd say there are two parts: understanding the inner-workings of a neural network (whether it be of a specific architecture or in general) and experimentation.

As most people will tell you, there is no formula for this kind of stuff.  That's not to say it's hand-wavy, but much of the decisions that go into neural network architecture is based on very high-level reasoning of what we want the model to do based on more intuitive ideas.

For example, Inception modules work well and are based on a simple idea: we want to extract features from images on many different scales.  The issues seen with simple CNN architectures was that you seem to have to make a choice between granular feature extraction or broad feature extraction in an image.  Sometimes one way is better than the other, but the next logical step would be to ask, ""how can we do both?""  Inception modules came about by tying to answer that question.  Now, with that being said, the specific design on InceptionNet is based on experimentation.  They had a general idea of what they wanted it to do, but they couldn't calculate and ""ideal"" architecture or something like that, so that started experimenting until they found a generally good model.

As far as how one should practically learn an intuition for developing new architectures (or improving existing ones), that comes from practice.  At some point you're bound to see an architecture that you think could be improved in the same way CNNs were with Inception, and you can experiment with it on your own.",1539958235.0
unrahul,"For me it was reading through connectionist approaches from old literature, recently I was looking into catastrophic forgetting and got plenty of ideas to try out , most of them didn’t work or had problems with end to end training but some did, so I guess broadening area of literature beyond current NN models myt help ..",1539958136.0
s_siddiq,"Deep learning book is awesome! 

Author: Ian Goodfellow 

It has three parts. Third part introduces many open research concepts.",1539974624.0
practicalutilitarian,The best way is to implement existing architectures by reproducing the results from a recent academic paper you like. As you are implementing the algorithm you won't be able to stop yourself from trying to tweak it to make it better.,1539955676.0
lefnire,"The YouTube Stanford courses are great for that, then go back to the fundamentals with the MIT press Deep Leaning book. I'm with you, each new CNN block (residual, bottle neck, ..) is a big WTF for me. For things like that you really need the math core (linear algebra, calc, probability)",1539959220.0
MohamedElshazly,"I usually feel like I'm not doing problem solving when I'm doing DL, I feel like I'm memorizing the code rather than thinking logically about what I'm doing and how to proceed further. I would really appreciate any advice",1540004801.0
HaohanWang,"Interesting, I have just answered almost this same question a few days ago on Quora: [http://qr.ae/TUGE4e](http://qr.ae/TUGE4e) 

Hope this helps. 

&#x200B;",1540059447.0
S_T47,"The following might be good (it’s a curated list of different papers/talks/ blogs):
https://github.com/yenchenlin/awesome-adversarial-machine-learning",1539950592.0
triployd,"hmm it depends on where you want to train your models. 

if you want to train your models on your laptop, get one that has a graphic card like gtx-1050 or above (1060, 1070, 1080, etc...based on your wallet)

if you want to train your models on servers, just buy a random laptop to make yourself comfortable. you don't need a nice graphic card on your laptop anyways.

i use a macbook pro and train all my models on servers...",1539931265.0
realRishabhSagar,"Between google colab and paperspace there are many options that will provide you much better value over carrying a heavy, hot and power hungry laptop everywhere.  Get a decent dell and use google colab for most learning and then switch to paperspace gradient notebook or jobs when you really need to train large models.  ",1539932798.0
Darshut,"What is a ""decent"" price for you ?   

You need enough RAM (16Gb preferably) and VRAM (the more the better but let's say 6Gb to match some graphics card specs). The issue is that such specs are already quite expensive... 

But to give you an answer I'd say :
- gtx 1060m minimum
- 8gb (that's definitely very low)
- Ryzen 7 or i7 CPU

Good luck ! ",1539955622.0
Tarox1988,"Maybe your best solution could be a Laptop with an external GPU/(+dockingstation).

Take for example a Razer Blade Stealth which is an awesome Ultrabook with quite a good CPU if you take the 8th Generation. Additionaly their external GPU where you can throw in whatever fits your needs, let's say a 1080Ti. After coming back you can still decide if you want to continue using the GPU as an external one or if you want to integrate it into a desktop workstation and resell the docking station.",1539987790.0
Hiant,Use Google colab and invest in a reliable VPN. If you need local compute I see a lot of guys using those black and red Dell gaming laptops with Nvidia 1050s,1540447025.0
SynbiosVyse,"Thinkpad P52 has a really good GPU but it's very expensive, big, and heavy. If you're looking for a light option the P1 or X1 can pack a TON of power. The T480 with MX150 is also not bad if you're looking for something a little cheaper.",1540520465.0
ItachiUchiha8045,"You can try paperspace.com  too, price are affordable as well has powerful machines to train model or else if your budget is high look for asus tuf series which has i5 7gen series with gtx 1050ti or acer predator series with same specs which which cost around 75-80k rs",1539932084.0
tayeb83,"[https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/) 

[https://nlp.stanford.edu/pubs/glove.pdf](https://nlp.stanford.edu/pubs/glove.pdf)

&#x200B;

[https://youtu.be/OQQ-W\_63UgQ](https://youtu.be/OQQ-W_63UgQ)",1539870955.0
KorChris,I recommend you refer this paper to follow up. https://arxiv.org/pdf/1708.02709.pdf (Recent Trends in Deep Learning Based Natural Language Processing),1539913223.0
12think,"https://github.com/google/sling

https://github.com/allenai/allennlp

https://github.com/swabhs/open-sesame",1539917465.0
adamits,"I would checkout Yoav Goldberg's textbook ""Neural Network Methods for Natural Language Processing"", and watch some of Andrej Karpathy's Stanford lectures.",1540096336.0
marrabld,"Computer vision with deep learning is very much, convolutional neural nets.",1539861350.0
thisismyfavoritename,"I second the other answers, this is not a task for deep learning or machine learning.

I think I would do something like computing the absolute intensity differences pixelwise and then apply a convolution with a kernel of ones of a given size (e.g. 15) to check if there are regions with a high number of mismatches. You could then apply a threshold on that and manually review those cases.",1539843427.0
chewxy,Onion skins and subtracting images? You probably don't need deep learning for this,1539842155.0
s_siddiq,"Hi, I think to spot differences in images, you can use other tools rather than an advanced algorithm like deep learning. It is unnecessary or even hard to utilize neural networks in this case. As you say, the differences might be just small errors in number of pixels. Neural nets only tell you the probabilities that indicate how much the image is different from what you've trained the net with. There might be other complex ways to implement the process, using deep learning but why just not use a simple lib like python PIL for now!

&#x200B;",1539842457.0
ennnyo,"Our company Rist inc. implemented what you want using deep learning.

And we apply it to the print screen inspection system.

Our algorithm can learn the difference you want to detect and the difference you don't want to detect  
Please give us messages!

[https://www.rist.co.jp/](https://www.rist.co.jp/)

[https://collatio.deep-inspection.com/](https://collatio.deep-inspection.com/)  
",1539848338.0
venkuJeZima,"I do not understand why those questions and why only such narrow range of answers, but you have mine.. good luck with project.",1539776998.0
bohrb,"actually I am excited to see how the data will be used. You got mine, please let us know once your project is completed.",1539832608.0
jorgemf,"98% accuracy makes me think something is very wrong with your model or your training set up. I would appreciate how did you do the training and what is the window for the prediction. If it is to predict the next time step them the model is a bit useless. 
Also, I don't know what you mean by accuracy, a relative error metric as means squared error is easier to understand for regresión problems but accuracy makes no sense. Can you clarify it?
Thanks!",1539780857.0
theoldcircinus,"You should use r\_squared as the error metric

Try comparing this to the ""naive"" model, where you just output the previous day price (curves will look pretty similar to what you show). That would be the very first step to know if your model is creating any value

&#x200B;",1539780254.0
Rezo-Acken,"98% accuracy as in ""usually the stock price is close to previous day but 2% of the time it goes way up or way down"" ?",1539781134.0
This_Is_The_End,What I've learned is denoising data comes for for a price which is basically lagging behind and done with a CNN it is bound to a trained distribution. How does this impact your model?,1539762736.0
Hiant,"No way dude, sorry it should be obvious that something is wrong with your model. My guess is your leaking data some where",1540447230.0
HungryQuant,"There's almost certainly something wrong with your model. Probably the test set leaking into the training. This might happen during the scaling process or your autoencoder could be implicity creating information about the test set and embedding it into the train set.

 I'd bet my life savings that these aren't true, independent train / test splits. I'm 99.9% sure there's something wrong.

If you could get this kind of accuracy on any given stock on any given day, you could create the world's most successful hedge fund almost immediately. If that's the case, I'm not sure why you'd give it away for free on the internet. I hope this isn't discouraging. I'm sure you can find where the issue is.",1540526336.0
RidgeRegressor,"""Leave a star!!"" - really?",1539773649.0
KorChris,So happy in progress of NLP and NMT.  Hoping it works at conversational tasks.,1539711248.0
E-3_A-0H2_D-0_D-2,"Just adding to this - you can hop on to https://grand-challenge.org/challenges/ for a list of all medical challenges in Deep Learning.

Hope this helps! :)",1539705406.0
Diigitalism,"Good article, you're just missing some punctuation here and there (commas, mostly) and I caught like 2 typos but great article nonetheless. Try proofreading your work next time or get someone else to edit it as well.",1539788174.0
OPLinux,"If you just split the original image in 4, I don't think it is possible to run the detection as if you run it on the original input image in 1 network.
It will behave more like 4 individual detectors and you might miss detections on the edges or have double detections.

One way to try and 'solve' that, would be to have some overlapping regions between your images and then do some postprocessing to filter out the overlapping bounding boxes (eg. NMS)

Have you also tried to downscale the image even further? Depending on the type of object you need to detect, you could get away with an even smaller resolution!

Finally, I don't know what detection network you are using, but you might want to take a look at mobile convolutions. I know there is a mobilenet (v1 and V2) with the SSD detector that reaches comparable results, whilst being quite a bit faster and less memory intensive.",1539711727.0
scottyler89,"Depends on a lot of things - this issue might be is due to the 'vanishing gradient' problem. If that's true, then I'd try switching over to elu activation functions and/or making a shallower network and see if you get some improvement on the validation set.",1539693651.0
bo0mb0om,"What are you trying to predict, what is the input data, what is the model architecture? Until you share those it is very hard to judge.",1539696292.0
grimfada,"Why is there no option to answer ""No"" to the third question? Or even ""At least not yet"".",1539681866.0
catchergg,"I think that any problem that is hard to represent in some mathematical form is hard for deep learning.

Take a look at language understanding. Although deep learning made great improvements it's still far from achieving real success. It's enough to look at Siri / Google assistant to understand that once you ask something out of a predefined scope it has no idea what you are talking about.

I think that the main issue is that we have no idea how to properly define a language. Not to mention more complex subsets like sarcasm or humor.

What's next? No idea. But I don't think that simply adding more and more layers will solve all our problems.

Machine learning is explaining stuff to an alien with a really fast mind. Maybe we need to get to the most atomic parts of complex problems and try to explain them better. ",1539674134.0
ransan123,"Ok I tried adding opencv-extra to my local config file for hunter but no luck..

&#x200B;",1539760991.0
nickbuch,"holy shit, based on your grammar and spelling Id say that you should really spend the time/effort to practice your writing.
",1539631748.0
secularshepherd,lmao,1539630276.0
VectorD,"See:
https://pdos.csail.mit.edu/archive/scigen/",1539638647.0
ztasre,"Get the biggest card you can for your budget. You might not even need your own card, Google Collab might be good enough to start out.",1539626359.0
pseudoRndNbr,"I would go with a GTX 1080Ti. I know you didn’t want to go with a Ti, but should come out cheaper than 2 1080s and tbh the 11GB of VMEM matter IMO. 

If that doesn’t fit your budget then I would suggest aws or gce (use spot/preemptible vms to cut costs even further) ",1539697792.0
polaroid_kidd,"Keep in mind you'll have to specifically make your training scripts run in two cards. I haven't done it but I can't imagine it's that easy.

On the other hand... 22GG of gdram...holy shit ..",1539702045.0
vlatheimpaler,You might ask in /r/aws or /r/awslambda to get responses from people who know more about AWS specifics.,1539617096.0
Hiant,I assume you are trying to deploy a trained model? Why do you need the full  opencv?,1540472440.0
nvnbny,"[https://github.com/nvnbny/progressive\_growing\_of\_gans](https://github.com/nvnbny/progressive_growing_of_gans)

&#x200B;",1539599697.0
chan-hee,"[https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/cycle\_gan\_model.py](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/cycle_gan_model.py)

I don't think it is in the original paper, but if you go to the GitHub of the authors of the CycleGAN, the term identity loss is implemented. It seems like, if lambda identity > 0, you want to make your generator(X->Y) to be identity mapping if you supply Y, and vice versa. Your linked code seems similar to the the original implementations of the authors  :)  


I am not sure about the reason behind the identity loss, but my guess is that it could help generator learn the mapping of becoming the Y (for Generator(X->Y)) faster.  To sum up, the implementation for the original paper is defined like this.(The loss is divided into two parts for alternative training for generators and discriminators)

  
Total Loss for Generator = loss\_Generator\_X->Y + loss\_Generator\_Y->X + loss\_cycle\_X + loss\_cycle\_Y + loss\_idt\_A + loss\_idt\_B  


Total Loss for Discriminator = (loss\_Discriminator\_X + loss\_Discriminator\_X)\*0.5  
",1539602921.0
athenysus,"Resnet, unet, vggnet are some examples. Note that the choice of loss function is important in distinguishing what the network accomplishes. For classification, a common loss function is cross-entropy",1539575670.0
Hiant,What are you trying to classify?  Usually I choose the model based on the type of data,1539579146.0
varunagrawal,The GPU can run multiple programs and is limited only by the GPU's RAM capacity. This means that a modern GPU can power your monitor and run a deep learning job at the same time. ,1539550734.0
flashhigh,"Actually, I think the discrete GPU is the one used for most heavy listing. Basic things like monitor graphics and imaging are handled by Intel embedded GPU on the cpu. The Intel HD graphics that comes with most processors is powerful enough to handle normal loads, while additional loads like games and deep learning models are handled by discrete GPUs ",1539579459.0
Karyo_Ten,"Interesting language.

How long did it take you to get that far?

It seems like it's a bytecode language with first class array operation right and even a GC, any highlights you want to share on its design?",1539555175.0
,[deleted],1539564578.0
mmughal,Thanks Ankit are you the one who posted ?,1539577078.0
alebrini,Is it possible to recover and read all the past discussions even if I start from the next week?,1539470794.0
MungoSoft,"Look for keras, easiest framework to start building some NN with a minimal coding experience. Try to find 2-3 jupyter notebook on the subject as a starting point. Google something like ""neural network mnist keras jupyter notebook"" ",1539455585.0
pascguerr,"First you need a strong linear algebra backbone. For that you can check Gilbert Strang's OCW course with the same name. Then it's better to first have some experience with machine learning other than first dive into deep learning as some topics not necessarily but complementary will be reused, such as PCA and SVD. For machine learning you can check the Andrew Ng's Coursera and for NN the Stanford Fei Fei Li's Course from 2017. Hope this helps.",1539445414.0
ndrd4,Let’s do this! I’m computer scientist bachelors degree student  in Mexico City,1539439428.0
flashhigh,"Hey man, I'm a student from Karnataka too. I'm interested in doing some work, I could send you my resume if you want. ",1539444385.0
HDidwania,Undergrad student from Bhubaneswar interested in research in fields of Deep Learning and Computer Vision. Will love to join.,1539457431.0
modanesh,"Hey, count me in. Here is my email in case you needed my CV or to contact: mohamad4danesh [at] gmail [dot] com",1539529966.0
sameerSngh,"Use OpenCv for image preprocessing and use keras on modified images to build your model.
If you want more time efficiency use keras with tensorflow backend (instead of theano).
Or instead use tensorflow entirely to build your model, you can find tutorial on its site.",1539424897.0
jonas_8_,Maybe there are better libraries for this kind of operation. Like OpenCV? I'm not sure ,1539417980.0
Nater5000,"Judging by how your training is setup, you could just convert the batches you train on and possibly cache the converted ones for later. This doesn't really speed up the process, but it distributes it over the training time which can be useful.

You could also convert them now, save them to disk, and have the converted images on disk for later usage. Again, this doesn't really speed up the process, but it could be beneficial.

In one of my projects, I imported images using imageio, which worked well. I was able to pass a parameter to the import function to import the image in grayscale, although I don't know how much faster this would be:

    image = imageio.imread(i, pilmode='L')/255

If you have an issue where imageio isn't working, you can try using the (now deprecated) scipy version. I used this code (via batch processing) in this [project](https://github.com/nathanmargaglio/CSE676-SD19-Classification/blob/master/HW1.ipynb) ",1539653656.0
_mulcyber,"I would say: take a pytorch tutorial and jump in.

Torchvision has a lot of predefined and pretrained networks, so you can use them without knowing much about math.

While understanding the principles of machine learning do not requires a lot of math, if you go in the thing in details it's pretty math heavy, so modifying model or playing around with losses, optimization algortihms, etc might be difficult.

Try to google the tools (criterions, optims, layers) you use in pytorch and learn more about them. You'll quickly know the math you lack. And I fear that their is no way around it.

But as I said, it's if you want to go into the details, you can easily run trainings just with pytorch tutorial. ",1539426754.0
kairess,https://github.com/kairess/cat_hipsterizer,1539403367.0
shadyshadok,Now this is a deep learning network I can get behind!,1539429931.0
Atlasi,"To answer the question of its limitations, I would quote the popular quant Marco Lopez de Prado where he said, ""my dog can recognize faces, but I wouldn't trust my dog to give me financial recommendation""",1539373759.0
kailashahirwar12,"There are many limitations to deep learning. Catastrophic forgetting is one of them. I have written a medium blog post explaining “Why we need a better learning algorithm than Backpropagation in Deep Learning” https://towardsdatascience.com/why-we-need-a-better-learning-algorithm-than-backpropagation-in-deep-learning-2faa0e81f6b

Fake news is a problem which can be put into outlier detection or classification category. There are many people and companies working on it. 

Use this if helpful: https://www.semanticscholar.org/paper/Unsupervised-Content-Based-Identification-of-Fake-Papalexakis/98e95b2af4de42f9155b285c4527e5492e367c83",1539401211.0
Inspirateur,"""even human can't do so""? I very much doubt that, maybe uneducated humans or humans not willing to actually try and find out if it's true...
I think scanning the sources, using differents metrics to get the reliability of the website, cross-check with other reliable websites etc, is a good start if you want to assess the reliability of a news, and it doesn't seem out of reach at all, for human or computer. ",1539395012.0
erdtlm,"If any data, and data processing.",1539506291.0
alexmeistercl,"Python for sure, you can iterate write and run/test without the compilation hassle like using java or c++, this combined with a python notebook becomes really easy, I would recommend to use [https://www.anaconda.com/download/#linux](https://www.anaconda.com/download/#linux) to manage your python environments and numpy for all the linear algebra.",1539364476.0
qwasz123,"Python. You can make a simple one using numpy in less than twenty lines.

Add pytorch and you can make convoluted cnns pretty easily. Of course, that's not from scratch though.",1539362517.0
benelott,"Definitely go for python. To use it in a more interactive manner (maybe you are used to matlab or mathematica), you could use jupyter notebook. Especially for smaller explorations and experiments, jupyter is an absolutely great tool. From your first implementation using numpy (for instance similar to this: https://www.python-course.eu/neural_networks_with_python_numpy.php), you can then go to deep neural network frameworks like keras with tensorflow back-end or pytorch. Do not worry too much about having a GPU and properly installing the libraries to support it. Contrary to what most posts on the internet try to sell you, you do not need any super strong hardware to start with the basics of deep learning. You only need it if you want to train DNNs which are trained on larger datasets with much shorter training time.",1539591627.0
Tusharminj,"Python, no doubt about it. I know one very good blog which I'm absolutely certain you're going to enjoy.

[https://machinelearningmastery.com/implement-perceptron-algorithm-scratch-python/](https://machinelearningmastery.com/implement-perceptron-algorithm-scratch-python/) This is where I started to learn from scratch. ",1539687004.0
scilona,What a world,1539384799.0
,[deleted],1539345495.0
Nater5000,"Judging by how much effort you want to out into it, I'd just try recreating it in Python. Since the repo provides the weights, you'd just need to reproduce the network in something like Pytorch and Keras, then load the weights over.

That, or if you have the data, just train your own model. But a non-Python deep learning implementation is rather rare (I've personally seen more deep learning done in Javascript than C), so you might not have much luck debugging any issues with it. ",1539654280.0
sputknick,"I'm sure the author ideally would like this to be a production tool to use in building NN, but I think the more immediate value is as a teaching tool. If you make NN more visual it will be easier to conceptualize what is happening. What is a weight? How many layers should there be? What is actually happening when a NN is training? Just like the last 20 years was dependent on people learning how to do web programming, I think the next 20 years will be dependent on people learning AI programming and anything that speeds up that process is a good thing.",1539344021.0
DevTechRetopall,Artificial Intelligence Editor,1539341992.0
fkxfkx,"For future viewers, here’s another shot at a link.
https://youtu.be/dUBmj2vE3p8

If it the editor works well, it will be more fun than jupyter notebook for this stage of application development.",1539343887.0
fkxfkx,"Looks interesting. Wish Reddit let us click to YouTube to get to the links in the description.

Or have OP provide it.",1539342843.0
sai_krishna_13579,"Start off by reading 

https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html

Then if you want to dive in (like a course )
Siraj's moove 37 course on school of AI ",1539340314.0
qwasz123,"I'd use a svm as the problem doesn't seem time dependent and they're typically easier to train (data wise).


Though data science is very much like cooking. I'd try both, tune a few hyper parameters and see how well each model functions on the testing samples.",1539389669.0
eric01300,Use densepose,1539335395.0
bo0mb0om,Have you considered retraining a speech to text network for English and retraining it?,1539332532.0
mailfriend88,"I think the first question would be:
Do you have the corresponding trainig data for this language?",1539333198.0
Electron_plumber,"This can wildly depend on the workload. If your workload transfers large amount of data to and from the GPUs, then you might have a bottle neck. 

&#x200B;

The way to figure this out is to look at the bandwidth you need per GPU. This isn't related to the number of lanes connected, but rather the amount of data transferred over time. The number of lanes only tells you the maximum bandwidth available, not the actual bandwidth that is used. Therefore, determine the bandwidth you need per GPU (based on your workload), then add up the bandwidth needed for all the GPUs you want to use. The resulting bandwidth will tell you how many lanes you need from the CPU to the PCIe Switch. Be sure to add in some extra bandwidth as buffer to allow for unexpected peaks. ",1539355212.0
thisCarIsFromCanada,"I have not been able to find information on this because no one has benchmarked it. All I know is if you have 40 PCIe lanes, you would be limited when using 3 gpu's (taking 48 lanes would be required). I imagine how much it is limited would be based on the algorithm, the gpu, the processor, and the version of PCIe.",1539308198.0
sabalaba,"## Results summary

Just a quick TL;DR of the full post: [https://lambdalabs.com/blog/best-gpu-tensorflow-2080-ti-vs-v100-vs-titan-v-vs-1080-ti-benchmark/](https://lambdalabs.com/blog/best-gpu-tensorflow-2080-ti-vs-v100-vs-titan-v-vs-1080-ti-benchmark/)

As of October 8, 2018, the NVIDIA RTX 2080 Ti is the best GPU for deep learning research on a single GPU system running TensorFlow. A [typical single GPU system](https://lambdalabs.com/blog/best-gpu-tensorflow-2080-ti-vs-v100-vs-titan-v-vs-1080-ti-benchmark/#typical-system) with this GPU will be:

* **37% faster than the 1080 Ti** with FP32, 62% faster with FP16, and 25% more expensive.
* 35% faster than the 2080 with FP32, 47% faster with FP16, and 25% more expensive.
* 96% as fast as the Titan V with FP32, 3% faster with FP16, and \~1/2 of the cost.
* **80% as fast as the Tesla V100** with FP32, 82% as fast with FP16, and \~1/5 of the cost.",1539290606.0
iszotic,:v is it possible to make AMD benchmarks? tensorflow can now compile in AMD too with RoCm.,1539308620.0
thisCarIsFromCanada,"What dollar amounts did you use? If 2080TI is 37% faster than the 1080TI, and it currently is selling for at a 100% higher price, I cant see how performance/dollar isnt showing it as the highest performing GPU.",1539298808.0
donsmith2060,"1080ti has 11gb of ram, 2080 only has 8gb, I'd go with the 1080ti",1539284716.0
Laboratory_one,"Agreed with the other posters, get an SSD. I would even skip the HDD completely and get a 1TB SSD. 

You don’t want your storage IO to be the bottle of your sweet build.

Otherwise, it looks good. ",1539277557.0
Ponzupontiff,"Add a 250 GB SSD for OS. This would greatly improve the responsiveness of the machine.

It’s your money, but consider whether you need those high specs. If you haven’t done work in machine learning before you could spend half as much money on the GPU and still be able to learn about the same amount.",1539274818.0
tlkh,"You NEED an SSD for your OS. 512 GB would be nice, although 256 is fine. 

Also, as the other commented said: have you been training models? You shouldn’t be buying a PC to get started with Deep Learning. You should start, understand why you need a PC over eg, Colab or Cloud GPUs, and then go for a PC. ",1539276556.0
dronecub,Deep learning is a subset of Machine Learning. So it's better if you can first learn ML. ,1539245523.0
realRishabhSagar,"Deeplearning.ai is a very good course.  My opinion is although there is some overlap, you can at least start to learn both topics in parallel.  That way there is variety in your learning schedule initially.",1539248479.0
edxsocial,"I'm not an expert in the field, but you may want to look at this Deep Learning MicroMasters program from IBM on edX:

[https://www.edx.org/professional-certificate/ibm-deep-learning](https://www.edx.org/professional-certificate/ibm-deep-learning)

It's a 4 course program with a capstone test at the end.

Regards,

Josh from edX",1539363743.0
apdaga,"I think Coursera is the best place to start learning “Machine Learning” by Andrew NG (Stanford University) followed by Neural Networks and Deep Learning by same tutor. This course is full of theory required with practical assignments in MATLAB & Python. It recommended to solve the assignments honestly by yourself for full understanding.

I have done the same. In case you stuck in between, You can refer my solutions just for understanding. Don’t just copy paste it.

(These solution might be helpful for you to understand the deep learning in better way…)

I have recently completed that and these are the solutions for the **Coursera: Neural Networks and Deep learning course by** [**Home - deeplearning.ai**](http://deeplearning.ai/) **Assignment Solutions in Python.**

&#x200B;

I have tried to provide **optimized solutions**:

1. Logistic Regression with a Neural Network mindset:  
[Coursera: Neural Networks and Deep Learning (Week 2) \[Assignment Solution\] - deeplearning.ai](http://www.apdaga.blogspot.com/2018/09/coursera-neural-networks-and-deep-learning-week-2.html)
2. Planar data classification with one hidden layer:  
[Coursera: Neural Networks and Deep Learning (Week 3) \[Assignment Solution\] - deeplearning.ai](http://www.apdaga.blogspot.com/2018/10/coursera-neural-networks-and-deep-learning-week-3.html)
3. Building your Deep Neural Network: Step by Step:  
[Coursera: Neural Networks and Deep Learning (Week 4A) \[Assignment Solution\] - deeplearning.ai](http://www.apdaga.blogspot.com/2018/10/coursera-neural-networks-and-deep-learning-week-4A.html)
4. Deep Neural Network for Image Classification: Application:  
[Coursera: Neural Networks and Deep Learning (Week 4B) \[Assignment Solution\] - deeplearning.ai](http://www.apdaga.blogspot.com/2018/10/coursera-neural-networks-and-deep-learning-week-4B.html)

Thanks,- Akshay P Daga",1539777942.0
Tanren,"Don't be afraid of Octave, the things you need to know for the course can be easily learned in 1-2 h its a non issue.",1539355879.0
pieIX,"Consider using AWS and pick a laptop that’s comfortable for coding. Something well built with a decent keyboard, like an older MacBook pro or a thinkpad. ",1539239395.0
MumbleBeeX,"I would suggest buying a PC with As much better hardware as possible and SSH into this machine to run your code.
Buy a cheap Laptop that can code well as in the other answer. I use a Thinkpad and its awesome for coding.
I use Jupyter Notebook hosted on my Machine, and Port Forwarding with SSH from my laptop so I can access the Notebook in my browser.",1539294049.0
grimfada,"The book is really good and thorough - we went through it in our doctoral studies course during last spring. What is however extremely useful is an intuitive and hands-on resource accompanied to match to the book's subjects. Personally I'd recommend checking SuperDataScience's Deep Learning A-Z MOOC at Udemy, as they go through the workings of distinct algorithms in an intuitive manner. While the course is listed at around 200€, its available at 10€ from time to time.",1539234798.0
k9thedog,"Hello Kate! Than you for your interesting talk at ECCV!

I've been working on very domain-speciffic image recognition and object detection models. For high accuracy, we need to train on the images from the target domain, but sometimes when applied in the real world,, they fail miserably when given an image out of the domain. For example, a model trained on vegetables will see broccolli when presented an image of trees. 

When it comes to computer vision for real-world problems, do you think domain-speciffic image models are the future, or maybe models that generalize well to different tasks?",1539165182.0
killtheperfect,"Out of curiosity, why don't you take online services like Google Cloud platform or Amazon web services? They can show you the exact amount you will get billed for and also they are no devops - you don't need to maintain servers. ",1539072306.0
Karyo_Ten,"Don't price on cost, price on the value you bring.",1539075434.0
HugoWagner,IMO Google/AWS is probably so much more efficient at providing the hardware that it will almost certainly be more cost effective to use a cloud service than to mayyybe get cheaper cost all together on the hardware but then spend way more time debugging and setting it up. The cloud services have already done that work and they are charging bulk prices basically. It's like trying to start your own orchard to make cider vs just buying the apples in bulk from an already established farmer. The second one is almost certainly more profitable for years and years.,1539102254.0
arachnivore,SLI is just for gaming. It doesn't help with deep learning. Your computer can use multiple cards without it.,1539058203.0
onkelFungus,"FYI: here are some starting points if you want to utilize multiple GPUs for your training.

* [https://github.com/tmulc18/Distributed-TensorFlow-Guide](https://github.com/tmulc18/Distributed-TensorFlow-Guide)
* [https://github.com/uber/horovod](https://github.com/uber/horovod)
* [http://www.cs.toronto.edu/\~ranzato/publications/DistBeliefNIPS2012\_withAppendix.pdf](http://www.cs.toronto.edu/~ranzato/publications/DistBeliefNIPS2012_withAppendix.pdf)",1539163831.0
spmallick,"Note: If you have a machine with a GPU, you are much better off using the original OpenPose repo. In case, you want to try it in your OpenCV application using a CPU, this post / code will help. 

Post

[https://www.learnopencv.com/hand-keypoint-detection-using-deep-learning-and-opencv/](https://www.learnopencv.com/hand-keypoint-detection-using-deep-learning-and-opencv/)

Code

[https://github.com/spmallick/learnopencv/tree/master/HandPose](https://github.com/spmallick/learnopencv/tree/master/HandPose)

&#x200B;

&#x200B;",1539024540.0
Manto1,"Maybe OpenAI Five https://blog.openai.com/openai-five/ ? Considering they use 128,000 CPU cores and 256 P100 GPUs and train for at least weeks (couldn't find exact info of training time).",1539190865.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/u_seuyou123] [Deep Contextualized Word Representations With ELMo from AI2](https://www.reddit.com/r/u_seuyou123/comments/9mk4yn/deep_contextualized_word_representations_with/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1539042645.0
initz3r0,"It gets rid of the vanishing gradient problem because it's derivative is exactly 1 (or 0 for x<=0), so you can propagate the error effectively to the early layers of a network",1539002844.0
Cunic,It's simple and often as effective as a more complicated function.,1538999436.0
henrietteyoungmc,"Accordingly to the Book „Deep learning an neural networks“ it is based on observations. But there is no such thing as a proof that ReLU is the superior activation function. 

So in specific cases tanh or sigmoid can still be better or similar.

But reLU is the easiest so you can roll with that, because there are no downsides known.

That’s how far I understand literature and recent papers.",1539005015.0
Karyo_Ten,"- You need a non-linear function
- It must be derivable
- Relu is stupid simple
- It works",1539008244.0
thisismyfavoritename,"Adding to all the other good answers, I think the major reason is that it was proven empirically to perform better on some image classification tasks.",1539009291.0
CookieTheSlayer,"If you have any machine with a gaming GPU from the last 5 years, it's probably fine and a decent way to experiment. If not, paperspace is one of the cheaper options, yeah. But if you plan on dedicating a lot of time into this (career?) It's probably cheapest to buy your own hardware",1538988354.0
iceporter,"check vast.ai its also cheap with better rig choices
oh and also vectordash.com",1539073516.0
yashkumaratri,Ya Used AWS and other as well but found paperspace cheaper and better you can get 10$ in credit using my code [https://www.paperspace.com/&R=PV7HABL](https://www.paperspace.com/&R=PV7HABL),1542173258.0
noitq,"I used Paperspace for gaming purpose. It is good and cheep. Btw, if someone register new, use my promotion code to get 10$   
\- code: E5269WJ  

\- or link: [https://www.paperspace.com/&R=E5269WJ](https://www.paperspace.com/&R=E5269WJ)

thanks :D",1538990142.0
dan994,Edinburgh is very good for AI and does a computer science/AI course I believe,1538943212.0
buzz_aldi,"Might start by looking into current uses such as:  
1. Automatically grading student essays

2. Identifying student areas of weakness and creating a personalized plan or test to help the student improve

3. Language learning using NLP (Liulishuo for example)

I would also encourage you not to look for solutions using AI but rather start by looking at what problems teachers and students face in the industry and then look for the best solutions to those problems.

&#x200B;",1539025064.0
Xorlium,"It's regression. You want to guess a numeric attribute and the closer you are to the truth, the better. ",1538793739.0
jorgemf,It can be either of them. You can decide the model has one output which is numeric and that number is the age. Or you can have several outputs where each represents one year or a period. In that case it is classification. ,1538790143.0
mdv1973,"As pointed out by jorgemf, you can do either.

Which you choose (can do both) depends on what you want to achieve with the predictions and how much training data you have (among other things).

The main advantage of the classification approach is that you can get a probability for each age (or age range), so you can do all sorts of things like finding the three most probable ages/ranges, express a level of uncertainty about the predictions, get possibly more accurate aggregate statistics, etc.

Regression, on the other hand, has the advantage of being able to predict a continuous variable, so it could it give you a prediction of the age as 25.16. I guess for ages this makes less sense, since this is usually treated as a discrete variable. Now, if you need the age (as discrete value), and the number of possible values is large (could be 120 in this case), then regression should be faster than classification, and require less training data.

&#x200B;",1538809229.0
sidharth_k,I would suggest renting a virtual instance on AWS etc. or a bare metal server in the cloud and avoid dumping 8k on a machine. But you’ve probably thought of that...,1538817974.0
tlkh,"Side note, multiple 1080Ti is likely to be a lot more cost effective compared to 2080Ti. For 8k USD (?) you might be able to get a 4x 1080Ti setup. ",1538845398.0
Inori,"If you don't want to do a custom build then the next best thing is to go for cloud solutions.  
Definitely do not recommend just dropping $8k on a pre-built setup.",1538819722.0
FrimannKjerulf,Would r/machinelearning maybe be a better place for this question? Are reposts in different subreddits allowed?,1538817178.0
FrimannKjerulf,"So custom building á DL workstation isn't that much of a hassle? I've built many machines before, and worked quite allot with Linux so I should handle it. The biggest problem I see at the moment would be selecting the best hardware thats going to give me the least amount of post-installation problems. Haven't been keep up with the hardware world since I moves to a Mac. Anyone know of a good parts líst for a build, or á good place to find one?",1538865427.0
fc560,"Check couple more companies:

[https://www.sabrepc.com/Deep-Learning-Solutions](https://www.sabrepc.com/Deep-Learning-Solutions)

[https://bizon-tech.com/us/workstations/deeplearning/bizon-g3000.html](https://bizon-tech.com/us/workstations/deeplearning/bizon-g3000.html)

I would prefer to invest in to my own pre-built workstation instead of cloud.",1540220079.0
abhishady,There's another brand called [ANT PC](https://www.ant-pc.com) ,1540970569.0
hswerdfe,"while not youtube NIPS 2017 videos are online.

[https://nips.cc/Conferences/2017/Videos](https://nips.cc/Conferences/2017/Videos) 

as well MILA summer school lectures are online

[http://videolectures.net/deeplearning2017\_montreal/](http://videolectures.net/deeplearning2017_montreal/) 

Arxiv Insights are great

[https://www.youtube.com/channel/UCNIkB2IeJ-6AmZv7bQ1oBYg](https://www.youtube.com/channel/UCNIkB2IeJ-6AmZv7bQ1oBYg)

Siraj Raval is another one 

[https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A](https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A)

[Fast.a](https://Fast.ai)i is yet another good one

[https://www.youtube.com/watch?v=Th\_ckFbc6bI&vl=en](https://www.youtube.com/watch?v=Th_ckFbc6bI&vl=en)
",1538830342.0
AngusOfPeace,How’s it better than Hyperas? Also does it work in Jupyter notebook?,1538772274.0
shaggorama,"That quote was referring to the course, not the library.",1538779574.0
OPLinux,"Compute times depend heavily on the machine you use, so without more information you probably won't get an adequate answer.  
A metric that is independent of the machine, is the number of flops or the number of madds. I am pretty sure you can find those online in some kind of comparison paper. (Iirc, the original yolo papers also provide some kind of metric, but I don't think it is per layer)

Lastly, yolo9000 is the paper of yolov2, so which one do you want?",1538844947.0
anilmaddala,"Which model are you trying to use? Are you doing classification of images? 

The input to the model is of constant size and is defined by the model. So if your using an existing model, YES you need to resize the image as per the requirements of the model.",1538722644.0
eric01300,"The input to the model does not have to be constant, but for training efficiency, it is good to keep the size same.",1538722956.0
louistoot,"input\_size is the number of features of the input vector x. Let's say you use temperature and precipitation as features, you put them into one vector of size (seq\_length (number of 't's), batch\_size, input\_size), where input\_size is 2. hidden\_size is the number of units of your LSTM cell. This means all the layers (input, forget, etc.) will have this size. num\_layers is for stacking multiple LSTMs (not shown in colan's blog). Once you created the LSTM layer in pytorch, it is flexible to take input of varying seq\_length and batch\_size, you do not specify this at layer definition.

The LSTM outputs (output, h\_n, c\_n): output is a tensor containing the hidden states h0, h1, h2, etc. h\_n is the last hidden states (just the final ones of the sequence). For a LSTM with 2 layers, h\_n will contain the final hidden state of both layers. c\_n is the same as h\_n but the cell states.",1538722233.0
grimfada,"Let's first start with the input. The documentation says that the input must be in the shape `(seq_len, batch, input_size)`. For sequences of 12 observations with 10 features each wrapped in a batch of 5 sequences the input to the model should have shape `(12,5,10)`. The ten refers to distinct features for a single time step, 12 refers to number of time steps per sequence and the five tells the model how many sequences to expect to find within a single input batch of observations.

As for the size of the hidden state with the PyTorch LSTMs, the size refers to the size of the simple Neural Networks acting as learnable gates (input, forget, etc). So defining the `hidden_state=3` would result in the LSTM having three learnable parameters per gate. You are able to see this for yourself by initializing a `torch.nn.LSTM` yourself and checking the parameters of the model. You'll quickly see that there are `4*hidden_state` parameters for crucial components.

As for the output you'll have freedom to pick yourself. If you want many-to-one LSTM, you'll just pick the last output given by the LSTM after it has seen a single input sequence of observations.",1538732026.0
ballgame75,"Those are just variable names, but input_size probably means the length of the input vector, hidden_size probably means the number of  neurons in your hidden layers and hidden_state couple be a few different things, depending on the context. Did you read their documentation? In their documentation hidden state is the state of your hidden variables or your lstm cell.",1538703573.0
Goron97,"Dear fellow scholars,

&#x200B;

we are working on a method that automatically estimates an age of a speaker from audio, i.e. without seeing the speaker. We did not find any study presenting how accurate people are in their estimates. So, we would like to know a human baseline. If you have 5 minutes of your time, please kindly fill in the simple form in the link above.

&#x200B;

There are 20 short recordings you can play freely, then guess how old the speaker is. The questionnaire is anonymous, but you can leave your email if you are interested in early results of the study. We will also appreciate if you distribute our survey to your friends.

&#x200B;

Thank you very much for your help.",1538684296.0
,[deleted],1538713797.0
wlorenz65,"Rita Singh claims that her program is able to extract 3d face information, health state, and income level from analyzing just a human voice: https://www.youtube.com/watch?v=Ls1_tqlpMww&t=6m50s",1538879070.0
eption_,"So, this ~~seo-guru~~idiot thinks that this post will actually help his company. smh",1538667681.0
viccpopa,"There are companies using transfer learning with pretrained models like VGG16 or Inception-V3. In fact, if you get the models from Keras, there are not many restrictions with its usage: 

https://github.com/keras-team/keras/blob/master/LICENSE",1538652771.0
nafestw,"From [http://image-net.org/download-faq](http://image-net.org/download-faq):

>Researcher shall use the Database only for non-commercial research and educational purposes.

Also, the images in the ImageNet dataset may be copyrighted as well

>The images in their original resolutions may be subject to copyright, so we do not make them publicly available on our server.

&#x200B;",1538734253.0
k9thedog,"Looking at the docs (https://keras.io/applications/#vgg16):

> input_shape: optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (224, 224, 3) (with 'channels_last' data format) or (3, 224, 224) (with 'channels_first' data format). It should have exactly 3 inputs channels, and **width and height should be no smaller than 32. E.g.  (200, 200, 3) would be one valid value**.
",1538642781.0
anilmaddala,Which backend are you using with Keras? Tensorflow or Theano?,1538639942.0
daviziiin,"I'm not sure why it states 48x48, according to the documentation and source code it should be a 32 pixels limit, which is due to the architecture having 5 MaxPool operations, reducing a 32x32 input to a 1x1 (assuming zero-padding and strides to maintain dimensions).

Source:[https://github.com/keras-team/keras-applications/blob/master/keras\_applications/vgg16.py](https://github.com/keras-team/keras-applications/blob/master/keras_applications/vgg16.py)

&#x200B;

The same will happen to other nets as it is very rare to train deep models with small (or rather weird shaped) images. I'm afraid the only way to use those networks is to change your input data.",1538653190.0
jpcoseco,"Vgg is trained to process rgb values your inputs are with color dimension 1 (grayscale). Maybe if you use the vgg preprocessing before it can help (it's a keras.vgg19 module i think)
Pd: what are you processing? Are you sure that is vgg what you need?

",1538652671.0
LoveOfProfit,"Mostly looks good, couple notes:

1) 2700x vs 8700k is a toss up. I'd save the money and go for the 2700x. 

2) 1080 Ti is the place to be if money concerns are a reality.

3) I'd look at a 500gb+ ssd personally. You can make 250gb work, sure, but if you'll be installing any games on there, you'll eat it up fast. Heck, keep the 250gb one you have and get a secondary 500gb non-m.2 ssd.",1538599595.0
shaggorama,"""Please, steal my work/data/coins.""",1538610943.0
soheil_zabihi,"I think it is L2 norm, and they should just wrote 2. I mean ||\*\*.\*\*||2",1538578764.0
tkchris93,"I'm almost positive that notation is incorrect. Is FR(x) a matrix or a vector? That'll change what the norm is. (Induced norm vs p-norm)

Willing to bet the author got confused between L2 regularization and the 2-norm.",1538579739.0
Karyo_Ten,Tesseract :P,1538592957.0
vlatheimpaler,"Tesseract is supposed to be tunable, but I haven't invested the time to experiment with that yet. My first experience with Tesseract was similar to yours though.",1538596841.0
Belenoi,"Honestly, the best is Google Vision. But it's very expensive to use past the first free queries. ",1538601718.0
Vankir,"You have a very limited options. Tesseract is the only free OCR. There are a number of comercial although: Nuance OCR, Abbyy OCR, TOCR. They do some image preprocessing and Abbyy even ask to pass them image as is without enhancements because they know better how to improve it. So They should produce better quality. ",1538633143.0
teamrework,"Not sure about a group, but you might be interested in this video interview from Ian Goodfellow from the Deep Learning Summit in San Francisco earlier this year. He'll also be speaking at next year's edition of the event in January. [http://videos.re-work.co/videos/774-interview-with-ian-goodfellow-google-brain](http://videos.re-work.co/videos/774-interview-with-ian-goodfellow-google-brain) ",1538567922.0
affinitive2,The code is open-source!,1538538857.0
lanemik,"If you end up having to pay, checkout [https://vectordash.com/](https://vectordash.com/)

&#x200B;",1538545129.0
erikqu,If you're enrolled at an academic institution you could probably bug someone in the computer science department about letting you get access to a GPU. Otherwise I think the only solution is to either rent time using AWS or just buying one.,1538540472.0
immortal333,if you're reaching out of 12gb ram in google colab. Not sure that anybody as company willing to give that much amount for free. anyway ask professor in cs department if they can give you access to gpu. ,1538563601.0
mebpin,I have just started to use google colab for few weeks. I am not aware about other platforms ,1538571875.0
rslimphase,Google colab offers free Tesla K80 GPU,1538598407.0
kailashahirwar12,"If it comes with less Trade-offs, it will be awesome.",1538559031.0
pcidev,Stack Pointer Networks for Dependency Parser ,1538453229.0
Vankir,Holistically-Nested Edge Detection,1538460375.0
radarsat1,used,1538477689.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/deeplearningpapers] [What do you mean by ""..where memory cells\/recurrent components are employed.""?](https://www.reddit.com/r/DeepLearningPapers/comments/9kmess/what_do_you_mean_by_where_memory_cellsrecurrent/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1538443753.0
_the_ant,"Have a look at this:

&#x200B;

[https://github.com/dawei6875797/Face-Aging-with-Identity-Preserved-Conditional-Generative-Adversarial-Networks](https://github.com/dawei6875797/Face-Aging-with-Identity-Preserved-Conditional-Generative-Adversarial-Networks)",1542336231.0
spmallick,"Post

[https://www.learnopencv.com/deep-learning-based-object-detection-and-instance-segmentation-using-mask-r-cnn-in-opencv-python-c/](https://www.learnopencv.com/deep-learning-based-object-detection-and-instance-segmentation-using-mask-r-cnn-in-opencv-python-c/)

Code

[https://github.com/spmallick/learnopencv/tree/master/Mask-RCNN](https://github.com/spmallick/learnopencv/tree/master/Mask-RCNN)",1538415536.0
vreten,"Thanks for such a great post, really enjoyed reading and following along. ",1538454126.0
anilmaddala,"If my understanding is right, you want to detect boxes independent of their color and labels on them. Correct? 

I am not sure if the current detectors would work as they rely heavily on visual featurs available on the object and not on the 3d shape of the object.

May be you can do combination of point cloud + detection?",1538371389.0
Cunic,"For PyTorch, basically the only preprocessing you **need** to do is to reshape your time series into [timesteps, instances, variables] (assuming you are not changing it to batch_first=True when you initialize your RNN).

If you have successfully applied an LSTM on your synthetic data, what is the problem with your real-world dataset? Your forward() method should not need to change at all.",1538341561.0
TheDailySpank,"I'm just going to put it out there... Like perpetual motion, if it were possible, it would have been done by now. But don't let that discourage you from learning how it doesn't work. Sometimes that's the best way to better understand something.",1538362066.0
Abhishek_Advance,LsTm are the way to go,1538339101.0
Mr-Yellow,"Predict random data?

/r/algotrading ",1538342392.0
water-and-fire,"SSH with port forwarding

See the instructions from a similar StackOverflow post 


https://stackoverflow.com/questions/38789359/debug-port-forwarding-for-using-jupyter-notebook-remotely",1538318662.0
mugbrushteeth,"This link might be helpful.  It is about setting up GPU in google cloud ubuntu vm. At the end it explains how to set up jupyter notebook too. 

https://hackernoon.com/launch-a-gpu-backed-google-compute-engine-instance-and-setup-tensorflow-keras-and-jupyter-902369ed5272
",1538319674.0
Karyo_Ten,The image can also be applied to supervised learning. ,1538295992.0
onclick360,"Read this article for more details :-

[https://onclick360.com/unsupervised-machine-learning/](https://onclick360.com/unsupervised-machine-learning/)",1538288478.0
anilmaddala,Generative Adversarial Networks +/ Variational Autoencoder + Neural Style transfer is one way to approach. Read up on these techniques to get an idea of what is possible with the current state of the art and you can mix them together to obtain the desired results.,1538286000.0
ResidualRisk,"As the other commenter said, GANs sound like the right approach for the problem. should be easy to find some examples for Tensorflow using the MNIST data set to start you off. Note that GANs are a pain in the ass to tune, so expect to lose some time there. ",1538296364.0
Perseus784,[Here](https://medium.com/@ipaar3/how-i-built-a-convolutional-image-classifier-using-tensorflow-from-scratch-f852c34e1c95) is little something that I have written for kick starting. Might be useful for learning deep learning as well as Tensorflow basics. ,1538303809.0
Cunic,"Ambitious first project, good luck out there! If you get good results, post back here, it would be very cool to see what comes out.

GANs are certainly one way to solve the problem, but just as a warning, they are particularly tricky to tune and will likely take a very long time to learn about before you are satisfied. Though there will be tutorials that can help bypass some of the deeper understanding.",1538341864.0
practicalutilitarian,"Typo. Probably should say ""Eq 1.5"" or some such. Look for nearby equations in the text.",1538284439.0
Cunic,"Does letting it train for longer help? Ideally, it shouldn't be a problem if you're retraining the variables with enough time for them to change enough.

Also what are the actual symptoms of the problem? Is accuracy just lower than expected and you're attributing it to this part?",1538342093.0
1991viet,Github repo: [https://github.com/1991viet/Hierarchical-attention-networks-pytorch](https://github.com/1991viet/Hierarchical-attention-networks-pytorch),1538215641.0
anti-gif-bot,"[mp4 link](https://g.redditmedia.com/6WdEq-TiySM1sJ8TR0vAdjuEgYch44zafinX5vjeiwU.gif?fm=mp4&mp4-fragmented=false&s=ab067717458cd7088161352f8df79fe5)

---
This mp4 version is 69.71% smaller than the gif (2.8 MB vs 9.24 MB).  


---
*Beep, I'm a bot.* [FAQ](https://np.reddit.com/r/anti_gif_bot/wiki/index) | [author](https://np.reddit.com/message/compose?to=MrWasdennnoch) | [source](https://github.com/wasdennnoch/reddit-anti-gif-bot) | v1.1.2",1538215668.0
Hsankesara,"Hey there is a good article on Hierarchical attention networks on Medium. 

“Hierarchical Attention Networks” @heetsankesara3 https://medium.com/analytics-vidhya/hierarchical-attention-networks-d220318cf87e",1538271635.0
Gabrer,"Have someone tried to reproduce the experiments and to get the same accuracy? How much does the prefiltering of the datasets affect the final performance?

The dataset can be downloaded from the Duyu Tang's homepage. [http://ir.hit.edu.cn/\~dytang/paper/emnlp2015/emnlp-2015-data.7z](http://ir.hit.edu.cn/~dytang/paper/emnlp2015/emnlp-2015-data.7z)",1541690648.0
1991viet,Github repo: [https://github.com/1991viet/Hierarchical-attention-networks-pytorch](https://github.com/1991viet/Hierarchical-attention-networks-pytorch),1538215499.0
anti-gif-bot,"[mp4 link](https://g.redditmedia.com/lTXbVXz-UVrIgWaJXCHi1PII0a4rf93hvyj8m1sxHIc.gif?fm=mp4&mp4-fragmented=false&s=b67bf1ffda9370d9842c0239124a22e0)

---
This mp4 version is 69.71% smaller than the gif (2.8 MB vs 9.24 MB).  


---
*Beep, I'm a bot.* [FAQ](https://np.reddit.com/r/anti_gif_bot/wiki/index) | [author](https://np.reddit.com/message/compose?to=MrWasdennnoch) | [source](https://github.com/wasdennnoch/reddit-anti-gif-bot) | v1.1.2",1538215539.0
putrakopo,I think you can get this on [https://wqu.org/](https://wqu.org/),1538233531.0
planmythings,"I am enrolled in both AI for trading and WQU. Both are pretty different courses with different outcomes and terms.

You can find projects which are taught at AI for trading here for free : [https://github.com/udacity/artificial-intelligence-for-trading](https://github.com/udacity/artificial-intelligence-for-trading)

For questions, PM me. ",1540750268.0
PandaDepress,"I dont know if you need a wireless card... you could connect directly with an ethernet cord. Also, I would want 1x 2080 ti over 2x 1080 (non ti). ",1538243023.0
abhishady,"The major problem with your build is the Power supply, which is very very less considering your workflows, I would suggest you to atleast have 850W PSU for 2 X 1080 config. My [Ant PC](https://www.ant-pc.com) workstation with 2 X 1080 Ti has V1000W Gold Certified PSU, The processor Im using is the same..",1539076138.0
SamStringTheory,There is a lot of work into this. Look up Bayesian neural networks. A previous discussion is here: https://www.reddit.com/r/MachineLearning/comments/6eh197/d_methods_for_uncertainties_in_neural_networks/,1538174749.0
sabalaba,"**TL;DR**

The RTX 2080 Ti’s single-precision (FP32) training of CNNs with TensorFlow is between 27% and 45% faster than the 1080 Ti for measured networks.

The RTX 2080 Ti’s half-precision (FP16) training of CNNs with TensorFlow is between 60% and 65% faster than the 1080 Ti for measured networks.

If you do FP16 training, the RTX 2080 Ti is probably worth the extra money. If you don't, then you'll need to consider whether a 71% increase in cost is worth an average of 36% increase in performance.",1538155549.0
ankudini,"Interesting. Even though there was no ground for that, I had higher expectations for the new card.",1538155826.0
fantom64,"I have a Titan V, how much ""more"" am I getting compared to the 2080 Ti (price-wise) ?",1538210927.0
Smallpaul,Why is this subreddit the appropriate place for this announcement? Seems like more of a Web language.,1538151084.0
louistoot,Pass the output through a sigmoid function (squeeze to 0-1) and multiply by 80 would be an option...,1538606722.0
Karyo_Ten,"Are you sure the NVLink on RTX pools memory?

https://www.reddit.com/r/deeplearning/comments/9jcyln/hardware_nvlink_on_geforce_rtx_does_it_pool_memory/

I wouldn't be surprised if Nvidia prevents that on GeForce and says ""use Quadro/Tesla""",1538126191.0
antiquemule,"A long read, but after finishing that you are a pretty smart cookie, as far as I can tell. Mindblowing stuff comparing current desktop supercomputers to Top500 supercomputers of the past. Spoiler alert: we're catching up fast.",1538127521.0
ScotchMonk,"is it too early to discuss,  or even consider  [Intel Nervana](https://ai.intel.com/intel-nervana-neural-network-processor-architecture-update/)? Anyone has more details about this chip and general availability?",1538137214.0
sabalaba,Here are some actual hardware benchmarks for the 2080 Ti: [https://lambdalabs.com/blog/2080-ti-deep-learning-benchmarks/](https://lambdalabs.com/blog/2080-ti-deep-learning-benchmarks/),1538155494.0
onclick360,Nice Article for RL,1538119829.0
theoneandonlypatriot,What the hell is that stock photo? Lmao ,1538140839.0
GtothePtotheN,...yet,1538091011.0
dkimpacta,"I recall Linus saying the NVLink utilises sli on the 2080 series 

This may also confirm it for you

https://mobile.twitter.com/nvidiageforce/status/1032048061785231361?lang=en

But I’m finding it hard locating the tech documentation for it",1538507955.0
ziakhan21,"No, RTX card doesn't  Pool memory. 

&#x200B;

refer this article as how NVlink differ with RTX and Quadro card : [https://www.pugetsystems.com/labs/articles/NVIDIA-GeForce-RTX-2080-2080-Ti-Do-NOT-Support-Full-NVLink-1253/](https://www.pugetsystems.com/labs/articles/NVIDIA-GeForce-RTX-2080-2080-Ti-Do-NOT-Support-Full-NVLink-1253/)",1538803628.0
heuristic_al,"If you plan to use multiple GPU's to do deep learning, you'll probably be able to train with larger batch sizes even without nvlink. It depends on what you are using the memory for. For convolutional networks, storing the activations for back prop takes far more memory than the parameters, and using data-parallelism, you can spread those activations across the GPU's.

If you are doing recurrent NN's, you might have the problem where you would like to have more parameters than can fit on a single GPU. For this, people often distribute their network accross multiple gpus. One GPU does the first part of the computation, then the second GPU takes over etc. This can be slow if the system isn't set up to process the next batch on the first GPU while the second is processing the first batch, but that setup is doable even without nvlink. I could see ""memory sharing"" making this setup easier.

However, my understanding is that nvlink isn't about memory sharing. It's just a fast connection between GPU's, which is useful when training on 4 or more GPUs.",1539720709.0
heuristic_al,"Are you already doing FP16 training? In addition to being faster (especially when using Volta or Turing), half precision training effectively doubles the number of activations you can store in memory.",1539720854.0
dkimpacta,But the question is can someone confirm whether RTX 2080s utiliser memory pooling,1539762401.0
gabegm,I would say you should learn how to walk before you run. You need to understand Machine Learning well if you want to do Deep Learning research.,1538040422.0
thisismyfavoritename,Maybe if you study machine learning deep enough you'll learn deeply and become a very deep deep learning researcher,1538049577.0
AdityaKhnn,"Study deep learning, and if you keep at it long enough, machine learning will keep surfacing and familiarising you with itself. ",1538067296.0
sanchit2843,I think you should wait to see actual deep learning benchmarks to come out rather than rushing to buy new card. Use cloud services like vast ai till then. Vast ai is cheap and provide 1080 ti. ,1538039137.0
This_Is_The_End,"If you do it just for the school get a 1080ti with 11GB memory. The latter is important to train large models. If you do it because you are addicted, get a Quadro RTX 6000 with specialized Tensor Cores. 

The GTX1080ti is very good for the price. If your case is cramped and the airflow poor you should consider a Founders Edition which is blowing the hot air out of the case. Otherwise it's likely the card will suck the warm air in for cooling. I made this poor experience with two 1080ti from EVGA. Even there was a lot of space between them, one card was always 10C warmer than the other. ",1538039440.0
teamphy6,"How big are your models?  Can they fit in 11 or 12GB?  Do you see benefits in training multiple models at once on separate GPUs?  Do you need single precision operations or will half precision work well enough?

Can your library of choice + your algorithm take advantage of the tensor core hardware in addition to the CUDA cores?

Depending on your answer to these I think the 2080ti or multiple (i.e. 2 to 4) used 1080ti at \~$500ea on ebay is the sweet spot.  Being able to use half precision and double your processing power is what makes the 2080ti/Quadros/Titans/Vegas shine, however the higher end Nvidia cards have the capability for much larger models in RAM, and the AMDs are of course crippled by lack of popular ML software support. 

&#x200B;

IMO the 2080ti and 1080ti are a good stepping stone on your way to a \~$6000 Quadro card.   

[https://www.anandtech.com/show/13217/nvidia-announces-turing-powered-quadro-rtx-family](https://www.anandtech.com/show/13217/nvidia-announces-turing-powered-quadro-rtx-family)

&#x200B;",1538052703.0
sabalaba,"Here are the actual 2080 Ti benchmarks: [https://lambdalabs.com/blog/2080-ti-deep-learning-benchmarks/](https://lambdalabs.com/blog/2080-ti-deep-learning-benchmarks/)

**TL;DR**

* The RTX 2080 Ti’s single-precision (FP32) training of CNNs with TensorFlow is between 27% and 45% faster than the 1080 Ti for measured networks.
* The RTX 2080 Ti’s half-precision (FP16) training of CNNs with TensorFlow is between 60% and 65% faster than the 1080 Ti for measured networks.
* If you do FP16 training, the RTX 2080 Ti is probably worth the extra money. If you don't, then you'll need to consider whether a 71% increase in cost is worth an average of 36% increase in performance.",1538155532.0
Karyo_Ten,"Use a region of interest pooling layer.

Also be sure to check medical segmentation techniques such as Unet, healthcare is probably the leading use case of bounding boxes and RoI.",1538036955.0
ScotchMonk,"I am not sure what DL models exactly those phone's implemented, but here's some references:

FasNet with code and paper.  [https://github.com/OeslleLucena/FASNet](https://github.com/OeslleLucena/FASNet)

LiveNet and paper([link](http://www.ee.cityu.edu.hk/~lmpo/publications/2018_ESA_LiveNet.pdf)): [https://github.com/houliang428/CNN-for-face-anti-spoofing](https://github.com/houliang428/CNN-for-face-anti-spoofing)

Demo on webcam, but no code: [https://www.youtube.com/watch?v=WQMrTl-erVk](https://www.youtube.com/watch?v=WQMrTl-erVk) : paper: [(link)](http://www.cse.msu.edu/rgroups/biometrics/Publications/Face/WenHanJain_FaceSpoofDetection_TIFS15.pdf)",1538004923.0
Mr-Yellow,Through the power of gimmick marketing ;-D,1537999835.0
ScotchMonk,Anyone have a S9 phone and tried fooling it with a selfie or a video of themselves(replay attack)? ,1538005999.0
,"Basically, at some point someone took the time to train a neural network to look at an image and detect if it has a face or not. The way you should read that statement is that ""someone created a function that takes an input of the pixel data of the image, and tuned its parameters in a programatic way such that certain texture transitions in certain regions will cause that function to output true if its a picture of a face"" 

More complex feature extraction and fake detection is just basically doing the same stuff with functions that just output more complex data, rather than just true and false. ",1537990563.0
Karyo_Ten,"IMHO, the future is training new talents because the field is very fast compared to the talent pool.

Also we will probably have more and more cross-over between genetic and bandit algorithms with deep learning for reinforcement learning/autonomous agents I guess.",1537980558.0
PSousa93,"Hi, 

&#x200B;

How are you training the algorithm? Even for the training and validation, both segmentation and output needs to be binary.

Usually, layers like softmax classifiers are used at the end of the network in order to have an output according to each class. In this case you will have two classes.

Btw, Precision and Recall are not metrics for measuring the performance of a segmentation, you should use metrics like dice coefficient, Hausdorff distance, or others that will have in mind the performance of each segmentation.",1537982207.0
the_real_jb,"The last layer of a binary UNet probably has a sigmoidal activation function, which means that the threshold is 0.5. 

If you're not sure, you should definitely dig into the model to see what the activation in the final layers are. Sometimes loss functions implicitly apply an activation, like [PyTorch's cross entropy loss](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss) which applies softmax to the final layers before taking the loss. In that case, in eval mode, you would need to apply the softmax function yourself.

In general, if you don't want to dig into your model at all, you should just go to your validation set (neither train nor test set) and look at your metric as a function of threshold. Here's a python code sample, based on PyTorch syntax, without any of the functions that really would make it work (i.e. modeling, calculating various metrics). 

     thresholds = np.linspace(0,1,100)
     metrics = []
     for threshold in thresholds:
        for (inputs, labels) in dataloader: 
            outputs = model(inputs)
            predictions = outputs > threshold
            metric = calculate_metrics(predictions, labels)
            metrics.append(metric)

    plt.figure()
    plt.plot(thresholds, np.array(metrics))
    plt.show()

Pick the value that maximizes your validation metric, then apply that to your test set.",1537984863.0
pcidev,"Help me build this community !!!

Try to find best materials and post on [r/NLP\_readingGroup](https://www.reddit.com/r/NLP_readingGroup/) .

Hope you help :)",1537976324.0
PandaDepress,"If this was your work, great job. Personally though, I would have tried a few things different:

1) data validation: instead of having 7 people look at the same image of an engine, and requiring 7 to be the same, I probably would have done 2 and if there was conflict, the image would be ignored.

2) I would have tried to infuse my own simulated banners. I am not sure how long it would take for people to find way past the filter but I would guess simple things like the vendors changing their font can have an effect. ",1538258793.0
lukasmach,"> to gauge framework usage, interest, and popularity

""They were telling us what to wear, how to dress and even which clothes to put on.""",1537936798.0
antiquemule,"Maybe I'm missing something here, but it seems silly to compare backends (Tensorflow) with API's (Keras) that run on top of them. Keras runs seamlessly on R, so that gives it a lot of presence.",1537946602.0
zakamas,You can use Flask or some other python web frameworks to use your model via web.,1537807344.0
kailashahirwar12,"Yes.

There are many tutorials available on Github.

&#x200B;

This Github repo can help you.

[https://github.com/elliebirbeck/model-deployment-flask](https://github.com/elliebirbeck/model-deployment-flask)

&#x200B;",1537807727.0
mfdl,"If you need to do it at scale, Amazon sage maker now supports pytorch.",1537888332.0
Cunic,Automatic video transcription?,1537799980.0
thiernodiop,Well the first problem i see is that parsing Chinese text can be challenging since there is no symbol to separate words. For example in English you have the space to separate words. ,1537861288.0
Cunic,"My first impression: If your goal is **lower false-negative rate**, why jump to RL? There may be an RL solution to the same problem, but it seems like you would just try again with a new solution and hope that it ends up solving this first problem. Your solution doesn't seem to match your problem.

If you are just interested in seeing how an RL solution performs, spam-filtering is a classic RL application example (state=document, action=pick spam/not-spam, reward=accuracy or users moving it around). There are of course other ways to define the problem.",1537799893.0
Karyo_Ten,"Start with Naive Bayes. That's antispam ""Hello World"".",1537804656.0
thisismyfavoritename,"RL not the way to go unless you have a way to give rewards to your system, in this case that would be correctly removing spam, which boils down to labelling your data.

Try more advanced classifiers, perhaps ConvNets on top of word embeddings. They have shown good performance at similar tasks, namely sentiment analysis.",1537881428.0
immortal333,"some of your points are not clear to me but still i will try to answer.

first i assume input is x and output is suppose to be y.
x is scaler.
y has multiple 1's.

now use of multilabelbinary is right.

now there are two problems. first is loss function. for training against multiple label binary you need to use binary crossentropy in keras.

second is i suspect that x can be 255*1 vector as input rather than scaler.",1537792762.0
Karyo_Ten,"Use a recommender system but instead of word embedding you have ""image embedding"". ",1537779008.0
E-3_A-0H2_D-0_D-2,"Many possible usages come to mind; summarizing the ones off the top of my head:


 - Use a Siamese n/w style architecture for 1-shot learning. For this, you'll need a face that the user really likes as your anchor image.


 - You can mix it up here - you can merge (concatenate/add/multiply) the image feature representations with word embeddings if a bio is present, and try to pass it through a feed-forward NN or something fancier.


 - You can make 'Conv-attention' like models that might tell you what part of the attributes of a face the user is interested in.


 - As mentioned in the other comment, you can use simple recommender systems with image features.


 - You can even do something as trivial as gather data and perform K-means to cluster all the liked faces together and manually zoom in into 'k' points of interest. You might even, with the positive faces, create a 'summary' image feature representation. You can then use simple Euclidean distance to measure how close the summary image is from the new incoming images.


- If you have a timeline of the images liked, you can even model (and even possibly predict) the user's preferences over time (provided you use the temporal information in the right way).",1537797987.0
trickyLoop,"Wrong sub. This sub is for Deep Learning, which is a sub field of Machine Learning, which is often described to the public at large as artificial intelligence.",1537768373.0
thijser2,"That's going to depend on the used implementation.

Why don;t you try it in whatever framework you are using? ",1537775540.0
liftoff01,"Why not train your model to predict the nth step element you are interested in? 
If you are interested in the 2-step forecast, train your model with the 2-step element in your loss function, instead of the immediate observation (1-step element)...and so forth. I hope that helps. ",1537776442.0
Cunic,"Maybe you're looking for something like seq2seq? Given a sequence, predict a sequence where each new step is generated based on the previously-generated step. Then you could essentially compare your predicted sequence to the real sequence to train it.",1537788758.0
UndeadPandamonium,Link is broken for me does anyone have a mirror ,1537748776.0
Karyo_Ten,I've gathered all my notes about depthwise convolution and use cases in [this Github issue](https://github.com/mratsim/Arraymancer/issues/128).,1537780004.0
italo3d,`model.predict(test_x).argmax(axis=1)` will return the predicted numbers.,1537715190.0
ElectricHobo,"Pretty helpful, I'm just starting on making a custom Classification and Localisation implementation, and had just started to wonder how to tackle the loss function. Just need to figure out how to evaluate the tuning.

",1537802745.0
french-crepe,I enjoyed this blog post http://tkipf.github.io/graph-convolutional-networks/ and it has a link to code.,1537708673.0
troltilla,Look up https://github.com/espnet/espnet its state of the art in end2end neural models for ASR. There's a good chance you won't have enough data to learn the model - another state of the art toolkit for traditional ASR algorithms is https://github.com/kaldi-asr/kaldi but it can be a bit difficult for people new to ASR.,1537983058.0
pcidev,"From what you have written, I got the idea that you want to build a bot that is controlled by voice ( Czech ). The idea is great and may be new in Czech . As a master thesis what you can is to build multiple modal and compare their performance and compare them .",1537700460.0
LearnedVector,"You should look at keyword spotting applications. This can translate speech directly into your classified commands (left, up, right). I’ve had success using just a single GRU to train do just that. It converts audio into MFCC features then use those features to classify  for a command. To get a bigger data set try augmenting the data using background noise from various sources. ",1537724351.0
surajpaib,"I'd found a post on reddit about vast.ai 

Pretty cheap GPU systems. Better specs than Google Colab. Around 0.1$ for a GTX 1080 Ti for an hour. You start with a free dollar in credits so about 10 hours free. It's still pretty cheap to use after that. 

Only issue is data is not secure on host systems. If you're prototyping and playing around should be perfect. ",1537702284.0
roicgm,"You can find an always updated list of GPU service providers here: https://github.com/binga/cloud-gpus

As you can see, there are lot of dead cheap options like vast.ai, salamander.ai and Paperspace. None of them, are free forever though. ",1537716821.0
pcidev,"You can use FlyodHud. It give some free hours.

&#x200B;",1537700125.0
namp243,Kaggle kernels,1538268101.0
dyaser,What about databricks?,1538357622.0
,[deleted],1537695640.0
ThatsALovelyShirt,"As someone already noted, you have two GPUs, one that consumes low power (the Intel one) for just desktop stuff, and one that consumes a lot more power (NVIDIA) for higher-end tasks. Your computer dynamically switches between them.

I'm also not sure what you mean by ""install Tensorflow on GPU"". Your GPU doesn't really get anything ""installed"" on it. You download the Tensorflow library, with which you would generally use Python or C++ bindings. These bindings then communicate with a more fundamental CUDA layer, which interfaces with your GPU to execute what are called ""kernels"", or small programs that run on your GPU. 

Honestly, if you're asking such a relatively simple question, you may find developing in Tensorflow rather challenging. What are you hoping to do with it?",1537683052.0
znihilist,"There are two items because the Intel CPU on your laptop comes with a built-in graphics card, and the 950M is the mobile version of the GTX 950 (doesn't necessarily mean same performance). However, I think the 950m should beat the Intel 4600. As for how to do it, just follow the installation guide on the official website: https://www.tensorflow.org/install/gpu",1537682207.0
This_Is_The_End,Install conda and the tensorflow-gpu package. They made it quite easy. ,1537684532.0
Everfast,"I will assume you use it as a loss in a [keras model](https://keras.io/losses/). The loss will be the mean over your batches during training and over your whole validation during testing. ""The actual optimized objective is the mean of the output array across all datapoints"".

The maximum MSE can be arbitrarily large. If your model predicts negative numbers or insanely high numbers your loss can be arbitrarily large.",1537607383.0
dankirsdot,"I am not sure I got you right, but you can as well check OpenPose. It can detect fingers quite accurately.",1537560500.0
logical_space,What makes this a GAN?  It looks like a basic RNN for predicting/generating sequences...,1537580280.0
secularshepherd,"I’m not religious, but I find this sort of thing disrespectful to people who are. I’m not saying that religion shouldn’t be criticized or mocked, but this guy made this Bible-generator just because he could. I think it highlights why ML folks should really think more about the consequences of their work. At best, he has yet another tutorial on text generation (I don’t believe anything about this blog was specific to the Bible), and at worst, he is mocking people. Y tho",1537538000.0
Abhishek_Advance,I would love to game it too.,1537535129.0
Cunic,"Very vague question, I would expect very vague answers.",1537543414.0
amrakkarma,It can be used in navigation systems when you enter a tunnel and lose GPS,1537523533.0
grumbelbart2,"Interesting approach. The 64x64x1 voxel subdivision is essentially a local depth map patch, but using average distance-to-center-point instead of euclidean depth.

I wonder if there is any definition of a local coordinate frame to make the matching easier. Is the ""1""-dimension of the voxel along the normal of the keypoint? Is the rotation of the voxel around that normal random or is there an attempt to extract a unique direction?",1537445707.0
mrathi12,"Typically LSTM sequence-to-sequence models struggle with long sequences because they use a standard encoder-decoder model. This architecture takes as input a sequence and outputs a single ""thought"" vector from the encoder, which it then passes to the decoder, which generates the output sequence. 

As you might imagine, it is hard to read in the entire paragraph before translating it, so the intuition behind attention is that we translate ""as we go"", and that for each word in the decoder model, we're focusing our attention on a few words at a time (we do this by ""weighting"" each of the output activations of the encoder - so high weight = more focus).

How do we learn these weights? We use a small neural net (typically 1 hidden layer) to map from the (previous activation of decoder, activation of encoder for timestep t) concatenated matrix to the attention weight for that encoder activation.

For CNNs, we can visualise the attention spatially - the following paper which generates captions from images is a good example: https://arxiv.org/abs/1411.4555

",1537347572.0
Hsankesara,"“Hierarchical Attention Networks” @heetsankesara3 https://medium.com/analytics-vidhya/hierarchical-attention-networks-d220318cf87e

You can check out this tutorial.",1537359297.0
bo0mb0om,Attention is a way of teaching the neural what parts in a time series it should look at to predict a new time series.,1537347504.0
mtanti,"Adaptive Computation Time for Recurrent Neural Networks
by Alex Graves
https://arxiv.org/abs/1603.08983

I have a really hard time understanding how to efficiently run such a system in something like Tensorflow. How do you modify the input sequence to have more or less gaps between the inputs? I understand that during test time an RNN can be used discontinously and can be fed inputs one at a time but don't you have to provide all the inputs at once during training time to allow gradient computation?",1537347664.0
mtanti,I would also like to understand the proof that 2 layer neural networks are universal approximators.,1537376970.0
Karyo_Ten,"Your notation is unclear, what does `(Wi[a^<t−1> ,x^<t> ]` mean?

The `a` doesn't exist in the graph. I suppose `a` is `h` (the hidden weights) but it would help if you keep notation consistent.

Also the weights for h and for x are different so you should ue a different name for both, see Nvidia article for a better notation: https://devblogs.nvidia.com/optimizing-recurrent-neural-networks-cudnn-5/.

Anyway, kudos for this. Backpropagating through RNNs is a pain and many made mistakes. Even the RNN R package has an equation bug on GRU:

  - https://cran.r-project.org/web/packages/rnn/vignettes/GRU_units.html
  - https://github.com/bquast/rnn/issues/31

Note: I didn't check your equations but the first thing you should do to make sure you have properly implemented backpropagation is to define a numerical gradient proc and use it.",1537347854.0
znihilist,Your data + how many used variables (read parameters) and their type.,1537287073.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/computervision] [How to calculate memory usage of NN’s](https://www.reddit.com/r/computervision/comments/9gzmyd/how_to_calculate_memory_usage_of_nns/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1537311160.0
practicalutilitarian,"Exceeding the no of examples with your trainable parameters is not a bad thing for models with regularization or random dropout or significant nonlinear relationships between those trainable parameters (LSTM, CNN). The only way to measure overfitting is to reserve a test set or cross validation ""fold"" whose accuracy you are monitoring and maximizing.",1537273031.0
practicalutilitarian,"Here's a [good tutorial](https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html) on visualizing convolutional layers. The same approach would work for other layers, but may not produce easily interpretable images",1537285158.0
tlkh,"The answer is always, if you care about speed and if you are training any ""real-world"" deep learning models, you probably want a GPU. It'll be at least 10x faster than even high-end Xeons.",1537197496.0
Zunnieee,"as far as I know , CPUs are just too slow for deep learning model 

maybe you should consider a GPU, which will make your training time faster ",1537246886.0
dun10p,"I know this is kind of old but I thought I would throw in my two cents. If you have the Xeon Gold 6128 lying around I would use that. I would not buy a new cpu though. As the other commenters have said, the best bang for your buck is using a gpu. However, the e5-2699 v4's are no slouch either. The key to using those is that you need to use the version of tensorflow that is compiled to run on avx2 instructions. See this guide [https://software.intel.com/en-us/articles/intel-optimization-for-tensorflow-installation-guide](https://software.intel.com/en-us/articles/intel-optimization-for-tensorflow-installation-guide) . 

&#x200B;

Is this cloud compute or do you own these machines? If it's in the cloud, there was a blog I read a week or two ago that found that 32 core vcpu instances were the best bang for your buck due to the increases in gpu instance costs on most cloud platforms. ",1542562169.0
ScotchMonk,"Not really, tpu is faster compared to older Kepler Tesla K80... but there's no comparison charts with Pascal (GTX 10x0 series) and Turing architecture (RTX cards with Tensor cores). 
And Nvidia will not let rest on their laurels and let anybody take their crown..😁",1537140327.0
manux,Try it and report back? Free paper!,1537158037.0
Inori,"What you're referring to is called [neural architecture search](https://en.wikipedia.org/wiki/Neural_architecture_search) and yes, [RL was applied for this task](https://arxiv.org/abs/1611.01578). In fact, there are so many various approaches that there's even [a survey paper](https://arxiv.org/abs/1808.05377) on the subject.",1537122782.0
mrathi12,"I'm currently writing a series of blog posts aimed at demystifying the maths behind deep learning. [http://mukul-rathi.github.io/blog.html](http://mukul-rathi.github.io/blog.html) (shameless self-plug). It is intended to bridge the gap between these online courses and the maths-heavy university courses like CS231n.

I've gone through the intuition as well as step-by-step through the maths, and I've also got code samples for you to implement the neural networks from scratch.

The relevant posts are: [https://mukul-rathi.github.io/2018/08/31/Backpropagation.html](https://mukul-rathi.github.io/2018/08/31/Backpropagation.html)

&#x200B;

[https://mukul-rathi.github.io/2018/09/10/CNNBackprop.html](https://mukul-rathi.github.io/2018/09/10/CNNBackprop.html)

&#x200B;

[https://mukul-rathi.github.io/2018/09/17/BackpropThroughAnything.html](https://mukul-rathi.github.io/2018/09/17/BackpropThroughAnything.html)

&#x200B;

Hope you find the posts useful, please comment if there are parts that could be made clearer. I'll be posting the RNN/LSTM backprop posts in a few days, in the meantime I have already posted the CNN and neural network backprop posts if you want to check that out.

&#x200B;

**EDIT: 18 September -** The RNN/LSTM backprop post has been posted",1537088959.0
emredog,"Make sure to check Stanford's CS231N class. The lectures and the slides are available online http://cs231n.stanford.edu/syllabus.html

And use a pen and a paper, try to work the chain rule and everything. It takes some time, but you have do it only once to get the intuition.",1537092083.0
irecebu,Checkout Michael Nielsen's neural networks and deep learning website. There is no RNN there but you shout be good to go once you understand basics which you'll definitely get by going through that website. ,1537103501.0
Schtecke,"Do you understand the mathematics that go into it? Backpropagation algorithm is basically just an application of the chain rule of derivation.

If you want to be systematic about it, you could try inventing and solving a few exercises to help you understand the details. Some suggestions:

1) Prove the chain rule of derivation

2) Derive/write down the chain rule for a function with argument of the type Ax + b, where x and b are vectors and A is a matrix.

3) Derive a general formula for the chain rule of a composite function of n functions when a) all the functions are different and b) when the functions are the same except for a parameter that acts linearly on the argument (as in the case of neural networks).

These are just some suggestions from the top of my head. In general, a good idea, when you don't understand something, is to go back to the abstraction level below what you don't understand and play with the maths a little bit.",1537093768.0
sasquatch007,"Nobody can answer this without knowing something about your mathematical background and what specifically you're struggling with. 

Answer those questions and I will try to help.

I will say though that this is pretty simple calculus, and I honestly think the deep learning people make it seem more complicated than it is. Even calling it ""backpropagation"" is a little silly; it's just the chain rule.",1537108738.0
thevatsalsaglani,https://youtu.be/I2I5ztVfUSE ,1537115388.0
pcidev,"Hi, 

If you aim to become deep learning researcher then you should lean things more deeply then just their implementation part. I would advice to start working on some project along with the reading stuff. 

**Step -1** Gain sufficient knowledge .

Learn concepts like Matrix algebra, calculus, probability and Statics. A good knowledge of these concepts  is required for reading research paper. (Also try to solve some questions)

**Step-2** Implement basic Stuff

Then learn basic ML stuff and deep learning concept. I will recommend [Deeplearning.ai](https://Deeplearning.ai) couse. ( If you want to go for research.). Implement the assignment by yourself. Doing this course will give you sufficient knowledge about the basic architectures of deep learning. ( it will take around 2 months, if watch videos at 2-3X, do assignment at the end of course )

**Step-3** Knowing Advance Stuff.

This is the right time for CS231 course. One of the best course. try to implement some of the architecture by yourself using Keras or pytorch). 

Follow some of the famous personalities on twitter and other social media. 

Tip : Look at the project of student of the course and try to replicated. Start from easiest. 

**Step-4** Learn RL 

Depending on your expertise took course. You can fins some of the best course on Udemy. Also if you have time go for berkely [CS294-112](http://rail.eecs.berkeley.edu/deeprlcourse/).

At this point of time start reading some of the research paper. keep an eye on the new advancement in the field.  Look on paper of various conferences like ICML. NIPS, AAAI, ICRA etc.

Apply this knowledge on some long term research project.

PS: You can check out Siraj Raval course Move 37 ( development phase ). 

&#x200B;",1537014580.0
Cunic,"I recommend first auditing Andrew Ng's Machine Learning Coursera course (https://www.coursera.org/learn/machine-learning), then auditing his Deep Learning course (https://www.coursera.org/specializations/deep-learning).

Watch all of the videos, do all of the homeworks. You'll be further down the ML/Neural Net road than most at this point.",1537016049.0
Cunic,"Could you rephrase your question? ""learn multivariate time series"" is pretty vague. ""LSTM did not learn"" is also not very helpful. What is the actual task? A clear problem definition would likely help you get clearer answers.",1537015314.0
,[deleted],1537013721.0
grandoldmikaduki,"If you want use these libraries out of the box soon, i recommend you to buy another Nvdia AIB.
Coz running Cuda code on ROCm is not merely running Cuda code, but needed some steps, and you may get stacked if you are not familiar with the compilation.

How to use Cuda code in ROCm are below:
1)Convert Cuda code into HIP with the script (hipify).
2)Fix the codes(like macros, structs, type of variables, and so forth) which aren't fitted to HIP ecosystem.

Still, Vega card itself are powerful, and ROCm becomes less buggy.
If you like your card and try new Lang/ecosystem, worth trying it.",1537146806.0
Abhishek_Advance,lambda labs gpu cloud seems to be offering the best price for the compute.  Better to use gpu's on cloud..... than buying and managing configurations and code conversions ,1537000913.0
pcidev,"What I think is that google is using there computational power and position to over-hype it. 

[http://www.fast.ai/2018/07/23/auto-ml-3/](http://www.fast.ai/2018/07/23/auto-ml-3/). 

Although Neural Architecture Search can be good research topic ;) ",1537015080.0
ThatsALovelyShirt,"Well CuPy is natively CUDA powered and seems to be purely geared toward python. Eigen is traditionally a C++ library and can be powered by CUDA or OpenCL.

Performance-wise, I'm not sure. It will problem depend on your architecture, platform, and application.",1536980695.0
iknowyoureadit_,Good stuff! ,1536969616.0
antiquemule,"Thanks for that. There are so many subspecies of neural network that it makes my head spin. I hope that after a few more weeks of reading this kind of stuff, clarity will emerge.",1536995778.0
ScotchMonk,"wow... the idea itself blows my mind! if you can do it for text, you can do it for voice comprehension. Probably the model can give a medical diagnosis simply from listening a story from patient describing his/her pain. Keep up the good work! ",1536923697.0
SamStringTheory,"Yes, this is expected. See this post on how learning rate affects training: https://towardsdatascience.com/understanding-learning-rates-and-how-it-improves-performance-in-deep-learning-d0d4059c1c10",1536844509.0
bellari,"Take a look at this Google guide. It gives suggestions for architecture and vocabulary.  
[https://developers.google.com/machine-learning/guides/text-classification/step-2-5](https://developers.google.com/machine-learning/guides/text-classification/step-2-5)",1536800604.0
Karyo_Ten,https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle,1536829924.0
FragLegs,"Check out this paper about Deep Crossing, which does something similar to what you are trying to do: http://www.kdd.org/kdd2016/papers/files/adf0975-shanA.pdf
",1536802342.0
brianytsui,"Q: What is the best practice to mix the current model that works only on text input with numeric features? 

A: My understanding is that the common approach would be to just treat the numbers as text. Like if you dig into the word embedding matrix, you will see a bunch of numbers. but of course that comes with many disadvantage and advantage.  Do you mind to clarify how are the numbers expected to relate with the problem?",1536791904.0
r3nj1thr,"Take a look at this paper 

https://arxiv.org/abs/1412.2306

",1536806915.0
Darshut,"If your dataset is good, then a few thousands pairs are more than enough... Is it labelled with user intents for each message ? Are entities tagged ? 
You can use an attention network to have a chatbot immediatly but it won't consider the discussion's context.",1536764729.0
spmallick,"We have a new post on #DeepLearning based Multi-Person #PoseEstimation.

[https://www.learnopencv.com/multi-person-pose-estimation-in-opencv-using-openpose/](https://www.learnopencv.com/multi-person-pose-estimation-in-opencv-using-openpose/)

This post builds on our previous post on Pose Estimation where we used the confidence map produced by #OpenPose to detect the pose of a single person. In this post, we use the Affinity Map produced by OpenPose to estimate pose of multiple people in the same frame.

The port uses #OpenCV's DNN module. We are sharing code in both #Python and #C++. With this new code, #OpenPose is fully ported for OpenCV users.

[https://github.com/spmallick/learnopencv/tree/master/OpenPose-Multi-Person](https://github.com/spmallick/learnopencv/tree/master/OpenPose-Multi-Person)

It takes a lot of effort and testing to produce a post like this one. So, if you like it, please share it with your friends and co-workers.",1536727413.0
Spenhouet,"Normalization can be of different forms. A simple normalization is to calculate the mean of a given set of data and then subtract it from every element in that set. The first part (Batch, ...) just refers to how the set is composed. For example in Batch normalization the set contains all entries of the current batch.",1536749065.0
hergertarian,"It’s not quite a one to one conversion, but you could use multiple input layers to create a multivariate VAR model equivalent. 

If you had just your lags (yt-1, yt-2) fed into an RNN layer, and any  structural regressors (x1, x2) fed into dense layers, you could get an effectively similar model. ",1536675598.0
Abhishek_Advance,"I have used this library based on LSTM created by NASA Jet Laboratory scientists to do Multivariate time series forecasting  and Anomaly detection.What you need is only first step-Forecasting. Even it has data engineering components/modules that are used to shape that data before feeding into LSTM.

It has produced good results.

Now i'm in the process of using genetic algorithms(Auto ML) to find the optimal parameter/hyper parameters required to model NN for my situation/data.

>[**https://github.com/khundman/telemanom**](https://github.com/khundman/telemanom)

&#x200B;",1536870319.0
ragas_,"Hi, RNN can handle both arimax and var models. But one need to do preprocessing of data before feeding into RNN model. ",1536676840.0
russellb23,"Yes. RNN are in a way similar to ARIMA, in accounting the historical points. 
For multivariate, you could try LSTM, a variant of RNN though.",1536680304.0
Cunic,"Time series forecasting or classification?

CNNs and RNNs can both be applied to solve either problem. Researchers claim both work well in different settings, if you have variable-length time series (and you want to keep them that way), RNNs will likely be your best solution. CNNs are likely easier to train than RNNs for time series classification, RNNs likely make more sense for autoregression.",1536694978.0
E-3_A-0H2_D-0_D-2,"You could Attention maps in order to see which areas of the image are being observed in order to make the classification. In other terms, these are the pixel regions which maximally activate the specific neuron(s) in the hidden layer that you are observing.",1536648348.0
Sontikka,I'm actually writing a thesis on a related matter. You usually compute the derivative of one output neuron before applying softmax or some other function. I would recommend using derivative x input instead of the plain derivative as that usually gives better results.,1536668870.0
s1korrrr,"Thank you for your job.

more useful links:

\- [https://paperswithcode.com/](https://paperswithcode.com/)

\- [http://www.gitxiv.com/](http://www.gitxiv.com/)",1536561186.0
kailashahirwar12,This is useful. ,1536647620.0
wh0star420,"Thank You !!!

&#x200B;",1536566887.0
kylepob,You will want to use the KerasClassifier or KerasRegressor wrappers to get this done. They provide an interface for the sklearn functions you want to use.,1536510855.0
felipecalderon1,"Deep Learning is a powerful tecnique but generating music is just one trick people figure out it would drive attention because it's kind of cool. 

&#x200B;

So they make some assumptions about music then send it to a RNN in end-to-end fashion. It outputs some interesting sounds and lyrics.

&#x200B;

Sure if you got domain knowledge and you put substantial effort fitting a technique into a problem then you will get better results than just trowing data at a RNN. ",1536454164.0
mystogan_enigmatic,"Check out this post
 http://colah.github.io/posts/2014-07-Understanding-Convolutions/",1536364395.0
DuckDuckFooGoo,"The Atrous method interweaves the convolutional filter coefficients with zeros so that the number of weights remain the same while spatial support increases. This is used to save memory when using dilated filters because the number of parameters grow as the filter size increases. The answer to your question is that filter dilations allows the image to stay the same size at every layer which allows for no loss of information when extracting features. However, the Atrous method is a technique that can be used with this architecture. I wrote a paper last week which proposed a no-downsampling FCNN for image segmentation of orthomosaics, and the reason why this helped me was because the images were high resolution. Since the images are very large the number of parameters will also be large so using zeros for some weights on the filter will decrease memory usage. Might be wrong since I haven’t implemented it yet",1536432736.0
GApproved,Dilated convolutions allow the model to learn features at large receptive fields while saving on the number of parameters compared to regular convolutions. ,1536358302.0
rahulbhalley,You might find this [Andrej’s course](http://cs231n.github.io/convolutional-networks/) helpful. ,1536381234.0
vtcsengineer,"Back in 2015 there was a group using [WiFi reflections to see people](https://www.popularmechanics.com/technology/news/a17974/wireless-signals-let-you-see-people-through-walls/).  In 2016 MIT developed a technique for [reading peoples' emotions](https://news.mit.edu/2016/detecting-emotions-with-wireless-signals-0920) using wireless signals as well.

However WiFi signals are incredibly weak.  Think half a watt tops paired with atrocious reflections, diffraction, refraction, absorption, scattering and general attenuation meaning the resolution is extremely poor.",1536354579.0
nunz,Of course we can! That's called RADAR (radio detection and ranging).,1536333635.0
frownyface,http://news.mit.edu/2018/artificial-intelligence-senses-people-through-walls-0612,1536339377.0
trickyLoop,Queue confused people saying “can I code a AGI in HTNL?”,1536309498.0
E-3_A-0H2_D-0_D-2," > Web developers need to run fast, AI is catching up...


I hate these kinds of statements so much.",1536327954.0
Karyo_Ten,Dreamweaver reloaded,1536314379.0
Cream_Of_Drake,"It's essentially using a drag and drop program after recognising what's on the paper I would wager; basically the AI is in the recognition not the coding, atleat that's my guess.",1536321327.0
AndyJessop,"It's a running theme that AI is one day going to make web devs obsolete, but posts like this only go to show just how far off that day is. This represents less than 1% of what you need to be able to do to produce a modern website.",1536321820.0
tulerworld,"Puhh. Whether this will really work as expected one day? 

This looks like something that will do the 20%, which is 80% of the result.

But as we all know the rest takes 80% of the time.

As of now I wouldn't be too concerned, if I was a webdev. ",1536315582.0
Handsome-Beaver,This is a sweet marketable demo,1536344835.0
Zunnieee,"... AI is running fast 

I just cant wait to see what they can do in the future ",1536508867.0
nickbuch,You could not ask a more vague question lol,1536292047.0
fvonich,Yes,1536435961.0
carlthome,"This is impossible to answer without more information. What's your budget? What types of models will you experiment with? If you don't know beforehand exactly what you're going to need, I think you'd be a lot happier if you invested in [a cloud solution](https://cloud.google.com/sdk/gcloud/reference/ml-engine/) so you can scale as you go. Development will look much the same but you don't have to bother with hardware. For quick prototyping you can use Colab.",1536267408.0
kylepob,"I majored in Biomedical Engineering and graduated in 2014. I got a job as a computer vision engineer and not surprisingly found that many of the state of the art techniques i wanted to use to upgrade our software relied on DL or ML. Over time, as I grew more comfortable with DL  I would start suggesting it when we were fleshing out new projects (both CV and non-CV). We also had a customer who was interested in a general survey of ML, so that was obviously a good excuse to start using DL. Originally my position wasn't in DL or even ML necessarily, but now it is a fairly big part of my day to day.",1536265492.0
yuehern,Great post! I am subscribing to your blog. Hope to see more posts like this.,1536256611.0
sparky_the_unicorn,"Full disclosure: I host one of my servers using their service.

The price is way way lower than what you find on traditional cloud computing providers. At least while the service is in beta and not many people know about it.",1536225973.0
_mortified_penguin,Are those prices for real? Why would you put a 1080ti for 0.15$ when leaderGPU and others ask for at least 0.6$? ,1536227241.0
somewittyalias,"It would be nice if someone who used this service as a renter would comment.  I saw they have 6 docker images available (tensorflow, pytorch, etc).  Can I install anything else I want?  Does it get deleted automatically?  I am afraid setting my environment would be painful.  Do the machines run reliably over long periods of time?",1536238178.0
a_mg,"You can have a look at this lecture pdf from Stanford: https://web.stanford.edu/class/cs224n/lectures/lecture16-guest.pdf
It introduces you to deep reinforcement learning applications for natural language processing if you don't know much about Deep RL. 

I didn't find any good tutorial because I was interested as well, but i think it wouldn't be so difficult to implement such algorithms based on the ideas from Stanford lecture. 

Just have a look to [Denny Britz](http://blog.dennybritz.com/2015/09/11/reimagining-language-learning-with-nlp-and-reinforcement-learning/) blog post about it for other ideas. It would be interesting to start developing a repo for NLP applications with Deep Reinforcement Learning. Maybe in PyTorch. ",1536240030.0
ispeakbinary01,The way i look at college is like a path for where you should start learning and what. But ofc it's more than that. The diploma you will get is a ticket for that interview you will go to later in life. Also you will meet new people have fun etc. College life is something good to experience because it will make you learn how to handle responsibilities and much more. Here where im from college is cheap. I study Computer Science but that doesn't stop me to learn AI on the side. I was in the same position as you and i decided to enroll. If you need more information feel free to dm me.,1536199549.0
whiteguy247,"My two cents on the matter are to focus on developing your critical thinking and problem solving skills in college and to go after a major that lets you do that to the fullest extent. At a larger public school this may be engineering, and at a liberal arts school this may be econ, so it will be up to your best judgement depending on where you end up.

&#x200B;

AI sounds cool and challenging, maybe something you want to devote a lot of time to, but as you probably realize, it changes every day. Once you learn to program and develop the core skills to solve data science problems, what's much more useful is being able to self-learn and make sense out of unordered problems. Knowing how to structure your thinking and leverage your resources to self-learn is much more critical to success than a certain tool or current skill.",1536368913.0
1991viet,Github repository for source code: [https://github.com/1991viet](https://github.com/1991viet),1536185781.0
spacehit,"Great job!
Do you have fps benchmark on the coco dataset?",1536219380.0
mtanti,"The weights are changed by gradient descent, which means that the more a pixel contributes to minimizing the error, the farther from zero it's weight pushed in order to make the pixel contribute more towards the output. On the other hand, pixels that are noisy are made irrelevant by pushing their weights towards zero. You can do something called sensitivity analysis by measuring the output's gradient with respect to the input in order to see which pixels contributed the most to the neural net's output.",1536134036.0
tkchris93,"You can give Google Colaboratory a try. It's free, but I sometimes struggle with establishing and maintaining a connection to the GPU",1536129115.0
AscendedSpidy,"You can use a service like FloydHub which gives you access to both CPU and GPU hardware. It isn't free but it is pretty cheap. It even supports Jupyter Notebooks, large data sets, and a few other nice to haves. Great interface too.",1536119313.0
alexmeistercl,"I faced the same situation while ago, it depends on what specifically you want to do nor the tutorial/course you are following.

I personally like paperspace, for basic/hobby stuff it’s like a $0.51 an hour + storage ($5 monthly) and $3 for an IP address, I personally don’t use an IP address because I do a reverse ssh tunnel to a DigitalOcean server and I have nginx to do all the proxy work for me, anyways, whay I learned is that you would need to spend some money to have access to a decent GPU.

If you are a student, try Amazon Educate, you can get somewhat 100 bucks to spend it on AWS, on AWS there’s public images/templates (I don’t remember the exact name) you can use to speed up the setup process like installing conda, cuda, other python packages you may need, etc... (check also github education package, they have an Amazon Educate code as well)

Free is stuff is great but is always limited in space, running time or other stuff which in the long term is annoying and frustrating (e.g.: colab.research.google.com and other free platforms are great though, for small things).

There’s others providers but it seems that they run on top of AWS, Crestle?

Also, checkout Google Cloud and AWS credits when you open/activate an account there, AFAIK Google gives a $300 for testing to spend on their services.",1536125881.0
E-3_A-0H2_D-0_D-2,"AWS On-Demand instances are pretty reasonable. The g3.4xlarge (Tesla M60, 8 GB GDDR memory) should suffice for regular Deep Learning problems. Plus, you get CUDA drivers and tensorflow, Keras, etc. pre-installed if you opt for the Deep Learning AMI; really accelerates the dev time.",1536131230.0
lightyagamikum,try deepcognition.ai,1536135259.0
bigslimvdub,"Amazon aws ec2 compute ? There are many free tiers of aws including specific compute tiers. If you aren’t running something super complex you may be able to do it for free for a year. 

https://aws.amazon.com/freetier  I believe is the address ",1536170932.0
edge_of_the_eclair,You should check out [Vectordash](http://vectordash.com) which lets you use a remote GPU instance for just pennies an hour. You can also start a Jupyter server on your Vectordash instance as well.,1536391110.0
LoveOfProfit,"Your question isn't entirely clear (language? Python?), but in general:

A train/test split function takes your labelled training data and splits it into 2 groups via sampling, at some ratio such as .75, such that 75% of your data is trained on and 25% is held out for testing purposes so that you have a better idea of how your model will perform.

A one hot encoder takes your categorical variables and translates them into a bunch of 1/0 variables, which results in a sparse matrix. Trivially, if you have a ""gender"" variable that's M/F, one hot encoding it you would have one feature that's 1/0. This lets an ML algorithm have an easier time handling these features.
",1536117370.0
Cartesian_Currents,"It's probably the sklearn functions/objects [OneHotEncoder] (http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) and [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).

These are pretty basic important concepts to statistical modeling/machine learning/data science. Here are individual articles for [one hot encoding] (https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f) and [training testing splits] (https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7). 

If you really want to understand the subject material of deep learning I suggest you start learning more basic statistical modeling, it will help you a lot. A good book is [ISLR] (http://www-bcf.usc.edu/~gareth/ISL/) (Introduction to Statistical Learning in R)which is available for free download from the authors website provided. The exercises are easy enough to complete in python, but solutions in R are common throughout the web.",1536121628.0
lightyagamikum,"to make it simpler:

* One hot encoder : let say you have this word : ""dog"".
you represent each letter with specific binary:
d = [1 0 0]
o = [0 1 0]
g = [0 0 1]. so, at the end you rewrite ""dog"" as [1 0 0], [0 1 0], [0 0 1]

* train-test split is just how much you want to split the data for training and testing (you have to understand the concept of training and testing). ",1536134908.0
Karyo_Ten,"I think the best way to learn about those is practical exercises on Kaggle.

Those are very practical topic that won't make sense until you try them.

Also be aware that we often used train, validation, and test sets terminology.

Train_test_split is used to split data in a training and validation set i.e. data for which you know the truth labels. And the test set is ultimate prediction goal (can be in the future). ",1536138128.0
schrodingershit,Learn AI rather than blindly using whatever you find on github,1536104959.0
nondifferentiable,"Reinforcement Learning: An Introduction - Sutton and Barto

Deep Learning - Goodfellow

go",1536107817.0
Laboratory_one,"I've found some success by studying 1 application vertical at a time, then building toy models to practice. It's been helpful in learning the inspiration and applications of different models.",1536111672.0
late_to_ml,"u/hamir_s Nice initiative. We have about 30 subscribers, hope we can build a supportive community ",1536114683.0
DerkvanL,"Not really an example, more like a commercial.",1536054859.0
hbagchi,"Presence of greens does not necessarily translates to residents using it. It's amazing they found strong correlation with obesity. Maybe, they can combine social and income data of residents for more insights.",1536025346.0
gururise,I can hear the feminist outcry and millions of offended millennials now... Fat shaming from space.,1536011000.0
svaisakh,"Hi everyone,

I'm an independent developer working on my own ML projects for about a year now.

For the past six months, I've been building a wrapper around PyTorch for making
Deep Learning development easier.

It's called MagNet.

*****

The core idea of MagNet is the Node.

Nodes are PyTorch modules that can change their properties dynamically based on the computational graph.

This way, they attach to each other... like magnets!

For example you can define a simple 3-layer CNN as follows.

    model = nn.Sequential(mn.Conv(32), *mn.Conv() * 2, mn.Linear(10, act=None))

Note that you only need to specify the bare essentials.

No need to specify the strides, paddings, input sizes, or even flatten the output before feeding it to the final layer.

Also, MagNet provides a customizable ``Trainer`` class which helps you avoid writing boilerplate code and focus on the project at hand.

    trainer = SupervisedTrainer(model)

    # Tracks the loss, metrics and adds a nice progress-bar
    monitor_callback = callbacks.Monitor()

    # Train the model for one epoch
    trainer.train(data(batch_size=64, shuffle=True), callbacks=[monitor_callback])

*****

I've found that MagNet has greatly reduced my codebase and increased my productivity.

I’m really excited to see how this framework will enable developers to create better Deep Learning projects,
and discover new possibilities.

Really appreciate if you guys could take a look at it and tell me what you think.",1535963879.0
kailashahirwar12,Still trying to get what a Node is in MagNet. Can you explain it more?. I am also working on a similar kind of project and would love to contribute to the repository.,1535987937.0
hbagchi,"How does it compare with the fastai PyTorch wrapper?
",1536025559.0
jcannell,"Yes, try [vast.ai](https://vast.ai/).",1539628044.0
pcidev,"Can we take it online 
",1535910832.0
Oblivious-Man,Could we have a sense of who the mentors will be at the retreat?,1535927946.0
StrikePrice,Kaggle offers free GPUs in their Jupiter notebooks (and other kernels). ,1535909686.0
hbagchi,Checkout Google colaboratory.,1535909275.0
allesfurwissenschaft,"For instance, there exists a problem which is called object detection & Recognition, and there's a project to solve this problem: Darknet. In Darknet, Yolo (v1, v2 and lastly v3) is written in C. There are also good libraries for face detection & recognition like Dlib which is written in C++",1535892294.0
,"I dunno about interview but c++ allows you to interact with large range of hardware type implementation like it allows a better hold on memory, pointers and other data types. In Python and java it's easy to write program coz much of memory and space task is handled by the language itself and you need only to concentrate on algo. But it puts a limit to what extent you can tinker with memory and data structures. 
Example:
Tensorflow is made to be used in Python but Tensorflow is itself written in C++",1535882223.0
ewlamy,"I think you’re more likely to implement inference at the edge using C++, as the performance benefits are valuable in embedded systems with constrained resources. Training models will be limited here, however, again because of the constraints previously mentioned. That said, the performance gains are probably out of reach for most developers that don’t have an expertise in C++ to begin with, so most application design is probably done in Python first and then reimplemented in C++ by an embedded software engineer with the appropriate background.",1535920095.0
cut_mountain_zero,"I donno if I am helping with this comment, but I believe cpp is not the preferred language in dl, I personally have only used python and interacted with people who use python. I would like to be corrected if wrong. Classical ML also would be better done in Python, ft. numpy",1535890146.0
utkarshmttl,"Can you explain what exactly are you trying to achieve? Wouldn't video analysis, for most cases, be equivalent to image processing? You just consider the video as a sequence of frames. ",1535880799.0
jwata,They write their own compiler to run DL models on Raspi’s GPU. Really cool.,1535819622.0
FaceDetector,That's really impressive.,1535825707.0
immortal333,those are really deep arch. to run on raspberry pi.,1535785755.0
Crypto_Rick_C-137,props to OP,1535852844.0
CountJeewb,This is so cool especially since it's on a raspberry pi zero. So surprised it was able to handle that,1535888551.0
clarle,"Are you building your own or just using AWS Rekognition / Google Cloud Vision?

If you're using the cloud services, then /r/aws and /r/googlecloud would be better subreddits to go to.  ",1535731863.0
random_forest97,telegram group would be helpful,1535695168.0
Karyo_Ten,"Why not Google+?

/s",1535698348.0
Don_Mahoni,We should move away from facebook,1535713602.0
Karyo_Ten,"Given that the smallest float32 > 0 you can represent is exactly 0.00000011920928955078125 (i.e. 1.19e-07, see [single precision to decimal conversion](https://en.wikipedia.org/wiki/Single-precision_floating-point_format#Converting_from_single-precision_binary_to_decimal)) and also [ULPs](https://en.wikipedia.org/wiki/Unit_in_the_last_place), I'm pretty sure your loss function or pipeline is wrong.

Check for vanishing gradients.",1535658521.0
nile6499,"loss going up or going down indicates it is learning something. It is possible your code is wrong is such a way that loss is decreasing but it is being represented as increasing, and when this happens make sure to check accuracy.

Increasing and decreasing loss values are pretty common which indicates they're learning... thats it.. nothing else. Unless you compare it with test result or anything.",1535727507.0
LoveOfProfit,"You can't just claim we've achieved ""machine intelligence"" and not explain what you mean, and how it's different from machine learning.",1535640309.0
trickyLoop,Define “intelligence.”,1535636955.0
kpskps,Dafuq is this ,1535640283.0
E-3_A-0H2_D-0_D-2,"Regarding the second question - yes; at inference time, you randomly sample from the softmax vector and feed it to the next time step. This prevents the machine from recursively predicting the same thing over and over again.",1535643331.0
JamesGeoffreyHill,"Key discussion points for me would be:

\- how exactly should data be concatenated in the neural networks to allow for the conditioning?

\- how exactly should the loss be written to allow for the conditioning?

\- what patterns should I be looking for in the G & D losses?

But I'm well aware that due to lack of experience I may not fully understand what the true problems are for me at the moment.",1535606479.0
audovoice,"I really am no expert but as I understand it a large part of the work is making the dataset. Meaning you need pairs of images so you can train the deeplearning to turn the first image into the second image. 

[https://github.com/yenchenlin/pix2pix-tensorflow](https://github.com/yenchenlin/pix2pix-tensorflow) I use this one because tensorflow works on windows. 

The I am guessing the example with the handbag is similar to what you want. You can make something like line drawings with canny edge detection, but be sure to make them negative first. A black image with white lines will not work.  

[https://github.com/mingyuliutw/UNIT](https://github.com/mingyuliutw/UNIT) also does something similar

&#x200B;

For style transfer this one seems less buggy for me. 

[https://github.com/hwalsuklee/tensorflow-fast-style-transfer](https://github.com/hwalsuklee/tensorflow-fast-style-transfer)

But these are interesting too.

[https://github.com/rtqichen/style-swap/blob/master/README.md](https://github.com/rtqichen/style-swap/blob/master/README.md)

[https://github.com/NVIDIA/FastPhotoStyle](https://github.com/NVIDIA/FastPhotoStyle)

[https://github.com/alexjc/neural-doodle](https://github.com/alexjc/neural-doodle)

[https://github.com/Heumi/Fast\_Multi\_Style\_Transfer-tensorflow](https://github.com/Heumi/Fast_Multi_Style_Transfer-tensorflow)

I hope this helps. Clearly one way of seeing if something is possible is seeing if someone had already done similar. ",1535708892.0
LewisJin,What does this doing for?,1535541626.0
ZhuangZhe,"I've tried. I actually left a comment on the repo asking for access to the data, but I haven't heard back from the author.

Maybe someone knows of a similar dataset that could be used instead?",1535544677.0
PapaCreameritus,How is this profitable other than keeping the users tranning data?,1535548748.0
potatomind,"So, how are you better than other tens of services with cloud gpu? I feel like there are too many of companies offering cloud gpus this days. Where have they been a year ago for my thesis... ",1535531812.0
thisismyfavoritename,"RBMs sort of went out of favor after VAEs and GANs were introduced.

Using AEs to warm start neural nets training has been proven to be some kind of regularization. No sources as I'm on mobile, sorry.

I would recommend reading more recent works, even though the basics will still be relevant.",1535516281.0
ThatsALovelyShirt,"It's beneficial for them to be the same, but if the software is written correctly, it shouldn't matter.  I've mixed titans and 1080 Tis no problem. ",1535503852.0
mdfeist,"It should be fine. However if you do parralize your code and depending on your code you might be bottlenecked by your slowest card. One common approach is to split the job over the GPUs. If you do this the faster card will always be waiting for the slower one to finish before updating the weights. So it will still work, but not utilizing the full power of the better cards.

Also I would look into SLI. I'm not sure SLI is supported on the 20 series. I think they moved to NVLink, but I could be wrong. I'm not sure how that will work with current deep learning libraries. It might not be a plug and play. It might require extra code to make the GPUs work together. 

If you are just running one job on one GPU at a time you should have no issues. The only issues you may have is if you try to run one job across multiple GPUs.",1535511277.0
scienceistoohard,"I think it would be better to get the latest variant of the i7; you can get the latest i7 and a motherboard for it and still be well within your budget.

You can compare the specs of the latest i7, and the Xeon you've picked out, here:

[https://ark.intel.com/products/64595/Intel-Xeon-Processor-E5-2670-20M-Cache-2\_60-GHz-8\_00-GTs-Intel-QPI](https://ark.intel.com/products/64595/Intel-Xeon-Processor-E5-2670-20M-Cache-2_60-GHz-8_00-GTs-Intel-QPI)

[https://ark.intel.com/products/126686/Intel-Core-i7-8700-Processor-12M-Cache-up-to-4\_60-GHz](https://ark.intel.com/products/126686/Intel-Core-i7-8700-Processor-12M-Cache-up-to-4_60-GHz)

Specs (supposedly) in favor of the Xeon:

\- More cores for the Xeon; 8 cores/16 threads for the Xeon vs 6 cores/12 threads for the i7

\-  Bigger Cache in the Xeon (20 MB vs 12 MB)

\- \*Much\* bigger max memory size for the Xeon (384 GB vs 64), but I think this doesn't matter for your purposes.

Specs in favor of the i7:

\- Much higher clock speed for the i7 (3.2 gHz - 4.6 GHz vs. 2.6 GHz - 3.3 GHz )

\- Much newer core architecture for the i7 (coffee lake vs sandy bridge \[[http://www.cpu-world.com/CPUs/Xeon/Intel-Xeon%20E5-2670.html](http://www.cpu-world.com/CPUs/Xeon/Intel-Xeon%20E5-2670.html)\])

​

The last of these is probably the most important, even though it's less easy to quantify. For deep learning stuff you'll probably be offloading a lot of things to the GPU when you need parallelism; the CPU is for when you need good single core performance.

Xeons are meant for servers; they can handle a lot of memory and (I believe) are more robust when run continuously for a long time. They're not really meant for your use case, and the one you're looking at in particular is old.

The latest i7 is almost always going to be much faster than the older Xeon that you're looking at; it runs at a higher frequency and has a newer core architecture. The only case when the Xeon will win is when you have an embarassingly parallel task to run, in which case the extra 4 threads might make the difference. But, again, most of your parallel tasks will be done on the GPU, so that's kind of a moot point.

I've had this experience personally; I was working with an older computing cluster that was running Xeons, and my newer laptop running an i5 would crush it every time unless I was running something that needed a huge amount of memory and was very easily parallelizable.

​

If you do decide to get an i7, keep in mind that the primary difference between i7-8700 and i7-8700k is that the 'k' model is meant to be overclocked, and the non-'k' model cannot be overclocked.

&#x200B;

EDIT: One more thing about the GPU. You've selected the best one you can get for now, but in a month Nvidia is releasing the next generation (i.e. the RTX 2080), which is supposedly going to be a lot better for deep learning since it has tensor cores. You might want to talk to whoever is funding this about getting some extra money to buy one of those when it comes out, or just hold off on buying a GPU until then.",1535492071.0
biasOfLearn,"Seams ok. Maybe bigger ssd or another gpu 
But it depends on the project data etc...",1535489449.0
TheIdesOfMay,"I don't enough about the server CPU market to advise you on that, but what I will tell you is that your PSU is massive overkill. 650W should be more than sufficient for current specs. Perhaps you could go for a cheaper, less powerful PSU and use the extra $60 you save on another HDD for training sets?

Actually, now that I think about it, the best thing would be to wait a month for the 2080. Its MSRP is $750 (only $100 more than 1080ti) and that'll be deciding component when you measure the system in terms of its DL ability. ",1535490537.0
nagromo,"Have you considered AMD Threadripper? You can get a higher clocked 12-core processor with quad channel memory for about the same price as either of your Xeon builds:

[PCPartPicker part list](https://pcpartpicker.com/list/xpcrq4) / [Price breakdown by merchant](https://pcpartpicker.com/list/xpcrq4/by_merchant/)

Type|Item|Price
:----|:----|:----
**CPU** | [AMD - Threadripper 1920X 3.5GHz 12-Core Processor](https://pcpartpicker.com/product/cRDzK8/amd-threadripper-1920x-35ghz-12-core-processor-yd192xa8aewof) | $474.33 @ B&H 
**CPU Cooler** | [Noctua - NH-U12S TR4-SP3 55.0 CFM CPU Cooler](https://pcpartpicker.com/product/NDtWGX/noctua-nh-u12s-tr4-sp3-934-cfm-cpu-cooler-nh-u12s-tr4-sp3) | $69.90 @ Amazon 
**Motherboard** | [ASRock - X399 Taichi ATX TR4 Motherboard](https://pcpartpicker.com/product/kjmxFT/asrock-x399-taichi-atx-tr4-motherboard-x399-taichi) | $283.98 @ Newegg 
**Memory** | [G.Skill - Aegis 16GB (2 x 8GB) DDR4-2800 Memory](https://pcpartpicker.com/product/Kp38TW/gskill-aegis-16gb-2-x-8gb-ddr4-2800-memory-f4-2800c17d-16gis) | $126.99 @ Newegg 
**Memory** | [G.Skill - Aegis 16GB (2 x 8GB) DDR4-2800 Memory](https://pcpartpicker.com/product/Kp38TW/gskill-aegis-16gb-2-x-8gb-ddr4-2800-memory-f4-2800c17d-16gis) | $126.99 @ Newegg 
**Storage** | [Samsung - 850 EVO-Series 500GB 2.5"" Solid State Drive](https://pcpartpicker.com/product/FrH48d/samsung-internal-hard-drive-mz75e500bam) | $123.50 @ Newegg Marketplace 
**Storage** | [Toshiba - X300 5TB 3.5"" 7200RPM Internal Hard Drive](https://pcpartpicker.com/product/kJfmP6/toshiba-internal-hard-drive-hdwe150xzsta) | $131.99 @ Amazon 
**Storage** | [Toshiba - X300 5TB 3.5"" 7200RPM Internal Hard Drive](https://pcpartpicker.com/product/kJfmP6/toshiba-internal-hard-drive-hdwe150xzsta) | $131.99 @ Amazon 
**Video Card** | [EVGA - GeForce GTX 1080 Ti 11GB SC2 Video Card](https://pcpartpicker.com/product/q438TW/evga-geforce-gtx-1080-ti-11gb-sc2-video-card-11g-p4-6593-kr) | $669.99 @ B&H 
**Case** | [Fractal Design - Define XL R2 (Titanium Grey) ATX Full Tower Case](https://pcpartpicker.com/product/GTGkcf/fractal-design-case-fdcadefxlr2ti) | $119.99 @ SuperBiiz 
**Power Supply** | [EVGA - SuperNOVA 1000 P2 1000W 80+ Platinum Certified Fully-Modular ATX Power Supply](https://pcpartpicker.com/product/dJ6BD3/evga-power-supply-220p21000xr) | $149.99 @ B&H 
 | *Prices include shipping, taxes, rebates, and discounts* |
 | Total (before mail-in rebates) | $2469.64
 | Mail-in rebates | -$60.00
 | **Total** | **$2409.64**
 | Generated by [PCPartPicker](https://pcpartpicker.com) 2018-08-28 22:56 EDT-0400 |",1535511531.0
tlkh,"I’m surprised nobody mentioned PCIE lanes. 

For multi-GPU setup, you’ll want to shoot for a greater number of PCIE lanes for your GPUs. However, this still greatly depends on WHICH Xeon you get, as the lower end Xeon have the same number of PCIE lanes as the i7, while i7 extreme edition has more PCIE lanes while remaining cheaper.

Do take a look at the individual spec for the processors you are considering and factor in PCIE lanes. ",1535532657.0
__Lau__,"Maybe buy a Titan Xp. And I would buy an i7 since it provides better performance, since you don't need a ECC or ultra stability",1535492020.0
mangoojoanna,Here are more good points about why not to use notebooks in ML development: https://docs.google.com/presentation/d/1n2RlMdmv1p25Xy5thJUhkKGvjtV-dkAIsUXP-AL4ffI/mobilepresent?slide=id.g362da58057_0_1,1535471996.0
secularshepherd,"TLDR: nbconvert exists but it’s silly, so just use vanilla Python.

Okay...",1535467496.0
rhascal,Can anyone speak to the quality or lack thereof for any of these books?,1535466884.0
Manto1,"You can use ModelCheckpoint callback to save the weights of the best model during training. When training is done you can load those weights and use the best performing model.

Here's an example of using the callback https://keras.io/callbacks/#example-model-checkpoints",1535442626.0
E-3_A-0H2_D-0_D-2,Look into auto-encoders.,1535392213.0
ChmHsm,What you're looking for is GANs (Generative Adversarial Neyworks) ,1535399686.0
Oryak,Take a look at GANs and there specifically for unsupervised image to image translation. That’s an unsupervised Problem where deep nets showed for the first time a big leap in performance,1535396030.0
itamblyn,"Auto-encoders do something equivalent to non-linear principal component analysis (PCA).

Generative adversarial networks are generative networks - you show them some data (labelled or unlabelled), and they will learn how to mimimic it.

DL is being applied to all areas (e.g. supervised, unsupervised, RL, etc)",1535405063.0
Rezo-Acken,"Generative adversarial networks. Word embeddings (with NN). Auto encoders.

In general any kind of representation learning is unsupervised.",1535405073.0
cwadamsmith,"Deep learning is a narrower category of machine learning that uses hierarchically stacked algorithms of increasing complexity and abstraction. In deep learning, the computer is fed a huge set of data marked with Meta tags. The program processes the training data to create a feature set for the object to be identified ",1535366990.0
dualmindblade,"Modern ANNs tend not to work like that. Instead of firing/not firing, a continuous non-linear function is applied, such as sigmoid (kinda like firing/not firing) or relu (less like that). This makes them more straightforward to train via gradient descent.",1535292364.0
Spenhouet,This is the same scam as posted yesterday just reworded to make it sound more legit.See here: https://www.reddit.com/r/deeplearning/comments/99o4ea/free_1080ti_gpus_for_ai_startups/I would suggest staying far away from this one.Just use Google Colab to use free GPUs for ML.,1535287055.0
ThatsALovelyShirt,Didn't this just get posted yesterday?,1535215461.0
tcdirks1,r/deepdream,1535237568.0
JasonRDalton,Will check this out.  Thanks.  ,1535230780.0
formeinredditdo,Is this legit? I could use for school lol,1535261974.0
thevatsalsaglani,"Can't launch instance. ""All tensorlets are busy"" error",1535277913.0
whitezl0,"A good way to check for the video card is the next thing:
1. Sing up and log in
2. Launch instance and go to the terminal in the JupyterLab
3. Type 'nvidia-smi'",1535216510.0
OddPositive,"when I try to launch an instance, it tells me that ""all tensorlets are busy"".",1535273765.0
Karyo_Ten,"Usually it's RAM --> VRAM but some data loaders can memory map the file and copy that directly to VRAM.

Most of the time you need to do preprocessing anyway so data is already in RAM",1535214327.0
solingermuc,"There is a seed to be set in Keras as well;

In addition, TensorFlow has its own random number generator that must also be seeded by calling the set_random_seed() function immediately after the NumPy random number generator, as follows:

from tensorflow import set_random_seed
set_random_seed(2)


To be crystal clear, the top of your code file must have the following 4 lines before any others;

from numpy.random import seed
seed(1)
from tensorflow import set_random_seed
set_random_seed(2)

Source:[seed keras](https://machinelearningmastery.com/reproducible-results-neural-networks-keras/) ",1535182298.0
Karyo_Ten,"you can reduce sources of non-determinism but as long as you use multithreading, floating point reductions (like sum that are used in loss functions) **cannot** be deterministic.

A CPU or GPU thread can be paused for any reason (OS deprioritizing, hardware throttling due to high temp, neutrino ...) and then instead of `((a + b) + c)`, you get `((a + c) + b)` rounding is different and result is different.

Fixing random seeds in Numpy and Tensorflow/Theano/Cuda GPU is not enough.

Relevant links from one of my old posts (https://www.reddit.com/r/programming/comments/873vvy/2_2_4_er_41_no_43_nvidias_titan_v_gpus_spit_out/dwas6sa/):

* https://www.twosigma.com/insights/a-workaround-for-non-determinism-in-tensorflow
* https://github.com/tensorflow/tensorflow/issues/3103
* https://discuss.pytorch.org/t/deterministic-non-deterministic-results-with-pytorch/9087
* https://github.com/tensorflow/tensorflow/issues/2732
* https://github.com/soumith/cudnn.torch/issues/270
* https://github.com/pytorch/pytorch/issues/2831",1535185413.0
Spenhouet,"To give a more basic and general answer: The randomness can come from different sources.1. Randomization of init values (for example weights)2. Shuffling of training data3. Random selection of training, validation and test data.4. Random validation, test subsetsThis is done to improve generalisation and to reduce the chance of getting stuck in a local minima.",1535203471.0
Karyo_Ten,"AVX and AVX2 improve perf tremendously on CPU as does Sandy Bridge and Haswell, though the new Intel security fixes are expected to gimp perf by a couple tens of percent.

Still, try do do as much as possible on GPU for image processing/deep learning and cache intermediate results of your ML pipelines and your CPU is enough.

Also Python interpreter is single threaded and will not use AVX so your high clock is good there.

I use 8xPCI-E 3.0 with a GTX 1070, it's good though Titan Xp, Volta and Turing might be restricted.

Forget about AMD, learning DL is hard enough, OpenCL means compiling many stuff from source, having no tutorial and hard to find support. You can try TensorFlow for Rocm though: https://gpuopen.com/rocm-tensorflow-1-8-release/",1535061758.0
fkxfkx,"For another perspective, forget outfitting your own machine.  It's a greased pig you'll never catch.

Learn to use the cloud resources of AWS, GCP, Azure, etc and learn to use resources you could never afford to build.



",1535139234.0
GotRedditFever,Get a ryzen CPU and Nvidia GPU,1535105128.0
Karyo_Ten,"Given Intel woes with Spectre, meltdown and L1TF and Threadripper perf, I think it's a perfectly fine setup to mix Ryzen and Nvidia GPU.

Absolute no to the FX9590. Pick any i3/i5 from Haswell+ if you want to save money.

Note that besides linear algebra code (including deep learning) everything in Python is single-threaded so good single-threaded perf is very important.

Sources: 

 - Intel forbidding benchmarks following security fixes: https://www.reddit.com/r/programming/comments/99ljxs/new_software_license_agreement_from_intel_forbids/
 - Anandtech on Threadripper https://www.anandtech.com/show/13124/the-amd-threadripper-2990wx-and-2950x-review
 - Intel perf hit due to L1TF:
    - https://www.phoronix.com/scan.php?page=article&item=l1tf-early-look&num=3
    - https://www.phoronix.com/scan.php?page=article&item=l1tf-foreshadow-xeon&num=5",1535042227.0
rambossa1,I’m threadripper (8 Core) & 1080TI,1535110512.0
bigslimvdub,8320e works great. 8 cores and 8 threads with 1mb L3 each. Depends on what your doing though. ,1535047539.0
Frosty_Cryptographer,"\+1 for AMD and newest Ryzens! :)  
I use Ryzen 2700X in my lab setup and it works great with FreeBSD and Linux. It have great price/performance ratio and it's good to support Intel competitors like AMD.

Check newest ThreadRipper 2xxxX CPUs if you need a lot of PCIe lanes, quad-channel memory and lot of cores.  
",1535184658.0
Spenhouet,"Give me your product idea and I give you access to a GPU... doesn't sound right. But even if this offer is sincere, I don't think it works as OP expects. The target group in general is stuffed with GPUs. The only one targeted here are students with no insight who can't afford a GPU.",1535074014.0
Darshut,"Hi,

The easiest way to build this kind of chatbot is to use a platform such as Dialogflow where you have to define your intents (ex: order_pizza, set_ingredient, ...) and entities (ex regina, ham, ...) providing some examples as well.

You can also find the chatterbot and deeppavlov libraries for goal-oriented chatbots.

You can also be interested in RASA_NLU if you don't want to use any framework.

Finally if you want to build it from scratch, I guess you need :   
- Intent classification  
- Entity extraction  
- Answer generation (selection or generation)  

https://dialogflow.com/  
https://github.com/gunthercox/ChatterBot  
http://docs.deeppavlov.ai/en/master/skills/go_bot.html#  
http://rasa.com/docs/nlu/master/  

I guess there is some cool stuff on coursera (Sequence models) and Udemy too on this purpose.

Hope it helps ",1535035567.0
audovoice,"Turns out it looks like total garbage. I am making them smaller, and this time I will be sure to invert the canny edge. Should I try anything else?",1535017205.0
Remi_Coulom,"I would choose 1x2080Ti without hesitation. If your code can use tensor cores, then it will be faster. If you take the cost of electric power into consideration, the 2080Ti is a clear win.",1534973178.0
Jsd5,"Definitely the 2080 TI, the tensor cores alone  make it far faster than the 2 1080 TIs.  These cores allow for feed forward networks to be trained ridiculously quickly , 110 TFLOPS for the single 2080Ti vs maybe 20-24 TFLOPs for the 2 10 series.  And that is not even considering the CUDA cores in the 2080Ti. Additionally deep learning across 2 GPUs is not easy and often not worth it (see some tensorflow tutorials for the hassle this can cause). ",1534973776.0
schrodinger_s_monkey,"Training on multiple GPUs has been proven to be quite difficult. Nonetheless, it is an interesting problem in the area of computer system and parallel processing research. People are trying data parallel and/or model parallel processing models and are getting mixed results. 

So if you simply want to do ML, go with one card. ",1534992730.0
sabalaba,"Here are some actual benchmarks for the 2080 Ti for you to decide. [https://lambdalabs.com/blog/2080-ti-deep-learning-benchmarks/](https://lambdalabs.com/blog/2080-ti-deep-learning-benchmarks/)

&#x200B;

I would just go with the 2080 Ti. Even on a cost efficiency basis, the 1080 TI is only 4% more cost effective for FP16 training and 21% more cost effective for FP32 training.",1538155762.0
corysama,"A a general rule in computer hardware:  For the same monetary cost, 2 separate things working together will have greater combined power than a single thing.  The single thing charges the same for less power because it is simpler to use.

Also, don't forget that 2 RAM banks have 2x the bandwidth of 1.",1534973275.0
Jadeyard,"And there still doesnt seem to be a really nice tool to easily and intuitively make nice graphs for neural networks. Most people seem to handcraft them a lot, e.g. recommending inkscape etc.",1534959828.0
__Lau__,"You are right there is not straightforward tool, but it helps a lot like if you use tf.name_scope to group similia structures. Furthermore you can create the tensorboard before creating the gradients so only the forward propagation is shown. Or you right click on elements and remove them from main graph. My VAE Board does not look messy.",1535521554.0
sudharsansai,"One option: Start with Andrew Ng's deeplearning.ai courses. You will get a holistic view of what is happening in DL. From there, you can start reading papers, understanding them and trying to replicate the results. See [this](http://qr.ae/TUNhhF) answer from Andrew Ng.

Second option (the one I adopted): Pick up an avenue/domain. In the case of DL, it's mostly one of Computer Vision (CV) or NLP. I am currently working on NLP but started with CV. Pick up a famous course from a top university (Stanford, MIT etc.) for the domain you chose. For NLP, I used CS224d and for CV, your best option would be CS231n. Audit the course, understand the concepts explained there. By the end of these courses, you will be pretty much ready to read papers, understand them.  Now go back to the advice of Andrew Ng and start doing some dirty work by trying to replicate some SOTA results.

Advice: Reading papers may be daunting at first, but believe me, you will be much better towards the end. While reading papers, start with the basic ones and try making your way through more difficult ones. Also, try to understand each and every component of the model discussed in the paper. Try tweaking them, criticize them, and this is how you become good!  You will start seeing some pattern. Also, by now, no project should seem ""very big"" for you.

Good Luck and more importantly, have fun :) ",1534886021.0
,"Go deep in the pussy and learn from inside. Try to get Recurrent Neural Orgasms. Also try to Backpropagate to her ass and FeedForward your semen in her mouth. 

Shift between XGBoost cowgirl and doggy CatBoost. Don´t go Light GBM on her. ",1534886337.0
iamwarburg,"Hi 

&#x200B;

I ran your code and I can't get any convergence either. I also tried to use your data generator and another implementation of the NALU: [https://github.com/kevinzakka/NALU-pytorch](https://github.com/kevinzakka/NALU-pytorch), but could not get convergence here either.  I was able to get some convergence by decreasing the number of elements in x... (but this is not replicating the experiment, just making an easier one...)  


I looked at your implementation and I agree in the way that you generate data.   
I noted that you draw from an integer distribution whereas Trask write that he draws from the reel range... However, I can't see that this should make any difference...?  


I also wondered if the poor convergence is because of a too large range? I cannot see anywhere in the original paper, which range of the elements in x that Trask et al. use?   


Have you made any progress in getting convergence for this task?  


Best  
Frederik

&#x200B;",1542445468.0
rajarsheem,"Loading your own pre-trained model is nothing dissimilar than loading the weights of a model you just finished training yesterday. The ease of use comes in pytorch when you want to do something with it --- add layers, modules, parameters or slicing out some intermediate layer from inside the model to be used in a new and different model. This is way easier in Pytorch than Tensorflow. I can say this because I have extensively used both.

Extra needless effort in TF like managing graphs when have two models -- one you pre-trained and the one you have now, is always a pain. Things are not as straightforward as this in TF.

To better illustrate, you can do something like this in pytorch:

old\_model = OldModel()   # call your old model class

old\_model.load\_state\_dict(T.load(path)\[""state\_dict""\]) 

new\_model = MyNet(hdim=100) 

new\_model.linear = old\_model.linear5

This calls your old model class. Loads its trained weights. Creates your new model MyNet. 

Uses the 5th linear layer in your old model as a layer in your new model with loaded weights.

All this ease of use is because of its modularity. 

Eventually, you should use that framework which helps you spend more time with the algorithm you want to solve more than the framework itself.",1534851818.0
rhascal,"I don't know what your website was trying to do, but it initiated something bigger than an ebook download. Cut it off.",1534775868.0
ekmungi,Look into Kaggle competitions. Some classic ones like distracted driver etc. Read different papers and try latest architectures,1534760347.0
merckxiaan,The fast.ai course is awesome,1534785204.0
AdventurousLimit,"if you wanna do more researchy stuff find a computer vision paper and try to reimplement it, so you can get practical knowledge (image prepro, data aug, random noise, etc.) that you can't from mnist",1536673062.0
,[deleted],1539174193.0
the_kind_king,[https://www.paperspace.com/&R=M948ZK2](https://www.paperspace.com/&R=M948ZK2) (M948ZK2) - August 2018,1540658938.0
glamdring001,"This works as of **Nov-2018**! https://www.paperspace.com/&R=NTA2D6W

**NTA2D6W**",1541297744.0
Kelvin977932,"For **November -2018** use following code, thanks!

# [KHDKGQE](https://www.paperspace.com/&R=KHDKGQE)

&#x200B;",1541992853.0
glamdring001,"**Works in November!** https://www.paperspace.com/&R=NTA2D6W

**NTA2D6W**",1542807803.0
glamdring001,"**Winter is coming!** This code **works for November/December 2018**!

**NTA2D6W**

https://www.paperspace.com/&R=NTA2D6W",1542807957.0
koryoislie,This one worked for me **U0K6DIK**,1543408638.0
yunusemrecatalcam,"Vov, I'm on a similar situation. I'm working as an embedded software developer now and planning to being a ML developer. I think I handled the most of the math of learning. I've used Andrew Ng's courses on coursera:

https://www.coursera.org/learn/machine-learning

https://www.coursera.org/learn/neural-networks-deep-learning


After completing these courses trying to implement algorithms without using ML frameworks is really important, because you know; when you implement the algorithm you feel like ""I really got this!""

Also there are YouTubers that make really useful videos about ML:

3blue1brown's deep learning calculus videos are really good and makes you able to what's going on in a neural network.
https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw

Siraj Raval has more practical videos but has a math of intelligence playlist that good for learn math of ML https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A",1534753610.0
Phnyx,"If you went through the fast.ai course you should be familiar with kaggle. Join a few current competitions, become active in the discussion and ideally create a few kernels to help other people. Write a medium post or two. 

List your new skills on LinkedIn and approach some Recruiters to explain your position. 

Then, just apply for jobs that interest you. This doesn't need to be ""deep learning researcher"" but could go more into the general machine learning engineer direction. There are also plenty of companies using deep learning for mobile apps so your background here could be really useful.",1534709950.0
iamiamwhoami,You could familiarize yourself with the android and iPhone machine learning and deep learning sdks. There’s going to be a huge market for deploying ML models in mobile phones in the next few years. I think it’s also a pretty niche skill set. ,1534713671.0
ai_is_matrix_mult,"I think implementing papers is a great idea. And publish the implementation on your github. Find a paper that interests you, and isn't implemented in a specific language (i.e., in keras but not pytorch, etc) implement it and post it on your github. You might not have work experience in DL yet, but if you have some working repostories that people use, it would say a lot. Also, consider going back to school for M.Sc. good luck!",1535152663.0
dt_magic,[https://www.youtube.com/watch?v=xtxxLWZznBI](https://www.youtube.com/watch?v=xtxxLWZznBI),1534645310.0
Phnyx,"Question is, why not use a regular GPU for this? If it's only 5 layers the training shouldn't take forever and inference could even be done on CPU.

How much data are we talking about here? And it's there a reason you want to go with 5 layers? Did you consider modifying an existing architecture like ResNet?",1534569763.0
Karyo_Ten,"Does the FPGA support OpenCL, if yes you can try running Caffe-OpenCL on it and you would only have deployment issues.

Otherwise forget it, use a GPU, especially if you don't know how to write an efficient Matrix Multiplication (GEMM) from scratch in C, Assembly, OpenCL or CUDA.

You will need to write it for your FPGA and this takes a while.

If you're curious how, here are some tutorials:

- C: http://apfel.mathematik.uni-ulm.de/~lehn/sghpc/gemm/
- OpenCL: https://cnugteren.github.io/tutorial/pages/page1.html
- Cuda Maxwell Assembly (GTX980) : https://github.com/NervanaSystems/maxas/wiki/SGEMM
- Cuda C++: https://github.com/NVIDIA/cutlass

My own implementation in pure Nim is available [here in my tensor library](https://github.com/mratsim/Arraymancer/tree/master/src/tensor/fallback) written from scratch. ([Nim is a young language](https://nim-lang.org/) with the syntax of Python, with types, and the speed of C) ",1534579122.0
r3drocket,"So supposidly you can do Opencl to FPGA, but I might take a look at Intel's movidious offerings - they have the tools to do deep learning model conversion and the hardware is relatively cheap and available. 

[https://developer.movidius.com/](https://developer.movidius.com/)

Essentially movidius is optimized for vision based deep learning and there are tools to convert existing deep learning models to it.  It's meant primarily for vision based deep learning tasks. 

I have a movidius usb stick  (\~ 80$ on Amazon) sitting on my desk and I should really see if I can abuse it to do rnn text classification.

Also intel is talking about integrating the movidius on laptops for deep learning acceleration. ",1534606104.0
rozgo,"First you stand on the shoulders of giants. Then you add or remove a node. The next person, after you, does the same.",1534563999.0
riku_iki,"You can start with simple model, and iterate by adding more advanced features, making sure that on each step you have model improvement.",1534555558.0
potcmotc,!remindme 5 days,1534553208.0
Arkhaya,I'm doing the same thing except using making j and z dynamic instead of static.,1534549350.0
E-3_A-0H2_D-0_D-2,Wasn't that the whole purpose of Deep Learning?,1534528568.0
ResidualRisk,Garbage. Not a single useful bit of info.,1534752640.0
eleitl,Did you try the docs? E.g. https://github.com/tesseract-ocr/tesseract/wiki/NeuralNetsInTesseract4.00 ?,1534421704.0
EnderTaco,"Definitely an interest read, might try it on my own models to see how it works for myself",1534397783.0
siyideng,"No Dropout, yes. BN instead? It makes my val acc drop 10%!",1535492738.0
UHMWPE,"You wouldn’t add a neural network layer to accomplish this per se, you would apply some transformations to your dataset (augmenting it) in a way that would cover the different types of images you would expect in your test set ",1534322535.0
somewittyalias,"A neural net could pick up detail you would not see with the naked eye.

However for image processing tasks, people *always* do *random* image transformations.  But the transformation is not done by neural net layers, but in python code before sending the image to the neural net.  One reason is that if you have only 200 images, a neural has so many parameters that it can ""memorize"" your images and get perfect answers on them, but fail on anything else (overfitting).  A related reason is that you want your network to generalize better, so you give it a wider variety of images, with colors and illumination which it does not normally see in the training data set.

You should look on the web to find examples of the transformations people use for images for the framework you are using, and also transformations used for cell cultures, which are probably different from transformations used on normal images.  There are many types of transformations: changing the illumination, constrast, colors, etc.  One that is nearly always use is that if you have, say 1000x1000 pixels images, you pick a random box of 800x800 pixels and use that as your image; you pick a different random box every time you reuse the image.  One transformation that is not usually done on normal image is rotation, or if it's done, it's only a few degrees, because you don't want your network to learn to recognize an upsidedown car.  However for cell cultures, you can rotate at any angle.",1534337197.0
Harrisonjansma,"Looks good! The code you put will create the above architecture. Each convolutional layer will have a ReLU nonlinear activation.

If you want ton improve this, you can try stacked convolutions. They give your model more processing power by allowing the network to learn more complex features. A standard stacked conv block looks like this.

model.add(Conv2D(256, (3,3) acitvation = 'relu', padding='same')

model.add(Conv2D(256, (3,3) acitvation = 'relu', padding='same')

model.add(MaxPooling2D((2, 2)))",1534360860.0
rylaco,"I am also planning on taking Fast.ai course. I saw Ng's course lacked GAN and RL. I took CS231N, available on YouTube because Fast.ai wasn't offering course using pytorch back then. You may like to watch last few videos of CS231N, they clear your theoretical understanding about popular techniques in DL.",1534299972.0
Phnyx,"The two PyTorch based courses from fast.ai will cover everything you need for now.

The first part was recorded a year ago so there are some new parts to the library and general processes by now. Work through the first part, then check the forums and most popular tweets by Jeremy Howard from the last year to see what was added.",1534314594.0
dork,artificial intelligence does not exist yet,1534262646.0
jaycrossler,"TL;DR:  for 40 years, people have thought AI would be huge. In the last two years, computing power and large data sets and better algorithms have finally created a tipping point where Deep Learning and other related algorithms can now solve very complex prom lens, and people who understand these techniques are finally making big money.  ",1534293450.0
somewittyalias,"It sounds fine.  But I assume you will use attention to compare to a different sequence, which is likely not of length 253.",1534209590.0
iamiamwhoami,"It looks Keras is complaining because you're specifying a 2d input shape and it's expecting a 3d one. Try adding an extra dimension to the input_shape parameter. The dimensions of the input vector should signify (bath_size, sequence_length, data_dimension).",1534139908.0
beanblabla,"You need to fit with shape (samples, timesteps, features) --> 3d array",1534140974.0
mankadronit,Set return_sequences to true,1534165984.0
free_reader,"You need to add return_sequences=True in first two RNN layers.
",1534154453.0
Phnyx,"I don't know about your dataset but let's take the Titanic dataset as an example instead.

Before your fill in missing values, create a new column with a 0/1 value to keep track of missing data from the source. If you impute and overwrite your existing column, your model will have no way of knowing if the value was missing and there might be a good reason why it was missing in the first place.

Let's say you have 30% of age values missing. Setting these to 999 will mess with the normalized values (regular values would be between 0.0 and 0.08 with the missing ones at 1).

Setting them to -1 is slightly better but still leaves too many out of the regular range. 

Setting all to average 30 gets them into range but leaves too many at 30 in total.

Instead of just using the total mean, you could use the mean of groupby ""class"", which is more intuitive and accurate to the actual values (1st class is usually older, 3rd class younger). You can also do groupby's with class, gender and more columns.

This would be your own, very simple classifier/regressor so why not use a real model for this? Use all existing data (original train+test where column is not null as your new train set, all rows with the missing column-values as your new test set and the specific column as your target. Use a bit more regularization than usual and fill in the missing values with the predictions.

This is not perfect but it can help neural networks as they work better with data in a normal distribution. For a tree based model, you don't really need any normalization and they often work with missing data right away.

Filling in missing data with a very simple average is too inaccurate. Using a very detailed model might create target leakage/overfitting so make sure to use good regularization.",1534108978.0
mdv1973,"1. Taking the average of the column values may be okay if there not too many NaNs to fill. You want to avoid messing up the distribution of values for that column and messing up correlations with other columns.
2. There are various methods out there for imputing missing values, some simple (like take mean or median) and some more complex (like matrix factorisation). You can get some [here](https://github.com/iskandr/fancyimpute). Personally, I study the data first to see if I can come up with a good method based on common sense or domain-specific knowledge before going with pure number crunching approaches.
3. You look for an impute strategy and apply it using the training data. This should get you a value or function that you can later on apply to new data such as a test set (or a single new prediction request).",1534135930.0
somewittyalias,"You could simply add dropout to the input vector.  This is frequently done even when there are no missing values.  Dropout randomly sets some values to zero and forces the network to learn to implicitly imputate the full input vector.

I have never seen this used for missing value imputation, but I don't see why it would not work.",1534112239.0
hockeydennis,0,1534125986.0
cognitivedemons,"Have you considered Google Colab with a free GPU? If you just want to study DL, I would suggest starting with Colab.
It not the depth of the DL models that requires the high end gpus but the data. If you need to learn DL, you can play with low res pictures in small batches. However, with 1050, you will not be really competitive, for instance, in the vast majority of kaggle competitions.",1534096592.0
zoopi4,No idea if it's good enough but if you are gonna buy it you should probably look into the Y530 it's this years versions it's a bit more expensive but the cpu is a lot better.,1534085383.0
Arkhaya,What kind of data and model you working with. I'm doing image classification. So I need a really powerful GPU.,1534087232.0
Razmyar,"If you have a choice, go with at least 1060 6GB. It has a different architecture than 1050 which makes it more suitable for deep learning models. ",1534087874.0
Spenhouet,"Why do people always think about buying a Laptop for ML? It is stupid.No, a GTX 1050 doesn't cut it. Just buy a laptop with a good CPU and without dedicated GPU and you will be fine running small models on you laptop. For bigger models you will need a desktop PC with a desktop GPU GTX 1080 or better. The best way ist to use the GPU cluster your university provides. My university has a 50 x GTX 1080ti cluster.",1534136021.0
tecra1776,"Be advised, most if not all laptops that have cards like this, are mobile counterparts to the ""real"" card and this is understood by referencing the model name with M, for mobile. The card is not the same performance wise as you're expecting from a desktop version of the same card",1534084965.0
Iknowyourusernames,Basically my entire phd career ,1534094410.0
i_build_minds,"In general, yes. It may be worth asking yourself why it’s not publishable, though. Generally the publishing process is there to help others gain access to your ideas, but it’s also there to encourage coherent, consistent results from authors too. 

(Obviously, work still needs to be done there, but it is a goal nonetheless!)

Following a more focused goal to getting your ideas “camera ready” will likely have benefits for you, and you can point people toward your papers with confidence.",1534081276.0
caz-,"Do you have published researchers to help you with this? If you've never published before, it's easy to underestimate how much fine tuning you need. I say this from experience. I am naturally good at technical writing (this is not just my opinion, as my PhD supervised with decades of experience said that I was in the top 5% or so of writing ability out of all the students they've supervised), and when I wrote my first paper, I expected maybe they'd spot a few typos when they read over it for me. I was disheartened when I got the draft back with dozens of comments on each page from each of my supervisors and it took weeks, and maybe three or four revisions, to get it into a state they were happy with.

The fact is that producing publication quality technical writing requires an extremely high level of refinement. It doesn't matter how good you are at writing or the field you are writing about; no person will know how to put it together without significant help from people who have gone through the process many times. Even when writing a single author paper, I still get more experienced researchers to look over it for me, and they always find ambiguities, technical falsehoods, and other such issues. If you write a paper by yourself and submit it without having other experts look at it, it will get torn to shreds by referees if it makes it past the editor.

As for publishing on arXiv. This should be for when you are waiting during the peer-review process *or* when you have published many times, and have a manuscript that you know to be high quality (with confirmation from at least a couple of other researchers in your field), but for some reason choose not to publish in a peer-reviewed journal. It should not be used to publish writing that you think is not good enough, or not novel enough, to make it through peer-review.

All that said, don't let it put you off doing the writing itself. It will help you immensely in understanding your own work, and you'll have practice for when you go on to your masters. If you have people who can help you now, great, although the fact that you're asking here rather than asking them suggests you might not. If you don't, write the manuscript, and maybe you can get someone to help you polish it up soon.",1534113414.0
PristineJack,"Sure, go ahead and do not hesitate to start writing! This will be a good exercise to structure your ideas and present your models and ideas in an understandable way for everyone. 

Also if you plan on continuing in the field this will be good practice for next projects and internships. If this is your first experience in writing you will probably struggle with the form so it's totally worth practicing.",1534087837.0
utkarshmttl,"You know there are journals/conferences that do not exactly demand novelty if you have reproducibility. Just an example, IPOL (image processing on line) is one. Other local conferences will also accept papers that are not entirely novel, a minor tweak will work even if your results are slightly lower than SOTA. So always write a paper and try to submit it. Find the best fits and start applying. ",1534101320.0
shakyshamrock,"My professor suggested a repro paper is appreciated as a workshop paper. To be more high-minded, reproduction is of course an essential part of the scientific process. The question is which workshop might be relevant.",1534126698.0
,"Being a beginner myself in the realm of DL, I'd volunteer!",1534059068.0
maveric_karan,me being a beginner as well would like to volunteer,1534059965.0
Lusfer21,I’m in! Let’s study together!,1534062487.0
Kishaan92,I'm in! ,1534063904.0
utkarshmttl,Me too,1534064769.0
nshmadhani,I am in ,1534067403.0
rojoko435,I'm in ! ,1534070115.0
square-1111,Me too,1534070511.0
vbvjain,I am in too. ,1534076571.0
EulerRulz,And my axe,1534077515.0
Soy-Michu,Looks great,1534083248.0
ariyanhasan,I am in... ,1534085970.0
Bexirt,I am in buddy.But wait is it just the specialisation or the course.I am not buying the course now just the lectres cus I am broke af :\\,1534088132.0
DhulipalaPranav,I'm in,1534089754.0
aiblockchain,I'm in too! Message me how we can do it together!,1534101971.0
_rascal,Same!,1534110509.0
bo123x,"Hi guys, join the slack group here [https://join.slack.com/t/dlchampions/shared\_invite/enQtNDE4Nzk0OTgxNjM1LTU0YTRiYjFmMjg0ZDVjMGU1MDRlZmE3MzdjMWU3MGU4MTY4N2EyZjgxNmZkZjRjYzVkNWVmOTZkMTc2Zjc0MWI](https://join.slack.com/t/dlchampions/shared_invite/enQtNDE4Nzk0OTgxNjM1LTU0YTRiYjFmMjg0ZDVjMGU1MDRlZmE3MzdjMWU3MGU4MTY4N2EyZjgxNmZkZjRjYzVkNWVmOTZkMTc2Zjc0MWI)",1534110720.0
umangkeshri,Hi I'm in.,1534140093.0
pcidev,I'm in .,1534158836.0
void_gear,I'd like to join.,1534395441.0
csnerdapple,how to join?,1534740601.0
afdaniele,"I joined the Duckietown team one year ago when I served as a TA for the Duckietown class at TTI-Chicago. It was a great experience for everybody, instructors, TAs, and students. I'd be more than happy to answer any question.",1534033870.0
stratanis,"I've been involved in the organization of the AI-DO and development of the hardware platform that will be used for the competition: Duckietown. 

I'll be happy to answer any question!

[Check out our site](https://www.duckietown.org)

[Join our community](https://www.duckietown.org/site/register)

[Learn how to use Duckietown to join the AI Driving Olympics robotics competition at NIPS and ICRA](https://www.duckietown.org/research/ai-driving-olympics)",1534026951.0
MartianTomato,"If you're having this issue (classifier is outputting 1/#Classes) it's likely (1) a bug in your code, (2) bad hyperparameters / exploding gradients, or (3) you're trying to make predictions about something unpredictable like stock prices. ",1533987677.0
bo0mb0om,"With most ml models, you output a real number, not a 1/0. Measuring accuracy directly is pointless for classification. Also, different models will have different optimal decision if thresholds. This is why losses like cross entropy are used and then the final classifier is selected using something like best f score.",1533979759.0
adikhad,"Plot 2 types of objects on a graph. Eg. We want to plot apples and Blue berries based on color and size ( x axis - color)( y axis - diameter of the fruit) and mark circles if the sample is an apple, square if it's a blue berry. If we can draw a straight line that will split the graph such that one side of the line only contains apples and other blue berries, we can say that the data is linearly separable.",1533973589.0
Phnyx,"Let's say you have coordinates of cinemas in New York. Half of them are in Manhattan and half are in Queens. Can you draw a straight/linear line on a map separating them in two classes? Yes, should be rather easy. And so can a simple neural network that just needs to learn to distinguish East and West.

Now let's say you have the same cinemas but you want to separate them by having more than 4 screens in total. There are some big and some smaller cinemas in both areas of the city and you can't just draw a straight line on a map to separate them. There might be some clusters or other patterns but to find these you need a more complex classifier than just a linear model.",1533973897.0
Foolprof,"2 sets of points in a vector space are linearly separable if for some hyperplane all points in the first set lie on one side of the hyperplane and the rest lie on the other.

Btw, this is still called “linear” because in higher dimensions, hyperplanes divide space into two parts, just like a line divides a plane in 2D.",1533976557.0
necroforest,What,1533944950.0
as_ninja6,what does this have to do with deep learning and what is the restriction??,1533958967.0
GotRedditFever,Initially random numhers close to zero are assigned and as the model learns the weights are tuned,1533924202.0
as_ninja6,"There can be multiple ways to initialize the weights. It can be zeros, large random numbers, small random numbers. But each have their disadvantages. The current best one is the Xavier initialization which is also random but works well.",1533958797.0
ady_anr,"Weights play a huge role in attaining minima quickly and efficiently. We might think ""duh, what difference is it going to make"" . But it really does. And by a lot.
Read this blog to exactly know how.
“Random Initialization For Neural Networks : A Thing Of The Past” @ady_anr_ https://towardsdatascience.com/random-initialization-for-neural-networks-a-thing-of-the-past-bfcdd806bf9e",1535384104.0
Arkhaya,"Weights depend on how fast you want the system to learn. You can make it real slow or really fast.

If you make it really slow then it will take a lot of time

If you make it too fast, you might miss the minimum.",1533944374.0
howard_without_the_h,I just bought your app as I’m interested in learning. What would you recommend the next step would be after going over all your lesson plans? I really want to learn how to program a simple AI algorithm to gain experience as well. ,1533918211.0
cthorrez,"Most machine learning algorithms make some assumptions about the data they are trying to model. The main one for most predictive algorithms is that the data is drawn I.I.D. from some unknown distribution.

In reinforcement learning this is violated since the next state is highly dependent on the previous state so training examples for the neural net are no longer independent. By using a replay buffer you see shuffling your observed data and training on random samples thus removing the correlated between examples in the same batch",1533914712.0
MartianTomato,"It has little to do with statistics and everything to do with the way DQN does function approximation. Function approximation means that we compute the values of different states using the same parameter set (i.e., the neural network weights), which means that updating the value of one state changes the values of (potentially) all states. 

Now states that are close in time usually have similar features/values, which means that their values are computed similarly, which means that a value update to one of them usually has the same (""correlated"") effect on the other.

So here the intuition with divergence with DQN/function approximation: the 1-step Bellman update updates state 1 with the value of state 2... suppose the value for state 1 goes up... But by the above discussion, the value of state 2 also goes up... so the update moved our target. But things are worse, because state 3 is also pretty close to state 1, and it's likely the value of state 3 also went up. So now next step, we move from 2 to 3, and update state 2 using the target at state 3. Again state 2's value goes up, but so do the values of states 1 and 3 (and 4,5,6 etc.). An agent that keeps cycling 1-2-3 now may never convergence. How do we fix this feedback loop? See the quote in OP. Also having a separate target network. ",1533900592.0
konddmy,"Fake it til make it. No one actually understands statistics.

As per the case, I’d recommend to learn RL separately from NN - they’re completely orthogonal.

And diminishing of *useful* “short-term” memory over time is more related to RNN than RL (see [gradient vanishing](https://en.m.wikipedia.org/wiki/Vanishing_gradient_problem)  for instance). It also often (but not always) more computational problem than statistical.

———

As a metaphor, humans (animals) have short-term memory traces for 24-hours along with long-term “experience buffer” (one is transferred to the other in sleep among other processes, see [memory consolidation](https://en.m.wikipedia.org/wiki/Memory_consolidation?wprov=sfti1)). The metaphorical [moral] reason from RL-perspective is that highly correlated (repetitive) morning-to-evening experiences bloat the memory with unnecessary details*, so we have to forget that over-detailed noise (thus regularize) during sleep etc. RNN/LSTM perspective is similar, it just concerns shorter timespans in practice (like during speech recognition).


*Some remarks:*

- *highly correlated doesn’t mean perfectly correlated, thus additional problem of “paying too much attention” to tiny differences (noisy variance) between correlated events in updates; as well as “panicking” on large differences between unrelated ones.*

- *The computational part of it often leads to amplification of such noise (feedbacks in back-prop, exploding gradients etc) and/or forgetting useful signal (due to rounding errors network can even stop its training, like with vanished gradients for example).*

- *In RL case this is even more obvious even without introducing gradient-based methods explicitly: if the exact action brings a reward for some time - agent gets “obsessed” with that optima and doesn’t really try anything new. So when “resource is depleted” (which looks like a large negative “update” on reward itself [in response to a learned set of actions]) - it doesn’t know what to do.*

————

TLDR: Basically “high variance” means overfitting here with emphasis on “noisy variance” in gradients/updates, not overall statistical variance in the data. So, “forgetful” random sampling just brings  some regularization and endorses explorative behavior.",1533899824.0
trickyLoop,"I’m pretty new to ML (so new that I haven’t taken any courses or really done anything using ML) so take this with a grain of salt, or wait for a more experienced person to correct me.     


Imagine you have a two minute memory, and you are trying to learn how to play basketball.     


You spend a bunch of time learning how to shoot the ball, and over time, you get better and better. This works alright, because you can remember how you shot the ball within the past two minutes.     


Then you start learning how to pass the ball, and over time you get better and better. Then you go back to trying to shoot, but it turns out you’ve forgotten how to shoot because of your two minute memory. Bummer.     


So how do we fix this? Sometimes you get to remember a random thing you did in the past (experience replay). Now, even though you have a two minute memory, you sometimes remember how to shoot the ball, so you never really lose the skill!     


This is a big oversimplification, but maybe it helps a little bit.",1533899098.0
tulerworld,"This is why I start to get annoyed by the ai community.

Knowingly misrepresting what these algorithms do. 

The article unveils nothing new, only describes that some company is doing supervised learning with emotions as classes...",1533880863.0
somewittyalias,"It seems to me it is a ""cheap"" way to do attention.  So it's not that relevant now that we have attention.

> So for example, instead of mapping the sentence a, b, c to the sentence α, β, γ, the LSTM is asked to map c, b, a to α, β, γ, where α, β, γ is the translation of a, b, c. This way, a is in close proximity to α, b is fairly close to β, and so on, a fact that makes it easy for SGD to “establish communication” between the input and the output. We found this simple data transformation to greatly improve the performance of the LSTM.

I guess this only work for pair of languages which have similar word ordering.  For French/English, the agent is at the start most of the time, so the agent would be ""a"" in English and ""α"" in French.  Possibly this would allow the network to quickly make a meaningful connection on most sentences, and then you hope that it helps a lot and training the whole thing is easier.  

From glancing at the paper quickly, this is what I think they are expressing.  But as with everything related to deep learning, it's probably just something they observed experimentally and then they tried to find an explanation.",1533829926.0
pcidev,What the intuition for that ?  Also of someone could provide mathematical justification that would be great and helpful ;)  ,1533825008.0
Karyo_Ten,"Was he working with Japanese or German? :P.

That certainly sounds strange and I would love to have benchmarks.",1533829221.0
YogSc,"You can use a CNN to classify images where there is a human Vs where there isn't one. You don't need to build a new model, ResNet/ResNeXT should be enough. ",1533824641.0
Infima,"Only partially answering question: make sure you have or can realistically get training data for whatever you choose your output to be.
",1533825778.0
HumbBest,"OK, what about using pre-trained DNNs? ",1533826846.0
pcidev,I think you can use stacked LSTM and conv net. ,1533829190.0
as_ninja6,"if you just wanna do human or not Then the model can just be a binary classification and transfer learning on the pre trained model would be fine.

But if you need to count the persons then object detection would be the only choice since segmentation approaches just give single label for multiple objects",1533833364.0
UndeadPandamonium,"You don’t make the output length a variable because even if you do, it will be fixed after training. What you do instead is create a stop condition and train the network to always output the stop condition as it’s last output. That way you get variable length without explicitly defining what that length is.


Edit: you can create the stop condition by just increasing the length of your OH vector by one and calling the last encoding the stop condition",1533798598.0
rajun54123,"What do you mean what type of model? You just said you wanted to use ""deep learning"" which uses neural networks. Personally it doesn't sound like this problem needs that at all. I'd start with a logistic regression which would probably work just fine for this use case.
Edit: to clarify logistic regression is considered a simple neural network. You can make it more complex by adding more layers, etc. if you are trying to do image recognition which it sounds like you are, there are some pretty good tools for computer vision using deep networks, but you'll have to get your hands dirty to find them.",1533784652.0
Arkhaya,"Your problem can be solved with machine learning. Deep learning is for more complex stuff rather than just no.

You can use linear regression, multiple linear regression. If your data is labelled. If its not and you want to create a dissection, you can use k-means.

Research on types of machine learning methods cause there are 4 and see which best suits you. (Basically supervised and unsupervised learning etc.)",1533818475.0
Phnyx,Yes,1533760495.0
persistence_hunter,Yes,1533764234.0
mhummel,"You should post this question on r/learnmachinelearning for a better answer, but you're neglecting the matrix of weights. The activation function has two purposes: 1) Providing non-linearity; 2) Constraining the output to a specific range of values. (e.g. if you need probabilities, selecting tanh as your output activation function isn't going to work so well). This constraint together with a good weight configuration is what makes the network produce output applicable to the task. So different tasks can use similar activation functions because it's just one component of a NN. 
 ",1533848747.0
carlthome,Fun! Curious how it compares to https://arxiv.org/abs/1605.06921,1533746489.0
akaysh,Does it have any useful application in real life? It should be fun in using this for something like suggesting dance moves for beginners.,1533745319.0
HairyIndianDude,GitHub Link: https://github.com/jsn5/dancenet,1533740737.0
Arkhaya,"What exactly do you need a suggestion for. 

The kind of project or how to fix and issue?",1533741439.0
PapaCreameritus,[Here you go](http://lmgtfy.com/?q=ml+projects+for+beginners),1533742234.0
VolatilePiper,What's your expertise level in the field?,1533747327.0
Arkhaya,"I hated the proccess of using tensorflow GPU or the normal version on Ubuntu. It seems so complex to do.

I was easily able to place it all up in Windows.",1533741552.0
amikelive,"thanks for the article. from our experience, it would be better to install CUDA with the network deb packages. Using run file may bring some unwanted effect to the system later on.

we have series of blog posts for installing machine learning system on ubuntu. for CUDA installation itself, here is the article: [https://tech.amikelive.com/node-679/quick-tip-installing-cuda-deep-neural-network-7-cudnn-7-x-library-for-cuda-toolkit-9-1-on-ubuntu-16-04/](https://tech.amikelive.com/node-679/quick-tip-installing-cuda-deep-neural-network-7-cudnn-7-x-library-for-cuda-toolkit-9-1-on-ubuntu-16-04/)",1533790545.0
E-3_A-0H2_D-0_D-2,"Not sure if this is what you want, but your input could be a (b, t, v) tensor {where b=batch size, t=number of timesteps, v=vocabulary} and your output can be a (a,1) vector where a is your action space. At every timestep's output, the LSTM would output a corresponding prediction on which action should be taken.

I think you should really read about and try the Attention architecture. ",1533724527.0
JuliRio,"As far as I understand what you want to achieve, I don't see the need to use LSTMs. LSTMs can learn to map input sequences to output sequences. The great thing about them is that they exploit relationships between the elements of a sequence. I think you're only interested in the presence or absence of certain words, not in relationships between the words. 

How about training a simple feed-forward network with output classes \[""my account"", ""bank"", ""balance"",...\] and classifying the words of a sentence individually? If a word leads to a sufficiently high activation of an output neuron, you can add the class of the output neuron to your list of key words. Words like ""show"" would not lead to a sufficiently high activation of an output neuron and are ignored therefore (I am nor sure whether this part is going to work in practice, if not, maybe one should have a look at open set recognition). 

Training such a network would require training data of the form (""outstanding balances"", ""balance""), (""balances"", ""balance""), (""my accounts"", ""account"") and so on. I did not understand how exactly your training data looks like...",1533744735.0
Alt_578,Following,1533723832.0
Karyo_Ten,"Gradients are multiplied by learning rate and subtracted to original value for stochastic gradient descent.

Then you have different optimizers (adagrad, rmsprop, Adam, sgd with Nesterov,... ) which optimize convergence thanks to a momentum term. ",1533660685.0
HyperbolicInvective,Interested,1533657926.0
sahilzingh,"In
",1533660281.0
notsoslimshaddy91,Intrested. ,1533661740.0
prateek0001,Interested,1533661827.0
DemiourgosD,How can I join? Group chat somewhere?,1533665762.0
oopsleon,Interested.,1533668564.0
JustEnoughTuna,Interested!,1533683588.0
Always_endsup_drivel,Interested,1533688106.0
Much_Mediocre,I'm a bit of a novice but am interested as well.,1533690244.0
TheeNinjaa,In. Is there a discord for discussion?,1533694099.0
RnabSanyal,Interested!,1533695290.0
stn994,Interested.,1533699707.0
as_ninja6,Interested. I've searching something like this for a long time. I've been thinking about action based RNNs I don't know whether they exist or not but I there's one I would love to work on that.,1533702620.0
DVDplayr,This is great! Thanks for making it happen!,1533705005.0
inquizeative,In,1533706708.0
wnhkcs,Interested!,1533707806.0
__illuminaughty,"Interested. But also, is there anything similar for Vision too? I'll be very much interested. Anyone?",1533729134.0
pcidev,"If it exists please let me know in comments.
Thanks :)",1533657433.0
evilpineappleAI,I am doing this for work. PM?,1533657847.0
pcidev,"We can start a subreddit and GitHub repo. for code. 

Can any one can suggest good research paper to start?",1533659199.0
pcidev,"I was thinking of [this](https://arxiv.org/abs/1803.01271) as a staring point. We could try to implement TCN.   
If anyone has better suggestion please comment.   


Read this, in case of any doubts ask in this thread. We will discuss this paper next week . Till then try to implement it. ",1533661185.0
pcidev,"Has anyone started reading ???   
What about Sequence model ?  
Do you know sequence model ?   
Why author asked to reconsider RNN as the base model for sequence modeling ?

Try to answer these question :)

Are you familiar with RNN CNN oir TCN,  if not let's discuss in r/NLP_readingGroup  
",1533741952.0
PTInvader,"I use it. It's fasttttttt, not sure relative to the 1080 though. Huge step up from the K80 (almost 10x) ",1533619178.0
Phnyx,"Take a look at the fast.ai benchmarks.. it's quite a bit faster, especially for larger datasets.",1533618503.0
maheshmaceee,"Try it out with Google cloud, it takes a day to get approved. I would say it completely depends on the model. If you have a smaller model I would suggest you to use 1080. But why not try V100 when Google is offering it for free",1533620324.0
tlkh,"GCP free credit $300, but you do need to place a ($70?) deposit to get access to a GPU. 

V100 is about typically about 30-40% faster than a 1080 Ti all else being equal, but it really depends on the batch size and the model (and if your framework makes use of the Tensor Cores)

Your mileage might vary quite a bit. ",1533621987.0
Spenhouet,Not answering your question but wouldn't it be cheaper to use a Titan V?,1533635957.0
MasterSama,"[1.It](https://1.It) depends, the summation is used to fuse two feature-maps together and get 1 resulting featuremap. in concatenation, you are just concatenating feature-maps and feeding them to the next layer. it means I want both streams of data to be processed in the next stage, whereas in summation you are implying, I am going to use these featuremaps and make them more pronounced!(they are of the same essence). the performance differs, clearly, concatenation results in the number of output feature-maps to be doubled and thus increases the overhead. 

Apart from that, the underlying reason may differ, for example in Residual connections, that summation is a part of the equation which needs to exists in order for the whole scenario to work(i.e. succeeding in mapping an identify function), but in Dense Block they are simply information pools for the upper layers and thus no summation is done. 

[2.It](https://2.It) is not necessary per say, if you want to decrease the flops  you usually do that. you also need to note that applying 1x1 in early layers of your network adversely affects your performance. This is more pronounced and visible if you have less data. in scenarios such as ImageNet this is still true, however not that pronounced. This usually heavily affects smaller datasets.",1533631227.0
darylflx,Bump,1534454736.0
fishscaleyu,[https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html#models-with-normalizing-flows](https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html#models-with-normalizing-flows) this article has the proof,1541400533.0
klingon33333,"It is very rare that you will have a novel problem set that has no related work and you will be able to tell ahead of time that you need a deep learning approach. Even if you are in a situation where your problem is very similar to another approach that uses deep learning there is still not a good way to know.

Now with that being said, assuming you have data to support deep learning approaches first you should try simple models (eg. try and solve your problem using a simple linear model). If the simple models don't work then start slowly adding complexity with each development cycle while also demonstrating that you are not overfitting. If you aren't overfitting and your models become more performant with complexity you can increase your model complexity more and more. As you do this there may be approaches that start to become deep, there may be approaches that become non-neural, or you will maybe be able to represent your problem differently to and thus be able to attack it in a totally different manner. Its all highly situational. 

People are fairly bad at estimating how complex a certain classification task will be except in very simple cases. If we could do this effectively then the task of finding the best model for a given problem wouldn't be such a large research area.

In terms of finding tests to run, the best I can recommend is attempting the problem with a linear model and then ramping up complexity while fighting overfitting. The task of determining what kind of model to use can be automated to an extent ([Neural Architecture Search](https://arxiv.org/abs/1611.01578), etc) but unless you are able to implement these things on your own you are somewhat stuck. 

I wish there was something like a statistical T-test that indicated how deep a model should be given your data but I don't personally see that happening anytime soon. 

TL;DR: Assuming you have lots of data start shallow and if it works then you are done, [if it doesn't work then just add more layers](https://qph.fs.quoracdn.net/main-qimg-2b37a6e5348538da6aad32fb3f4a7370.webp) 

",1533578484.0
VivaciousAI,"In my professional experience, deep learning is only used for very complicated problems such as creation of word2vec/doc2vec vectors (if not using gensim due to licensing issues), image classification, semantic analysis, etc. If the problem is too simple, it's very easy to overfit. 

From what I've seen deep learning on normal problems doesn't add too much. There's also ML algorithms that deal with non-linearity such as random forests. You can even use linear algorithms on non-linear problems with feature creation.

There's even instances of using basic ML algorithms on top of DL algorithms to improve. Deep learning shouldn't be considered as the end all solution. Unless it's a complicated problem, I've seen multiple instances where a DL algorithm performed (AT BEST) the same as something like a random forest algorithm and even a naïve-bayes.

The usefulness of DL algorithms in image classification is that it is able to create features for you. Images are complicated datasets and CNNs tend to be very good at creating features themselves when applying multiple filters.",1533575238.0
intvar,"Deep learning is very broad. So I don't think we can have a test to see if deep learning is a good solution. However, there is tons of empirical knowledge about deep learning. I will try to list some of the points that helped me in designing solutions.

1. What is the state of the art for problem X : This is the MOST IMPORTANT factor . Most likely you might be working on a problem which is already being attempted by many researchers in your community. Read papers to see what is the state of the art. If state of the art is SVM, start with SVM .
2. Data set size: If your data is not large, deep learning models might not be the right choice. It's very likely to overfit.
3. Cost: Deep learning cost money and time. I work with LSTMs and often question ""Do I really need to train my model for 2 days for 4% improvement over a much simpler model? "".  If you can afford that 4%, it's great, otherwise stick to simpler models.

Hope it helps.",1533575991.0
Rezo-Acken,I only know basics about GAN so bear with me but aren't you confusing the objectives of the descriminator vs the one for Generator ? The goal of generator is to maximize similarity between fake and real so Lc while the descriminator goal is to make correct classification so minimize Ls. Hope my answer puts you on track...,1533571124.0
somewittyalias,"The total loss for AC-GAN, eq. (2) + eq. (3) or L_S + L_C, is the loss which the *discriminator* tries to maximize.  The part which is not clear in the paper is when they then write: ""G is trained to maximize L_C − L_S"".  But G is only affecting the second part of each of those equations related to the fake images.

The discriminator has two tasks: discriminate fake/real and classify the object class C.

The generator also has two tasks: *fool* the discriminator (minimize L_S for fake images, which is equivalent to maximize -L_S) and *help* the discriminator correctly classify object class C (maximize L_C) on fake images.  There is an *adversarial* relationship between the discriminator and generator when trying to discriminate fake/real, but they are *cooperating* for the object classification task.",1533573566.0
davidc9320,"There is nothing wrong in borrowing a dataset from another work! Actually in research the most common thing to do is to test new methods on well-known public datasets, so that anybody can have the opportunity to recreate the experiments or propose a new method on the same data.

[Kaggle](https://www.kaggle.com/) might be a good starting point to get the inspiration for your project!",1533627051.0
davidc9320,I think [TensorFlow Mobile](https://www.tensorflow.org/mobile/mobile_intro) might be what you are looking for :),1533626746.0
Atlasi,Try Google Colab,1533558857.0
fahimk1998,"Since you haven't mentioned your OS, I would suggest first install Conda and then create a virtual environment and install Tensorflow.",1533562998.0
charl3sworth,I have never used it but doesn't Google's version of ipython notebook (I forget the name) come with some compute time for free with a pretty good GPU? Seems a good way to get started for free.,1533554693.0
utkarshmttl,Diabetic Retinopathy has tons of papers! Just go to google scholar and search diabetic Retinopathy deep learning? Or try Github and most repos will provide the links of underlying papers? This is a very commom topic in DL. ,1533498672.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/deeplearningpapers] [Some help please!](https://www.reddit.com/r/DeepLearningPapers/comments/94u9o1/some_help_please/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1533496038.0
grjh,"There was a kaggle competition three years ago on this topic. The winner’s solution forum post could give you interesting references too.

https://www.kaggle.com/c/diabetic-retinopathy-detection",1533512462.0
,[removed],1533487116.0
bubblride,GIFs are just little uncompressed videos. Every frame is a picture,1533411942.0
bubblride,"Nope. Frame Rates are part of the art. Low frame rates gives you that shitty sluggish amateurish look that you always wanted. High frame rates are more smooth and the GIF creator completely missed the point of compression, e.g.mp4. 
You can an use exiftool (bash) to extract the frame rate from a GIF file. You might use it as feature bcoz it tell you something about that look.",1533446860.0
_B_L_A_N_K_,I’m really interested in this thanks for sharing ,1533388715.0
sinefine,This is awesome! Subbed,1533401840.0
yuxiaojian01,That's great!,1533435397.0
aminhotob,That is good. Thanks for sharing,1533504292.0
,[deleted],1533365018.0
JuliRio,"It is explained very well here (p. 328):

 [https://www.deeplearningbook.org/contents/convnets.html](https://www.deeplearningbook.org/contents/convnets.html)",1533589391.0
SamStringTheory,"It comes from the mathematical definition of a convolution and its usage in conventional computer vision tasks.

In deep learning, whether you flip the kernel or not doesn't matter as long as you stay consistent because you are learning the kernel weights anyway. It would only matter if you are trying to interpret the kernel properly.",1533320958.0
akTwelve,"Depends on your learning goals. The deep learning frameworks (e.g. TensorFlow) do all of the linear algebra for you. If you plan on creating a lot of original neural networks, you probably want a strong mathematical foundation. If you want to work at a higher level, you probably can keep moving. I don't even remember what the Moore-Penrose Pseudoinverse is and I learned it a few months ago.",1533341685.0
thisismyfavoritename,"In a nutshell, lets say you want to solve a linear system of equations, Ax = b. If A is not square, it can't be inversed. You could then write the following optimization problem to solve the system: min_x ||Ax-b||^2.

This least squares problem is convex and has a global minimim at A'Ax = A'b.

(A'A)^-1 is the Moore-Penrose inverse.

Most lin alg libraries will handle those things behind the scenes though so you don't really have to bother about it. Understanding how to solve linear systems will help you understand the theory behind algorithms, e.g. PCA.",1533350204.0
antiquemule,"I think it'll be OK, as u/akTwelve said. I'm pretty terrible at maths, so I just skate over the hard bits in papers and textbooks. In my opinion, you can still get a good feel for what's going on. I'm never going to be able to write algorithms, but then that is not my aim.",1533550540.0
retroYogi88,"[https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.4-Linear-Dependence-and-Span/](https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.4-Linear-Dependence-and-Span/)

This blog is helping me to run through chapter 2 intuitions. Hadrien did a good job and took his time to explain certain basics. ",1533685558.0
bubblride,It's time to re-train the cows' pattern recognition model,1533295816.0
mochan_s,"https://magenta.tensorflow.org
There are some podcast interviews out there.",1533232735.0
,"For me the biggest risk is thinking that can solve everything, like every significant technological advance we have made.

Another thing is at some point is going to have to cope with legislation limitations. This could result in frustration and disenchantment (Bio-technology are having this problem).",1533130187.0
mhex,If the generator learns faster use different learning rates for the generator and discriminator and decrease the learning rate for the generator. See [https://papers.nips.cc/paper/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium](https://papers.nips.cc/paper/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium) for the theoretical idea behind it. Hth.,1533188666.0
Karyo_Ten,"> Gaining a basic knowledge of these mathematics topics which are required for the development of machine learning can be done through resources such as:
> 
> - The open course in machine learning by Andrew NG at Stanford Coursera.
> - The Linear algebra course on EdX
> - The convex optimization course from Stanford
> - Larry Wasserman’s book on statistical inference.
> - The Harvard Stat lectures of Joseph Blitzstein
> The Khan Academy linear algebra courses.
> Mathematics for Machine Learning Specialization by Imperial College London
> - Mathematics for Machine Learning by MIT
> - Data Science Math Skills by Duke University
> - Intro to Inferential Statistics by Udacity
> - Applications of Linear Algebra Part 1 by Davidson College (edX)
> - Introduction to Mathematical Thinking by Stanford University (Coursera)
> - Introduction to Linear Models and Matrix Algebra by Harvard University (edX)

Completely unneeded, that's like saying you need to know how a processor work and networking and DNS and TCP-IP works before building a website or that before driving you need to know how to build your car.

Start building a model, be aware that you may need those resources at one point, but be practical.

A data scientist is a driver, you have very nice libraries in Pandas, Scikit-learn, Keras, PyTorch, Pillow. Know those tools and how to use them. When you want to extend those tools for specific use cases will be the good time to find answer.

If you try to understand everything without a specific practical goal in mind (say object detection or named entity recognition) you will just drown yourself in the wealth of knowledge.",1533121164.0
____vitAmin,Short answer: Yes,1533107685.0
pgbabse,"Yes, I do ",1533139037.0
PristineJack,"[This](https://blog.ycombinator.com/learning-math-for-machine-learning/) was posted on HackerNews recently.

I had a math background so I'm not really targeted by that, but I think the author was right about alternating between learning math and coding in order to progress step by step and not getting bored.",1533286714.0
sebchris,"From an engineering standpoint, this is an incredibly bold project. The idea of nodes ranking neighbors by their predictions of objects' movement is ingenious. The only question I had was related to the chains. Obviously long chains are ideal for these kind of trustless networks, but how do objects with short paths work with long chains? Is it really reasonable to expect everything to be moving across long chains of nodes? If not, then how do you distinguish between an innocently short path and a less credible prediction? Penalizing nodes that simply see more short-range traffic might be problematic, but then I haven't really thought about it much.

From a philosophical perspective, I don't know how I feel about this. Rationally, I understand that this kind of open information is important for understanding the world and others and could help immeasurably in accountability. However, I don't know if we are morally ready for such a shift in focus, such a bright light shining in on our lives. Maybe in a century, but not now. This is deeply troubling.",1533054239.0
NordThoughts,"[https://youtu.be/aircAruvnKk](https://youtu.be/aircAruvnKk)

Start with lines and edges.",1533071831.0
blob911,"I like the cs231n course from Stanford. It has a lecture on detection and segmentation, discussing the algorithms (available on Youtube: https://youtu.be/nDPWywWRIRo)",1532988055.0
Schnei1811,"This is a paper I published comparing Faster R-CNN and YOLO

https://arxiv.org/abs/1803.10842",1532986919.0
zergling10000000,"Short overviews of popular object detection approaches:

- http://cv-tricks.com/object-detection/faster-r-cnn-yolo-ssd/

- https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e

However, if you want to dig deep into the details, you should just read the original articles.
",1533031109.0
VivaciousAI,Why would you use deep learning? A simple edge detection filter applied over an image would give you the same results,1532979442.0
string111,"My approach would be to create a dataset (approx. 2000 images) label the paper corners and use a ResNet (probably ResNet34 would be sufficient). Exchange the classifier of the ResNet and let it output a tensor of size `(batch_size, 8)`, 8 because you detect for paper corners and each with x, y-coordinate.

Use a regression loss, probably MSE would do the job and Adam-optimizer.

EDIT: For each different document class that you want to predict the edges, the training set should have around 800-1000 images, following the ImageNet conventions, which seem to work pretty nicely.",1533196691.0
,[deleted],1532956762.0
somewittyalias,You can google around.  I have seen many people keeping a list of useful data sets.  Here is [one example](https://github.com/awesomedata/awesome-public-datasets#datachallenges) on github I found by googling.  ,1532918522.0
Random_Works,Kaggle is a good source to look for datasets,1532904679.0
itamblyn,[https://en.wikipedia.org/wiki/List\_of\_datasets\_for\_machine\_learning\_research](https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research),1533041546.0
necroforest,Testing on the training data,1532922072.0
Rezo-Acken,"What do you mean ? BoW is a representation of a text as a vector of counts. It is therefore no longer a sequence of words. RNN take sequences as an input. I guess you could view the vector as a sequence of numbers and pass it through a RNN but that would be terrible due to the size of that vector and the hypothesis of sequential dependency doesnt make any sense.

Another option is if you actually have a sequence of texts (multiple emails from same person for example) and represent each as a BoW vector. Then it makes sense to use a RNN for the texts sequence.",1532876767.0
thisismyfavoritename,"BOW can be used with RNNs, however you have to do it in such a way that you pass sequences to the RNN, not ""flat"" 2D matrices. E.g. have matrices of the shape (n_records, vocabulary_size, sequence_length).

In order to benefit from RNNs, you need to give them the actual sequences. Also keep in mind that RNNs have much more parameters than most models and might not perform well with BOW if your vocabulary size is large. A better approach might be to use word embeddings.",1532873251.0
secularshepherd,"Cool, so it runs in Eager Execution / closer to bumpy or pytorch, huh? Do you think it’s ever practical to train models in tensorflow.js, though? I can see the case for reimplementing a model, loading pretrained weights, and predicting client-side",1532912544.0
abrahamac,"I don't know a lot about that, but i've seen an article in Medium where explain a bit how tweak an Neuronal Network with an Genetic Algorithm. It takes 1/9 to get the same accuracy than a brute force method. See if It serves you. https://blog.coast.ai/lets-evolve-a-neural-network-with-a-genetic-algorithm-code-included-8809bece164",1532804384.0
nottingpill,Try a new notebook,1532787466.0
exceptionalyfool,How did you get that info ?,1532811887.0
l0ve_y0u_t00,I even tried to run my code on Tor browser using a completely different account and I'm still being alloted to 11MB and my code stops when I start importing image data. It's so frustrating. Is there anyway I can speak to the creators of Google colab project?,1533042203.0
sebchris,"If you're building and running custom code on your machine, a thousand times Linux. Compiling anything on Windows is the reason there are suicide hotlines. Otherwise, either one should work. I would suggest something like Google Colaboratory if you want to get your feet wet.",1532761702.0
snes-jkr,"I’m using windows and also used cloud services with Linux, had no problems with either. Nowadays it should not matter much.",1532756996.0
secularshepherd,"If you’re interested in using cloud services for training and stuff, those are all in Linux, so if you develop on your local machine and try to run it in the cloud, you might have compatibility issues with Windows. But if you’re just doing it for fun, I don’t see anything OS specific about ML",1532754573.0
stefecon,"Thank you all for your constructive comments. I would spend more time with Windows then. Worst case scenario when I cannot install CUDA or CNN for my GPU, I will make a switch to Linux. ",1532831551.0
tulerworld,"On Linux it is less messy if it comes to containers. You can use the Nvidia docker container and don't need to install anything.

Docker works on windows, but it is terrible.

I am not so sure about native installation of CUDA that was a pain with all the dependencies. Maybe it is easier on windows? But I can't imagine that. 

Besides all of that, most tutorials are Linux first. 

Also you learn a skill you might need, if you go deeper into deep learning. ",1532761083.0
sanemate,"I am in a similar boat, I have a 1080ti which I intend to use. My machine currently has Windows 10. Does it make more sense to Dual Boot into Linux if I want to use my GPU for training? Thanks",1532760354.0
TheDuke57,"A year ago I would have said you have to use Linux, now i think you can use windows. You will need some patients to work through cuda install, but pytorch should just work now. The one thing I hate working with windows though is that multiprocessing is horrendous to work with on windows. 

However it you want to do more complex stuff where you need things like openCV or other libraries built from scratch, just go Linux and not have your life. 

It is also important to know that gpu pass through in docker does not work in windows due to some underlying stuff in the OS itself.",1532790157.0
antiquemule,"I'm thinking of buying a decent PC (with an nVidia GPU, of course) for machine learning and it had not occurred to me recently that Windows would be a bad choice. I'm too old to start with Linux, unless I'm forced to...

I hope that my question is not too naive: Doesn't the fact that I'll be using R on top of Keras on top of Tensorflow remove all the awkward traps that Windows causes at low levels?",1532797250.0
cameldrv,"It really comes down to ideas about what is and isn't ""noise.""  An autoencoder with very low capacity might learn to store a very very high level representation such as simply ""this is a dog.""  The fact that a dog is in the scene at all tells you a lot about an image.  If you give it a bit more capacity, maybe it will store where the dog is, or the kind of dog, then maybe that the dog is walking in the grass, and on and on.

As you add more and more information, there are diminishing returns.  Ultimately to perfectly reconstruct the image, you need information like ""pixel #293484 of one of the cameras used in the dataset is slightly less light sensitive"", and even ""due to shot noise, pixel #392909934 received 3 more photons than expected on this image.""  You might say that the information about these details is not really contained within the image.

Information that only applies to a single image will tax the capacity of the autoencoder, and if there is insufficient capacity, information that only applies to one image in the set will not make the cut to be stored.  You might call this sort of information noise.

If the autoencoder had enough capacity to represent all of this, it's not really doing its job to remove the noise from the image and only preserve the representation of important, generalizable concepts that apply to a larger set of images.",1532717902.0
NonLinearResonance,"I haven't read the book personally, but I think you may be misinterpreting ""everywhere"" a bit. In this context everywhere refers to only the input samples in x. No matter how large your dataset is, x cannot represent all possible values. There will be background noise and bias present in the samples you are training on. More data is helpful of course, but it's better to design your system in a manner that encourages generalization. If your autoencoder learns to produce the training x samples perfectly but fails on test data, it's probably just learning to memorize all those specific samples, background noise and all (classic overfitting).  

Regularization helps with overfitting because it encourages learning simpler rules for explaining data. The goal is to capture regularities in the training data without modeling too much of the random noise present. 

Some sort of bottleneck is also common in autoencoders for similar reasons, usually a reduced size for the latent representation where encoder meets decoder. Since features are compressed into a smaller space, the system is encouraged to get rid of redundant or overly-specific stuff and retain features that are generally more useful for data reconstruction.

Hopefully, that helps somewhat. I also think it's important to remember that when talking about f or g, that these are approximations of functions which are mostly intractable for real-world problems. Our approximations are likely to be pretty far from the ""true"" function, but we use these methods and tricks to nudge them a bit closer when possible.",1532720057.0
ImWritingABook,"There’s nothing wrong with a perfect score per se, but an autoencoder wants to maximize bang for the buck of the features it passes accross to its decoder.  If it is doing a perfect job it implies its “budget” is too big and it isn’t learning to be economical with (and therefore, we infer, learning the best abstract representation of) features.  

If you give someone with a photographic memory a photo of a chess board and they represent it back to you as the same image, pixel perfect, you have *less* confidence they have figured out the features related to chess than if you give them the photo and they turn in a messy 8 by 8 grid with clunky but recognizable icons of the different pieces In the right squares like you find in chess puzzles in the newspaper.",1532734491.0
konddmy,"Without proper regularization (like injected noise) - it would just learn an identity function becoming a useless repeater :). In practice this would happen on every layer - so encoder would be identity and decoder would be identity, just because it’s most optimal way to get an identity to the output (less information would get destroyed on every layer and BP “knows” that).


From my experiments, even with some (improper) regularization - it would still learn [partial] identity and just literally remove part of the input vector. If hidden parameters vector is shorter than input - it just stucks with sparse representation on the output (part of the input vector gets removed to fit hidden layer in size, *however with few more tricks you can get it to work as PCA*) and doesn’t go anywhere further. Until you add some noise.

Overall, the whole point of auto-encoder IS to learn how to filter noise (regularize) - so it’s basically a compression with loss *(just more powerful than PCA as regression allows to compress non-linear dependencies as well)*. 

In a very over-simplified scenario - one could even say that natural noise in the input is undistinguishable from injected noise, thus it gets filtered “in company”. 

P. S. As you could see from the last part of Goodfellow’s book - unsupervised learning is a subject of active research, thus I’d take any explanation (including mine) with a grain of salt, and just focus on experiments instead.

—————

As I understand, “large dataset” just improves the performance of estimator (Loss-function), if you calculate (some kind of) mean on such dataset- it would be closer to a population mean (mathematical expectation), allowing us to “compress” the data into some useful knowledge. In reality, sample never equals population, so we can’t just “blur” data-points with each other (no matter how big dataset is) and say it’s a useful representation. That’s why auto-encoders are trying to learn non-linear factors in play.",1532774669.0
thijser2,Hmm you could try to predict produce yield based on satellite pictures. Maybe track certain diseases and spot problems based on satellite pictures? ,1532705392.0
jsaun1,Plant Phenomics [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5737281/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5737281/),1532716282.0
Kottman,Im Building a an all electric robot for feeding animals based on computer vision^^ there is so much potential in every sector of agriculture.,1532719557.0
MungoSoft,"This one is cool: [https://www.ecorobotix.com/en/](https://www.ecorobotix.com/en/). "" The first ever completely autonomous machine for the ecological and economical weeding of row crops, meadows and intercropping cultures.""",1532711645.0
thijser2,"Btw have you considered contacting the world food program (wfp), they might have ideas that could benefit a lot of people. ",1532717755.0
oanufriyev,Search for Andrew Ng initiatives,1532763454.0
cthorrez,Is there a link here? I'm on mobile and can't seem to find the slide in question.,1532702851.0
secularshepherd,"I would disagree with almost everything you said. If you think of ML models fundamentally as function estimators, you’ll find that DL is just another type of ML technique. People usually call SVMs, LR, etc “classical ML” techniques. Accuracy is also not a function of dataset size. The reason why deep learning is better is because deep learning models can be arbitrarily complex and fit more complicated functions. When it’s all said and done, you should always go with the least complicated model that gets the job done, so I think there’s always going to be a place for classical ML too. Just my two cents, so feel free to challenge me too",1532662298.0
manux,"There is no Deep Learning vs Machine Learning... 

Machine Learning is a set of **algorithms**.

Deep Learning is a set of **hypothesis priors**, priors in the form of stacked non-linear projections. Traditionally these priors used to be ""my function is linear or polynomial"" or ""my function is gaussian"" or ""my function can be expressed with a kernel"".

""Deep Learning"" is a set of tools to build function approximators. Once you have that you can do whatever the hell you please with your function approximator, density estimation, regression, classification, reinforcement learning.

There will always be problems better solved with other tools... this whole DL vs ML ""debate"" is getting out of hand.

</rant>",1532654251.0
dafo111,"I would argue that deep learning is a subset of machine learning. Semantics aside, ML algorithms other than DL will live on for a long time because a lot of ""simple"" tasks are so easily and efficiently solved with simple models like regressions or trees or others",1532638517.0
Sicarius154,"One thing to consider is that just because DL may be able to solve the same problems as other ML algorithms given larger datasets and more computational power does not make it feasible. For some, larger datasets and compute power simply isn't possible. ",1532813871.0
maveric_karan,can i start with dl before ml,1532663712.0
moravak,So when is DeepCube Zero coming? ,1532680649.0
moravak,wtf is this awful form? Why would anyone need to fill this crap?,1532671806.0
necroforest,Real analysis ,1532612665.0
k9thedog,"That's a huge number of params in the dense layer. Maybe it freezes trying to balance memory swapping?

Can you check if the same thing happens when you train on a smaller batch of samples? ",1532602615.0
tkchris93,"A few things in addition to the comment about running out of memory.
* The code you posted suggests that you're predicting 1600 different classes. Is that correct?
* You don't have a nonlinear activation on your last layer. Is this intended? Depends what loss function you're using (which doesn't appear to be part of the code you posted here)
* Once you get your model trained, model.evaluate() can also give you memory trouble. I can't remember if that is computed in batches by default or not.",1532618879.0
fahimk1998,Well you can try decreasing the batch size and see if it helps. Decreasing the batch size will slow down your training and might affect your model's accuracy but if decreasing it helps then you can try it out.,1532662086.0
Sicarius154,That is a heck of a lot of classes to predict. If bet it was a memory problem ,1532814172.0
suraty,"Thank you so much to everyone.
Yes, the reason is probably related to many parameters in the final layer.",1533047662.0
pieIX,Possibly [elan](https://tla.mpi.nl/tools/tla-tools/elan/). It’s very powerful but not exactly user friendly. ,1532585477.0
IdeasRealizer,"[Muvilab](https://github.com/ale152/muvilab) (MUltiple VIdeos LABelling tool) may be useful for you.

* Open source
* Python based (so customizable)",1532589480.0
redoyt,For spatio temporal annotations like bounding boxes on videos check [vatic](https://github.com/cvondrick/vatic). There is also a [docker container](https://github.com/jldowns/vatic-docker-contrib) for it. ,1532615000.0
biasOfLearn,I would add Random noise as well. About voice synthesized I have no idea of the impact that may cause.,1532520907.0
double_ewes,I am an organic farmer interested in robotics and weed control. Do you see the agricultural industry as a potential market for your technology?,1532450249.0
AustinCL,Could be used in search and rescue missions to help rescuers cover more ground or go into areas too dangerous for rescuers. ,1532493454.0
UdvPeter,I think this is not the right sub,1532430576.0
WolfThawra,"So I can see how a vocabulary of code can be applied to something like an arcade game, with a relatively simple input and a limited output. However, is there reason to believe it would work for vastly more complex tasks than that, such autonomous driving?

Generally, is there any way of implementing this on images for classification / segmentation problems for example?",1532371086.0
smetko,Swedish company Visage Technologies has computer vision projects in matlab. I dont know why tho,1532370036.0
algebrazebra,It is certainly not very common to use Matlab for deep learning. The very few companies that use Matlab for deep learning are probably engineering companies that already use Matlab anyway and need a simple neural network for whatever niche task that sprung up. ,1532381646.0
antiquemule,"Many big companies are going to prefer paying for Matlab, rather than using open source, for liability, after sales service, etc.",1532380455.0
kylepob,"Yes, my company utilizes MATLAB, but mostly for traditional ML techniques related to computer vision. For deep learning endeavors we tend to use python and then call everything from MATLAB, as it is easier to maintain the entire pipeline in MATLAB and call DL functionality when needed. ",1532645281.0
fishy_commishy,Yes,1532369487.0
venkuJeZima,Yes. For example me,1532375649.0
antiquemule,"Looks like an interesting way to avoid the arbitrariness of choosing a network structure (Network Architecture Search - NAS) before starting learning. Here network structure and hyperparameter choice are integrated and co-optimized. They warn against spending too little time on NAS before starting learning. ~~No canned code on offer~~, and it looks expensive in computer time, no surprise there, they're doing a demanding task.

Edit: My mistake - canned code *is* on offer.",1532358381.0
FaceDetector,"Recently researchers from Google used evolutionary algorithm for architecture search and achieved state-of-the-art accuracy on ImageNet.

Will be interesting to see what results can be achieved using Bayesian optimization as proposed in this paper.",1532419141.0
mdv1973,"tf.subtract(A, tf.diag\_part(A))",1532281253.0
riga7,"I would create a bool mask, set the diagonal entries to `False` and use [`tf.boolean_mask`](https://www.tensorflow.org/api_docs/python/tf/boolean_mask):

```
# assuming the NxN tensor is ""t""
tri_mask = np.ones(N**2, dtype=np.bool).reshape(N, N)
tri_mask[np.diag_indices(N)] = False

t_nodiag = tf.boolean_mask(t, tri_mask)
```

You can even set the `axis` argument of `boolean_mask` to broadcast the masking over an axis.",1543588117.0
yoyo27705,"I = diag(1,1,...,1)\_{nxn}   
E = \\begin{pmatrix}  
1         & \\cdots &1//  
\\vdots&             &\\vdots //

1         & \\cdots &1//  
\\end{pmatix}\_{nxn} 

N = E - I 

then you can make Hadamard product between your matrix and the matrix composed as N",1532275306.0
khizanov,The most useless video ever!,1532268025.0
eatyovegetablessssss,Tldw?,1532317056.0
konze,"In my opinion, deep learning is not meant for desktop computers. You might get a desktop computer which is able to train you CNN in reasonable time. However, if you go deeper you will get into problems with GPU memory and overall performance. 
If it is a course at a college/university ask the lecturer if there is a server you can use for training. Otherwise stick to the cloud services.",1532174979.0
sharky6000,Is there a source for these numbers? Is this based on a study?,1532094754.0
Spenhouet,"Your post confuses me.First of, deep learning is just a buzzword for artificial neural networks with more than one layer. Neural networks are a machine learning technique. Your differentiation doesn't make sense.Second, all the points that make you skeptic about neural networks also effect all ML techniques. So you should be skeptic about all ML techniques.Only your last point, ""black box"", makes sense. That is a real problem of neural networks compared to other ML techniques. There is a ton of research going on, trying to solve this issue. In fields where life depends on the decisions of a neural network, it is not acceptable to just trust the output of the neural network. Additional problems here are the susceptibility to adversarial attacks and the unknown behavior for unknown input.",1532088864.0
arachnivore,"Just from a semantic standpoint; comparing deep learning to machine learning is like comparing Honda to cars. It makes little sense. Deep Learning is a sub-field of Machine Learning. The best I can guess is that you meant to compare deep learning to **other** machine learning techniques.

With that somewhat pedantic point out of the way:

Deep learning has a lot of skeptics because techniques like support vector machines (SVMs) are built upon rigorous mathematical foundations, while deep learning has a common (though undeserved) reputation of being a less rigorous field where people just try arbitrary variations based on hunches and share wether or not they helped.

The truth is that SVMs and other non-DL techniques tend to be based on assumptions that keep them within the realm of analytical methods while also limiting their their real-world applications. Assuming the system you're trying to model is very linear in nature, SVMs allow you to prove properties like optimality, which theorists love. However; all real-world systems actually contain non-linearities that cause problems for SVMs. Under ideal conditions; there may be a linear relationship between how far you've depressed your gas peddle and how much your car is accelerating, however; if you're red-lining your car and the engine blows a head-gasket, or if you stall the engine; that linear relationship breaks.

Deep learning systems, on the other hand; have the ability to model non-linearities. The problem is that mathematicians have many more tools for analyzing linear systems than non-linear systems, so it's more difficult to build a solid analytical justification for why certain techniques work better than others. It can give the feeling that researchers are more-or-less stumbling in the dark.

One huge advantage of deep learning is that it permits layers of abstraction. Other ML techniques require a lot of what's called ""feature engineering"", for instance;

If you wan't to build a text sentiment analysis system using an SVM, you will probably have to pre-process the text into a sequence of labeled words or ""tokens"" (pairing each word with a grammar annotation like ""noun"" or ""verb"") and if you run into words that aren't in your lexicon, you'll probably have to ignore them or do something complex.

A deep learning system, on the other hand, can discover relevant features from the most granular, unprocessed version of the data (e.g. individual characters) because it can gradually learn more abstract features through multiple layers. Even if it's never seen the word ""Wherefore"", it will likely learn that phrases beginning with ""Wh"" are almost always questions, and phrases ending in questions are more likely than not negative sentiment, so ""Wherefore art thou Romeo"" probably has a negative sentiment. Or maybe copious repetition of letters denotes excitement so ""Yaaaaaaassss, Queen!"" is a positive sentiment. Those are character-based features that most hand-engineered word tokenizers will fail to capture.

The obvious problem is that humans understand hand-engineered features while features learned from the most granular form of the data can be very difficult to make sense of.",1532133027.0
speyside42,"I don't agree with the author. It is a super simple idea which has been used before, but here it was systematically evaluated. Mathematical interpretations yield little meaning if the effect is dependent on the problem and the data.The ImageNet experiment was actually underlining that there was barely an influence on classification accuracy (as expected).  The indices intuitively make sense for shallow conv nets that can choose to make use of exact location information. Not the holy grail but certainly an interesting insight.

My criticism of the paper is that they did not include results on common Object Detectors (FasterRCNN/SSD/..) while claiming that it benefits detection accuracy.

This blog post is just shallow, itself.",1532093321.0
moravak,"I'm not the author, nor I know him personally. No need to throw your Friday frustrations on me.",1532100867.0
Spenhouet,"In general, don't do it from scratch:1. Read the paper2. Find out what the general architecture or model is that they used3. Search for implementations of that architecture / model on GitHub4. Implement that and validate that it works5. Modify to fit the authors descriptionAdditional tips:- Before you invest to much time, look if the used dataset is even publicly available- Use the right tool and frameworks. Don't reinvent the wheel- If performance is not important and it's not for production than use a abstraction / wrapper framework like Keras, ... it will make your life easier.- If details are missing in the paper that are necessary for the implementation (often the case) don't hesitate to write the authors a email.",1532037254.0
AzraelFTS,"There is no magical recipe, simply respectfully ask for the implementation if they still have it and can give it to you. Say that you are interested by this work and why, generally people appreciate that and help you.  Still, there is no guarantee.",1532036829.0
Karyo_Ten,"Curse at the authors/reviewers, go find the equations, implement them, find the backprop, realize that ""backpropagation equations are left as an exercise to the reader"", curse again, fumble your way around, check numerical gradient, vow to change science, ???, profit",1532037211.0
lucidrage,"That's great and all, but when will they start working on time machines?",1532027524.0
bluesky314,"""**great value"" LOL, with** $3,600.00 for Early Bird you must be a loaded fellow to call it that",1532021221.0
IdealColesLaw,Perhaps they meant underfitting.,1532003308.0
Spenhouet,I can only guess that he is talking about underfitting. That your model found a low dimensional (compared to your data dimension) representation of your data that is actually to general.But you will not know for sure until you ask him directly instead of speculating about what he could have meant.,1532003458.0
UdvPeter,Intel has bunch of software libraries and hardware IPs for embedded machine vision including DNNs. They also have an ASIC distributed as movidius neural compute stick. I have used the latter for a project and was pleasantly surprised by the performance but don't expect too much for the price.,1532008419.0
whiletrue2,any more ideas?,1532283770.0
shishirm,Visual question answering is a good field to use attention mechanism.,1531989528.0
wild_thunder,"I think your discriminator isn't learning. Based on the error message in the notebook, I'd venture a guess that model.trainable only works before calling compile. Therefore, when you initially set discriminator.trainable = False and then compile, the weights are being frozen for the whole training loop.",1531977400.0
Spenhouet,Can you please stop that spam?,1532037389.0
shuklaswag,"Woah, I had no idea there were so many techniques for optimizing hyperparams in neural nets. Thanks for sharing!

By the way, since this is on the NeuPy website, I was wondering how NeuPy stacks up against other Python DL frameworks like Tensorflow and Keras? I hadn't heard of NeuPy before today, but it seems to have solid documentation.",1532016349.0
somewittyalias,"I don't know why it's not working, but I have a few suggestions.

1.  You can easily generate an infinite amount of training data.  That will avoid overfitting.  Don't create a data set, but instead generate data as needed.  Or create an enormous data set.

2.  Your input is quite complicated.  Why not just `[0, 13]` to mean `add(13)`  and `[1, 13]` to mean `check(13)` instead of passing two commands for each operation?  You would have more chances of at least getting something to work if you keep your problem as simple as possible.

3.  For an `add(13)` command, don't set an output.  That means don't compute any loss for an `add` command.  The output might be -100 or +100, but you don't care at all and won't even look at it.

4.  Maybe play around with the number of time steps in each sample.  Currently you have 5 commands (10 / 2).  Maybe try 5 to 20, varying on each sample.  However you might lose efficiency if your examples are not all the same size.  Maybe for each mini-batch keep the number of time steps constant, but change it between mini-batches.

5.  Maybe start with the possible values of the memorized integers being only (-2, -1, 0, 1 , 2).  You are going at least to 13 in your example, which might be too ambitious for now.  I'm not sure how you set the state space dimension in a DNC, but make sure it's not too small or too large.  If you have 5 values like I suggested, maybe try a state space dimension between 5 and 20.",1531878234.0
Krunkworx,How does this have anything to do with deep learning?,1531810348.0
Cartesian_Currents,It sounds like your loss function optimizes a parameter that doesn't affect your accuracy.,1531721200.0
IdeasRealizer,I don't think Deep Learning and Arduino are compatible. Image data acquisition can be done using an Arduino board but deep learning is computation intensive. Consider [RaspberryPi](https://www.raspberrypi.org/) instead.,1531670653.0
ScotchMonk,"Likewise, Raspberry Pi and Intel Movidius stick as mentioned above will give you better performance. The Minimum MCU speed required are around 200 MHz... or those that can run classic computer vision projects. DL is computer intensive as shown here, with low fps [OpenMV youtube](https://youtu.be/PdWi_fvY9Og). ",1532304922.0
Simulation_Brain,"I recently ran across this. It was a very recent arxiv paper; it was not using a vanilla dcn, but I don’t remember what mod they’d used that allowed this performance. Sorry that’s not more helpful. ",1531671640.0
WolfThawra,Quite badly written. The whole website is a bit suspect.,1531567219.0
kartoffel234,"Not entirely sure whether I understood the question or not. Preprocessing the data for image classification usually involves some kind of augmentation (e.g. flipping, cropping, ...) which gives you more variety in your training data but also makes it harder for the model. Therefore it generalizes better and will perform better on unseen data if your augmentation are meaningful in some sense. So generalization of a model in general will increase training time.",1531561970.0
MumbleBeeX,"Somebody correct me if I am wrong, 
When you are augmenting the data, you ate basically applying those cropping/blur /etc on individual copies on the image. So, 3 augmentations would lead to 4 different copies of the same image being passed through the network. Of Course you don't process each image individually but in batches,  but the overall computation would still be more. I am ignoring the probability parameter in augmentation where you decide (be drawing a binary random variable) if you want to make a a particular augmentation of a particular image. 
So with p=0.5 expect only half of computations.",1532083847.0
adikhad,Please specify the problem..,1531555960.0
gchlebus,"A common cause for errors is feeding to small images. The ms-ssim works on five resolution levels by default (orig, 2x, 4x, 8x and 16x downsampled), which requires input images of at least 176x176 size. The number of levels can be controlled via the lenght of the power_factors list (one of the function parameters). HTH",1531520095.0
highly_toxic,Just curious what's your purpose to learn reinforcement learning if you don't want to know CNNs or RNNs? (GANs maybe fine),1531504333.0
cthorrez,"You definitely don't need GANs at all.

As for reinforcement learning, it doesn't necessarily have to be deep reinforcement learning. You can learn a lot about the concepts of RL just by reading Sutton and Barto. You can learn the basic algorithms like  Q learning, actor critic, sarsa, and concepts like exploration, reward shaping, on and off policy algorithms without knowing anything about deep learning.

When you get to more current stuff, all the value functions are either MLPs or CNNs and the policies are either MLPs or LSTMs so you will want to learn those things eventually. Buy you definitely don't need them to get started.",1531537192.0
onenuthin,"It’s already a “nano” degree, so I wouldn’t recommend skipping anything.",1531524754.0
Talcmaster,"In the strictest possible sense you might not necessarily need to know those 3 things to understand RL, but for practical applications you're probably going to need to understand CNNs and RNNs to make anything.  GANs not as much, but they are neat and worth checking out.  ",1531527661.0
WordLord,"For DeepRL yes.for RL,not so much.but don’t skip it.",1531535080.0
nonamefhh,J . U . S . T . D . O. I . T . ! . ! . ! .,1531601682.0
Speech_xyz,"Of course not!!!
Reinforcement Learning and Deep Learning are nearly completely orthogonal.
Most reinforcement learning does not include any deep learning models.",1532064078.0
fnaticdisease,Maybe [this](http://cs231n.stanford.edu/reports/2017/pdfs/810.pdf) will help you. Your best shot at this would be to find a pretrained model and fine-tune it on some data of your own.,1531474883.0
eye_of_saur0n,"Have a look at opencv's MSER (Maximally stable extremal region extractor). The function outputs regions of text in an image. Once you get the regions, you can extract individual lines and do the necessary post-processing.",1531541886.0
ThatsALovelyShirt,"Isn't that essentially what overfitting the data is, in a way? I thought that was an undesirable characteristic.",1531452259.0
vaglino,"If I understand your question correctly, by the ability of a network to recreate the training data, you are talking about a model whose output best approximates the training input. This amounts to training a network with input/output couples (X,Y) where X = Y. In other words, the training input equals the expected training output. Such models are referred to as autoencoders.

 https://en.wikipedia.org/wiki/Autoencoder

The first part of the network - the encoder - reduces the dimensionality of the data, effectively learning lower dimensional representations (or features) of the input data. The second part of the network - the decoder - uses the output of the encoder to reconstruct the original encoder input.",1531446508.0
randomchickibum,Its impossible to recreate the exact same training data unless you have 100% accuracy on training as well as the test set. Autoencoders can be used to generate a well round approximation. Generative models can be used generate fake data (if you just want some training data from the models.),1531450514.0
multiscaleistheworld,"That depends on the data characteristics as well. If the data set itself is based on model or some type of highly orderly data, such as phase diagram then the training data can be reproduced almost exactly with low noise level. ",1531455924.0
BrosephBSJ2LC,"Thank you for the responses. I am actually looking at this from a privacy/policy perspective, wondering on if I use sensitive type data to train a CNN deep learning model are there risks in widely distributing that trained model. One researcher I spoke to thought yes and that he had come across a paper articulating a similar situation",1531506114.0
hastala,"To clarify:   
From what I understand, you're proposing a kind of hierarchical composite of neural network modules:   


*Every successive layer = ""composed of several of""*

    Level 5 -- Large-scale ""brain""
        Level 4 -- Somewhat more specialized ""hemispheres""
            Level 3 -- Even more specialized ""lobes""
                Level 2 -- ""Neural tissue"", a conventional neural network
                    Level 1 -- A neuron

Where every layer N > 2 is an ensemble of its constituents (layer N-1). Is that right?   


And where does all the circle geometry come into play? ",1531427616.0
antiquemule,"This is madness. Chimpanzees are highly intelligent, but have no theory of mind, so conciousness did not precede intelligence. Good luck with the cargo cult science.",1531471724.0
somewittyalias,"Neural nets cannot quite deal with integers, or categories, internally.  You can't really represent ""393"" in a neural net, unless you come up with something fancy.

However you might be able to get what you have done to work if you limit the second numbers to only ""-2, -1, 0, 1, 2"".  A neural net might find a way to deal with a few categories internally.",1531401274.0
hergertarian,"I might be misunderstanding the example you’ve created, but this looks like it might not be a good match for RNNs. RNNs can track sequences within a single observation (row), and you data set seems to require tracking sequences across observations. 

It’s also worth noting that most neural network packages will shuffle (reorder) observations, and that is is generally assumed that observations are unordered. 

You may need to change the structure of your observations, by doing something similar to a GROUP BY, to get the results you’d like. ",1531404419.0
serpimolot,It's hard to say without seeing exactly what you're doing. Can you share your code?,1531395252.0
nile6499,"Let's solve together, This problem arises either because of total memory exceeding GPU's capacity, or you've been GPU for too long, or lastly there is something wrong with architectur.
We can try together, I could be wrong so help me.

convnet.add(Conv2D(64,(10,10),activation='relu',input_shape=input_shape)) --> (250-10)+1 ->241x241x64
convnet.add(MaxPooling2D()) --> 241/2 --> 121x121x64
convnet.add(Conv2D(128,(7,7),activation='relu', kernel_regularizer=l2(2e-4))) --> 121-7+1 ->115x115x128
convnet.add(MaxPooling2D()) --> 115/2 --> 58x58x128
convnet.add(Conv2D(128,(4,4),activation='relu',kernel_regularizer=l2(2e-4))) --> 58-4+1 --> 55x55x128
convnet.add(MaxPooling2D()) --> 55/2 --> 28x28x128
convnet.add(Conv2D(256,(4,4),activation='relu',kernel_regularizer=l2(2e-4))) --> 28-4+1 --> 25x25x256
convnet.add(Flatten()) --> 25x25x256 -> 524288

convnet.add(Dense(4096,activation=""sigmoid"",kernel_regularizer=l2(1e-3))) --> 524288x4096 --> 2147483648????
may be the above parametere calculation is the reason for your problem????
Obviously, I have not added bias...

Correct me If I made mistake, It's been weeks since I did calculation.

 ",1531368831.0
tkchris93,"A few options, some of which have already been mentioned, but I'll repeat them so they're all in one place:
* Reduce batch size
* Downsample your input (maybe start with 64x64). This will cut down memory costs. Also, the architecture you have here is a little shallow for input this big. In general, going with downsample input data and a deeper network (see next point) will likely be more effective.
* The kernel sizes you have are pretty big. It's been shown empirically that instead of using a (10,10) or (7,7) you can get better performance out of multiple layers of size (3,3). Meaning, instead of one Conv2D(64, (10,10)), you would split that up into three or four stacked Conv2D(64, (3,3)). This approach actually uses fewer parameters too.
* Pool more aggressively (if you don't want to downsample). Use MaxPolling2D(4,4) instead. This will result in significantly lower dimension once you get to the Flatten layer.
* 4096 dense layer is where a lot of your parameters will be. I would drop that all the way down to 256 or something just to get your model running, then you can scale that up if you desire.
* As a last ditch effort (which I don't think will be necessary after trying out the points above) scale down the number of filters per layer until you can fit within the memory constraints.

Also, definitely take a look at siamese_net.summary(). Those dimension counts and parameter counts help a lot. 

Lastly, remember that because you are doing a Siamese net, the amount of VRAM necessary to perform calculations is nearly doubled even though the parameters are shared.",1531408972.0
waterRocket8236,"This is because the model architecture is too big for the GPU memory. Second, Reduce batch size because  that batch of images are read in memory. Third, image dimensions. Try using less dimension e.g less than 250 X 250. This is only if you can reduce or is ok for you do, otherwise.",1531371922.0
naomi_fridman," Auto encoders, learn the real essence of the data, that's what makes them fascinating. They can be used for data augmentation , data classification, dimentionality reduction and probably much more.",1531299011.0
waterRocket8236,One suggestion: Explain and help people understand semi supervised and supervised learning approaches for AI.,1531287515.0
pronk46,"An image is just a grid of pixels, each of which has its brightness represented as a number between 0 and 255 (for grayscale images).

Nielsen is just saying that he's taking values of the picture elements (pixels) and directly providing them to the neural network as input.",1531270951.0
jaystile,"I seen from your earlier comments that you have your question figured out but I wanted to chime in. In this case, the input values are being normalized. This procedure makes the greatest value 1 and the least value 0. This a common procedure to apply for input data as it gives the network less things that it has to learn. I've found from experimenting that the more you can consolidate and decimate your data the less the network has to learn so it will train faster.",1531316224.0
GotRedditFever,Wouldn't waste money on new cooler if you aren't doing serious overclock. The AMD stock cooler is really good.,1531250027.0
Phnyx,Looks good to me but I would use 2x16 GB Ram instead of 4x8 to leave room for upgrades if prices ever go down again.,1531248504.0
ThatsALovelyShirt,"You should really consider Intel. You can get an 8700k with nice motherboard for less than $500. I have mine overclocked to 5 Ghz with a cheap air cooler.

Nearly all ML libraries -- Keras, Torch, Tensorflow, etc. -- are compiled with Intel MKL libraries, which are hand optimized for Intel chips. They are *significantly* faster. 

Not only that, my memory bandwidth doubled between my overclocked 1800X and my 8700k, with the *same modules*.",1531277735.0
VivaciousAI,Any reason for AMD? A lot of ML packages use Intel MKL if you're looking for anything other than deep learning,1531259375.0
Seagate_Surfer,"Thanks for considering Seagate! If you come across any questions for the HDD, we are here to help so feel free to hit us up.


---
Seagate Technology | Official Forums Team

---",1531330096.0
antiquemule,"Go to [arxiv.org](https://arxiv.org) and search on ""siamese network"" in the abstract field. I found promising hits very quickly, but I'm not sure exactly what you want.",1531248269.0
pvskand,Here is a recent FG 2018 paper : [https://arxiv.org/pdf/1803.01260.pdf](https://arxiv.org/pdf/1803.01260.pdf),1531274945.0
Kaio_,"so many opportunities to be had with this . 
DOOM --> DOOM 2016",1531239121.0
yoyo27705,"love to learn as well.

Almost same plan here",1531185785.0
dewayneroyj,There are new graphics cards coming out very soon that will be much more powerful than the 1080s for around the same price.,1531186284.0
assembly_programmer,"Take a look at [this](https://youtu.be/dtFZrFKMiPI) video of Siraj. What you want to know is at the last 2 minutes of video, where other setup is suggested, but you should watch the full thing. Having a machine is way better than cloud services.",1531186852.0
ScotchMonk,"Definitely check out these articles before buying:

[https://medium.com/mlreview/choosing-components-for-personal-deep-learning-machine-56bae813e34a](https://medium.com/mlreview/choosing-components-for-personal-deep-learning-machine-56bae813e34a)

[https://medium.com/yanda/building-your-own-deep-learning-dream-machine-4f02ccdb0460](https://medium.com/yanda/building-your-own-deep-learning-dream-machine-4f02ccdb0460)",1531198763.0
dewayneroyj,"Also: I’ve built a deep learning machine for my home. You need the following:

- Graphics Card
- CPU
- SSD 
- HDD
- RAM
- Motherboard 
- Case 
- Fan 
- Power Supply 
- Operating System ",1531186827.0
felipecalderon1,"I would advise to buy everything at the same time, the sooner the better. The gpus are getting very expensive. I could sell my 1080 ti, 30&#37; more expensive than what i bought them.",1531187073.0
stupac62,"Maybe this can give you some ideas: 
[https://www.servethehome.com/deeplearning10-the-8x-nvidia-gtx-1080-ti-gpu-monster-part-1/](https://www.servethehome.com/deeplearning10-the-8x-nvidia-gtx-1080-ti-gpu-monster-part-1/) ",1531200414.0
nunz,"Here is my build: https://pcpartpicker.com/user/contracode/saved/vgkyf7  


The case is cool, but you don't need a glass one like I got. Plus, if you want four GPUs, I'm not sure the case and mobo combo will work. 


You don't need RGB RAM, like I got. It looks cool, but other than that, it's an extra $100. 


You don't need Windows, and if you do, your university probably has an enterprise license. 


The fan I used works, but needs to be upgraded. 


New Nvidia cards and new Threadrippers are coming out soon, so it might be best to wait, unless there is a planned upgrade and maintenance budget.",1531263325.0
fkxfkx,"Don't waste your time or money.
Learn how to use resources in the cloud, AWS, GCP, Azure, etc.

",1531185945.0
abevallerian,"I think this is what u r looking for
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.441.7873&rep=rep1&type=pdf
",1531190097.0
cameldrv,"It probably does make sense.  It really depends on the real-world cost of getting the answer wrong.  If you want your cost function to be relative error instead of absolute error, you could fit your model to the log of the house price.  Of course, if you're doing linear regression, now your line is in log space.

Linear least-squares regression makes an implicit assumption that the residuals from the regression will be normally distributed.  Assuming a gaussian noise model implies minimizing squared error.  If the ""noise"" (the remaining unpredictability after regression) in your problem is not normally distributed, your regression can break badly.  This is what spawned the LASSO and L1 regression generally -- they are much more robust to noise not matching the model.

The tradeoff to not making as many assumptions about the noise model is that you'll need more data to get the same accuracy.",1531171821.0
hergertarian,"Relative error can be used, and is actually helpful for many cases (along with many other losses). 

Squared error has some nice properties, that make it more convenient to use. These properties include the fact that Ordinary Least Squares linear regression is the best linear unbiased estimator (if the Gauss Markov assumptions are met). 

Because of these convenient properties, least squared is really, really common. However, I’ve used and seen others use a wide body of losses, include MAPE and MAE. ",1531404829.0
antiquemule,"It has to be squared, so that its minimum makes the predictions as close as possible to the data. Without squaring, positives and negative errors could balance out. You can use the square of either the relative or the absolute error, as seems most appropriate. Or you could use the sum of the absolute errors instead of the sum of their squares. That gives a more robust answer, less sensitive to outliers.",1531170191.0
UdvPeter,It helps in finding the minimum of the cost function as it ensures it's convex,1531177841.0
adikhad,I would personally recommend Andrew Ng's course on deep learning and neural networks. I am doing his deep learning specialization. And it's great,1531160044.0
Xkv8,"Andrew Ng’s course, like others will suggest is a great place to start. It’s a little complex, though. Another good method would be to start with Jeremy Howard’s fast.ai course, then follow up with Ng. Fast.ai is a top down course where you work backwards from the final result, gradually looking at more complex details.",1531167827.0
antiquemule,"Interesting, but a commercial solution. No github repository with Python code to run. [LIME](https://github.com/marcotcr/lime) is a completely different way of getting understanding of neural networks that ***is*** ready to run. ",1531170662.0
jtsymonds,"TLDR: Neural Networks are powerful but complex and opaque tools. Using Topological Data Analysis, we can describe the functioning and learning of a convolutional neural network in a compact and understandable way. The implications of the finding are profound and can accelerate the development of a wide range of applications from self-driving everything to GDPR.",1531150883.0
hinduismtw,The website on mobile is Unreadable! https://imgur.com/a/ogNHizA,1531196990.0
QryptoQuetzalcoatl,"How does the above method (TDA) compare with or improve upon existing diagnostic methods, such as deconvolution? For example, in the following article:  
[https://arxiv.org/abs/1412.6806](https://arxiv.org/abs/1412.6806)  
the authors do a good job of comparing their work with some of the existing methods. It would be really great if a similar scientifically-oriented comparison could be elucidated in the above TDA blogpost.",1531736902.0
Eruditass,This sort of problem is much better suited for classical /r/computervision approaches than deep learning.  ,1531150874.0
rekt_brownie,"If you’re thinking of “bent” as just not straight, then you may want to frame this is an anomaly detection problem rather than a binary classification problem, then you could just feed your model pictures of straight objects. 

How big are your pictures?",1531148625.0
shubhamsy777,"I'm travelling currently and will try to help whenever possible, but I would really like to know how you got a large enough dataset for straight/bent objects?",1531148502.0
lowerhouse,"spoiler: code for downloading the pretrained model has 47 lines. 

edit: of course, there is still more work than that in the article. just wrote for headline. ",1531134136.0
LewisJin,"Yes, if you do this you should probably have a DIY drone",1531136290.0
xEtherealx,I could do it in 25 LOC.  Look at all those superfluous print statements!,1531159509.0
UndeadPandamonium,Clickbait. Clickbait everywhere,1531135451.0
secularshepherd,"If you’re trying to work in the field, you’ll need a higher education, but if you’re just trying to learn the basics, there’s plenty of free resources online.",1531085323.0
kdnbfkm,"Doubtful, until getting to a point of trying the basics. Crawl before you run. What were you interested in again...? It doesn't matter. No newbie does ""interesting"" things unless very lucky, creative, or easily amused by whatever baby projects they can do.",1531122043.0
antiquemule,"I hope so, because I'm doing it :-).- Python is easy to learn.- Keras is an awesome front end for Tensorflow, Theanos, etc. which do the heavy lifting.- There are countless tutorials, blog posts and courses covering every aspect of the subject.- arxiv makes all the latest papers available fast and for free.- CloudML gives you 300 hours of GPU time for free when you sign up.

So if you have the right kind of brain and enough commitment, I really do not see why you cannot have a good shot. Note that I'm ***not*** saying that this route will get you a job. That's a different thing.",1531135466.0
trevorm7,"Check out this guy's YouTube channel: https://www.youtube.com/watch?v=I74ymkoNTnw
He taught himself machine learning and does some interesting things with it.",1531848732.0
antiquemule,"This is an oldie-but-goodie. >1,000 citations! pdf available via Google Scholar.

### [Cooperative multi-agent learning: The state of the art](https://dl.acm.org/citation.cfm?id=1090753)

 Cooperative multi-agent systems (MAS) are ones in which several agents  attempt, through their interaction, to jointly solve tasks or to  maximize utility. Due to the interactions among the agents, multi-agent  problem complexity can rise rapidly with the number of agents or their  behavioral sophistication. The challenge this presents to the task of  programming solutions to MAS problems has spawned increasing interest in  machine learning techniques to automate the search and optimization  process. We provide a broad survey of the cooperative multi-agent  learning literature. Previous surveys of this area have largely focused  on issues common to specific subareas (for example, reinforcement  learning, RL or robotics). In this survey we attempt to draw from  multi-agent learning work in a spectrum of areas, including RL,  evolutionary computation, game theory, complex systems, agent modeling,  and robotics. We find that this broad view leads to a division of the  work into two categories, each with its own special issues: applying a  single learner to discover joint solutions to multi-agent problems (team learning), or using multiple simultaneous learners, often one per agent (concurrent learning).  Additionally, we discuss direct and indirect communication in  connection with learning, plus open issues in task decomposition,  scalability, and adaptive dynamics. We conclude with a presentation of  multi-agent learning problem domains, and a list of multi-agent learning  resources. ",1531068298.0
Cunic,Can you be more specific?,1530997523.0
wesleylaurencemusic,"Try a long short term memory (LSTM) network or a gated recurrent unit (GRU). These network architectures are best for sequence data such as audio.

Check this out, using LSTMs to classify phenomes:

http://scholar.google.com/scholar_url?url=http://wwwknoll.informatik.tu-muenchen.de/pub/Main/Publications/Graves2005b.pdf&hl=en&sa=X&scisig=AAGBfm0INp1ieqeyUal_facv46VRybXugQ&nossl=1&oi=scholarr",1530938678.0
troltilla,"Where did you get it from that the brain operates on 150x150 per WAVE sample? 

AFAIK a simplistic description is that the sound is first converted in the outer ear to mechanical vibrations, exciting a membrane in the cochlea. Different parts of the membrane respond to different vibration frequencies, thus triggering the nearby neurons, which send spikes of impulses to the brain. If your claim is based on some work I'm not aware of please share the reference. 

As to your problem are you sure you need to get the waveform back from the representation? It surely makes things harder. ",1530950514.0
antiquemule,"Not my domain, but I just Googled ""deep learning video"", as it seems like a similar data problem. Seems like many approaches work. Am I missing something?",1531068696.0
_first_of_his_name_,Kaggle is surely the best place.,1530885888.0
yoyo27705,"Kaggle+1

Also depends on the level of commmand you wish to achieve. 

GitHub/GitLab or even the old-fashioned SVN are all possibilities.",1530896487.0
pepitolander,did you check [https://www.reddit.com/r/datasets/](https://www.reddit.com/r/datasets/) ?,1530902307.0
amanullahtariq,Wow! this drone looks very useful other than the battery time.,1530879507.0
caseyscottmckay,"[The Unreasonable Effectiveness of Recurrent
Neural Networks](https://web.stanford.edu/class/cs379c/class_messages_listing/content/Artificial_Neural_Network_Technology_Tutorials/KarparthyUNREASONABLY-EFFECTIVE-RNN-15.pdf)",1530844569.0
dewayneroyj,I think [this](https://medium.com/@timanglade/how-hbos-silicon-valley-built-not-hotdog-with-mobile-tensorflow-keras-react-native-ef03260747f3) article should provide you with some insight. ,1530825050.0
antiquemule,"Clickbait title! ""No"" is the fairly predictable answer. But a nice newspaper article, worth a quick read.",1531071137.0
hergertarian,"Keras examples (https://github.com/keras-team/keras/tree/master/examples) are a great starting point. There are a few good computer vision example, and a few great NLP examples. 

I’d recommend starting with image classification. ",1530924096.0
lilsmacky,"It all depends if you can gather data that in some way contains information where regular users differ from malicious users. If you have that, then sure.",1530773623.0
-TrustyDwarf-,"It depends on your training data and you should just test the model with images of your actual scene to decide if it works well enough or not.

If your training data only contains images of ""cats sitting on grass"" and ""dogs sitting on concrete"" then the model will probably not recognize cats/dogs in a forest very well because it learned to look at the background (grass = cat, concrete = dog). But if the model was training using cats and dogs on a huge number of different backgrounds, it'll probably also work on new backgrounds because emphasis is on the cat/dog and not on the background.",1530786203.0
ScotchMonk,"it will work in different backgrounds. Take a look at YOLO algorithm. It can find objects that even move in a scene... like picking a face in a crowd from a CCTV camera. You just need to train it with the same object located in different scenes.. for example in snow or desert or in forest. 
 ",1530785657.0
ScotchMonk,"Yes, you can say that. Just make sure your training dataset is good - the size of the object to detect is big like 50% or more, and the same object in different background scenes. You can augment your training data if you don't have enough photos for training. You can read this excellent post from [Keras blog](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) . Here's another post about YOLO algorithm if you are not familiar. [YOLO a gentle guide link](https://www.dlology.com/blog/gentle-guide-on-how-yolo-object-localization-works-with-keras-part-2/) ",1530833366.0
assembly_programmer,"There is no right or wrong. In some cases, using a bigger one in the generator makes both train way faster, other cases you will get noise output and it will not evolve. In the big projects I have worked, the learning rate was always the same for the discrimination and the generator BUT the generator was updated two times every batch and the discriminator once. This is different since the two generators updates happens with the same discriminator state, so converging towards it. ",1530741108.0
,[removed],1530694842.0
hqng,"There're some implementations on keras and theano. Here is keras version:
https://github.com/sadeepj/crfasrnn_keras",1530852425.0
DillyDino,"This list kinda sucks dude. 

Seriously, no mention of Goodfellow's GAN's, Schmid/Hoch's LSTM, Bahdanau's Attention, Srivatsava's Dropout, Szegedy's Batch Norm, DeepMind's Atari....not to mention, not a single publication from Bengio's team. I ain't diggin' it.",1530688131.0
CaseOfTuesday,"**Pure clickbait:**
This list is pretty bad. It contains 2 papers about tensorflow, but lacks e.g. the original LSTM publication.  EDIT: and some authors are quoted by their first name instead of their last name.",1530684592.0
antiquemule,"The title kinda gives away the clickbaitishness. Not worth the click, IMHO.",1531071275.0
somewittyalias,"Can you give more details?  Reference to paper?  Are you using the same architecture?  What do you mean by ""exactly the same"" images?

Also, what is the dimension of the state representation in between the encoder and decoder?  This should be small and it should be impossible to ""exactly"" reconstruct an image from a low dimensional representation.",1530647968.0
theMachine0094,Looks like the discriminator is learning too fast and inhibiting the generator. Did you try by scaling the losses of discriminator and generator before evaluating the combined loss ?,1530650258.0
DrDetection,"TL;DR We have released our deep learning based face detector that you can run on any device.

The library and examples are available here:

[https://github.com/brodmann17/Brodmann17FaceDetector](https://github.com/brodmann17/Brodmann17FaceDetector)",1530637511.0
make-them-dance,What,1530625688.0
eric01300,Am I on r/deeplearning?,1530627158.0
LewisJin,What does progan do?,1530871068.0
nnexx_,Check out wavenet !,1530602039.0
generic_buzzword,"From experience in audio classification, 2D spectrogram classification works much better than raw waveform classification, at least for me. Be careful not to add artefacts by zero-padding your fft when you take your spectrogram.

As for the actual task, I’m having a bit of problem understanding what the approach would be in particular but I reckon the following approach would be interesting:

Audio waveform -> CNN -> some k-d latent space. I’m not sure if you can/want to make this latent space follow a particular distribution or not.

Latent space -> decoder model to produce key-words. This is your classification task on your training set.

Keyword Encoder -> Latent space (so you can go from key-words to audio via a consistent space)

Some audio generative model (i.e Wavenet) that takes this latent encoding + other info to produce your audio.

Is this sort of what you’re after? 


",1530670640.0
laertez,"About the movie idea: I asked multiple people and found out that they rather prefer an actor/actress then themselves.   So it might not be a big game changer then we think.  

BTW: Most agree that in porn movies this IS a game changer. Yet, the legal and ethical problems are really hard for a data product.",1530602437.0
E-3_A-0H2_D-0_D-2,Try using class weighting.,1530557712.0
assembly_programmer,"The main problem is that, if the font size is too small, the GAN will struggle with the tiny details. What type of model are you using? How deep?
The best way to overcome it would be to make the GAN deeper and use convolution. Depending on the task, you may need 5 conv layers.",1530549672.0
ale152,"You can find it on github:
https://github.com/ale152/muvilab",1530482668.0
Kottman,Ok noob here^^ what does that do :D can u explain it a little bit?,1530484021.0
ScotchMonk,nice work dude! You can license it to some smaller video surveillance companies hehe. ,1530531561.0
eric01300,It would definitely be useful for annotating videos!! Great work,1530518697.0
,Awesome!!! I really needed it. Trying to build a model to detect accidents/crash incident in videos. It's very helpful. Thankyou,1530555206.0
kookaburro,Is the F/OSS license is for non-commercial use only?,1530521643.0
Roboserg,"1. Try both and report the results. My feeling is the 20 outputs model would be enough and should work better / faster.

2. for multi class categorization yes, one hot.

3. softmax for multi class

4. you use 100*100 classes OR 200 classes (animals, colors)",1530454192.0
theMachine0094,"With a small dataset your model will be prone to being overfit. A good rule of thumb is to see if the training data is a good representation (in terms of diversity) of the end-use scenario for the model you are training.

I cannot address the pix2code example you mention, but generally speaking if you are training a model to convert A (in your case image) to B (in your case its js code), sometimes you might be able to achieve B to A conversion with deterministic logic. What that means is that if you can automate the process of generating images from js code samples, then you can write your self a script that creates the data-set. This does not guarantee the quality of the data, but is a good strategy to overcome the scarcity of datasets when playing around with deep learning.",1530302868.0
Roboserg,For my master thesis I use 5k images / text (multi modal input) and 1k test data. It still works fairly good (90% accuracy and 94% average precision @ 480 cutoff). If I had more data though :(,1530343548.0
E-3_A-0H2_D-0_D-2,"workers = number of **CPU** threads to create (creating batches, data augmentation)


max_queue_size = maximum allowable number of batches in the batch queue.",1530266993.0
bartekm3,"Keep in mind that, when using custom generator (not the one that inherits from Sequential) you will probably get into trouble since they will be cloning the same data (not splitting duties between themselves).",1530278750.0
Maleficus187,[https://towardsdatascience.com/howto-profile-tensorflow-1a49fb18073d](https://towardsdatascience.com/howto-profile-tensorflow-1a49fb18073d),1530299460.0
allliam,You should read this paper: https://arxiv.org/abs/1711.10337 (Are GANs Created Equal? A Large-Scale Study). One of their major findings is that GANS have a large variance in their performance between runs and are quite sensitive to hyper parameters (did you change any between runs?). It could be you got lucky the first run. ,1530243810.0
mdv1973,"The algorithm you describe for your regression case is batch gradient descent. That is where you compute the loss (and gradient) for all samples given current set of model parameters, and repeatedly do this to find parameters that minimise the cost.

The mini-batch gradient descent does the same thing, except that instead of the loss (and gradient) for the entire batch of samples, you calculate it only for a subset (mini-batch) in every step.

There is also the stochastic gradient descent version which only uses one sample at each step.

In each case, you start from some initial model parameters (weights), and then repeat GD (on all or some samples) until you have found an optimum set of parameters.

The choice of batch versus mini-batch is not specific for regression versus mnist (classification). It is usually chosen for practical reasons (learning speed, memory usage...) or to insert some randomness in the GD steps.",1530211732.0
StartupJeeliz,test the demo: [https://jeeliz.com/demos/faceFilter/demos/threejs/halloween\_spider/](https://jeeliz.com/demos/faceFilter/demos/threejs/halloween_spider/),1530196310.0
lucidrage,Awesome work! Can deep learning turn me into a hot animu grill yet?,1530228479.0
naturale0,"awesome but, eww",1530240473.0
rulyone,lol,1530203842.0
E-3_A-0H2_D-0_D-2," >  ""expected"" output values are between -1 and 1


What is the activation of your last layer? I'm assuming it's tanh?",1530118979.0
AlphaPulsarRed,Gstreamer,1530112554.0
PM_ME_YOUR_CALL_LOGS,"Try Deepcut or deeper cut. They do multi person pose estimation, so you don't need to specify any bounding box. They detect 14 joint points iirc.",1530075908.0
gamingsherlock,Did you use python for this?,1530019730.0
_ty,"Super cool, I like how you balanced building your own tensorflow models with just using off the shelf components.

While then digitization infrastructure is cool and potentially has applications beyond what you’ve done, I don’t follow how this helps combat fake news.",1530036891.0
julvo,"The problem is that there is not enough GPU memory. Training takes more memory than the evaluation, as the gradients will be stored for backprop. There are two ways to resolve this: 1) reduce the batch size, or, 2) use a smaller model",1530012254.0
PTInvader,What kind of GPU/ GPU RAM are you using?,1530223680.0
No_Free_Dinner,"If you have a project in mind, just tackling it would be the best move in my opinion. You already know how cnn works. Looking at the implementation of the modern architecture would give you an idea about the best practice to code those networks, but might not necessary deepen your theoretical knowledge in deep learning ",1530079147.0
hardhat528491,"Following this right now - [https://blog.paperspace.com/tag/series-yolo/](https://blog.paperspace.com/tag/series-yolo/)

Will try to get this object detector working. Let's see from there..",1530547954.0
chitrang6,Wonderful and Mind blowing ,1529947793.0
VivaciousAI,Fun fact. Motorola uses satellites to do shit like this to see areas where people speed and then sells this info to police so they can then set up speed traps,1529967791.0
E-3_A-0H2_D-0_D-2,"Not really well acquainted with Tensorflow, but the docs in Keras show you a very illustrative example.


[Here](https://keras.io/applications/)'s the link. Navigate to the section named **Fine-tune InceptionV3 on a new set of classes**",1529933023.0
nnexx_,"You should use reverse kinematic with Lagrange method to solve it analytically.

You are trying to find the transformation function f that minimize travel time, this is hardly a deep learning task.

If you really want to stick with ML, do reinforcement learning with the final score being how close you are to the target final positions. Check out Q-learning, but be careful it’s a pretty deep rabbit hole ",1529905005.0
DevFulch,Could you be more specific as to what you are trying to optimize?,1529900188.0
MartianTomato,"I doubt you'll be able to compete with transfer learning from a pretrained Imagenet network... 22000 / 20 classes gives you 1100 images / class. 50% test accuracy on 20 classes is not a terrible result: 85% on CIFAR10 was once considered very good... and CIFAR10 has 5000 images / class for 10 classes. Maybe you can do better, but why exactly? ",1529872609.0
pronobozo,use the imagedatagenerator.,1529873254.0
neuvfx,"Just started lesson 10 this morning, within a couple hours its now all around me.",1529874532.0
EndyJBC,"The laptop you just referred above is more than enough to run deep learning applications (especially if you just started learning stuff and wanted to run your code and see how it performs). This might be enough for 50&#37; cases, but when you want to test out heavy deep learning application that consumes GB's/TB's of data to load, train and to do enormous amount of computations, you might want to divide your workload among series of clusters which is nothing but the cloud. Let's say you calculated and found that your laptop is capable of doing a particular task in 10 minutes. It literally means your laptop will be hung and busy for 10 minutes. At the same time, 5 parallel clusters with much less power can divide and complete the task in 3 minutes. This ETA is very crucial in production because of competition grabbing customers and business. Now you see why companies goes for cloud :) For individual use, yes this is perfectly fine, but as your research grows high and when you deal with much heavier applications, you might want to see other options.",1529842858.0
country_dev,"Depends on the instance. I use a p2.xlarge instance on AWS, I don’t currently need anything too beefy, and it runs for $0.90 an hour.  Which is roughly $657 a month. Is continuous use really necessary for you?  It takes seconds to start and stop these instances, so if you’re not using it or training a model, shut it down and save some money.",1529840904.0
chuongdk,1070 is very good for DL. cloud is expensive because data center need to buy 5000$ graphic card which has the same performance as a 500$ geforce card,1529863849.0
tkchris93,"I use the msi gs65 at work and the model you're suggesting appears to have nearly identical specs. It works quite well. If you're wanting to put Linux on it, be aware that it's not trivial, but still possible",1529848939.0
unkaitopia,Some layers like dropout and batchnormalization act differently during training and testing. Pytorch switches between these two modes by simply calling .train or .eval,1530688151.0
assembly_programmer,"A nice and well done start. Knowing the math and how it works is by far better than knowing how to use a lib (like tensorflow) without knowing how it works. By the way, how long did it take to train? If you want other challenger, i suggest implemeting other model, like RNN, LSTM, GRU, Conv2D, Bach norm. I learned a lot doing it.",1529787142.0
award28,"I took a Machine Learning class at my University last semester and have been enthralled with ML since. I've tried to make a few neural networks in the past but with no success.


Once I had a break from school and time to focus strictly on neural networks, I starting reading the [Deep Learning Book](http://www.deeplearningbook.org/) and [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com) online Books. Both helped me cement an understanding of how Neural Networks work, and the latter really helped me understand and successfully build my first Neural Network!

I was super excited and wanted to share with the community. If you'd like to take a look at the code and give me some criticism I'd love to hear what you have to say. Also, if you like what you see feel free to leave a star :).",1529784751.0
LewisJin,In which language do u using？,1529818224.0
Karyo_Ten,"You don't correct the input values, you correct the weights so that the transformation f(input, weights) == output. I.e. your input doesn't change but you adjust the weights so that for given inputs it maps to the proper output. Gradient descent is like checking how much change in weights/bias contribute to the change in output. And you adjust weights depending of those contributions (SGD is change * learning_rate, Adam adds momentum components).

What you are suggesting is like saying:

  - Okay, this box is not stored at the proper place
  - You fix reality by moving the space around the box
  - Now box is at the proper place.

Input is reality, don't change reality, move the box.",1529753751.0
bonkersone,Take a look into corenlp from stanford,1529744182.0
zell10_10,Look up into CNN for text by Kim. It out performs most statistical method and there are source code online. Provided you have the data to train your own sentiment analysis model. ,1529758774.0
Klhnikov,"Hum your model dont looks like it have 14 layers, maybe you did not cleared the backend session between tour tests...

Try K.clear_session() on top of your model def... With K as the backend",1529739635.0
assembly_programmer,"I'm not the best with Keras, since i use tensorflow, so i could be wrong. But my guess is with the dimensions. What are the batchs dimensions? Since you added a dense layer, keras expects Y batch to be of shape (batch_size, 8192). If your batch is of shape (batch_size, 4500, 8192) that would be incompatible.",1529787615.0
snes-jkr,"Your video steam is just a sequence of images taken by the camera (unless you link it in some clever way to be sequence dependent). 
object detection API will do object detection, not character recognition, but might serve as a first step to get the region of text.  You can get quite far with classical CV already. Have a look, you are probably not the first one having this problem.

A quick google revealed there is some project already for that: https://github.com/khitk9738/EyeVis",1529740238.0
as_ninja6,"First of all, You'll need a huge dataset for this. After collecting your own dataset or picking an existing dataset, choose the architecture there are some architectures for text detection like resnet and CTPN. I think if you are beginner pull a pre-trained model from github and use transfer learning on your dataset. since this is live the model need to be quick and tensorflow lite will be a better option for mobile deployment",1529740919.0
DemiourgosD,"Can you link the github repo, I see there are more than one called Lime.",1529674179.0
antiquemule,"We don't look for local minima, but since they are easier to find, they turn up unwanted. Then clever strategies are needed to escape from a local minimum to go off and look for the global minimum that we are really looking for.",1529660390.0
LongjumpingGift,We calculate error on each example which gives us loss function and we want to reduce the loss. We try to reduce the loss so we try to find optima. We always try to find global minina. ,1529658036.0
antiquemule,"Use robust statistics to stop outliers screwing up your fit too easily. E.g. minimize the absolute deviations instead of the squared deviations. I always do that, even when working in Excel.",1529660593.0
its_ya_boi_dazed,Cooks distance comes to mind. ,1529653393.0
karthi6019,Regularisation means we are solving curve fitting/overfitting problem to generalise model by adjusting the model weights... So it may minimise/maximise weight values...it doesn't contribute to vanishing gradients,1529650965.0
antiquemule,"Regularization is the process of setting the ""right"" value for a parameter that controls the tradeoff between overfitting and ""underdescribing"". You want your curves (if you are in 2D) to be just wiggly enough to fit the data, but not so wiggly that they are fitting noise. The weight values are not necessarily small. They are just big enough to get the desired wiggliness.",1529690263.0
thisismyfavoritename,"None. Define a ""scoring"" function and sort.",1529638049.0
Pik000,I wouldn't say that's really a deep leaning /ml problem. If you wanted predict who would get in etc that might work but for straight ranking choose points for each feature and add them up.,1529641085.0
Karyo_Ten,"If you can do a Msc in Data Science, go for it.

Start playing on Deep learning tutorials and competitions on [Kaggle](https://www.kaggle.com/). Share your code in repo after competitions, it will help you tremendously to build a portfolio and stand out in the future.",1529592136.0
hergertarian,"I think I'm in your shoes, but a few years down the road. I did in undergrad in Physics (astro) about 4 years ago, and afterwards went to an MS Data Science program. The job market was less saturated then, but everyone in the class had a great job within 2 months. Looking around now, many of my peers from grad school have had varying levels of growth and success, but those who have really applied themselves are stepping into advanced or management positions. 

If you'er interested in going down a deep learning path, I might recommend 1) beefing up your Python / coding skills, and 2) start building projects. For 2, it might be helpful to start w/ working through [computer vision examples](https://www.reddit.com/r/deeplearning/) from Keras's examples. 

Hope this helps! 

(edit) Full disclosure, I'm an instructor at Metis, where we teach data science to people and companies. ",1529594171.0
whatarelightquanta,"i am also an undegrad physicist studying deep learning, i would like to study together if you are interested",1530733819.0
ThenThereWereThree,"
 The former has a higher CPU clock rate and RAM, so I would run with that. ",1529583464.0
biasOfLearn,"You should read the paper that ""created"" leaky relu, they did not gain any substantial advantage using leaky relu over relu ",1529576223.0
npielawski,"The fire model has two arguments: <squeeze> and <expand>.

1st step: 1x1 convolution of depth <squeeze> (input: X)

2nd step a: 1x1 convolution of depth <expand> (input: step 1)

2nd step b: 3x3 convolution of depth <expand> (input: step 1)

3rd step: concatenate (input: 2nd step a + 2nd step b)

There is a ReLU after each step.

In the SqueezeNet architecture, the fire modules have the following variable settings (squeeze, expand): 2\*(16, 64), 2\*(32, 128), 2\*(48, 192), 2\*(64, 256)

Is that the type of answer you expected?",1529573901.0
,[deleted],1529539100.0
wobeert,"Apparently it's possible. If you have a GitHub with good content you have a good chance. Iirc, Ian Goodfellow said that he contacts people if they have a project that solves some sort of major DL problem. 

There's plenty of free stuff. Before taking any courses online I would strongly recommend standfords csc231n class (videos on YouTube). I'm not sure how much background in math you have but I found it fairly easy to understand. I think you just need a good understanding of probability and statistics. If you want a good intro on the subjects you need, head over to Deeplearningbook.org and look at the first 5 chapters. It gives you a quick intro of the maths you'll need. From there you can take a look at some of the subjects you need to touch up on.",1529541557.0
nunz,"I'm about to make the switch, professionally, to the analytics, machine learning, and deep learning arena, where I have been mostly IT, systems and software architecture for the past 8 years.  


If you are serious about getting into ML/DL, by all means take Andrew Ng's courses. That's what I did, and it paid off. It gave me enough working knowledge to apply to real problems at my day job. Then, lo and behold, an opportunity arose for me to make the change.  


The worst case scenario is that you will be prepared for something new. I truly think that DL will power a lot of the future, and having a background in it at its start is hugely important. I went so far as to build my own DL rig, which mines Ethereum when I'm not training models on it. :)  


Whichever way you go-- best of luck to you!",1529548114.0
luongminh97,"I'm just a lowly undergrad so I don't really have any authority, but here's my two cents on the subject: if you're still in college like me, just do a statistics/math major and double that with a CS major/minor if you have time. If you follow this path, then digging deeper into deep learning afterwards would be very achievable. Even if deep learning turns out to be a fad and dies down within a few years, your degree is still very valuable.",1529566536.0
datalligator,"Hmm I wouldn't bet on getting hired just because you've done a bit of ML no matter how deep. There's more to this field than just hacking on a grab bag of techniques. We had the same ""shortage"" in data science and employers asking for a masters/PhD in Maths/stats/physics or 15 years experience of some domain is just another way of filtering out the gazillions of CV's from loads and loads of bright young things.  Find a problem you'd like to work on, do some good work on it, then find a bunch of people with funding who share your interest (or go get some funding yourself) and bang on their door until they will see you. The only way you'll get through the noise off the bat otherwise is to have both deep technical skills and more importantly domain experience which takes a few years and a few jobs to gain and in which case they already know about you and have probably been headhunted for any role. I think showing a genuine commitment to a problem domain and the tenacity to work on it, come what may, and then be able to articulate your findings and insights would sell you to me.  It's important to talk about the set backs, how you worked through them as well as all the good things you have achieved. It's doesn't have to be original research but novel and creative application to some non-trivial problem which you care about would work better. Be motivated to learn ML and everything it requires (quite some mathematical sophistication) because you can think it can help solve a problem and not just the size of your pay check. Good luck. ",1529588710.0
pwaine,Wondering the same thing!,1529535210.0
Karyo_Ten,"What framework are you using? TensorFlow/Keras used to have HWN (height, width, batch id) convention while all the others have NHW convention. You seem to be missing a permutation of axis at the very least

Regarding medical segmentation have a look at U-Net, it's made for that. Also be sure to check the kernels for Data Science Bowl 2 (heart segmentation) and Data Science Bowl 3 (lung segmentation) on Kaggle as well as Luna16 (lung segmentation).

Good luck and have fun",1529537262.0
brokenplasticshards,"Gonna need more information than that to help you. What framework are you using? What is the error you get? What did you expect to happen instead? Have you preprocessed the data properly? Can your loss function read the data well?

If you get no error, use print statements to see where your code terminates/crashes. If you do get an error, and you don't understand it, print some info about the variables/structures involved. Also try and see if your network does run with lower resolution images or smaller filters, because it might be a memory issue.",1529514762.0
GodofExito,"So from [https://keras.io/activations/](https://keras.io/activations/)
> You can also pass an element-wise TensorFlow/Theano/CNTK function as an activation

So you just build the Leaky ReLU using tf functions available to you (tf.where for example)",1529516116.0
kvechera,"Founder here, please ask you questions",1529522740.0
antiquemule,Can you estimate how much cheaper this will be than buying from AWS or CloudML?,1529660740.0
KlikTrikN,"How much computing power can you sell? How much ram is needed ? Can you sell just let say iMac 2011 computing power that you are not using all day ? 
",1531657078.0
pwaine,"Extremely interesting!

I’m just starting out with zilch programming experience... does anyone have a view on how long it would take the average (reasonably mathsy) person to learn enough to use something like DensePose? Thanks!",1529535443.0
water-and-fire,"Is Microsoft going to push out new libraries and offerings then retire them every year?

- Azure Machine Learning 
- Azure Machine Learning Studio 
- Azure Cognitive services for computer vision and NLP
- Azure Machine Learning VM

All these services are in alpha or beta yet they charge you for using them. The demos are buggy and the documentation is often incomplete. 
Most of the building blocks of their services are also from some FREE open source machine learning libraries. 

If Microsoft is that good at AI why don’t I see they use their software to snatch a top spot at a open source machine learning competition like DawnBench 

http://dawn.cs.stanford.edu/benchmark/",1529547825.0
nnexx_,"I think it allows for bigger gradients so it trains faster.

Think about it : when you learn a complex task like calculus for an example, you learn step by step, you begin with a simple task like a basic addition. Then if you get it right you can make the problem more complex step by step.

This is what’s happening in a GAN. When the Generator gets it right, the Discriminator becomes better thus the task harder. When the generator gets it wrong, the discriminator barely change, allowing the generator to « do another try ». 

If you start with a pre trained Discriminator, the risk is that the Generator is not likely to succeed at the task, thus it will be very slow to train in the beginning (it has no obvious improving directions as it gets basically everything wrong). 

I think GANs with pretrained Discriminators exist, but you have to be careful with them",1529482079.0
tpinetz,"""but I was wondering if training the discriminator first as much as possible before using this trained discriminator in the (loss function of the) training of the generator would work equally well."" - It is actually the convergence condition of the Wasserstein GAN, that the discriminator needs to be optimal. However there are all kinds of problems doing this in other GAN variants, from vanishing gradients to mode collapse.

The problem with optimally training the discriminator is that you have to do this after every iteration of the generator, and while the generator distribution mostly does not change a lot, the discriminator might have to change a lot. For example consider the simple problem of learning to estimate the mu parameter of some gaussian distribution. The generator will at some point overshoot the mu parameter and then the discriminator has to completely shift to adjust to that change. Therefore, it is hard to tell, when your discriminator is actually converged/optimal, especially in the non-convex domain of NN.",1529483577.0
spacehit,"Maybe this will help you:
(labelme)
https://github.com/wkentaro/labelme/blob/master/README.md",1529520503.0
zeref08,Wow thats really amazing!,1529410178.0
weelamb,Would love a code release on this. I’d be interested in training on a v100 for bigger results (if you haven’t),1529418263.0
dewayneroyj,Depends on your Mobo. I have an Ryzen build (Mobo that only accepts AMD CPU). The GPU is more important than the CPU. ,1529387591.0
,[removed],1529924129.0
secularshepherd,That’s pretty much impossible if you’re doing it right. Maybe double check if you’re running on the GPU?,1529367671.0
mittalsuraj,"Try increasing the batch size!!!
I’ve generally found those to be the culprit ",1529378035.0
TrustAnonymity,"Thanks everyone! I think it was a silky mistake from my end. I was running the whole process for each example sequence.
And since the data is quite large, it was taking considerable time. 

I'm now performing mini-batch training (performing an update after each mini-batch and no updates after each epoch) and it's quite faster. 

Thanks again.",1529489078.0
prithvi45,"Andrew Ng and Siraj Raval's video tutorials are quite popular on internet these days. But would like to recommend this video tutorials [`https://www.edyoda.com/resources/videolisting/99/`](https://www.edyoda.com/resources/videolisting/99/) 

Found it to be very informative, lot of hands on coding is covered in this course. The best thing is, **It's Free !**  ",1529485644.0
sauravshenoy,Stanford has some amazing tutorials along with Udacity ,1529326434.0
ale152,"3b1b is the best way to start
https://m.youtube.com/watch?v=aircAruvnKk",1529328436.0
adikhad,Andrew Ng,1529347617.0
k9thedog,"If the goal is to detect if the string is one of the strings from the list, maybe machine learning is not the best tool for this.

But assuming you're just toying with it and you want to see what kind of japanese-company-like sequences your model will accept, it might be an interesting problem to play around with. Since your input is not a sentence, but a series of characters transliterated from another language, how about treating it as a sequence of characters? With the number of characters < 100, you can just use one-hot encoding for each character.

You can also try generating funny strings that sound like Japanese company names (""Nimdaito"", ""Asakamita Corporata"" etc).

Karpathy's old blog on the topic: [http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)",1529329903.0
hega72,"Habe you considered seeing it as a computer vision problem ?
",1529318744.0
TransferTrip,I want an easy-to-use program/plug in (after effects) that lets you input your own style and uses seamless mapping and has variable permitters so you can change the effect to your liking. There are smart people in this community! Why isn't anybody doing this yet!,1529287668.0
sksiitb,MGMT is amazingly trippy!,1529300883.0
E-3_A-0H2_D-0_D-2,"Really seems interesting. Would love to play around with the data as well, if you allow it. Would you mind uploading the data to Kaggle/any other data repo?",1529308144.0
hockeyalexx97,"This was an interesting read. Thanks for sharing. I’ve read several papers on doing a recommendation system that also implemented a model to rate the outfits (which would be used to recommend the outfit). 

What accuracy were you getting? I’d be interesting in looking at the data - as you said you were willing to share! ",1529244591.0
hergertarian,Awesome use of representation learning! It would be awesome to see how the triplet loss works for emerging fashions / data that's shifting over time. ,1529594773.0
nnexx_,"If you’re data is temporal, think you should avoid using lstm.
Prefer Wavenet and dilated convolutions. It’s faster to train, less prone to overfitting, it profits from avances in computer vision, it’s lighter and make a whole lot more sense (kernels, time delays, filters, derivatives, integrals are trivial with a wavenet, but quite a hastle with LSTM)",1529138440.0
akTwelve,"Every library I've tried has this problem. Of course this happens with all software, but these machine learning frameworks are changing particularly rapidly.",1529143170.0
372995411,"thank you, decent content.",1529122180.0
tecra1776,"Subbed, thanks bro",1529183937.0
antiquemule,"Cool summary of the latest trends from the chairman of a Data summit conference. My attention was grabbed by the use of graph representation to apply deep learning to fraud detection. Speaker is from Paypal. No sign of the slides online, so I'll check the guy's publication record.",1529661347.0
secularshepherd,Can you ELI5 how the mask r cnn solves the issue of the imbalanced data (Waldo to not-Waldo)?,1529098393.0
eric01300,It’s surprising that the network was successfully trained only with few images.,1530003630.0
maratmkhitaryan,overfitting?,1529033178.0
dewayneroyj,Machine learning models be like...,1529077646.0
nunz,Looks like a blatant money grab. What value can you get from this course that you can't get from a $40 book? ,1529060916.0
No_Free_Dinner,"Let x be an input image and F be a series of conv/relu/conv/relu/ etc...   


Say we initialized our network's weight in F from a unit gaussian. If we do not have skip connection, the network would need to learn the appropriate weight to learn x = F(x).   


Whereas if we have skip connection, it is easier to set the weight in F to 0 and we have x = 0 \+ x",1529001605.0
StartupJeeliz,"GitHub: [https://github.com/jeeliz/jeelizWeboji](https://github.com/jeeliz/jeelizWeboji)

(With this library, you can build your own animoji embedded in Javascript/WebGL applications. You do not need any specific device except a standard webcam).",1528996839.0
rylaco,"Interesting, I would check it out, how many avatars do you have or could we map it to anything?",1528994654.0
somewittyalias,"There is no easy answer.  The question you are asking, or more generally solving A x = b for x, is pretty much all of what linear algebra is about.  There are different methods to try and solve this.  You are not even guaranteed that there is an answer.

I would suggest you read on linear algebra.",1528986917.0
somewittyalias,"For the future, questions like yours should be posted on 

https://www.reddit.com/r/MLQuestions/",1528994071.0
mdv1973,"multiply both sides of the equation with the inverse of the matrix (X' \* X), and simplify to get

h = inv(X' \* X) \* X' \* Y

(all matrix operations)",1529004876.0
somewittyalias,See https://en.wikipedia.org/wiki/Linear_least_squares_(mathematics),1529022116.0
nickbuch,"I may be interested.  Can you discuss high-level how you intend to overcome ""Catastrophic Forgetting"" ?

arXiv research paper for reference

https://arxiv.org/abs/1708.06977",1528916510.0
Geeks_sid,"I know this paper. I've got it worked up for classification. I used distillation loss to make sure that it doesn't forget the older losses. I also tried another approach to freeze the whole network and concatenated the neurons. But for detectors, I have zero idea on what's gonna go where atm.",1528916945.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/u_fkxfkx] [Anybody working on or wants to work on Incremental Learning for Object Detectors?](https://www.reddit.com/r/u_fkxfkx/comments/8qw9jj/anybody_working_on_or_wants_to_work_on/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1528925740.0
TrustAnonymity,"Seq2seq course on coursera deep learning specialization would be a good start in my opinion if you want to understand different architectures with the practical exercises. If you have any further questions, please don't hesitate to ask. ",1528893848.0
DeepInEvil,[http://karpathy.github.io/2015/05/21/rnn\-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/),1528838596.0
tlkh,"If you have “money to throw”,  the only logical choice is a DGX Station from NVIDIA. You’ll get support and service worldwide too, and it’s pretty much the most powerful rig you can put on your desk. ",1528777273.0
spaceguy,"What do you mean by:

>it doesn't implement the \_\_call\_\_ as the function with corresponding signatures in that example.

Because `tensorflow.python.layers.base.Layer` defines a `__call__` method, the `CudnnLSTM` class inherits it.",1528765031.0
brokenplasticshards,"Theoretically, it could, but the artifact must be somewhat clear to humans as well. The network would look for patterns such as jpeg compression artifacts or something, and ignore the meaning of the natural image. Of course, it'll not perform well on artifacts you haven't trained it on. So you need lots of relevant data. And patience.

And how would you handle things like an non\-corrupted image/screenshot of a corrupted image?",1528753170.0
372995411,"Found the bug, I've only trained the net for 1 epoch.",1528776379.0
JustinQueeber,"> I changed:
>
>   model = build_model([1, 50, 100, 1])
>
> to:
>
>   model = build_model([1, 49, 100, 2])

Just because you are trying to now predict two future data points as opposed to one, you do not change the output shape of your network to 2. Instead, you are predicting two single **timesteps** into the future. You are using the same network that predicts one single output, but are now unrolling it two times into the future, as opposed to one. Just the same way you unrolled it 50 times for the input data to learn from. Each of those 50 unrolled timesteps only had one output, and so the two future unrolled timesteps also only have one output.

Googling ""unrolling a recurrent neural network"" should lead you to a better understanding. Essentially, unrolling an RNN just means repeatedly using the exact same network (1 input and 1 output, in your case) at every time step of different, sequential input data points, passing the hidden state of the network between each.

I use TensorFlow and not Keras, so I am unfamiliar with the `model.fit` and `model.predict` functions or how they work. However, if you are predicting two timesteps into the future, you must first make the first prediction, take this predicted output and feed it in as input into the following timestep, thus allowing you to make the second future prediction.",1528719779.0
E-3_A-0H2_D-0_D-2,You can label your layers. You can set a layer name by the 'name' argument. Would you please call model.summary() and show us the log?,1528708019.0
sa59,More buzzwords please!,1528650746.0
PTInvader,"I've gotten the OD part down, but I'm unable to obtain the lower-level attributes without building a ton of image classification models. 

Basically what I was hoping to get is: 

- **Input**: An image let's say [this example image](https://i.pinimg.com/originals/2f/6f/b6/2f6fb655a598d25e15fe733d616b2461.jpg)


- **Output**: I think it would make sense to get the details in a hierarchical manner for each box

  - Top ---> Shirt --> (long sleeve, plaid, cutaway collar(or whatever it is).  

   - Bottom  --> pants --> (black, full length, fitted)

I appreciate any help from anyone who can point me in the right direction. 
I have a taxonomy of attributes ",1528686961.0
Kushashwa,"Good question. Here is a good read on this, I believe this should help.

> I assume that your optimization reaches some local minima and continues to move around the minima. At such state the objective function and validation performance should both become stationary distributions and the optimal value should occur with uniform probability anywhere between when the epoch when the local optimum is reached and infinity.

Link & Source: [https://www.researchgate.net/post/How\_does\_one\_choose\_optimal\_number\_of\_epochs](https://www.researchgate.net/post/How_does_one_choose_optimal_number_of_epochs)",1528554543.0
somewittyalias,"If you are only going to be ""playing"", you can just stick to your CPU.  They are not that bad.  For reinforcement learning, they were faster than using a GPU, but maybe they improved the code so it's faster with GPUs now.

If you really want to use your graphics card, read the doc of the different frameworks.  I know tensorflow does not really support AMD at the moment.  

Just reading and understanding the basic math and theory should keep you quite busy until you buy a better card.  You can still run basic experiments on your CPU.  People mostly use the MNIST handwritten digits when playing around with neural nets and you can do that you your CPU.",1528513552.0
brouwa,"If you want to start tinkering with deep learning models, I suggest you take a look at Google Colab. You can use free GPU backends - with some restrictions indeed, but that should nevertheless keep you busy for a while.",1528538424.0
ryukinix,Buying a calculus book,1528506197.0
tlkh,"If you want to run your code on a GPU (and since you’re just getting started) Keras + plaid.ml is a good option. I’m not sure if it works on your exact GPU, but they do support running on Radeon GPUs. ",1528522923.0
Echsu,You might want to check out AMD's ROCm project: https://github.com/ROCmSoftwarePlatform. They have ports of Caffe and Tensorflow that should work on AMD cards with some tinkering.,1528543930.0
maykulkarni,"Tensorflow HIP claims to support AMD GPUs so you might want to try installing it. Last time I tried doing it, I gave up due to some obstinate installation error. ",1528514075.0
GuerreroJaguar,"I don't know about character analysis, but I work in time series analysis and forecasting. 

In time series we usually convert the time series (a sequential set of samples equidistant in time) into a set of vectors. These vectors are built by rolling an sliding window of lenght *m* over the time series. So you will end with vectors in the form:
[s\_1, s\_2, s\_3], [s\_2, s\_3, s\_4], [s\_3, s\_4, s\_5]....[s\_{N-2}, s\_{N-1}, s\_{N}]

for all s in S, where S is the time series and N is the lenght of the time series for an *m*=3

So, the number of inputs needed by the RNN is this *m* value. ",1528504899.0
,I like you,1528478341.0
captainskrra,Ever heard of the KATE editor? xD,1528478782.0
luongminh97,"It's usually cross entropy, which is -1 times the label times the natural logarithm of the softmax ( -y * ln(y_hat))",1528378792.0
DeepInEvil,reconstruction loss and KL divergence loss in case of VAN generally,1528472187.0
pynberfyg,How is this relevant to deep learning?,1528382294.0
Penguin474,Who posts to a sub without reading any other post on the sub? haha,1528383149.0
rylaco,I guess we need to apply a clustering algorithm to find the hidden relationship here.,1528400434.0
rylaco,Another one.,1528401225.0
thisismyfavoritename,"Why don't you expect them to be integers? They represent a size.

First one is the # of input features, if your time series is univariate then its 1.

2nd and 3rd are number of hidden units in your 2 LSTM layers. Those numbers are arbitrary and should be tuned.

4th is 1, because you're predicting a number, hence when you evaluate the loss your model must output a scalar.",1528334723.0
janvandepoel,"I personally followed the fast.ai course right after taking the DLspecialization. It is a very practical course for the latest in deep learning. I just published my review today: [Up to speed with the best in deep learning](https://www.zerotosingularity.com/blog/fast.ai-up-to-speed-with-the-best-of-deep-learning/).



",1528269790.0
desperatetomorrow,"Good post. Right now I m taking up part1 2017 version. I like the top down approach of fast.ai and the way fast.ai library is built making learners less job to do on the logistics n utilities setup easier, but I am skeptical on How fast.ai library is adopted in industries ? ",1528290456.0
ScotchMonk,"I have exactly the same thoughts as the author of the post. The Coursera courses by Andrew Ng are excellent too, to understand the DL fundamentals and the intuition behind the algorithms . Coursera dun have SW libraries ... you build from scratch so you know what's going on under the hood and understand the limitations. Then you can truly appreciate fast.ai library in real world applications.😁 ",1528283430.0
matthewsilas,"I watched part 1, did the ML course, then part 2, now part 1 again, but I'm trying to do it with just vanilla pytorch... it's near impossible (for me). I'm OK with vanilla pytorch & decent in keras.

My biggest complaint is that the fast.ai library is undocumented & too magical. Last weekend I put a debugger into the `fit` method to see what was going on so I could reproduce it. Even after watching all the videos, I'm still not sure why he chose the layers he did, or how he found the parameters for each. The `fit` method is basically a bunch of conditionals that inject layers with default values. Clearly the end result is great, but if I can't implement it in vanilla pytorch, I feel like I don't understand it. I keep hoping he'll post a video where he hits the debugger, and walks through the _why_ of every step because until then, i'm just a monkey pulling a lever.",1528331556.0
ScotchMonk,"I haven't  gone really far in fast.ai, tend to fall asleep halfway through watching the videos 😁. I usually try to Code and open the video side-by-side.The problem comes in fast.ai library when you don't get the  expected result, and clueless from that one line of code in the library. Most of the time is the input data in your training set doesn't match with the API's requirements. Coursera deeplearning.ai course will tell you why, but if you are a SW developer who just wants to use it and doesn't care about what under the hood, it's still fine. That's when fast.ai library is handy. ",1528329866.0
BhavyaGul,Ssh into the server from your terminal. There find home/[your username] by running the command cd /home/[your username]. There do a pip install or whatever for Keras. ,1528250890.0
Moni93,"Issue solved:
I should have called the tensor associated to the operation, and not the operation itself.",1528203189.0
E-3_A-0H2_D-0_D-2,Try the FIW \(Faces in the Wild\) dataset,1528187570.0
dewayneroyj,"It’s seems like the field of deep learning that you’re interested in is ‘Computer Vision’. There are are plethora of resources openly available on the internet. Check out some Udemy courses, medium articles, YouTube videos, etc. I learned by simply choosing a problem I wanted to solve and focusing on how to implement everything along the way. ",1528165014.0
not_so_tufte,"One of the things that's hard when you're getting into research is this ""unique idea"" piece. The reality is, you will RARELY have a truly unique idea. I've had moments where I have had a totally original idea \- totally from myself \- but then done some research, and people had this idea 50 years ago!

So, don't think that you need a totally, never\-before\-seen idea. What will make you unique is your \*context\*. So maybe you want to do an analysis of hospital data, like, ""Are there days of the week where people get injured more?"" People have this data, and they have done these analyses. But you know what they haven't done, probably? That analysis as it relates to your specific city, or county, in the past five years.

So my recommendation would be: read blogs and articles, find something that is interesting, then take that exact same idea and apply it to something local. You'll have to hunt down the data, do the interviews, write up the analysis. And that's as good of a learning experience as you need.

Edit:  Also, I'd recommend focusing on machine learning, rather than deep learning. Same ideas, more accessible. ",1528172134.0
PTInvader,"I would recommend you to fine\-tune a pretrained models on something like imagenet or open image dataset.

Check out the TF object detection API and pick one of the models in the model zoo to fine\-tune",1528169354.0
Thiccpeas,COOL!,1528131082.0
abinmn619,There are some android apps out there that solve handwritten math problems. I have once tried an app named PhotoMath,1528083365.0
mdv1973,MS OneNote has this feature. I do not know what tech they use.,1528085921.0
venkuJeZima,That guy is good,1528101093.0
kilgoretrout92,"The discriminator should output a real value between 0 and 1 i.e. binary cross entropy loss. The generator must produce an image, with the same dimensions as the images from your training data. 

        linear = tf.layers.dense(z, 1024 * 8 * 8)
        linear  =  tf.contrib.layers.batch_norm(linear,is_training=is_training,decay=0.88) 
        conv = tf.reshape(linear, (-1, 128, 128, 8))

Note that, 1024\*8\*8 != 128\*128\*8, so I think something is wrong here. This should fix that.
       

        linear = tf.layers.dense(z, 1024 * 8 * 8)
        linear  =  tf.contrib.layers.batch_norm(linear,is_training=is_training,decay=0.88) 
        conv = tf.reshape(linear, (-1, 8, 8, 1024))

Using padding='SAME' for the transpose convolution scales up the height and width to height\*stride and width\*stride. You can use this to decide on the number of layers that should be present in the generator using this arithmetic. Note that the number of filters in each layer is completely in your control. 

All the best !",1528047385.0
DemiourgosD,"Same problem here, no way to use it.",1528055066.0
jaykavathe,Is anyone able to rent out their rigs atall?i have maxed out 1080ti rig running at 16x 16x 16x 16x but haven't heard back :(,1529808610.0
vv111y,"hmm. I want to be on the other end. I offered my 1080, and messaged the founder I want to buy more gear, ie. multiple 1080ti. 
No response. Several people are complaining they are dragging their feet responding.
I already have much of the hardware that they requested which miners don't have, I just need cash for the gpus. 
Maybe I should go solo",1528144163.0
Schnei1811,This is great! Do you know of a resource for how to retrain YOLO from your own data?,1528056159.0
MWatson,"Thanks, that looks like a short and understandable example",1528027029.0
dewayneroyj,"Look into startups working on the problem you’re interested in solving. To make yourself more attractive to potential employers, build a machine learning portfolio of certain projects relating to the field of machine learning that fascinates you. ",1528036931.0
cy1994,"Start [here](https://github.com/kjw0612/awesome-deep-vision#image-captioning), its not very up to date so you can follow the citations from here on Google Scholar",1527997575.0
mtanti,"I like to check the MSCOCO captioning challenge leaderboard which includes links to arxiv papers most of the time (not all participating systems are published though).
http://cocodataset.org/#captions-leaderboard",1528003042.0
mtanti,Look up HPOLib for a standard benchmark for hyperparameter tuning.,1528012057.0
Karyo_Ten,"Getting more data, tuning the architecture of the model, training on more epochs, having  dynamic learning rate would have far more impact than hyperparameter search.

Stochastic hyperparameter search is slow (either through random search or genetic algorithm) and assuming you can't get more data or change the architecture, time is probably better spent on data  augmentation or running for more epochs anyway.",1527981663.0
lmericle,"\>from scratch

\>using Tensorflow

Pick one.",1527922103.0
harvey_slash,Great work! I was hoping you would show the actual conv operation from scratch. Maybe that as part 2 of the project?,1527921724.0
Perseus784,[Github](https://github.com/perseus784/BvS) link.,1527923866.0
Shikshuka,How are you doing with this project so far?,1540819547.0
RidgeRegressor,This has nothing to do with deep learning?,1527905296.0
GreekManifesto,[www.kaggle.com](https://www.kaggle.com) [www.freelancer.com](https://www.freelancer.com)  ,1527806679.0
mtanti,GANs are difficult to use with text because words are discrete so unlike image colours they are not differentiable. So people try to use differentiable proxies such as LSTM states (see Professor Forcing) or the softmax directly compared to one hot vectors (see Adversarial Generation of Natural Language). You can also use REINFORCE to get around the non-differentiability.,1527828132.0
DeepInEvil,try variational auto\-encoders.,1527842691.0
PTInvader,[http://vision.stanford.edu/pdf/Fei\-Fei\_Li\_ICVSS09\_bookchapter.pdf](http://vision.stanford.edu/pdf/Fei-Fei_Li_ICVSS09_bookchapter.pdf),1528171340.0
cameldrv,"If you're doing DL, the total amount of heat is probably not something you can do a whole lot about.  The miners only care about performance per watt, and each GPU can act essentially independently without much communication with the outside world.  Therefore, 1x PCIe connections are fine, and since underclocking the GPUs gives better economics overall for mining, they typically underclock, so there is less power consumption per card.

Using typical DL frameworks, you want as much compute power on one PCIe bus as possible, and you want that bus to run as fast as possible, hence the DGX\-1 and NVLINK, which is even faster than PCIe.  There certainly are training algorithms that run on clusters, but you lose efficiency as the cluster grows due to being bottlenecked by communication bandwidth.

This means that your GPUs are going to be running at 250W each, and that all turns into heat and needs to be removed.

That said, there's no reason I know of that you couldn't put the GPUs on risers and spread them out like the miners do, you just need to use PCIe 16x risers instead of 1x risers.  They are available.  The other option from a noise standpoint is to go to water cooling of the GPUs and CPU and use a big radiator.  This lets you get by with lower airflow velocity and less noise, but it's expensive and annoying to get set up.",1527801911.0
jaykavathe,I am curious. Are you using these rigs yourself or renting out?,1529808258.0
captainskrra,Failure.,1528041342.0
Rezo-Acken,I recently learned doing that in the Avito competition. Pretty efficient eay for dealing with mix data. ,1527771716.0
thewhizz,"They both have the same amount of memory but the 1080 will have a few more cores. I don't imagine you'll see much of a difference in model training times. This seems like a gpu for home use so I wouldn't be too worried about the delta. If this is for business then you should push for a Titan card or something to get as much ram as possible. 

Personally I use a 1070 at home and i haven't hit any project killing issues. ",1527747225.0
Roboserg,Maybe wait for 2080 or 2070 since it will be more bang for buck?,1527768736.0
Chemikill,"Between the 1070 and 1080 I would say almost 100% stick with the cheaper card. The speed difference will be negligible in the long run. In my experience speed has not been as big of an issue as memory size. And similarly to what /u/thewhizz said, if this is for business then getting a 1080ti/Titan/p5000 might be the way to go.",1527758584.0
MasterSama,Get the 1070TI or pay a bit more and get yourself a 1080TI which would be a good deal. ,1527748776.0
Karyo_Ten,"DL algorithms are memory-bound. The faster your memory speed, the faster your training, hence why the 1080Ti with 11Gbps of memory bandwidth is usually faster than Titan with 10Gbps even though it is cheaper.

Note that this can only be compared within the same generation of GPU (Pascal).

Also this is ignoring HBM2 vs GDDR5X vs GDDR5",1527771222.0
ScotchMonk,better wait for newer cards 2080/2070 coming by end of July ... and GTX 1080/1070 prices will drop and then we have better bargains. ,1527785979.0
news2747,"My first Idea would be an Object Detector that is looking for people and then count them. This in combination with sliding window over different parts of a (hopefully) bigger image. And in the end just color the pixels based on the number that was counted.

Some more information about you data sources would be helpful.",1527720402.0
generic_buzzword,"Just on that, I reckon a process could be:

Planar Camera Calibration (as an assumption) -> Head detection -> Recover planar position with least squares -> Bin and form heatmap -> Overlay over a map of plane ",1527766978.0
ihjyuhgyhhg,"Hey op, you still working on that project?",1537194487.0
gunfupanda,"Not a mathematician and my explanation might be off. If someone wants to correct things, feel free. 

Gradient descent is the first derivative of a cost function. The first derivative allows you to calculate the rate of change of your cost function. Think of the cost function as your position. Gradient descent is your velocity towards the minimum. Unfortunately, computers can't really do calculus efficiently, so we approximate it most of the time. That's what stochastic gradient descent is. It's using algebra to fake doing calculus for performance, and it's good enough for our purposes, because we don't need our exact velocity, just the fastest one. ",1527725664.0
otsukarekun,"Say you want a simple linear classifier, as in a line that splits two classes of data. How can you find the equation of that line?    

One idea is to start with a random line and slowly move the line by updating the parameters so that the classifier is more accurate. But, we need to know what direction to move the line and by how much. That's where Stocastic Gradient Descent comes in.    

If you remember your calculus, the derivative of a function at a given value is the slope of the line at that value. So, if we find the derivative of the error with respect to the parameters of the line, we can find out how much the error changes if we change the parameters \(an infinitely small amount\). This is called the gradient. The idea is that if we update the parameters of the line the opposite direction of the gradient then the error would go down. To understand this, imagine a graph with the y\-axis the error and the x\-axis a parameter. On that graph, there is a line, such as a parabola. If you take a random point on that parabola and the slope is going downward \(negative\), you want to move the parameter that direction because you want to reduce the error. If the random point is going uphill, you want to move backwards downhill.     

Then the idea is to just keep moving the parameters in this way until you reach a minimum error that you are happy with. Unfortunately, you can't guarantee that the parameters you learned are the absolute best set \(global minimum\), but they are probably a good set. ",1527740704.0
allliam,"Here is my eli5:

Let's say you have some colored balls and want to give one to each of your friends to make them as happy as you can. You could start out giving each friend a random ball and see how happy they are. Then ask each friend which ball is their favorite and how much happier they would be to have that new ball. You must choose which friend should get to choose their favorite color first. The change in happiness for each choice is the gradient. If you choose the friend whose happiest would increases the most, you are doing gradient descent. If you think your friends might be picky you might do better by picking friends more at random and swapping more (but still favoring friends who's happiness would improve more). This is stochastic gradient descent.

... for machine learning just replace balls with model parameters, happiness for loss, and swapping with small changes in your parameters and the rest stays the same.",1527745293.0
mtanti,"A neural net consists of many knobs called parameters where different settings of the knobs make the neural net do different things. You want to find the setting of knobs that results in the least mistakes when performing a desired task.

Gradient descent is just checking what happens when you increase or decrease each knob. Is the error going up? Then don't use that setting. Is it going down? Then keep that setting and try improving it further. Unfortunately this tends to result in a configuration where no changes will improve the error and yet you can do much better with a very different configuration.

One of the ways to avoid this is to measure the error on a different aspect of the task for each knob update rather than optimizing for the same task all the time. The task aspect, which usually means taking a small part of the problem you're solving, is chosen at random, known as stochastically. This is stochastic gradient descent and it solves the previous problem by changing things up all the time in order to avoid getting stuck in a single configuration as the same configuration cannot be stuck in all aspects of the problem you're solving.",1527753136.0
pgtgrly,"A Neural network can be expressed as a non Linear function of a given input(features) and the weights (parameters). Now a function is something that maps an input to an output. 
Now to compare how accurate something is we need use a term known as loss which itself is given by a function that takes the output of a neural network and maps it to a real number (your metric). Generally, the smaller the metric the more accurate the network. 
Now think of a function as terrain for a given input with the height of terrain changing for each set of weights, here weights are your coordinates. It might be difficult to think this in terms of a multidimensional problem so just imagine that you have 2 weights and extrapolate. (keep in mind that we are keeping the input fixed). When you differentiate the loss metric with respect a given weight (using chain rule, read up about it in CS231n lecture) you are looking for the direction of the weight where the loss increases. Thus you go in the opposite side to decrease the loss. 

Now here is the thing, you have optimised your network for one set of input. There will be more kinds of inputs out there so you repeat the process again with another input. And thus the terrain changes. 

So what is the point of ML if the terrain changes for every input?
That is because there is some underlying pattern underneath all the inputs that is all the inputs can be represented as a probability distribution and thus there will be a function (which upto some extent) will give the minimum loss for all the inputs.

What exactly is stochastic over here?
Two things. 1. We usually chose the inputs in a random order.
2. Since the terrain changes from input to input  it leads to a stochasic behaviour

Why use stochastic GD?
1. Since the terrain changes from input to input, the local minima of one input might not be local minima for the other. Hence it does not get stuck easily

SGD uses one input before changing the weights. Batch GD goes through a given batch and takes the averages in gradient before changing weights.


",1527969293.0
InterstellarRun,You don't need a graduate degree that is specifically machine learning. Doing a graduate degree in computer science (or even other related fields) can get you there as long as you're doing the kind of research you want to get into.,1527733048.0
tpinetz,A dataset list for deep learning without neither cifar nor imagenet feels wrong to me.,1527689958.0
bluesky314, **MNIST has been beaten to death. Its time to erase it from the internet**,1527760215.0
itamblyn,"If anyone is interested, we recently released a DL dataset for quantum mechanics. Data is hosted here: https://doi.org/10.4224/PhysRevA.96.042113.data

The paper was published in December (https://journals.aps.org/pra/abstract/10.1103/PhysRevA.96.042113) with an open version on the arXiv if you don't have a subscription: https://arxiv.org/abs/1702.01361",1527797183.0
codemetro,"Here is a dataset for abstractive summarization using Reddit TL;DRs

1. [Paper](http://aclweb.org/anthology/W17-4508)
2. [Dataset](https://zenodo.org/record/1043504/files/corpus-webis-tldr-17.zip?download=1)",1528120854.0
StartupJeeliz,**Jeeliz FaceFilter API on gitHub:** [**https://github.com/jeeliz/jeelizFaceFilter/**](https://github.com/jeeliz/jeelizFaceFilter/),1527603532.0
E-3_A-0H2_D-0_D-2,"First of all, 25396 images seems like a very low number, *especially* if you're building your own CNN. The n/w will most likely always overfit to your data and pick up sampling errors. Karpathy himself has quoted - ""Don't try to be a hero"". Most of the time, you'll be battling the variance, which will just result in you dumbing down your own network. Have you tried transfer learning?


Regarding the memory problem - It's pretty obvious. You're loading two tensors of shapes (25369, 204, 204, 3) and (25369, 39) directly into memory, which seems to be too much. If you're using Keras, use the ImageDataGenerator utility and it's flow_from_directory method. You can stream images from folders with a specific batch size - very handy while handling a large number of images.",1527583834.0
troltilla,"Yeah. Kaldi is quite efficient as its implemented in C++ and uses optimized libraries like MKL and CUDA for compute intensive stuff. The algorithms are pretty much state of the art, excluding end to end architectures. It is not very novice-friendly, so it requires you to have a certain level of understanding of ASR algorithms and proficiency in Bash (to understand/run experiment scripts) and C++ (to understand the internals and implement a product). In order to train a domain specific ASR you will need a representative dataset for your domain, both in terms of recordings and text data.",1527712087.0
E-3_A-0H2_D-0_D-2,"Not sure,  but you could go for a concatenated NN, more\-or\-less like a Siamese network.

Train one network with 20 input nodes and don't add the last \(in your case\) binary output layer. The second\-last layer will now contain 'encoded' information of the state of the game for one team.

Then again, I think your problem can be modeled to a time\-series domain. You could try the same architecture with RNNs.",1527585535.0
thisCarIsFromCanada,"I was curious about MTailor as well so I searched reddit for reviews. I came across [this AMA](https://www.reddit.com/r/malefashionadvice/comments/2bt602/cofounder_of_mtailor_ama/) which tells me the cofounder has no idea what he is doing. Here are a few key mistakes he made:

1. when they say ""20% more accurate"" what they actually are saying that if they measure someone 4x, the standard deviation is 20% smaller than the standard deviation of across 4 professional tailors.
2. They note that the standard deviation on the neck measurement is 1/2 inch... you don't measure standard deviation in inches...
3. On a measurement of the circumference of the neck vs circumference of the torso, they seemed surprised that the standard deviation was larger on the torso... while it doesnt have to be larger, I would guess that there is a non-constant variance on a problem such as this.
4. He noted that one of the main benefits against other online tailor shops is that people are generally nervous about measuring themselves incorrectly... while that may be true, that really just tells me that they are making money because their competitors inspire more fear, not that they offer a better product.
5. They note that they create a 3d model with your body and use that for the measurements. I am surprised they can get a high level of accuracy without using a 2 camera system.

Final note, I have read review about people complaining about their sizes. I am not sure if it is because only poor fitting users comment or because of something else.",1538841273.0
nnexx_,Use gephi. ,1527532957.0
bajajahsaas,What is the state-of-art for topic modelling?,1527542857.0
E-3_A-0H2_D-0_D-2," > similar looking objects


What do you mean by that?",1527500838.0
GotRedditFever,Never heard of a casual convolution.,1527490071.0
LongjumpingGift,Here its written that its law of rare events..meaning it does not occur many times.. then how does number of events is tending to infinity?,1527483058.0
suki907,"import tensorflow as tf
tf.enable_enable_eager_execution()

Then it all acts more like numpy, executing calculations instead of building a graph. You can use regular python debugging tools.",1527429360.0
rematto,"Doesn't that bug have to do with the input tensor?  You need at least 3 dimensions to your input tensor \(e.g. red, green, blue\).",1527480459.0
metaobject,IMO this is highly problem-dependent.  More info needed.,1527389650.0
itamblyn,"Classical ML is good when you have small datasets with clear and meaningful features. Using a CNN for automatic feature extraction takes a lot of labelled data which may or may not be available depending on the application.

Neural networks aren't particularly good classifiers on their own - they are good an learning features, especially from data which has spatial or temporal encoding (like an image or a sound file).

Another advantage of classical ML is explain-ability - it's easier to tell someone ""we think you have disease D because of X,Y,Z"" rather than ""there exists a manifold of points in a hyperspace of features within which we found a clustering of you and other sick people"". ",1527798327.0
Humblefool_14,I am in. I have read the book some part more than others and willing to talk about it.,1527388201.0
kulchacop,"In case you did not know about this already, this might be helpful.

[Accel.ai Deep\-Learning\-Book\-Review](https://github.com/AccelAI/AI-DL-Enthusiasts-Meetup/tree/master/Deep-Learning-Book-Review)

[Accel.ai Online Discussion: Deep Learning Book Chapter 8 Part 1](https://www.meetup.com/Artificial-Intelligence-Deep-Learning-Enthusiasts-Meetup/events/250962408/)",1527391406.0
Nuraci,"I started the book yesterday. Where can I join? Are there other resources, forums or videos that go along the book that anyone knows of?",1528300292.0
Dresdenboy,[Link](https://arxiv.org/pdf/1704.04760.pdf),1527361783.0
DiscoverAI,"To clear up any confusion, the main difference between the DeepLearning and AI Tutorials subreddits is that AI Tutorials subreddit focuses specifically on tutorials geared not only towards deep learning, but the wide range of Artificial Intelligence, as far as Computer Vision to reinforcement learning down to Natural Language Processing.",1527353568.0
Will_653241,"I did this for a school project but Berkeley’s AI class has a python programming framework to teach AI/ML(get them confused) to students. It’s a Pac-Man game where you can do min/max algorithms. And min/max with alpha-beta pruning. And the last project was implementing qlearning. If you can find it on their site i know it was on GitHub. 


http://ai.berkeley.edu/project_overview.html


EDIT: added link


",1527349914.0
arturs1011,Check this out it is a three part series and helped me alot to wrap my head around q learning http://outlace.com/rlpart3.html,1527367560.0
jayasuryajsk,"[https://www.youtube.com/watch?v=aCEvtRtNO\-M](https://www.youtube.com/watch?v=aCEvtRtNO-M) check this out

[https://github.com/llSourcell?utf8=&#37;E2&#37;9C&#37;93&tab=repositories&q=q\+&type=&language=](https://github.com/llSourcell?utf8=%E2%9C%93&tab=repositories&q=q+&type=&language=) ..these repos contain youtube video link in description ",1527342059.0
Arno_Nymus,"Several possibilities come to mind. Maybe your model is unable to overfit because of lacking complexity. Another possibility is that your loss function does not work as intended, you either reach a local mimimum or the global minimum of your loss and the intended minimum differ too much.

The problem with answering the question is not that it is too theoretical, but that is not clearly enough specified. If you describe the problem in a more formal way I might find other answers. ",1527256692.0
trialofmiles,"Are you using a validation set during training as the mechanism of monitoring overfitting? 

If both your training and validation sets have extreme class imbalance and you are using categorical cross entropy loss, you would expect to see low loss and high accuracy even if your model is ALWAYS producing the “dominant” label.  I suspect your model is under fitting and simply isn’t learning any of the infrequent labels, based on the limited details given.

If you have class imbalance, I’d look at using inverse frequency weighting in your loss or down/upsampling your data to resolve any class imbalance in your training set.",1527333746.0
JackieLoong,"From my perspective, i cant get what  big contribution it has.  ",1527493575.0
doodeoo,The (TBD%) sounds important to answering this,1527205404.0
pepitolander,A fingerprint is about \~1cm² which is well under the 7&#37; of the size of a face.,1527207765.0
batu_tw,"In conv1 layer by 2-stride convolution and afterward in pool1 with 2-stride max pooling, the input size reduces to 1x56x56. I think that choice is simply made by trial and error no solid theory behind that but intuitively higher strides
make us lose some information so better don't do go with high stride values. Even the Geoffrey Hinton thinks pooling is a big mistake but works well :)",1527215583.0
YocB,"Part of the Perceptual Image Restoration and Manipulation \(PIRM\) workshop at ECCV 2018

https://pirm2018.org",1527151196.0
realTommyWiseau1,"I'm not 100% certain on how Amazon's system works (no can be really since it's probably a well guarded secret) but I think the best way is detecting text in images is to use a neural network trained on handwritten letters of the English language. 
You can use Python OpenCV and scikitlearn if you're a beginner. This wouldn't be very sophisticated but it would definitely work with an accuracy of about 60% (I think). 

Use open functions to convert the image to grayscale, threshold it and then use an edge detector (maybe a simple sorbel). Then you need to segment each letter into a separate image. Then pass each individual image to the classifier for classification (the neural network I mentioned earlier) and finally, put it all together in a simple string based on the output of each images classification. 

Start with simple one word images and then go for images with multiple words (the spaces will be tricky) 

Anyway, I hope this helped. I know it isn't very detailed or actually gives you any of the Python functions but those are easily searchable. ",1527109772.0
akTwelve,"Textboxes++ is a cutting edge text spotting and recognition neural network. I've been reading up on it and trying to rebuild it for my own purposes. If nothing else, the code repo and research paper linked in the readme should get you moving in the right direction. https://github.com/MhLiao/TextBoxes_plusplus/blob/master/README.md",1527121669.0
somewittyalias,"The first thing is that you should be skeptical of your own work.  You say it workds for ""most deep neural networks"", but I wonder how many different experiments you carried out.  Did you try to train a cutting edge very large network on imagenet?  That requires big computers and days to train.  Maybe your method only works for a subset of some small networks for a few specific tasks.

Also, did you make a fair comparison with standard methods?  You might not have optimized the hyperparameters very much for standard methods, but spent a lot of time optimizing them for your new method.  You could easily get a factor 10 from unfair hyperparameter optimization.  There is also a lot of work in general to speed up training and maybe you are not comparing your method with the best methods.",1527087761.0
LSTMeow,"It is very plausible that such things exist. It is also possible that this particular find is already being exploited by companies that want to have a competitive advantage in training. Will your method work in large batches like those that are used when doing multi-gpu training? if so, than you can actually think about making money from this, that is, until someone else stumbles upon it or reverse-engineers your method.

Personally, I would find an active researcher with a nice track record and do a joint publication. My opinion is that getting your name on a new solver, especially if it is generally better than everything else, is a good way to advance in this field, and maybe score some of those prestigious high-paying jobs.

At any case, since training strategies are more often black-magic than not, it could be that this was already reported somewhere but as an afterthought.
You wrote that it is for 'most DNNs', is it also true for 'most datasets'? 
",1527088058.0
dosibjrn,"Thorough testing, consider patenting, publish results, license patent.",1527098442.0
reprac,"There are a number of competitions for ai and machine learning that exist for a variety of subject matters. If you've found something that can train fast with decent results maybe try your hand in some competitions?  Regardless of how you do, you'll probably be around others that you can chat with about your new algorithm.  Good luck!  ",1527180929.0
tyrilu,"If this is true, I’d suggest trying to fully verify your claim first. Then, approach an organization like MIRI so that they can try their hand at creating good artificial general intelligence with the speedup. There are only so many opportunities we have where an entity thats actually focused on ethics would have an advantage over profit-based selection.",1527088244.0
casconed,"Work through the [Deep Learning specialization](https://www.deeplearning.ai/) on Coursera. You'll start building deep networks ""from scratch"" and work up to using frameworks like Tensorflow and Keras.",1527083299.0
ai_is_matrix_mult,"Right, everyone learns differently. What about training a network to do something which you can easily generate data for. How about adding watermarks to images and then training a network to remove it. Or using the gray scale version of a color image and training a network to ""colorize it"". I'm purposefully not citing any works since I think it's best to just think of the problem and solution on your own. After your implementation you can reference Dr. Google :)",1527116693.0
OverFlow10,"Along with the already mentioned Coursera Specialization, you might also want to check out the one from fastai. ",1527108793.0
GotRedditFever,What kind of data are you working with?,1527059885.0
Maleficus187,"The input layer are your inputs. For one particular case if you have 8 variables which represent that case then your input layer is of size 8. Each input will connect to each neuron in the next layer by multiplying by a weight specific to that input variable and that neuron and then applying an activation function. The value of the neurons is the sum over all of its connections to the previous layer.

Mathematically this is just matrix-matrix multiplication. If you have 8 inputs and you specify 10 neurons in the next layer then you have a matrix which is batch size x 8 which are your inputs, and a matrix which is 8 x 10 which are the weights connecting the inputs to the next layer. The resulting matrix will be batch size x 10. Then you apply the activation function to that matrix and add a bias term.

In the final layer if you have 1 output then your number of neurons will be 1, so it still follows the same prescription as above.",1527071114.0
f_c_s,You need to apply non Max suppression to the output tensor and other stuff. The functions that do that are cython_utils. I have run the network with a frozen graph without problem using that.,1527108047.0
ScotchMonk,https://blogs.nvidia.com/blog/2016/07/29/whats-difference-artificial-intelligence-machine-learning-deep-learning-ai/,1526987755.0
galpusha,"This image explains well the difference [http://www.machinedesign.com/sites/machinedesign.com/files/Machine&#37;20Learning\_Fig2.gif](http://www.machinedesign.com/sites/machinedesign.com/files/Machine%20Learning_Fig2.gif), it's taken from a book Deep Learning \(Ian Goodfellow, Yoshua Bengio and Aaron Courville\), Chapter 1 ""Introduction"". In the same chapter you can read the intuition behind deep learning [http://www.deeplearningbook.org/](http://www.deeplearningbook.org/) ",1527027167.0
Rezo-Acken,Fast.ai and coursera deep learning specialization are good starting points if you preffer online methods.,1526992304.0
sauravshenoy,"Just started about a week ago as well, Stanford has a bunch of lectures on YouTube are absolutely amazing!",1526996689.0
SubtractOne,"Hi there Jan!

Okay so the basic idea with Q\-learning is that we are trying to learn a value \(Q\-value\) that represents how good a state and its successors are. A higher Q\-value is associated with the idea that the next state plus it's successors will give you the higher potential for a reward. There is a discount factor for how much each of the following states will factor in. Therefore a discount of .999 means the next states are very ""important"" to the calculation while a discount of .1 would mean that they are minimally important.

So with deep Q\-learning, you use a neural net to learn this Q\-value. It is trained with the inputs being the state, and the output being trained to get close to the Bellman equation. Once trained, you would pick an action to take by finding that state it would get you into, running that through the network, and determining which action would give you a state that has the highest Q\-value.

Now the data to actually train it completely depends on the action space and states space in the problem. If you are working with an inverted pendulum styled problem, the actions would strictly be \(left, right, none\). *Also for your question on how to do ""no action"", you would create an action that does nothing.* So for this inverted pendulum, the state space would also be fairly small \- \>\(angle, velocity, etc.\). In my experience this example takes \~100 episodes to converge of around 500 time\-steps, so the data is not even close to an issue. 

However every time an action is added, the problem gains another [dimension of complexity](https://medium.freecodecamp.org/the-curse-of-dimensionality-how-we-can-save-big-data-from-itself-d9fa0f872335). So it takes more and more data, to where something with 15 potential actions\(where the best action could be any combination\) could almost be impossible to converge to a desired solution. Now the one thing to completely change how your algorithm learns is the exploration function. This guides the agent at the beginning, where the most basic one is a  completely random choice of actions. This could work for low dimensional action spaces, but at a higher level, you would never come close to reaching some of the possible states.

Another note is that a commonly used thing is the *replay buffer*, which you can intuitively think as the agent basically retraining on some of the previous data just as you can ruminate about a past event\(or be happy about something that happened, thus enforcing that idea of whatever made you happy should be done more\). This takes a random batch of past states and trains on them, almost like adding more data, and regularizing by losing some of the time dependence.

I hope this answers some of your questions, and if you have any more I'd be happy to answer. I also have a model working on some simulated environments and on a robot if you'd want some example code. I applaud you trying to craft it yourself!",1526930667.0
guruji93,"I found the start up code given in MIT Self driving cars challenge called ""DeepTraffic"" (Google it) much useful to tweak and learn. You will get your questions answered when you experiment ! By the way, get your Q learning part done before moving to Deep Q learning (first things first)! ",1526939866.0
E-3_A-0H2_D-0_D-2,"How about using an elastic EC2 instance instead? Graphics card prices are constantly fluctuating in the market right now (due to the mining boom).


You could also try out Google Colab.",1526912133.0
SomeConcernedDude,"You could go with a refurbished Dell Optiplex and this small form factor 1050 Ti:

[https://www.amazon.com/dp/B06XHZ29N5/ref=twister\_B01M3U7DDB?\_encoding=UTF8&psc=1](https://www.amazon.com/dp/B06XHZ29N5/ref=twister_B01M3U7DDB?_encoding=UTF8&psc=1)

I have an Optiplex 7040, and the Zotac 1050 Ti Small Form factor fits into the PCIe x4 slot. It's a x16 card but still works. However, I notice that its speed outperforms the CPU mostly with single precision computing, not double precision. From what I've read though, using single precision is not too critical for deep learning applications and it allows one to fit more data in the card's memory.",1526915254.0
omg_drd4_bbq,"Pretty much have to go with all used at that point. I spent $700ish 1.5 years ago, with lots of cyber monday deals, and it's a pentium i3 with a gtx1060 3gb. And that was before the crypto boom. Good luck!",1526911383.0
Phnyx,"Buy a used workstation with at least 16 GB of RAM and 4 cores. Even with that budget I would prefer a 64/128 GB SSD with a 1 TB HDD.

This is enough for all traditional kinds of machine learning. For deep learning you can use Google Colab or Kaggle kernels in the browser.",1526921011.0
Jadeyard,Nvidia jetson maybe?,1526939464.0
400_Bad_Request,Build your own one ,1526972679.0
E-3_A-0H2_D-0_D-2,Try autoencoders. The Keras blog has a complete implementation of an autoencoder. ,1526888142.0
suki907,I'd run them through a standard image model and cluster the top level feature vectors.,1526909213.0
kamranjanjua,"Try DBSCAN. It is a pretty good image clustering algorithm. If you prefer hierarchical parameter optimization to find the optimal values of the bounding value in DBSCAN i.e. epsilon, I'd recommend HDSCAN. It tries different values and then uses an optimal fit. I tried both and found that manual selection of epsilon in DBSCAN yields much better results. 
You can find the implementation here here: https://github.com/chrisjmccormick/dbscan.",1526970824.0
snes-jkr,"Clustering what, semantic or visual similarity?",1526889318.0
alwaysrescued,"Good article. It starts a little low, but then it shine.",1527265099.0
absurdlyinconvenient,"hah, that entire flowchart in the thumbnail is for OCR, the actual sudoku bit is just ""Solve Sudoku"", that's some r/restofthefuckingowl stuff right there",1526889354.0
Kottman,I'm to stupid for sudoku ffs xD ,1526883992.0
pgbabse,Nice,1526851926.0
snes-jkr,Pretty cool! ,1526889405.0
for_profit_for_truth,Sounds like a good task for YOLO.,1526770892.0
poiguy,"[ I work on TensorFlow and Cloud TPUs ]

Quick correction on cost - a Cloud TPU currently [costs $6.50/hour](https://cloud.google.com/tpu/docs/pricing), whereas a p3 instance with four V100's [costs $12.24/hour](https://aws.amazon.com/ec2/pricing/on-demand/). Since Cloud TPUs are network-attached, you need to rent a separate VM to drive them, but you can use a very inexpensive one since all of the heavy computation happens on the Cloud TPU. I'd recommend starting with an [n1-standard-2](https://cloud.google.com/compute/pricing), which brings the total cost of using a Cloud TPU to about $6.60/hour - still only 54% of the cost of renting four V100 GPUs.

Since high performance computing is hard on any platform, we've put a lot of effort into optimizing a variety of open-source reference models for Cloud TPUs. For example:

* [AmoebaNet-D](https://github.com/tensorflow/tpu/tree/master/models/experimental/amoeba_net) for state-of-the-art image classification
* [ResNet-50/101/152/200](https://github.com/tensorflow/tpu/tree/master/models/official/resnet), a well-known family of image classification models
* [Inception v2/v3/v4](https://github.com/tensorflow/tpu/tree/master/models/experimental/inception), another popular family of image classification models
* [MobileNet](https://github.com/tensorflow/tpu/tree/master/models/official/mobilenet) for low-resource image classification
* [RetinaNet](https://github.com/tensorflow/tpu/tree/master/models/official/retinanet) for object detection in large images
* [Transformer](https://github.com/tensorflow/tensor2tensor/blob/master/docs/cloud_tpu.md) for machine translation, language modeling, sentiment analysis, and image generation
* [ASR Transformer](https://tensorflow.github.io/tensor2tensor/tutorials/asr_with_transformer.html) for speech recognition

I highly recommend starting with one of our reference models and adapting it for your use-case if possible. What kinds of ML tasks are you working on?

Of course, you can also start from scratch and build and optimize a model for Cloud TPU on your own. If you go this route, I would still recommend analyzing our reference models first, and you are also likely to find our [performance guide](https://cloud.google.com/tpu/docs/performance-guide) and [troubleshooting guide](https://cloud.google.com/tpu/docs/troubleshooting) helpful.

We provide tools for debugging and performance tuning that are integrated into TensorBoard. You can learn more about those tools [here](https://cloud.google.com/tpu/docs/cloud-tpu-tools).",1526758208.0
NicoleK1993,"This project will be presented at Siggraph 2018 in August.

Thursday, 16 August 2018 2pm \- 3:30pm 

[https://s2018.siggraph.org/session/?sess=sess146](https://s2018.siggraph.org/session/?sess=sess146)",1526785467.0
itamblyn,"Once you train a GAN, you can use the discriminator to detect stuff which looks _different_ than the training distribution (https://arxiv.org/abs/1710.08053). Does that answer your question? ",1527799065.0
0xd05,Do you mean Python packages that you can import and use? Have a look at Keras (https://keras.io). It’s a high-level set of APIs to rapidly prototype and develop NNs.,1526609654.0
Six_Machine,The [pytorch tutorials](https://pytorch.org/tutorials/) are pretty good as well. They have different models based on difficulty level as well.,1526894252.0
lilsmacky,"**compile** - When you have built a network, your model, this puts it all together so that it is ready for training.

**fit** - Where the training/learning happens. The compiled model inputs training data containing labeled examples that your model can use to optimize itself.

**evaluate** - Evaluates how accurate the model is. Should preferably be checked on labeled data that was excluded from the training data.

**predict** - Take new unlabeled data and try to label it using the trained model.",1526578944.0
Jandevries101,Responses would be great!,1526924912.0
onenuthin,Do you have any examples to share?,1526583111.0
otakuman,"**Abstract**

This video shows how the neocortex implements neural networks capable of signal multiplexing. The author proposes a model using groups of neighboring pyramidal cells as the minimal unit of an artificial neural network, and tests his proposed architecture, achieving superior results than those obtained with traditional neural network architectures and backpropagation.
",1526539737.0
LynnHoHZL,[GitHub] https://github.com/LynnHo/AttGAN-Tensorflow,1526536263.0
nitinshivaraman,"Backpropagation is a way to correct the initial weights assigned to the inputs and the layers beyond. In supervised learning, after understanding that the output prediction accuracy is low, backpropagation is done to adjust the weights in order to improve the accuracy. 

P.S. It is important to be careful to not overfit the data during this process.",1526522105.0
donghit,"Any differentiable (and sometimes non-differentiable by approximation) component can be part of a back-propagation graph.  

I think there are many poorly generalized backprop explanations out there.  Here is the most [simple case](https://youtu.be/i94OvYb6noo?t=12m21s) and i think this will bridge the gap in knowledge as to what you can use this algorithm for.",1526532543.0
Karyo_Ten,"All neural net layers in frameworks currently use gradient backpropagation for training. CNN, RNN,... ",1526540136.0
itamblyn,"Backprop is an algorithm which is used to optimize the weights of a feedforward net. It can be used for many topologies, including CNN.

CNN are a class of net \- they are usually optimized with backprop, but in principal this could be done another way \(e.g. a genetic algorithm\). Hope that helps.",1527816462.0
sudoankit,"I research on CV/AR and a bit deep RL and use mostly Tensorflow/Keras, PyTorch and MXNet beside OpenCV, PyBullet and Mujoco. 

If you're new to Deep Learning, I would suggest you to use scikit-learn and then gradually as you pick up the important papers/theory of DL ( dropout, SGD. mini-batch, LSTM's, ADAM, SqueezeNet, GAN, VAE etc ) start using Tensorflow.

",1526499363.0
Stochastic_Response,did the person who made this powerpoint create caffe? I dont know anybody in the industry that uses it and in the pros list for caffe it lists quick AND fast lmao,1526514557.0
Rhonin3105,just use tensorflow/keras,1526497605.0
iacolippo,"Use PyTorch or Keras if you don't want to deal with boilerplate code. Use Tensorflow if you want to build a product (although this is going to change in the future with PyTorch 1.0)

I don't know MXNet",1526651795.0
Rhonin3105,Maybe you should provide more details?,1526491335.0
GodofExito,"well, i guess you can as detailed [here](http://lab.fs.uni-lj.si/lasin/wp/IMIT_files/neural/doc/Omondi2006.pdf). Even tho that is quite old.",1526425201.0
m_ninepoints,"Not in general, and not without significant modification. Most kernels rely heavily on having many floating-point compute units available, and FPGAs are not general computational devices in the same way a GPU is.",1526429365.0
StockDealer,"Maybe consider using an Armstrong adaptive logic network instead, which provides a boolean function tree which can be trained and are extremely fast.",1526430161.0
dan678,It seems to be in the pipe for Xilinx boards.,1526430323.0
E-3_A-0H2_D-0_D-2,"Hmm, this certainly seems like an interesting use-case. I'm not really an expert in sequence modeling, but here are my inputs:

 - What volume of data do you accumulate through the hour?

 - Would you be able to give us an abstract representation of your data?

 - LSTMs could be helpful for you if you'd like to detect anomalies *ahead* of time. For instance, given the way the input is at certain time-steps, the LSTM may unroll itself several time-steps ahead and predict the onset of an anomaly, if any.",1526449891.0
mladensavic94,"wit.ai does something like this. You update some existing model with your data, each time you add some batch of data it starts training again and you always use last trained model. 
Since you have big throughput i think it would be good to use same principle. Run 2 processes: 1 that loads last model and continue to train it with lastest data and second to predict in real time with last existing model.",1526470243.0
itamblyn,"Try something like word2vec + an autoencoder, and see if stuff lands outside of the cluster.",1527799172.0
OnceReturned,"Here is one resource:

[https://github.com/sfikas/medical-imaging-datasets](https://github.com/sfikas/medical-imaging-datasets)

Here is another, more extensive one:

[http://www.radrounds.com/m/blogpost?id=1791588%3ABlogPost%3A179002](http://www.radrounds.com/m/blogpost?id=1791588%3ABlogPost%3A179002)

Here is a third, which overlaps somewhat with the second:

[http://www.aylward.org/notes/open-access-medical-image-repositories](http://www.aylward.org/notes/open-access-medical-image-repositories)

And a fourth, which may also overlap somewhat with the others:

[http://www.via.cornell.edu/databases/](http://www.via.cornell.edu/databases/)",1526431438.0
snes-jkr,Various other challenges besides kaggle. Other than that it will be hard without access to a hospital. Data is precious and expensive to acquire ,1526405735.0
Hotflux,"I've been following news on Synthetic data for a while now. I'm pretty certain it can't replace real-life data, but it might work complementary",1526457080.0
ICOMysticc,"Tech gives me headache, can you guys advise me some channels to improve my knowledge regarding these technical topics??",1526497212.0
eobermuhlner,"I have done it same as you: oversample training set but not validation or test set. Only intuitive reasoning, no source. ",1526925067.0
free_reader,"I was also looking for such tools but the frameworks do not provide a way to extract inner weights. 

I am writing a python module which takes in keras model(.h5 file)  and then makes predictions using numpy.
 https://github.com/VishnuDuttSharma/KeLiPTo/blob/master/python/module.py

You can make changes in the code of RNNs here, by creating list for each gate output at each timestamp and return it as the output. The math is exposed here so you are free to play with other constituents as well.

For your usecase, I think you should first train the keras model, and then save a slice of model(input to RNN) and feed to this code. You can look here how to do it: https://medium.com/@vishnusharma619/keras-functional-models-few-pointers-for-debugging-c58072bbcba9

",1526359734.0
Karyo_Ten,"That is not deep learning.

Look into Topic Modeling, Latent Semantic Analysis, Word Embeddings, Collaborative Filtering (last 2 are supervised) 

Start with TfIdf + TruncatedSVD(100)",1526369906.0
saig22,"I don't understand the input shape, why isn't it (113627, 50, 48), how are you encoding characters?
You could try a time distributed model: at every step you predict the next character, so for ""somethin"" as input u got ""omething"" as an output, it usually achieve better results (don't forget to return sequences on both LSTM layers). This way you don't need to change sequence length to check accuracy at each step.
You're talking about a regularization but I probably missed it, which regularization is it? L2? if not you should try it.
You can also test different dropout rates.
As long as performances regularly improve on test set you can continue the training.
Hope it helps, I'm not an expert btw.",1526306495.0
thtr1310,"I wouldn't say there is a variance problem yet. That would occur if you push past zero slope in the validation loss, looks like you can still go for a few more epochs. The separation between the two curves can be interpreted as evidence of some differences between your training and validation datapoints, as there should be. 

Increasing the regularization strength too much would lead to a higher gap between the training and validation loss curves, but you might see a net lower loss after, maybe 150 epochs? I dunno. 

Try to play around with different embedding layers before inputting the data into the LSTM layer. These can capture relationships between synonyms/similar words in the validation set not present in the vocabulary in the training set, and can lead to lower losses in less epochs. 

I would also play around with batch normalization, but you might need to experiment with where and how to apply it. 

Good luck!",1526313842.0
DeepInEvil,It is always a good idea to save the model with the best validation performance and use it later. ,1526314213.0
dedicated2fitness,you're using indian idioms a lot. it's kinda confusing,1526259900.0
pratikshethcool,"Taking your game of analogies further, first thing is every single thing you do is an optimisation problem. whether you optimise your time against your work or studies vs partying, almost everything. secondly if you are working with CNN it is like making a tea with ingredients in it put without any order like u added milk sugar ginger first and when it came to boil then u added tea-patti. this way of optimising the tea would definitely get u better tea but it might not be the best way. we might need to analyse the process of tea making and its order(CapsNet). same thing is with LSTM. Imagine you are throwing marbles in circle to make a perfect star with those marble. you randomly throw them to make perfect star and try to make changes on individual marbles and adjust ur way of throwing it.. if you want cleaner looking star u need smaller marbles(algorithmic change) or bigger hands(memory requirements). changes in attention on individual marble to make proper changes is called ""attention""(coincidentally). btw i forgot the point of this post and kept on writing what came to my mind so sorry for exploding gradient.",1526279739.0
doctorMLR,"How sparse are the labels? I am guessing they are mostly 0s with a few 1s. In that case, the model can reach high accuracy on sparse labels by always guessing 0s. In the loss function, you should use a weighting on the positive labels on the order of sqrt(num labels) has worked well for me in the past (you will need to tune this).

Tensorflow has such a primitive built in: https://www.tensorflow.org/api_docs/python/tf/nn/weighted_cross_entropy_with_logits",1526230488.0
Karyo_Ten,"Do not look at accuracy when doing multilabel classification, it is wrong.

Use a metric that distinguish between precision and recall (i.e. Take into account false positive, false negative, true positive, true negative) like Matthews correlation coefficient or F1 loss. 

For example on this multilabel satellite image tagging (17 labels) competition on Kaggle, I use F2 Loss, there is a simple Keras version and involved Pytorch version: https://github.com/mratsim/Amazon-Forest-Computer-Vision",1526284753.0
Moni93,"Hello! I had exactly the same issue as you. First, accuracy is missleading when dealing with multi label classification as mentionned in other comments. In other words , having an accuracy near to 1 doesn't mean your model is doing well because when you have many zeros , these labels dominate the accuracy: suppose for example that your output is of shape 20, if 17 of these 20 outputs are Zeros, in the worst case if you are having wrong results in the 3 remaining outputs , your accuracy is going to be 17/20 , which is very near to one , still your model is not good because it gives wrong results for the three usefull outputs.  I advise you to use your own metric , so using tensorflow is more flexible : I myself wanted to do it with Keras by creating my own metric but I got results that don't satisfy me at all because Keras is doing some backend calculations that I can't track, which is not the case if you use immediately Tensorflow because you built everything from scratch and have in fact more visibility within your code , and thus could control everything.  I advise you to use precision and recall for each output independently ( that part really sucks if you try to do it in Keras,  well I couldn't do it ) .   
Second point to specify, make sure that your input data is normalized: because otherwise you may have an overflow issue, and your gradient will be stack , and you'll get constant results all the time ( for example constant accuracy or whatever thing you are calculating).
Hope it helps , good luck.",1526288140.0
E-3_A-0H2_D-0_D-2,"1) Would you please indicate the volume of data with you?


2) Would you be able to show us a 'loss' vs 'epoch' graph for your training and validation dataset?",1526306136.0
ballsandbutts,"I just took a quick glance, but noticed in your vae_loss function you are picking up Z_log_sigma and Z_mean from global scope; not passing them in. That can't be right, can it?",1526088302.0
omg_drd4_bbq,"I got the same results you did on my system.

I noticed the wiseodd blog uses K.sum() in the vae_loss, while francois (on keras blog) uses K.mean. Not sure if that makes a differenc. Tried using mean, but I couldn't get the dimensions right, and I would have to retool everything to get it to work.

I removed the kl term so that it's only using recon loss (turns it to a standard AE). It does better but still really poorly, so maybe it's not the kl term?

Sampling looks alright. 

Curious...",1526128878.0
omg_drd4_bbq,"Yeah I'm at a loss why this isn't working, it all looks up to snuff.

Here's what I would do: rebuild entirely based on Francois Chollet's examples. They are more likely to be up-to-date with the keras api than some blog.

I recommend just spenting time building it line by line. Experiment. Break stuff, and try to fix it. That's a good way to get intuition about these things.

Good luck!",1526129970.0
snes-jkr,This question is unanswerable without knowing what kind of text it is and what the goal of the text production is,1526049322.0
DeepInEvil,"Semantic relation can still be compared, but the quality of writing evaluation is a very challenging task, even for humans. Basic grammar checks can be automated. ",1526050301.0
GaryBishop,"Helping a kid with a disability play a game would IMHO make a great project. Lots of DL projects play games but that isn't the goal here. The goal is to *help* play. For example, you might reduce the number of available moves in checkers to 2, one good and one bad and let the player choose. In Tetris you might only offer 2 choices of translations and rotations with no time limit. Lots of games could be made accessible to kids with cognitive and/or physical impairments this way. ",1525977444.0
Khancity,there it is! The functional paradigm being utilized in another way! ,1525963800.0
ruyin2018,The opinion of this article seems outdated.,1532520722.0
eggie5,"Under the section ""Issues with collaborative filtering systems"" you highlight 2 issues w/ CF. However, they are both describing the same issue: the cold start problem for users and items.

Under the ""Practical examples of collaborative filtering"" section you describe matrix factorization as a Fully connection network, when infact the example you provide is infract not a neural network at all, but rather a matrix factorization problem solved w/ SGD.",1532904302.0
neuvfx,Awesome!,1526479024.0
vector_machines,"There's a great video on this one,
https://youtu.be/M17D2j0QjoQ",1525881999.0
drakesword514,"If you think the loss is still decreasing.. Try increasing the number of epochs or increase the learning rate. How have you initialized your weights? Random or zeros, or some of the specific methods like Xavier? 

Try using batch normalization layer to make the data 0 mean and unit std deviation for each mini batch. Or make the entire data 0 mean  unit std deviation, this way you don have all inputs to be positive, which could restrict freedom of movement for the weight vector. 

",1525850498.0
Rezo-Acken,Theoretically you could have a network so big it learns the training set. How does validation rmse looks like in comparison is mmore important.,1525872203.0
jean-pat,Possibly train a UNet network with Image+labels,1525870435.0
jean-pat,The hard work is to have labels,1525870503.0
waterRocket8236,"Why ""col_axis = 1"" and is the same happening with different
 ""fill_mode"" ? ",1525833778.0
GotRedditFever,"Maybe add an LSTM instead of a fully connected layer at the bottom of your CNN? I haven't tested this myself, but you could create a network and train it on a very small dataset and see what happens.",1525791577.0
cy1994,"You can look at object tracking instead of detection, or something like tubelet proposal networks",1525794072.0
ishang_ucsd,Checkout jifengdai.org,1525798164.0
jecs321,feels like a scam,1525756770.0
snes-jkr,The ads are getting... dumber?,1525762049.0
GotRedditFever,Buy it directly from Udemy that's what I did,1526541405.0
CaffeinatedPengu1n,"Really nice, I do not have time now to dive in it, but (if I understood), you used the tv show's data where it was easy to detect uncertainty and then after teaching the ai, it could apply to more complex videos.",1525738881.0
cy1994,"You could use attention in your network, it does the same thing but is trainable. ",1525757352.0
brokenplasticshards,Perhaps this network just doesn't generalize for the problem. Try and see if different hyperparameter settings do reduce the validation error.,1525708050.0
DillyDino,"Adam works well out of the box as an optimizer for multi-label if you want something to give decent performance quickly. 

As far as the loss function, Sigmoid cross entropy can be good. If sparse, you can toy with weighted labels as well, or with a positive classification threshold during prediction that is something other than 0.5 ",1525711468.0
wild_thunder,[softmax crossentropy](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits) probably,1525696391.0
Important_Fan,Great read!,1525701207.0
ceekaychng,Well done. A good survey. ,1525744710.0
anr1312,"Try posting on Matlab answers: https://in.mathworks.com/matlabcentral/answers/
You should get a quick response
",1525657423.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/neuralnetwork] [What is some software that takes my favourites, files, and\/or email and categorizes them into different area of interests?](https://www.reddit.com/r/NeuralNetwork/comments/8hfh2i/what_is_some_software_that_takes_my_favourites/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1525616623.0
foadsf,Her!,1525669593.0
Nukeyyy,"Why not just get definitions for words, then compare the inputs to the definitions, and the output is the word who's definition is most similar to the input.",1525626859.0
shoaib98libra,Woah. Dope stuff dude. You just earned a subscriber.,1525613082.0
shoaib98libra,Dope stuff dude. You just earned a subscriber.,1525556634.0
ChristianGeek,"4 months old, only contains tutorials from OP’s YouTube channel.",1525536896.0
MikeDoho,Coursera: Deep Learning Specialization,1525404909.0
NetworkForce,"you may take some courses from udemy or coursera for the begginner. the best ever resource for the student 

https://github.com/ChristosChristofidis/awesome-deep-learning#papers
https://medium.com/machine-learning-in-practice/my-curated-list-of-ai-and-machine-learning-resources-from-around-the-web-9a97823b8524",1525443187.0
carryonbag78,www.fast.ai,1525400456.0
Karyo_Ten,I suggest you head over /r/mlquestions and /r/learnmachinelearning. Plenty asked what you're probably looking for. ,1525412147.0
Smeikchen,I found this one quite nice ( free MIT book with Ian Goodfellow as one of the authors) : http://www.deeplearningbook.org,1525479825.0
vector_machines,https://YouTube.com/c/aijournal,1525593910.0
sksiitb,"Whats ""totally"" new?

Do you know how to code?",1525400339.0
ChmHsm,"You can:  
1- fine-tune a pretrained model.  
2- look for people who already did this or at least smthn similar to it, hint: kaggle.com.",1525363314.0
abhishekyana,"I don’t know completely but my guess is.

1) Increase the rate of capturing the image. With this you can remove the smoky blobs from the images that are separated with little time. We can overlay the images and remove the blob areas. Making a single picture out of series of pictures with small delay.

2) Collect the data for long amount of time. Because 1  in a 6 is close to the desired image then increase the duration to 6 times to collect the required amount of data for analysis. 

Quality of the data is the key. So, I guess these will help. 
Hope so😅",1525348523.0
chankeam,"Hey, I think this is a really interesting thought! I actually beatbox myself and am studying into deep learning. My question is this I guess: where does deep learning come in with beatboxing? Do you mean like deep learning taking the beatboxing audio and completing a full song by its own? Or maybe taking the beatboxing track and automatically making it sound a lot more like real instruments?",1525339173.0
dewayneroyj,"Definitely. The only problem is finding a dataset that contains beatboxing audio. Once you find the data, build the model. The model will eventually learn to make its own Beatboxing sound. For something like this, I assume one would use an RNN/LSTM.",1525354154.0
Karyo_Ten,"The article only talks about the speed aspect while the main difficulty currently is deploying and maintaining a Python pipeline in production: relevant XKCD from 2 days ago - https://www.xkcd.com/1987/.

Pytorch and Caffe2 being maintained by a merged team is the most exciting news for me! 
",1525327818.0
maykulkarni,"If you're looking for deep learning + NLP Stanford's CS224D would be perfect for you. The lectures and assignments are available online.
http://cs224d.stanford.edu/",1525285222.0
casconed,"The final course in the Coursera Deep Learning track, [Sequence Models](https://www.coursera.org/learn/nlp-sequence-models), was approachable and pretty high quality",1525286728.0
grumbelbart2,"Those two might get you started.

http://papers.nips.cc/paper/5647-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations.pdf

https://arxiv.org/pdf/1603.05279.pdf",1525258945.0
iacolippo,"Github repos of the work of Courbariaux and Bengio cited by grumbelbart2:

(Theano)
https://github.com/MatthieuCourbariaux/BinaryNet
https://github.com/MatthieuCourbariaux/BinaryConnect

(Torch)
https://github.com/itayhubara/BinaryNet

It's pretty easy to do it in PyTorch too. You just have to be careful with how you want to perform the backward pass (quantization is not differentiable).",1525270253.0
rosemont,"Google 'German Traffic Sign', which is a popular data set for deep learning.",1525221623.0
NetworkForce,"""Traffic sign recognition with hinge loss trained convolutional neural networks"" IEEE 2014
you may find it on google scholar :) ",1525443981.0
nattyblack,Buy it,1525124858.0
berkankadioglu,I think latter is a good idea. Maybe the keyword you are looking for is word embedding. Good luck.,1525139572.0
Karyo_Ten,"Do you really need a single model?

You can build 4 models with a dynamic framework and weights shared for the core part. Input and outputs can have different layers/embedding to deal with the varying shapes.",1525082646.0
tmclouisluk,Maybe you can try rnn,1525091367.0
obsoletelearner,https://replika.ai/,1525148404.0
pseudo_brilliant,"Here's a bunch:

[https://github.com/floodsung/Deep\-Learning\-Papers\-Reading\-Roadmap](https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap)

And a helpful collection of resources:

[https://medium.com/machine\-learning\-in\-practice/my\-curated\-list\-of\-ai\-and\-machine\-learning\-resources\-from\-around\-the\-web\-9a97823b8524](https://medium.com/machine-learning-in-practice/my-curated-list-of-ai-and-machine-learning-resources-from-around-the-web-9a97823b8524)

And here's a course I just finished where we read 2 papers a week and presented lectures on the topics:

[http://crcv.ucf.edu/courses/CAP6412/Spring2018/Schedule.htm](http://crcv.ucf.edu/courses/CAP6412/Spring2018/Schedule.htm)",1525052133.0
iLoveBigData,Check out http://www.deeplearningbook.org/ . Use the bibliography as a starting point on papers to read. ,1525074737.0
gurudevX2,"Here, a curated and comprehensive list for not only papers but other stuff related to deep learning:
https://github.com/ChristosChristofidis/awesome-deep-learning#papers
Hope this helps :)
",1525068616.0
NetworkForce,"Here is two book i have read it first for the beginner
https://drive.google.com/open?id=1ZZkCtFc5Jaul6bp88NIQJuuQYgGrC0WQ",1525446052.0
mynameiscorey,Thank you kindly for sharing this! I was about to try rolling my own templated stacks with cloud formation but this will save me a bunch of time and be a better solution in general :) ,1525022011.0
brownmamba94,"Hi, I wanted to share this repo I created for helping people get started with requesting and provisioning Amazon EC2 Spot instances, specifically GPU instances for Deep Learning workloads on the cloud.

The code can be found here: https://github.com/vithursant/terraform\-aws\-spotgpu

I use Terraform, which is my go\-to DevOps tool for infrastructure as code \(IaC\). The main advantage of this module is automation as it provides an easy way for automatically submitting requests for instances and creating instances on AWS, without having to worry about clicking through the AWS Management Console and setting up security groups, VPCs, routing tables, subnets, installing libraries, etc. The module also provides some custom configurability to set the following variables:

\* aws region

\* instance type

\* number of instances

\* spot price

\* ebs volume size

\* ami id

By default, the module uses the latest AWS Deep Learning AMI for software configuration, which already has NVIDIA drivers, TensorFlow, PyTorch, Keras, MXNet, Caffe, Caffe2, etc, installed. There are instructions for getting started with sample output images on the GitHub README file.",1525011020.0
yazriel0,"Try this recent discussion

https://www.reddit.com/r/machinelearning/comments/8exedi/_",1525049836.0
new__vision,Here is a project where LSTMs were used to predict object trajectory: http://guanghan.info/projects/ROLO/ ,1525031784.0
Raomystogan,Why don't you ask this on machine learning sub? ,1525021925.0
400_Bad_Request,"Good question, do let me know if you find the answer ",1524936367.0
qwasz123,"If you don't ship the model with training data then the models licence belongs to you.


As for scraping data from websites, that's a gray area in terms of legality.",1524937147.0
1800not4you,"I work in the media and entertainment industry, specifically with these types of use cases i.e. metadata augmentation via deep learning. Unless you have explicit rights, it is not. I could use DMCA or any number of infringement routes to shut you down. This area is still new, once it gets traction and competition + monetary value heats up, you can expect to see more takedowns.",1524940225.0
Rezo-Acken,"People really need to stop looking at time series prediction from a stock perspective (or bitcoin price). If you could make something good that can be described in a medium blog post it would be known by now (and the market would have adjusted anyway). Not that it's useless for the field but it's really a terrible example for beginners.

Now. Whether or not they are better than previous methods (on a good problem like say daily tourism at the Eiffel Tower) I doubt it. RNN are not made for univariate economic time series. Stop using it for that. Instead use bayesian methods for time series or ARIMAs or more fancy stuff I don't know about. RNN are good to deal with sequences of things that usually neural network deal well with. 

Let me explain. Something like daily tourism is just a single value over a low frequency of time with limited interaction. Trying to use a Neural Network on that is similar to trying to solve a simple linear regression with a deep fully connected network. It's just dumb and you'll spend your time fighting overfitting. Now if you have a sequence of images, a sequence of words embeddings, or a large vector of features then it's a good case of use. ",1524953470.0
snes-jkr,"I think the problem is rather the unpredictability of the data, as x(t) is not just dependent on x(t-1) but a lot of hidden variables that are not part of the data set, especially for the bitcoin set.",1524925527.0
nnexx_,"It really depends how long the time correlation is. For text data or monthly data yeah sure. For multivariate time series at a few hundred Herz or more they are totally useless. In this case you should use long reach 1D convolutions with non zero stride. It’s computationally more effective (a single lstm cell is 3 gates) and can capture long range interactions as well as learn some basic signal processing like moving averages and derivatives.
",1524947564.0
Karyo_Ten,"I tried to predict traffic for 3 months with a 1 hour resolution in a challenge with RNNs here with a multi-attention neural network: https://github.com/mratsim/McKinsey-SmartCities-Traffic-Prediction#results. Long story short I failed.

In teacher forcing mode (predicting 48 hours) it works quite well, but 3 months at a one hour resolution (11k) is too much.

What I found interesting is how RNNs can capture quick variations which ARIMA doesn't capture, so probably an ensemble with ARIMA with quick variations corrected by RNNs can produce something interesting.",1525082866.0
helmetti,Anaconda itself has Cudatoolkit 9.0.,1524921998.0
nafestw,Doesn’t it work with current versions which are CUDA 9.1 and cuDNN 7.1?,1525001316.0
RetardedChimpanzee,"I would say yes, only if you have thunderbolt 3. Deeplearing really requires full bandwidth of your PCI lanes. ",1524880882.0
mdfeist,"Probably possible, but it might be a pain to get Cuda to run properly with an external NVidia GPU. A quick search looks like people may have done it. It's just something to look into.",1524885585.0
modminman,I did it and it didn’t work well at all.  It would crash my system multiple times a day.  ,1525115817.0
1800not4you,"price comparison on this is wrong. disposable instances arent a match for ris, they should have used ec2 spot.",1524890036.0
thewisegeneral,"This is not true actually. Getting deep CNNs like Resnet , InceptionNet work took many things like skip connections , batch normalisation and other things that you can read about . This is because deeper networks are much harder to optimize and it was observed that simple stacking conv layers infact worsened convergence. Fully connected layers have much more parameters than CNN due to each node being connected to each node and hence are even harder to optimize without batch norm and other techniques. ",1524811322.0
ajgamer2012,Curse of dimensionality ,1524836055.0
devanishith,"Could it be that, for bigger problems (imagenet), no body has enough resources nor need to built that deep fc networks?
Fc networks are really huge(in size). VGG has only few fc layers as opposed to many conv layers. But huuuge fraction of parameters are still only in the fc layers.",1524839085.0
m_ninepoints,"Simple. Because without it the predictive function would just be a linear function of the inputs (and most things we try to predict using NNs are highly non-linear). I don't think this or the linked post justifies an ""activation function"" per se. We could have done any number of things to produce some non-linear output. However, activation functions are easier to reason about, and in conjunction with the chain rule, allows us to implement backpropagation efficiently.",1524803322.0
xEtherealx,"Vs. a simple threshold, activation functions impart more information onto the next layer by indicating a 'distance' from the boundary condition.",1524849409.0
snes-jkr,"At current GPU and RAM prices you might even make a win (jk)

I think 250 sounds quite reasonable for such a setup, even in 6 months. And for that money you might just keep it and use it later again.",1524782295.0
Stochastic_Response,"why not just use a gpu instance? i think google has a promo

https://springcoupon.com/google-cloud-coupon-800-usd-free-credit/",1524803316.0
frapa32,"Yes you can. People are crazy about hardware, but you do not need 3k$ for a deep learning box. You can build a good setup like motherboard (~50) + pentium (~50) + 8gb ram (~80) + gtx 1050 to (~200) + case, power, disk (~100 unless you use some you already have). If you have some extra money you can go for a 1060 at about 300-350. 

This setup is more than sufficient for basic tasks and a bit of fun. I worked at the university with a 1050 to (which nobody seem to ever consider when doing deep learning) and it works quite well.",1524900915.0
Echsu,"This is great. I don't know anything about head injuries, but I'm tempted to try out some 3D convolutional network approach on the images.",1524770324.0
bubushkinator,"This is what CNN does, it has a small, sliding window to detect small features in the big picture

Please let me know if I am just not understanding your question.",1524750941.0
Rezo-Acken,Thanks exactly what I was looking for lately,1524745858.0
GotRedditFever,"Interesting project you have there, hope someone helps you. Am interested too. There are videos on YouTube discussing using deep learning for gesture recognition. But am not sure whether they use CNN-LSTM. I presume they will have to.",1524695429.0
artificial_intel423,"Sparse matrix works. Not sure where the confusion is. Your input dim is the number of ngrams. Pro tip btw, convnets are inherently good at ngram detection without explicit creation",1524710552.0
ragas_,You can look at r2rt's rnn blog post. He showed working of lstm with basic tensorflow code and explained it. Then he gets into tf lstm module. ,1524680404.0
Pierre_Yves,"My favorite series on Neural Network (4 videos), are the ones from 3Blue1Brown:

https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi",1524669366.0
brokenplasticshards,Do the MNIST tutorial on their website.,1524636373.0
dewayneroyj,This isn’t enough data. ,1524634221.0
persistence_hunter,"Hi   
So palm reading itself is a discredited science, atleast in popular opinion. Why would someone use your product given that no one believes in the premise to begin with?  
EDIT: I am not trying to attack you guys.",1524700078.0
ArksSystem,anyway to get the code? :),1524558623.0
Says_Watt,Heh guys this is super cool to me and I'd love to learn how to make my own. I'm completely new to coding so any tips on where to start (I don't expect this to be easy) ,1524590064.0
nrkrkdjbd,Wow. It’s definitely better than me in understanding basketball ,1524575700.0
ilayaraja97,"This is very cool. I have some questions though. What classifier did you use? Any research papers you referred? Can I get tips on an emotion classifier. So far I implemented it using neural nets using keras but can't figure out what layers to use inside it. I'm an undergrad student, for a project. ",1524771688.0
tryredtdy,So cool!,1524627233.0
grappling_hook,Do you mean like handwritten responses or what?,1524477822.0
errminator,interested in ideas of either handwritten responses or text based input. Any ideas?,1524491334.0
pseudo_brilliant,"I'm actually doing so right now for a school project. I'm attempting to use the STARGAN implementation to allow for style transfer of various famous artists. I'm using a 10 class subset of the Imagenet dataset with about 10,000 images and a 6 class subset of wikiarts with about 6,000 images. I'm also running a separate experiment with CIFAR10 instead of imagenet. I'm on iteration 2500 of 20000 so it's still got some work to do.",1524411290.0
deeplearner93,How long would it approximately take for your project to get done?,1524341802.0
tryredtdy,Super cool. Congratulations ,1524380273.0
Jazoom,"So this will be classification, not object detection?

  


I'm just guessing based on the Kaggle challenge you picked.",1524395809.0
Wrosinski,"Especially when it is meant that 10% is needed for an expert, better start aiming for ranked competitions instead for non-tiered, meant for learning and practice, when you advertise methods by kaggle ranking.",1524379373.0
DeepInEvil,Maybe use something like tfidf over logistic regression? Might work better,1524301663.0
hosford42,"Lots of graphs, but no description of the algorithm?",1524278957.0
ryandiy,where are the details?,1524429523.0
theoneandonlypatriot,Am I wrong or is there literally no explanation for how this works? ,1524205610.0
zapekanka,"Are you sure that the version of python you're running matches the pip command you're using?

Try to figure out what's what with e.g.:
`which python`
`which python3`
`which pip`
`which pip3`",1524171718.0
Barrerayy,Mine is working fine on Ubuntu but I'm using anaconda to manage my environments. This sounds like a python version issue rather than a tenserflow one.,1524347162.0
,[deleted],1524213298.0
posthumour,How would this be different/better than [imagenet](http://image-net.org/)?,1524147735.0
sksiitb,"Also lets give it a name and vision. 

So that its concrete. Like lets say ""million images project. Having a million images for every class"" perhaps. ",1524187841.0
jordimaister,"I would like to participate in that project. I've been thinking about it too!! 

Store the images with tags and tag coordinates. And every user could upload and/or tag. 

Also thinking that the storage and data transfer would be the most expensive part of the project. ",1524157222.0
TheDuke57,"I would recommend looking into how iNaturalist is it up. Users take images of plants or animals, can guess at a label if they want, thren 2 experts have to verify it is correct. The system works pretty well. www.inaturalist.org",1524159186.0
sksiitb,"I've thought about this and I think this is really important if we need unbiased algorithms. 

If we leave it to the free market, it'll always be prioritised to cater the early consumers and ignore the reat.

Look at all the biased algorithms. Some cannot identify certain races. 

And a bias free open source database sounds like the key. ",1524187689.0
nunz,I'm totally on board with this. Let's do it!,1524192070.0
Echsu,"Yeah, let's! Taking the photographs might not even be necessary since there are already photo databases ([Pixabay](https://pixabay.com/) for example) with public domain photos.

I would think that the best way to go about it would be to create a web interface that shows people photos and asks them to label/annotate them. This way the barrier to start contributing would be extremely low.",1524141286.0
rambossa1,Facebook login or a Facebook phone kit login ... lol bad timing,1524140657.0
sedlich,Why do you think it is better then Alpha Zero? I do not see yet anythin.,1524133853.0
hewhosaysni,"I'm able to install it like this:

1. Install nvidia drivers:

sudo apt-get install nvidia-367

2. Install required libraries for CUDA:

sudo apt-get update

sudo apt-get install -y build-essential git python-pip libfreetype6-dev libxft-dev libncurses-dev libopenblas-dev  gfortran python3-matplotlib libblas-dev liblapack-dev libatlas-base-dev python3-dev python3-pydot linux-headers-generic linux-image-extra-virtual unzip python3-numpy swig python3-pandas python-sklearn unzip python3-pip python3-venv

3. Install CUDA

wget https://developer.nvidia.com/compute/cuda/8.0/prod/local_installers/cuda-repo-ubuntu1604-8-0-local_8.0.44-1_amd64-deb

sudo dpkg -i cuda-repo-ubuntu1604-8-0-local_8.0.44-1_amd64-deb && rm cuda-repo-ubuntu1604-8-0-local_8.0.44-1_amd64-deb

sudo apt-get update

sudo apt-get install cuda

4. Reboot and disable secure boot",1524114102.0
nunz,"Try using the commands out of this Dockerfile.


https://gitlab.com/nvidia/cuda/blob/ubuntu16.04/9.1/base/Dockerfile",1525813465.0
thisismyfavoritename,"Odd size kernels have a center pixel, which allows to map the result of the convolution to the subsequent layer (as a pixel).

It also makes sense to consider a symmetric neighborhood around a pixel to compute a convolution on spatially equivalently correlated pixels.",1524112720.0
,"Interesting question. I think if you'd apply bilinear filtering it could be possible, but if you do that you just end up with more work.",1524155448.0
tryredtdy,Am looking forward to try it out.,1524117712.0
Bewaretheicespiders,"Thats not actually multi label classification, which would be picking (0-K) out of N labels for each image. As you describe its 3 different categorical outputs.

Now for your model: 

1-Your dataset is too small for the number of weights your model has. You should probably build on top of layers pre-trained on imagenet, like replacing the last layer of vgg.

2- The number of weights in the dense layers crushes the rest of  the model, opening the door wide for overfitting.

3- Since your problem, as you describe it, is not actually multilabel, you should not use binary crossentropy loss. You should have 3 different outputs and 3 categorical cross-entropy losses.",1524066737.0
thewisegeneral,I don't think this means it is broken. The same thing is true for PyTorch. Maybe it was built that way....,1524026249.0
gunfupanda,"For more traditional ML algorithms, I'd recommend using scikit-learn or something similar. Tensorflow and pytorch are deep learning frameworks, and if you don't want to do deep learning, you need a different tool. Sklearn does things like KNN, simple regressions, etc fairly easily. ",1523980532.0
Razmyar,"TensorFlow would be a good choice if your algorithm could benefit from Tesnsors. Debugging is not that hard, just not straightforward. You may wanna check TensorFlow Debbuger (tfdbg). Regardless, u may find the combination of Python and Numpy easier for implementing general ML algorithms. Also, understanding the algorithm's mathematical model will help u a lot. ",1523973908.0
inkognit,use pytorch,1523977519.0
theslt,"Well in fact you're just decreasing the dimensionality of the data. A dimension that only has two values. so there might not be any practical difference between these approaches.
I wonder the remaining dimensions will rull the efficiency of the model(s). So by training two separate models you're just increasing the complexity off the whole system. I suppose!",1523940710.0
theslt,"One can call an output a ""Sparse"" vector but in fact, this concept is used to describe the input vector and parameters of a model. If the output vector goes ""Sparse"" then it's a good thing. in fact, this is the thing that we are searching for. to get the parameters in a way that makes very narrow spikes in certain places of output, if you look at it as a PDF.",1523879152.0
waterRocket8236,"I too have a doubt regarding multilabel classification. Here 's the scene:- 

Lets say there is one test sample X and two classes A and B. Now X is predicted and have probabilities for all two labels A and B. Let the probabilites be 0.7 for A and 0.2 for B. In actual labels, X is a B type during training. What should a researcher/engineer do about X in this case? Is it okay to think that it is of A type or not ? 

More knowledge/thoughts about this is highly welcome.",1523944381.0
GodofExito,"So simply speaking the CNN's construct relatively simple higher level features (so detecting 'sh' instead of s and h). They remain simple because of the size of the CNN Kernel. These features can then be used by the RNN to construct (semi-)variable length features.

One possible outcome would be transforming chars into phonetics (CNN layer) and then producing sound from phonetics in the (RNN layer).

This of course has nothing to do with sentiment analysis but i hope it helps you.",1523836457.0
schrodingershit,"That's the 2 mins paper, paper right? ",1523812601.0
grumbelbart2,"In the bigger picture, this is transfer learning, where some classifier that was pre-trained on data A is then used for data B.

You'd usually use the largest part of the pre-trained network as initialization, replacing only the last layer(s) that are connected to the output. You then run some training cycles of this, using your own images.

Intuitively, this works because the first layers of a network to some extend learn low-level, generic features. You'll often see, for example, that the first layer is some sort of line detector, while later layers react on more complex shapes. Since such shapes are somewhat generic and found in most images, it makes sense to transfer them between different domains.",1523793615.0
kookaburro,"Take a look at Prophet from Facebook
https://towardsdatascience.com/time-series-analysis-in-python-an-introduction-70d5a5b1d52a",1523700702.0
tryredtdy,"I would think so. Depending on what timeframe you want to predict, you will have to have the training data shaped. I had looked at NYC cab data but was using clustering to find out busy spots.",1523728072.0
BruinBoy815,"Prophet package is pretty good.
I would also look at ARIMA and holtz winters modeling ",1523732143.0
tryredtdy,"Shaping, I meant ..changing the dataset bin windows based one how you want to predict.",1523754434.0
country_dev,This is awesome and exactly what I have been experimenting with lately. We need more blog posts like this!  There's a lot of intro to DL resources out there but very few that discuss actually deploying the models to a production setting. ,1523637048.0
Felflare,Thanks for sharing!,1523634526.0
ahme0307,Confusing title. It is mainly how similar are you to the training set.,1523625496.0
thewisegeneral,Would be interesting to see some adversarial attacks on this network . Haha. ,1523645586.0
naptownhayday,I cant help but think this data set might have issues. It would be better if they found posed them all a certain way for example the lower ones (at least pictured maybe not the whole set) aren't smiling which makes many people seem less attractive. Even the author of the article managed the vary his score fairly significantly with some lighting and angle changes. Interesting idea but needs more work. ,1523654861.0
fishy_commishy,I’m literally the guy in the photo,1523665040.0
gagejustins,Understanding autograd is on my bucket list,1523544593.0
HaohanWang,"Hmm, I have an example that might be useful for you. 
There was an undergraduate who contacted me with barely any knowledge in the field other than finishing Andrew Ng's machine learning course, and he spent around four months worth of spare time (not sure how is that compared to your one-month full-time commitment here) and did this project with me:
https://arxiv.org/abs/1803.07276 
I believe the techniques we use in this paper can also be applied to any other related healthcare questions, one-month is probably a good time for you to try if you are already familiar with neural nets.  

BTW, there are public cancer vision datasets, try: 
http://www.cancerimagingarchive.net/",1523503427.0
GauBhakshak,toxicity of twitter user from it's tweets. One month might be tight but this idea has struck with me for a while.,1523526287.0
Mrshadow143,"hmm...., I have done similar work I have done a blind guidance system using OCR (optical character recognition ), face recognition etc and the output will be in bot sound so that he can here. I have used google API for its fast response. You can use Tesseract for offline version but efficiency using tesseract is very low

https://github.com/chekoduadarsh/Fast-Blind-Guidance-System-using-Deep-Learning

In the below link there is code for tesseract also

https://github.com/chekoduadarsh/Image-to-Text-converter-OCR",1523533678.0
DelosBoard2052,Ooh... ,1523499494.0
trialofmiles,"There is a single convolutional layer that is applying a linear filter to an input feature map and yielding an output feature map. Depending on the padding scheme, the output map could have the same Number of Rows and Columns or it could be different.",1523492461.0
ukrdailo,Was involved in smth similar some time ago: https://github.com/BAILOOL/Assistant-for-People-with-Low-Vision,1523520725.0
Mrshadow143,"Diagram Tells how basic convolution operation is done
https://en.wikipedia.org/wiki/Kernel_(image_processing)",1523458800.0
Kottman,"I hate the acronym big data, here in Germany it gets used way to often from companies that doesn't even care about deep learning.",1523521773.0
1800not4you,Can we stop using this term please? Unless you are hitting petabyte scale your big data is nothing more than.. little data.,1523839194.0
kalpatris,"I wonder about CPU decoding. I never cared about codecs, because every movie was played at proper FPS on my 2013 MacBook Air. But recently I downloaded 4k H.265-encoded movie and it turns out that MacBook just wasn't able to decode it at proper speed. I hope that AV1 will be more efficient.
",1523458834.0
Jazoom,"I tried installing YOLO today. It worked but when compiling for CUDA it couldn't find it. Apparently you need to put the CUDA path in the config file but I have no idea what path to use. I found one cudart file in the Colaboratory file system but its directory doesn't work.

  


Apparently using Colaboratory isn't as easy as one would hope.

  


To answer your actual question, when using TensorFlow I certainly had trouble with memory limitations.",1523359137.0
Rezo-Acken,"In his DL specialization he talks about relu and it is used in the assignements. Its also used in all other courses that uses DL that Ive been a part of so far.

Its true that the introduction to NN always start with sigmoids but it makes sense from a historical point of view and to explain the concept of non linear activation.

However tanh and sigmoids are still widely used. Especially in NLP and to join parts of complicated models because it can act as a gate. LSTMs are composed of sig activations for example. Sigmoids are also a common output function. What is not efficient with these two is when they are stacked but it doesn't mean that having an on/off switch function is not still useful.",1523362703.0
potatomind,"Can you please make a Pytorch-like name for your framework? Like Inferno, or Hell, or Firestorm. As a proud pytorch user I can use only frameworks with proper names.",1523314999.0
Deepblue129,"Sorry for the deleting the first post, it was a bit vague!",1523283117.0
smart_dumb_smart,"Dense layers are ok but not good enough to do this kind of work. I would recomend conv2d layers (or the conv2d transpose).

This is a good read: https://towardsdatascience.com/up-sampling-with-transposed-convolution-9ae4f2df52d0",1523281308.0
tweettranscriberbot,"^The linked tweet was tweeted by [@NaveenGRao](https://twitter.com/NaveenGRao) on Apr 07, 2018 22:22:11 UTC (2 Retweets | 4 Favorites)

-------------------------------------------------

[@witeken ](https://twitter.com/witeken )Yes, we will be sharing some benchmarks there! But, it is important to realize that some of these problems are memory bandwidth bound or contain serial operations so existing compute architectures can work well

-------------------------------------------------

^^• Beep boop I'm a bot • Find out more about me at /r/tweettranscriberbot/ •",1523140196.0
inderjalli,This author deleted the post. Any idea why?,1523466815.0
ajmooch,"You can take the dot-product of the model's 50-dim output with the word embedding matrix and then use the softmax over that to determine the word your model applies the highest probability to. This is commonly known as tying the weights of your model's encoder and decoder, see [here](https://github.com/pytorch/examples/blob/master/word_language_model/model.py) for an example",1523090100.0
thisismyfavoritename,"What is the correlation between the ID and the label? From what I understand, there is none, unless there is something specific about the ID that relates to say being in the positive class? You should look for other features for each of your IDs...",1523020365.0
hinduismtw,"For the given ID number is the label a constant ? Do you always get the same label for the same ID numbers ?

What is the correlation between the Sequence number and label ?

Your post doesn't give enough information about the problem to suggest anything.",1523100506.0
to_be_deleted_soon,"He's certainly a polarizing individual but his experience and success cannot be overstated, so this might be valuable.",1523012643.0
haffi112,How can we confirm this is him and not some guy collecting emails?,1523058831.0
ScotchMonk,"I believe he will expand on what he taught in the ""Structured Machine Learning Projects"" in Coursera. 
In that course, he share mostly about his work experience in real projects, how to solve ML project problems and planning the next action (i.e. change model, change hyperparameters, need more data, high bias or variance?). The wrong step or analysis will waste weeks or months of effort, gaining no performance/accuracy in your model. This lesson alone is worth a lot more than learning algorithms and coding..which you can pickup in any other courses.",1523276689.0
LordFenix56,Omg it's amazing. You can only notice is edited because of that error in the road.,1522985736.0
chouhansolo,How did you do that?,1522979490.0
ItachiUchiha8045,"lol i know this standup comedian(random chikibum)
Grt work!!",1523003056.0
neuvfx,"It's cool, did it require optical flow or anything fancy?",1523405555.0
rob_frobisher,You could consider an X299 or Threadripper build to increase the number of PCIe lanes you have available - would also give you the option of getting more cores. ,1522941485.0
rob_frobisher,I would also consider an EVGA G2 or G3 unit for the PSU. ,1522941670.0
dewayneroyj,Start with the GPU and build around that.,1522936266.0
onmywaytostealyagirl,thanks for sharing! good aggregation ,1522991321.0
rylaco,"Well, world models paper is interesting.",1528401830.0
thegymnosophist,There are 8 ?,1522930911.0
nnexx_,"I might add mocha to the list, a Julialang based implementation of Coffee. The power of coffee in a easy to use language without the need for config files. It may still lack some of the advanced features quiet yet but it is growing fast",1522935790.0
porygon93,"Not a course, but I suggest you to take a look at this book.
https://www.amazon.com/Deep-Learning-Practitioners-Josh-Patterson/dp/1491914254",1522915353.0
imewx,Java community has a great library (also with GUI tools) called WEKA for deep learning. You might want to have a try and read official documents.,1522915669.0
Echsu,TensorFlow has a Java API. You could probably take any course that uses tensorflow and write the code in Java instead. It's also a good exercise for you to rewrite the already given code in Java.,1522920169.0
ilielezi,"It should be easier to switch from Java to Python, than having to rewrite a ton of code in Java that you can get in the internet for Python. It isn't only that most libraries don't support Java (and those who do like TF, it is clearly much less support than for Python), but pretty much every implementation of models is on Python.",1523028294.0
rosandonary,"i suggest you to use Python than JAVA


as other do，this rule can reduce a lot problem when you just a newbegin.",1522913435.0
HoldItCaulfield,"I'm sorry I think that's against the terms of service. If I can give you a suggestion (I'm assuming you don't want to/cannot pay), you can apply for financial aid. It's pretty straight forward, all you have to do is apply and wait around 2 weeks",1522897123.0
dewayneroyj,I’m auditing the course also. You can still “view” the content on Jupyter Notebook.,1522905824.0
nickbuch,"What do you mean, functionally?  What are your input units, if not series?",1522895717.0
varunagrawal,"You scale the bounding box by the product of the scale factors in each of the layers of the network. I believe that after conv and maxpool, the total scale factor is 8, so you scale the bounding box by 1/8th.

Source: SPPNet paper appendix. This is the paper Fast RCNN draws inspiration from most. ",1522910520.0
GotRedditFever,Any good tutorials on how to implement it with Keras?,1522883121.0
GotRedditFever,RemindMe! 2 days,1522883132.0
Schtecke,"""Close to some vector"" means close according to whatever loss function you define. In this paper, they discuss quadratic cost functions. This means that the distance between two vectors x and y is defined as |x-y|^2 = (x_1-y_1)^2 + (x_2-y_2)^2. Another common choice is the cross entropy: https://en.wikipedia.org/wiki/Cross_entropy .

The output of a classifier like the one discussed in the bit you quoted is essentially a probability assigned to each of the classes. If F_1 > F_2, it means that it's more likely (according to the network) that the object belongs to class 1. For labeled training samples we always have F = [1,0] or [0,1] since we know with certainty which class the object belongs to.
",1522859850.0
ford_beeblebrox,"*  [**Deep Learning: An Introduction for Applied Mathematicians** 
by Catherine F. Higham, Desmond J. Higham 
](https://arxiv.org/abs/1801.05894)

*17 Jan 2018*

Multilayered artificial neural networks are becoming a pervasive tool in a host of application fields. At the heart of this deep learning revolution are familiar concepts from applied and computational mathematics; notably, in calculus, approximation theory, optimization and linear algebra. This article provides a very brief introduction to the basic ideas that underlie deep learning from an applied mathematics perspective. Our target audience includes postgraduate and final year undergraduate students in mathematics who are keen to learn about the area. The article may also be useful for instructors in mathematics who wish to enliven their classes with references to the application of deep learning techniques. We focus on three fundamental questions: what is a deep neural network? how is a network trained? what is the stochastic gradient method? We illustrate the ideas with a short MATLAB code that sets up and trains a network. We also show the use of state-of-the art software on a large scale image classification problem. We finish with references to the current literature. ",1522908914.0
imguralbumbot,"^(Hi, I'm a bot for linking direct images of albums with only 1 image)

**https://i.imgur.com/weDBdld.jpg**

^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&subject=ignoreme&message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&subject=delet%20this&message=delet%20this%20dwsp2vq) ",1522855050.0
Artgor,"One important this is that DL usually works when data has some internal structure - like pictures or images.

If you have a lot of different categorical and continuous features and need to make churn prediction or credit scoring, then xgboost/lightgbm could work better.",1522862229.0
Freyj,"I am not sure I am the right person to judge whether it would be viable in terms of jobs, but if you want to learn there are plenty of resources available for free online to learn and expand your knowledge. 

In my opinion it would be interesting to learn python because many frameworks in deep learning use it (but c++ is also a  good option, I am only speaking from my small point of view).

You can maybe read the ""Deep learning"" book by Ian Goodfellow, Yoshua Bengio and Aaron Courville (or keep it as a reference during your work on deep learning).

Also if you need a little more background maths for the deep learning aspect, I have found that the 3blue1brown youtube channel had a really interesting video series that helped me a lot when I started out. (here : https://www.youtube.com/watch?v=aircAruvnKk)

And depending on what you want to do with it, and which framework you use, the communities are usually pretty friendly and helpful. 

I personally use pytorch but have found that the tensorflow playground helped have a sort of vision of what goes on in a network. http://playground.tensorflow.org/ 

I hope I was some kind of help, have a nice day ! 
",1522831974.0
thundergolfer,"Given the sub you're posting in I'm going to assume you'd like a job in software DL.

In short, it will not at all be easy and you don't really have any formal training as that minor from many years ago doesn't really count. 

Getting even good general software job isn't easy. These jobs require a lot of technical competencies that can't be picked up in a matter of months. DL is highly mathematical and extremely hyped in industry, so the bar is generally PhD level. 

That said, it's totally viable if you're willing to dedicate years to training and educating yourself, just as a college student would do. There's interesting NLP problems in the journalism space too, so you could bring in that experience. ",1522833914.0
dewayneroyj,"It’s never too late to learn. If possible, try to mesh journalism and AI. ",1522840366.0
LikeMike81,"Thanks for the reply - my university education isn't that far back -I got my degree in 2015, was already working full time for several years then. But like you pointed out it was more of a get your toes wet than an in depth so course.

Would that new Microsoft edx learning course (Microsoft professional program for artificial intelligence) be a good start? Do those courses ""count"" in the eyes of companies or are they more something I do for me to get a better understanding of the whole subject? ",1522834979.0
realfake2018,This is nice. Do you crawl over the world to create the list or is it localised to few countries?,1522828032.0
gunfupanda,"Learning rate decay might help it converge, if you're not using it yet. It sounds like you might be overshooting the minimum and then thrashing. ",1522819780.0
happy_pirate,If you are not using it.flip it like a tent and keep the fan beside it..better cooling is acheived since heat is removed from both the surfaces,1522771250.0
rosandonary,maybe you need GPU,1522838195.0
PM_ME_YOUR_CALL_LOGS,"Haha. On a serious note, does it help?",1522767822.0
snes-jkr,The same weights are applied. Also else your memory would explode.,1522746539.0
akanimax,"How many classes do you have? If the network is predicting one single class for all the examples typically points to the fact that there might be many examples of that class and for the network, it is locally optimal to predict everything to belong to that class so that the loss is temporarily minimised.",1522733100.0
vlatheimpaler,What did you end up doing on this? I'm interested in doing something similar.,1531518653.0
spracked,"Basic steps would be for me:
1. Write modular reference implementation on host/classic architecture
2. Choose an appropriate core part, port it to FPGA and cross check results with reference implementation
3. Repeat 2 until desired degree of parallization or other goals are met",1522631521.0
sksiitb,"I've been working in the exact same idea. Nope. There are none. I have had to label features manually. 

Do get in touch with me though. We might be able to help each other out. ",1522488226.0
utkarshmttl,"https://arxiv.org/abs/1803.03827

Maybe it will help. ",1522510135.0
pronobozo,"You should learn it and experiment.

",1522473175.0
Everfast,"If I recall correct the CapsNet performs similar as a CNN (except that CapsNet has a different working principle), thus if a CNN works I expect a CapsNet to work as well. Training a CapsNet and implementation might be harder since there are not many applications with CapsNet yet and there is therefore less knowledge and tools
",1522517188.0
Everfast,"[Create a custom loss function](https://github.com/keras-team/keras/issues/369) where you adjust the weights. For example binary crossentropy: L = -Ytrue * w1 * log(Ypred)-w2 * (1-Ytrue) * log(1-Ypred)

with w1 and w2 you can increase the importance of true positives (w1) or false positives (w2). 

Maybe you can also try  L = 1-precision as a loss function but I have never tried it. People sometimes use this approach for Dice-score. But be aware that focusing solely on precision will give you probably unwanted results, especially in imbalanced data sets.

[Maybe relevant discusion](https://stats.stackexchange.com/questions/190315/what-loss-function-should-one-use-to-get-a-high-precision-or-high-recall-binary)",1522403332.0
lugiavn,"Assuming the output of the binary classifier is a probability distribution (positive vs negative), which you threshold at 0.5 to classify as positive or negative, you just need to change that threshold to trade off between precision and recall, for example a threshold of 0.99 might get you close 100% precision and close to 0% recall",1522728159.0
schrodingershit,"Yes, keep the dimensions checked though ",1522368830.0
maykulkarni,Yeah why not,1522380411.0
menonsandu,"You can
Keras and tensor flow are used together in many applications",1522402559.0
ChmHsm,That's absolutely normal practice. ,1522606636.0
spaceandthyme,"You can and should. In fact, you probably will on many machine learning projects.

Technically Keras is a higher level abstraction over many frameworks, including TensorFlow and if you're doing machine learning with Keras straight out of the box without additional configuration you're using TensorFlow.

Keras has a lot of great built in tools but has missing some essentials like the ability to calculate a confusion matrix, build an ROC curve, or generate an ROC-AUC score. For those - and others - you'll probably find yourself turning to Scikit-learn.",1522676083.0
onmywaytostealyagirl,this is also accompanied by a somewhat hilarious academic paper... https://pjreddie.com/media/files/papers/YOLOv3.pdf,1522335861.0
waterRocket8236,Nice one,1522387152.0
thisismyfavoritename,"Might be overfitting, or your validation set is significantly different (comes from another probability distribution?) of your training set.",1522329120.0
crowoy,"I've added this: https://imgur.com/gHrec65

Which doesn't look right either.",1522345492.0
lugiavn,"How many epoch are those? 

Use early stopping to avoid overfitting",1522729158.0
robert7806125566,"the first layer filters just weighted sum of the raw image.
I think it holds true if you use residual connections.",1522213347.0
xEtherealx,Why do you need to upscale?  You're not going to gain any signal/information by doing so.,1522164653.0
jamesw966,"He says ""eigenvector""",1522164667.0
fuck_your_diploma,"Right around 40'~45', before

 ""...PCA, dimensionality reduction, things that...""",1522160419.0
fuck_your_diploma,This video is a capture from Christopher Wylie todays MP hearing on brexit/etc on this address: https://www.youtube.com/watch?v=X5g6IJm7YJQ,1522160592.0
eleitl,"##Abstract

Although artificial neural networks are powerful classifiers, their internal structures are hard to interpret. In the life sciences, extensive knowledge of cell biology provides an opportunity to design visible neural networks (VNNs) that couple the model's inner workings to those of real systems. Here we develop DCell, a VNN embedded in the hierarchical structure of 2,526 subsystems comprising a eukaryotic cell (http://d-cell.ucsd.edu/). Trained on several million genotypes, DCell simulates cellular growth nearly as accurately as laboratory observations. During simulation, genotypes induce patterns of subsystem activities, enabling in silico investigations of the molecular mechanisms underlying genotype–phenotype associations. These mechanisms can be validated, and many are unexpected; some are governed by Boolean logic. Cumulatively, 80% of the importance for growth prediction is captured by 484 subsystems (21%), reflecting the emergence of a complex phenotype. DCell provides a foundation for decoding the genetics of disease, drug resistance and synthetic life.",1522141168.0
PM_ME_Sonderspenden,But can these simulated cells form a neuron? ,1522159513.0
zephyrppt,A naive guess would be that they're using it for data augmentation. Learn a distribution and then generate additional samples from that distribution. ,1522059882.0
tpinetz,I would guess for style transfer. Something I found is this link (https://code.facebook.com/posts/158223298060942/using-ai-for-new-visual-storytelling-techniques-in-vr/).,1522060920.0
its_ya_boi_dazed,Isn’t this the point of r/learnmachinelearning and r/maghinelearning. Why would you start another sub specifically for tutorials when the top two already have that content. ,1522024507.0
thisismyfavoritename,You can use linear programming to solve any **linear** optimisation problem.,1521998792.0
p-morais,"If you have a linear model and a linear cost function, then sure...",1522020913.0
p-morais,"The better question is why *wouldn't* you be able to do those things at the same time? RL and SL you can do together no problem, and people often do. UL is weirder, but only because UL in the context of neural networks (or in any context, honestly) isn't tremendously well defined. But if you can represent UL as minimizing some cost function with gradient descent, then again you can do it in combination with RL and SL no problem. ",1522021446.0
semi23,Neural network can solve any ML problem. The difference is the activation function you use ,1522057128.0
tiger287,Hey! Looks interesting can you direct msg me !,1521985188.0
pronam,I am. ,1521985609.0
Humblefool_14,"I am in, I guess :P. Pm me 
",1521987767.0
arkiazm,I am in too,1521990267.0
asjad02,I am interested,1521992211.0
abhisheknadgeri,tag me in too ,1521994836.0
cy1994,I'm interested!,1521995127.0
random_mistborn,Hey! I am. I haven't started reading it though. PM me.,1521995146.0
papydur,Sounds interesting. I would like to join too!,1521997234.0
ankush92,I am interested too! I have already covered some portion of the text as part of the Machine Learning course at my University.,1521997896.0
sanemate,In too!,1522002070.0
thisismyfavoritename,"To check for a *stationary point*, you only need the first derivative (gradient). This is usually monitored in neural nets.

Computing the second derivative (Hessian matrix) is usually **extremely** expensive for neural networks as there are a huge amount of parameters. In addition, the Hessian only allows to distinguish between the *nature* of the stationary point.

Finally, there has been work that suggest that obtaining only a local minima (or being near to) might be enough to get models that generalize well.

On mobile, so no sources.",1521941650.0
,[deleted],1521918433.0
zenogantner,Maybe you could try to learn a mapping from 1 channel to 3 channels?,1521889672.0
anonDogeLover,Does the pretraining really help? You can remove lots of parameters by just replacing RGB channels with a single pixel intensity channel,1521908707.0
anonDogeLover,"Also, maybe try something with fewer parameters, like inception v3. VGG is huge and your dataset is small.",1521908796.0
Echsu,"Is there any way to get this newsletter on some other platform than Facebook? I like reading these updates, but don't like using Facebook. Just copy pasting the contents to these Reddit posts would already be fine.",1521891225.0
Ader_anhilator,Deeper learning ,1521865175.0
coder18694,Deeper learning,1521867724.0
letsmachinelearnguy,General intelligence?  ,1521933586.0
lifeadvicesponge, Differentiable Programming,1522068719.0
visarga,Bullshit article - the premise and the alternatives presented. The author picked a few buzzwords here and there and made a soup.,1528002959.0
,Try this: https://pdfs.semanticscholar.org/9ea6/5687a21c869fce7ecf17ca25ffcadbf77d69.pdf,1521808307.0
thisismyfavoritename,"Many libraries do this in parallel efficiently. For instance, hyperopt (if you're using Python).

It's not clear to me what are the benefits of such thorough tuning procedures for non-Kagglers. In my experience, a rough grid search might be enough to get you decent results (depends how much ressources you want to invest for slight gains of performance?).",1521774625.0
realfake2018,"Are we talking something like in the following link about Mackay's quick and dirty method:

https://youtu.be/EJfqOAi_rj8

Note: Please watch one just before in the list to get full understanding.",1521790809.0
SomeConcernedDude,Can anyone comment on how docker containers impact computational speed?,1521758674.0
snes-jkr,"Good work!

Would have been cool to have it in a format where I have the code and get the info while clicking on the line or hovering. But I guess that would require a custom website",1521703965.0
omg_drd4_bbq,Awesome stuff! Did the output glean anything profound? ;),1521728434.0
Everfast,"dropout is actually used to regularize the network, to reduce over-fitting. My personal experience is that trying (if possible) is the best way to know which one works the best. Quick google seatch gave me this article pleading for [dithering](https://arxiv.org/ftp/arxiv/papers/1508/1508.04826.pdf) but dropout is more widely used and there are probably plenty of articles suggesting the opposite

[relevant interesting discussion on the machine-learning forum](https://www.reddit.com/r/MachineLearning/comments/3jq44s/dither_is_better_than_dropout_for_regularising/)",1521672074.0
brodaciousr,"I personally haven’t tried dithering. Are you applying the Floyd-Steinberg method? I need to do some more reading and learn more about dithering to really give an informed opinion. 

For those who may be curious, I found a slightly outdated, but relatively recently updated [article](http://www.tannerhelland.com/4660/dithering-eleven-algorithms-source-code/) that explains image dithering with several examples. 

Assuming the network isn’t too computationally expensive to train, I would apply both methods and see which produces better results. I believe it is most common to use a dropout rate of 0.5. Great question. ",1521679575.0
neuvfx,Goodbye rotoscoping!,1521893182.0
,What happens if you use train_img_data immediately in mode.predict? So reimport the pictures and predict them.  ,1521632577.0
cameldrv,"Sounds like a software problem somewhere, but even if you fix it, this architecture isn't going to perform on a test set.  You are going to need convolutional layers.  The dense layers aren't going to be able to generalize, and you will massively overfit.  You have 40 million parameters in just the first layer and only 1400 data points, and the dense layers don't give the network the ability to generalize something it learns from one pixel position into another.",1521665432.0
Moni93,,1521736152.0
-TrustyDwarf-,"No definite answer at all.. but k-means uses the euclidean distance to some cluster centroids, not cosine similarity to a sample's neighbors. So k-means does something very different and might just not work as well for this data set.

Also how did you figure out the number of clusters for k-means? Setting number of clusters = number of news categories might not be good enough since there could be many small sub-categories (tennis may be very different compared to football, even if both are in the sports category).. so it's kind of hard (but important) to get the number of clusters right. Also what if for example the ""sport fishing"" cluster is closer to the ""nature"" cluster than to the remaining ""sports"" cluster?

Have to tried TSNE clustering? How many clusters does it show?",1521643757.0
TheChaoticGood,Is this something your going to make public?,1522365815.0
dewayneroyj,Check out the [Color FERET](https://www.nist.gov/itl/iad/image-group/color-feret-database) dataset. It contains over 2000 photos with over 800 different labeled males and females. ,1521555275.0
DemiourgosD,"Not sure about the GPU, but I am on 750ti with CUDA 6.5 and I use pytorch 0.3 (0.3.1 won't work anymore unless you install from source): https://github.com/pytorch/pytorch",1521569159.0
snes-jkr,Whats the point? Juist train on the CPU or invest 50€ in another old GPU,1521584866.0
blackHoleDetector,"Keras' flow from directory is quite nice to use for this. Since your directory structure isn't set up for this, I assume all your images are stored in one folder, and the titles of these images are their labels? I'd suggest writing a program that goes into your root folder, and sorts your images into sub folders based on labels. Then, use flow from directory. 

If needed, in [this video](https://www.youtube.com/watch?v=LhEMXbjGV_4&list=PLZbbT5o_s2xrwRnXk_yCPtnqqo4_u2YGL&index=10), I show how the image data should be structured on disk and then show how to work with flow from directory in code.",1521551724.0
atulshanbhag,"You can use any pretrained CNN model like VGG16, VGG19, ResNet. Inception, DenseNet, etc as feature extractors (don't use the fully connected layers) and run a classifier on top of these extracted features (MultiLayer Perceptron or SVM) to classify the cars.",1521509323.0
inkognit,they do work,1521488081.0
,"I'm pretty sure tensorflow works on mac. You can also use Amazon AWS or Google Cloud, those are easier to set up, but they'll charge you per hour (20-70 cent). Learn with CPU instances first if you want to try that.",1521489707.0
rekt_brownie,"It’ll be painfully slow unless you have a GPU. Even then it’s a huge hassle. 

If you have an Nvidia chip, install CUDA. I build pytorch from source for CUDA on my Mac, it works fine. 

If you have a newer model w AMD, there are some initiatives to get the popular DL frameworks working with whatever AMDs competitor to CUDA is called. 

Honestly, it’s not to expensive to open up an account and use Paperspace or AWS, and that’s what I do when I need serious horsepower. You’re looking at less than a dollar per hour. It’s also easier to set up these things on Ubuntu because there are already a lot of prebuilt binaries out there. ",1521494670.0
hilldex,Yep. There's just a fair amount of setup that's involved. Tensorflow documentation will help with the how.,1521493420.0
youngchul,"I have a 5 year old Macbook Pro Retina, and it works fine for me. I have installed Tensorflow and Caffe. Caffe was quite a bitch to install though, even though it was only a CPU enabled install.",1521495675.0
p-morais,I run Pytorch/Tensorflow/Theano on my mac through an Ubuntu virtual machine via vmware Fusion.,1521536422.0
TheMVS,"IDEs like PyCharm have virtualenv and package managers and you can install TensorFlow. You are an student, in my opinion you should start using Keras with TensorFlow so you'll learn basics faster.",1521554473.0
DemiourgosD,Depends on the GPU model. Which one do you have?,1521452260.0
atulshanbhag,"You can't train VGG16 on MNIST unless all layers are zero padded. Also, 16 layers is just too many layers for a dataset like MNIST, even a 2-3 convolution layer network achieves 99% accuracy in within 20 epochs. ",1521468351.0
,"Not 1337 advice, but:
1) I think nowadays it's much easier than in 2014 because the frameworks are better and more high level. Dropout is a feature in tensorflow so you don't have to build all that stuff yourself.  
2) It wouldn't matter if you flip them in training or include in the dataset. Comes down to the same thing. But if you flip twice, you got the original again. You can also skew the images which gives you more options, but in case of facial keypoints I don't think that's wise.  
3) I haven't really looked into it much, but I think this problem is similar to detecting cars on the road (but detecting faces in a picture). [This article](https://www.reddit.com/r/deeplearning/comments/855if0/autonomous_driving_car_detection_with_yolo_model/) posted yesterday shows the YOLO algorithm. If you apply such a ""realtime"" algorithm your video will process much faster. If this is not so much an issue (1 face always middle in the shot for example), you can leave that out perhaps.",1521457024.0
autotldr,"This is the best tl;dr I could make, [original](https://medium.com/@Synced/baidu-apollo-releases-massive-self-driving-dataset-teams-up-with-berkeley-deepdrive-5e785ab4053b) reduced by 72%. (I'm a bot)
*****
> Apollo Scape was released under Baidu&#039;s autonomous driving platform Apollo, which Baidu hopes will become &quot;The Android of the auto industry.&quot; Apollo gives developers access to a complete set of service solutions and open-source codes and can enable for example a software engineer to convert a Lincoln MKZ into a self-driving vehicle in about 48 hours.

> Haifeng Wang, Baidu Vice president and Head of Baidu Research Institute, told Synced, &quot;The partnership will incorporate Apollo&#039;s industrial resources and Berkeley&#039;s top academic team to ramp up the innovation of theoretical research, applied technology, and commercial applications.""

> Apollo Open Platform and BDD will jointly conduct a Workshop on Autonomous Driving at CVPR 2018 this June in Salt Lake City where they will organize task competitions based on Apollo Scape.


*****
[**Extended Summary**](http://np.reddit.com/r/autotldr/comments/859lrp/p_baidu_releases_apollo_scape_possibly_the_worlds/) | [FAQ](http://np.reddit.com/r/autotldr/comments/31b9fm/faq_autotldr_bot/ ""Version 2.00, ~297954 tl;drs so far."") | [Feedback](http://np.reddit.com/message/compose?to=%23autotldr ""PM's and comments are monitored, constructive feedback is welcome."") | *Top* *keywords*: **Apollo**^#1 **Scape**^#2 **driving**^#3 **research**^#4 **data**^#5",1521353163.0
snes-jkr,"No comment, no explanation, what’s the use of this",1521369573.0
Everfast,Maybe take a look at largest connected components or [this](https://www.scipy-lectures.org/packages/scikit-image/auto_examples/plot_labels.html). You can also subtract the smallest from the largest coordinate in the direction you are interested in for every component and pick the largest.,1521323322.0
vibhorkalra,Based on the images you shared finding the largest digit can entail finding the largest digit either by area or by or by perimeter. In either case you can accomplish this with the help of functions in OpenCV post finding contours in the image which shouldn't be hard. ,1521386846.0
,"Scroll to 1/3 of the page for it to get interesting.  
Really cool, improvement of 900x pretty much. One thing that baffled me a bit is that at the image below ""Output Images with detected objects (cars) with YOLO"" (the gif of the final results), there are a lot of misses still.",1521330074.0
rkleinklein,"Many people use a particular notation, in which x = s(x). That way, x' = ds(x)/dx = x * (1-x).

Hope it helps!",1521273597.0
gntc,Here would be fine,1521298003.0
swagh1611,Also StackOverflow ,1521310025.0
anderl1980,"Omg, too obvious :-)",1521316972.0
jinturri,"Looking here https://www.gigabyte.com/Motherboard/GA-X99-UD3P-rev-10#sp, the specifications say ""When an 28 lane CPU is installed, the PCIE_2 slot operates at up to x8 mode and the PCIE_3 operates at up to x4 mode"". So your 2nd GPU will only have access to 8 lanes while the first has access to 16 (though if PCIE slot 4 is populated, slot 1 operates at up to x8). I've never encountered this so I'm not sure how this will affect general performance (particularly deep learning performance), though I prefer everything to be operating at max speed.",1521267896.0
pangresearch,"Yes running at x8 is slower, not by half or anything vs x16, but more than 10%...depends on many variables.

You also need a beefier PSU.",1521524633.0
Echsu,"Linear regression model is a special case of a neural network with no hidden layers and identity activation function.

EDIT: Normally, you would use a deeper and less trivial neural network (with more layers and different activation function) when a simple linear regression doesn't do the job. In the simplest case, you could observe that the regression model works poorly and try to remedy the problem by adding a hidden layer with non-linear activation function.",1521196073.0
nnexx_,"By definition it will not be linear, but yes, regression is possible with NN. You just have to leave out the activation function of the last layer so you can predict on R",1521193293.0
mikaslanche,"If you're familiar with Python, you may have heard of Keras before. It's a library that makes working with Deep Learning really simple.

As I am learning how to work with Deep Learning too, I will try to give you an example of a simple Linear Regression implementation using Keras.

[It can be found here.](https://gist.github.com/mikaelsouza/2de822fb8db23938a5dd8b9d11a33cbf)

It's using [The Boston Housing Dataset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) as an example of Linear Regression.

I tried to make it as simple as I could, so it may not perform that well. but I hope this helps!

If anyone could point mistakes that I could improve, I will be really grateful :)",1521208585.0
roccolacatus,"I recommend you a simple computervision techinique called perceptual hashing: https://www.youtube.com/watch?v=312WOOVMzhQ
",1521183852.0
StartupJeeliz,Our free API on GitHub: https://github.com/jeeliz/jeelizFaceFilter,1521136669.0
anti-gif-bot,"[mp4 link](https://g.redditmedia.com/wW1DmCP-YYL25blrhtI3-T7EnMosZjEOo8rVW066qdY.gif?fm=mp4&mp4-fragmented=false&s=66ccd53e898f1fb22c39e46980e290d2)

---
This mp4 version is 94.22% smaller than the gif (285.08 KB vs 4.82 MB).  


---
*Beep, I'm a bot.* [FAQ](https://np.reddit.com/r/anti_gif_bot/wiki/index) | [author](https://np.reddit.com/message/compose?to=MrWasdennnoch) | [source](https://github.com/wasdennnoch/reddit-anti-gif-bot) | v1.1.2",1521131512.0
apachaihop,"try the deep learning course on coursera by Andrew Ng. it’s in python but you will learn how to make it from scratch. once you are good in C you can port it from python. 

source: i did the first two courses and it was best way to learn about deep NN",1521161158.0
gregw134,"Fast.ai

Edit: Just read you have to write it in C...sorry and good luck.",1521133498.0
zkyzky,"Why not use some opensource frameworks such as pytorch, tensorflow or caffe? You can save lots of time by using one of those.

If you have found some codes related to your project, you could dive into them and build some new features. I would not suggest building from scratch anyway.",1521151923.0
MandirWahiBanaienge,"http://deepbayes.ru

In Russia, you can check. Good thing is they started the application period just few days back and after looking at their assignments I feel enough time is there to finish them. ",1521109584.0
Ravers,Attended [this](http://grammars.grlmc.com/DeepLearn2018/) last year and it was a blast. Check the speakers to get an idea.,1521111891.0
Faunt_,"I can't seem to be able to access the website, it gives me a 404 error",1521115971.0
tdionis,For some of you who use Supervise.ly to annotate images: now you can train & run neural networks to build AI faster. And it's still free!,1521072227.0
F4nelia,"If you run a watch nvidia-smi I'll bet you will see your gpu is at 0% most of the time, occasionally jumping to 100% for a fraction of a second at a time. If you look at system resources you will see 1 Cpu core pinned to 100%. It's not keras fit_generator being slow it's your generator. This is because the generator queues up data from for your nn to process, and your nn can only go as fast as it is given data.

 For example: you have 500 images stored on your hard drive you want to learn from, and you want 50 batches of 10 images per epoch. Each batch your generator is called and has to load 10 files from disk to ram, then they are given to nn to learn from. So the speed you can fit_generator runs at is dictated by, in this case, your drive read speed and how fast your Cpu can load them. A way to mitigate this is by using the multithreading flag in fit_generator, and setting workers to as many threads as you have, this way you will have multiple generators serving your nn. If you do this make sure your generator are threadsafe! You can also set queue size in fit_generator for the case when your worker threads serve faster than your nn can process, in which case it queues up a number of batches equal to the queue size, then turns off workers until more is needed. ",1521052523.0
,"From what I've just learned from the Stanford course, you subtract the mean of all values, so your data is zero centered. Don't know if that helps",1521044846.0
pahtrel,Is it just me or is there no link here just a picture?,1521034507.0
StartupJeeliz,"So sorry guys. Here is the link of the Github: https://github.com/jeeliz/jeelizFaceFilter

And the link of this integration exemple :
https://jeeliz.com/demos/faceFilter/demos/threejs/luffys_hat_part2/",1521123010.0
Rezo-Acken,"The model used only predicts the next day and when it comes to the test set, the previous days for each predictions are assumed known. This means that the forecast of the test set... well actually uses truth values of the test set.

The PACF plot is not commented while it actually says everything. The lag1 pacf is very close to 1. This means previous day value is a very strong predictor of the next day value. This means that as a result a simple next day forecast is bound to be good. When it comes to the test set it makes it look very precise since truth values of the previous day are used to forecast the next day...

Edit:There is even a code error see below",1521040861.0
Captain_Price_777,"Could someone summarize how well it performed in real,maybe current data. I did a rough scan and error seemed too good to be true.",1521035033.0
autotldr,"This is the best tl;dr I could make, [original](https://towardsdatascience.com/how-i-implemented-iphone-xs-faceid-using-deep-learning-in-python-d5dbaa128e1d) reduced by 93%. (I'm a bot)
*****
> Thanks to an advanced front facing depth-camera, iPhone X in able to create a 3D map of the face of the user.

> Using deep learning, the smartphone is able to learn the user face in great detail, thus recognizing himher every time the phone is picked up by its owner.

> Understanding FaceID&quot;The neural networks powering FaceID are not simply performing classification.&quot; The first step is analyzing carefully how FaceID works on the iPhone X. Their white paper can help us understand the basic mechanisms of FaceID. With TouchID, the user had to initially register hisher fingerprints by pressing several times the sensor.


*****
[**Extended Summary**](http://np.reddit.com/r/autotldr/comments/84bja8/how_i_implemented_iphone_xs_faceid_using_deep/) | [FAQ](http://np.reddit.com/r/autotldr/comments/31b9fm/faq_autotldr_bot/ ""Version 2.00, ~295735 tl;drs so far."") | [Feedback](http://np.reddit.com/message/compose?to=%23autotldr ""PM's and comments are monitored, constructive feedback is welcome."") | *Top* *keywords*: **face**^#1 **network**^#2 **picture**^#3 **using**^#4 **unlock**^#5",1521012323.0
onkelFungus,Good starter configuration with nice GPU. Maybe memory could be an issue because 8GB RAM is not much when you think about certain problems like word embeddings.,1521014623.0
youshouldnameit,"Depends on how you measure surpassing. If you measure it as higher percentage of correct labels in test, then yes. In theory it can perform worse in training then on test  for example if the training data is a lot harder to label",1520980206.0
Ilyps,"Can you tell us more about your methods? I won't be looking at code, but I can probably give some feedback and suggestions. ",1521292095.0
IHaveNeverEatenABug,"The first sentence in the article:
> In this post, we’ll implement a deep neural network with one hidden layer (and ReLu and softmax activation functions) purely in SQL.

Guys, a one layer feed forward neural network is not deep.",1520961757.0
jandremarais,"I have worked on this problem before. We just used a fairly standard CNN trained on a dataset with ""live"" examples and ""spoof"" examples. Did quite well - F-score of >98%. But you will need smarter ways if you have limited data. Maybe look into something like depth estimation or images from multiple angles, etc. The hardest part is to get it to work with different cameras.",1520931145.0
H495zl,Can we use some modules to gain some datas about the differences between 2d with 3d (such as using Infrared module to  measure the depth of some special areas in target object) ? And then we try to distinguish them?,1520932208.0
Rezo-Acken,The python community is not dosappearing any time soon. So if early adoption is a concern for you I wouldnt risk it for now. Unless your company is all about R and it facilitates your work.,1520943405.0
traviton,"Packt books are bottom of the barrel, compared to O'Reilly, Manning, and Wiley.",1520917616.0
HP_Envy,"OP is a notorious Reddit spammer, he has created over 100+ accounts to spam his affiliate / referral links, he links to Twitter which contains an Humble (or Amazon or GameStop) affiliate link.

Check his post history.",1520949810.0
PM_ME_Sonderspenden,This loss function is completely unrelated to the number of neurons in the output layer. Also with millions of classes the sparse version of this function makes sense. ,1520890074.0
thegymnosophist,"the coolest thing there is the GPU usage graph between keras and pytorch !
I'm surprised how different it is. It was not wholly clear that the greater, more consistent usage in pytorch is necessarily, better though. ",1520930107.0
varunagrawal,"Deciding the architecture of a CNN is mostly an art that few are skilled at and which involves a fair amount of trial and error. There is work that illustrates how Google used meta learning to perform an architecture search but the computational cost is too high for the deep learning layman to use.

These days, most people just use the 101 layered versions of the recent ResNet and ResNext architectures and they are almost always fine tuned rather than learned from scratch. Unless you have a million sample dataset, there is little to be gained by training from scratch. ",1520829916.0
pieIX,[The second inception paper](https://arxiv.org/abs/1512.00567) has some useful heuristics for designing CNNs if you want to roll your own. ,1520834372.0
hardToFindANick,"The error is mentioning the feed dict key for which you use the variables x and y. Your print statement is displaying the shape of one feed dict value. 
 ",1520673642.0
omalleyt12,"Looks like you changed the variable y_ to float_y_, thus float_y_ should be the key used in your feed_dict

That error is saying you've defined either x or y_ as an int somewhere else in your code",1520871536.0
ffmurray,[relevant xkcd](https://xkcd.com/927/),1520645981.0
iacolippo,I answered you on StackOverflow,1520591528.0
FalsyB,"This could be Shazam for food!
",1520580286.0
the_bored_potato,"One of the simplest implementations is what I went for as well - Flask. You can write your own web server and open up endpoints as you like. 

http://flask.pocoo.org/

You'd have to host this on a vm or your own machine and open up port 5000 (or whichever you specify) for it to be accessible via the web.",1520546727.0
Cunic,"At first glance:

Eqn 21: the loss function is J = (1/2)(y_t - z_t)^2. The derivative of J with respect to z_t with respect to z_t is just the chain rule: derivative of (1/2)(f)^2 * derivative_of_f. So it's just the inside part: y_t - z_t as the derivative of z_t w.r.t. itself is just 1.

Eqn 22: now we need to compute the derivative of J w.r.t. W (the matrix that maps from hidden units to outputs. so that is dJ/dz * dz/dW. We already know that dJ/dz = (y_t - z_t) so we only need to compute dz/dW. We know that dz = softmax(Wh_t + b). Since softmax changes at the same rate with respect to each input we only need to consider W * h_t + b where the derivative of z_t with respect to W is h_t. Additionally, to compute the loss at timestep t, we need to sum the contribution of each hidden layer on the output of each timestep, so it's the sum of dJ/dz*dz/dW for each t!

Eqn 23: Same as eqn 22, but this time we need the derivative with respect to h_t instead of W. The term dh_t becomes important when deriving the loss function with respect to the hidden_state_transition and input_to_hidden parameter matrices!

I hope that helps :)",1520617828.0
geodesic42,"[Hurray, it's more of the same trite, marginally true DS mantras.](https://www.excella.com/insights/5-data-science-trends-that-need-to-end).

The baselines are important for data exploration, but all these nigh identical blogs praising the simple models need to be careful not to down play the necessity of moving past them in the final product.

I will say, I'm really glad to see it mention CNNs and boosting at least! Weird juxtaposition to see them next to linear and logitistic regression though. So, it's a big big step in the right direction from the usual blog posts on this topic.",1520400179.0
letsmachinelearnguy,Bad spam bot.,1520397922.0
smart_dumb_smart,"I have used https://www.ncbi.nlm.nih.gov/projects/genome/guide/human/index.shtml
It contains lots of downloads, with the sequence and any medical condition",1520251030.0
Emiller8800,"Try r/bioinformatics. Contrary to popular belief you don't just get one really long string when you sequence DNA.

I remember being in your position a few years ago and thinking that everything was entirely determined by just the sequence. It's extremely more complex and you might not have the hardware requirements to process the huge datasets.

Not to deter you, just pointing out there are reasons you don't see NN uncovers genome mysteries. I'm just thinking you hadn't thought this project through entirely since you want to determine gender when in most cases that are determined easily by the number of X chromosomes in normal individuals. 

A project that might be easier to manage with similar principles might be determining cryptokitties' traits based on their genetic code.",1520287593.0
yaseada,"Hey thanks I was looking to use Mxnet, only used pytorch for now, and it's exactly a topic that I wanted to study too ! :)",1520303313.0
itsdevkay,Do I need tensorflow ? I’d really like to use it but in the case I can’t do I need it ?,1520211402.0
Accumulative,"I'm having the exact same issue on a 2009 MacBook, it must be a hardware issue?",1520275549.0
gokstudio,"Is your python up to date? IIRC, tensorflow supports 2.7.x and 3.5.x",1520243563.0
FearlessAnt,Get ready to be disappointed by the results.,1520189251.0
autotom,"Honestly this might not be the best idea for your thesis. 

Crypto is way more volotile than stock markets, and anyone who could predict stock markets with sufficient accuracy to base a thesis off would be the richest person in the world. ",1520205456.0
Echsu,"Either you can predict the prices, in which case you shouldn't publish the thesis but instead drop out of the school and make a lot of money using your predictions, or you can't predict them, in which case the thesis won't be very interesting.",1520260543.0
deepfakesclub,"While pure price prediction is probably difficult, you might be able to do something related, like predict the price of bitcoin using the price of altcoins... maybe too easy, but you get the point.",1520219869.0
,"As everyone has pointed out, it's not the best idea to attempt this project because crypto price prediction is pretty much impossible. What you could do is attempt a deep learning model for portfolio optimization though. Someone has already done this and the paper is on arvix, and he got decent results.",1520797026.0
hunduk,"Hi,
Its funny because I am actually working basically on the same thing in my thesis, so PM if you want to share some information.",1523129239.0
french-crepe,You should start by taking an intro to DL online course and do the homework. That should fix the getting started issue and make you accustomed to a DL framework. Andrew Ng’s or Hinton’s courses are good.,1520185476.0
deepfakesclub,"The hospital baby samples in your paper are horrifying :o

Can you describe what you hardware was like - you mention storing 26 TB of data. Also, what GPUs did you use?",1520184037.0
010100100000,Very nice! What type of NN? CNN? And how did you train it? Virtually? ,1520212709.0
gokstudio,"Awesome work and write up!

I have a few questions about the project

1. I don't understand what you mean by black box here, could you clarify?
2. It seems to me that the reversal attack can work iff you had the same image in your dataset. Let's say you want to recreate an image of a tree (or the coca-cola logo) with a pix2pix net trained on celebA, this is essentially what you showed with the poisoning attack. So,   why do you call this general?


One other thing was that the hash poisoning attack you mentioned would be true of any application of hashing. It's natural for multiple inputs to map to the same hash because there are so few hashes compared to inputs. 

So, hash collision is an issue even in non-adversarial settings. Any competent use of hashing would have to handle this one way or the other. One common way is, like you mentioned, using multiplayer hashing algorithms.",1520110128.0
qwasz123,"If you want something like Siri that's more Ai. I forgot the name but there's a free programming language for building chat bots.

For Alexa I'm unsure of her backend but honestly... You just need to research and try it out. It won't be easy but it'll look great once you start taking the first steps.",1520102973.0
Dr1T,"Checkout LinkNet from the same lab as ENet. 

It's heavier than ENet but improves significantly on performance and it's still meant to perform semantic segmentation in real time",1520096649.0
pastaking,Does anyone have a tldr?,1520077378.0
StoneStalwart,This guy is about as good at giving a lecture as pigs are at flying. ,1520106775.0
dumstick,This guy steals research. Fuck anything he is involved in. ,1520082057.0
MountainHawk81,The video doesn't seem to be working...is there another link somewhere?,1520361627.0
k9thedog,"If the competition's noodles are really visually different in a way that can be seen in the photo (color, curliness, length, thickness and such), you can train a model and automate the blog crawling.

But if the visual difference between the noodles is slight, your model may latch on to other differences in your data set. For example, if, by chance, most of company A's noodle photos are taken in sunlight, but company B's noodle photos are taken in artificial light, the model will focus on that and not on the actual noodles. Gathering a good photo dataset with no such bias may be harder than asking all employees to take photos. From my experience, if you ask people to take photos and don't give them guidance, they'll all assume the same thing about your request and do that - it's like a magical group telepathy. If you give them too much guidance, they'll follow it to the letter and you end up with very little variation again. People are really bad at acting randomly when asked to do so.

See [this paper](https://arxiv.org/abs/1602.04938) for a contrived example of a model trained to tell husky from wolf and focusing on the background in the photo instead.

That being said, it's a very interesting challenge. ",1520002029.0
wild_thunder,I think it's a hard problem. I can't see much difference between the two pictures so have a hard time believing that some net could do any better at distinguishing them,1519996150.0
niujin,"I would say a few thousand pictures of each category would be necessary. You could take Inception in Tensorflow and retrain with transfer learning. It would be an easy task except that the pictures look very very similar.

However how you approach it depends on

* what will it be used for?
* what accuracy do you need?

what should it do if

* you give it a picture of something that isn't noodles?
* you give it noodles from a third company?
* it's unsure what's in the image?",1520524614.0
letsmachinelearnguy,"NV's lead in consumer graphics, leading to a huge revenue disparity between them and AMD RTG, leading to a huge disparity in R&D spend, leading to better tooling and evangelizing and software updates and support, leading to popular libaries like Tensorflow being built on their proprietary CUDA API.

It's a long and vicious cycle, but it has been a decade of Nvidia beating AMD in the market.  Crawling back from this point could be very challenging.  For reference, the Steam hardware survey shows something like 86% to 8% advantage for NV over AMD.  Ouch!  And if you look at their hardware its pretty clear NV's margins are much better as AMD requires larger dies, more memory bandwidth, and more power to create equivalent (gaming) performance.  These all translate to higher material costs for cards that are competitive with cheaper cards from NV.  Retail pricing reflects performance and value to the consumer, not the BOM.  

Or a better way to say it, NV works smarter, not harder, using R&D to produce efficiencies instead of throwing more hardware at the problem.  When your sales volume is high enough, saving $0.20 per graphics card by spending $1m on R&D makes sense. For AMD, their sales volume is too low to make this happen.  Again, vicious cycle. 

Looks like there is an open issue and there as been some effort on OpenCL with Tensorflow, feel free to read the status updates:
https://github.com/tensorflow/tensorflow/issues/22

There are lots of technical debates online about OpenCL vs Cuda if you want to dig around the web, but I tend to think the real causal force here is the R&D dollars Nvidia is able to bring to bear on the problem, and the opinions you will find online are more symptoms of that. ",1520021128.0
Hathery,"I've been itching to learn a bit about the industry and to be able to create & train ML models myself; I'm glad Google decided to put out a course where I wouldn't have to worry about the quality of instruction.
",1520007718.0
matej1408,"If you go with CNN you can fit all predictions in one model. You can have couples layers of convolution layers (5-10 for the beginning) followed by lets say one dense layer. Now lets assume that you want predict couple things:
1. Is male or female (0 or 1)
2. Colour of clothes- you can try to predict category of colours like light blue, dark blue, medium blue etc. Try to put all colours into maybe 30 bins, then try to predict bin. Categorisation is much easier than regression. Regression is when you try to predict exact values for RGB of that colour.
3. Types of clouthes, lets say 10 categories.

Now after dense layer you can try to make 3 parallel smaller dense layer for each of which is predicitng one of 3 things above, so each one would have different output size (1,30,10)",1519922348.0
Rezo-Acken,"My biggest question is do you have a labelled dataset and how big is it ? 

You need to look into deep learning with convolutional neural nets. Taking the coursera course should be enough to at least get started.

Depending on size it will affect how you train it. Unless you have a huge one I suggest doing transfer learning from VGG or Inception and only retrain some of the end layers. How many you retrain depends on how big the dataset you have is. For prediction yoi can predict both at the same time. However from a code perspective its up to you if you want to focus on only the gender first to build a working pipeline. If you are at ease with deep learning you can skip that and directly predict both. In either case you probably dont need different models.

Finally make sure your pre processing is good too.",1519923406.0
,[deleted],1519862223.0
inkognit,"Embeddings allow you to do that, but for known words. unknown words are treated with an unknown token ",1519896447.0
LoveOfProfit,/r/2meirl4meirl,1519859335.0
throweedev,Awesome work! ,1519853969.0
iacolippo,"If you're talking about Torch7 and not Pytorch, it takes a computational graph and a set of numerical values for the inputs and returns a set of values for the gradients at those input values (symbol-to-number differentation) (source: Deep Learning book, Goodfellow, Bengio, Courville)

If you want to understand the actual code: it calls a backward function that calls accGradParameters and updateGradInput to update parameters of the layer and the value of the gradients at the input of the layer.
Source: http://torch.ch/docs/developer-docs.html#_

You can get the logic of backpropagation through Reshape and Concat by reading these functions in their source code: 

https://github.com/torch/nn/blob/master/Reshape.lua
https://github.com/torch/nn/blob/master/Concat.lua ",1520242412.0
utkarshmttl,I am a student at a Centre in Delhi where many students arw enthusiastic about and have decent experience in Deep learning. HMU for more. ,1519841442.0
,"Meetups in your local city, you can meet many people who are in the same train as you are.",1519845303.0
jyotipch,Doing some individual efforts.,1519826180.0
stn994,I will return here in 1 month(Leaving this comment for reference).,1519830832.0
stargrazie,"I am.working on something as well, if someone wants to collaborate or just discuss anything about DL in general feel free to message me.",1519855449.0
sr1har1,"I'm Srihari from Chennai working on machine learning and computer vision using deep learning. Do pm me for collaboration.
Also actively participate competitive machine learning hackathons in hackerearth, kaggle :)",1520169261.0
Envenger,I have been working as a freelance game developer and software engineer for last 4 years now. And just starting an AI startup focused on game related AI but plan to move into other things later. ,1520434022.0
akaleeroy,"Never mind classifying your music library by genre! You can just do a Discogs lookup. The big prize here would be to detect song similarity. You could then navigate your entire library as a graph hopping from related tune to related tune, maybe even across genre gaps. Or train to extract certain musical features across releases and genres. For example stuff like the [Amen break](https://en.wikipedia.org/wiki/Amen_break) or [a specific kind of electronic sound](https://www.youtube.com/watch?v=RDibwiKBq78#t=45s) you like.",1519810461.0
Magick93,"Hello

Yes I am working in this space. Feel free to PM any questions.",1519808512.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/algotrading] [Seeking anyone with Machine\/Deep Learning experience in Financial trading](https://www.reddit.com/r/algotrading/comments/80r6xx/seeking_anyone_with_machinedeep_learning/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1519773906.0
,[deleted],1519880451.0
DemiourgosD,Thanks for posting. This link is broken for some reason: https://github.com/%E2%80%A6/%E2%80%A6/blob/master/courses/dl1/lesson1.ipynb,1519803015.0
anonDogeLover,Look up Siamese networks,1519764357.0
gogonzo,"> Inception V3

> Any ML Problem

Abstractive summarization of long documents with out of vocabulary terms? No way",1519694556.0
carlthome,"Softmax is really only meant for one-hot encoded vectors. When each sample has multiple ones (i.e. multi-hot labels) you should probably use a sigmoid activation function instead so you don't normalize the outputs to sum to one, but still maintain the boundedness you desire.",1519682915.0
ConcreteGidget,Why don't you just use seaborn? (I am a noob so thats an actual question),1519679700.0
sr1har1,"https://github.com/brucechou1983/CheXNet-Keras

They serve a similar purpose I think, feature wise heat map using gradcam. Do have a look and let me know if it helps",1520166634.0
kookaburro,Have you tried something simpler e.g. Naive Bayes or decision tree?,1519572913.0
nnexx_,What is the context here ?,1519579711.0
oathbreakerkeeper,What have you tried? ,1519614187.0
omalleyt12,"Just out of curiosity, can't complex-valued N-dimensional inputs or outputs simply be treated as 2N-dimensional real-valued and then run through convolutional layers as normal? Is there something I'm missing here that would call for a different architecture specifically designed to take account of the complex nature?",1519613866.0
carlthome,"The most obvious would be any data that relies on spectral transforms where you want parameters in the Fourier domain, I guess.",1519506165.0
omalleyt12,"50% for the training set as well as your validation set? If so, there's likely an error in your code/methodology, since 50% is equal to random accuracy for this problem.

If your training set performs well but your solution doesn't generalize to the validation set, then other things could be going on.

In general, I wouldn't add the segmentation step to any classification problem like that. I say that for two reasons

1) You're introducing another potential source of error

2) For classification problems, ""attention"" is solved pretty well by using a GlobalMaxPool or GlobalAvgPool as the last layer of a fully convolutional network",1519494892.0
kingnesh,"CNN's use a concept of weight sharing. Essentially the filters are the weights so your weight matrix would be 20x20. If you had multiple filters (n), your weight matrix would be 20x20xn",1519351481.0
mdda,Here's a [quick example of a 3x3 convolution kernel operating on an image](http://redcatlabs.com/2017-03-20_TFandDL_IntroToCNNs/CNN-demo.html) (output on the right).  Try playing with the numbers.  The key thing is that it's the same 9 weights applied as a sliding window across the entire image.,1519393735.0
InterstellarRun,"I'm certain the largest datasets are all corporate datasets. Google, Facebook etc definitely have many pedabytes (probably much more) of data.",1519356319.0
polaroid_kidd,Buddy of mine is writing his thesis on NMT and his datasets are roughly 60gb. These are all public datasets.,1519379247.0
nunz,"Once you create the instance, download the key pair that's created. You'll use that to connect with PuTTY or ssh, depending on your local OS.  


From there, you don't need to upload the code to your EC2 instance. Just clone it. :)",1519327454.0
houqp,"Shameless plug, you can also give Floydhub a try. It provides fully managed environments for all DL frameworks plus experiment management, versioning, serving and much more. You should be able to get your code running with one command without worrying about how to manage EC2 instances.",1519345167.0
prassi89,"Learn the lower triangle, and construct the conv matrix yourself. Pass gradients only to the triangle, constructing the conv at runtime.",1519321316.0
tech_ai_man,no,1519554249.0
brokenplasticshards,"You could set a threshold. If the face doesn't fit well enough in any of the 10 identity classes it's classified as ""unknown"". But to remember it you'd have to train.",1519305704.0
Tobaganner,I was wondering if the program would learn to speak on its own and gain a sort of personality,1519336431.0
the_bored_potato,"Absolutely! This is called natural language processing and would ideally use recursive neural networks. You'd have to be more specific about your requirement for a ""proper word"" or  ""response"" though.",1519249859.0
LoveOfProfit,The demos on [this](https://audiodemos.github.io./) page are really neat.,1519231063.0
fuckme,"I haven't fully read it yet, but I am interested to see how this approach is similar to/differs from Tachotron2, which I don't think is a one/multi-shot type of thing.",1519231243.0
thijser2,If you want to do neural transfer in python why don't you use [this](https://github.com/anishathalye/neural-style) instead? Or if you want pycaffe [this](https://github.com/j2kun/style-transfer-pycaffe) one? ,1519204797.0
klingon33333,"This is a fantastic course, highly recommend
",1519221337.0
cthorrez,Is there a way to access assignments?,1519255441.0
,[deleted],1519212587.0
Miejuib,"Hey guys, I put together a little project to implement dynamic neural networks in tensorflow, and I figured someone might find it interesting, so here it is! Hope you enjoy",1519165669.0
WearsVests,"LightGBM had a whole thread about this:

https://github.com/Microsoft/LightGBM/issues/331",1519171177.0
junkwhinger,"I wrote a short blog post on making a text classification model (LSTM) with PyTorch. Using parameter grid I generated 24 models (varying LSTM layers, uni/bi-directional, dropout, etc). My best model produced f1-score of .56 (6 labels), but I think there should be a way to improve the model performance. Any advice would be greatly appreciated :) Thank you! ",1519131077.0
sleeepyjack,Step 1: use Tensorflow instead.,1519038097.0
geodesic42,"Keras: great if you can get away with a basic NN.

TensorFlow/PyTorch: when building a simple network by defining abstract layers isn't good enough.",1519131989.0
drakesword514,"There is this ransac algorithm. Random consensus sampling. To identify the underlying plane / surface.
Check this out maybe,

http://cg.cs.uni-bonn.de/en/publications/paper-details/schnabel-2007-efficient/

",1519103014.0
p-morais,"Dagger is actually pretty simple. The motivation for it is that if you do straight supervised learning on data from an expert the error from not perfectly fitting the training data will compound over time and your policy will quickly get further and further away from the expert trajectory, until eventually it's somewhere it's never seen in the training data and will behave unpredictably.

Dagger fixes this compounding error issue by having a step in the training regime where the policy runs for a while, and then the expert ""corrects"" the policies mistakes and these corrections get added to the training data. The result is that the policy is taught how to correct it's own bad behavior, and stay close to the expert's trajectory.

It's basically a loop like this: 

1. Gather some (state, action) data from an expert and train a policy on it.
2. Run the resulting policy
3. Have the expert label what *it* would have done at all the states the policy visited. Add this data to the training data.
4. Train the policy on the new, aggregated, data.
5. Goto 2",1519114451.0
carlthome,Not deep enough. Needs more layers.,1518992647.0
Kaio_,"this is data moshing...how does this relate to the sub?  
r/brokengifs  
r/datamoshing",1518995901.0
Jsd5,"I have run through memory issues on my Titan X pascal (12 GB) when training LSTMs of similar sizes, so that would probably be the main concern. Or you will just have to get clever with graphics ram.",1518800870.0
naikio,"Hi!

If I understand your code correctly, you actually never go to a reduced feature space until the Dense Layer. You just do convolutions with ""same"" padding, so you input a Nx300xchannels image, that becomes Nx300x64 (which is a much greater space than the input) after the first convolutions, then Nx300x32, and so on until the Dense layer.

In this way you are never compressing your input (which should be the purpose of the encoder, I guess) until your Dense Layer, and then dramatically reduce it from Nx300x16 to 1024.

You could try to get rid of the Dense Layer, and instead perform max pooling after each convolution. You feature maps would then look like Nx300xchannels --> (n/2)x150x64 --> (n/4)x75x32, and so on. This way you would gradually reduce the feature space and force the network to learn a useful compressed representation.

Then do the opposite in the decoder (i.e. re-increase it again to match input size. You can do it by using conv2d_transpose in Tensorflow).

Hope it helps! :)",1519638331.0
nnexx_,I think using RNN such as lstm are a way better option when dealing with sentences :),1518778549.0
marrabld,Why use an RNN or LSTM? They're better for time series data.  Why not start with a perception. Start with a couple of layers then go deeper if you need to.,1518741716.0
omalleyt12,"It's hard to know without more information but I'll take a stab at it

Assuming your polygons are 2d, you could connect and plot the polygon points to create an image for each polygon (if 3d, maybe try a 3d array with every point inside the polygon set to 1 and every point outside it to 0 but I'm not as confident that will work well), then you could feed these images to a CNN. You'll need to scale each image to an appropriate size, so you might want to include additional inputs to the CNN like how scaled up/down the plot is, depending on how much size matters to the class labels",1518749313.0
Rezo-Acken,"Why have you used RNN or LSTM ? These are aimed for sequential data like sentences or time series. Isnt your data an image of the polygon ? In that case you want to look at convolutional neural networks.

What does your data look like ? An image or just point coordinates ? If image go CNN. If coordinates you have a few routes to try. Create the image and make a CNN on it or directly try a multi layer perceptron. But for that Id probably add some feature engineering like distance and the like to help training.

Can you share data or ots private ?",1518751955.0
,"Correlation training can be achieved with Hebbian learning, which is more representative of biological process. ",1518710013.0
new__vision,There is a fascinating object detection/tracking architecture here that includes a CNN and LSTM: http://guanghan.info/projects/ROLO/ ,1518731457.0
omalleyt12,"If I understand correctly your output dimensionality is the same as the number of sensors? 

If that's the case, my intuition would be to first try stacking the sensor frames so that depth dimension is time. Keep say the 5-10 most recent sensor measurements for each sensor (may vary based on your application). Then you've essentially got an image segmentation problem and you can apply techniques from that domain ",1518748646.0
BeatLeJuce,of course they can,1518677561.0
nameofthename,No.,1518662011.0
OPLinux,"I learned a lot from the freely available YouTube lectures from Stanford (https://m.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)

Btw, don't think the simplicity of Pytorch means it is less capable! While it probably has some missing functionality compared to tensorflow, you will definitely be able to implement all the networks you encounter whilst learning about DL/NN.",1518645694.0
yaseada,"I loved cs231n, all the videos are for free on YouTube and the assignments on the website. I did all the assignments, it's easy to see when you're wrong or not and others have put their solutions on github.
I used pytorch and loved it, it's very intuitive and kind of numpy like ? 
If you need any help OP I still have my assignments, I also should push them on github, when I did the pytorch ones there weren't any pytorch solution online at the time.

Fastai seems nice, I hesitated a lot between this one and the cs231n, but I think cs231n held more theoretical concepts and coming from physics while not beeing a very good programmer was good. I've heard that fastai is more for people who like to code but I might be wrong !
Andrew Ng DL seems nice too, to bad it's not for free :)",1518661156.0
Rezo-Acken,Fast.ai and the deeplearning specialization on coursera are where Id go to start. Needs extra project practice though...,1518639998.0
lushdogg,"I’ve taken this course.  It is excellent as it teaches you the theory, math and how to code a basic neural network. 

When you take the later courses on CNNs or how to write NNs in Tensorflow, you will understand what is happening behind the scenes which is super valuable.  ",1518612656.0
jtmont8,"What I like about this write up is it's end-to-end. Most of the ML write-ups leave you with a Keras model. Leaving many questions around how you turn the model into a product. Especially if you have to move the model to a non python platform. Really good read, enjoyed it.",1518538517.0
chunf,"Really good read, but I am wondering how do you integrate or run the entire end 2 end process in a production environment, can someone shred some light or provide some best known methods on this? Thanks.",1518857886.0
RetardedChimpanzee,"Manually train a subset of images train a network on it. It will be bad, then you feed your training set through your model. Have it annotate all your images.  Retrain, re annotate images with Better trained model, then retrain again. ",1518485095.0
infinitone,I've been thinking about using edge detector algorithm -> bbox... haven't had a chance to test it out.,1518501735.0
Boxsc2,"I've done this a few times and you have some options:


1) Background subtraction 
 
If your object is moving you can remove the background and take the motion blob, wrap that in a bounding box and you have the object in a bounding box per frame.


2) A Tracker

There's quite a few trackers like KCF - https://docs.opencv.org/3.2.0/d2/dff/classcv_1_1TrackerKCF.html. Where you tell the tracker the objects initial position and it will try to find it in subsequent frames. ",1518542224.0
nnexx_,"Try adding dropout to your flat layers. Also, how big is your dataset ?",1518461736.0
abhisheksgumadi,"1. detect all faces.
2. crop out the face regions.
4. embed the face crops using a pretrained facenet model.
5. cluster them using may be DBScan or some other clustering method. ",1518474357.0
nickbuch,This article would have been relevant like 3 years ago,1518469921.0
zenogantner,"I would start with programming, and do this for about 1 year.

Not being able to program will seriously limit your ability to run experiments and try out things.
You do not have to become the best programmer in the world, but ML without programming makes no sense at all.

After a year of programming, head to http://www.fast.ai/ and work through ""Deep Learning for Coders"". It's free, quite dense, and state of the art in terms of software and models used.",1518452561.0
nnexx_,"First start with machine learning, then you can go deep. Andrew NG courses in coursera is a pretty solid way to start (but can be a little brief sometimes). 
I personally recommend Washington University’s Machine learning specialization. It’s goes more in depth but requires a bigger involvement.
After that try to challenge yourself and read other people code ",1518448962.0
Rezo-Acken,"My suggest path: 
Python programming on EDX from Mit

Machine learning from U washington on Coursera or the course from andrew ng depending on the amount of time you want to put in. Just remember that ML is important to know... a DS that only knows DL in his toolbox is not going very far.

Deep learning specialization on Coursera by Andrew Ng.

Then from there you have the basics and can chose different stuff. More training on fast.ai and intro to DL for example. Or dive right into Kaggle and DL problems to just be better at Python with DL.",1518492216.0
nishank974,"Start with machine learning by Tom Michell CMU. This will set all your concepts. Then go ahead cs231n by Stanford University for deep learning algos.. then cs231a by Stanford University again..  
hope this helps",1518474535.0
szul,"Work towards getting solid skills in Python. Learn Python the Hard Way and Dive into Python are good books for that. Then get familiar with Jupyter Notebooks or Azure Notebooks (Jupyter in the Cloud backed by VMs). After that, take some data science and machine learning courses on Udemy or Edx, or use the video tutorials on Safari Books Online. Then you’ll have the familiarity with the mathematics and subject matter for Deep Learning, in order to tackle CNTK or TensorFlow.",1518485936.0
BruinBoy815,Following.... ,1518342014.0
artificial_intel423,"Check and make sure those classes are for sure present in your validation/test set. If they are in the train but not test, it won’t have anything to compute, even if you had observations that were classified as those classes incorrectly. ",1518278355.0
ConfidentHorse,Looks like one of your training samples produces an inf output. This is usually the result of a division by 0.,1518706724.0
weelamb,"https://github.com/abisee/pointer-generator

Here there’s a paper, blog, and working/updated code for a state of the art text summarized. Really good for getting familiar with text summarization. Her code is very well commented too. Without too much work I’ve been able to train the model on different data and modify the model.

For imo the newest in text summarization, check out this paper. They replace the RNNs in traditional seq2seq models with the Transformer architecture. There are many implementations of the transformer in pytorch and tensorflow

https://openreview.net/pdf?id=Hyg0vbWC-",1518210584.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/deeplearners] [where\_to\_start\_for\_text\_summarizer](https://www.reddit.com/r/deeplearners/comments/7wepsv/where_to_start_for_text_summarizer/)

- [/r/neuralnetworks] [where\_to\_start\_for\_text\_summarizer](https://www.reddit.com/r/neuralnetworks/comments/7weix4/where_to_start_for_text_summarizer/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1518194975.0
HugeSherlock,"Looks good, but why all codes are in pictures?",1518212188.0
porygon93,"The bounce issue holds if you monitor the validation loss, not training. It seems like it is converged after some fluctuations, which is perfectly okay. However, this graph is not quite informative. Is it training or validation loss? Is it loss per epoch or step? Are you doing a classification or regression? These are important info.",1518162959.0
Rezo-Acken,Got a few sources to add on my training. Thanks. At least this was honnest on the time commitment... im usually not a fan of those learn X in Y months but that one has a sound path.,1518099427.0
sankit123,Half the courses are in tensorflow and remaining half in pytorch. What kind of learning plan is that? Pick a framework and stick to it. ,1518113753.0
spaceandthyme,"Solid recommendations on resources
",1518108893.0
YearLongSummer,"Says a year of coding experience as recommended prior to first course. What language, and how do we fast track that? ",1518155363.0
wdroz,"If your final activation function is RELU, you can't reach negative number in Y.

I would try to increase batch size maybe 100-150 and add several batch normalizations.

I can't say much without looking at the training and validation curve.",1518077687.0
JSwuggin,Might wanna take a look at DeepMind’s Neural Turing Machine and Différentiable Neural Computer,1518095931.0
luongminh97,"No it won’t. Random arrays have no inherent structures in them to be discovered. Arrays can be of any length, and contain numbers of arbitrary size. Neural nets are not designed to handle this kind of problem. They won’t ever even come close to the nlogn comparison based sorting algorithms.",1518071338.0
xiaodai,"I found this which is use brainfuck but it's close in concept to what I am asking. 

http://www.primaryobjects.com/2013/01/27/using-artificial-intelligence-to-write-self-modifying-improving-programs/",1518152708.0
thisismyfavoritename,"If I understand correctly, your inputs would be temperatures that you extracted from a message using nltk or whatever, lets say 60 F, and from that you want to associate the label ""cold""?

You basically would need a data set with labels cold, comfortable, hot for ranges of temperatures and then you could train any classifier.

However, I'm not sure that would be worthwhile considering your problem is fairly easy and can be solved by a simple if statement as you said.",1518057422.0
Chitoyo,Check fuzzy logic or fuzzy neural networks. It allows to implement this kind of rules,1518093480.0
nickbuch,"Are you using OpenCV to isolate the phone from each of these images?  Many of these images are 50% background/lighting, which is a bad input for the CNN.  Should be a straight forward task for a CNN with well-formatted inputs, considering your data is so well-labeled.",1518067312.0
nomology,"Some of these 'broken' screens are super subtle damage, so very hard to detect. Try to first classify heavily broken screens vs. non-broken, and work your way up.",1518048151.0
wdroz,"For handle the bias toward one class, you can try to tune the ""class_weight"" parameter to penalize more the other class.",1518091571.0
vkr_tj27,You can check in Faster RCNN for sequence modeling and train it on activity recognition dataset.,1518011106.0
datavizu,"this library does video recognition: https://github.com/ageitgey/face_recognition

You can see their code and understand",1518089322.0
blackHoleDetector,"Regularization, in general, helps to reduce overfitting. The first key to reduce overfitting is to add more data (if possible). In your scenario, you've basically already done that, given all the data you have. As a next step, if your model appears to still be overfitting, then maybe introduce regularization. It's kind of a test-and-see situation. Try training and validating without regularization, and see if your model is overfitting. If it is, add regularization and see how the model performs with it. If it's not, then there's no need to add regularization,",1517980341.0
not_so_tufte,"Can't answer the question directly, but just want to point out that there is much more going on in the human brain than simply neuronal ""connections"". The brain has multiple sub-systems which interact to prime and inhibit impulses (e.g. attention), hundreds of different neurotransmitters that can modulate information transfer at a given connection, and it is all nested within a biological system that's susceptible to changes in hormone levels and the presence of nutrients.

So even if our computational power is increasing, we are still more complicated than our silicon counterparts  -- although that isn't necessarily better!",1517943342.0
keegan118,"Current neural networks have roughly 1-10 million “neurons”, roughly somewhere in-between the amount a bee and a frog have.
Since 1985 networks have doubled in size every 2.4 years. If this growth continues it is estimated that these networks will have the same amount as humans in 2056.
Source: Currently taking Deep Learning ",1517963538.0
Jrowe47,"It's closer to 10,000 synapses per neuron.

The closest algorithmic model of biological neurons is Numenta's Nupic/HTM implementation. They simulate hundreds / thousands of neurons with 2048 synapses apiece, usually, running on high end desktops. Representing biologically plausible models is resource intensive. There's enough computational capacity to model a human brain with the latest supercomputers, if you had a good blueprint for the sensorimotor / cognitive hierarchy.

https://en.m.wikipedia.org/wiki/Numenta

www.cortical.io - practical commercial products based on the tech

It's also worth thinking about edge cases - people with water on the brain, lacking a cerebellum, having a tiny cranium, missing a lobe, etc. If you approximate a lower limit for neural quantity, and synaptic density, it's probably possible to simulate the equivalent of a human level intelligence with 300-500 synapses per neuron, with 700,000 neurons - the synapses would have to be connected in just the right way.",1517967555.0
cameldrv,"As not_so_tufte says, a human neuron is a lot more complex than an artificial neuron.  Roughly speaking though, you could say that a modern image recognition neural net has on the order of 50 million neurons and 20 billion connections.",1517949137.0
initz3r0,"How much expierience do you have? I suggest you start looking at convolutional neural networks. Siraj raval on youtube as great tutorials and also publishes his work using jupyter notebooks. Have a look at keras and tensorflow if you're not familiar with those. There is a lot of existing work, study that and then try your own implementation.",1517930324.0
nishank974,If you only know neural nets then I’d suggest you to start with cs231n course ,1517926616.0
magicfoxs,"Publicly there is no such thing as a junior deep learning position, but there are junior data science jobs. ",1517873974.0
snes-jkr,Try looking for internships or data science jobs. Do you have any other qualifications?,1517871403.0
tomrearick,"Omar,
Congratulations on completing your Nanodegree. You are now qualified for a Nanojob. Do you realize that you are competing in the marketplace for real jobs with people with real 4-5 year degrees from real universities?",1517875407.0
rootcage,"Data Science or ""Deep Learning"" in your case is a mixture of programming, mathematics and computer science. You want to be proficient in all facets to make yourself a solid ""scientist"".",1518047393.0
BruinBoy815,"Hi Omar, firstly I am greatly interested in your opinion on thoughts on the nanodegree program. If possible, please message me about it. As for advice that might help. I would say put up some of your projects iron github and showcase your portfolio. With that being said there are not that many deep learning positions open you might want to focus more efforts on data scientists as I know those positions place heavy emphasis on machine learning now a days which gives you a good edge. Next, after applying around and posting your portfolio just keep participating in meetups and get to know more people in field. Don’t get discouraged by others saying you need to graduate from a top university honestly employers don’t give a shit. If you can get job done, which you’ve proven, you will stand a good chance. 

",1518342340.0
McNoobertron,"One issue with different sized input is effect of transfer learning - are you using the same procedure in both cases? Additionally, when I have tried using transfer learning in 227x227 network I have noticed significantly worse accuracy in PyTorch (was trying to use weights with network which has 224x224 image size)",1517839087.0
matej1408,"You probably meant that network is overfitting 224 images. You can always try to resize 512 to 224 and then train the network. It is also possible that dataset with 512 images is harder to learn, but maybe it is better for generalisation. You really should monitor cross-validation error to determine on which dataset network perform better.",1517825036.0
Klhnikov,"Not an expert here but you should start by defining how many features you have to extract from the images, their types (categorical here I guess), and maybe in different game situations (secrets etc), that you have to detect too..., And then, a bit of data processing...

But you definitly dont need Deep learning to do what you planned for...

Imho, you underestimates DNN power here, I mean, why not using machine vision to detect cards and stuff, and use the DNN to guess next move ?

And yes your CNN will extract features from the frames you sent to it, light models can fit at 60fps...",1517791926.0
nnexx_,"TF is an industry standard and allows for much better customization. On the other end it is less « friendly », coding will be slower and will require more planning ",1517650894.0
thntk,TensorFlow Eager is official in [1.7](https://github.com/tensorflow/tensorflow/releases/tag/v1.7.0-rc1). Production code could use Eager now. Switch or not switch?...?,1522234252.0
rmacmaster,"cool, thanks for sharing.",1517619684.0
DemiourgosUA,Good one. Is there anything like this on momentum?,1517992682.0
artificial_intel423,"Just curious, why do you want to do this in H20? LSTM is not a parallel algorithm ",1517612146.0
,"IF it’s always wrong, maybe you swapped the labels somewhere. What loss function did you use?",1517440669.0
mdfeist,"I would recommend the second one with the 1080 ti. Yes, RAM can be important; however, most deep learning applications are GPU optimized and almost all the heavy processing is done on the GPU. Also 16 GB of RAM should be more than enough. ",1517448601.0
mtolmacs,"The title is click bait. GDPR won't make ""AI illegal"", it will only require extra documentation on the source of training data, explanation of the algorithms applied and the impact of automated decision making, *when* these tools are making decisions affecting data subjects. Which is just a subset of ""AI"". 

Even if it does affect the data subject, it doesn't mean the provider of the automated decision making service *have* to explain how the algorithm reached a decision.  The idea in GDPR's right to explanation is to offer enough information to the data subject to make an informed decision whether to opt out of the automated decision making process.

[Better article on the subject ](https://iapp.org/news/a/is-there-a-right-to-explanation-for-machine-learning-in-the-gdpr/) 

",1517383368.0
Rezo-Acken,Its a sad day in data science when we start to have clickbait articles similar to celebrities press.,1517404833.0
waxymcrivers,I think all but the last seem fair.,1517381976.0
jghaines,"Yeah, I’ll get right on that....",1517305392.0
kailashahirwar12,Sure,1517329547.0
Escorpioneer,Amazing,1517499654.0
immortal333,is it really free? or you have to compromise your code?,1517247466.0
sansprenom,May I say that as an AI student I can’t help myself but being in love with google?,1517263649.0
sanemate,Damn. I just built a 1080ti based system :/,1517285968.0
rraallvv,Has anyone been able to install Cuda Toolkit on Google Colab notebooks?,1519658060.0
RedEyed__,"At my opinion the deeplearning.ai by Andrew Ng is the best choice.
It's available on Coursera.
In the hometask you should implement convolutional network w/o any frameworks.
",1517224887.0
spurra,I recommend http://cs231n.github.io/,1517244579.0
matej1408,"There are videos from Stanford CS231n lectures on youtube. It starts from basics concepts of CNN and goes to the more advanced topics by the end (GAN, fooling NN, style transfer). Basic intuition behind Inception, YOLO, AlexNet and GoogLe net is also explained. I have found these lectures very helpful for understanding CNN's.",1517827829.0
party-horse,"Being in a similar situation some time ago (trying to build ML models from scratch) I have devised a pure python ML library for computational graphs: [graphAttack](https://github.com/jgolebiowski/graphAttack).

Have a look, maybe you can re-use some of the code snippets. I would especially recommend [convolutionOperation.py](https://github.com/jgolebiowski/graphAttack/blob/master/graphAttack/operations/convolutionOperation.py)
for 2d convolution and max-pooling as well as [controlCNN.py](https://github.com/jgolebiowski/graphAttack/blob/master/controlCNN.py) for building a CNN.

Sorry for the self-promotion but I have figured it might be of some use to you. Having a simple and understandable library for people to tinker with and get their hands dirty was why I wrote this. Let me know if you have any questions.",1517246592.0
omalleyt12,I really enjoyed http://neuralnetworksanddeeplearning.com/,1517361547.0
apockill,"Hey, here's a state of the art action recognition network: https://github.com/yjxiong/temporal-segment-networks

Disclaimer: it may not be state of the art anymore, it was when I read the paper last. I recommend reading this paper and getting caught up on the state of action recognition before getting too deep in the project.",1517158326.0
fgadaleta,"What exactly do you mean by ""my dataset is terrible""?
Are you generating data yourself? If so, I am not affiliated whatsoever but have you tried tools like [mockaroo](https://www.mockaroo.com/)? 

If not, converting to csv with a separator and reading from ```pandas.read_csv``` is usually a quick&dirty solution 
If that's the case, I believe you are dealing with dataframes, not datasets ;) 
",1517177161.0
sirelkir,"I think it's the first option: no one has something like this working already. 

For XAI to work the neural nets of it need to be several orders of magnitude more advanced than the neural net you apply it on. And given current state of AI the stuff it could oversee is really basic. 

Also training am AI like this would need a massive library of what is a good and useful explanation and what is not. Which is really hard to put together even for a human",1517100702.0
kailashahirwar12,"I think it is very important to go in the direction of XAI because current deep neural networks are not able to describe how they make decisions and what features/properties they have considered to make that prediction. 

In my opinion a neural network should be aware of it's surrounding(other neural networks) and how & why it is making a particular decision.

They should be able to explain their decision process and the features. ",1517245527.0
tkchris93,"I've used a 1060 3gb and a 1070 8gb on two different desktops at work so I can help with a direct comparison. Overall runtime for the CNNs I've been running is about 20-30% faster on the 1070. I tried to test a CNN with 30m parameters on the 1060 this week and I kept running into ""resources exhausted"" errors. Yes the price of gpus is absolutely ridiculous right now, but I've still been seeing deals every now and then on laptops and desktops that would make fine personal deep learning machines. Good luck!",1517048634.0
LoveOfProfit,"GPU prices are currently pants-on-head retarded due to the mining craze. You'd probably be better off using cloud compute while you wait for prices to chill out.

FWIW I bought a 1070 a year and a half ago for $380.",1517009363.0
snes-jkr,"Go for the 1060 6G

In the end the G count more, and 6G should already enable you many things. The speed is not too much different and if you have to wait 20h or 18h for your results will not make a difference.

On the long run there'll be new graphics cards and they'll outperform both by margins anyway.",1517045168.0
nishank974,"Try this

https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d",1517234444.0
nishank974,"I suggest you not to buy any laptop. I setup my own mother board with 1070ti. Becz some day new gpus will come into the market and in you cannot replace the old one on a laptop. If new ones enter the market, the prices of older gpus will drop then u can buy that and connect with ur gpu using SLI. I know building fr scratch will take time. But thats the cheapest and best solution.
",1517048157.0
Momer,"Mods, can we disallow job postings from third parties?",1516968261.0
LoveOfProfit,You should try posting this in /r/mljobs instead.,1516972490.0
jbmt19937,"https://www.nature.com/articles/s41467-017-02597-8
Cooperating with machines | Nature Communications

You might find the this recent paper interesting",1516935891.0
BruinBoy815,Wow! This is amazing. Why is this not further up? ,1518342415.0
DemiourgosD,"Instead of posting a screenshot, just copy the code here to make it much readable for us. Also, what are you trying to classify? What framework are you using?

If you are classifying images, you can always improve your accuracy by reusing an existing model trained on millions of examples. This approach is called Transfer Learning, here is a good starting point: https://towardsdatascience.com/transfer-learning-using-keras-d804b2e04ef8",1516818420.0
InterstellarRun,At the very least you might want to add 2 more convolutional layers and a couple fully connected layers as well. 1 of each isn't enough to get very high accuracy.,1516845040.0
lushdogg,Python is free. ,1516891420.0
vivekchoksi,Why are you using two different generators in the code example? Shouldn't I use the label mapping from the same generator I use for prediction?,1516812363.0
_ZombieSteveJobs_,"Choose an advisor that aligns with your needs. If you're completely self motivated and focused, choose an advisor that is more hands off. If you benefit more from guidance and direction, choose an advisor that is more hands on. ",1516802529.0
firstmonkey,"I am not able to give you a full answer for all your questions, but half an answer is better than no answer I guess.
First of all, Deep Learning is a subfield of machine learning, so I would suggest getting into that first as an introduction on what deep learning is trying to solve. For that there is a course on coursera which is supposedly very good for that by Andrew Ng https://www.coursera.org/learn/machine-learning

For your concerns regarding your programming skills, it seems like you're good with what you have: deep learning is mostly python! You will use a framework for the actual computing such as TensorFlow (most well known since it is used by google) or thenano. For this you don't need a deep knowledge of the language, since DL is a lot about the modelling of the network, which is (essentially) just using different primitives of the framework (which you will learn as you learn about DL). Ive been taking DL classes and other ML classes and have never used a low level programming language such as c since all the computationally heavy lifting is done by the framework anyways.

There is several problems with DL which are worth investigating such as tractability (how did the neural network come to its conclusion), which can be solved with novel approaches (such as sum-product-networks for example; research in this field is done by Pedro Domingos, University of Washington https://homes.cs.washington.edu/~pedrod/).

About the rest I cannot really give you advice on jobs etc. Good luck!

EDIT: good introduction video about machine learning by Pedro Domingos: https://www.youtube.com/watch?v=qIZ5PXLVZfo&t=1s",1516802714.0
nishank974,"Yes you can. I did it with my Samsung laptop. I attached 1070ti graphic card externally but I had to remove the wifi chip attached to pci-express . The latest upgrade what new laptops are having is that they come with usb type-C port which is connected directly to the pci express. So no need to open up the laptop. 

Check whether your laptop has any of these configurations. Hope this helps.",1516793829.0
veltrop,"Looks generally ok to just get new GPU, but first verify that your PSU has sufficient wattage, you might need to upgrade that.",1516798530.0
matej1408,"I am assuming that you have labels for anomalies part of image ie. where is it on image. Than you can use some kind of architecture that is used to find objects on image (Faster CNN, YOLO).",1516796375.0
_ZombieSteveJobs_,"Have you tried using ImagNet pretrained weights as initialization to the models in keras.applications, and then refine the training on your data/classes? It might decrease your training time. You'll probably need to add regularization as well.",1516804193.0
,"Yes, do it. It would be awesome to have auto refs for pick up games. Plus there’s tonnes of training footage. Let’s start proof of concept with offside in soccer.",1516795390.0
timsl,"Check this out, focal loss, improves one-stage detectors (ssd, yolo): https://arxiv.org/abs/1708.02002

And this paper that compares a few different detectors by speed/accuracy with different feature extractors: https://arxiv.org/abs/1611.10012",1516734092.0
_ZombieSteveJobs_,Did they change the courses?,1516714402.0
emrecolako,"They added a preview here, if you're still interested: https://classroom.udacity.com/courses/nd101-preview",1516815165.0
mhex,"don't rescale, try 300x300 random crops instead",1516654416.0
_ZombieSteveJobs_,"I use scikit-image to partition an image into blocks of the desired size: http://scikit-image.org/docs/dev/api/skimage.util.html#skimage.util.view_as_blocks
",1516667627.0
marrabld,You could set a stride length in you convolution layers.,1516702906.0
DemiourgosD,"Try emailing the authors directly? Maybe they will be ready to share it with you if your purpose is to improve the state-of-the-art. Also, post it here, if you happen to find anything. ",1516554664.0
nnexx_,"Hey, it would be great to credit Andrew NG and the coursera deep learning specialization for the dataset and the vast majority of the code.

Or at least shuffle the paragraphs a bit",1517130005.0
DemiourgosD,"Can you explain what ""Wikipedize"" mean? 
""to provide an article and get tags containing articles's topic"", how do you ""provide"" an article???

Anyway, for now, if you need keywords extraction, just use textacy: https://textacy.readthedocs.io/en/stable/
This is so far the best implementation of the TextRank algorithm. There is also one in gensim, https://radimrehurek.com/gensim/summarization/keywords.html, but in my experience it is no so good as the textacy one. You don't need to train it, it just works out of the box.",1516393773.0
range_et,"We had a similar discussion here , check it out :
https://www.reddit.com/r/deeplearning/comments/7mohl7/question_deep_learning_weather_data/
",1516405417.0
JustinQueeber,"There are an infinite number of different cost functions. All a cost function does is give a single valued representation of how far off the network's predictions are to the supervised labels.

What do you mean by ""solving various cost function""? 

Again, any cost function just outputs a single value, and so the choice of cost function has no effect on the activation function.",1516359769.0
ragas_,Have you tried object detection model such as yolo or google object detection algorithm.?,1516303034.0
Pik000,"Id look at using a CNN, google keras which is a high level layer for tensotflow.

It has alot of preprocessing options as well which allows you to rotate, change contrast, cut part of the picture out, etc which allows you to add alot of extra pictures.

Let me know how you go.",1516318271.0
tiger287,"Okay ! So basically correct me if I am wrong your algorithm is cropping the image out right ? So all you need is recognition. I would suggest search for transfer Learning using keras. That would give you a quick and easy solution. There are many deep neural networks trained and you will get the weights file which you can pre load, all you will have to do is create your image dataset with at least 50-100 pictures of a similar sign from say different angles and pass it to the model , customize it a bit (which you will learn how to using transfer Learning) and you can get yourself a model for recognition. Just remember more the images, better your model will be . Cheers !",1516366572.0
legendaryvirus,This is a really good question. I also have some problems with my Robot project. Hope for the answer too.,1516294276.0
spurra,Maybe be more specific with what you mean by marked voice data,1516277563.0
eronturon_1995,Would you online training?? ,1516576703.0
vsakthiv,+1 Even I am looking for the way to do this on our own images,1517575952.0
serael,Your links 404,1515998495.0
Everfast,"well it depends on your architecture and your goal. I would recommend to look up papers on scholar with a similar goal and start by copying this architecture, for example [U-net](https://pdfs.semanticscholar.org/0704/5f87709d0b7b998794e9fa912c0aba912281.pdf) if you want a high resolution classification/segmentation.

",1515938083.0
Rezo-Acken,"Guessing and trial and error is pretty much whats its about. The only principle is that you increase the number of channels as you reduce the width and heigth after pooling.

Also use relus (or variants). Besudes that there is no rule for the number of layers. Just add some and see if loss decreases. In deep learning its all about analysing loss and valudation. To decrease bias you need a bigger network so stack layers and to decrease variance you need regularization.

Also like Everfast said. If you find a similar problem start by copying theirr network. It will give you a good starting point.",1515945000.0
Rezo-Acken,"Softmax needs to be fed logits. A logit is just in general the last linear matrix calculation of a classifier. The weights and bias asked are the parameters necessary for calculating a logit befre the softmax function and cost calculation.

If you go to tensorflow website I think its pretty clear what you have to do from their example of the difference between train and predict. If you have A the before last layer activation output you usually then do an additional AW+b then softmax and if training then calculate loss. Well in that case your function gets the weights as W and bias as b for training and not encoded Y it seems.  That coupl be fun to verify but what this means is that your function receives the full weights and biases but then use a sampling strategy to use and optimize these weights and biases. But the key poont is that these are not new weights.They are your usual num class sized parameters.

Then for prediction that weight and bias are now optimized and you use them as usual for calculating first the logits as AW+b and then applying softmax and argmax on it.

Hope this helps.",1515821317.0
deepneuralnetwork,Can you share more details on how your original network is structured / how you’re training it? Initial gut feel is something is wrong in how you’re training - that doesn’t seem like either enough data or high enough network complexity to result in 24hr training times on a GPU.,1515851557.0
Rezo-Acken,"I really like online courses to get started on a subject. It never makes you an expert but it helps you to start your own project.I would do in that order:

1. Python basics with 6.00.1x at EDX. You cant do anything without programming in this field.

2. Linear algebra and calculus. Like first course. Mostly understand derivatives and matrix multiplications. You should know easily what matrixes and vectors are.

3. Machine learning basics and in particular regression and classification

4. Neural networks and deep learning. The DL specialization on coursera is pretty good introduction.

All this will take you quite some time already. During or after force yourself to do simple projects with your own or kaggle s datasets. I would explore more advanced stuff only when you are comfortable with the subject you just learned. Every time you finish a subject train yourself first then go to next. For example after programming in Python make a rudementary tic toe game. After Algebra make sure you can resolve provlems. After regression start an easy project with it. After deep learning start with a shallow network etc.

The first thing you will learn in programming is that complicated tasks should be resolved by small easy bricks. Apply that to learn a field :)",1515797039.0
astrowhiz,"From what I've read I think finance is one of the most impacted and successful, with the potential to be predominantly AI driven.

There's always a massive amount of numerical data that's often in a conducive format for deep learning.


In the future AI is going to impact most industries I think. I'd add security to the industries impacted/potentially impacted.",1515777507.0
immortal333,every industry can use artificial intelligence in some way and boost productivity which can result into lesser job in that industry.,1515773678.0
bminixhofer,"You're looking for *object detection*. There is a tradeoff between speed and accuracy here and multiple models depending on what you need.

[Faster R-CNN](https://github.com/yhenon/keras-frcnn) is the most accurate model i know of but it runs at only about 3 frames / second.

[YOLOv2](https://github.com/experiencor/basic-yolo-keras) is a faster model but comes at a slightly lower accuracy. It runs at over 30 fps.",1515760803.0
spurra,Semantic image segmentation? Check slide 5 of this: http://www.cs.toronto.edu/~tingwuwang/semantic_segmentation.pdf,1515760152.0
TopcatTomki,"Perhaps Attention during an image captioning task, but that seems quite heavy handed.",1515755749.0
vrishabc,"Look into the examples/documentation for google's mobilenets. If you include training data with xml coordinates for bounding boxes, the cnn output predictions will be bounded.",1515792808.0
Rezo-Acken,Look into Yolo algorithm. Must be existing keras version of its implementation. Also go to Coursera and audit for free week 3 of the concolutional NN course.,1515796067.0
blackHoleDetector,"jpg is fine. If you use the ImageDataGenerator's flow from directory function, then _how_ the images are actually organized/stored on disk makes a difference-- in terms of the directory structure and the title of the directories. 

If you're not already familiar, I have a video showing how this image data preparation and organization is done [here](https://youtu.be/LhEMXbjGV_4).",1515714897.0
pronobozo,"png, jpg, bmp should be fine. With the flow from directory argument you can reshape the target size.

",1515708725.0
atulshanbhag,"Store images as jpg files on your system. Its okay if you run a preprocess first to resize images to a smaller size as per your requirement and store these images while deleting the others. This will save some space. Then depending upon your RAM requirements and time of training you prefer, select a batch size and load images in batches to train your model only for those images in a batch. So your memory only keeps a batch of image at a time. Use flow_from_directory if using Keras or write a generator which does this.",1515713638.0
Nahid59,"I guess 80% of the neural network process is matrix algebra (transpose, dot product, etc..) so basically all the concepts of matrix algebra",1515693726.0
mdfeist,I find that most of the math is pretty basic. It mostly draws from first year University calculus and linear algebra courses. The hardest part is understanding how all the math is put together to build a network. Then learning what works and doesn't work for different types of data. The best is to watch a couple YouTube videos on how a neural network works and then go from there.,1515698911.0
Pimozv,"Have a look at the ""linear algebra"" section of the deep learning MIT book TOC :

http://www.deeplearningbook.org/contents/TOC.html",1515703646.0
gregw134,"If you're looking for an engineer's guide to doing deep learning in practice, take the fast.ai course. It doesn't require any linear algebra.",1515700893.0
CaseOfInsanity,"Also need to learn math vectorization when programming. In machine learning codes, optimisation involves refactoring for loops into math functions which calculate the same thing but in a different way, I'm currently studying it now and as someone familiar with matrix algebra, I'm having a hard time with intuitively converting for loops into vectorized functions.",1515723437.0
Rezo-Acken,"The only difficult part is coming up yourself with formulas for gradient backpropagation. Its really not intuitive to make derivatives in matrix form. 

But you can skip it and do deep learning none the less. The frameworks usually take care of the backprop for you so you could make many conventiinal models without having the slightest clue what the exact calculations behind the scenes are.

Anyway you need basic libear algebra and an understanding of derivatives beside that in order to understand gradient descent.",1515739164.0
Rezo-Acken,Interesting will see when I have time to try it the example optimizers myself.,1515678891.0
complectere,"I had done some research over which model could be implemented, as a conclusion encoder-decoder LSTM with attention is most plausible I guess. Feel free to contact to the above email. thx
",1515955410.0
atulshanbhag,"Oh yes they can! General Adversial Networks (GANs) do the role of generating ""new"" samples given some existing samples by learning the distribution of those samples. 

[Here]( https://www.theverge.com/2017/10/30/16569402/ai-generate-fake-faces-celebs-nvidia-gan) is a very good example of stuff such networks can do. Quite amazing!",1515639774.0
k9thedog,"See here for a good overview and comparison of CNN models:
https://www.topbots.com/a-brief-history-of-neural-network-architectures/",1515648741.0
atulshanbhag,"VGG aka Visual Geometry Group is a research group based in Oxford, that released their model called VGG16 and VGG19 trained on ImageNet database. The difference between those 2 is that VGG16 uses 16 weighted layers whereas VGG19 uses 19 such layers and is deeper.",1515631803.0
Belenoi,You should ask this on /r/learnmachinelearning,1515658938.0
atulshanbhag,"They may not have specific names for different types but there definately are a variety of architectures used in CNNs based on the dataset. Mostly, such architectures have fancy names which suffix ""net"" to it, but they do not imply a different type of CNN always, rather its a different architecture applied to a different problem most of the time. The names are often given after the problem being solved, the group of researchers behind the architecture and the paper.",1515632120.0
Rezo-Acken,CNN just means the network uses convolutional layers. Considering that everything else can be different from a CNN to another its like asking whether there is only one type of dog :),1515679900.0
BeatLeJuce,"Suggesting Theano in 2018 (a lib that has officially been deprecated by it's developers, who suggest you switch to something else) and doesn't have Pytorch are no-goes. ",1515590634.0
Ader_anhilator,Terrible article,1515591505.0
Belenoi,No mention of MXnet or Caffe2 ? This seems a bit outdated...,1515610416.0
s1mmerz,"No PyTorch, MXnet/Gluon, Caffe2?",1515959389.0
,[deleted],1515590964.0
Benjamin_DL,"Also, the iOS version is coming out next week! Apologies for the wait.",1515650385.0
Yomniac,"Checkout my repo [github repo](https://github.com/jarpit96/image_colourization_tf) 
It’s written using tensorflow
You can use this or refer to one by nilboy on github
As for dataset u can take from imagenet website
For research u can refer to Zhang or Larsson paper. We used Zhang approach which can be witnessed at his website algorithmia.",1515559782.0
bar_n,"Hi and thank you very much for your answer! :)
there is something unclear when I try to use your solution - 
there is a path which looking like a hard-coded path, or it could be defined in my computer - the use of ""ILSVRCData/input-apple.txt"".
is that file of yours or it have to be saved in my computer ?

thanks again!",1515962410.0
LordKlevin,[pytables](http://www.pytables.org) is a very nice library for creating and manipulating h5 files. [vitables](http://vitables.org) is a decent gui built on top. ,1515513625.0
moazim1993,I printed every chapter in the part 2 of the book. It’s pretty nicely explained for the denseness of the material.,1515513930.0
astrowhiz,"Thanks for the link. I was thinking of buying this book, had no idea it was available online.",1515515281.0
CleverLime,Is it any good?,1515419624.0
mattowy,"Like people said before, 350 categories is truly enough. Try using the already pre-trained networks for those 1000+ classes and fine-tune the last, lets say, 2 layers + add the logistic regression on top of it. 

It's just an idea but since we have categories we can try to enforce the correctness by creating the output of  a shape (350, 2) where we store the category and the domain. We'll only take the category of the highest probability and then seek for the domain there. It's just an idea. Try and let us know about the results!",1515420378.0
blackHoleDetector,"I think you could train a single model to be able to classify all 350 domains. Then, depending on the output of each input, you could logically label it with the higher-level domain. So, for example, if your model classified an image of a shirt as ""shirt,"" then you could write code that understands that ""shirt"" is a sub-domain of ""clothing,"" and label the high-level domain as such.",1515262536.0
matej1408,On ImageNet they use one model to recognise over 1009+ categories I think that you can handle 350 categorise with one model also.,1515263189.0
Asperico,"I think it's a lot better to have one single net because learning one domain helps in learning in a different domain. This way you need a lot less images in each domain, but of course if you have lot of different pics and time you can try to train 20 nets and another net that chooses the right category.",1515267553.0
atulshanbhag,"I would train a deep CNN on all 20 categories, one main reason being I could use this model as feature extractor for the binary models and compare results. Doing so will save a lot of time in the second part, for binary classifiers over all 20 categories. The otherway round, it makes no sense to overlap 20 models to make a single model which categorizes 20 classes, since the training time is going to be quite a lot even while combining, in addition to the earlier 20 models.",1515321842.0
ragas_,Deep learning is a vast area. But since you are working on vision field stanford's computer vision course is good starting point for it. It step by step introduced you to deep learning and finally to computer vision. The exercises are good way to test your understanding. ,1515047310.0
jfrmqOX,Better solution: be sure your dataset is in the ssd when performing the training. don’t install windows at all,1515000276.0
Asperico,"You can get a ryzen 7 for the same price with 2 more phisical cores because you don't need mkl, and add more ssd space for the dataset. Or if you want, put the dataset in raid 0 configuration or, if it's small, buy lot of ram and keep it all in memory. Linux caches a lot of data in memory by default or you can use tmpfs to create a folder located in ram.
One note on the gpu: if you can afford the price, look at the new titan v.",1515268163.0
bigslimvdub,"Why not use a Vega frontier, that basically outperforms any Nvidia card for 1/2 or 1/4th the price? Is it because poor linux drivers?",1515355380.0
joelthelion,It seems like the author is primarily interested in promoting his platform. The problems he describes are extremely widespread in the academic world and not necessarily a sign that something is wrong with this specific community. ,1515002812.0
webauteur,"You should probably ask this at /r/LanguageTechnology/

",1514924376.0
,"Well.. an ontology is a structured set of relationships between things.  So with NLP you would have to parse the text, do object/noun recognition and then infer some relationship based on the verbs as well as properties of things which may be things as well.  In a deep learning world you would need annotated examples that go from some text input to a tagged ontology. Word2Vec will probably be involved in some way.  You'd probably use a LTSM or a CNN or RNN of some sort. 

So there you go, three things to go research and figure out how to combine to do what you want. Good luck! 

Also see:  https://databricks.com/session/semantic-natural-language-understanding-with-machine-learned-annotators-and-deep-learned-ontologies-at-scale",1514942904.0
webauteur,"I recommend the book [Doing Math with Python](https://www.nostarch.com/doingmathwithpython). This book shows you how to do various mathematical operations through code. But it does not teach you mathematics or how to code, it is just a bridge between the two. The math is really basic algebra, statistics, calculus, and graphing so it is not tailored towards deep learning, but it is easier than the advanced math. ",1514924195.0
Starwhisperer,"I definitely would not recommend jumping into deep learning without a solid foundation on fundamental mathematics and statistics. Even those with math degrees or other STEM degrees have a tough time in learning and retaining knowledge in artificial intelligence. There's just so much to learn.  
  
With that being said, it can be done and it's fun to learn all the fundamentals. It shouldn't be skipped. I would say read an intro to algorithms/data structures book, this will help your computer science skills and understanding of time complexities, oop, etc... Then, look into understanding basic probability/statistics. So an intro to probability/ stochastic processes and applied statistics will be useful. Understand expectations, variances, MLEs, Bayes, all that jazz. Also, look into basic linear algebra and different decompositions and why they are useful. Then, like you wrote, you can jump into more python/r coding using a machine learning class (there's plenty of books and moocs available). Then from there, I would venture onto deep learning and reinforcement learning. But, jumping into deep learning at first, while, you may be able to implement the code, you won't be able to actually understand the process or the steps behind it. ",1514864232.0
delivaldez,"I definitely suggest Andrew Ng’s deep learning course (coursera) and Fast.ai. 
Andrew Ng simplifies mathematical operations as much as possible and helps you through with explicit calculations. If you want to learn without going through lots of prior mathematics and start building basic models. 
Andrew’s approach is bottom up. 
Fast.ai is much easy on mathematics and very easy to follow. You start building immediately and learn top down.
Recently I have been going through François Chollet’s Deep Learning with Keras book and it goes hand in hand with Fast.ai",1515318718.0
marrabld,Bag of words doesn't mean literally words. Its features (eg SIFT) that have been extracted from the images.  Google bag of words and sift. On my phone so I'm being brief.,1514705265.0
tkchris93,"Yep. And the resulting vector won't exactly equal queen, but if you look at the vectors close by, queen should be the closest.",1514592488.0
Imstarboy,"The first technique seems like you are doing mean and not sum of the probability. If you follow this approach it sounds like ensemble technique. You are combining performance of all your models and generating accuracy. 

Second technique focuses on finding median of the accuracy of 5 models which gives you an idea of how models perform on given data and your hypothesis assumptions ( like if your feature effect on target variable).  This technique sounds like what your question is about. 
",1514577938.0
vkr_tj27,"As mentioned before, you can use Tensorflow with Python instead of Matlab, as it will have more support from the deep learning community and it is open source.
Go for Ubuntu 14 or higher, Tensorflow-gpu environment is easy to build in Linux than in Windows.",1514561487.0
marrabld,"Make sure you have enough pci express lanes on your MB. And take note which slot they're assigned to. For example, an older board I had, had 4 slots. The firs two had 16x pcie lanes, and the last two had 8.  So to make sure I had full bandwidth I had to put both cards in slot 1 & 2. That put the cards right on top of each other and one card ran quite a bit hotter than the other. But I never had to make any extra cooling modification.   If you get a MB that has slots with separated 16x  slots then you can leave a gap between cards. Cooling will be better. I think most modern well known boards should be fine now days.

Make sure you get a large enough power supply. Those cards will require a bit more power to run. I'm running corsair 1200watts but that is a bit over kill. I would guess that you need about 800 watt supply. But check that yourself to make sure. 

Computers are pretty idiot proof to put together. Linus Tech Tips do a 'how to build a PC'  on YouTube. I would follow along with that.   Google it. I'm on my phone.

Use Keras (python) with tensorflow backed. Python is Matlab-esc but free from license restrictions. But that is just my opinion. Use what ever you need to get you papers written.

Good luck.",1514513277.0
carlthome,"Applying deep learning to weather prediction is not a novel idea, just check e.g. Google Scholar. This is a good start: https://arxiv.org/abs/1506.04214",1514511167.0
ShibeForceOne,"I have asked myself a similar question, but with stock prices.
Since this isn't being used, I'm guessing (at least for stock trading) it doesn't work",1514488192.0
harrybhines,Won’t work. Too randomized,1514495407.0
Xavier_ORourke,"That's cool, thanks man :)",1514514109.0
snes-jkr,Maybe because w and h are more meaningful for scales of objects and the center gives a better representation of the location. In the end it should not matter as it is the same data just in a different format. You could just add another tiny layer on the finished model and train it on the subtraction task.,1514450886.0
ashwinraju101,"We want to make sure that xmax > xmin and ymax> ymin. By calculating xcenter, ycenter, w,h it is easy to obey that condition. ",1529381450.0
barbek,"There are usually two types of transfer learning:
1. When you freeze first 80-90% of the network layers and train only last layers.
2. When you just use pre-trained weights as your starting point and train full network without freezing layers.

In this context, please elaborate more: what is confusing and where did you see training data size mentions?",1514406786.0
snes-jkr,"Nice article. I agree that for highly temporal data such as Sound or EEG we might need to think differently than for classical vision. Much research still needs to be done in this area, but it's far less intuitive than with vision.

Btw Google actually created a workaround for WaveNet to sample in real time https://deepmind.com/blog/wavenet-launches-google-assistant/",1514451803.0
barbek,"I don't have an access to the paper you are referring to, but usually: Hand-crafted means that someone used hard-coded rules in order to get some kind of score/feature.
Deep feature means that we trained deep network to convergence and now those trained layers are having deep features.
Example:
I want to make an autonomous vehicle which will be able to navigate in the labyrinth. My hand-crafted feature might distance to the wall or my speed, measured by the hard-coded algorithm. 
I also could train some kind of CNN to get deep features, which will help me to notice other objects and etc.",1514365377.0
astrowhiz,"Not sure if you still have the problem.

I actually have both versions installed but am using ver8. Make sure the environment variable Path points to version 8 first (sysdm.cpl, advanced tab).

You have to restart then in a command prompt type nvcc -V and it will tell you what version is being used. I'm keeping 9 for when NN's start using more.",1514407080.0
Casteless,"""Asteroid Attack""",1514321322.0
,if i buy something i own this shit! does not matter if i am a datacenter or an individual! it ... is ... MINE. i can burn it. i can wipe my ass with it. i can even do deep learning with it! this must be against the law (germany). any suggestions?,1514534697.0
LoveOfProfit,Need AMD to come to the rescue with some good software to bring some competition.,1514130969.0
fuuuuut,Good news for blockchain projects Golem and Sonm !! Both i have a position in.,1514367200.0
l3fth4nd3r,Found this good article about the whole story [A “First Order” Rising? NVIDIA’s New Policy Limits GeForce Data Center Usage: Universities and Research Centers In A Pinch](https://wirelesswire.jp/2017/12/62708/) ,1514273764.0
RetardedChimpanzee,"I feel as if this could be a good thing. It would keep the price down for the general consumer market. Else I’d fear supply/demand could drive the GTX1180 to be a $1000+ card.  It sucks because their price to performance ratio is way higher, but consumers can’t afford the same as what corporations can. ",1514144162.0
emmanuel-monarc,When did they change that ?,1514122482.0
julvo,"1) If you have a finite set of possible words, you can make a dictionary and feed the index from the dictionary into the network (typically one-hot encoded).

2) Alternatively, you can feed character by character. Again, you would have a list of possible characters and feed the indices

There are also compromises between the above two approaches, such as using a dictionary of ngrams.

3) Semantic embeddings such as Glove or word2vec",1514118574.0
JustinQueeber,"What type of neural network? 

If you're using an RNN, my initial assumption would be to set the input length as the maximum length of your words, left or right pad any shorter words with 0, and then use masking to avoid the padded zeros being used in your loss calculations.",1514120987.0
FearlessAnt,Use embeddings,1514124101.0
CollectiveHoney,Explain like I’m a mom who doesn’t understand cell phones please.,1514040259.0
ncasas,"This does not answer you question, but I think it's relevant to your situation:
>A recent change in Nvidia driver's end-user license agreement forbids the use of GeForce cards (like our 1080Ti's) in datacenters (see section 2.1.3 of https://www.geforce.com/drivers/license/geforce).
This piece of news has received a strong backlash on technical internet forums. See hackernews and reddit threads:
- https://news.ycombinator.com/item?id=15983587
- https://redd.it/7lbt60
There seems to be a lot of confusion on how to interpret the new terms, (e.g. an explicit definition of ""datacenter"" is not provided in the EULA) and there has not yet been an official statement by Nvidia on the public backlash.",1513953067.0
chronosynclastic_inf,what exactly is your problem? What do you mean by a good working model?,1513872663.0
DiscoverAI,"Wait I made a youtube video on exactly this. Check it out at: https://www.youtube.com/watch?v=Lou6NocBuIM&t=4s, and if you enjoy the content, feel free to subscribe for more content! :)",1513873372.0
usualdev,"I don’t know the answer but your question is interesting. Can you please explain how many photos per person you have trained ? Do you have the camera on the same angle of training photos? What’s your recognition success rate? I am planning to use facenet  api and your answers would be useful.
",1513880241.0
thewhizz,"I have implemented my own version of facenet and am planning to use it for the same task. You should be able to train your own facenet model if you have access to a GPU or leverage the pretrained model referenced on the project's github wiki -- i'm sure you found the one by david sandberg (sp?). This model is useful because it will give you good embeddings that you can use to perform cosine similarity (dot product with the normed vectors). But you need something to feed into this model and you cannot just send whole images, only face chips. There are other neural nets out there that will pull out the face chip (maybe sandberg's repo has something) and you could also use dlib that has a straight-forward object. So your project would look something like this: 

camera feed -> sample frames -> extract face chip -> embed with facenet -> consine similarity -> what reference chip is this new chip closest too?

There is a lot of work bundled in that pipeline especially if you want to get deep into implementing your own face detector or neural net. Everything else is just networking. ",1513891399.0
RetardedChimpanzee,"* 4X NVIDIA Tesla® V100 16 GB/GPU
* 256 GB LRDIMM DDR4
* Data: 3 x 1.92 TB SSD RAID 0
* OS: 1 x 1.92 TB SSD
* Intel Xeon E5-2698 v4 2.2 GHz 20-Core
* 2X 10 GbE
* Liquid Cooling
* 1500W Power Supply

By quick math, that's $35,000 of GPU, $3,500 CPU, $2,700 of SSD, and $4,000 of RAM. Which comes out to be $45,200 before the MOBO, PSU, Case, or cooling. So actually not a bad price at all. 
",1513795778.0
ak00n,Also customer support which is backed by a real dat scientist. Don't forget to call PureStorage and pick up a flashblade to while your at it.,1513834986.0
zionsrogue,You can turn pretty much any model in Keras into a multi-GPU model. I have a writeup demonstrating how this is done [here](https://www.pyimagesearch.com/2017/10/30/how-to-multi-gpu-training-with-keras-python-and-deep-learning/). I hope that helps!,1513790976.0
carlthome,I think you have a bug somewhere because these cost functions are essentially the same (they just differ by a constant term).,1513810014.0
enderwagon,https://github.com/soumith/ganhacks,1513732468.0
Everfast,"Looking at figure 2 in the paper. The conv1 layer generates features. Thus 256 new 'images' are created. The PrimaryCaps uses a [9x9] patch for all features, as an input. (Thus [9x9]*256). And is placed on these feature maps with stride 2 (obtaining a total of 32 capsules?). 

This means that the primary capsule sees 256 times 81 Conv1 units whose receptive fields overlap with the location of the center of the capsule.

edit: nvm I think I am wrong, just leave it here as a reminder of my lack of knowledge. useful [link](https://www.youtube.com/watch?v=2Kawrd5szHE)",1513787097.0
,"It looks like it's fully connected at first and then drops out by the coupling coefficients during training:

"".. to ensure that the output of the capsule gets sent to an appropriate parent in the layer above. Initially, the output is routed to all possible parents but is scaled down by coupling coefficients that sum to 1. For each possible parent, the capsule computes a “prediction vector” by multiplying its own output by a weight matrix. If this prediction vector has a large scalar product with the output of a possible parent, there is top-down feedback which increases the coupling coefficient for that parent and decreasing it for other parents.""

The other piece of the puzzle is this:

"" Even though we are replacing the scalar-output feature detectors of CNNs with vector-output capsules and max-pooling with routing-by-agreement, we would still like to replicate learned knowledge across space. To achieve this, we make all but the last layer of capsules be convolutional. As with CNNs, we make higher-level capsules cover larger regions of the image. Unlike max-pooling however, we do not throw away information about the precise position of the entity within the region.""

So max pooling is a scalar output with no routing vs a capsule network which is a vector output with routing by agreement.",1514049224.0
MandirWahiBanaienge,"I started with Google's Udacity course and then did Stanford's CS231n. Along with the courses I tried applied tensorflow for some basic models with help from either github or through the tutorials present in official tf library. I am still learning but coding few simple CNNs helped me a lot. Play with the network structure, learning rate and activation functions.",1513695361.0
usualdev,"I started 6 months ago with coursera and didn’t finish it due to loads of theoretical lectures then joined data camp to learn python to understand the code more and finally came across http://www.fast.ai
As a developer I like to see the implementation first then deep dive into theory to understand more. Checkout fast.ai and then you’ll take it from there ",1513707895.0
HHG123456,Tensorflow Sentdex ,1513723854.0
deepkailin,"In fact,I’m a newer too.i have the same problem.",1513691972.0
blackHoleDetector,"This is referring to the activation function specified for the layer. The reason for the plural *activations*, rather than the singular *activation*, is because when you specify an activation function for a layer, the function is applied to **each** neuron within that layer.

I specify the way this works in my video on [Activation Functions in a Neural Network explained](https://youtu.be/m0pIlLfpXWE), which is part of my larger [Deep Learning & Machine Learning playlist](https://www.youtube.com/playlist?list=PLZbbT5o_s2xq7LwI2y8_QtvuXZedL6tQU).",1513555500.0
EtaoinShrdluXxx,"It’s the activation function that calculates the output for each node from its inputs. Could be something like Relu, tanh or Sigmoid. ",1513545255.0
elephantail,Whats the name of the book? ,1513574775.0
elephantail,http://fredericgodin.com/papers/drelus-dual-rectified-preprint.pdf,1513575245.0
F4il3d,Very nice channel. I could not hit the subscribe button fast enough. ,1513526275.0
inbread,"Nice channel dude, the video on feature visualization was great",1513520754.0
wellshitiguessnot,I subscribed. Quality ML content on YouTube is hard to find.. mostly news stories or old news calling AI robots or terminators.. more intellect weighing in is always refreshing.  Thanks for the time spent putting this channel together. ,1513540271.0
afourteia,Great video! keep up the good work. Subbed,1513572567.0
KayRice,Great channel will subscribe and check it out.,1513588196.0
luke0201,"Wow, I can't imagine how much effort you put into these videos. I hope to see you keep up the nice work ;)
And of course subscribed your channel",1513613380.0
vincentcent1,"In http://neuralnetworksanddeeplearning.com/chap3.html, it is claimed that this cost function is still minimized when y is not 0 or 1. But when you substitute y = 0.5, when a is equal 0.5, the cost function is not equal to zero. I'm not sure whether there is a gap in my understanding or that this cost function only works for expected binary output y = 0 or 1",1513505924.0
arachnivore,"Make a Deep Learning Dungeon master for D&D!

I know there are tons of papers on text generation and language modeling (especially character-based language modeling). I bet there are papers on sequence style-transfer. Train a text model on tons of text including fiction (maybe scrape some D&D forums). Then see if you can generate text in the style of J. R. R. Tolkein or something.

This might not have enough relevance to electrical engineering...",1513442527.0
Clemkoa,NIPS is organising a challenge: implement one in an open source project one of the papers thar were presented this year. It can be a very nice start because there is a lot of choice. But it might be a bit difficult because state of the art ,1513438943.0
blackHoleDetector,You could also take a look at [Kaggle competitions](https://www.kaggle.com/competitions) to get potential project ideas. ,1513453220.0
happy_pirate,"https://www.researchgate.net/publication/309184127_Text_Summarization_Using_Unsupervised_Deep_Learning

Go through this.",1513520912.0
NonLinearResonance,"You can try the [hyperas](https://github.com/maxpumperla/hyperas) library for Keras hp tuning. A friend sent me the link a while back. I haven't tried it myself yet, so I can't vouch for how well it might work.",1513344525.0
srmsoumya,"Doing hyperparameters tuning may be a bit difficult to do with Neural Networks because of the number of parameters to learn. However you could implement the same techniques in GridSearchCV with smaller datasets, jason from mastering machine learning has a chapter on this.",1513322633.0
iamwil,"For your own neural network that you’re going  to train? If not, why would you do this?",1513459453.0
mentaken,There are a lot citizen science projects that collect data from users that is then used for deep learning. https://en.wikipedia.org/wiki/List_of_citizen_science_projects,1515260612.0
snes-jkr,"Nice, any documentation?",1513090916.0
Autogazer,"Pretty cool, but it’s where’s Waldo isn’t it?",1513105329.0
-TrustyDwarf-,"Can be read without subscription too, right? http://www.rsipvision.com/computer-vision-news/
",1513010432.0
arachnivore,You should post this to /r/videos,1513037333.0
blowuporblowout,Awesome,1513010029.0
Smallpaul,Cool. Are you planning to share the code or toolkit you used?,1513015554.0
readytoruple,This is fantastic.,1513034437.0
NWCoffeenut,That. is. stunning. Bravo!,1513035227.0
Magicokito,"Amazing, has the original style been taught with the ""Take on me"" video only?",1513035521.0
pastaking,Wow! How did you do this?,1513050643.0
doublethink104,Amazing. Please tell us how you did it i really want to try it,1513085649.0
pcastonguay,">In the construction of feedforward networks of quantum neurons, we
provide numerical evidence that the network not only can learn a function when trained with superposition of inputs and the corresponding output, but that this training suffices to learn the function on all individual inputs separately. When arranged to mimic Hopfield networks, quantum neural networks exhibit properties of associative memory. Patterns are encoded using the simple Hebbian rule for the weights and we demonstrate attractor dynamics from corrupted inputs. Finally, the fact that our quantum model closely captures (traditional) neural network dynamics implies that the vast body of literature and results on neural networks becomes directly relevant in the context of quantum machine learning.",1513004623.0
BusyBoredom,"Your CPU and GPU are fine. The CPU will be used primarily for preprocessing data, it's not terribly important for most current ML applications. 

You're gonna want an SSD though. An SSD is a significant enough quality of life improvement that I'd even sacrifice half the RAM or drop the 1070ti down to a 1070 if that's what it takes to afford it.

Edit: Go with 2x8GB sticks instead of 1x16GB stick too if you can, duel channel is pretty important now.",1512952217.0
jaMMint,"So, I got you:
- much faster RAM @3000Mhz, in 2 sticks which helps bandwidth

- GTX 1080 with 3 fans, great for overclocking and runs cooler

- 240GB of SSD

- Better and fully modular PSU

I saved money mostly on the MB and case, without much features lost there.

[PCPartPicker part list](https://de.pcpartpicker.com/list/LJp9hq) / [Price breakdown by merchant](https://de.pcpartpicker.com/list/LJp9hq/by_merchant/)

Type|Item|Price
:----|:----|:----
**CPU** | [AMD - Ryzen 5 1600 3.2GHz 6-Core Processor](https://de.pcpartpicker.com/product/mV98TW/amd-ryzen-5-1600-32ghz-6-core-processor-yd1600bbaebox) | €191.80 @ Mindfactory 
**Motherboard** | [MSI - B350 PC MATE ATX AM4 Motherboard](https://de.pcpartpicker.com/product/c2DzK8/msi-b350-pc-mate-atx-am4-motherboard-b350-pc-mate) | €79.90 @ Caseking 
**Memory** | [G.Skill - Trident Z 16GB (2 x 8GB) DDR4-3000 Memory](https://de.pcpartpicker.com/product/ZskwrH/gskill-memory-f43000c15d16gtzb) | €152.74 @ Amazon Deutschland 
**Storage** | [Western Digital - Green 240GB M.2-2280 Solid State Drive](https://de.pcpartpicker.com/product/yVH48d/western-digital-green-240gb-m2-2280-solid-state-drive-wds240g1g0b) | €84.13 @ Amazon Deutschland 
**Storage** | [Seagate - Barracuda 2TB 3.5"" 7200RPM Internal Hard Drive](https://de.pcpartpicker.com/product/CbL7YJ/seagate-barracuda-2tb-35-7200rpm-internal-hard-drive-st2000dm006) | €62.57 @ Amazon Deutschland 
**Video Card** | [Gigabyte - GeForce GTX 1080 8GB WINDFORCE OC 8G Video Card](https://de.pcpartpicker.com/product/V4M323/gigabyte-geforce-gtx-1080-8gb-windforce-oc-8g-video-card-gv-n1080wf3oc-8gd) | €549.90 @ Caseking 
**Case** | [BitFenix - Nova ATX Mid Tower Case](https://de.pcpartpicker.com/product/hJM323/bitfenix-case-bfxnov100kkxskrp) | €32.90 @ Caseking 
**Power Supply** | [SeaSonic - 520W 80+ Bronze Certified Fully-Modular ATX Power Supply](https://de.pcpartpicker.com/product/TgW9TW/seasonic-power-supply-m12ii520bronze) | €65.35 @ Amazon Deutschland 
 | *Prices include shipping, taxes, rebates, and discounts* |
 | **Total** | **€1219.29**
 | Generated by [PCPartPicker](http://pcpartpicker.com) 2017-12-13 02:13 CET+0100 |",1513127820.0
BreakawayEnso,"If you're comfortable with numpy, I highly recommend learning Pytorch. Disclaimer: Pytorch is still quite new so the documentation isn't at par with that of Tensorflow or other frameworks but it's still good enough for you to implement whatever network you want. 

If you care about docs, then check out Keras. Keras is a higher level abstraction to Tensorflow so you can create really deep networks in 20 lines or so. [Tutorials-1](https://github.com/sachinruk/deepschool.io)

[Tutorials-2](https://github.com/tgjeon/Keras-Tutorials)",1512929736.0
dude12345609,Start with tensorflow. It's easy to work with ,1512929391.0
waterRocket8236,"I say know all. Tensorflow, Caffe2, PyTorch.

But First Get to know Keras + Tensorflow or PyTorch.",1514268034.0
elephantail,So does it optimize faster that gradient descent? ,1513585216.0
Loggerny,"Based on the builds I've been seeing, get 8650k .",1512760095.0
juhotuho10,"why not just threadripper 1900x? 8 cores, 16 threads and 64 PCIE lanes, definitely wont be a bottleneck and the value will be much much better",1512821544.0
naptownhayday,I know you said you don't want to build one but it really isn't as hard or time consuming as it sounds. If you build it yourself you can get the exact system you want with the exact hardware you need and you can make changes to fit your needs. Need it to be quiet? You can find a nice case to put it in and it'll be quieter than anything you can buy outright. You could get a weaker processor since you don't expect to use it much and spend that extra money on a stronger gpu. Install Ubuntu from the get-go and then you don't pay a markup for Windows. I would also say buy a really nice power supply because you're gonna let this thing run all the time at max strength. Some cheap PC manufacturers will just dump a cheap psu in there but given your use you really need something built to last. ,1512742731.0
RetardedChimpanzee,Any reason why the small constraint? If your wanting to put it on a mobile platform look into the T2 w/ carrier board,1512705820.0
Loggerny,You might need more power. I would consider a larger supply if you consider adding GPUs in the future. The power supply operates at peak efficiency when it is at 50% capacity.,1512759502.0
RetardedChimpanzee,"Cpu cooling is fine. For most applications your GPU will be maxed out, but your cpu will not. I decided to go with a h100 for my rig just to remove the CPUs heat from the case. As I’d leave my 1080 at 100% usage for a week at a time I figured it would help keep it cooler without CPU heat. ",1512708062.0
RealOden,Thanks!,1512777334.0
kerrmudgeon,"> CUTLASS is a collection of CUDA C++ template abstractions for implementing high-performance matrix-multiplication (GEMM) at all levels and scales within CUDA. It incorporates strategies for hierarchical decomposition and data movement similar to those used to implement cuBLAS. CUTLASS decomposes these ""moving parts"" into reusable, modular software components abstracted by C++ template classes. These thread-wide, warp-wide, block-wide, and device-wide primitives can be specialized and tuned via custom tiling sizes, data types, and other algorithmic policy. The resulting flexibility simplifies their use as building blocks within custom kernels and applications.

Developer here: let us know what you think, and [check out the source on Github](https://github.com/NVIDIA/cutlass)!",1512603835.0
ZhuNorman,"I am learning  Andrew Ng's Deep Learning Specialization on Coursera. https://www.coursera.org/specializations/deep-learning
It is really helpful, and includes a lot details. Maybe, you can check it.
",1512610510.0
Pik000,What did you use to train it? How did you get the training data?,1512541631.0
inkognit,"try both. you'll realize very quickly that pytorch is the way to go :p
Seriously, it's so much easier to program and to try new stuff out. This is especially important in research",1512409079.0
luke0201,"It seems that you are not some kinda ""professional developer"", so the size of the community would seriously matter. Then I would recommend TensorFlow. Though it is known that TF is slow, but it wouldn't be a problem if you're not preparing some ML contests.",1512446425.0
iamspro,"Try both, pick the one you like. Personally, I found PyTorch easier to learn. Some people feel the opposite.",1512407656.0
apockill,"Honestly? I think you answered your own question. The framework doesn't really matter- the community matters. Right now, the community is more active with tensorflow. That's where you will find the most tutorials, questions on stack overflow, and implimented models. I also happen to like tensorflow, but I would pick it for those other reasons alone.",1512405081.0
waterRocket8236,Try both and see what works out. Truth is TF really has a large community support while the other does not.,1514268263.0
waterRocket8236,Kind of. If you are okay with building small models and less batch size.,1512403367.0
jandremarais,"As simple as possible? Use a classification layer with a sigmoid activation (instead of softmax for multiclass classification) and train with binary cross-entropy loss. Then you just need to decide on a thresholding function to determine how high the score for a label should be for it to be included in the final predicted label set (e.g. all labels with a score higher that 0.5). Can use cross-validation to find the optimal thresholds for a specific evaluation metric.

If you are ready for the more advanced approaches - these are some interesting reads:

https://arxiv.org/abs/1702.05891

https://arxiv.org/pdf/1704.03135.pdf",1512454081.0
HairyIndianDude,"You can modify inception retrain code provided by google tensorflow to do this. Check this https://github.com/BartyzalRadek/Multi-label-Inception-net, also the code is explained here: https://towardsdatascience.com/multi-label-image-classification-with-inception-net-cbb2ee538e30.

 Just for clarity this category of classification tasks are called multi-label classification where data can belong to multiple classes. 

Edit:
The code is basically what /u/jandremarais has suggested already, that is replacing the classification layer with sigmoid instead of a softmax. 

",1512479799.0
marrabld,Could you duplicate the images and apply one unique label to reach duplicated image? Not sure if it will converge.,1512416065.0
RetardedChimpanzee,Yolo can. Add another line in your annotation file,1512418218.0
carlthome,"Are you primarily interested in a learning experience or do you expect the system to work at the end of your project? 

People study these things for years at university so I worry it might be naive to start from scratch with zero experience. Also, the preexisting APIs are pretty great.

If you have a limited corpus it's definitely doable, but I think most direct tutorials assume some knowledge of gradients (LSTM) or conditional probability (HMM).",1512391782.0
nondifferentiable,"I don't know about fully-connected NNs but for Convolutional Neural Networks, some of the filters are interpretable. The filters respond to a combination of shape, color and texture but they usually represent more than one object.

I recently gave a talk about this: https://docs.google.com/presentation/d/1nAYijCr-_sKuDJYwMFFd_ETZ6XfxMf4r8zAkF4FNr34/edit?usp=sharing",1512318663.0
SeanPedersen,https://distill.pub/2017/feature-visualization/,1512308458.0
youngerRobotworker,You may find this interesting https://www.youtube.com/watch?v=1zvohULpe_0,1512494094.0
artificial_intel423,"Hidden units with respect to image classification can be thought of as capturing different information about an image; for example one layer might represent edges (edge detection) and another layer might capture texture based features from the image. This is a simplistic answer, but hopefully it paints the picture (pun intended). ",1512264871.0
snes-jkr,"I would disagree with your conclusion to the other answer:

I would not be surprised if you could extract object types from the hidden layer activations. In the end for scene recognition that might we one way the network learns the scene, by detecting certain objects and colors and their correlates.

It will certainly not be one neuron for a type of object, but in generally you should be able to extract objects from the activation ",1512290595.0
snes-jkr,They probably use something like WaveNet?,1512256861.0
carlthome,"The convolution kernel can't be applied at the edges of your signal without inserting some extra values. Check out the padding argument:

    Conv1D(padding='same')

Though it might be better to do reflect padding instead of adding zeroes, depending on your particular signal.",1512308224.0
notsorealsanta,I don’t think LSTM is supposed to work like this.,1512226176.0
TokyoBanana,"Wouldn't you check to see if the chat sent was a command first then handle it appropriately if it's a command, and if not send it to the tensorflow chat model to be handled?",1512093820.0
