author,comment,timestamp
DrHampants,"These are really, really nice and simple to follow. Thank you so much!",1543604075.0
DrChrispeee,"[The last article](https://www.reddit.com/r/statistics/comments/a0i172/a_quick_and_simple_introduction_to_statistical/) I made and posted to /r/statistics was quite well received and was actually distributed by curators of https://medium.com/topic/data-science, so I figured I would post it here as well! 

This time about **visualizing** and **plotting models** in **R**.

So here it is: https://medium.com/@peter.nistrup/visualizing-models-101-using-r-c7c937fc5f04

I would love to get feedback if you have any, I'm by no means an expert and this is clearly more ""*how*"" and not a lot of ""*why*""!


",1543599898.0
Darth_Marrr,Nicely done! I will keep this in the back of my mind when inference is on the menu.,1543605487.0
frinkahedron,Nice!,1543659679.0
Core_Four,This got me rock hard +1,1543604846.0
Thaufas,"Come on people: if you like the article, hit the ""clap"" button on the left!",1543620934.0
PSJupiter2,"The folks over at stack overflow solved this, if anyone is interested the link is here:

[https://stackoverflow.com/q/53559830/6440392](https://stackoverflow.com/q/53559830/6440392)",1543603620.0
TheEnlightenedDancer,It's odd to create data inside a function like that.,1543658106.0
clamiam45,"[The Elements of 
Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/)",1543561269.0
efrique,"If you haven't already read it, I'd start with ESL II. 

Edit: was just about to go get the link but I see someone has already linked it. Consider this a ""I second that recommendation""

There are other books that are worth looking at depending on what you really need, but I'd definitely focus on that one for now. ",1543614735.0
sn0wdizzle,Tidyverse or bust. ,1543561464.0
themightyteebs,"I think I might be a bit of a unique case, but my criticism of existing R stuff seems to mirror the larger community's, so here goes:

As a professional programmer making a shift into the social sciences, I can pick up the technical details on my own (and, indeed, that's what the docs and sites like stackoverflow are there for: specific technical questions); what I'm missing is a good, thorough run-through of an R workflow for people new to the entire enterprise of data analysis.

In short, I can figure out the how to do something, and post-facto the why of why it is done that way; what I, and in my limited experience the larger community, would like to see is the *what*, if that makes any sense.",1543565888.0
Wusuowhey,Maybe make different series according to topic.  Wouldn't mind seeing a pure machine learning one myself.  Less fluff than Raj/Python guy and more explanation and examples would make you stand out I think. ,1543569821.0
clamiam45,"As R users we have many different ways of working. I'm thinking of tidyverse vs data.table vs base R. We have a lot of packages for modeling too. 


When you take on particular problems, I would love to see some toy examples with syntax and performance comparisons between the different styles.",1543630558.0
yaymayhun,"I am not sure if I understood your question correctly. To get the `proportion` variable, you can do:  

    library(dplyr)

    my.dt.2 %>% 
      group_by(wk) %>% 
      mutate(proportion = N/sum(N)) %>% 
      ungroup()",1543539567.0
rayray0313,"Not really. You can say it was violated, and report the p value and correction (greenhouse etc..). I’ve seen papers report it this way. ",1543506675.0
rayray0313,"It should give you the p value. If sig, report the greenhouse or hein felt value. ",1543502752.0
Darth_Marrr,In a few brief sentences: what is the difference between OLS regression and Cubist Regression?,1543541081.0
me_be_here,"I think this should work. I'm not on a computer to test it but it splits on presencia then use a join. 

df2 <- df %>% dplyr::filter(presencia ==1)

df2 %>% dplyr::left_join(df %>% dplyr::filter 
(presencia ==0))

Edit: format

Edit2: it did not work, but this does:

full_join(df %>% filter(presencia == 0), by =c(""lon"", ""lat"", ""Obs0""))

You end up with a split presencia column but this can be combined with a mutate call following the join. ",1543522241.0
MisterDadToYou,unique() and some combination of split apply combine. ,1543463875.0
pfunkman,"[Pipes were first implemented in the Unix shell in 1973...](https://en.wikipedia.org/wiki/Pipeline_\(Unix\)\#History) 

> ""The next day"", McIlroy continued, ""saw an unforgettable orgy of one-liners as everybody joined in the excitement of plumbing.""



",1543460625.0
wouldeye,"I love this question because it implies that like... the majestic %>% was out there in the wild and no naturalists had explored yet enough to find it. 


Nah the pipe operator is pretty old, I think. Didn’t Perl have a version of it? I think was just implemented/written into R later on in the language’s life and ggplot2 already existed by then. 

If you wanna see what ggplot2 would feel like with %>% instead of +, try ‘ggvis’ which I think of as ggplot3. ",1543457957.0
efrique,"""pipes"" as a concept were certainly around before R was a twinkle in a kiwi's eye.

IIRC pipes were introduced in magrittR and by Wickham and  Francois in dplyr at essentially the same time (deliberately introduced), but there's a simple sense in which you could say that pipes were 'discovered' in R because you can implement a kind of pipe very simply in vanilla R (one example is John Mount's bizarro pipe,  `->.;`). In that sense, pipes *were in R already* but had not been noticed before a few years ago.



",1543465850.0
Atheriel,"It's not intended to be a mysterious statement. The **ggplot2** library (which I believe dates back to c. 2006) predates the ""pipe operator"" `%>%`, which is a part of the **magrittr** library (c. 2014). Hadley wanted to combine functions together in `ggplot2` at the time, so he used the `+` operator built in to R to do so.

Later on, when **magrittr** was introduced, he realized that this was an attractive alternative, which is why much more recent ""tidyverse"" packages -- such as **dplyr** -- use it instead.

It's worth remembering that **ggplot2** was Hadley's first package -- written when he was a graduate student, no less -- and thus predates many of the conventions we see in the other ""tidyverse"" packages.

(Personally, I would argue that **ggplot2**'s design and use of `+` are far more effective than any alternative using `%>%`. The failure of the **ggvis** project is a good illustration of this.)",1543463283.0
fang_xianfu,"It's like... if he had said ""created"", it would sound like hubris or arrogance, because the concept of piping is nothing new. Probably in an earlier version it did say ""created"", and someone who proofread it questioned it. 

It's more like he means ""before we realised it was applicable here"".",1543472326.0
Darth_Marrr,"""Nobody in this town works without a retainer guys. Let me tell you, if you think you can find someone that can then you have my blessing."" Chucky (Good Will Hunting)",1543429284.0
spsanderson,Or if your script starts like this do you even R?,1543422756.0
redstoneglowstone,It is all fun and games until you are find yourself completely crippled without dplyr and you don't want it to be a dependency for your package...,1543423024.0
y2kse,    #!/usr/bin/env Rscript,1543427031.0
_Wintermute,[A hommage to Jenny Byran's presentation](https://imgur.com/tPLkeXP),1543448531.0
Agent_KD637,"|#install.packages(""tidyverse"")",1543422970.0
Darth_Marrr,Why is this necessary?,1543426822.0
bubblride,"First, thank you for your effort. As a disclaimer, I use R mostly for EDA, and python for ML, and i have no idea about R package dev.

The problem with all these messy input arguments of the `predict` functions makes a lot sense. As you already mentioned, `sklearn` has more rigorous about such things. It uses base classes (e.g. TransformerMixin, etc.) to ensure that people wrap their code into new classes so that pipelines won't fail (you are forced to conform with it). These base classes is basically the sklearn API. I think the biggest advantage is that I can dream up my own sklearn-ish class, put it into a separate package that would work fine with all the other sklearn-ish packages and the original package as well.

I spotted that you used something like `set_engine(""stan"")`. Does it mean that all new models, classifiers, etc. need to be integrated in `parsnip`? These S4 classes allow inheritance according to [this](http://adv-r.had.co.nz/OO-essentials.html) (And i still have no idea what i am talking about).

```
setClass(""Person"")
...
setClass(""Employee"",
  ... bla bla ...
  contains = ""Person"")   # <<< look here
```

If `parsnip` has the goal to streamline all models, classifiers, etc. you might aim at providing base classes

",1543433887.0
R6stuckinsd,"""I am documenting the slog because the slog is required. We will move on to better things afterwards.""",1543415336.0
shujaa-g,"(a) `expand.grid` is part of `base`, not `lattice`.

(b) It's strange that you assume that the process for generating points is the issue that needs solving. Seems like having enough memory to store a result of a certain size is the immediate problem. You mention 1000 points in each dimension, so for 3 dimensions you want 1000^3 = 1e9 points, for 4 dimensions 1e12 points... 

R, like many other programming languages, uses 8 bytes to store a single `numeric` value. So if you wanted to create 1e12 numeric values, (that is, 1 value at each point of a 4-dimensional 1000-point-each-dimension grid), that by itself would take roughly 8e12 bytes, or about 8000 GB of memory, and that's ignoring the memory for storing the grid coordinates themselves. My laptop has 8 GB memory. The *most memory you can get* on a single AWS machine is 1,952 GB on a x1.32xlarge instance. 

It doesn't matter *how* you generate the grid or the predictions. This is how much memory it will take to store the result.

(c) So, you need a different solution. Maybe you can work something out with iterators (like in the `iterators` package) if you want to work within this grid framework, but at arbitrary points instead of at every point. Or maybe MCMC is a good solution. Can't really give advice without knowing more about your problem and goals.

(d) But you certainly need to abandon this approach.",1543415325.0
tacothecat,I dont think i understand what you're asking for.   If you dont have enough ram to hold the grid in memory then you will have to make some kind of loop to iterate over the dimensions.  How many dimensions are we talking about,1543411568.0
clamiam45,"The model.matrix function in the stats package will help you create a one hot encoding scheme in preparation for a regression.

    dat <- data.frame(season = as.factor(sample(1:4, 100, replace = TRUE))) 
    dat2 <- model.matrix(~ season, dat)",1543388052.0
roe_boat,"Here is a somewhat clunky alternative to some of the other methods shared here.  It assumes that 'season' isn't a factor.

`for (i in 1:4){`

  `assign( paste(""Season"", i),ifelse(rawData$season== i,1,0))`

  `rawData<-cbind(rawData,` [`as.name`](https://as.name)`(paste(""Season"", i)))`

`}`",1543422027.0
maltiv,"You probably don’t need to do this. Make your variable a factor. It will be treated as if it’s «one hot encoded» when you include it in a model. 

If you do need it then there are packages that do this like caret::dummyVars. ",1543388181.0
SemanticTriangle,"There are many ways to do this, but I usually use

    dplyr::mutate(dummy = 1L) %>%
    tidyr::spread(key = Season, value = dummy, sep = ""_"")

to dummify one column at a time.",1543391070.0
jimmyjimjimjimmy,"Doubt this is of much help, thought I'd share, just in case. tfdatasets::dataset\_map and tf$one\_hot will do the trick, but it works on tensorflow datasets not a data frames.

Here's an example from [https://tensorflow.rstudio.com/tools/tfdatasets/articles/introduction.html#transformations](https://tensorflow.rstudio.com/tools/tfdatasets/articles/introduction.html#transformations).

`library(tfdatasets)`

`write_csv(mtcars, path = 'mtcars.csv')`

`mtcars_spec <- csv_record_spec(""mtcars.csv"", types = ""dididddiiii"")`

`dataset <- text_line_dataset('mtcars.csv', record_spec = mtcars_spec)`

`dataset <- dataset %>%` 

  `dataset_map(function(record) {`

`record$gear <- tf$one_hot(record$gear, 3L)`

`record`

  `})`

&#x200B;",1543425747.0
MrSquat,"The better way would be to use a cast function like dcast. That takes the unique values of a column and creates columns with that name.

It doesnt make sense to expect the values to be 0/1. The reason to have each season in a seperate column would be because youd want the values of another variable in the colums.

If you have a variable and want that marked by the season with only one season per row then this is accomplished with a column named season with four factors.",1543388457.0
Mathman27,"The last line in the lasso block has an extra `sample(1:n, size[j])` that I don't think shoot there. I'd be surprised if that's what's causing the error, but I've had stranger ones before.",1543371100.0
Darth_Marrr,You have to run this without the loops and see if it is working line by line. Manually!,1543374868.0
shaqerd,Double check the brackets,1543369611.0
efrique,"What's going on here?

     test.lasso.mis[i]sample(1:n, size[j])=     mean((y.hat.test != y.test))",1543371069.0
datatitian,It means you left an enclosed parenthesis somewhere before that },1543377467.0
Quasimoto3000,"Yes, but these days it’s most common to interoperate between c++ and r. Of note, a valid C file is a valid c++ file. Check out RCPP.",1543366272.0
zdk,"see the docs:

[https://cran.r-project.org/doc/manuals/r-release/R-exts.html#Interface-functions-\_002eCall-and-\_002eExternal](https://cran.r-project.org/doc/manuals/r-release/R-exts.html#Interface-functions-_002eCall-and-_002eExternal)",1543349244.0
shujaa-g,"In R, `p<distr>` give the *cumulative* probability function for <distr> distribution, `d<distr>` is what you want for the non-cumulative mass or density function.

Use `dbinom` not `pbinom`.",1543350937.0
dirtyfool33,Can you give some example of how you are doing it? ,1543350768.0
efrique,"They're not equivalent functions. Try this:

     a <- c(1,3,2,2,1)
     b <- 1:3

     a %in% b
     a == b   

`%in%` is set-membership. 

`a %in% b`  means ""is each element of a somewhere in b?""

 `a == b` is an element by element comparison

if you use `%in%` when you mean `==` you will get wrong answers

[Edit: fixed a mangled edit in the original]",1543332675.0
ct0,"I usually use == when I want to filter something specifically, and %in% when I want to filter to every string of a list and I defined earlier.",1543582589.0
Deleetdk,\`==\` is faster and thus preferred when you only need to test for identity to one element.,1543369783.0
mystery_trams,"Maybe I’m ignorant but my understanding of powerbi is that the data are still rectangular, because the sql code in the background updates the WHERE joins and filters. That can be done through base::merge, and other functions depending on where your data are.
In terms of having the filters interactive through a front end look at shiny. Is that close to what you mean?",1543333431.0
JoshLemonLyman,"!= is ""not equal to.""",1543322842.0
canadaface,"!= denotes not equal to.

Have a look at [this guide](https://www.statmethods.net/management/operators.html)  for more operators.",1543322891.0
,"I will answer more generally to the broad question of what '!' means.

'!' means negation. So !TRUE is FALSE and !FALSE is TRUE.

!(a == b) is the same as (a != b). So you can deduce that '!=' means 'not equal'.",1543330348.0
Cronormo,"The exclamation mark is a ""unary operator"" (says wikipedia). It basically translates to `not`.  

`!=` means `is not equal to`. It's the opposite of `==` which means is equal to. It's generally used for logical (true/false) operators as it reverses them. So any function that returns `TRUE` can be turned into the opposite by adding an exclamation mark before. This can be very useful.",1543323078.0
shujaa-g,"The real lesson here should be how to get help. If `?foo` doesn't work (`?!=` doesn't work, naturally), try with quotes. `?""!=""` will take you to the appropriate help page.",1543329380.0
Woiiwod,"the != is a test of non equality and returns TRUE if knn.pred is not equal to mpg01.test. If they are equal it returns FALSE. 

Ex; 1 != 2 : TRUE

Ex; 1 != 1 : FALSE

In R you can use ""!"" to reverse any logical statement. If you write !TRUE in the console it will return FALSE. The same for an assigned logical or a vector of logicals. ",1543323092.0
NinjaInUnitard,"Thank you all, you've all been very helpful! ",1543324468.0
TrueBirch,">Can someone explain what the exclamation mark means?

No!",1543330741.0
Atheriel,"Others have answered the direct question, but I thought I'd expand a bit and point out that `mean(knn.pred != mpg01.test)` is a concise (but confusing) way to compute the error rate. Since `knn.pred` and `mpg01.test` are both vectors, the result of comparing them with `!=` is *also* a vector. Further, in R, taking the mean of a logical vector will give you the share equal to `TRUE`. So this really says ""what percent of my predicted values match the test set""?",1543375399.0
bobalvarezz,[Here ](https://users.cs.cf.ac.uk/Dave.Marshall/PERL/node35.html) is another link if you want to know about logical operators.,1543323806.0
PersnickityWicket,"I can’t wait until you see your first %>%!  It’s my favorite part of R, at first I was like what the actual 🦆, but now I’ve love “and then”",1543338585.0
g_squared2,"I think it means ""and not"" or ""different than""",1543322806.0
friendlyimposter,"Get rid of dat1$, that‘s referring to the whole data",1543316842.0
flyos,"The `*` in a formula in R doesn't stand for ""multiplication"", but for ""interaction"". In a formula, `var1*var2` is basically interpreted as `var1 + var2 + var1:var2`. 

Now, if you expend `var1*var1` like above, you'll see that is basically ends up with only `var1` as the rest is redundant.

If you want `*` to be interpreted as an arithmetic operation (same goes for `^`), you need to say so to R. This is exactly what the `I()` function does.

TL;DR: You're right, you should use `I(var1^2)`.

Another possibility is to compute `var1_sq = var1 * var1` as a new column in your data before using `glm()`, which would result in a cleaner output.",1543319515.0
msfellhauer,"Here is where you will run into issues..... the great majority of 30 year fixed mortgages are underwritten to Fannie and Freddie guidelines. Most banks do make those types of decisions, at the retail level, at least for mortgages. Banks actually have products designed (other loans), as loss leaders, specifically to combat fair lending. Meaning, they develop products specifically for the people you are referring to, so they don’t get written up on fair lending practices.

Your approach, in terms of what you are looking at, is commendable. I like the methodology as well.

I’ve been in banking 25 years (every level) and completed some research (during my doctorate) on this exact topic, and this is where I ran into the dead end. I’d like to learn more about where your coming from, though. Any any help I can be, please let me know.",1543300329.0
efrique,">  More specifically, I want to create a model that shows when gender, race, etc. explain a significantly high percentage of the dependent variable. 

Take a look at [Simpson's paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox)
and particularly the [Berkeley gender bias](https://en.wikipedia.org/wiki/Simpson%27s_paradox#UC_Berkeley_gender_bias) example


A much higher proportion of male applicants to graduate school were admitted. That looks like clear gender bias -- and if you fitted a binomial logistic regression model based on gender, you'd get a *highly* significant coefficient - but when you try to figure out which departments were doing the discriminating... it turns out there are a few with (relatively small) effects in each direction (and accounting for department, the overall bias is slightly in favor of women).

It would have been possible for there to be zero bias in every department and still have been an overall disparity by gender (indeed it could have turned out that *every* department was biased in favor of women but the overall figure would look biased in favor of men) 

If you're not very careful you can easily conclude bias where you've just left out a relevant covariate (perhaps one you didn't think of), or to say there's bias against group A when in fact the bias (accounting for some other variables) is in favor of group A (so actions taken to reduce bias based on the naive analysis would actually increase it).
",1543305255.0
danderzei,"Use the Kruskal-Wallis Rank Sum Test. Use this test to find out if two or more medians are different (like an ANOVA).

Tutorial: [http://www.r-tutor.com/elementary-statistics/non-parametric-methods/kruskal-wallis-test](http://www.r-tutor.com/elementary-statistics/non-parametric-methods/kruskal-wallis-test)",1543301615.0
Food_and_Stuff,"Well it depends on what you mean by ""sets,"" but assuming you mean 3 groups, then, both sort of yes and no.

1) Wilcoxon / Mann-Whitney U tests are for pairwise comparisons, so A to B. You could do Wilcoxon tests on all three pairwise comparisons (A:B, B:C, A:C), and then depending on your questions, do some p-value adjustment for multiple testing (with p.adj() ), and you can just report all three p or q-values. 

2) If you're looking for non-parametric tests that would replace an ANOVA in comparing multiple groups, you could look at Kruskal-Wallis tests (kruskal.test) or other non-parametric variants.

Hope that helps. 

Also, just for my own due diligence, your 3 sets of data do need to be comparable in that they're adequately controlled and designed to be analyzed as such. 

&#x200B;",1543301899.0
efrique,"*which* specific test do you mean by ""Wilcox""? 

Are you referring to a test by someone called ""Wilcox"" (like Rand Wilcox, say) or are you actually talking about a Wilcoxon test (and if so, which one? Rank sum or signed rank? )

",1543305858.0
grasshoppermouse,Why do you have `data=data` in the call to the svyglm function? This looks screwy too: `+IDS+ +PSU+STSTR`,1543282860.0
s3x2,"Pretty much every model fitting function in R can pass its output to `predict()`, which allows you to retrieve a variety of model-estimated quantities given arbitrary variable values.

[This link explains the specifics when working with a `coxph` object.](https://stat.ethz.ch/R-manual/R-devel/library/survival/html/predict.coxph.html)",1543285834.0
Molefucker,There’s a package called survminer that gives all the hazard ratio parameters etc plus more ,1543296206.0
booze_n_goose,"Hello Nyle, 
I am trying to understand your questions but struggle to do so. Can you specify the structure of list_rwi please by running str(list_rwi) and post the output here? This way I might be able to help you

I figure list_rwi is a list consisting of 12 data.frames. Each of these data.frames consists of 3 columns. You want to divide its 13th row by the mean of row 11 and 12. The result for every data.frame would be a vector of the 3 calculated ratios.",1543236742.0
samclifford,So your code is meant to take the 13th row of a data frame and divide it by the average of the 11th and 12th rows? And then after that you want it to return a vector containing that calculated value for each of the 12 files? So ideally you're getting something back with 12 rows and three columns?,1543239192.0
Darth_Marrr,In part 3: are you using the NW Estimator to determine what power transformation to use/attempt?,1543248996.0
DrChrispeee,"I posted this on /r/statistics as well: https://www.reddit.com/r/statistics/comments/a0i172/a_quick_and_simple_introduction_to_statistical/

There's a bunch of feedback over there which I've tried my best to respond to, so check it out if you like!",1543262383.0
redstoneglowstone,This was really good. I learned a lot by reading it. Thank you for your post.,1543335194.0
sad_house_guest,"As someone interested in developing my first R package, do you have any style tips? I've been using devtools to maintain a personal package of functions I regularly use, but if I were to develop a package for others to use I worry that I'm not following best practices for package development.",1543201370.0
AkeOrdur,First of all congratulations ! I have a few questions : what was the process to get approved by CRAN ? How did you chose your license ? Have you used the usethis package to help you structure your package ? ,1543212647.0
wouldeye,Also why did you choose binomial?,1543169585.0
gauchnomics,"> as.numeric(X..of.Vote) ~ Incumbent.Score + Party.Score +
> X2016.Pres.Score + as.numeric(Fundraising) + as.numeric(TOTAL.SOCIAL.MEDIA.SCORE)
, family = binomial

There is a lot wrong here. As already pointed out, binomial should be used for a logit (i.e. binary) regression. So you want to drop ""family = binomial"" completely from your function. (also if you wanted a logit regression, or anything not Gaussian/normally distributed you'd probably want to use [glm](https://www.statmethods.net/advstats/glm.html)). 

To the actual terms you're using, you'll likely want to reformat some of them. You're not the first person to regress (what I assume is 2018 House Share) on past elections. It's highly [correlated](https://fivethirtyeight.com/features/everything-is-partisan-and-correlated-and-boring/) with 2016 Presidential vote. 
If that isn't the most important term in your regression, you've likely misspecified something. If 2016.Pres.Score is not on a continuous scale, you can pull that data from [here](https://www.dailykos.com/stories/2012/11/19/1163009/-Daily-Kos-Elections-presidential-results-by-congressional-district-for-the-2012-2008-elections). 

For example I'm not sure what a political party binary is doing when you already have a binary for incumbent (and vice versa).  You  may want to combine them in some way. One way to convey both pieces of information is to use a factor variable for incumbency where Dem is is the Dem is running for releection, Rep is the Republican is and otherwise Open.  If you're looking for a dataset with incumbency, you can get one from ~2017 [here](https://docs.google.com/spreadsheets/d/1VfkHtzBTP5gf4jAu8tcVQgsBJ1IDvXEHjuMqYlOgYbA/edit#gid=0). You'd have to fill in some of the special elections yourself (e.g. PA-18). It may be easier to just reformat the data you already have, but I can't comment on variables I haven't seen.

Finally if your dependent variable is party vote share: are you including uncontested races (I wouldn't) and are you including both candidates on 2 rows (again, I wouldn't)? Since I don't know how vote share is formatted, I would make sure it's standardized so 100 (or 1) is the Dem candidate (or Rep, it doesn't matter) winning 100% of the vote and 0 is 0. You can then have a term for dem fundraising and rep fundraising and dem media score and rep media score (you may even want to do this as a difference or share). If you're dataset is structured so you have twice as many rows as races, it may be a pain to restructure. However you should do it anyway, because you don't have 870 observations, you have [393](https://www.washingtonpost.com/graphics/2018/politics/midterms-uncontested-candidates/) observations. Your results will be skewed by uncontested races and your variables of interest(fundraising, media score) will be difficult to interpret with nearly duplicate rows.  For example, you want to account for the fact  Ted Cruz and  Beto O'Rourke both have high fundraising and high media presence. 

Finally, if you posted your results (or even where to find the fundraising/media data), I'd definitively be interested in how predictive the terms were.",1543177853.0
yagirltildz,"Hello reddit! I am not a statistics minor/major and am writing my thesis as a Political Communication major. 

I am attempting to see if the amount of social media followers a Congressional candidate has affected their total vote share percentage. I am holding constant other electoral fundamentals such as incumbency score or how the district voted in 2016. 

&#x200B;

I ran this model lm(formula = as.numeric(X..of.Vote) \~ Incumbent.Score + Party.Score + 

X2016.Pres.Score + as.numeric(Fundraising) + as.numeric(TOTAL.SOCIAL.MEDIA.SCORE), 

family = binomial)

&#x200B;

Below are my outputs. Could someone help me interpret them? Thanks in advance! ",1543169346.0
wouldeye,"“As.numeric(fundraising)” has me concerned. If it’s being coerced to numeric, have you checked that it is happening the way you want it to? (i.e. it actually does look like numbers after being coerced)",1543169542.0
calicliche,"You might want to include 2014 vote share rather than or in addition to 2016. The presidential election is typically quite different demographically than the midterm, with implications for down ballot races. Also, is your dependent variable continuous (0% to 100%) or win loss (0 or 1)? If continuous you should be doing a standard OLS type regression. Your control variables being binomial doesn’t impact this. You can interpret those coefficients as the boost or drawback holding each of those properties has on total vote share or likelihood of being elected (if your dependent variable is win loss). ",1543170525.0
yagirltildz,"Thank you so much! I actually pulled my data from DailyKos. My sample size is small, only 65 districts. Do you think that’s why 2016 Presidential selection is not statistically significant? ",1543179080.0
yagirltildz,"Yes it is on a 0-100 scale. I agree, I think it’s too highly correlated as only 25 districts are not matched up. ",1543184015.0
fool126,Is SE(xbar) = SD(xbar)? Is SE just another name for SD of a statistic? ,1543162920.0
AnInquiringMind,"I'm trying to recreate your problem but I can't seem to do it. But, from what you've said, I'm 90% sure that it has nothing to do with the creation of the time series object itself. Your code is very simple, straightforward, and correct.

The problem is more likely in your plot function, but autoplot() is (unfortunately) used in multiple libraries and can produce some interesting results.

Have you tried ggfortify::autoplot(data1) ?",1543150710.0
ichikaren,"data is a data frame while ts use vector. So I think you should use the dollar sign $ to refer a column in data frame. the code should be 

```
data <- read.csv(""path"", header = TRUE, sep = "";"")

data1 <- ts(data$column1, start = c(2017, 1), frequency = 12)
```",1543460346.0
AnInquiringMind,"One way is to use the dplyr library to reshape the data for easier plotting. There are ways to do this faster but I'm on mobile and all I have to go on are the pictures of the dataset:

    participation$id <- rownames(participation)
    participation <- gather(participation, ""state"", ""value"", -id)
    participation$key <- ""participation""
    
    prevalence$id <- rownames(prevalence)
    prevalence <- gather(prevalence, ""state"", ""value"", -id)
    prevalence$key <- ""prevalence""
    
    merged <- bind_rows(participation, prevalence)
    
    tidy_data <- spread(merged, key, value)
    
    ggplot(tidy_data, aes(x=state, y=id, size=prevalence, colour=participation)) +
      geom_point()
I've never used ggballoonplot but geom_point() in ggplot2 should do the same thing?",1543154043.0
rfc2100,"With the RStudio 1.2 [preview release](https://www.rstudio.com/products/rstudio/download/preview/), you can *finally* add new color schemes without having to edit the horribly-named CSS files. 

You can use an online theme editor [like this one](https://tmtheme-editor.herokuapp.com/#!/editor/theme/Monokai), modify until you're happy, then add the theme file through the RStudio global options. If you're using RStudio server, you'll need to upload it to the host, first. I also had to add an extra blank line at the bottom of the .tmtheme file to make it happy. ",1543109464.0
DataDouche,"It’s beautiful, thank you!",1543107858.0
BerryGuns,"This is fantastic, cheers. Works fine in linux for anyone wondering.",1543157875.0
I_just_made,"I love this theme, definitely going to use it!",1543158058.0
iamasharkskin,"Hey! Thanks for doing such a wonderful job to make the editor look beautiful! I have encountered a small bug where any of the tool windows look weird. Image: [https://imgur.com/yONORID](https://imgur.com/yONORID)

&#x200B;

Let me know if I should make this a github issue! I am using Ubuntu btw ",1543344300.0
PupHendo,Which editor theme is this? It's super cool.,1543110933.0
LiesLies,"You can call the project ""daRkStudio""",1543122524.0
Darth_Marrr,"There is a package in R called Swirl. I have not used this, but I have heard of very good reviews about it on Reddit as well as a fellow Graduate Student with whom I suggested it to. The benefit of Swirl is it teaches you R *in R.* Most other courses are online, in an html platform or you would need to read a book. All good and all helpful, but for your purposes I think Swirl would be an optimal strategy. Plus they have a course on statistical inference and machine learning from what my grad colleague has told me.

[https://swirlstats.com/](https://swirlstats.com/)",1543032055.0
vonkrumholz,"Try the UCLA tutorials. Very straightforward explanation of how to do analyses without getting bogged down in R basics or nuances. Here's one on logistic regression: 


https://stats.idre.ucla.edu/r/dae/logit-regression/",1543075289.0
julialert,"I don't recall going through regression analysis training with **datacamp** (google it!) but I'm sure they must have it. Either way, it's a great interactive tutorial-based website that you can learn a lot from. That's how I started out when I took my first R coding class in college.",1543034772.0
Atheriel,"Honestly, you are very likely to get what you want from almost any R book, tutorial, or course, because linear regression will *always* be among the first topics you'll go through. Provided you have nice data to begin with, running regressions and examining the results will only take a few lines of code, and once you've done it a few times you'll probably have most of the skills you need. Don't worry about picking *the* book -- pick *a* book, even if you don't think you can finish it.

IMO, it's much more difficult to teach people statistics than R, and by your admission you've already got the background. Remember that R was explicitly designed for people like you -- those aiming to do statistics, but lacking a software background. You can do it.",1543031855.0
rayray0313,"RStudio is r, but RStudio includes other features that basic r does not. For example, it has 4 windows that give you a space to write notes, save codes, and easily install packages. It looks prettier too. It still requires knowledge into r coding. I find r code easier to remember than spss syntax imo. I learned r from a pdf I found years ago that was titled, “rough and dirty cheat into r”. That was all I needed. I also signed up for a free month of data camp a few years back, and it helped with data management and other cool things. 

I do recommend jamovi. It is really user friendly, and it is getting better with each update. All I’m waiting for is for them to include a restructure command, and then I’m fully converted to jamovi.",1543028192.0
El_Commi,"Audit the R course on Coursera. 


Alternatively just dive in  it's a steep curve. But worth it in the end. Break your tasks into smaller chunks and just Google the solution and ask around. Theres a wealth of info online. ",1543018690.0
Crypt0Nihilist,"The R Book

The R Cookbook

Both of these will get you going at your own pace, the first builds everything up and the second gives you quick-wins like doing your regression. 

There are many good online courses, I'll leave the others to go into those.",1543074017.0
one_game_will,"Statistical Rethinking by Richard Elreath is essentially a course in Bayesian statistics in R, including linear regression on up. It also focuses on practical issues in implementing analyses and potential pitfalls, rather than just theory.",1543076329.0
kayjaymatthews,"Discovering statistics using R by Andy Field, but you’ll shell out money for a book when there are a lot of free resources. ",1543067770.0
Zippityzinga,what biostats books do you recommend as someone who is interested in learning more,1543095100.0
ravingrabbits,"I took basic R programming courses at datacamp  which were provided from my university and self supplement with books.

&#x200B;

R for Data Science - authored by Hadley Wickam (The guru in R stats!) and Practical Statistics for Data Scientists by Peter Bruce is a good read.

&#x200B;

I have 0 expertise in biostats and coding prior to this. You will be fine :)

&#x200B;

Just a note, datacamp is legit because the founders are the one maintaining the R repo right now. 

&#x200B;",1543162611.0
rayray0313,"Try jamovi. It’s free. It’s basically a gui for r, but it provides code to give you an idea on how to use it.

Learning r is simple. A basic regression analysis looks like those:
Regression<-Lm(dv~ iv)

Then
Summary(regression)

",1543024036.0
Juko007,Never knew there were any! Just googled a bit and stumbled across this article listing a few: https://decisionstats.com/2012/04/13/easter-eggs-in-rstats/,1543002387.0
PM_ur_good_deeds,Wow! How did you find out about this?,1543003605.0
AKANotAValidUsername,Ha! I cant believe ive never seen these... and ive been using R since '04. ,1543022410.0
tacothecat,"you are grouping by SA and trying to max on it at the same time

&#x200B;",1543078179.0
dugorama,a glm procedure?  if your dep var is a ratio or binary then try to sort on the values of the dep var before running.,1542996551.0
flyos,"The whole point of `read_csv` is to be as type-explicit as possible and use defaults as safe as possible, which is why anytime there is something weird, it defaults to characters.

The `readr` package philosophy is thus that if you want a factor, you set it up after reading the data using e.g. the `parse_factor` function, or explicitly during the data reading by using the `col_types` argument of `read_csv`.",1542961186.0
_Wintermute,"Use data.table's `fread` instead, it has a `stringsAsFactors` argument",1542915486.0
Statman12,"> setting everything to char for some reason.

Are you sure all of the data in the CSV *are* numbers? If there is a character somewhere in there, the whole variable might be converted to character. For instance, if missing values are denoted with a period, or if there are numbers with commas, etc.

There is an argument to `read_csv` which lets you specify the intended type of data.",1542922632.0
guepier,Do you have caching enabled? Try disabling it: caching large data sets using R's built-in functions (which knitr does) can be extremely slow.,1542915112.0
s3x2,"Is this an interactive workbook or are you just compiling into a static document? You can create a drake workflow that detects whether workbooks have been added, refresh the rds and then remake the rmd.",1542906776.0
Hype_x,I’ve noticed this too.  Code chunks that run instantly in the console take several seconds longer in Rmarkdown.  ,1542916827.0
Thaufas,"Have you tried using `RNotebook` instead of `RMarkdown`? Syntactically, the two options are exactly the same. However, `RNotebook` was invented specifically to deal with the very issue you describe (*i.e.* frequently recompiling the Rmd is too slow).

If your Rmd code is **never** completing, then RNotebooks might not be the answer, but normally, there is no downside to using RNotebooks instead of the original RMarkdown, while there is significant potential upside in terms of efficiency while making many saves to your RMarkdown document while working interactively in RStudio.",1542928039.0
nicholes_erskin,"It's always going to be much easier for people to help you if you post your code, because otherwise the best anybody can give is informed speculation.",1542933672.0
addfunr3,"If it's fine in the console, have you tried using source () to run the code as a script inside the markdown file? That way, you're not reading in a static rds file.",1542921912.0
Tarqon,Code that has to re-draw a progress bar over and over in the output of a code block runs way slower in my experience. See for instance https://github.com/tidyverse/readr/issues/793.,1542922692.0
Darth_Marrr,Investigate using child documents and set `cache = TRUE` in the chunk headers to avoid re-calculation.,1542924290.0
Shadowing_Lemma,"Sorry if I'm taking the simplicity angle *way* too literally, but is this what you were trying to do?

require(ggplot2)
require(dplyr)

dat <- data.frame(weeks = runif(100, min=0, max=25), activity = sample(c(""putting"", ""chipping"", ""practice"", ""full game""), 25, TRUE))

dat %>% ggplot(aes(x = activity, y = weeks)) + geom_boxplot()",1542901114.0
beavvis,"I think that making the data ""long"" would solve your problem, not sure from question if you had done that already. Use the gather function so that your data has two columns one for activity, and the other as time to return. Then ggplot(data, aes(activity, time))+geom_boxplot() should give the graph you are looking for. It wouldn't really make sense to send the mean values to a boxplot since they summarise a range of values.",1542935335.0
KopfJ4ger,"Edit: Nevermind, I incorrectly assumed that you were comparing two populations.

~~What you really want to do here is create a plot comparing the mean time of your two populations (x = your population variable, y = mean time to return) and then use facet_wrap() to iterate on the activities.~~",1542923631.0
VincentStaples,but why doe,1542930017.0
samclifford,"You add a fourth group, Total, that contains summed numerators and denominators of all your color groups.

Also \`split\`, \`map\`ping \`tidy\` and \`map\_df\` makes handling the output of the proportions test a little easier. Arguably. Using \`sprintf\` makes for less repetition of rounding and multiplying.

    diamonds %>% 
        dplyr::filter(color %in% c(""D"", ""E"", ""F"")) %>%
        count(color, cut) %>% 
        group_by(color) %>%
        mutate(denom = sum(n)) %>%
        bind_rows(., group_by(., cut) %>% summarise(n = sum(n),
                                                    denom = sum(denom)) %>%
                      mutate(color=""Total"")) %>%
        ungroup %>%
        inner_join(., 
                   split(x = .,
                         f = list(.$cut, .$color), drop=T) %>%
                       map(~prop.test(x = .x$n, n = .x$denom)) %>%
                       map_df(~tidy(.x), .id="".id"") %>%
                       dplyr::select(.id, estimate, conf.low, conf.high) %>%
                       separate(.id, into = c(""cut"", ""color""), sep = ""\\."")) %>%
        dplyr::mutate_at(.vars = vars(estimate, conf.high, conf.low),
                         .funs = function(x){x*100}) %>%
        dplyr::transmute(cut = cut,
                         color = color,
                         n = n,
                         denom = denom,
                         props = sprintf(""%.1f (%.1f-%.1f)"", 
                                         estimate,
                                         conf.low,
                                         conf.high)) %>%
        dplyr::select(-denom) %>%
        gather(variable, value, c(n, props)) %>% 
        unite(temp, color, variable, sep = """") %>%
        spread(temp, value)",1542887186.0
Financialacumen,"My attempt, which is admittedly slightly less advanced than the above:

    df <- diamonds %>% 
      filter(color %in% c(""D"", ""E"", ""F"")) %>%
      count(color, cut) %>% 
      group_by(color) %>%
      mutate(denom = sum(n)) %>%
      rowwise() %>%
      mutate(props = paste0(round(prop.test(n, denom)$estimate * 100, digits = 1),
                            "" ("", 
                            round(prop.test(n, denom)$conf.int[1] * 100, digits = 1), 
                            ""-"",
                            round(prop.test(n, denom)$conf.int[2] * 100, digits = 1),
                            "")"")) %>%
      select(-denom) %>%
      gather(variable, value, c(n, props)) %>% 
      unite(temp, color, variable, sep = """") %>%
      spread(temp, value)
    
    df <- bind_cols(df, df %>% mutate_at(vars(Dn, En, Fn), funs(as.numeric(.))) %>% select(Dn, En, Fn) %>%
      transmute(Total_Fn = rowSums(.))) %>%
      mutate(denom = sum(Total_Fn)) %>%
      rowwise() %>%
      mutate(Total_props = paste0(round(prop.test(Total_Fn, denom)$estimate * 100, digits = 1),
                                  "" ("", 
                                  round(prop.test(Total_Fn, denom)$conf.int[1] * 100, digits = 1), 
                                  ""-"",
                                  round(prop.test(Total_Fn, denom)$conf.int[2] * 100, digits = 1),
                                  "")"")) %>%
      select(-denom)",1542887806.0
AlisonByTheC,"Maybe you have a comma in one of the values?


Try summing the integer column or even taking it over to another tool like Excel to see if you can spot the issue via another tool.  ",1542893535.0
gwesp,Can you provide a minimal reproducible example? E.g. give us a subset of the data causing the issue plus your code. Which will cause the issue and a link to the version of your amelia version?,1542896299.0
AllezCannes,"Yes, it is possible, although Shiny isn't specifically designed to act as a survey collection instrument.",1542866174.0
Thaufas,"> I’m envisioning this as a curve where the x axis is the price difference and the y axis is the overall revenue.

That's a good place to start, but it only tells part of the story. Generally, in an upsell situation, the higher priced product has a higher margin. Therefore, the goal isn't just to maximize revenue (the ""top line"" or ""gross sales""). Rather, the goal is to maximize earnings (the ""bottom line"" or ""profit"").

In an ideal situation, the client would tell you the profit margins for  each product, and you would simply maximize the total average profit for a specified period. In the absence of such information, you should state that you assume both products have the same profit margins and then do exactly what you already planned (ie maximize revenue).",1542868169.0
DogEarBlanket,"One approach is to build a model of demand as a function of the two prices. You can approximate that with deterministic demand or build a distribution.  If you use deterministic, you can then have two variables: the price of the base item and the amount of the up-charge. If you can approximate the demand function with a fitted curve, you can use (likely) nonlinear optimization to determine a price/upcharge to maximize the total profit.   


Given that in practice, ""optimal"" means ""much better than what we are doing now"", I would identify a set of candidate price/upcharge pairs and use MC simulation to determine a set of metrics on the profits using your fitted demand distribution (mean, median, 25th perc, 75th perc, likelihood of exceeding current average profit, etc.). It's a more brute force approach, but will clarify your decision space and outcomes. You really don't need to determine a base price and upcharge, but to illustrate the impact of potential decisions and let them decide based on their risk preferences.",1542895142.0
Ader_anhilator,Interesting problem. You mention this would be for a client of yours so I should assume you're getting paid for this. Do you not find it unethical in the least bit that you are looking to potentially crowd-source a solution where you offer zero compensation? ,1542863014.0
ravingrabbits,"I take the second Outcome1:GroupA is a typo and should be Outcome2:GroupA?

&#x200B;

Either way, R uses reference coding

&#x200B;

So the odds ratio of group B will be the same as expo(Outcome1) or exp(0.546) as it is the baseline reference",1543164493.0
danielw29,"The usual way is to estimate a regression with group indicator, post treatment indicator and interaction. The coef of the interaction is the diff in diff estimator. Assumption of course being that the exogenous changes are equal for both groups.
Edit:
Spelling.
Also to clarify:
You can estimate this as an ols model [lm(...)] and use the t statistics to make inference. ",1542839272.0
sausagepizzaguy,"I think LAVAAN package has a function specifically for this. [link to pdf](http://lavaan.ugent.be/tutorial/tutorial.pdf), unfortunately, I'm not entirely sure which one. 

You are basically looking for a regression while controlling for the differences in the test group correct? This sounds like looking at the interaction of whatever variables against their own null comparisons? ",1542840655.0
jinnyjuice,"My guess is that your question isn't answered already if my interpretation of your question is correct instead of the other comments. Either way, here it goes.

* If your variable is in levels, make another variable that is in proportions/logs (and vice versa).

* Construct a confidence interval for both the treatment and the control group for levels and logs.

* Test the hypothesis that the two groups have parallel trends through the confidence intervals (level-level, log-log).

If it passes both tests for level-level and log-log, then this should give you some supporting evidence that there are parallel trends.",1542848432.0
scrample2401,"I'm looking into something similar, can someone confirm if this type of approach is correct?

> glm(formula = intervention + group + intervention * group, data=mydata)

In which intervention and group ate binary variables, and  the coefficient on intervention indicates how much the main group changed?",1542855158.0
MyBarkingSpider,"I believe dimnames must be a string or NULL.  

You dont have to ""manually"" write every number as string.  Its just a line of code.

x<-c(1:10)

y<-as.character(x)

You could replace the 'X' with something more logical or unobtrusive, like ""p"" for product.

I tend to do this by habit because I often have leading zeros in my data names, i.e. 0001276, etc.  And the leading zeros are important for API queries.  I immediately prefix with an ""s"" so that the leading zero's are retained if/when I read/write to a CSV.  I have a function that just strips the ""s"" before I use it for a query.

&#x200B;

&#x200B;",1542836009.0
chandaliergalaxy,"    > txt <- ""1,2,3
    + a,b,c""
    > (data1 <- read.csv(text=txt))

      X1 X2 X3
    1  a  b  c
    
    > names(data1) <- substring(names(data1), 2)
    > data1

      1 2 3
    1 a b c

Or,
    
    > (data2 <- read.csv(text=txt, check.names=FALSE))

      1 2 3
    1 a b c
    
Note:

* You can also use `check.names=FALSE` as an argument to `data.frame()` when you are creating a data frame from variables directly and not importing from a file.
        
* You can also use `names(data1) <- sub(""^X"", """", names(data1))` to search and replace base on patterns more generally.",1542848327.0
flyos,"Why do you need the look-ahead? If you just want anything that doesn't start with ""~"", can't you use the following?

    list.files(path = ""yourpath"", pattern = ""^[^~]"")",1542821341.0
rayray0313,It is correct. I am familiar with this type of model set. ,1542827749.0
shujaa-g,Could you edit your post indenting the code by 4 spaces to make it more readable?,1542821309.0
IpwndGoliath,"Some pseudo r squared measures are adjusted and others are not. R squared will never decrease when adding variables, I can’t imagine non adjusted pseudo r squared would either, though I haven’t seen every formula for every measure.. 


The simplest and most likely answer is that it’s an adjusted pseudo r square and is penalizing the measure for the additional factors. ",1542815534.0
efrique,"When the first person dies, that's one death out of 5, but when the second person dies, it's one death out of *3* not 4. When the third dies, it's one death out of 2. So the total for the KM survival function should be (1-1/5)\*(1-1/3)\*(1-1/2) or about 0.267.

So your plot should level off at 0.733 ",1542772665.0
efrique,"what are v1, v2, v3? ",1542765859.0
bunsenthebeaker,Background running!,1542741504.0
redstoneglowstone,"The preview releases have run a lot better on my higher resolution display than the current release. Still a lot of noticeable bugs though, so I can see why they are keeping it categorized as a preview.",1542744516.0
ryapric,"Two of my biggest gripes have been no PPT knitting, and the numeric filter options in the viewer. This is great!",1542756835.0
ozjimbob,"The background jobs feature is very cool - now all it needs is the ability to run jobs on remote servers, or even submit them to PBS jobs queues on HPC...",1542763621.0
Darth_Marrr,The new theme and color scheme is  sublime! The text is soo crisp!,1542773059.0
ozjimbob,"Is it just me, or has anyone noticed the Console -> Stop button now behaves differently?  If pressed, it used to stop the script right where it was and not continue further commands...now it seems to stop the current executing command, but then continue to try to run the rest.  Which leaves me mashing the stop button. ",1542934878.0
IsomerSC,"Cool post. Couple of thoughts:

1. Copyedit: What did Mookie do so well**  
2. Hard to intuitively understand the size of the resultant bands, maybe put a % text onto each band?  
3. There's some random text at the top, not sure what this is about?: function() { if (/lang=de/.test(location.toString()) { return } var elements = document.querySelectorAll(""button""); Array.prototype.filter.call(elements, function(element){ return RegExp(""Senden"").test(element.textContent); }).forEach(function(el) { el.innerText = ""Send""; }); }(
  
",1542738215.0
IsomerSC,"using dplyr/ggplot2:  
1. filter(return>30 & return <40)  
2. geom_histogram()",1542832208.0
MURUNDI,I would like to do something like this chart 4 in  [https://cran.r-project.org/web/packages/emojifont/vignettes/emojifont.html](https://cran.r-project.org/web/packages/emojifont/vignettes/emojifont.html) ,1542705396.0
zorp_,"Could you paste in the code? If you can’t, I feel you, I’m on a phone too",1542725796.0
suity1,"i think the `emmeans` package is the best way to proceed

&#x200B;

    library(emmeans)
    library(magrittr)
    m1 %>%
       emmeans(consec ~ IV1)

&#x200B;",1543002819.0
redstoneglowstone,"I haven't tried it --- 

The first thought I had was using one of those pre-paid CCs that are typically used as gift cards. That way if you go over your limit there is only so much they can charge to that CC you provide. Might keep you safe from unexpected overcharges. ",1542671314.0
AlisonByTheC,"Have you tried the here.com API? You can do 125,000 pairs a day now I believe.  ",1542691385.0
esotericish,"    df %>%
      ungroup %>%
      filter(PPValue < FTValue) %>%
      ggplot()",1542663721.0
JamMcFar,"If you would like to stick to base:
> temp <- df$PPValue - df$FTValue

> df[which(temp < 0),]
",1542668183.0
redstoneglowstone,"I like to use dplyr for these kinds of problems like /u/esotericish already posted. For base R try

&#x200B;

`new_df <- df[df$PPValue < df$FTValue,]`

data.frames (and the subsclasses) are index by \[row, columns\]. Leaving either blank returns all, so by leaving the columns entry blank we are returning all columns from the data.frame. In base R, you can index in one of 3 ways: vector of positive integers, vector of negative integers, or vector of logical. For positive integers, each integer provided in the vector is selected. For negative integers, only index values not provided are selected (e.g., c(-1) would return everything but the 1st position index). For logical vectors, the length must equal the max index value and each TRUE value would return that row and FALSE values would not return a row.

&#x200B;

I'm going by the last method of selecting; df$PPValue < df$FTValue returns the TRUE/FALSE vector needed.",1542670744.0
alchemy3083,"data0$lon is a factor.  Coercing to numeric yields the numeric level of each factor, while coercing to character yields the character value that level maps to.

So you could normally just use

    newData=as.numeric(as.character(data0$lon))

But you have the additional issue of commas instead of periods as decimal separators, which as.numeric() doesn't know how to handle, so you need to substitute those between steps.

    newData=as.numeric(gsub("","", ""."", as.character(data0$lon)))

Depending on how you're getting this data frame, it's probably easiest to handle this stuff during import.  

    read.table(file=""myfile.txt"", stringsAsFactors=F, dec="","")

Telling R what your decimal separator is will make sure it reads lon and lat as numeric classes.  Currently it sees them as characters, which it then tries to force into factors when it builds data0.  Outright telling R to not turn strings (really ""characters"") into factors is just generally a good idea.
",1542671498.0
crested_penguin,Does as.numeric(as.character(...)) work? This usually solves things for me.,1542662241.0
geocompR,Read your table in with `stringsAsFactors=FALSE`... that’s usually the easiest way. ,1542682936.0
Moody_Mudskipper,"Try `as.numeric(sub("","", ""."", data0$lon))`. But the best thing would be to solve this upstream at the time you read the data.",1542667133.0
BlitzBackwards,"If you are using read.table/csv for your data input use the dec = "","" argument. 
As mentioned above a factor is an integer vector with predefined values which are (in most cases) not the same as the names (levels) assigned to them. 
Decimal separator mix up has caused my so much headaches in the past, now I always change the reagional settings in my computers, no more commas.
",1542672494.0
fasnoosh,"Try readr::parse_number(as.character(data0$lon))

Help page: https://www.rdocumentation.org/packages/readr/versions/1.1.1/topics/parse_number
",1542682245.0
danielw29,Convert to character first and then to numeric. Factors have numeric values of 1:G. Where G is the number of factors independently of their labels. ,1542662288.0
shujaa-g,"Any variables that are `factor` class will be handled as categorical. When given to most modeling functions they will internally use `model.matrix` to create dummy variables for each of the levels, except for the first which is left as a ""reference level"", fit by the intercept. It is good practice to make the most common level of each factor the reference level. The `relevel` function makes this quite easy.

Even if you are using stepwise regression, hopefully you are at least able to use something like `MASS::stepAIC` that use AIC as a criterion rather than just statistical significance or other unpenalized metrics. That will avoid at least some of the pitfalls of stepwise selection. `MASS::stepAIC` takes a starting model as input, not a design matrix, so:

> 1) Can I just use the model.matrix() function and input this as my design matrix into the forward regression function?

**No.** This may depend on the particular function you are using, but the common `MASS::stepAIC` does not take a design matrix as input.

> 2) Should I one-hot-encode those variables that are discrete (some are multiclass)?

Depends. If you one-hot multiclass variables, that will allow the step-wise algorithm to select each level individually. For example, if you have a `race` column that is either `white black asian hispanic` if you leave it as a `factor` column in your data then the stepwise selection will either include `race` (internally one-hot encoding all the races save the reference level) or not include `race` at all. If you one-hot encode `race` yourself using `model.matrix`, then the model fitting functions do not know that the `black` dummy column is in any way related to the `asian` dummy column, and it is possible the model may select, say, just the `asian` dummy variable and ignore the rest of the race information.

> 3) Do you ever center and scale discrete variables (if you do not one-hot-encode)?

​Depends. Do they have a meaningful order? Is distance between the levels interpretable? If the variable just happens to be integers but isn't really categorical, I would treat  it as continuous. If there are small number of ordered options like a Likert scale, centering and maybe standardizing seems useful.  You can look at `?contr.treatment` for some alternative encodings to one-hot. [Standardizing dummy variables themselves is sometimes recommended](https://stats.stackexchange.com/q/69568/7515). Or some recommend [scaling continuous variables by 2sd so that they are more comparable to dummies](http://www.stat.columbia.edu/~gelman/research/published/standardizing7.pdf).

All of this depends on your data and your goals. I don't see any connection between this third question and ""your main concern"", *""the actual implementation in R and how R handles these cases.""*",1542662954.0
battlestarmetallica,"Try `tidyr::full_join(House14, CCES14, by = ""state_District)` ",1542657419.0
TonySipriano,"It depends on what you mean by 'merge' - do you want only the observations in both sets, or do you want all observations from both sets, or something else? Take a look at the different types of joins, for example in this post: [http://rstudio-pubs-static.s3.amazonaws.com/227171\_618ebdce0b9d44f3af65700e833593db.html](http://rstudio-pubs-static.s3.amazonaws.com/227171_618ebdce0b9d44f3af65700e833593db.html)

The package `dplyr` then makes things pretty easy.",1542659996.0
Edozz_stats,"probably you need an all.x=TRUE option, instead of all=T",1542662989.0
shakkyz,Merge should work with either all.x = TRUE or all.y = TRUE,1542666849.0
Ader_anhilator,What about copulas? ,1542647854.0
shujaa-g,"Have you looked at the CRAN Task Views? Both the [Multivariate Distributions section of the Multivariate Statistics Task View](https://cran.r-project.org/web/views/Multivariate.html) and the [Continuous Multivariate Distributions section of the Probability Distributions Task View](https://cran.r-project.org/web/views/Distributions.html) sound very promising. Since you  mention log-normal in your question, the [`compositions` package looks relevant]( https://CRAN.R-project.org/package=compositions).",1542649228.0
willbell,The stats4 package can fit a multivariate distribution with the mle() function if you provide the negative log likelihoods.,1542650253.0
efrique,"I'm curious why you'd use `fitdist` to fit a lognormal; the MLE is trivial - just take logs and fit the normal: the ML estimate of µ is the sample mean and the ML estimate of σ^2 is the n-denominator version of sample variance; by the time you wrote the call to fitdist you could have had your estimates and standard errors anyway.

With joint distributions, you need to actually specify a joint distribution - a nontrivial task. There are often *many* mutivariate versions of a distribution (e.g. about half a dozen ""mutivariate exponential"" distributions), depending on which aspects of the univariate you need to retain/can afford to lose when going multivariate and what properties you need it to have. Fitting is often relatively straightforward compared to choosing.

Beyond building or finding some multivariate family you like, there's also the copula approach; with that you totally separate specifying the multivariate dependence from specifying the marginal distributions. 

> how could one make inferences on the change of the parameters of these distributions? In my case, I compute X and Y with a different number of observations, and as new observations are added the parameters of their distribution change in a way I can model with an exponential regression. 

An asymptotic test based off the likelihood ratio test (or one of the other asymptotic tests related to MLE -- a score test or Wald test) would be my first thought, though in some cases you may be able to do something else.


",1542687710.0
Runner1928,The various MCMC packages are super handy. I started with rethinking and also use brms and rstan.,1542658901.0
Core_Four,"Lol at r1 r/badeconomics

Thats why you [post your analysis](https://old.reddit.com/r/wallstreetbets/comments/6lweua/its_not_a_bubble_yet/djx9eil/)  in r/wallstreetbets like a true golden god.",1542652902.0
samclifford,"Just as an FYI, you can either wrap your code in three tick marks (\`\`\`) to start and end code formatting, or put four spaces at the start of the line to tell Reddit you're about to write some code.",1542643766.0
fang_xianfu,"The true correct answer is to beat your upstream data source with a rubber hose until they fix their shit, because that's atrocious.

Also you make find if you're opening the CSV in Windows that it uses Unix line endings and that's why they appear on one line. It could also be using a non-printing character as a delimiter. Opening it in a better text editor or a hex editor might help.",1542609972.0
ZoharAbuSaid,"Using dplyr:

    library(dplyr)

    yourdata <- mutate_if(yourdata, ~all(is.numeric(.) && . %% 1 == 0), as.integer)

This will run the as.integer function on numeric columns where all values have a modulo 1 equal to 0 (whole numbers).

Edit: To improve performance, use is.double instead since is.numeric returns true for integer columns too.",1542580173.0
efrique,"Depending on circumstances, either 

     if(all.equal(x,as.integer(x)))  {x <- as.integer(x)}

or

     if(identical(x,as.integer(x)))  {x <- as.integer(x)}

on a column, depending on whether you want fuzzy equality or not (beware; it's easy to choose identical but screw yourself up); if in doubt make use of the tolerance in `all.equal`


If you need this applied to multiple columns, you can write a function for use with an `apply` function.
",1542588374.0
infrequentaccismus,"I’m not exactly sure what you mean, but check out the options in scale_x_date() for changing the labels of your date. Do you have an example of what you have now and what you want it to look like when you’re done?",1542588100.0
ZoharAbuSaid,Yes,1542578004.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/rlanguage] [Shiny Reactivity Help x-post](https://www.reddit.com/r/Rlanguage/comments/9y9q2j/shiny_reactivity_help_xpost/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1542574864.0
ELKronos,"I would guess there are characters, like ""o"" instead of ""0"", or a space before/after numbers?

Did you check them in your CSV? I'm presuming you're not asking about NA for system missing data and don't have case IDs which may include letters within them sometimes?",1542519617.0
zdk,add the \`freq=FALSE\` argument to \`plot\`,1542516375.0
efrique,"just put the `prob` argument in your plot call

    x1=rnorm(500)
    x2=rnorm(500)
    p1=hist(x1, breaks = 30,plot=FALSE)
    p2=hist(x2, breaks = 30,plot=FALSE)
    plot(p1, col = rgb(0, 0, 1, 1/4), xlim = c(-5, 5),prob=TRUE)
    plot(p2, col = rgb(1, 0, 0, 1/4), xlim = c(-5, 5),prob=TRUE, add = T)
",1542521288.0
Darth_Marrr,"Interesting, but doesn't quite compare to Xtable, albeit Xtable has its own set of idiosyncrasies to contend with.",1542485199.0
samclifford,"I use knitr for the most part when converting data frames to tables, but will switch to kableExtra if I need something difficult done. Most of the time I'm rendering to LaTeX so if I'm writing a table manually I'll just write out the LaTeX. ",1542526242.0
crackrocknbach,Stargazer hasn't let me down yet,1542525224.0
robsalasco,I use huxtable!,1542742214.0
Trek7553,"How do you pronounce the name? ""em-em-pipe"" or ""mmmm-pipe""?",1542486208.0
Moody_Mudskipper,This all started there FYI: https://stackoverflow.com/questions/47475923/custom-pipe-to-silence-warnings,1542492968.0
Stuporficial,This is awesome. Thank you. ,1542736036.0
zdk,Run selectize on the server side,1542466702.0
Darth_Marrr,"I have looked at your code but by no means have I attempted to run and parse out the details in R so take the following advice with a healthy amount of skepticism.

What I deduce from that error is that in one of your iterations of your CV implementation you have a response that is all ""1"" 's. Silverpulle in 1981 wrote a paper on the unstable and mostly infinite coefficient values that are returned from Logit models when there is close to or complete separation of the classes. In your case you may be experiencing the later simply because there is exclusively one case present. Also, your data set is called Spam. I would imagine you are detecting Spam out of a series of emails. Spam is not common and in this data set would probably be considered a ""rare"" event thus presenting you with the challenge of an imbalanced data set.

You are better off using the Caret package to partition your data or to run CV. It handles this issue well.

[https://topepo.github.io/caret/data-splitting.html](https://topepo.github.io/caret/data-splitting.html)",1542488156.0
AGINSB,Do you have missing or na values?,1542455576.0
s3x2,Seems trivial to write your own function. Not sure about your notation though. Why are you conditioning on Y_i? Are the weights supposed to indicate that getting certain predictions is more important than others?,1542406988.0
sandalguy89,Are you another actuary?,1542405793.0
samclifford,Have you read about this technique somewhere and are trying to implement their idea? ,1542440053.0
cavedave,"ggplot2 code.I found this at https://twitter.com/ryantimpe/status/1062490517202706434

and the code is at https://github.com/ryantimpe/ChartOfTheDay/blob/master/1_TRexSkeletonsPercentage.R",1542312662.0
ExcellentOdysseus,Rad,1542316200.0
Darth_Marrr,What about the stegosaurus? Does he get a special plot formation??,1542345908.0
aaronsaunders,I think this is pretty cool. The faceting means that the sizes being compared are equal and you get an intuitive sense of how much t-rex they have found. Color is also informative.,1542351286.0
quabbage,Dinosaurs + ggplot2 = win,1542352795.0
forever_erratic,Only 12 T-rex skeletons have been found!?  I had no idea.,1542318336.0
CaffeineJag,"Ended up on LinkedIn for mortgages
https://www.linkedin.com/feed/update/urn:li:activity:6468861233448644608/?commentUrn=urn%3Ali%3Acomment%3A(activity%3A6468831452673953793%2C6468861178419388416)
",1542387245.0
Statman12,"I think you should:

1. Fix any typos (e.g. “2 arguments” but you list four) and clean up the notation.
2. Describe what have you done so far and where are you getting hung up.",1542389364.0
IsomerSC,"lapply may be what you're looking for. this ""applies"" a function to a long list of things, in your case the list would be the file names. CHeck this SO post:  
  
https://stackoverflow.com/questions/30242065/trying-to-merge-multiple-csv-files-in-r",1542310620.0
2strokes4lyfe,"    csv_list <- list.files(“path_name”, “\\.csv$”)    
    df <- ldply(csv_list, read_csv)
    write.csv(df, “outfile.csv”)",1542311921.0
RememberToBackupData,"If your homework assignment is about using a `while` loop then you should do it that way, but you need to be more clear with us about what you've tried and where you are stuck. If you had provided a reproducible example and the code you had tried, you might have an answer already.

---

Otherwise, if the homework is not about `while` loops and you just need to compile the data, you can use  `apply_to_files()` in [desiderata](https://github.com/DesiQuintans/desiderata/):

    rain <- apply_to_files(path = ""Raw data/Rainfall"", pattern = ""csv"", 
                           func = readr::read_csv, col_types = ""Tiic"", 
                           recursive = FALSE, ignorecase = TRUE, 
                           method = ""row_bind"")
    
    dplyr::sample_n(rain, 5)
    
    #> # A tibble: 5 x 5
    #> 
    #>   orig_source_file       Time                 Tips    mV Event 
    #>   <chr>                  <dttm>              <int> <int> <chr> 
    #> 1 BOW-BM-2016-01-15.csv  2015-12-17 03:58:00     0  4047 Normal
    #> 2 BOW-BM-2016-01-15.csv  2016-01-03 00:27:00     2  3962 Normal
    #> 3 BOW-BM-2016-01-15.csv  2015-11-27 12:06:00     0  4262 Normal
    #> 4 BIL-BPA-2018-01-24.csv 2015-11-15 10:00:00     0  4378 Normal
    #> 5 BOW-BM-2016-08-05.csv  2016-04-13 19:00:00     0  4447 Normal

It uses `lapply()` internally, so have a look at the source code.",1542311910.0
Wusuowhey,"The most basic way with dplyr is to remake the columns. Since you want new columns instead of the originals, use transmute() instead of mutate()

      new_data <- df1 %>%
      transmute(estimate1 = paste(estimate1, "" "", ""("", estimate1_error, "")"", sep = """"),
      estimate2 = paste(estimate2, "" "", ""("", estimate2_error, "")"", sep = """"))
    
       estimate1 estimate2
      1     1 (2)     3 (4)
      2     2 (3)     4 (5)
      3     3 (4)     5 (6)",1542292136.0
TrueBirch,This is a really important lesson for business leaders to learn when they start launching data projects.,1542289476.0
Tarqon,This is why autoML type services will never replace data professionals. ,1542391024.0
Darwinmate,"I think this line:

     files <- list.files(""../input"") 

Should be checked. I would put a hard path (C:/Users/Aakash Aich/Documents/input/) and test the code. Make sure `files` actually contains something. `NA` is being produced probably because you have no files in `input`

I would skip this and read in the file directly. You're trying to learn sentiment analysis, not automation/function writing. ",1542273234.0
villain170,"Have you setup your directory structure correctly?

You may also want to use this resource https://www.tidytextmining.com",1542279443.0
caitRgator,"I haven't used it, but there is a wavelet package available for R. That might be a good direction to start researching methods?",1542254824.0
carlgorn,Seewave in R,1542261326.0
leparachuteaile,The signal package allows to build spectrograms with specgram() function,1542266600.0
noahpoah,"Ooh! A friend of mine developed a package that can almost certainly help with this. It's called [phonTools](https://cran.r-project.org/web/packages/phonTools/index.html), and it has a function called [pitchtrack](http://www.santiagobarreda.com/rstuff/pitchtrack/pitchtrack.html).",1542324661.0
Statman12,"You'd have to adjust the column names to get them how you like, but there are a few options. For instance:

- `nnet::class.ind( mydf$x )`
- `model.matrix( ~ x - 1 , data=mydf )`

The latter doesn't require any package. There are also some [further](https://stackoverflow.com/questions/49276914/mutating-dummy-variables-in-dplyr?rq=1) options using other packages.",1542225911.0
SLPeoples,"Don't forget to drop one to avoid the dummy-variable trap!

10 red, 01 white, 00 blue.",1542232401.0
FIERY_BUTTHOLE,"FYI this is known as ""one-hot encoding""",1542226531.0
slammaster,I'm on mobile so I can't check the name but there's a library called fastDummies that can do this really easily. The function is dummy_cols I think,1542225638.0
Bandoozle,Try spread in the tidyverse,1542245485.0
KoolAidMeansCluster,"> mydf %>% mlr::createDummyFeatures()  
  
Converts all factors into dummies (FYI: Will not convert characters).",1542662044.0
notsoslimshaddy91,"
library(dplyr)

library(magrittr)

mydf %<>% spread(key = x, value =  x,  sep = '_')

",1542254219.0
shujaa-g,"Could you work on the formatting? It's really hard to read your post. Indent code four spaces for code formatting. Put your links directly in the parens, not in footnotes.",1542227851.0
10101010101111,"Correct me if I am mistaken, but surely g^(-1)(eta) is a probability only if exp(-eta) is small enough (smaller than one) which implies eta must be positive. Maybe adding this to the 'valideta' function would help?",1542269702.0
ZoharAbuSaid,"This is a syntax error. You are surrounding ""random"" with accents instead of regular quotation marks which will lead to unwanted behavior.",1542237284.0
ckvp,"You can use package (""parallel"") to do computing with all of your computer's cores in R.

https://nceas.github.io/oss-lessons/parallel-computing-in-r/parallel-computing-in-r.html
http://dept.stat.lsa.umich.edu/~jerrick/courses/stat701/notes/parallel.html

Also, are you at a university? There may be computers available that can quickly process your code.",1542222982.0
coffeecoffeecoffeee,"Can you stop it from running after a certain number of MCMC iterations and [generate some diagnostic plots](https://cran.r-project.org/web/packages/bayesplot/vignettes/visual-mcmc-diagnostics.html)?  It's possible that for some reason your Markov chains aren't converging.

(If you build your model in Stan, you can get a ton of these via the `shinystan` package.)",1542237826.0
uniqueturtlelove,"Can you give any insight into the structure of your model??

&#x200B;

How many predictors are you using? How many data points does it contain?",1542222468.0
paykoman,Try Microsoft Open R,1542228124.0
DeuceWallaces,"That package offers parallel computation. You will have to implement it. Negative binomial and zero inflated models have long computation times in my experience with JAGS and rstanarm.

On a side note, you probably couldn't have made a worse post seeking help if you tried. You have no model specifics, no model parameterization, data structure, no minimum working example, no PC specs...",1542222942.0
mrcflckrz,Setting up R on a server and keeping your data and scripts there and running R in the browser on several computers  is not an option?,1542212143.0
redstoneglowstone,"Disclaimer - I've never worked with such a large data set before. 

If the data folder is in a subdirectory relative to the working directory, then just using relative paths instead of physical paths is a good solution. system.path() is a useful function that is OS sensitive.

How is the data transferred between your two PCs? Is it that both PCs are on the same network and grabbing data from a network path? If so, the network path can be made to be the same string on Linux and Windows.

Once my data is loaded into R, I like to make a binary cache of the R variable using the save() function. R handles its binary files a lot faster than the .csv files. I'll use a boolean flag ""use\_cached"" or something similar that tells my script to reload the CSV or the .rda file.

Lastly, another option is to move all the data into a database server such as MySQL and use that interface.",1542212514.0
guepier,"As suggested in another comment, the answer is definitely to use relative paths, and to store the data (or a symlink to it) in a subdirectory of the project path, *regardless of how else you structure your project*. Whatever else you do, don't ever use absolute paths for data.

Having a config file with paths is possible but unnecessary.",1542213642.0
daanzel,"You could, instead of creating a config csv file, create an `.Rprofile` file in your project directory that contains the PC-specific settings, such as for example `datadir = '/home/user/myproject/datasets/'` for the linux one. That way you don't have to dilute your code with `read.csv()`\-stuff. 

When running R studio, this works slightly different since your working directory is set to your user folder so R wont read the .Rprofile file. In Rstudio you can run the following at the start of your script to change the working directory to the location of the script, so you can use relative paths from there on:

`library(rstudioapi)`  
`setwd(dirname(getActiveDocumentContext()$path))`

&#x200B;

Aside from this all, the downside of keeping multiple copies of your data is when they for some reason start to deviate from each other. Make sure each PC has access to a 'single point of truth'. A simple solution could be to set up some shared storage and run from there. You could even do this with google drive or something similar.",1542214022.0
paddedroom,"Docker image.

[https://ropenscilabs.github.io/r-docker-tutorial/](https://ropenscilabs.github.io/r-docker-tutorial/) ",1542220930.0
sad_house_guest,"You might try git large file storage with github or bitbucket to back up and transfer changes between your two computers, and the `here` package to construct platform-independent relative paths, which would require you storing your project folder as an R project, which is a trivial change to make if you aren't already doing that. People also use docker for this sort of thing but git lfs should be sufficient for your needs.",1542221356.0
WasteCadet88,"I've set up an R package with a function that specifies useful paths on different system.  Something like the following:

assignFolders = function()

	{

	user = Sys.info()['user']

	if(user=='macUser')

		{

		baseDir='/path/to/base/mac/'

		} else if(user=='linuxUser') {

		baseDir='/path/to/base/linux/'

		} 

	return(baseDir)

	}

So in any Rscript my first thing to do is library(myLibrary);mydir=assignFolders(). Works well for me across multiple systems (mac, linux and clusters), and I can specify multiple directories of interest as a list output instead.",1542220413.0
abstrusiosity,"For generating a sequence of numbers with a fixed interval, use the ""seq"" function.  Your case needs only sequential integers, so you can use the simpler "":"" notation.

    total <- seq(1,n,by=1)
or

    total <- 1:n

You could then use ""paste"" to combine that with user and date,

    paste(user,date,total,sep=""_"")    

...but that wouldn't give you the leading zero before the single digit values.  To specify that kind of formatting, use ""sprintf"".

    sprintf(""%s_%s_%.2i"",user,date,total)

The formatting notation for sprintf can be excruciating, but it gives you fine control over the output.",1542218435.0
rharrington31,"I believe that you're looking for `seq()` ([documentation](https://www.rdocumentation.org/packages/base/versions/3.5.1/topics/seq)). Using that, your script could look something like:

    user = ""me""
    n = 96
    date <- format(Sys.time(), ""%Y%m%d"")
    total <- seq(from = 1, to = n, by = 1)
    exp_vector <- c(paste(user, date, total, sep = ""_""))    

This gives an output of:

    ""me_20181114_1""  ""me_20181114_2""  ...  ""me_20181114_96""

This gets you a lot of the way, but is _slightly_ different than your original pseudo-output. Specifically, the last characters for 1 digit numbers look like `1` instead of `01`. You'll need some string manipulation to fix that issue. That will look a little bit different based upon whether or not your `n` can be greater than a 2 digit number or not (i.e., if `n = 100`, do you want your output to look like `001`, `002`, ... , `010`, ... , `100`?). Is there a maximum to the number of digits? Do you need the numbers to scale accordingly or should your output always look the same? For example, if your maximum `n` is 10,000, then do you always want the 4 0s to be appended to single digit numbers or should it be based upon the number of digits in `n`?",1542218622.0
true_unbeliever,"I could be wrong but I doubt it.  Most DOE courses use JMP, Design Expert or Minitab.  So you might have to learn with a trial version of whatever s/w is used and then learn the R tools.

You could try /r/design_of_experiments. It’s a small sub but maybe someone there has something.

Edit:  or self learn with Lawson’s book",1542150756.0
Thaufas,"There is a UT-Austin course on statistics that has several lectures on DoE with examples in R. The syllabus is available at <http://www.lithoguru.com/scientist/statistics/course.html>. That page contains links to full course notes, data sets, and YouTube videos.


",1542164245.0
TrueBirch,Data Camp has some related courses,1542153191.0
_Wintermute,"R is going to R.  
If you look at the documentation for `[` it even says:

> Character indices can in some circumstances be partially matched (see ‘pmatch’) to the names or dimnames of the object being subsetted (but never for subassignment).  Unlike S (Becker _et al_ p. 358)), R never uses partial matching when extracting by `[` and partial matching is not by default used by ‘[[’ (see ‘exact’).

And to make matters even more confusing column names behave differently. My advice would be to never rely on the partial matching and try and avoid row names if you can as R doesn't handle them well at all.",1542135280.0
wouldeye,Can’t wait to try it tonight. ,1542120857.0
TrueBirch,This is really neat!,1542129031.0
Juko007,rapply() ist a blessing when you're dealing with lists of lists! ,1542124748.0
H4CKTHEPL4NET,Didn’t even know these three apply functions existed. ,1542116402.0
jarth_or_north,"Don't forget the most important apply function in (base) R: [dendrapply](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/dendrapply.html) /s

",1542143237.0
Quasimoto3000,The Purrr package is fantastic for vectors. ,1542127646.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/rlanguage] [rapply, vapply, and eapply](https://www.reddit.com/r/Rlanguage/comments/9xaiv4/rapply_vapply_and_eapply/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1542282959.0
syrphus,"Thanks. I didn't know about either `rapply` or `eapply`. 

In what sort of scenario would one find `eapply` useful?",1542286591.0
steve11767,"I had a similar problem downloading fundamental data on companies. Unfortunately, I can't seem to access the website you linked. I will, however, tell you about my experience and maybe you can get some helpful hints from that.

First, I didn't use R. I used Selenium with Python to control Chrome. I'm sure the RSelenium library works similarly, so that shouldn't be an issue. I had Selenium enter each company name into a search dialogue, search for that company and download its data. It did this automatically for over a thousand companies. Therefore, manually going through 800 names should not present any technical difficulties.

You have to be careful though, since this can place a lot of strain on the server and your IP address might get blocked. In my case, I just inserted some random time delays between operations so that my behaviour looked a little more natural. Depending on how paranoid the website admin are, you might want to use proxy rotation. Alas, I have no personal experience doing that. I suspect that proxy rotation might be more difficult in R than Python.

I'll try and see if I can access that website a little later in the day. It might in fact not be necessary to go through the whole process of entering each name. I remember scraping a news site once, where it turned out that you could just send a query that returned a JSON file with direct links to all the info I wanted.

Hope that helps. I'll try and have a look at this again a little later.",1542102469.0
esotericish,see my reply here: https://www.reddit.com/r/Rlanguage/comments/9wms0f/help_with_a_unique_web_scraping_situation_xpost/,1542121886.0
uniqueturtlelove,"A little confused by your questions, did you run t.test between the groups?

&#x200B;

Here is a lecture I give on power analysis and statistical testing in R. It should have everything you need!

&#x200B;

[https://drive.google.com/file/d/1H1c9Cgjas8AKRv-wciBktrZSfsyxGcWO/view?usp=sharing](https://drive.google.com/file/d/1H1c9Cgjas8AKRv-wciBktrZSfsyxGcWO/view?usp=sharing)

&#x200B;

[https://drive.google.com/file/d/1XyjoaT486Lxi3ahegHsFP3KLFR\_BPkGp/view?usp=sharing](https://drive.google.com/file/d/1XyjoaT486Lxi3ahegHsFP3KLFR_BPkGp/view?usp=sharing)",1542130869.0
sad_house_guest,"You're right to store data in the long format for plotting in ggplot2, and you're on the right track with your ggplot code. In regards to A, you can use the `color` and `shape` aesthetics in the `geom_line` call to change the color and shape of the lines, respectively. 

In regards to B, try `data$week <- factor(data$week, levels = paste0(""W"", 1:12), ordered = TRUE)` to change your week variable to an ordered factor so that it will plot correctly. Alternatively, if you don't want the ""W"" in your x-axis labels and would prefer just to have the numbers, `data$week <- as.numeric(gsub(""W"", """", data$week))` will remove the W and convert week to numeric, which will plot in order as well.

There are lots of places online to find help on ggplot2, but you're on the right track  - [this cheat sheet](https://www.rstudio.com/wp-content/uploads/2016/11/ggplot2-cheatsheet-2.1.pdf) might be good for quick reference, and you can usually find questions similar to your own on stack overflow.",1542070401.0
Eleventhousand,I would do small multiples.  Look up facet_wrap,1542079588.0
2strokes4lyfe,"    color = name

Add that to your aes() function when calling ggplot. And you’re right in using long format data for ggplot. That’s where it really shines for most tasks. ",1542091172.0
deanat78,"I'm not a fan of bots myself, but in the spirit of not being an overbearing dictator I don't want to get to a place where accounts get banned (even bot ones). That said, I did just ban CommonMisspellingBot just because of the chain reaction that occurs from that user and it doesn't seem like a big loss. We will not do a blanket bot ban though.",1542046576.0
Statman12,"Yes, please!",1542041322.0
Trek7553,At least ban all of *those* bots. That is ridiculous.,1542043099.0
SemanticTriangle,Mass Effect boardroom ending solution: what if we wrote more bots to control the bots?,1542065272.0
guepier,Not to be rude but you’ve used the word “duplicates” four times in your text and title. Did you try googling for it? The solution is literally the first hit for [the search query ‹R duplicates›](https://www.google.com/search?q=R+duplicates&oq=R+duplicates).,1542018702.0
ExcellentOdysseus,unique(),1542011401.0
ajskelt,"    df <- df[!duplicated(df$address),]

&#x200B;",1542032247.0
mrcflckrz,"library(dplyr) has distinct()

[https://dplyr.tidyverse.org/reference/distinct.html](https://dplyr.tidyverse.org/reference/distinct.html)

&#x200B;

As usual, I strongly recommend the tidyverse solution...",1542189402.0
TeslaIsAdorable,"    > library(readr)
    > parse_number(""$500 CAD"")    
    [1] 500",1542037027.0
doomgasp,"Library(tidyverse)

My_Data %>% mutate(Clean_Var = as.numeric(gsub(""[^ 0-9\\.]"", """", Dirty_Var)))



This deletes everything not a digit or period, then converts to a numeric.",1542001968.0
aaronsaunders,"The readr parsers have a nice side effect that parses numbers from text that you might have use for here.

    library(tidyverse)
    
    df <- tibble(original = c(1, 'abc', '$500 CAD'))
    df %>% 
      mutate(clean = parse_number(original))

See 11.3.1 in Hadley Wickham's R for Data Science.

[https://r4ds.had.co.nz/data-import.html#parsing-a-vector](https://r4ds.had.co.nz/data-import.html#parsing-a-vector)",1542067899.0
ojdp19,This feels like spam. Just a link to your blog and nothing else. Not to mention the numerous grammar errors. Did you even bother to edit this before submitting it to Reddit?,1542003649.0
Darth_Marrr,"Thanks! Next on your to do list is to display tidyverse's ability to perform complex nuanced tasks. For instance, finding the variance of the subset of values above the third quartile for a particular covariate, but only those values that apply to men above a certain age and living in two distinct geographies. Something along these lines! ",1541983536.0
Squeezie91,"I'm using R quite a lot and know the power of dplyr, but not really using it a lot. After seeing the examples in the link, I don't see any much usefulness over my base r dataframe manipulation. ",1542004911.0
Hype_x,The tidyverse is an amazing place.  Get pumped for efficient data flow with minimal effort.,1541981353.0
g_squared2,"Thanks for the link guys, I'm new to R, and used to STATA. This may help me get going with some tasks I used to do easily there, but where I struggle in R.
",1541984471.0
doughfacedhomunculus,What's the study design? Could you not use a linear mixed effects model with lme4?,1541991908.0
goodgameplebs,How many observations do you have in each cell?,1541976232.0
sharkbaitlol,"Great read, Keras is an amazing tool for developing neural networks. The python version allows you to utilize a ""community GPU"" for faster processing. Does this exist on the R version?

&#x200B;

&#x200B;

Cheers",1542663722.0
ConnentingDots,"For instances in PCA, parameters in PC1 are the values used in the linear combination. You'll be able to see the estimated values for these in the graph. Usually one should try to give an interpretation of the first few PC based on those values.",1541876860.0
Digging_For_Ostrich,"Without any context or real knowledge on the tools themselves, it sounds to me like what is the specification of the tool, what they do, how they differ, and what they offer to the user. Just a guess however.",1541874547.0
deadsalle,tryCatch,1541828575.0
2strokes4lyfe,"    df <- try(someFun())
    if ( “try-error” %in% class(df)) {
        # do something
    } else {
        # do something else
    }",1541842688.0
DataPsuedoscientist,safely and possibly from purrr are great ,1541855330.0
Darth_Marrr,"Actually, I like this! Not sure how worthwhile it is, but I do like it!",1541820196.0
mouse_Brains,Note that [timevis](https://github.com/daattali/timevis) also exists,1541864730.0
1giov,"This looks very cool, might use this for a upcoming job application",1541871268.0
nstnct420,This is so good. Just the necessary info and to the point.,1541929343.0
eatyourveg,Thank you for putting this idea into my head. My next fun project will be putting my CV into markdown. ,1542825116.0
efrique,"Whoever is trying to get you to do it that way doesn't seem to understand R; step 2 in particular -- that's not how you should do such a loop in R, even if you were silly enough to write a loop for this in the first place.

But I am confused what possible difficulty you could have with 1c. If you have done 1a and 1b, 1c is trivial -- you literally don't know how to sum a vector? ",1541817123.0
green_tealeaf,"Did you set it up in your user crontab, or via the system-wide crontab?

If it was the system-wide crontab then that wouldn't have known to look in your personal R package library. Installing as root would put it in the system-wide library, so available to every user.",1541838216.0
ohheyitsdeejay,"I think they essentially do the same thing but require is used within functions because it outputs a FALSE value if the package does not exist. 

EDIT: I strictly use library",1541804647.0
efrique,"Their differences are clearly spelled out in the help page (it's the same page for both).

`?require`:

>  ‘library(package)’ and ‘require(package)’ both load the namespace
         of the package with name ‘package’ and attach it on the search
         list.  **‘require’ is designed for use inside other functions; it
         returns ‘FALSE’ and gives a warning (rather than an error as
         ‘library()’ does by default) if the package does not exist**.  Both
         functions check and update the list of currently attached packages
         and do not reload a namespace which is already loaded.
",1541806456.0
guepier,"See the Stack Overflow answer to: [What is the difference between require() and library()?](https://stackoverflow.com/a/51263513/1968)

tl;dr: Always use `library`, never use `require`. Experienced R programmers recommend against using `require`. It’s flawed and does redundant work compared to `library`. Contrary to the other answers I *always* recommend against `require`: If you need to load packages conditionally inside a function, use `requireNamespace` instead of `require`.",1541891303.0
atreadw,"Also -  `library` allows you to load a package into a specific position in the package search path, `search()`, which  `require` doesn't let you do.   In other words, if you have two packages that have functions with the same name, you can prioritize which package's functions will be called over the other when referencing a particular function's name.

For example, let's say you load `dplyr` and `plyr` (\*not usually recommended, but this is purely for explanatory purposes).  Run the code below:

    require(plyr)

    require(dplyr)

Then, if you call `mutate` (a function in both packages), R will call `mutate` from `dplyr` because it was loaded second.  However, `library` allows you to specify where in the search path R will look first for a package's functions, so you could do this:

&#x200B;

    library(plyr)

    library(dplyr, pos = 3)

&#x200B;

This, assuming no other packages are loaded, would put `plyr` before `dplyr` in the package search path even after `dplyr` was loaded.  To see where your loaded packages are in the search path, just run:

&#x200B;

    search()

&#x200B;

&#x200B;",1543064976.0
ajskelt,"I've just messed with this a little bit.  I think the only way you should have NA's is if there is no one in that group who is ""gender = 1"" and isn't missing a wage value.  

&#x200B;

Did you verify if there are any groups where some people had a value for MeanWage and others were missing?",1541794474.0
biledemon85,"I'm not sure this is exactly what you're looking for but I gave it a shot using only `dplyr` and some data I made up from inferring what your data might look like. You really should supply the data with the code problem, makes it much easier for people to help you out.

&#x200B;

    df1 <- tibble(
      ED = c('a', 'b', 'b', 'b' , 'a', 'a'),
      YEAR = c(2017, 2017, 2017, 2018, 2018, 2018),
      Gender = c(1, 0, 1, 0, 1, 0),
      Wage = c(NA, 1000, 2000, NA, 2000, 1000)
    )
    
    df2 <- 
      df1 %>% 
      mutate(gender1Wage = case_when(
        Gender == 1 ~ Wage,
        TRUE ~ NA_real_
      ),
      gender1Wage = case_when(
        is.na(gender1Wage) ~ 0,
        TRUE ~ gender1Wage)) %>% 
      group_by(ED, YEAR) %>% 
      mutate(meanWage = mean(gender1Wage))
    
    # # A tibble: 6 x 6
    # # Groups:   ED, YEAR [4]
    #   ED     YEAR Gender  Wage gender1Wage meanWage
    #   <chr> <dbl>  <dbl> <dbl>       <dbl>    <dbl>
    # 1 a      2017      1    NA           0        0
    # 2 b      2017      0  1000           0     1000
    # 3 b      2017      1  2000        2000     1000
    # 4 b      2018      0    NA           0        0
    # 5 a      2018      1  2000        2000     1000
    # 6 a      2018      0  1000           0     1000

I calculated an intermediary column `gender1Wage` where if it's not part of the required group I set to 0 and then did the mean on the same. So you get the mean across the overall group but only using rows that meet your criteria.

&#x200B;",1541796803.0
shakkyz,"Give mutate(MeanWage = mean(wage[gender == 1], na.rm = TRUE)) a shot.",1541800180.0
ZoharAbuSaid,"A bit late reply but this is my suggestion:

    MeanWages <- df1 %>% 
      filter(Gender == 1) %>%
      group_by(ED, YEAR) %>%
      summarise(MeanWage = mean(Wage, na.rm=T))
    
    df1 <- df1 %>% left_join(MeanWages)",1542238836.0
shakkyz,"Apply a filter before the group_by 

filter(gender == 1 & !is.na(wags)) %>%",1541795099.0
RvlvrWhite,"Looks like you have a heteroskedasticity issue (your error term is correlated with position along your independent variable). This will not affect the estimate of coefficients, but does affect the appropriate error term that ought to be associated with those estimates.

The [Robust package](https://rdrr.io/cran/sjstats/man/robust.html) will allow you to correct (for an lm object) you standard errors.",1541807117.0
HoberMallow90,"DTOutput and renderDT with:

- width = 650
- class = ‘compact hover row-border order-column’
- rownames = F
- options = list(autoWidth = F, scrollX = T, orderClasses = T, search = list(regex = T))

Then pipe any desired DT::format... functions.



",1544105476.0
wouldeye,"Okay I mostly have it figured out. 

The only outstanding problem is that shiny doesn’t work well with the html markdown option of df_print. RenderDataTable works fine in general but I’d like to not display all 200 columns at once (which markdowns df_print truncated to browseable by default). Any ideas?",1541784653.0
MarijnBerg,"I think it's just called variable highlighting.

As for replacing them, RStudio has ""replace all"" and ""replace all in selection"" options in the find/replace bar.",1541777476.0
sumwunelse,Great idea. It will be more and more important to highlight good journalism. Appreciate you putting this together. ,1541770158.0
postalot333,"Make it Nobel prize, Pulitzer isn't really worthy enough of someone who's really good at formatting graphs. 
Or get over yourself, data people.
",1541779295.0
Krynnadin,RStudio with markdown will knit to docx,1541727301.0
,[deleted],1541737340.0
fasnoosh,Maybe try the reddit datasets on Google bigquery?,1541767569.0
Darwinmate,"900k, jfc wtf are you doing. My general rule, if I think you're going more info this seems spammy to me. ",1541753190.0
,partial matrix? hahaha your lab mate got you good,1541771585.0
TrueBirch,Thanks for sharing! This looks like a potentially useful package.,1541708276.0
RememberToBackupData,"FYI  for tidyverse people, `magrittr` does this via the _compound assignment pipe operator_:

    > x <- c(1, 2, 3, 4, 5, 6)
    > x
    [1] 1 2 3 4 5 6

    > x %<>% mean()
    > x
    [1] 3.5

However, I think `dotdot` integrates more cleanly with workflows that aren't inside the tidyverse, since the tidyverse is built around the magrittr pipe-forward and other packages are built around the normal compound assignment operator `:=`, which `dotdot` takes control of. Nice package!",1541745215.0
Darth_Marrr,Didn't understand its function at first...Now I love it! Good job!,1541740832.0
dontchokeme,"Hmm, interesting, I wonder if it works in tidyverse as well...",1541711269.0
James_RW,I think it's called an area plot. The ggplot2 package should enable you to reproduce this. There's a function called geom_area,1541634741.0
shujaa-g,"There's a bunch of questions on SO for doing this. [Here's one](https://stackoverflow.com/q/17959817/903061), [here's another](https://stackoverflow.com/q/7883154/903061). Pretty easy in ggplot.",1541639931.0
PupHendo,"I used to see these a lot a few years ago (mainly NBA & NFL from memory) but not so much anymore. 

I always found them to be a great way to see how a game flowed, and where teams were more dominant throughout the match. 

If anyone has any advice on making them in R that'd be awesome! Thanks!",1541634578.0
ExcellentOdysseus,"d3.js my dude

For a while they had the guy who invented it on staff

",1541636887.0
phonomir,These are some great graphics. Anyone care to elaborate on how these may have been made for us beginners?,1541636678.0
,[deleted],1541636735.0
nesion,Great share! ,1541645200.0
madmongoose1,"Really cool, thanks for sharing!

What is a ""report"" like this called? I want to google this through the night!",1541683675.0
TrueBirch,Getting And Cleaning Data is quite good,1541620973.0
No_Cat_No_Cradle,"If you want more once you get through swirl, they're not free but I really like Data Camp's tutorials.",1541625448.0
dartkite,"`library(jsonlite)`

`library(purrr)`

`library(data.table)`

&#x200B;

`# This isn't strictly json - it's a file where each line is json`

&#x200B;

`# Read data line by line`

`raw <- readLines(""`[`https://files.pushshift.io/reddit/comments/sample_data.json`](https://files.pushshift.io/reddit/comments/sample_data.json)`"")`

&#x200B;

`# Convert each line from json to a list`

`json_list <- map(raw, fromJSON)`

&#x200B;

`# Convert each list to a data.table`

`dt_list <- map(json_list, as.data.table)`

&#x200B;

`# Harness the power of rbind list`

`dt <- rbindlist(dt_list, fill = TRUE)`

&#x200B;

`# Done`

`dt`

&#x200B;

`# Here is the same thing in a single pipeline`

`dt2 <- readLines(""`[`https://files.pushshift.io/reddit/comments/sample_data.json`](https://files.pushshift.io/reddit/comments/sample_data.json)`"") %>%`

`map(fromJSON) %>%`

`map(as.data.table) %>%`

`rbindlist(fill = TRUE)`

&#x200B;

`# Happy to explain any steps - enjoy`",1541620953.0
GrayHaven,"I think you can use bind_rows() or bind_cols() depending on whether you want 20 columns or 20 rows. According to their description, they'll take lists: lists of data frames and lists of vectors anyway. I'm not sure about lists of lists. You might have to flatten the internal lists to vectors with unlist() or whatever the tidy version is. 

Hope that helps. ",1541602835.0
chonggg511,Ive done this recently with many lists within lists. Lookinto rbindlist in the `data.table` package. You might have to do some lapplying.,1541606521.0
NinjasInTheWind,"Another solution is to use the `Reduce` higher-order function with `rbind`, or the `melt` function within the `reshape2` package.",1541619115.0
efrique,"so you mean a list like this?

    dlist <- list(a=list(1,2,3),b=list(6,7,8))

    > dlist
    $a
    $a[[1]]
    [1] 1
    
    $a[[2]]
    [1] 2
    
    $a[[3]]
    [1] 3
    
    
    $b
    $b[[1]]
    [1] 6
    
    $b[[2]]
    [1] 7
    
    $b[[3]]
    [1] 8

Try

    data.frame(sapply(dlist,c))
      a b
    1 1 6
    2 2 7
    3 3 8",1541630482.0
AlisonByTheC,"Great, now I need to learn German AND Random Forest.   I can’t catch a break here.  ",1541614468.0
a2a2a2a2a2a2a2a2a2a2,"Jawoll!

Danke",1542470883.0
AkeOrdur,Why ? ,1541595279.0
nyquilrox,"Though I agree with the other comments here, I would say that yes, you should remove effects with 0 variance before removing correlations, and you should never remove one that is to account for nesting/nonindependence in your data. 

Granted I do mixed modeling in ecology, but you shouldn’t be including random effects for no reason. Good reasons to include a random effect are (1) to account for nonindependence and (2) you have reason to believe it matters. ",1541578104.0
s3x2,Sounds like a silly way to waste your time unless you've got computational resources to spare. I would start simple and build up instead.,1541565865.0
rayray0313,"Why don’t you just see if the model fit improved (I.e., aic, bic, log liklhood)? Maybe you don’t need random effects. This can easily be done in any software. Check out jamovi, it does it well. ",1541557749.0
ScoutEU,"I work in a data scientist-sy role in the government, there seems to be a lot of jobs going (UK)

I don't know where you got the idea that R is looked down upon, most of the data scientists I work with know both Python and R and use the right language for the job, but there is no preference for either. ",1541538029.0
trngoon,"R is only looked down upon by people who really don't have any idea what they are talking about. The analytics frameworks (dplyr and the complete tidyverse stack) are literally 5 to 6 years ahead of the equivalents in Python (pandas and some others in the scipy stack). Pandas is really not good and most people agree with that. I have had to convert a lot of dplyr code to pandas code and its like taking a machete to a wedding cake. I cringe every time. Go ahead and do these conversions yourself and you'll see what I mean about how far ahead R tools are for analytics.

Python has better deep learning capabilities and is certainly a better choice for many machine learning projects. But not always and any machine learning project can certainly be done in R (although DL should really be done with Python). And anything analytical or statistical is much better suited to R. R is certainly not looked down upon compared to SAS. I won't even touch on that because it is really not the case for anyone.  

R has some of the best and most renowned data scientists and statistical/graphical computing geniuses in the world working on its packages and future performance (people like Hadley Wickham and Max Kuhn to name two).  

A lot of people from comp sci fall for the beginner notion that Python is just better in every way than R. Its really not. As someone about to graduate their masters degree in this area and has been hired as a full data scientist for January - I know exactly what tools they use. R is used a lot for many different things and there is a lot of development which is allowing it to handle massive data better. Two of my good friends were offered amazing jobs in Toronto and Ottawa Canada both for their ability to use R to analyze and model data. They are making great money and their R ability is really what won them their current lifestyles. That being said, knowing Python and the relevant libraries is also important.",1541595850.0
antiheaderalist,"I work in healthcare and the field is definitely open.  My team works primarily in R and Excel, it seems like the field is a mix of R, Python, and SAS.

I came in from a non-analytics field and got my entry-level position largely because I learned R in college, then refreshed online.

I suspect there's a lot of R in the field because many leaders transition in from academics. I'd say definitely look in healthcare if you're interested, or try to find other fields that are relatively young and have close ties to academia.

I will say we recently almost hired someone for a senior analyst position who was transitioning over from an IT/DBA role, but he had to turn it down because it would've been a sizeable pay cut.

I will say that in our interviews one of the most important things we look for it's willingness/interest in learning.  Right now we're looking for people with an R background, but we know there's a good chance we'll have to pick up Python or something else in the near future.",1541541807.0
noviceProgrammer1,You can reach out to universities around and see if they are professors looking for R developers. You could leverage your knowledge of linux and web development. I did research part time at UCSF and used this contact to leverage and network and landed a full time job at another university.,1541995991.0
flyos,"`which()` is going to remove the NAs for you:

    yada <- df$var1[which(df$var2 == max(df$var2, na.rm=T))]",1541626347.0
vali_son_of_odin,"I was able to get around it by doing it this way: 

temp <- df$var1[df$var2 == max(df$var2, na.rm=T)]

yada <- temp[!is.na(temp)]

but I don't like how it took the extra line it took to accomplish since I was originally trying to embed it within another function",1541530954.0
neumatron11,"You could add the [is.na](https://is.na)() condition into your initial line with & or more simply use dplyr's filter function. Run the following snippet to see:

    library(dplyr)
    df <- data.frame(
    var1 = 1:5,
    var2 = c(NA, 1L, 1L, 0L, 0L)
    )
    df
    df$var1[df$var2 == max(df$var2, na.rm=TRUE)]
    df$var1[df$var2 == max(df$var2, na.rm=TRUE) & is.na(df$var2)]
    filter(df, var2 == max(df$var2, na.rm=TRUE))$var1

&#x200B;",1541541308.0
SemanticTriangle,"For OP, reddit formatting is to precede your text with four spaces: and then it

    looks like code.",1541547579.0
BerryGuns,"It looks like you'd want to use the model for the LSD.test.

    res <- LSD.test(model, DFerror = 72, MSerror = 68.73, alpha = 0.05)

As it is Pop isn't defined as it's part of the  'BeresfordNCSRPYield' dataset, Trt also isn't defined for the same reason.

The reason

    model<-aov(Pop~Trt,data = BeresfordNCSRPYield)

doesn't give an error is because this function allows you to define the dataset.

Edit: for what it's worth the naming convention for that dataset will be a bit of a pain to work with.",1541518398.0
slammaster,"    BeresfordNCSRPYield <-read.csv(""C:/Users/Desktop/BeresfordNCSRPYield.csv"")
    BeresfordNCSRPYield=transform(BeresfordNCSRPYield,Plot=factor(Plot),Trt=factor(Trt))
    BeresfordNCSRPYield$Pop<-as.numeric(BeresfordNCSRPYield$Pop)
    library(agricolae)
    model<-aov(Pop~Trt,data = BeresfordNCSRPYield)
    summary.aov(model)
    res <- LSD.test(Pop, Trt, DFerror = 72, MSerror = 68.73, alpha = 0.05)
      Error in match(x, table, nomatch = 0L) : object 'Pop' not found
    res <- LSD.test(Pop, Trt, DFerror = 72, MSerror = 68.73, alpha = 0.10)

There, that's readable.  I'm guessing where the error fell since you didn't include it in line.

I don't know anything about LSD.test, but the [help file](https://www.rdocumentation.org/packages/agricolae/versions/1.2-8/topics/LSD.test) suggests that you should pass a model to it.  

You're trying to pass the elements of a dataset (`Pop` and `Trt`) without passing the dataset, but looking at the helpfile the LSD.test doesn't accept a dataset, so I think you're using LSD.test incorrectly.
",1541518400.0
Darth_Marrr,"This is a broad question and I expect to be down voted:

How could I incorporate a Slanky graph into finance? Specifically the financial trading realm?

&#x200B;",1541542833.0
Loco_Mosquito,"I've been using `ggthemr` but these are great, thanks for sharing!",1541456373.0
FrickelFrackel-,My R coding has never been the same since finding this package. ,1541471872.0
,"That package is one my favourites. 

Inspired by it I made my own colour palette package based on the Harry Potter franchise: https://github.com/aljrico/harrypotter

I thought you might like it",1541494690.0
Core_Four,Awesome! I'm also a big fan of the XKCD package. ,1541462081.0
1337HxC,"There's a small handful of people who run analysis on my floor. About 50% of us stick to Wes Anderson palettes unless the PI throws a hissy. Although, I usually take the ""accessibility!"" approach in defending it - like, why would I make this figure red and green?",1541481504.0
nu_naut,"I'd approach it as follows:

\- the prior is a beta distribution, parameterized with some value alpha and some value beta. These are what we're trying to solve for.

\- assume the participant is rationale (totally invalid if he's human, of course -- refer to Dan Ariely)

\- at every T intersection, the participant gains one more data point. If the sign proved correct, alpha <- alpha + 1. If the sign was incorrect, beta <- beta + 1.

\- Participant makes his decision based on whether the mean of the posterior distribution after the previous move is >= 0.5 (believe the sign) or < 0.5 (disbelieve the sign).

\- Mathematically, the cumulative sum of ""sign"" gives the incremental update to the ""alpha"" that we're trying to solve for, and the cumulative sum of (1 - sign) gives the incremental update to ""beta"" that we're trying to solve for.

\- So, for example, we know that the participant didn't trust the first sign, so we infer (alpha) / (alpha + beta) -- i.e. the mean of his prior, is < 0.5.

\- His distrust (mean < 0.5) persisted all the way to the 8th sign, where his prior plus experience says (alpha + 4) / (alpha + beta + 7) < 0.5. But that 8th sign being correct swings the mean of his belief to (alpha + 5) / (alpha + beta + 8) >= 0.5 .

\- a heuristic search for alpha and beta using the 31 different inequalities available would give you a solution if the assumptions are valid.",1541508161.0
shujaa-g,"Pick your favorite

    # base R
    data$group_prevalance = with(data, ave(individual_outcome, group_id, FUN = sum))

    # dplyr
    data = data %>% group_by(group_id) %>% 
      mutate(group_prevalance = sum(individual_outcome))

    # data.table
    setDT(data)
    data[, group_prevalance := sum(individual_outcome), by = group_id]

(edit: deleted extra `)`)",1541444365.0
sampling_life,"Not near a computer to verify but I think you can do: data %>% group_by(ground_id) %>% mutate(outcome_count = sum(individual_outcome))

I think sum works with group_by

Edit: looks like it does.",1541445330.0
shujaa-g,"From `?map`, you can use `map_int` to return an integer vector, this let's us skip the `as_vector`:

    SIMFactors %>% map_int(length) %>% prod

The piping is of course optional, you could just as well do these:

    prod(map_int(SIMFactors, length))
    SIMFactors %>% sapply(length) %>% prod

But I think you'll find it hard to beat `base::lengths` as a simplification:

    prod(lengths(SIMFactors))    # I find this very readable
    lengths(SIMFactors) %>% prod #  but whichever you prefer

In words:

    prod(lengths(SIMFactors)) # Product of the lengths of the SIM factors
    lengths(SIMFactors) %>% prod # Take the lengths of the SIM factors, then multiply them together
    SIMFactors %>% lengths %>% prod # You know  that SIMFactors things? Yeah? Well look at the lengths 
                                   # all of them, and then multiply those lengths together.

",1541439550.0
Kroutoner,"`SIMFactors %>% map_int(length) %>% prod`

Purrr provides variants of map depending on the output type. For example map_int returns a vector of integers rather than the typical list output.

",1541439923.0
kenderpl,"The benefits of purrr are in its internal consistency - for example, *apply don't have fixed order of arguments (Map aka mapply has them flipped). Furthermore, the benefits of chaining are not apparent until you have more functions in the pipe. Deeply nested functions are not pleasant to parse.",1541444602.0
the1whowalks,"As someone working on a project with permutation tests, I found this extremely helpful. 

Briefly, how might two samples (say from repeated measures at time 1 and 2 on the same 10 participants) affect the coding? ",1541453300.0
KoolAidMeansCluster,"close...  
df1 %>% select(-c(names(df2)))",1541436563.0
Statman12,"Not sure that it's particularly pretty, but this seems to work:

    df1 %>% select(  names(df1)[!(names(df1) %in% names(df2))]  )",1541432909.0
SemanticTriangle,?tidyr::separate,1541414979.0
WasteCadet88,"sapply(myvector,FUN=function(string) strsplit(string,split="","")[[1]])",1541427503.0
SemanticTriangle,"Back off on the renderPlot and just ggplot directly:

    ggplot(dataIn, aes(x=date, y=name)) + ...

make sure both date and name are columns in dataIn.

Generally, you should ggplot from a single data frame to make your life easy.",1541412702.0
noahpoah,"Andrew Gelman [blogged about this](https://andrewgelman.com/2018/10/28/mrp-rpp-non-census-variables/) recently, linking to [a paper](http://www.stat.columbia.edu/~gelman/research/unpublished/SSRN-id1644213.pdf) that may be useful for you. There are some other potentially relevant papers linked in the comment thread there, too.",1541432852.0
mouse_Brains,"You can check out this repo of mine. I used `pdf_document: includes: before_body:` on the yaml to add a latex title page, independent of all other options. Because I believe when you set toc: true it'll always place is to the same place regardless of where you try to place it

https://github.com/oganm/ThesisProposal/blob/master/proposal.Rmd#L7

The latex file is a separate document in the same repo

https://github.com/oganm/ThesisProposal/blob/master/title.sty",1541394296.0
joyoftech,I guess you could use bookdown (probably overkill though).,1541405266.0
Vicroline,"I am not familiar with R markdown but I see you use Latex code here. Have you tried making a separate .tex file for your title and include it in your markdown code? 

You could try something like this: make a ""title.tex"" file with your desired title page information, place it in the same folder and include it in your markdown report with:

`\input{./title.tex}`

`\newpage`",1541417279.0
geosoco,"The page is using javascript to load this data and build the HTML. Just loading the page as html won't work. 

if you look in the firefox network debug console (menu > web developer > network). Reload the page, and look for some json requests. you may be able to use the urls to request raw json data for whatever you want. ",1541399062.0
CohoCharlie,"I think the site's content is being loaded via a script. The content of ""ism-sidebar"" div class isn't in the source, rvest isn't finding it because it doesn't exist in the HTML, it's created by javascript somewhere.

Might want to look into a more robust webscraper like selenium.",1541398221.0
Darth_Marrr,A [Box-Cox](https://ncss-wpengine.netdna-ssl.com/wp-content/themes/ncss/pdf/Procedures/NCSS/Box-Cox_Transformation.pdf) transformation may work.,1541375061.0
rayray0313,You can log transform if positively skewed. Square root transformation may work as well. ,1541375314.0
Darth_Marrr,Do you know why do you want normality of your predictors?,1541377528.0
villain170,You would need to do some type of join.  I assume you might be able to join them by date and or time?,1541356794.0
craigjclemson,"does anyone know if this package works on a Team Drive or a sheet that is shared with you, but you don't own? ",1541356877.0
Lysistratus,"You could also use visual studio 2017 to do this if you’re in a Windows environment.  

https://www.i-programmer.info/news/90-tools/11891-atom-v-visual-studio-code-the-unexpected-consequence-of-consolidation.html",1541282835.0
Turin_Martell,"I'm not an engineer but you might want to check out the [Beaker notebook](https://www.youtube.com/watch?v=iMPfLz6kKv8). It allowed you to switch between R and Python objects (might have been just dataframes) within a notebook. Unfortunately TwoSigma [archived it](https://github.com/twosigma/beaker-notebook-archive) in favor of building Jupyter [notebook extensions](http://beakerx.com/). The only problem was it was a pain to install and the API was a little clunky, but it really filled a niche that reticulate or rpy2 (?) doesn't come close to addressing in my opinion. (Then again I haven't gotten reticulate to even work yet, so take that with a grain of salt.) Right now I do all my non-modeling work in R, then export a csv to use sklearn in Python. I'd love to just run that in another cell within the same notebook with object persistence. 

Let me know if you need someone to provide feedback or help because I thought about building something similar, but decided it was beyond me. ",1541293507.0
shaggorama,https://atom.io/packages/hydrogen,1541295366.0
Phoogi,Make sure you look at Spyder 3.  I think your talent would be best used enhancing or forking an existing project.,1541340068.0
Lysistratus,"Cool. I use 2017 pro at work.  AFAIK you can’t use code professionally, but haven’t read the Eula recently.  But I honesty love the IDE and where MS is going. Also check out their data science VM they have. It’s also free like code. 

",1541341597.0
Ader_anhilator,I'm failing to see the benefit. ,1541279074.0
jeremyfirth,"You are wary, which means cautious or nervous. Weary means tired.",1541309963.0
RememberToBackupData,"Aside from what everyone else is saying, I tried Atom before and would not use it again. It was very slow for me compared to good ol' Emacs.",1541287072.0
ColorsMayInTimeFade,Nope nope nope. Anaconda is a no go. I want 100% of dependencies or none of them. Docker isn’t that hard.,1541302024.0
Darth_Marrr,"      propt <- prop.test(0.326087, 45, p= 0.21, alternative = c(""two.sided"")) 

Your code above has p = 0.21, which is your null proportion or in the output is recorded as  `null probability = 0.21` . Therefore, you are not testing whether zero is in a 95% confidence interval, you are testing whether 0.21 is in your 95% confidence interval. In your case it isn't.

Now, had you been comparing two proportions, than typically we would see a significant result if the difference is ""significantly"" different from zero i.e. zero is not in your 95% confidence interval, but that is a different test entirely.

&#x200B;",1541268310.0
efrique,"> I thought if it captured zero the difference was not significant. 

What is the null hypothesis for your proportions test?
",1541310478.0
Oshobooboo,"Just a guess, as I've never used that exact test, but it might be doing an inverse logit transformation where a negative proportion would be impossible to obtain for the lower limit.",1541267425.0
Poppin_Juicy_Zits,"Confidence intervals are not always congruous with formal statistical tests. This is because the standard error used to calculate the confidence interval is based on empiric data, while the standard error of the statistical test is derived under the assumption of the null. 

ALWAYS rely on the formal statistical test for inference, NOT interval estimates. 

",1541269391.0
villain170,https://www.r-bloggers.com/implementing-the-gradient-descent-algorithm-in-r/,1541259249.0
unclognition,"Given a vector of of elements to be rearranged...

    perms.with.reps <- function(input){
      factorial(length(input)) / prod(factorial(table(input))
    }
    mississippi <- strsplit('mississippi','')[[1]]
    perms.with.reps(mississippi)

I think.

The number of permutations for the case of all unique items is given by `factorial(length(input))`, but when some items are identical, that method needs to be adjusted to account for the duplicates. How many duplicates are there? Well, consider `input = c('C','A','T','T')`. In that case, CAT^(a)T^(b) is identical to CAT^(b)T^(a), where I'm using the superscript to disambiguate identical elements. For each arrangement of CATT, there are two identical options, one for each possible arrangement of the two Ts. To scale that up to the general case, `factorial(length(input))` is overcounting the number of permutations by a factor of the number of permutations of each of the sets of identical elements.

&#x200B;

So for MISSISSIPPI, we have 11! permutations, but that is inflated by 4! ways of arranging the Is, 2! ways of arranging the Ps, and 4! ways of arranging the Ss. So there are 11!/(4!2!4!) ways of arranging the letters, accounting for duplicates. That's what my function above is doing.",1541203528.0
efrique,"> if you have three blue balls and two red balls, how many permutations are there given that balls of the same color are interchangeable?

5!/(3! 2!)

And if you had 7 balls -- 3 red, 2 blue, 2 green then it would be 7!/(3! 2! 2!)

This is just a straight multinomial coefficient 

https://en.wikipedia.org/wiki/Multinomial_theorem#Multinomial_coefficients",1541206669.0
WayOfTheMantisShrimp,"Bootstrap sampling does not tell you if your sample is representative of a population; that is where you have to hope your experimental design was proper. This is why we do multiple stages/scales of experiments, because that is the only way to become more confident that we are reflecting the population. 

You can try k-fold cross-validation to test how well your fitted model might generalize, but this still relies on having a good quality sample. (Process is to fit a model on a subset of your data, and then test how well it can predict the remaining portion of the sample)

**Bootstrap methods may help you test your model** for robustness. The general outline is training the model on multiple data sets that were re-sampled from your pilot sample, and then comparing how the model adapted to all of the slightly different distributions.

*Interpretations of bootstrap simulations:*

If the model stays fairly stable, despite being fed training sets that might skew a bit one way or another, then that is probably a good model that will generalize well, even if the population distribution is a little different than the pilot sample. That confidence becomes invalid if you find that the population exhibits behaviour entirely unlike your pilot sample.

If your fitted model varies significantly, or sees dramatic changes in performance between differently re-sampled training data, your model fitting process or choice of model might be prone to over-fitting. This is not a problem if the population distribution is *exactly* like your pilot sample; it may lead to disappointing performance if the population has any deviations from the pilot sample (this is generally undesirable). This scenario still doesn't mean that your pilot sample is good or bad, but it may lead you to conclude that your modelling process might not be ideal for the chosen application ... but it might also mean you simply don't have enough data at this stage to make your chosen model effective. This is why is pays to know the idiosyncrasies of any model you chose to try.",1541218988.0
Darth_Marrr,">1) without putting in to much effort, can I use bootstrapping to simulate what the results might be if I were to carry out the experiment on a larger sample (the whole population)? 

Bootstrapping is generally used to estimate standard deviations/variances on the sample mean as well as calculating other statistics. 

Do you need a particular statistic to be estimated?

It is not clear what you are trying to achieve. You want to use bootstrapping to ""...simulate what the result might be"" - What result?",1541217506.0
sohaibhasan1,"You don't need to write a function or use apply. You just need to use the ifelse function. You can do this the dplyr way by using ifelse within a mutate. Something like this:

Data <- mutate(Data, newvar = ifelse(x==1, 'control', 'experiment'))

What's happening here is you're using the mutate function, which requires a dataframe as the first argument, and then any number of ""mutations"" to the frame. The one above creates a new variable by using ""ifelse"", which takes a condition, followed by what the output should be if the condition is met, followed by the output of the condition is not met.

The base R way is similar:

data$newvar <- ifelse(data$x==1, 'control', 'experiment')",1541184342.0
dm319,"You'll probably hear this a million times over, but the imperative if/elseif/else style of programming is not really R's style - always best to attempt a functional / vector approach first.  Here's my first thought:

    test.df <- data.frame(x1 = c(1,2,1,2,1),
                          x2 = c(""a"", ""b"", ""c"", ""d"", ""e""))
    
    a <- vector()
    a[1] <- ""experiment""
    a[2] <- ""control""
    a[4] <- ""pants""
    
    test.df$group <- a[test.df$x1]

This has the advantage of using an associative array (ok not exactly - in this case you can use a plain vector as the key is a number) called \`a\`.  When you index this array using the square brackets, you are returning the text strings that you assigned.  The advantage of this method over \`ifelse\` is that it is more explicit, and will return an \`NA\` if the number does not match an assigned text string.  It also allows you to use the same method for columns with more than 2 types of values.",1541190669.0
NinjasInTheWind,"    > test.df <- data.frame(x1 = c(1,2,1,2,1),
    +                       x2 = c(""a"",""b"",""c"",""d"",""e""))
    > test.df$group <- ifelse(test.df$x1 == 1,""experiment"",""control"")
    > test.df
      x1 x2      group
    1  1  a experiment
    2  2  b    control
    3  1  c experiment
    4  2  d    control
    5  1  e experiment",1541187240.0
YepYepYepYepYepUhHuh,"If you're not particular about which is numbers are associated with which factor levels, you can always just extract the levels of a factor to give numbers 

    as.numeric(as.factor(test.df$group))

and then add it to your data frame using the methods described in other comments (either mutate or base R).",1541185410.0
Ader_anhilator,"Use data.table: data[ , newcol := ifelse()]",1541193018.0
ryapric,"While I'm happy you have found the right solution, no one has actually told you what you were doing wrong, so you don't repeat the same mistake later. Please see [a previous answer I gave](https://www.reddit.com/r/Rlanguage/comments/9hg2l4/ifelse_statement_for_multiple_variable_assignments/e6c4tpo) on the use cases for `if` statements vs. the `ifelse()` function.",1541349488.0
redstoneglowstone,"There are a few issues with the code you present - I recommend you go through an R tutorial as a first step, as you seems to have a few misconceptions. To list a few: your variable x in your function has no purpose, ""=="" is a logic operator, not an assignment operator, and if you changed it to an assignment you wouldn't be placing the value in the correct spot in the df.

&#x200B;

For this, I like to use the inner\_join() function from dplyr. 

`require(dplyr)`

`test.df <- data.frame(x1 = c(1,2,1,2,1), x2 = c(""a"",""b"",""c"",""d"",""e""))`

`grouping.df <- data.frame(x1=c(1,2),group=c(""experiment"",""control""))`

`result.df <- inner_join(test.df,grouping.df)`",1541546527.0
DoItForTheGild,Look into the mutate function within the tidyverse. ,1541184204.0
endaemon,"While I agree with /u/uniquetutlelove that having a consistent y-axis across facets would be better, if you simply must allow the y-axis to vary across plots, use the argument `scales = ""free_y""` in your `facet_wrap()` call.",1541169235.0
uniqueturtlelove,"If it were me I would probably just log scale the data. Sorry for the simple and short answer but have you tried that?

Also try this,

&#x200B;

[https://www3.nd.edu/\~steve/computing\_with\_data/13\_Facets/facets.html](https://www3.nd.edu/~steve/computing_with_data/13_Facets/facets.html)

p + facet\_wrap(\~color, scales = ""free\_y"")",1541168773.0
jackbrux,"```facet_grid``` won't do this, you need to make 2 plots, one with ZTP removed, and one where ZTP is the only treatment. Then put the graphs side by side with \`cowplot\` or \`grid\`.",1541161424.0
kenncann,"I think another option would be to check out facet_wrap(). I believe it will ignore the options when data isn't there (please correct me if I'm wrong) and you can organize the columns/rows so that that last plot is just separated on its own on the last line.

You can [read more here](https://stackoverflow.com/questions/20457905/whats-the-difference-between-facet-wrap-and-facet-grid-in-ggplot2where) the top response mentions what I said about ignoring plots when data isn't there",1541168935.0
guepier,"Do not, I repeat, ***do not* generate SQL queries like that**. It's [the most common security vulnerability](https://www.google.com/search?q=owasp+top+10+security+vulnerabilities) in the world. It is really that dangerous. And there's no excuse for it, even if you think your script is just executed locally, there's no input from untrusted sources, and security concerns are not relevant. Don't. Code use cases change quicker than you can say “hopscotch”.

[Use the library's functionality for *prepared statements* instead.](https://cran.r-project.org/web/packages/DBI/vignettes/spec.html#_bind_values_to_a_parameterizedprepared_statement_)",1541146902.0
MindlessTime,"Maybe stupid question:

Why is `PASTE` capitalized? I’m familiar with the (lowercase) `paste` function, but not an (UPPERCASE) `PASTE` function. Is that a different function that is part of another package?",1541134795.0
bluestorm21,"Why would you loop when paste() is already vectorised?

It's not a question of purrr, lapply, or for, it's a question of how to use base operations as intended. And how to query the DB the fewest number of times to get the data you need.

Just pass all the client IDs to paste at once, collapse into a single string and use `IN` instead. 

If they must be individual queries, still use paste to generate all the queries first and then loop over those instead. That is still very inefficient, but at least you're not calling paste n times as well.

As others have mentioned, there are safer methods than using paste, and you might consider those as well. 
",1541178472.0
another30yovirgin,"I'm not entirely sure I understand the problem, but what's the problem with a loop? Loops are quite useful, and there's no efficiency gain using *ply functions if you don't care about the output. As an alternative to paste, I find sprintf extremely useful in building SQL queries.

     generate_id <- dbGetQuery(conn2, ""SELECT DISTINCT client_id FROM table_b"")$client_id

     for(a in generate_id) {
       dbGetQuery(conn2, sprintf(""INSERT INTO SELECT * FROM table_c WHERE client_id = %s AND event_date = CURRENT_DATE - 1"", a))
     }",1541131808.0
Economist_hat,"These are good, solid basic skills to practice when you ""need"" a loop:
Think about using lapply (and the apply family and also`{purrr}`)
Consider using `{data.table}`. It has the convenient function `rbindlist()` which lets you lapply then immediately collapse your stack of `data.table`s.


    library(data.table)
    MakeAndGetQuery <- function(id) {
      q <- paste0(""INSERT INTO SELECT * FROM table_c WHERE  client_id = "",
                   id, 
                   "" AND event_date = CURRENT_DATE - 1"")
      as.data.table(dbGetQuery(conn2, q))
    }
    all_yer_data <- rbindlist(lapply(generate_id$client_id, MakeAndGetQuery))
    setkey(all_yer_data, id, ...)

",1541114661.0
dmuney,"I’m not sure that it’s apa, but check out stargazeR, it’s a package that converts tables to the format that you would generally see in published papers",1541120593.0
Soctman,"[Try this link.](https://web2.uconn.edu/writingcenter/pdf/Reporting_Statistics.pdf)

I typically report larger regressions using tables, but if you have just a few predictors, reporting the significance of the coefficients as well as the fit of the model (a la chi-squared and r-squared) you should be just fine. The link provides a good example.",1541128378.0
Thaufas,"The answer depends on how you define *equality*. A simple way is to use the `match()` function.

        student1_grades <- as.factor(c(""A"", ""B"", ""C""))
        student2_grades <- as.factor(c(""B"", ""B"", ""C""))
         
        match(student1_grades, student2_grades)
      #  [1] NA  1  3

        match(student2_grades, student1_grades)
      #  [1] 2 2 3
         
Note that `match()` will return the position of the first match of element i in vector one in vector 2. If it doesn't find a match, it returns `NA`. However, once it finds a match, it returns the first position where it occurred and stops checking the vector. Is that what you want?",1541127524.0
nyct0phile,"FYI, I need to keep them as factors, so converting to another type won’t work.",1541100722.0
GildedFuchs,"What exactly are you trying to do? Your question is a bit vague and non-specific. You'll get better answers if you let folks know your goal. There's lots of great NLP libraries. I like both packages, also quanteda and udpipe. ",1541205223.0
flyos,"This should get you started I think:

    library(tidyverse)
    strings <- c(""p2m2s3a1c"", ""p5s2"", ""p3m4"", ""p4c"", ""p3s2a2"")
    df <-
      strings %>% 
      str_extract_all(""[a-z][0-9]"") %>% 
      map_dfr(~ separate(as_tibble(.), value, into = c(""param"", ""value""), sep = 1), .id = ""ID"")

The rest should be handled with some use of spread().

EDIT: You can use str_detect(strings, ""c"") to handle the ""c"" column once everything else is formatted.",1541097008.0
samclifford,Each column can be defined by searching with regex for that letter and if it's found replacing the entire string with the number immediately after the letter. Not sure exactly what the pattern and replacement strings would need to be though. ,1541094418.0
another30yovirgin,"This should do it. You could probably do it without stringr, but this works pretty well:

     library(stringr)
     x <- data.frame(original = c(""p2m2s3a1c"", ""p5s2"", ""p3m4"", ""p4c"", ""p3s2a2""), stringsAsFactors = FALSE)

     for(a in c(""p"", ""m"", ""s"", ""a"")) {
       temp <- sapply(str_extract_all(x$original, paste0(a, ""\\d{1,}"")), function(b) ifelse(length(b) < 1, 0, b))
       x[[a]] <- as.numeric(gsub(a, """", temp))
     }

     x$c <- grepl(""c"", x$original)

Note that you only need {1,} if you think there might be more than one digit. If it's only one digit, \\\\d should be enough.",1541130024.0
RememberToBackupData,"You only need to transform the C column. It could be done in the function, of course, but that just adds so much unnecessary stuff that could be done in a couple of lines of `dplyr`.

    library(stringr)
    
    data <- c(""p2m2s3a1c"", ""p5s2"", ""p3m4"", ""p4c"", ""p3s2a2"")
    #> [1] ""p2m2s3a1c"" ""p5s2""      ""p3m4""      ""p4c""       ""p3s2a2""  
    
    recode <- function(vec, letter, fill = ""0"", clear_alpha = TRUE) {
        matches    <- str_extract_all(vec, paste0(letter, ""\\d?""), simplify = TRUE)
        transposed <- t(matches)[1,]  # From a matrix into a normal vector.
        
        filled     <- str_replace_all(transposed, ""^$"", fill)
        
        if (clear_alpha == TRUE) {
            filled <- str_remove_all(filled, ""[[:alpha:]]"")
        }
        
        return(filled)
    }
    
    data.frame(
        p = recode(data, ""p""),
        m = recode(data, ""m""),
        s = recode(data, ""s""),
        a = recode(data, ""a""),
        c = recode(data, ""c"", fill = """", clear_alpha = FALSE)
    )
    
    #>   p m s a c
    #> 1 2 2 3 1 c
    #> 2 5 0 2 0  
    #> 3 3 4 0 0  
    #> 4 4 0 0 0 c
    #> 5 3 0 2 2  ",1541104703.0
ayrankills,"Here is an idea:

&#x200B;

    library(tidyverse)
    
    pattern <- c(""p"", ""m"", ""s"", ""a"", ""c"") %>%
      paste0(""("", ., ""([0-9]*))?"", collapse = """")
    
    c(""p2m2s3a1c"", ""p5s2"", ""p3m4"", ""p4c"", ""p3s2a2"") %>%
      str_match(pattern) %>%
      as.tibble %>%
      mutate(
        p = as.numeric(V3),
        m = as.numeric(V5),
        s = as.numeric(V7),
        a = as.numeric(V9),
        c = V11=="""") %>%
      select(p:c) %>%
      replace_na(list(
        p = 0,
        m = 0,
        s = 0,
        a = 0,
        c = FALSE))

&#x200B;",1541098249.0
Giftbox,"This should do what you want using a regular expression.
    
    library(glue)
    library(tidyverse)
    
    raw = c('p2m2s3a1c', 'p5s2', 'p3m4', 'p4c', 'p3s2a2')
    
    data <-
      c('p', 'm', 's', 'a') %>% 
      setNames(., .) %>% 
      map_dfc(~str_extract(raw, glue('(?<={.x})\\d'))) %>% 
      replace(is.na(.), 0) %>% 
      mutate_if(is_character, as.integer) %>% 
      mutate(c = str_detect(raw, 'c'))",1541110856.0
shujaa-g,"A little bit of sample data (just a few rows, made-up numbers/categories are fine) would go a long way to making this easier to understand.

Also please provide more detail on both your response variable and your predictive use-case. That you have ~100 binary predictors measured monthly is nice and clear. Tell us more about the status indicator you want to predict:

- is it also binary?
- is it something irreversible, when it goes to 1 it nevers goes back to 0 (e.g., death?)
- are your ~100 predictors time-varying from month-to-month, or are they static for the whole year for an individual?
- do you have censored data (e.g., some individuals only have data for less than 12 months)
- are you looking to predict beyond the 12-month span?
- for an individual, do you  need to predict just the next month, or the next several months?
- theoretically (not statistically, medically) how much previous data matters. Does the status in August depend pretty much only on July status, or maybe also on June and May? How far back is it reasonable to look?
- do you expect patterns that differ by individual? E.g., some people (based on indicators) might tend to alternate every other month 1 0 1 0 1..., whereas other people might be more streaky 1 1 1 1 0 0 0 ...? Or would that be really surprising.
",1541087961.0
Trek7553,"I am somewhat of a novice, but here's how I would approach it.

Think of each row as totally independent. If you want to incorporate the January outcome into the February data, add a column for either ""Last month outcome"" or ""January outcome"". Which one you choose will depend on how much seasonality your data has. Repeat this for as many data points as you want from previous months for that member.",1541087393.0
crackrocknbach,"You are correct. Since your data are longitudinal, it is important to model the covariance structure. You can specify covariance structures for marginal models (ie mean models) with a the gls() function. If you'd rather fit a mixed model (models individuals) you can use lmer() or lme(). Lmer is easier but only estimates an unstructured covariance matrix, which can sometimes cause convergence issues. 

Ps those functions are for continuous outcomes. You will need to use the generalized versions if your outcome isn't continuous. ",1541096389.0
zdk,"I prefer to not supply dplyr solutions unless specifically  asked for.

&#x200B;

`df$ColB <- df$ID[match(newdf$ID, df$ID), 'ColB']`",1541122223.0
lederi,If you use dplyr left join you will get all the data. Then you can just select the new column using dplyr again,1541083326.0
lederi,"yeah do mutate(ColB = ifelse(is.na(ColB.y), ColB.x, ColB.y))",1541083807.0
shaggorama,Maybe collapse the stack into an animated gif?,1541092748.0
Krynnadin,Tidyr::gather,1541039718.0
Economist_hat,"If you have small amounts of data, use 
`tidyr::gather`

If you have large amounts of data, use 
`data.table::melt`

Probably avoid `reshape2::melt`. Purely out of the fact that you should learn the (more) reusable syntax  in `tidy` or `data.table`.",1541115138.0
efrique,There's a variety of potential ways. One way: see `?stack`,1541035196.0
roarixer,"reshape2::melt

",1541049423.0
another30yovirgin,"I still just use reshape. The documentation is bad, but once you get the hang of it, it works just fine.",1541049757.0
Oshobooboo,"Some quick tips:

The lme4 package might be friendlier than nlme, and I think it's updated more and find it has better online help(?)

I would build up your model by first including the variables that seem more like a 'baseline' model, then add in variables that you hope to predict beyond those variables. For example, you might enter gender earlier, then work in some time varying variables later to show what they predict beyond that. Often these may be level 2, but not always (eg, you might control for weekends or time of day earlier in your model building).

When to add random effects is more in depth and I would read some materials online discussing that more. I try to err on the side of including them if it seems to make sense in the model, but you also want to keep the number of random effects limited because it'll creat estimation and convergence problems.",1541030930.0
cokechan,https://www.amazon.com/Analysis-Regression-Multilevel-Hierarchical-Models/dp/052168689X is the definitive text on the subject. I highly recommend this book to understand the fundamentals of multilevel modeling.,1541043406.0
Slabs,"Technically speaking, it is not appropriate to test for the random intercept by comparing likelihoods (e.g. using (anova(m1,m2), see the top answer here:

[https://stats.stackexchange.com/questions/56150/how-can-i-test-whether-a-random-effect-is-significant](https://stats.stackexchange.com/questions/56150/how-can-i-test-whether-a-random-effect-is-significant)

&#x200B;

It is more appropriate to use a simulation-based approach to testing the need for a random effect (e.g. using the  RLRsim package in R).

&#x200B;

That said, using your approach, I would do as you said -- test for random intercept first, then for random slope. There are no hard-and-fast rules for adding other explanatory variables. I would probably add them all in first, and then run various other models to check sensitivity of results to model selection.",1541079670.0
rayray0313,"I have experience with lme. I can help. You can start adding random slopes and see if it reduces aic. If not, then don’t add them. Then you can add fixed effects to see if they contribute. Let me know if you have questions. ",1541028718.0
YepYepYepYepYepUhHuh,Are all of the values in X also present in Y?,1541027675.0
CohoCharlie,If they have the exact same columns why not just use updated data frame? ,1541030472.0
mouse_Brains,It is unclear what you're trying to do. Please try to add sample code and data for questions like this so we can unambiguously understand what you're trying to do and what is going wrong.,1541030589.0
Jyqft,"You could do this, assuming `x` and `y` represent the two data frames in your example:

    library(dplyr)

    x %>%
      select(-Hours, -Minutes) %>%
      left_join(y, by = c(""Line"", ""Color"", ""Name""))",1541178596.0
aizheng,Could you show what your data actually looks like? I don't really understand what the problem is.,1541005809.0
IceNinetyNine,"for the PCA I do:

`pca=princomp(data, cor=TRUE)`

the error is princomp can only be used with more units than variables.

So I tried using prcomp:

`pca=prcomp(data, scale.=T)`

Error in prcomp cannot rescale a constant/zero column to unit variance.

Does this mean some of my gene families all have the same number?

&#x200B;

&#x200B;",1541006042.0
COOLSerdash,"Fellow R-users.

I was a bit taken aback that the newest ggplot2 update broke my script. I therefore take the liberty of cross posting my question from Stack Overflow (which is allowed according to the side bar). Please notify me should this post be in conflict with the rules of this subreddit. Thanks for reading. ",1541004219.0
coriscause,"I think doing geom\_raster instead of geom\_col might work, but I'm not sure why geom\_col does not work.",1540998875.0
Thaufas,"I've used ggplot2 for visualizations like the kind you describe.

[Example Figure](https://i.imgur.com/P3b28Zz.png)

    NDP <- 150
    WEEKS <- 1:15
    MODS <- 1:6
    XLIM <- 1:16
    YLIM <- 1:16
    
    df1 <- data.frame(emp = sample(LETTERS[1:15], NDP, replace = TRUE),
                      wk = sample(WEEKS, NDP, replace = TRUE),
                      lrn_mod = as.factor(sample(MODS, NDP, replace = TRUE))
    )
    df1
    
    ggplot(df1) +
      geom_tile(aes(x = wk + 0.5, y = as.numeric(emp) - 0.5, fill = lrn_mod, height = 0.8, width = 0.8)) +
      guides(fill = guide_legend(""Learning\nModule"")) + 
      # geom_raster(aes(x = wk, y = emp, fill = lrn_mod), hjust = 1, vjust = 0) +
      theme(panel.grid.major = element_line(colour=""black"", size = 1),
            panel.grid.minor = element_line(colour=""black"", size = 1)) +
      scale_x_continuous(breaks = XLIM, minor_breaks = NULL, limits = range(XLIM), labels = format(XLIM, width = 2)) +
      scale_y_continuous(breaks = YLIM, minor_breaks = NULL, limits = range(YLIM), labels = LETTERS[XLIM]) +
      xlab(""Weeks"") +
      ylab(""Employee"") +
      theme(axis.text.y = element_text(size = 13, hjust = 0, vjust = -0.5),
            axis.text.x = element_text(size = 13,
                                       hjust = -0.9,
                                       vjust = 0),
            axis.title = element_text(size = 20),
            axis.title.x = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0)),
            axis.title.y = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0)),
            plot.title = element_text(size = 30,
                                      margin = margin(t = 0, r = 0, b = 15, l = 0)),
            plot.margin = margin(1, 1, 1, 1, ""cm"")) +
      ggtitle(""Learning Module Training Summary"")",1541139202.0
vonkrumholz,"Stick with Rmarkdown. Sure you have to learn a few new things and think about formatting text within your code now. Did you use LaTeX or Markdown on your last go? Markdown is much simpler and less frustrating to work on. 

The benefits vastly outweigh the downsides when it comes to sharing results and quickly replicating reports or automating reports. I have a few automated Markdown reports that query a database, munge the data, and produce tables and graphs once a week. The graphs are of the interactive plotly type, which end users like. I output these reports as HTML files onto a Shiny server (this will work with any server though, just needs to serve HTML) within our internal network which then automatically renders the report (no emailing required). End users simply bookmark the URLs and check when needed. 

Rmarkdown will also produce nice PDFs you can easily share as well as Word Docs. If your team  uses GitHub, another way to avoid the ""run code then email"" workflow is to have your report code in a repo, run the code, then push the results to Git as a GitHub document. Team members can then navigate to this same location report after report.

I can't think of any alternatives to Rmd or Jupyter, probably because these are both mature and well implemented projects that require a lot of non standard functionality. ",1540998655.0
Core_Four,"markdown or jupyterlabs

**e** r notebooks with r studio is the path of least resistance honestly",1541018367.0
icybrain,"RMarkdown/knitr will be your friend

[Some reading that you would probably find helpful](https://bookdown.org/yihui/rmarkdown/)",1541025684.0
hdgdtegdb,"Install RStudio and try R notebooks.  You don't need much/any markdown knowledge to create a basic notebook, and you can save as a self-contained html file for sharing when complete.

&#x200B;

Edit: Spelling",1541014191.0
mrcflckrz,"You only want to show them results, they don't have to work with it?

RMarkdown might help you to build reports.

Or just setup a shiny app, where they can view (and download) the results in a webpage. That would come handy for them.

&#x200B;",1540997924.0
CJL_LoL,We're experimenting more and more with rmarkdowns to produce html docs at the moment. You can build a theme that you can apply (essentially CSS) so you can keep on brand. Would definitely recommend ,1541017001.0
efrique,"> will put the results in report format (eg, a single Word doc, webpage, or PDF)

RMarkdown will sure do that

I found it hard to learn by reading the manual. But I was used to Markdown (its what reddit uses and what Stackoverflow/Stackexchange uses), and used to sprinkling LaTeX through Markdown (math.SE and stats.SE both do that for example), so I just started using it to write documents, and that was simple enough to get started with. 

Actually, more than being simple, it was a godsend because I was working with someone who was writing in Word but I hate its mathematics; using RMarkdown I could produce all the mathematics I wanted by writing pretty much exactly as if I was writing a post on crossvalidated and *bam* end up with something that was a proper Word document, and the other person could just copypaste stuff into their own documents and edit the mathematics I was sending right there in Word. We each got to remain happy and sane. I have been working with someone else on something in Word for a longer period, and I am planning to shift all my side back into RMarkdown (since I am already working in R for it) and just produce word output for them, too. It's so much easier to work with people who want to use Word for me now.

[For a lot of the stuff I need to do, I find it faster and more pleasant to use RMarkdown's version of Markdown-with-LaTeX in it than writing directly in LaTeX. It hits the sweet spot for me just as a document editor right now. Any time I have something that can produce plain text - even my phone, I can write .Rmd easily enough and paste it into whatever when I am back at my laptop. Stuck in a doctor's waiting room without my laptop? I can do some work (albeit slowly).]

Then since I was already using it for something basic, I started to pick up RMarkdown more easily than before (though I am still a beginner with it really, it's been easy going this time as I only need to deal with one new thing at a time now). 



",1541025031.0
tjpalanca,"Rmarkdown rendering to html, viewable with a browser which anyone will have, is probably your best bet. Jupyter also has some facilities for outputting to HTML, but it requires you to use Jupyter instead of Notepad++. Rmarkdown is tied to RStudio but doesn't require it to function.",1541036530.0
Thaufas,"Go with RMarkdown, specifically as RNotebook in RStudio. If the work isn't confidential, then publish it straight to the public RPubs server and send them an HTML link.

If it is confidential, then either set up your own private RPubs server and publish there or save your source code and output to a private Github repo. RStudio has adequate Github integration. ",1541039178.0
Darth_Marrr,"If you want beauty and elegance then R Markdown is the way to go, but the price is time and frustration. The learning curve can be steep and I have found R Markdown to be fickle in particular circumstances, however the PDF/HTML outputs are too professionally attractive to not learn.

If you feel you don't have the time or patience then Jupyter notebook is the way to go. Not as professional as R Markdown, but gets the job done in a reasonable amount of time.",1541041606.0
AlisonByTheC,"I’ll probably be eviscerated for this but PowerBI or Tableau are way more convenient.  

I know, I’m a heretic but I don’t have time to deal with Outlook stripping html or markdown file attachments from emails, and I would be pissed if anyone gave me reports as PDFs.  

Alternatively you can create git repositories with all your results and files and they can clone everything with one sentence.  

",1541025065.0
villain170,"There's also a package for this

[https://github.com/bnosac/taskscheduleR](https://github.com/bnosac/taskscheduleR)",1540990017.0
Five_bucks,"I created a batch file: example.bat

    ""C:\R DIRECTORY HERE\R.exe"" CMD BATCH --no-save ""C:\R SCRIPT DIRECTORY HERE\example.R""

Within Task Scheduler, you simply run the batch file and as long as R.exe and the R script are pointed to correctly, your script will run.",1541009209.0
JackOneill,Anyone had good success getting .rmd files to compile on a schedule with this? ,1541069635.0
danderzei,Who says that data science can't be fun? In this article I have translated a text game from the 1982 book [Creepy Computer Games](https://usborne.com/browse-books/features/computer-and-coding-books/).,1540960391.0
RememberToBackupData,"If you’re familiar with the apply family then you don’t need purrr. But if you’re not, then purrr simplifies them. You will want to know apply anyway for making packages. ",1540957323.0
,"If you want to learn R as a programming language then try a book called **The Art of R Programming**.

From ""Whom this book is for"" section:

> Many use R mainly in an ad hoc way - to plot a histogram here, perform a regression analysis there, and carry out other discrete tasks involving statistical operations. But this book is for those who wish to develop software in R.

It is not too difficult of a book but it goes over some of the basic elements in more depth compared to other books I've read.

Regarding `purrr` - my advice is to not go there. I use `Map` from base R instead and it serves all of my purposes. But I give the same advice regarding all of the `tidyverse` so take that as you will. I used to use `readr` and `lubridate` in the past but now they are too intermingled with the tidyverse stuff. i.e. `readr` returns `tibbles` by default.

Another reason to not get too hung up with tidyverse is that it's tailored to simple-structured datasets like shopping histories, airport departure times and things like that. When you go to MRI scan data or microarrays/sequencing data or rasters with multiple layers or anything more complex it stops being useful.

At least that has been my experience.",1540978791.0
tjpalanca,"I think your choice of whether or not to use the `tidyverse` depends on your workflow. I find that for the right kind of problems, maybe 80-90% of what I deal with, the `tidyverse` is an effective abstraction. If you don't work on problems that are within that realm (tidy/tabular data), then you might be better off doing something else, or waiting for a tidy version to be released (such as `sf` for geospatial data.).",1540995249.0
mouse_Brains,Using depends on what you are trying to do as others stated but learning purrr isn't much of an investment. It's mostly self explanatory if you already know how the apply and friends work so you might as well take a look. The hardest thing about purrr is remembering how many Rs were there the end. ,1541030926.0
IamGrabear,"Our proposal was just updated here:

[https://github.com/datasnakes/beri-isc-proposal/blob/master/proposal.md](https://github.com/datasnakes/beri-isc-proposal/blob/master/proposal.md)",1541038011.0
SataMaxx,"First of all, you didn't pick the correct css selector. There are 28 `.title-expected` on this page, and they are the class for the text of the headers of the tables. You'd have better luck here with a `div.block`, there are only three and they each contain one of the tables.  
Don't use external tools when examining webpages. The standard developer tools and inspector in all modern browsers do the job perfectly.  
Also you used `html_text` but it is `html_table` that you would need to use to parse HTML tables.

However…  
The tables are paginated, indicating that it is rendered using javascript (here with [jTable](http://www.jtable.org/)). Capturing the `table` element for parsing will result in you getting only the first page of the table.

Fortunately it is often the case that the data for such javascript rendered tables is embedded within the html source. Digging a bit around in the `div.block`, you can find a `script` tag which contains a JSON representation of the data.

There is one `script` per `div.block`, so you can use the `div.block script` css selector to fetch all three tables. Keep only the third (the one you're interested in), extract the text using `html_text`, then you need to extract only the JSON string containing the data. You can then use the `jsonlite` package to parse the JSON (it might need unescaping first).",1540953657.0
mongooseondaloose,"Take a look at the [`%within%`](https://lubridate.tidyverse.org/reference/within-interval.html) function documentation from lubridate for starters. It may not be the most elegant but the first idea that comes to my head is a full/cartesian product join between the dates in the Tidy Tuesday dataset and your weekly `tidy_dates` dataframe. Then you can flag for whether the dates from the Tidy Tuesday dataframe are `%within%` the begin-end interval.

Alternatively, you could use the [`week`](https://lubridate.tidyverse.org/reference/week.html) function from lubridate and specify the origin date as 2018-04-02. My idea above is extensible to intervals that aren't evenly spaced weeks, but using `week` may be cleaner.",1540944331.0
jackbrux,May be related https://community.rstudio.com/t/tidy-way-to-range-join-tables-on-an-interval-of-dates/7881,1540944391.0
jackbrux,"Something like:

    df %>%
        rowwise() %>%
        mutate(week_number = tidy_dates$week[which(date >= tidy_dates$begin & date < tidy_dates$end)])

May work.",1540944561.0
bluestorm21,"There are a lot of cool ways to approach that problem, but off the top of my head I might do something like this (which probably isn't ""tidy"") :

    idx <- vapply(date, function(x) which(x >= tidy_dates$begin & x < tidy_dates$end), integer(1))
    week_nums <- tidy_dates[[""week""]][idx]

&#x200B;

You could also use something from purrr to accomplish the same thing with more tidy-like syntax.",1540946249.0
RememberToBackupData,I like the Fuzzy Join package by drob for this kind of thing. ,1540957402.0
LeahWJ,"Sorry, I figured it out. It is the lowest error of the different tunings.",1540917518.0
shujaa-g,"Put your CostData into long format (use `reshape2::melt` or `tidyr::gather` for this), then join the data frames together, then aggregate. If you post your data in a copy/pasteable way instead of as images I can try to show you how in more detail.",1540924515.0
displaced_soc,"You might try something like `y=(max(df$y)-min(df$y))/4 `for the position 25% from the lowest value? Ggplot adds a bit space to top and bottom, i cant find it know (on mobile), but there is a function somewhere online, so you can add that to calculation (if it’s linear to distance  between lowest and highest value, and I think it is, there shouldn’t be need for other modifications). You could also manually set limits in addition to this.",1540912981.0
Tarqon,"I've found precomputing it to work best. 

    data(mtcars)
    
    mtcars %>% 
      mutate(cyl = factor(cyl)) %>% 
      group_by(cyl) %>% 
      mutate(label_pos = (n() * 0.75)) %>% 
      ungroup() %>% 
      ggplot() +
      geom_bar(aes(x = cyl, y = stat(count))) +
      geom_text(aes(x = cyl, y = label_pos, label = cyl))  +
      scale_y_continuous(expand = expand_scale(mult = c(0, 0.05))  ",1540936304.0
ColinQuirk,"I believe this is easier with annotate function:

https://github.com/tidyverse/ggplot2/issues/1244",1540908759.0
PandaMomentum,"I think the only free DRG grouper is the one Medicare provides as a standalone Windows application for MS-DRGs -- see [https://www.cms.gov/Medicare/Medicare-Fee-for-Service-Payment/AcuteInpatientPPS/MS-DRG-Classifications-and-Software.html](https://www.cms.gov/Medicare/Medicare-Fee-for-Service-Payment/AcuteInpatientPPS/MS-DRG-Classifications-and-Software.html) It runs interactively, and in batch mode (I forget exactly how you have to configure your input files but there is documentation at the link).

WARNING! MS-DRGs are not exactly the same as commercial DRGs -- in particular MS-DRGs were built for the Medicare elderly population and do not assemble or weight pediatric and birth-related stays appropriately. There are two other groupers ion common use for Medicaid and commercial populations -- but 3M owns and licenses APR-DRGs and OptumInsight licenses APS-DRGs and afaik there is no free grouper software or logic available for those products.

&#x200B;",1540923272.0
mattindustries,"[There are some for ICD-9 it looks like](https://www.hindawi.com/journals/jce/2010/569517/tab2/), but this is not my area at all. ",1540914327.0
AlisonByTheC,"You should use CCS categories. 

https://www.hcup-us.ahrq.gov/toolssoftware/ccs/ccsfactsheet.jsp",1540940614.0
Thaufas,"1. My first thought:  Not rendering the *.Rmd files as *.md on Github is a terrible idea. Rmd and md are similar enough!

1. My second thought: Well, if Yihui Xie recommends it, there must be a good reason. Let me read the article before passing judgement.

1. My third thought (after reading the article): I fully agree with him. For people who are new to the ecosystem, there is no way they will understand all of the gotchas. Getting burned early is a good way to discourage people.",1540856067.0
mrrchit,"Yihui recently announced that RMarkdown files will no longer be rendered as Markdown on GitHub. I think he outlines a pretty good case for this in the linked blog post. What do you think r/rstats?

Does anyone know of a good way to automate the kniting of a README.Rmd as README.md for a GitHub repo? I've always thought it was great to be able to incorporate code chunks, their results, figures, and tables into a project's readme file.",1540852873.0
fang_xianfu,Fantastic. This has annoyed me for ages. Too bad it will take a while to filter down to our on-prem GitHub deployment.,1540912342.0
Dreyups,And I still can't have my plots render in the .md file. Nice. ,1540913066.0
biaskeen,In case anyone is wondering - there was an update a few days ago to Gmisc and now the label supports expressions!! All of my problems (that I posted about here) are solved!,1542065053.0
Ader_anhilator,"Depends on your data. Use the most granular data you can. If you have daily data, then use a single step. If hourly, use 24 steps; don't aggregate to daily. My 2 cents",1540846157.0
tjpalanca,I would match it to the time period for your VaR so you are matching the target distribution as closely as possible. E.g. use 1 day for 1 day VaR,1540884253.0
YepYepYepYepYepUhHuh,"You'll have to be more specific about what you're testing or measuring. Are you comparing means across groups? Variance? Mixed-effects? What are the distributions like?

There are a number of ways to do pairwise contrasts in R but it will depend on what your data look like.",1540843757.0
infrequentaccismus,"One easy way is to group by the two columns that form the unique rows and mutate a row number.  If you have duplicate rows, this will add the same row number to all rows and WILL NOT remove them.

&#x200B;

`df %>%`

`group_by(pat_i, pat_j) %>%`

`mutate(unique_id = row_number())`

&#x200B;

If you want to remove duplicates, this is one solution:

`df %>%`

`group_by(pat_i, pat_j) %>%`

`slice(1) %>%`

`mutate(unique_id = row_number())`

&#x200B;

&#x200B;",1540824276.0
SataMaxx,"It's simpler to assign IDs to all the patients at once, then use `join` operations to bring the IDs to the main dataframe.

 

**Creating the patient directory**

    tibble(Patients = c(df$Patient_i, df$Patient_j)) %>%
      distinct %>%                       # keep each patient only once
      arrange %>%                        # sort the patients (not necessary)
      rownames_to_column(var = ""pat"") -> # add the IDs
      patients

 

**`join`ing the original dataframe with the IDs, for both columns**

    df %>%
      left_join(patients, by = c(""Patient_i"" = ""Patients"")) %>%
      left_join(patients, by = c(""Patient_j"" = ""Patients""), suffix = c(""_i"", ""_j"")) ->
      df",1540846828.0
madmongoose1,"I assume that you want the new variable to hold the sum of the monthly values, by year.

Using dplyr:

    library(dplyr)
    library(lubridate)
    
    dat %>% 
      # create year variable
      mutate(Year = year(REF_PERIODEXPENDITURES)) %>% 
      # group by year and sum the value of interest
      group_by(year) %>% 
      mutate(yearlySum = sum(RESTAURANT_EXPENDITURE)) -> dataWithYearSum

&#x200B;

&#x200B;",1540796566.0
RememberToBackupData,"Add a new column that marks each year, use dplyr::group_by() to group by year, and then dplyr::summarise() to average within the grouping. ",1540789795.0
Thaufas,"You can just copy and paste this:

χ²

You can also use the following when you set the caption:

`expression(chi^2)`",1541140571.0
tylermw8,"That's about equivalent to what I've seen. For loops can be faster in R, but you need to know how R handles various data structures and avoid common pitfalls. The `apply` family of functions are basically wrappers around for loops themselves--they just ensure the user won't encounter those issues which result in poor performance. Here's a blog post I wrote about the subject:

[There's no need to apply() yourself](http://www.tylermw.com/theres-no-need-to-apply-yourself/)",1540764640.0
shujaa-g,"One of the big changes in R 3.4 was JIT compilation by default. From the release notes:

> The JIT (‘Just In Time’) byte-code compiler is now enabled by default at its level 3. This means functions will be compiled on first or second use and top-level loops will be compiled and then run. (Thanks to Tomas Kalibera for extensive work to make this possible.)

This can cause `for` loops to be slightly faster than apply family functions, instead of pretty much the same as they used to be. ",1540771079.0
Thaufas,"My source code is listed below. Also, this inspiration for this work occurred in a [related subreddit](https://www.reddit.com/r/RStudio/comments/9rxl8x/how_to_replicate_a_sample/e8l6twj/).
    
    library(microbenchmark)
    
    rm(list = ls())
    NUM_VALS_TO_SAMPLE <- 50
    NUM_SAMPLING_ITERATIONS <- 10
    RANDOM_SEED <- 2 ^ 5
    
    # Dataset to be generated multiple times
    gdp2017 <- rnorm(n = 10 * NUM_VALS_TO_SAMPLE, mean = 1000, sd = 100)
    
    SampleFromGDP <- function() {
      set.seed(RANDOM_SEED)
      sample(gdp2017, NUM_VALS_TO_SAMPLE, replace = TRUE) }
    
    # Option 1 - With a loop
    f1 <- function() {
      x <- rep(0, NUM_SAMPLING_ITERATIONS)                                  # initializes a vector for your averages
      for (i in 1:NUM_SAMPLING_ITERATIONS) {
        samp <- SampleFromGDP()
        x[i] <- mean(samp)
      }
      return(x)
    }
    
    # Option 2 - Using the replicate() function
    f2 <- function() {
      y <- replicate(NUM_SAMPLING_ITERATIONS, mean(SampleFromGDP()))
      return(y)
    }
    
    # Option 3 - Using the sapply() function
    f3 <- function() {
      z <- sapply(1:NUM_SAMPLING_ITERATIONS, function(x) {mean(SampleFromGDP())})
      return(z)
    }
    
    CheckForEquality <- function(AllVals) {
      rslt <- sapply(AllVals[-1], function(x) {
        identical(AllVals[[1]], unlist(x))
      })
      return(all(rslt))
    }
    
    # Create a function to get results that can be called iteratively
    GetRslts <-  function() {
      
      rslts <- microbenchmark(f1(), f2(), f3(),
                              check = CheckForEquality)
      levels(rslts$expr) <- c(""for-loop"", ""replicate()"", ""sapply()"")
      return(as.list(rslts))
    }
    
    # Get the actual results
    vals1 <- replicate(10, GetRslts(), simplify = FALSE)
    
    # Create boxplots
    bplotlst <- vector(""list"", length = length(vals1))
    for (i in 1:length(vals1)) {
      dat <-
        cbind.data.frame(meth = vals1[[i]]$expr, tval = vals1[[i]]$time / 1000)
      subTitle <-
        paste0(""Random Seed="",
               RANDOM_SEED,
               "" Num Vals per iteration="",
               NUM_VALS_TO_SAMPLE)
      bplotlst[[i]] <- boxplot(
        tval ~ meth,
        data = dat,
        main = paste0(
          ""Microbenchmark Timings for 3 Methods: Iteration "",
          i,
          ""/"",
          NUM_SAMPLING_ITERATIONS
        ),
        col = c(""red"", ""yellow"", ""green""),
        xlab = ""Method"",
        ylab = ""Execution Time (µs)""
      )
    }
",1540764318.0
guepier,"First off, I don’t *at all* get the same results as you — in fact, on my machine the `replicate` is around 20% slower than `for`, and `sapply` is around 15% slower. That’s significantly different from your 50% number.

The results also come with heavy tails for all benchmarks, which makes comparing them hard: in fact, the distributions mostly overlap. There’s a statistically significant difference but it’s quite low and there’s a lot of noise.

Furthermore, both `replicate` and `sapply` perform additional work because both conditionally simplify the result, which requires keeping track of the result data type. You can avoid this work by using `vapply` or `unlist(lapply(…))`. Both of these are almost on par with a `for` loop: I get them at 7% (for `vapply`) to 7.2% (for `unlist(lappply(…))`) slower than `for` loops.

",1540812550.0
dooogan,10 simulations is probably too few to give you a strong idea of the differences. Try 1000?,1540766060.0
murgs,"replicate isn't intended for that use, so I'm sure it has some extra overhead.

And sapply has the added convenience overhead of figuring out what the output will look like. try comparing it with vapply or apply and your results may differ.

In practice the difference doesn't matter, not compared to something like vectorizing the computation. ",1540799213.0
Miii_Kiii,"So i really wanted to check how purrr package handles this. It seems it is the slowest of them all. How can it be, since in the authors book Advanced R it is claimed it is written in C and much better than original apply-family functions.  


[result](https://imgur.com/FYJAp0I)

    
    library(microbenchmark)
    library(purrr)
    
    rm(list = ls())
    NUM_VALS_TO_SAMPLE <- 50
    NUM_SAMPLING_ITERATIONS <- 10
    RANDOM_SEED <- 2 ^ 5
    
    # Dataset to be generated multiple times
    gdp2017 <- rnorm(n = 10 * NUM_VALS_TO_SAMPLE, mean = 1000, sd = 100)
    
    SampleFromGDP <- function() {
    set.seed(RANDOM_SEED)
    sample(gdp2017, NUM_VALS_TO_SAMPLE, replace = TRUE) }
    
    # Option 1 - With a loop
    f1 <- function() {
    x <- rep(0, NUM_SAMPLING_ITERATIONS) # initializes a vector for your averages
    for (i in 1:NUM_SAMPLING_ITERATIONS) {
    samp <- SampleFromGDP()
    x[i] <- mean(samp)
    }
    return(x)
    }
    
    # Option 2 - Using the replicate() function
    f2 <- function() {
    y <- replicate(NUM_SAMPLING_ITERATIONS, mean(SampleFromGDP()))
    return(y)
    }
    
    # Option 3 - Using the sapply() function
    f3 <- function() {
    z <- sapply(1:NUM_SAMPLING_ITERATIONS, function(x) {mean(SampleFromGDP())})
    return(z)
    }
    
    # Option 4 - Using the purrr::map_dbl() function
    
    f4 <- function() {
    y <- purrr::map_dbl(1:NUM_SAMPLING_ITERATIONS, function(x) {mean(SampleFromGDP())})
    return(y)
    }
    
    CheckForEquality <- function(AllVals) {
    rslt <- sapply(AllVals[-1], function(x) {
    identical(AllVals[[1]], unlist(x))
    })
    return(all(rslt))
    }
    
    # Create a function to get results that can be called iteratively
    GetRslts <- function() {
    rslts <- microbenchmark(f1(), f2(), f3(), f4(),
    check = CheckForEquality)
    levels(rslts$expr) <- c(""for-loop"", ""replicate()"", ""sapply()"", ""purrr:map_dbl"")
    return(as.list(rslts))
    }
    
    # Get the actual results
    vals1 <- replicate(10, GetRslts(), simplify = FALSE)
    
    # Create boxplots
    bplotlst <- vector(""list"", length = length(vals1))
    for (i in 1:length(vals1)) {
    dat <-
    cbind.data.frame(meth = vals1[[i]]$expr, tval = vals1[[i]]$time / 1000)
    subTitle <-
    paste0(""Random Seed="",
    RANDOM_SEED,
    "" Num Vals per iteration="",
    NUM_VALS_TO_SAMPLE)
    bplotlst[[i]] <- boxplot(
    tval ~ meth,
    data = dat,
    main = paste0(
    ""Microbenchmark Timings for 4 Methods: Iteration "",
    i,
    ""/"",
    NUM_SAMPLING_ITERATIONS
    ),
    col = c(""red"", ""yellow"", ""green"", ""blue""),
    xlab = ""Method"",
    ylab = ""Execution Time (µs)"",
    outline=FALSE # added this because replicate was returning outliers orders of magnitude higher than average
    )
    }",1540816638.0
ElaboratedMistakes,It's not 50% faster in general. It's only 50% faster for your really small number of sampling iterations. For higher numbers the difference gets negliable. ,1540769706.0
demarius12,On my phone so I can’t provide the code without testing but for 3.5 million rows I would def use the data.table package. The aggregate() function is waaaaaay too slow for almost any aggregating. Dplyr might work too but I’ve never tried it with that many rows of data. ,1540764438.0
aizheng,"In theory, you should be able to use 
merge(prod.agg, prod.data), and then do your thing.
I do recommend you use either data.table or dplyr, which will noth make this type of thing much easier. 
Here is an example question that does something similar.
https://stackoverflow.com/questions/41560396/r-aggregate-based-on-multiple-columns-and-then-merge-into-dataframe",1540765047.0
RememberToBackupData,"Why are you using a colon? Do you mean a semicolon? Right now you’re making a vector range from xsamp to xbar[i], or am I reading this wrong?

The problem is that you originally tried to put two commands on one line without separating them. You either need to separate them with a semicolon or, better yet, just put them on separate lines for readability. ",1540760006.0
waded64,"Try adding `out.extra = ''` to the code chunk. I had this issue with a figure and this fixed it. You might also try a `.Rnw` file and using Sweave, it tends to do a better job converting to Latex documents.",1540737345.0
RememberToBackupData,"In my personal package [Desiderata](https://github.com/DesiQuintans/desiderata/), I made a function `desiderata::show_colours()` that takes an arbitrarily-long vector of colours and plots them into tiles. Nice for comparing different palettes. https://github.com/DesiQuintans/desiderata/blob/master/images/base_colours.png

I also made `shush()` which suppresses all console printing (including regular `print` and `cat`) for clean Rmarkdown output, and `clippy()` which puts dataframes and vectors into the clipboard. `%pctin%` is an operator that tells you the percentage of elements on the LHS that are present in the RHS.

But my favourite and most-used functions are the dataframe ones: 

- `overwrite_df()` is a regex find-replace that works across an entire dataframe. I use it a lot to blank empty and zero-only cells before outputting it as a table in Rmarkdown (that's its default setting).
- `drop_empty_rows()` and `drop_empty_cols()` remove rows and columns from a dataframe if all of the cells in a selected range match with a regex query (by default, zeroes and empty strings).
- `sort_cols()` lets you sort a dataframe's columns alphabetically, and then pull selected columns to the left afterwards.
- `collapse_df()` takes all the cells in a column range and collapses them into a vector, which is useful for taking columns of numbers and putting them into a histogram.",1540695560.0
Mighty_Peeniz,"The missmap function in the Amelia package does an awesome missing values visualisation. X axis is all the vars in the dataset in order. Y axis is all the rows in order, and the value in the viz is a coloured cell that show where a value is NA. It's great for quickly seeing the proportion of NAs in a field, whether there's any pattern to them, and whether the pattern of NAs is the same or similar to any other fields in the dataset.",1540697870.0
undernutbutthut,I've got to give DPLYR some love. I dig the use of the left_join function to remove information I don't need and join two tables together at the same time.,1540739728.0
vaguely_specific1,"OP, did you mean to use `vects` instead of `x`?

Use library(fcuk) and add_fcuk_to_rprofile() as a spellchecker.  use it once, never have to import it again.",1540688838.0
I_just_made,"A personal set of functions I made for myself that I think are kind of neat;

I do a lot of molecular biology, and as part of that I need to break up DNA / chromatin into fragments which are run on a gel to visualize the lengths.

The functions I wrote start with the imager package to:  
1) import the gel image  
2) automatically locate the lanes on the gel  
3) quantify the average signal of each lane as it moves down the gel  
4) Returns a line plot (x = distance down gel, y = average signal) for each lane.

The only difficulty I have had is in figuring out how to automatically deal with labeling ladder locations.  Since different ladders can be used to measure different sizes, it isn't always one size fits all... but you also have gel quality where sometimes it isn't run far enough and signal can aggregate / can't be separated easily.  So at the moment, there is some manual annotation; but it provides a clear, easy to understand interpretation of the sample's size distribution that I feel is easier to interpret than visually inspecting the gel and saying ""yep, that's within x range!""  Critical to use? Nah, but a great aid in optimizing / QC.",1540745300.0
shujaa-g,"Things I've found useful and many people don't know:

- `pmin` and `pmax`
- built-in constants like `month.abb` and `month.name`
- Using a matrix in `[` to subset a matrix, pulling individual values based on their row/column combo

The opposite, something I've seen people use needlessly: `ordered` or `factor(..., ordered = TRUE)` when the want to assign order to the levels of a factor. All factors have an order of their levels, the only things that setting `ordered = TRUE` is needed for is if you need to use comparisons like `<` or `>` on your factor levels, or you want to switch up the default contrasts.

",1540688518.0
tacothecat,Enframe and deframe from tibble,1540688683.0
tjpalanca,"combination of `here::here()` and GNU `make` - so useful for projects with a mixture of rmarkdown, r scripts, and making everything easily reproducible",1540690102.0
grasshoppermouse,"Good 'ole c when I need to strip off attributes that are screwing things up: 

>c is sometimes used for its side effect of removing attributes except names, for example to turn an array into a vector. as.vector is a more intuitive way to do this, but also drops names.",1540696047.0
Conrivore,Lapply!,1540725954.0
Taiwaly,I made a set of functions to create interaction terms for categorical - continuous and categorical - categorical and continuous - continuous variables,1540711364.0
VisuelleData,"My favorite is the f() function from pryr. It lets you write anonymous functions as shown: f(x, y, x^2 + y^2).
  
  
I also like the tictocify() function in the frite package (my package), it modifies any function to provide runtime. I pretty much always use it on purrr:map() when I'm writing personal code
",1540732360.0
ashwinmalshe,"If you deal with a lot of wide data and need to transpose it, I prefer to use `merged.stack()` function from `splitstackshape` package : 

https://cran.r-project.org/web/packages/splitstackshape/index.html

**The problem it solves**: Consider you have data where columns have multiple constructs measured in different time points. For example, you are dealing with (annual) sales in dollars, sales in units, and profits of multiple brands over 2011 to 2017 where each brand is in a single row. The columns have names like `sale_dollar_2011`, `sale_units_2011`, `profit_2011`,  `sale_dollar_2012`, `sale_units_2012`, `profit_2012`, ``...`, `profit_2017`. Thus, there are 7 * 3 = 21 columns that you want to transpose to a `tidy` format. However, you want the data transposed to 3 columns such that `sales_dollar`, `sales_units`, and `profit` are 3 columns and one additional column for the `year`. You can do it in one line by using `merged.stack()` function above. Check it out. An important advantage it has over other solutions is that it also handles missing values easily while transposing. 
",1540742699.0
MortGillu,"`psych::headtail` to get head and tail of any object, I have made a modified version of it to only display first 5 columns. I use it as a data check sometimes when dealing with matrices/dataframes of large dimensions.",1540745697.0
,"Mine is vectorised statistical tests on rows and columns of matrices: https://github.com/KKPMW/matrixTests

I work with geneticists and the frequent task is to run a t-test or ANOVA or something like that comparing cases and controls across each gene.

The functions in matrixTests run a lot faster than doing the test in a loop and return a well formatted data.frame as a result. They they care of missing NA values and other edge cases as well.",1540832413.0
willbell,"mise::mise()

Much better than rm(list = ls()) or any pther console clearing command.",1542697444.0
o9hjf4f,"If you are using pdf_document it is probably because they are floats, most likely in LaTeX table environments. Look at the possibilities in print.xtable",1540679246.0
StephenSRMMartin,"This is a latex ""issue"" (I say ""issue"", because it's not a bug).

Two options for you: Use the 'float' package in latex, then say the float placement should be 'H' (capital H). Or, before a section heading, type \\clearpage; that will print all the tables that were unable to be placed in the prior section, before starting the next section.",1540703850.0
webbed_feets,I don't have a fix for your problem. I've never been able to get Latex tables to render properly. Consider making markdown tables instead. You can get almost as much control.,1540750024.0
Ader_anhilator,Aren't you supposed to use the gamma dist on your exam for severity?,1540678639.0
ashwinmalshe,"In your first scenario when you are splitting your sample in training 60% and validation+/testing 40% do you keep sampling with replacement from entire 100% sample? That means effectively you will have many training sets and many validation +/testing sets! That’s incorrect because your training data and testing data overlap. 

You should partition your data into training and testing only once. After that you will do bootstrapping with replacement **within** the training sample. You can’t do bootstrap without replacement in training data and keep the training sample size the same because as in your second scenario there will be only one sample. That’s not random forest. That’s like a standard CART. ",1540647248.0
GoodAboutHood,"It really depends on your dataset. Many times it’s better, sometimes it’s not. It’s one of those things you can test to see which one ends up with a better result.

But in general using bagging introduces more diversity in the subsets that a predictor is trained on, leading to more diversity in the predictions, leading to a better ensemble prediction with a decreased variance.",1540660914.0
villain170,"A case when might work https://dplyr.tidyverse.org/reference/case_when.html

As in

    Price = case_when(PriceLow == 1 ~ 1, 
                                        PriceMedium == 1 ~ 2, ...


Or try looking in to this method https://stat.ethz.ch/pipermail/r-help/2006-October/115705.html",1540640529.0
rayray0313,So from factor to numeric?,1540653372.0
Lareine,"    df %>% gather(Price, count, c(PriceLow, PriceMedium, PriceHigh)) %>% 
      filter(count > 0) %>%
      gather(Taste, count, c(TasteBad, TasteMedium, TasteGood)) %>%
      filter(count > 0)

(where ""df"" is the name of your data frame)",1540657310.0
FatalMojo,"    df['Price'] = df[['PriceLow', 'PriceMedium', 'PriceHigh']].values.argmax(axis=1) + 1
    df['Taste'] = df[['TasteBad', 'TasteMedium', 'TasteGood']].values.argmax(axis=1) + 1
    df = df[['Price', 'Taste']]

&#x200B;",1540666530.0
shujaa-g,"`grid.arrange` isn't the easiest to work with, [lots of more user-friendly options here](https://stackoverflow.com/q/8112208/903061).
",1540609957.0
another30yovirgin,"What about:

     ## Store my ggplots as lists
     plots = list(p1 = list(c1.Fa, c2.Fa), p2 = list(c3.Fa, c4.Fa), p3 = list(c5.Fa, c7.Fa), p4 = list(c8.Fa))

     ## Plot Two at a time
     lapply(plots, function(a) do.call(""grid.arrange"", c(a, ncol = 2, nrow = 1)))

Of course, that requires you to change the way you set up the list of plots.

",1540648591.0
jackbrux,https://ggplot2.tidyverse.org/reference/facet_grid.html,1540636293.0
samclifford,How are the plots built? Do they share common variables and geometries? ,1540619618.0
s3x2,"The `parse` function turns a string into an unevaluated expression, while `eval` evaluates an expression.

I'm not sure what you're trying to do with the dollar sign on the dataframe. I assume you were trying to retrieve the first column:

    fun1 = function(df = df1, var = ""var1""){

            plot(df[,1], eval(parse(text=var)))

    }

    var1 <- 1:nrow(mtcars)
    fun1(mtcars, ""var1"")

Note that `parse` must be used with the `text` argument to accept a string, otherwise it expects a file.

PS: I've read a few times that needing to use these functions is a sign you are doing things wrong, but I've yet to see actual suggestions of what should be done instead...",1540606414.0
shujaa-g,"You can't use strings (or numbers) with `$`, but you can use `[` for either.

As the other comment says, it's not at all clear what you're trying to do. `df$1` doesn't make sense (do you mean the first column, `df[, 1]`??), and plotting a string like `""var1""` on the y axis doesn't make sense. My best guess is you're trying to plot the first column on the x-axis and an input column on the y-axis. In that case, do this:

    fun1 = function(df = df1, var = ""var1"") {
        plot(df[, 1], df[, var])
    }

---

    > fortunes::fortune(""magical"")
    
    The problem here is that the $ notation is a magical shortcut and like any other magic
    if used incorrectly is likely to do the programmatic equivalent of turning yourself into
    a toad.
       -- Greg Snow (in response to a user that wanted to access a column whose name is
          stored in y via x$y rather than x[[y]])
          R-help (February 2012)

(For single columns, `df[[1]]` and `df[[var]]` will also work, and may even be preferred. `df[, var]` will return multiple columns if `var` has length > 1, whereas `df[[var]]` can only work for one column and will give an error if `var` has length > 1. So if you *know* you want only a single column, `[[` is safer.)",1540608858.0
another30yovirgin,It's possible that you're looking for get(). ?get,1540646529.0
villain170,http://adv-r.had.co.nz/Computing-on-the-language.html,1540606544.0
fang_xianfu,"Questions like this are a bit frustrating for me. You're getting a lot of good answers to your question, but nobody is telling you that doing it is a bad idea.

Passing strings in this way is very messy. It's easy to introduce bugs and have things not work, and it's very hard to debug. In general there is usually a better way to do what you're trying to do than passing strings, so if you could explain the context that might help to find a better solution",1540663225.0
LiesLies,"Didn't read thoroughly, but you can make a list of data frames in your function and return that list. Then, you'd access elements of that list by index, or by name.

Edit for a more complete, general answer:
If the tables have the same header structure, you can also return one table that has all data in one dataframe (think: stacked vertically) with an added column that specifies the which dataset those observations belong to. Then you subset the returned dataframe depending on which you want to use.",1540586148.0
RememberToBackupData,"I don’t understand why you want to split your dataframe. Why don’t you just add two new columns to it, one from day/night and one for day number, and then just filter on those two columns as needed?",1540586092.0
mtg_liebestod,"Put the dataframes into a list, and then return the list.",1540586261.0
Emrys_Wledig,"Your question has definitely been answered by the others here, but for posterity you *can* easily return multiple data frames in one object using the S4 class system. 

[Here's a tutorial](http://adv-r.had.co.nz/S4.html) and a quick example. 

```
setClass(
    ""twodf"",
    representation(
        df1 = ""list"",
        df2 = ""list""
    )
)

foo <- function(df_1, df_2){
    output <- new(""twodf"",
                  df1 = df_1,
                  df2 = df_2
              )
    return(output)
}

example <- foo(df_1, df_2)

# Work with the first data frame
example@df1 

# Work with the second data frame
example@df2
```
",1540589655.0
efrique,"> Can a function be made to return multiple data frames? 

Sure, return multiple objects in a list, like say `lm` does or any number of other functions in the base distribution of R. ",1540590569.0
Oshobooboo,Tukeys test,1540583834.0
rayray0313,"Emmeans(model, pairwise ~ a*b, adjust = “Bonferroni)",1540602060.0
Cryptokudasai,this looks like a great thing to look into (!)-- sorry I can't help !!,1540637361.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/rprogramming] [Can anyone here provide some insight to this?](https://www.reddit.com/r/rprogramming/comments/9rtv7y/can_anyone_here_provide_some_insight_to_this/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1540643244.0
mouse_Brains,Is this just a matter of raising contrast? I am surprised people use transparency when trying to hide info ,1540496190.0
Im_int,I'm extremely surprised there are redaction tools that use transparent brushes. I mean it's so obviously wrong!,1540512629.0
joe_gdit,"Have you checked out https://github.com/r-lib/httr ?

edit: better link",1540496594.0
Agent_KD637,Logistic regression,1540474216.0
nopstoc,"try glm(X1 ~ Y1 + Y2, data=mydata, family= binomial)",1540474665.0
adric10,"The interpretation depends not only on statistical significance, but on the scales of the predictor variables. If they’re on different scales or have different measurement units, they’re hard to compare to see which one has the greater effect. Normalizing the variables can help, though. ",1540548326.0
mrcflckrz,"It depends on what you want to do.

You can still access for example Insights data of your owned company pages. You can still access posts, likes and fancounts for pages.

You can not analyse networks, friends connections etc. anymore. In that respect, everything written before 2018 is outdated.

&#x200B;

&#x200B;

&#x200B;",1540820613.0
zorp_,"This is important, excellent image handling is a key future point for R. Current cloudyR methods for using the google API are fine and imagej/Fiji is cumbersome. I hope they are working on loading image_plot and image_montage methods.",1540474324.0
Er4zor,I deeply miss MATLAB for its image processing tools! Otherwise I'd easily do all my PhD work in R.,1540497875.0
cavedave,Seems like an interesting talk. I have no connection to the speaker.,1540464532.0
VincentStaples,/r/offmychest,1540453115.0
Kono_Diogenes_da,"Some tools are made so that non-statisticians (like economists?) can perform analysis without having to worry about the math. 

You have to remember that not everybody is a statistician, and for many, many people doing any kind of math is like pulling teeth. That's one of the big reasons that statistical software like Stata even exists!",1540474365.0
rayray0313,"Set the contrast manually, and then run the mode.",1540435271.0
rayray0313,"Yes. 
Name<-c(-1,1)
Contrasts(variablenamewherecontrastbelongsto)<-cbind(name)

",1540438208.0
Sir_Enity_Now,"Start with a problem that matters to you--you'll be more willing to learn if you're trying to figure something out for your own work. When you need help, google it. You'll often find bits of code on Stack Exchange or reddit. Any problem you're having has likely been answered a thousand times online. It might be a disappointing answer, but it's always the best starting point for learning a new programming language, imo. ",1540425738.0
rohan36,[https://swirlstats.com/](https://swirlstats.com/),1540419562.0
s3x2,"Look at the Data Science Specialization from Johns Hopkins on Coursera. It'll take you from zero to quite capable over a few months, totally free (unless you want the certificate).",1540421911.0
mrrchit,[R For Data Science](https://r4ds.had.co.nz/) is my recommendation.,1540426446.0
william12323,"Depends on what you are doing, but the tidyverse is pretty useful. This is a decent guide. [https://r4ds.had.co.nz/](https://r4ds.had.co.nz/)",1540455688.0
Krynnadin,Datacamp,1540434531.0
Elrond_Hubbard_,"My big hobby is sports analysis, and I recently quit my job to do it full time. I learned R by trying to make better projections, and I started with *Introduction to Statistical Learning with Applications in R*, the pdf is available online in full. 

It's geared more toward predictive analytics (as the title suggests), but I learned about actual programming with this than rSwirl or any other beginner R course. Can't recommend it enough, just try to stick to a schedule of a chapter every 1-2 weeks or so.",1540453029.0
hopeyesperanza,"This is a really great starting point:

https://cran.r-project.org/doc/contrib/Verzani-SimpleR.pdf",1540423452.0
dugorama,"Start with easy graphs, the reward will keep you going. Also, use a guitar front end, rstudio",1540475338.0
math_is_my_religion,Try the package swirl. It’s a learning guide dressed up as a package. ,1540435698.0
wouldeye,r/learnrstats ,1540435945.0
new_in_R,I used the free R course by Udacity and it gave me a good start,1540446925.0
Econ_dude,"Put ""+"" between commands. E.g.

ggplot(mapping = aes(switch, meanAcc))  +  geom\_col()  ",1540407711.0
thaisofalexandria,I would no call my plot 'plot' since it's a R function.,1540408581.0
atroiano,[https://github.com/tidyverse/reprex](https://github.com/tidyverse/reprex) .  this also helps with reproducibility when you do have a problem.,1540411741.0
lederi,"group_by(year, frequency) %>%
summarise (avg = mean (data))",1540390826.0
shujaa-g,"From a tidy data perspective, this should be two tables, a yearly aggregate table and a year/month table. So you should create `yearly_figs = filter(MoreData, frequency == 'A') %>% select(Client, year, Data)` and `monthly_figs = filter(MoreData, frequency == 'M')`.

Then you can proceed to check your monthly data...

    monthly_figs %>% 
      group_by(Client, year, subperiod) %>%
      summarize(subperiods = n(),
                       avg_data = mean(Data, na.rm = TRUE)) %>%
      full_join(select(yearly_figs, -frequency), by = c(""Client"", ""year"")

You can use `tidyr::complete` to fill in all the missing year/month combos and look at`NA` values introduced that way to identify specific problems.
",1540394300.0
VincentStaples,Yes,1540379080.0
jbuckets89,"Yes, please feel free to share analysis and/or code (or problems with code!)",1540380953.0
Darwinmate,"Please read the tutorial:
https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html

it answers your questions",1540359739.0
supersonicsacha,"Use join in the stringr package. I would use inner_join and then use ""by = "" to specify column. 

Here's a good cheat sheet I use for reference on joins to give you more information: http://stat545.com/bit001_dplyr-cheatsheet.html",1540659032.0
Loco_Mosquito,Look up left_join and anti_join.,1540377992.0
grumpymonk2,"There is an unmet need for the ability to easily do reproducible analysis in R, where reproducible means recreating the environment exactly, including the exact version of R and all packages. Pretty much like the conda `environment.yml` concept.

You're competing with the current CRAN, Microsoft's MRAN, Rstudios package manager, packrat and jetpack, to name but a few. And conda has R too, but with limited package support.

I'd recommend finding a niche within that and making a strong statement as to why you're doing something unique.

Just my opinion, but basing this on the use of R in a large corporate environment.",1540320880.0
webbed_feets,"Looks great.

Can you explain a bit more why you would use this over packrat? That wasn't totally clear to me.",1540331272.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/python] [beRi Suite of Tools for Managing R (built with Python) • r\/rstats](https://www.reddit.com/r/Python/comments/9qrqu5/beri_suite_of_tools_for_managing_r_built_with/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1540319731.0
aaronsaunders,How does this compare to conda? ,1540330519.0
s3x2,"Can you articulate what exactly is that ""lot of information"" that would be lost and why that worries you?

k-means assumes that the only relevant thing in describing a group is the position of its centroid (i.e. n-dimensional mean). Taking the mean of your measurements is exactly what you would do if you were trying to estimate where that centroid is locates for each group. The only thing you lose is some notion of the variability of that estimate, but you weren't going to get any of that from k-means anyway.

I think you're using the wrong approach. I would just look at the centroid distances if I wanted an assessment of similarity since the groups are known. k-means is an unsupervised method, which means you're not supposed to tell it what the group structure is, presumably because you wish to anticipate how hard it would be to tell what group a future unlabelled case belongs to. But QDA would probably be a more principled tool for that purpose.",1540293438.0
pugl33t,"OK I've found the issue - the \`glm()\` function captures the original call \`using \`[match.call](https://match.call)()\`. However \`cv.glm()\` re-runs the call within the \`parent.eval()\`, environment. 

&#x200B;

In my case the \`.x\` isn't part of any parent environment, and thus we get the issue.",1540337893.0
bvdzag,"What is the `typeof(x)` where `x` is the variable of interest? And the `typeof` the other variables involved in the correlation?

Also, `cor` does not accept `na.rm`. Use `use = ""complete.obs""` or `use = ""pairwise.complete.obs""` instead.",1540269476.0
efrique,"The variable itself contains one or more ""Inf"" values (but no NAs or NaNs)?

If that's the case, then the *actual* mean should be ""Inf"". 

R has some odd behavior if a vector can contain both NA and NaN (it's order dependent!)

    a4 <- c(1,2,5,9,NaN,NA);mean(a4);mean(rev(a4))

I had assumed one or the other would take precedence. (However, either beats out `Inf`, which to me is correct behavior.)
",1540269655.0
,[deleted],1540272848.0
Zmamo,I just ran into this today in pandas. You could try to replace inf with another value. That should give you a real number value for your mean. ,1540275568.0
cgrewal94,"Could be an easy fix. Are you assigning the change to an object using the assignment operator ""<-""?",1540247444.0
BeerSharkBot,"You have to assign that to a new dataframe (or overwrite the original). The pipes create output, but they don't automatically store them. Run through and of the tidyverse tutorials to get a solid foundation. ",1540247462.0
s3x2,"Summing scores implies you think a unit increase can be treated identically across questions. Unless you're counting apples or something with clearly defined and obviously comparable units, it doesn't make any sense. And if my intuition is correct, your questionnaires are using Likert scales to produce a score, which is absolutely not something that puts them on a clearly defined and comparable unit scale.

If two groups of questions are supposed to be measuring the same thing and you want to compare that, then you don't really care about scores but what they say about some underlying, unmeasured condition or concept.

I would build a structural equation model that links each group of questions to a separate latent variable and regresses one of the LVs on the other one. The `lavaan` package lets you do that.

Code for the model would look something like:

    library(lavaan)

    '
    latentVarQ1 =~ Q1_1 + ... + Q1_J
    latentVarQ2 =~ Q2_1 + ... + Q2_K
    latentVarQ1 ~ latentVarQ2
    ' -> model.formula

    sem(model.formula, data) -> model.estimate

Where `J` is the number of items in questionnaire 1 and `K` the same for the other one and I assume you want to predict questionnaire 1 from questionnaire 2's results. If the items are indeed Likert-type then you also need to add the `ordered` argument to the `sem` (or convert all the items to an ordered factor previously).

Note that this assumes a reflective model for the latent variables (read more on reflective vs formative latent variables to see which applies to your case).",1540233702.0
endaemon,"The `car` package offers `linearHypothesis()` to do this, and extends the possibilities to multiple testing:

    my_lm <- lm(formula = mpg ~ wt + qsec + am, data = mtcars)
    summary(object = my_lm)
    
    # test Ho: qsec == 0.5
    car::linearHypothesis(model = my_lm, hypothesis.matrix = c(0, 0, 1, 0), rhs = 0.5)",1540226157.0
efrique,"Well, you can extract the coefficient and the se from the regression output and do that computation all directly in R. No doubt some packages will do that already but it's only a line or two of R code anyway; I'd just type in the R code and have my p-value before I'd got halfway through searching for a package to do it.
",1540252186.0
AnInquiringMind,"I'm having trouble understanding your data structure. Could you provide a sample (or perhaps a snapshot of the current plots)?

For showing distribution across groups, I've become quite attached to the ggridges package, which creates ridge plots.",1540166489.0
cavedave,Would a heatmap work? https://www.reddit.com/r/dataisbeautiful/comments/7rcvgx/the_temperature_of_the_world_since_1850_oc/,1540201717.0
Eueee,Is there a specific algorithm you want/need to implement? Do you need a guaranteed correct solution or is a heuristic okay? Are you allowed to brute force it? What have you tried? We need more details,1540169212.0
longshortdogsFTW,You’re probably getting confidence intervals for each predicted value.,1540150280.0
efrique,"you need to have a variable called the same thing as your x in the fitted model.

",1540163295.0
villain170,How many values are you getting,1540150018.0
willbell,It is probably in your object produced by train() and being used to predict confidence intervals for each test data point.,1540156383.0
villain170,Try specifying the 9 columns you want in two separate calls.,1540138342.0
boxuancui,"With the latest version you can use the `nrow` and `ncol` argument, e.g., `plot_histogram(..., nrow = 3, ncol = 3)`",1540147939.0
cyran22,"In the Rmarkdown chunk, you need a results=""asis"".


    ``` {r results=""asis""}
     xtable(mydf)
    ```",1540101733.0
deanat78,"Demo [here](https://github.com/daattali/colourpicker/blob/master/inst/img/plothelper-demo.gif?raw=true) :) 
",1540105830.0
Leowee,Nice! I'm gonna test it later transforming ggplot to plotly!,1540130854.0
mm8999,"I use ESS every day at work, thanks for your work on this!",1540130916.0
mrrchit,"Does anyone have any tips for binding operators, such as `%>%`, like the smart assign key binding?",1540084681.0
RememberToBackupData,Probably igraph. ,1540060757.0
AllezCannes,"igraph or ggraph, which is the ggplot-friendly version.",1540061310.0
danderzei,"This approximates it quite well:

    suns <- read.csv(""suns-mavericks.csv"")
    library(igraph)
    g <- graph.data.frame(suns, directed = TRUE)
    E(g)$weight = suns$Weight
    edgelabel <- E(g)$weight
    edgelabel[edgelabel == 1] <- NA
    png(""Suns-Mavericks.png"", width = 200, height = 200, units = ""mm"", res = 300)
    plot(g,
         layout = layout.fruchterman.reingold(g),
         vertex.size = degree(g) * 1.5,
         vertex.color = ""#e45f1f"",
         vertex.frame.color = NA,
         vertex.label.dist = -3,
         vertex.label.color = ""#e45f1f"",
         vertex.label.family = ""Sans"",
         edge.width = E(g)$weight * 2,
         edge.color = ""lightgrey"",
         edge.curved = .2,
         edge.label = edgelabel,
         edge.label.color = ""black"",
         edge.label.family = ""Arial"",
         edge.label.cex = 1.5,
         main = ""How the Suns Players Assisted One Another"",
         sub = ""Mavericks vs. Suns"")
    dev.off()

This will require fine-tuning and post processing to add shadows to the vertexes.

Image: [https://prevos.owncube.com/public.php?service=files&t=bc6dc5f24836e1c90f3ddff21e788430](https://prevos.owncube.com/public.php?service=files&t=bc6dc5f24836e1c90f3ddff21e788430)",1540065066.0
The_Biggest_Gentile,"I posted this earlier in another comment, but in this [blog post](http://kateto.net/network-visualization) you can use igraph and ggnetworks",1540061669.0
aftersox,"If R was used, I'd guess ggraph. But there is definitely some post-processing done here.",1540061773.0
Oshobooboo,"The nodes have shadows, which I've never seen a clean way to add in R.",1540078915.0
bek2113,It looks like visNetwork,1540099465.0
mbillion,General Question - is there a good place to get up to date data sets on sports games?  Like games that were played last week or yesterday or whatever?,1540100541.0
jlrc2,The main thing that makes me say it probably isn't ggraph is I'm unaware of any relatively easy way to get drop shadows with ggplot2.,1540127538.0
paradeto,"Doesn't seem to be base R. 

Note: the peculiar arc in edges.",1540060275.0
beveridgecurve101,Confident it's the 'sunbelt' package https://www.google.com/url?sa=t&source=web&rct=j&url=http://kateto.net/sunbelt2016&ved=2ahUKEwiV3fil7pbeAhXGmq0KHY9JAj4QFjAAegQIAxAB&usg=AOvVaw2G9L8OyR8JmbuVgTEl5Yuo,1540101871.0
abecker93,"Mean imputation is usually a bad idea due to the effects is has on variance, but forest-based imputation techniques as well as chained-equations imputation techniques generally do a good job, especially when you don't have a high percentage of missingness. I'd check out the package 'mice' and the package 'missForest'.",1539982317.0
rayray0313,"If you have missing data, run a linear mixed effect model or lme.

You can say : data<- lmer(dv~iv, +(1|subjects), data=dataname, method =“REMl”, na.action=“na.omit”).",1540052059.0
shaqerd,"That seems like the worst possible way to visualize where you eat. What do the colors mean? What's wrong with a bar chart or table?

&#x200B;",1539969189.0
swill128,"Try enclosing all your stings in double quotes so Jimmy John's becomes ""Jimmy John's"" not 'jimmy' 'john' 's'.

Also word clouds are useless means to visualize data. I imagine they only became popular because managers are stupid.",1539978703.0
flyos,"The other day, someone mentioned the [unpivotr](https://cran.r-project.org/web/packages/unpivotr/index.html) package which is designed to handle precisely this kind of cases.",1539967788.0
bluestorm21,"I'm not certain how well a data.frame, tibble, or data.table data structure would suit a nested table in that native format. I guess it depends on what you intend to do with the data. 

Under a tidy-data paradigm, you could read in the table skipping the first row and then use `tidyr::gather()` or `reshape2::melt()` on the two sub heading columns to collapse them into a key and value column (the key column you could label with your upper heading).

Having the data stored in this way is usually preferable in cases where the sub headings correspond with levels of the heading and are not actually variables in and of themselves (sub headings of male and female under the header of sex would be a good example). ",1539958492.0
_Wintermute,"If you're not afraid of python, pandas can read multi-indexed dataframes just fine.",1539964177.0
aftersox,"If this is a one time import, make a copy and edit the file so that it imports correctly.",1539966873.0
maxblasdel,"I would save it as a CSV, get rid of the formatting, then use dplyr to group the data. ",1539976913.0
Cronormo,"You are trying to call `chocolate` as it's own separate object. However, it is not a single object, but a part of `candyData`. If you write `candyData$chocolate` (or `candyData[,""chocolate""]`) R now knows it has to look inside that object to find your desired variable. ",1539955935.0
MrLegilimens,"You’re not actually showing the code you’re doing, just the error message. I bet it’s a problem with your code, not with R reading the data.",1539955892.0
flyos,"Why not:

    df %>% group_by(x) %>% summarise(mean = mean(z))

?",1539947059.0
blankepitaph,"Currently on mobile so I haven’t tried this myself, but `purrr::discard` might help. Something like `discard(~ is.nan(.x))` at the end of your pipe chain? ",1539955271.0
samclifford,"It's just as simple as telling `split()` that you want to drop any empty levels.

```
df <- data.frame(x = rep(c(""A"", ""B""), each=10),
                 z = c(rnorm(10,10), rnorm(10,15)))


levels(df$x) <- c(""A"", ""B"", ""C"")

df %>%
  split(.$x, drop = T) %>%
  map(~mean(.$z))
```",1539963976.0
baracapy,"I don't know if this is necessarily best practice, but you might want to look into [drake](https://github.com/ropensci/drake), especially for more complex projects.",1539908345.0
mongooseondaloose,"This looks a lot like the pipeline I've been using for a couple years now! The one distinction between yours and mine is that each script that is `source`d ends with a [`save.image`](https://stat.ethz.ch/R-manual/R-devel/library/base/html/save.html) call, where I export the current working environment (datasets, functions, other objects) to a file of the same name. My `main.R` file hosts any and all library loading I'll need across each script.

Working sequentially, the front matter of each script (after the first) loads its predecessor's end-state environment, so if I need to pick up from the end of `analysis.R` and test changes in `table.R`, I don't need to run any time-consuming tasks again--I can just run `table.R` and it will load the environment that existed after my last run of `analysis.R`. 

Using that pipeline, with git for version control, I've been able to work on some long-term projects and not lose my head.",1539911961.0
openclosure,"I used to do this, but I have moved to a slightly different workflow that is more bare-bones pipeline oriented. Stealing /u/mattindustries nice formatting, it's something like

    📂source
    |-📄data.R
    |-📄train.R 
    📂data
    |-📄train.csv
    |-📄test.csv
    📂scripts
    |-📄00_dataload.R
    |-📄01_features.R
    |-📄02_train.R
    |-📄03_predict.R
    |-📄04_output.R
    📂output
    |-📄chart.png
    |-📄scores.csv
    📄replicate.sh
    📄exploratory.R
    ⚙️conf.json
    
Basically `source` will have all my functions and heavy duty logic around loading, cleaning, training...etc, and are sourced inside the `scripts`, which are almost entirely higher level control flow statements, and usually accept arguments (usually to provide a location for the output of the last step). At the project root I will have something like `main.R` or `replicate.sh` that provides the arguments I used to create the output, along with an `exploratory.R` file that has some commonly run diagnostics, debug scripts, and just serves as a general scratchpad. Probably the biggest benefit is that if you want to productionize this in some way, you're already most of the way there. This works good for me because most of the models I build are batch processes that send results daily/weekly. So I will create a `daily_run.sh` and also a `weekly_run.sh` to just do those prediction windows. 

I also find personal benefit in forcing myself to decide what is at the function level and what is at the control flow level. If you design your functions correctly, you don't really have to change the source files too much once they're set up. I will usually make them sufficiently general that I can copy/paste them between projects with minimal changes. Like at work I have a `pipeline.R` file that defines functions like `clean_missing()`, or `recode_levels()` with defaults that match my intuitions for the data I use daily. So depending on the model my `01_features.R` script can be like 20 lines where I call 5-6 functions and save the results. ",1539915260.0
Holophonist,That's what I do so I hope it's good practice!,1539908197.0
Thaufas,"Using `source()` is not a terrible approach, but you have to be really careful. When your workflow gets really complex, if you're using `source()` to call files from multiple places, you run the risk of circular calls, which can break your environment.

If you're working with `knitr()` and *RMarkdown*, you are much better off to use `knitr::load_chunk()` and then call the code chunks as you need them. The [Code Externalization](https://yihui.name/knitr/demo/externalization/) section of the knitr documentation explains the concept well. Yihui Xie, the author of knitr gives some advice in this [Stackoverflow post](https://stackoverflow.com/questions/33269865/how-to-make-code-chunks-depend-on-all-previous-chunks-in-knitr-rmarkdown#33272773).",1539915667.0
mattindustries,"I usually have it set up like this

    📂functions
    |-📄functions.R (or seperate files)
    📂import
    |-📄doc.xlsx
    |-📄doc.csv
    📂export
    |-📂xlxs
    |-📂csv
    |-📂png
    |-📂pdf
    |-📄readme.html
    📄run.R
    ⚙️conf.json
",1539908240.0
MeanMrMustard92,Use [makefiles](https://thomasleeper.com/2016/09/make-make-make-again/) for any large projects. I find it really useful because it focuses on the output (be it a table / plot / executable etc) and specifies dependencies explicitly.,1539923602.0
grumpymonk2,"It's a massive improvement on a single, large script.

However, it's not perfect - there's a limit to how well you can scale code written in this way. For example. it can be very hard to test and debug code written as a long linear script (whether it's a single file or many), and keeping track of variables across multiple \`.R\` files is not easy. It's also strongly coupled - if you change \`load.R\` you may end up breaking \`analysis.R\` but you need to be very familiar with your code to know that.

A better approach is to try to write as much of your code as functions as you can, and then use these functions in a single file (\`main.R\`) or with multiple files like you do now. 

Functions have many advantages 

\- they're reusable between projects

\- they're easier to test and debug, because each function should do one fairly small task

\- you can write unit tests for them, and then make changes to your code without having to worry about breaking other areas (see [https://cran.r-project.org/web/packages/testthat/index.html](https://cran.r-project.org/web/packages/testthat/index.html))

\- functions can be documented in a standard way using \`roxygen2\`

&#x200B;

For extra marks, combine that with version control using git and ensure reproduce-ability with packrat!

&#x200B;

&#x200B;",1539933245.0
mghoff330,"Also curious about this!

RemindMe! 1 day",1539908177.0
KingZer0,RemindMe!2days,1539924019.0
theghostofelvis,Check out the section on [lubridate](http://r4ds.had.co.nz/dates-and-times.html) in R for Data Science. Hope it helps!,1539900560.0
SlimShady292,I would just throw that into excel and switch the formatting there. Then cbind() that vector back on to your data,1540231848.0
petit_juju,"If I've correctly understood what you would like to do this is the simplest way I know:

`A <- ""the quick brown fox jumps over the lazy dog""`

`as.factor(unlist(strsplit(A,' ')))`

&#x200B;

Good luck! ",1539901467.0
blankepitaph,`stringr::word` ([documentation](https://stringr.tidyverse.org/reference/word.html)) coupled with `as.factor` might be your best bet. You could use `dplyr::mutate` to vectorize this operation over the entire column. ,1539893332.0
504aldo,"Library(stringr) is your best bet OP



    string_vector <- c(""the quick brown fox jumps over the lazy dog"", ""the quick dog jumps over the lazy brown fox"", ""the dog just ate cheetos"")

    library(stringr)
    str_extract_all(string_vector, ""[a-z]+"", simplify = TRUE)

**simplify:** If FALSE, the default, returns a list of character vectors. If TRUE returns a character matrix.

and if you want to then bind them and transform them to factors.

    dt <- data.frame(cbind(dt, str_extract_all(dt$string_vector, ""[a-z]+"", simplify = TRUE)), stringsAsFactors=FALSE)
    Levels:  ate brown cheetos dog fox jumps just lazy over quick the",1539907070.0
lederi,"One way off the top of my head:

library(tidytext)

library(dplyr)

  x <-  c(""the quick brown fox jumps over the lazy dog"", 
          ""the quick dog jumps over the lazy brown fox"", 
          ""the dog just ate cheetos"") %>% 
 as_data_frame()

unnest_tokens(x, word, value)

",1539895781.0
jkapow,"One way is you could separate_rows() to tidy the data, then factor the column

https://rdrr.io/cran/tidyr/man/separate_rows.html",1539898141.0
MrLegilimens,anova() and TukeyHSD(anova()) is what you're looking for. ,1539882744.0
dugorama,"First of all, don't do anova with unbalanced cells. Just not robust. And the answer is not to throw away information by sampling down. Really, it's time to migrate to multiple level 'mixed' regression, maybe glm of some sort. ",1539880720.0
SlimShady292,lmtest() might be good for that,1540061060.0
engti,"you need to pass your file location to read excel

read\_excel(""path\_to\_file"",sheet=1)

&#x200B;

the above will read the first sheet for the specified excel file.",1539875633.0
SemanticTriangle,Why don't you add a geom_line using the predicted points from your model as the input data?,1539855310.0
Thaufas,"Perhaps some of these plots might give you inspiration.

[Option 1: Multiple Segments on a single plot](https://i.imgur.com/ozXyjgB.png)

[Option 2: Break the segments into multiple facets](https://i.imgur.com/bOFLUlO.png)

[Option 3: Use box plots with individual points jittered and superimposed](https://i.imgur.com/kibgGVM.png)


    library(datasets)
    library(tidyverse)
    
    # mtcars
    model <- lm(formula = mpg ~ wt:cyl, data = mtcars)
    
    summary(model)
    
    mtcars %>% ggplot(aes(x = wt, y = mpg)) +
      geom_point() +
      geom_smooth(method = ""lm"") +
      ggtitle(""Original Plot with Smoothed Region"")
    
    # Option 1: Plot multiple segments on single plot
    mtcars %>% ggplot(aes(x = wt, y = mpg, color = as.factor(cyl))) +
      geom_point() +
      geom_smooth(method = ""lm"") +
      scale_colour_discrete(name = ""Cylinders"") +
      ggtitle(""Option 1: Multiple Segments on a single plot"")
    
    # Option 2: Break the segments into multiple facets
    mtcars %>% ggplot(aes(x = wt, y = mpg, color = as.factor(cyl))) +
      geom_point() +
      geom_smooth(method = ""lm"") +
      scale_colour_discrete(name = ""Cylinders"") +
      facet_wrap(. ~ cyl) +
      ggtitle(""Option 2: Break the segments into multiple facets"")
    
    # Option 3: Use box plots with individual points jittered and superimposed
    mtcars %>% ggplot(aes(x = cyl, fill = as.factor(cyl))) +
      geom_boxplot(aes(y = mpg)) +
      geom_point(aes(y = mpg), position = ""jitter"") +
      scale_colour_discrete(name = ""Cylinders"") +
      ggtitle(""Option 3: Use box plots with individual points jittered and superimposed"")",1539886497.0
byram,"I'm not familiar with that package, but I can tell you ""textreg"" is misspelled in your first code pic, when you try calling library on it (it says ""texreg"", with no ""t"").",1539839432.0
RememberToBackupData,_Read your error messages._,1539854057.0
revgizmo,"Just a quick glance: your calls are inconsistent:
library(texreg)
install.packages(“textreg”)",1539839571.0
r_bitquant,Geom_ribbon in ggplot will do this. Set the y max and min equal to the two lines and then set the color you want to shade the space. ,1539830184.0
VincentStaples,"Yes, use ggplot.",1539830215.0
Conrivore,"Base R is useful in certain situations. Here I would recommend looking into the polygon function. In the example below, it takes as it’s argument the line, and some parameters like color and border. You can overlay this multiple times to achieve what you want. 

This example is from: https://www.statmethods.net/graphs/density.html

Good luck.


 # Filled Density Plot
d <- density(mtcars$mpg)
plot(d, main=""Kernel Density of Miles Per Gallon"")
polygon(d, col=""red"", border=""blue"")
",1539833921.0
lucretiuss,"I don't know the answer to your question, but why are you using base r?",1539827893.0
infrequentaccismus,"Wrap scale inside ifelse() in the mutate call. 

    mutate( my_col = if_else(is_correct, scale(other_col), other_col))",1539823486.0
r_bitquant,Can you do a group by ppt and correct trials?,1539822564.0
mLalush,"Option 1:

```
data %>%
  mutate(b = case_when(
    a == 2 ~ 1,
    a == 1 ~ 0
  ))
```

Option 2:

```
data %>%
  mutate(b = ifelse(test = a == 2,
                    yes = 1,
                    no = 0))
```",1539798581.0
Er4zor,"```
data %>% mutate(b = ifelse(a == 2, 1, b))
```    ",1539797972.0
Statman12,"Like this?

    data %>%
      mutate( b = ifelse( a==2, 1, b)  )",1539797988.0
engti,"data %>% mutate(
  b = case_when(
          a == 1 ~ 2
          )
  )

I think something like above shouldnwork. Be care care if it doesn’t find a match, it’ll put NA in those rows. The case when statement can have multiple conditions.

https://stackoverflow.com/questions/38649533/case-when-in-mutate-pipe",1539798159.0
seeellayewhy,Anyone here attended one of these before? The DC one is coming up soon and I plan on attending. The talks look to have been interesting in previous sessions. What's the atmosphere like?,1539835206.0
Darth_Marrr,"My suggestion: do this out manually to see what the final result would look like and, thus, you can make a comparison with your method above. 

First find your principle components, second, regress the PC's on Y i.e. Multivariate Regression, thirdly, apply a variable selection technique, so that you find the ""best"" PC's to run your regression (reducing dimensionality), and finally, apply cross validation using the caret package, or write a loop yourself, to tune your model appropriately. 

Too often these ""one and done"" packages are created and the source code isn't verified, but based on community consensus you can find packages that hold more weight over skepticism, so let them be your guide. 

I would also offer the advice that PCA reduces the dimension of your X (design) matrix, but in no way does so with your response in mind. PLS is a method that does this, but it has its flaws.",1539832153.0
ExcellentOdysseus,"On mobile, can’t access this sadly. But I just wanted to say best of luck jumping through the rest of the SOA’s hoops",1539779173.0
ExcellentOdysseus,"Hi again, I was able to take a look at your work. You will be one of the first people to take this practical, correct? If you can produce work of this quality under those circumstances, I think you'll make ASA. You code is clean and well documented. You know R better than most actuaries I've worked with, and you're able to communicate effectively. Consider making your coefficient table more readable, maybe put it into a data frame, or push it into excel. 

Do you know any VBA? I know it's probably not ideal for this project but we use it a lot, esp to clean data(although this may to be standard about offices). I don't know how this exam is scored, but I think if you're able to throw some in there, you will look better to a grader. 

Good work, Jack. ",1539838194.0
Rctuary,"Hey -- do you want a job? Check out my posting history and send me a PM if you're at all interested.

I'm also going through these modules and ISLR -- will probably also join you, sitting in December. Best of luck!",1539895285.0
Angryhamstrings,"Try:

approx(sapply(A, '[', 1), sapply(A, '[', 2))
",1539743779.0
notqualifiedforthis,"I think I've accomplished it with the below.  I may need to replace the chartr portion of this because I'll have more than 9 groups within an incident.  Open to ideas.

    df %>% group_by(incident) %>% mutate(test = chartr(""123456789"",""ABCDEFGHI"",match(group,unique(group))))

&#x200B;",1539732568.0
Darwinmate,"Your `time` variable is perfectly formatted, so why not treat it as a `date` object? If you are used to `dplyr` then `lubridate` should be very similar (part of the `tidyrverse` ).

Convert to datetime object (called `POSIXct`):

    df$time <- mdy_hms(df$time)

An example of usage would be: 

    filter(df,  mdy_hms(""10/16/2018 10:05:00"") > time)

Combined it looks like this:

    df %>% mutate(sequence_label = case_when(mdy_hms(""10/16/2018 10:00:00"") == time ~ ""A"",
            mdy_hms(""10/16/2018 10:05:00"") == time ~ ""B"",
            mdy_hms(""10/16/2018 10:15:00"") == time ~ ""C""))

`case_when` is very similar to `ifelse`.

If your time intervals are less strict, you can add an or using pipe eg:

    mdy_hms(""10/16/2018 10:05:00"") <= time | mdy_hms(""10/16/2018 10:010:00"") >= time ~ ""A""

",1539755919.0
hmt28,"This is an issue of the locality of the variables within the function. When you create variables within a function of R those variables are only accessible within that function.

So if you were to call `asCodes2` you would likely get an error `Error: object 'asCodes2' not found`

To solve this issue you can use the `<<-` operator when assigning variable names to make them accessible in the global environment. That is, the variables created inside the function will then be accessible outside said function.

Better practice would be to return a list of all variables of interest from your function. For example, you could add `return(list(asCodes2, asBasePrice, asLandRate, asDepreciation))`. When you call `GetCostTables` then, assign that to a variable name so you can easily access the returned list of the function.

EDIT: added example of how to return variable list",1539728056.0
Thaufas,"This code is more robust and flexible than your original code. It also gives you a better understanding of a common R paradigm, which makes use of lists instead of loops. Using my code, you can easily change the name of the file containing all of the worksheets, and you can add more worksheets by simply adding more *worksheet names* to the `SHEETNAMES` array. Let me know if you have questions.


    # Code is not tested
    
    library(readxl)
    
    # # Your original function
    # GetCostTables <- function() {
    #   asCodes2 <- read_excel(""CostTablesStatic.xlsx"", sheet = ""asCodes2"");
    #   asBasePrice <- read_excel(""CostTablesStatic.xlsx"", sheet = ""asBasePrice"");
    #   asLandRate <- read_excel(""CostTablesStatic.xlsx"", sheet = ""asLandRate"");
    #   asDepreciation <- read_excel(""CostTablesStatic.xlsx"", sheet = ""asDepreciation"")
    # }
    
    # Define a new function to read the worksheets
    GetCostTables <- function(fileName, worksheetName) {
      stopifnot(!is.null(fileName), !is.null(worksheetName))
      stopifnot(is.character(fileName), is.character(worksheetName))
      return(read_excel(fileName, sheet = worksheetName))
    }
    
    FILENAME <- ""CostTablesStatic.xlsx""
    SHEETNAMES <- c(""asCodes2"", ""asBasePrice"", ""asLandRate"", ""asDepreciation"")
    DataFrames <- vector(""list"", length = length(SHEETNAMES))
    names(DataFrames) <- SHEETNAMES
    DataFrames <- lapply(SHEETNAMES, function(x) {
      read_excel(path = FILENAME, sheet = x)
    })
    str(DataFrames)
    
    # Save a copy of the data frame containing the worksheet, 'asCodes2'.
    df_asCodes2 <- DataFrames[[""asCodes2""]]",1539739443.0
lakenp,"Yould use the `<<-` assignment operator to assign global variables within your function, but it is highly ill-advised because you would create so-called side-effects of your function (i.e., it doesn't `return` anything useful, but does assign four variables as a side effect).

What you coud look into is making a custom `read_excel` function like you did, including a conditional to test what file you try to load in and based on that pick the right worksheet. That function you could subsequently use in an `*apply` or `purrr::map*` function to load in your datasets interatively. 

Another option (imo the cleanest) is to use a list of directories and a list of worksheets simulatenously in a `purrr::map2` function with the regular `read_excel` as function entry, which would then load in the right worksheet for each excel and return them in a list.",1539755450.0
PSJupiter2,"Thanks All,

&#x200B;

I used the <<- operator in my function and it works as expected.  I am too green to know what the pitfalls of this are.  But I am sure Eureka! will happen.

&#x200B;

&#x200B;",1539786756.0
TheCreamsman,"    odds <- df %>%
      filter(subject_id %% 2 == 1)    

or in base

    odds <- df[df$subject_id %% 2 == 1,]",1539725599.0
keepitsalty,"In terms of general rules, the closest thing I can think of is [The R Inferno](https://www.burns-stat.com/pages/Tutor/R_inferno.pdf). But you're probably talking more about the easter egg side of things. I mean there is the `brrr` package which plays raps song snippets, which is pretty hilarious. ",1539740730.0
_Wintermute,"Could we please adopt of a few of the python ones?

    Readability counts.
    Errors should never pass silently.
    If the implementation is hard to explain, it's a bad idea.

Or actually just make R package authors read the whole list?",1539766937.0
s3x2,"`install.packages(""fortunes"")`",1539722933.0
Er4zor,"I'd say there are no common guidelines.    
Actually, base R is **awful** in terms of consistence, good practices, good documentation.

The Tidyverse is much better, and it shares a [manifesto](https://cran.r-project.org/web/packages/tidyverse/vignettes/manifesto.html)",1539772844.0
Jordo82,"I believe the issue is caused by a couple of things.  First, year is a factor, which ggplot treats differently from a numeric.  But the larger issue is that you've got 60 observations of yield for each year.

    barley %>% 
      group_by(year) %>% 
      summarise(n())

Unless you somehow summarise the data, it's going to try to plot every one of those observations for each year.  And without specifying how to group the observations, ggplot can't know how to connect the points from 1931 to 1932.  For example, in the second line you mention:

    ggplot(barley, aes(x = year, y = yield, col = variety, group = variety)) + geom_line() 

You're grouping by variety now, but there's still 6 different sites so there's 6 observations per variety per year.

I'd recommend deciding how you want to summarise the data prior to feeding it into ggplot.  For example, if you just want to look at the mean yield across all varieties and sites:

    barley %>%
      #convert year from factor to numeric
      mutate(year = as.numeric(levels(year))[year]) %>% 
      group_by(year) %>% 
      summarise(AvgYield = mean(yield)) %>% 
      ggplot(aes(year, AvgYield)) + 
        geom_line()",1539717277.0
hmt28,Hey cool tutorial! I quickly glanced it over and one thing that stuck out to me was the test/train splits were done by hard-coding index values. If you were to do it this way I’d recommend randomly shuffling the observations prior to splitting your dataset. ,1539709919.0
moqiwf,"Maybe it's just me, but I don't think his code is rendering properly in the blue code blocks.",1541503134.0
dontcometomontana,"Your loop is overriding lambdas every iteration. 

Make pois a list or a matrix and add every iteration into it with your i variable. 

I can give an example later at my computer if needed. ",1539652220.0
efrique,"Why would you loop for this?

Do you need it in a vector (10 values for lambda1 stacked on 10 values for lambda2 etc ...) or in a matrix (say a row for each lambda) or in a list (separate vectors for each lambda)?

",1539667287.0
flyos,"A vectorised solution to avoid the loop and apply/map functions:

    K <- 10
    lambdas <- c(0.25,0.5,0.75,1,2,3,4,5,6,7,8,9,10)
    l <- rep(lambdas, each = K)
    pois <- rpois(K * length(lambdas), lambda = l)",1539690451.0
infrequentaccismus,"    purrr::map(lambdas, ~rpois(10, .x))",1539656714.0
jimmyrigby94,"I had problems too - kept getting a server error. Eventually, I just ran the app via rstudio.",1539630520.0
vegetableagony,"They are the same in virtually all situations. (The one exception is if you want to assign a variable inside a function call e.g. rnorm(n <- 5) which is generally not a good idea )

People have strong opinions in both directions about which to use. ",1539619547.0
guepier,"The other comment sums it up well. For a more technical discussion of the differences, see https://stackoverflow.com/a/51564252/1968.",1539660387.0
mouse_Brains,"Teaching community seem to think <- is easier to understand than = for newbies. I say that's silly and you shouldn't assume R students cannot grasp how assignment works without the operator showing the direction of said assignment.

I personally ""use"" <- as an indicator of copy pasted code in case I forgot to take notes. If I took the code from somewhere else, it'll have <-'s in it while my own code would never have them.",1539759255.0
moosejock,"https://i.imgur.com/fyJiVgu.png

The answer is pretty clear. ",1539705070.0
Darth_Marrr,If you're in Rmarkdown why not just knit to PDF?,1539618655.0
questionquality,"You will probably do better in the future if you solve this homework problem by yourself. If you have any specific problem (ie show us what you tried and where *specifically* you're stuck) you can ask that. But as it is, you've just pasted your homework problem ..",1539608767.0
Quasimoto3000,"Dude, what’s with your space bar?",1539779214.0
brazzaguy,Thank you ,1539676183.0
The_Biggest_Gentile,I think this [tutorial ](http://kateto.net/network-visualization) might help,1539568957.0
eldenv,"I think I have tried to do something similar, and I made a post about it here: https://lookatthhedata.netlify.com/2017-11-12-mapping-your-oyster-card-journeys-in-london-with-tidygraph-and-ggraph/

Basically, you can combine ggraph and ggmap.

If you store your ggmap to a variable `map`, and then you also store the first line of your ggraph call, e.g `gg <- ggraph(routes, layout = ""nicely"")` then all you have to do is:

`ggmap(map, base_layer = gg) + [the rest of your ggraph geoms etc]` and it works out fine!

",1539600554.0
aftersox,"I have done this, but it hasn't been easy. I normally use Gephi and igraph, but ggraph and ggnetwork have come a long way towards integrating network visualization into the ""grammar of graphics"" framework.

GGnetwork would work exactly for your (and my) needs, if it didn't have all these default interpretations of the parameters. My solution is to use the ggnetwork object, which is incredibly useful on its own, as a dataframe to plot the network on a ggmap object. You position the nodes using lat and long, then use geom\_segment to draw the edges. I've had pretty good success using it that way. I'm having trouble finding a snippet of code I used for this, but if it helps I can look a bit harder.

&#x200B;

Edit: Nevermind about ggnetwork. I just built the dataframe myself:

First I had a function that pulled a network (I was plotting dozens of them for different regions) and built this object from the network data.

`g2 <- prep_g(g, e.att = e.att, cutoff = cutoff)`

`vs <- get.data.frame(g2, what = 'vertices')`

`es <- get.data.frame(g2, what = 'edges')`

`es <- merge(es, vs %>% select(name, x = lon, y = lat), by.x = 'from', by.y = 'name') %>%`

`merge(vs %>% select(name, xend = lon, yend = lat), by.x = 'to', by.y = 'name')`

Then plotting worked like this. And I  used ggrepel for the labels.

`ggmap(region) +` 

`geom_segment(data = es, color = 'grey30', alpha = 0.8,`

`aes(x = x, y = y, xend = xend, yend = yend)) +`

`geom_label_repel(data = l$vs, aes(x = lon, y = lat, label = abbrev)) +`

`geom_point(data = l$vs, pch = 21, color = 'white', aes(x = lon, y = lat, fill = as.factor(memb), size = size)) +`

`scale_color_brewer(palette = 'Set1') +`

`theme_blank()`",1539582202.0
jcintasr,Amazing! I will take a look in no time. This night probably. Thanks again!,1539586588.0
Wurtzinator,I'll check it out tonight! Great thanks! ,1539600782.0
Js5h,"I'm not sure what's going on with the weekly/monthly moving averages calculated in the first video. Isn't the data weekly sales by default (since there's one entry per week)? Then a ma(7) or ma(30) won't be a weekly and monthly moving average, or am I misinterpreting the dataset? ",1539630300.0
Darth_Marrr,"> automated grading or R scripts 

Forgive my ignorance, but can you please elaborate on what this is?",1539538757.0
mLalush,https://github.com/MansMeg/markmyassignment,1539538779.0
don_draper97,"My method would be to just create a new column called birthweight_dummy or whatever you would like. 

Then, I would use the `ifelse` function to fill the column with the dummy variables as you specified. Something like this:

`nat92$birthweight_dummy <- c()`

`nat92$birthweight_dummy <- ifelse(nat92$birthweight < 2500, 1, 0)`

In plain English, this is essentially saying ""If the birthweight is less than 2500, change that value to a 1. Otherwise change it to a 0. Store this vector in the column we just created."" 

This is one method of doing what you want. Note that you COULD just replace the original birthweight column with the vector resulting from the ifelse function, but as a personal preference I would not replace the original column since you might need it later. ",1539529460.0
blankepitaph,"I haven't used `cut` much, but if I understand correctly, this can be done with `mutate` from `dplyr`:

`dplyr::mutate(nat92, dummy = ifelse(birthweight < 2500, 1, 0))`

This will create a copy of the data frame with a new column called `dummy` containing 1s and 0s based on the value of `birthweight`, all in one step (...and kind of circumventing the need for an actual dummy column, really) 

If you want to replace the old column outright, overwriting the original data frame:

`nat92 <- dplyr::mutate(nat92, birthweight = ifelse(birthweight < 2500, 1, 0))`",1539529625.0
bluestorm21,"You can just make a new column defined exactly how you described it: 

`class <- nat92[, 1] < 2500`

That will return a logical vector, but you can get an integer vector using `as.integer()`.

You can use `cbind()` to add it to a matrix or data.frame, or define it in a mutate block for a tibble.

EDIT: obviously `cbind()`, not `rbind()`",1539530647.0
s3x2,"Have you cleaned that data? 208g shouldn't be possible. Also, you don't need a dummy variable at all. The correct procedure is to use birthweight as is with any model that works for numeric vars and then apply the threshold to the model's predictions.",1539531518.0
keraj93,"I'm not exactly sure what you mean, but if you run a logit/probit or often other probability regression you should know that the coefficients do not directly show the ""marginal effects"". They need to be calculated.",1539509294.0
Kono_Diogenes_da,"In a logistic regression, the beta coefficients for your predictors represent the change in *log odds ratio* resulting from a 1-unit increase in the predictor. 

Example: if your predictor has a coefficient of 2, then a 1-unit increase in that predictor results in a change in odds ratio of exp(2) = 7.39. That's a strong predictor! ",1539528621.0
efrique,"does this help at all?

https://stackoverflow.com/questions/35185814/including-tex-file-in-rmd


ALso see the comments here:

https://stackoverflow.com/questions/42027792/input-tex-in-rmarkdown
",1539491842.0
samclifford,Have you seen this page? https://rmarkdown.rstudio.com/pdf_document_format#custom_templates,1539496874.0
Hasnep,You can also save a .tex file as a .Rnw file and use Sweave.,1539509469.0
Galileotierraplana,knitr in r package lets you compile latex and R code into pdf,1539507015.0
datatitian,`expand.grid` to find the combinations and `mapply` for the multi-variable version of apply.,1539491307.0
ryanmonroe,"Use 

    Map(func, param1, param2)",1539485461.0
Ringbailwanton,"You’d need to parse the results, not sure what output you expect, but just nest the lapply:

lapply(param1, function(x) { lapply(param2, function(y) {customfunction(x,y)}}))

You could do.call, or try purrr and map and bind rows.
",1539535871.0
dontcometomontana,"lappy(param1, fun(x) customfunction(apply(param2, fun(y) secondcustomfunction(x,y))) 

You can nest an apply in your lappy/sappy function and use both of the variables x and y in the apply function. ",1539533400.0
veggiemedley,"What’s the role of data frame B?

Anyway, you have lots of options. Create an empty matrix or data frame and populate it during the loop.

Sounds like you can just use the apply family though, apply(A, 2, function).

FYI, I also downvoted because this is not a well thought out question and one that could easily be answered via google and stackoverflow ",1539476446.0
maxblasdel,"Create an empty list outside the loop: results<-list()
Populate the list at the need of the loop: results[[I]]<-output. 
This is pretty much what I do for every loop I use. ",1539480924.0
efrique,">  The later are a subset of the 11 that JMP selects, but overall there are 29 variables. Why the difference?

Likely different default criteria. To begin with you would check the documentation for each. If one of them doesn't let you get the information on what it's doing in a way that would let you reproduce it exactly in code, it would be reckless to use it at all.

>  This is a big issue in Statistics that no one is addressing!

Well, no. Neither is it a big issue (since forward selection or any other ""best subsets"" approach is a pretty bad idea in general so most people avoid it in favor of some form of regularization, so minor implementation differences in defaults are hardly a burning problem), nor is it unaddressed; people have certainly looked at effects of changing various things that might be changed, but one may do so under any set of assumptions using simulation, so it's a problem anyone can tackle to some extent if they need to. 

---


The package is here:

https://cran.r-project.org/web/packages/leaps/

its documentation is here:

https://cran.r-project.org/web/packages/leaps/leaps.pdf

gzipped source code tarball is here: https://cran.r-project.org/src/contrib/leaps_3.0.tar.gz

So presumably you can find out exactly what it does by reading the code.

Since it's based on fortran code by Alan Miller, you may even be able to find Alan's original code here:

https://jblevins.org/mirror/amiller/#subset


",1539487587.0
eldgrimr,"Stepwise regression isn't really used in my experience due to various issues (bias, overfitting, wrongly distributed statistics, etc.). You may find [this blog and comments](https://andrewgelman.com/2014/06/02/hate-stepwise-regression/) helpful in understanding why.",1539455513.0
Quasimoto3000,Too many walls of code. Needs more exposition. ,1539439850.0
Petzval,"As of July Google's geocoding API only works when billing is enabled and thus API key is provided. If I'm correct the version on CRAN does not allow passing an API key so it can not be used for geocoding.

I use OpenCage API through [opencage package](https://cran.r-project.org/web/packages/opencage/index.html) but this also has a 2500 requests per day limit.

[https://developers.google.com/maps/documentation/geocoding/usage-and-billing](https://developers.google.com/maps/documentation/geocoding/usage-and-billing)",1539369678.0
ConwayPuder,Thanks,1539379971.0
ReimannOne,"> Why does this error happen?

Because you don't have an API key.  All the documentation says that you're allowed 2500 requests per day without a key, but in practice it doesn't work that way.

Without a key, the geocode function will often return an error when you ask for several addresses to be geocoded.  

You've found the solution (get an API key), but you don't want to use it.  There's not much else anyone can do to help you.",1539443335.0
Ringbailwanton,What kinds of projects are you interested in?  Package vignettes are usually a pretty good overview for functionality.  From there you could search “library(packageName)” in the GitHub search and look for projects that use that package.,1539360930.0
agclx,Most packages are open source.  The [CRAN task view](https://cran.r-project.org/web/views/) give a good overview of the influential packages.  For scientific working the [Journal of Statistical Software](https://www.jstatsoft.org/index) has most articles with R code.,1539446154.0
mouse_Brains,Ropensci has all sorts of packages from very simple to complex. They also pass a code review so they should be pretty readable.,1539759703.0
JaeHoon_Cho,"Correction of the title (should be modelvar)

But yea, this page was just to show that the R^2 explains the variance of the model/total variance .

I understand how you obtain the variance of the residuals, but am having trouble understanding the variance of the fitted.

So if we had a scatter plot and we could make a linear regression model of it. The values that sit along that line would be the fitted values. I don’t really understand how you find the variance of a line, and I don’t even think that would be possible unless the line was bounded on both ends. So I think my understanding is wrong.",1539356997.0
Pine_Barrens,"As much as I do love the tidyverse and still think it's the best vocabulary to manipulate data across Python/R/SAS, etc, people should also take some time to learn data.table! There are certain things it is extremely good at, and other packages often take advantage of some of those features to make themselves pretty fast",1539349974.0
Hoelk,"I have to advocate a bit for `data.table` again here. 
Theres a detailed rundown of [data.table vs dplyr on stackoverflow](https://stackoverflow.com/questions/21435339/data-table-vs-dplyr-can-one-do-something-well-the-other-cant-or-does-poorly) that says everything that can be said to that topic, but I decided to post some personal opion here:

Background: I've used `dplyr` in the past and switched completely to `data.table`, the main reasons: 
* much lighter dependencies. 

Other advantages of data.table are:
* the superb subset assignment syntax (see bellow), 
* speed (rarely an issue in practice), and 
* ease of use for programming as you can usually get around NSE . In cases where you can't, the whole rlang framework will probably let you do things where data.table fails, but I never came across such a case. 

I do not care about any of the database operations with dplyr, I kinda prefer to send SQL statements myself.

Some `data.table` syntax might be arcane, but the same can be said for dplyr, especially when it comes to dealing with NSE.

So... here we go:

# Subset assignment

A very common operation for me where data.table is imho the clear winner, for speed and clarity of code.

```
dat[x == 5, y:=3]
dat[x == 3, y:=2]

vs

dat <-
  dat %>% 
  mutate(
    y = case_when(
      x == 5 ~ 3,
      x == 3 ~ 2,
      TRUE ~ y
    )
  )
```

# grouped_by, transmute

Tie: dplyr code looks slightly more elegant, but you don't have to worry about NSE in the ""by"" statement in data.table. Transmute looks very similar for both cases (just drop the ""by""/ replace summarize by transmute)

```
dat[, .(
  x = sum(y)
  r = sum(plum)
),
  by = ""z""
]


dat %>% 
  group_by(z) %>% 
  summarize(
    x = sum(y),
    r = sum(plum)
  )
```

# pipability

Winner: dplyr, but data.table has set functions instead. This hurts a bit because I really like pipes, but you cant have everything. It's not that you can't use pipes with data.table, but it often looks kinda ugly and so there is no real reason to bother with them.

```
dat <- dat %>% 
  rename(blubb = schwupp, wupp = fupp) %>% 
  arrange(blubb, schwupp) %>% 
  select(schwupp)


setnames(dat, c(""schwupp"", ""fupp""), c(""blubb"", ""wupp""))
setorderv(blubb, ""schwupp"")
dat <- dat[, .(schwupp)]

```

# printing

I prefer the print method for data.tables to tibbles, but again you have to decide that for yourself


# Reshaping

`data.table` clones the syntax of `reshape2`, whereas the tidyverse way is to use tidyr. I find the reshape syntax as well as a the tidyr syntax kinda awkward, but both are easy to use when you get used to them. Again, massive dependencies for tidyr, non for data.table.

# Conclusion
So is `data.table` a clear winner here if you just look at the examples above? Maybe not, but consider `data.table` has no dependencies (except methods, which is a part of base), and dplyr has 10. That makes it imho much saner to use data.table inside a package than dplyr.
",1539363622.0
CasinoMagic,"StackOverflow questions and answers is a poor proxy for a popularity metric.

The number of package installs or downloads would be way better.",1539353412.0
legend67,How is dplyr being mentioned before it was released?,1539369994.0
fasnoosh,"Couldn’t “# of stack overflow questions” also mean “crappy documentation and/or hard to understand syntax/interface”? If the help docs explain my question, I won’t even think to consult stack overflow",1539394774.0
DeclareVarNotWar,"It also helps that RStudio and the people behind it are doing tons of stuffs to promote the `tidyverse` in general especially on social media. Moreover,  the syntax of`dplyr` or `tidyverse` verbs are much easier to learn than those of `data.table`",1539413174.0
jpiburn,"Is it just me or does the creator of data.table kinda seem like a dick? Very defensive and combative when it comes to comparisons with dplyr. 

I mean the package is great and better at certain things than dplyr, but his attitude in posts I've seen are very strange for someone that produces open source software. Almost angry ",1539404211.0
lakenp,Could you share your theme settings to generate this plot?,1539353897.0
another30yovirgin,"To be honest, I think both trends are bad for the language. When you give me dplyr or data.table, they both are so different from base R that they look like a different language. Since I'm slightly familiar with both, I can usually figure out what they mean, but back in the day when I was new to the language, they would just confuse me. The result was that I'd find myself on stackoverflow looking at answers that are based on one of those packages and I wouldn't understand them at all. The solution becomes just copying and pasting and hoping for the best--and also not learning. ",1539386731.0
Haligonia,dplyr sucks,1539371068.0
ohnodingbat,"I'm not sure either tags or answers mentioning something are a good measure of ""popularity"" as much as that people have questions about it.  Over in the political sphere, mentions/tags of a particular politician are too numerous to count but is the person ""popular"" in the way that golden retrievers are popular? ",1539358036.0
ZZ-TOP,I love R!! Great job mate. ,1539352051.0
swilts,"I use to love data.table, don't use it as much lately though. It's really a shame that the authors didn't find a way to merge with dplyr somehow",1539352074.0
RememberToBackupData,"You’re not referring to the data that you loaded into the file variable, you have a lot of syntax errors, and none of it is very close at all. You need to go back through your lessons and learn from the basics without trying to skip ahead. 

If you don’t find your lessons helpful, try googling “swirl rstats” and using that package to teach yourself. ",1539349242.0
VincentStaples,">LEAVE = ifelse(LEAVE == 1, (""routine""), (""intensive care"").  

You're missing a parenthesis here after intensive care, and why are routine and intensive care in brackets? Read your errors.",1539349040.0
GildedFuchs,"So, I put together a bit of a tongue-in-cheek data.table solution to this a few days back. The main piece of advice I had was to go back and refer to the basics. 

This stuff is about as simple as R gets, and you will quickly be out of your depth in a course that utilizes or assumes any knowledge of the language. There is a ton of reference material (including your lesson notes). There's not really a shortcut to learning this stuff.

Here's a guide, ""R for Beginners"" which may help:

https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf",1539354148.0
G_NC,"I'm a big user of Stan (and, by extension Rstan and brms) and the R environment is really, really easy to work with. I use a ton of the tidyverse packages which make data management and reshaping really easy. I'm sure a lot of this stuff could be done more efficiently in Python, but from a simplicity factor I really think R is the winner for Bayesian computation. ",1539347650.0
GameDaySam,"I wouldn’t say “R” is the clear winner. Like with everything else it is very good at some things and very bad at others.

I personally prefer python to everything else because of code readability and flexibility.",1539342225.0
Mooks79,"I’m not convinced calling any of them a numerical computing language is particularly accurate. That would imply prioritising numerical computation speed over readability. Maybe Julia attempts to do that - or at least doesn’t prioritise too strongly the other way. I’d say you’d have to go to something like Fortran to start using that phrase. 

Those languages are all about much more than pure computation speed. Although they can be made significantly faster by various tweaks. 

The question of best is really a broad one. Best for what, exactly? It ain’t speed, unless you’re writing a code heavy problem, which is not too computationally tasking, that you’re going to run once or twice. 

Edit - plus that speed comparison on the link is essentially useless. 

R is notoriously slow with loops - but is the code made vectorised for R? That would speed it up a lot. It’s completely daft to write loops in C and compare them to loops in R - unless the code can’t be vectorised for some reason.

The Python benchmark is using an additional library compared to base R - apples and oranges.

Use an optimised BLAS library in R and it’ll speed up an enormous amount. 

All in all it’s not a great article. ",1539343918.0
true_unbeliever,They forgot Gauss by Aptech.  It’s still a contender.,1539341992.0
_Wintermute,I'm not sure you can seriously consider R as a numerical computing language when it can't cope with 64 bit integers.,1539387006.0
another30yovirgin,"Sadly I find that most economists still use Stata.

>Third, documentation and quality of packages can be indeed very bad (especially ones you get from Github).

Seems like an unfair critique, since those are, by definition, the ones that haven't been embraced by CRAN (yet). ",1539389684.0
VincentStaples,"plotly is incredibly resistant to customization unfortunately. There may be a solution to this issue, but I wouldn't be surprised if it turns out that there isn't and that they won't address the problem themselves.",1539345496.0
SemanticTriangle,[Logistic regression](https://www.statmethods.net/advstats/glm.html)?,1539308057.0
sharkinwolvesclothin,"I'm not familiar with stata and not quite sure what you are looking for, but if you take do log(y)~x, the coefficient is percent change in y between the groups. Percent change in x doesn't make sense, so not being able take log is a feature, not a bug. ",1539320897.0
ryanmonroe,The tidyverse solution is to have everything in a list and not scattered in your global environment.,1539285895.0
jpiburn,"This might help
https://dplyr.tidyverse.org/reference/select_helpers.html",1539282422.0
bluestorm21,I think you have an xy problem here. Could you give a little more background on what you're doing?,1539294596.0
northernwildling,Check out the broom package for tidy model statistics ,1539297807.0
VincentStaples,Just awful.,1539291168.0
VincentStaples,Why have you posted this? The output goes wherever you put it in the UI layer.,1539227339.0
VincentStaples,"There is no sliderInput in your code, but just move textOutput out of the mainpanel into the sidebarpanel.",1539227833.0
RememberToBackupData,"    # Install, update, and attach packages from three repositories at once:
    
    librarian::shelf(dplyr, DesiQuintans/desiderata, zlibbioc)
                       ↑        ↑                      ↑
                      CRAN     GitHub                 Bioconductor

This update adds Bioconductor support to [my package-management package, `librarian`](https://github.com/DesiQuintans/librarian). If you have the package `Biobase` installed in your library, `librarian` will try to install from the Bioconductor repos too. This version is currently only available through Github:

    install.packages(""devtools"")
    devtools::install_github(""DesiQuintans/librarian"")

---

**FAQ: What makes `librarian` different from `pacman`?**

1. Pacman uses two different functions for installing and attaching from CRAN, Github, and Bioconductor. Librarian uses one function for everything.
2. Pacman has a different interface for installing packages from different sources (CRAN/Bioconductor uses bare names or strings, Github uses strings). Librarian uses bare names only.
3. Librarian has better control over unloading packages, allowing you to also unload the dependencies of packages in safe or unsafe ways.
4. Librarian wraps `.libPaths()` to make it easy to create new library folders.

---

**FAQ: Why is this a helpful thing?**

Because I can attach 17 packages in 2 lines:

    librarian::shelf(knitr, kableExtra, PEASdata, tidyr, tibble, dplyr, janitor, readr, stringr, glue,
                     DesiQuintans/desiderata, vegan, ggplot2, ggthemes, forcats, magrittr, purrr)

And if I feel like testing out new packages, I can just add them to the list and `librarian` will install them for me. And if I want to update all of those packages, I can just add `update_all = TRUE` to the end of that.",1539219043.0
rharrington31,"I'm not sure if this is actually answering your question, but it sounds like you're talking about conditional evaluation of code chunks. Take a look at this [documentation](https://bookdown.org/yihui/rmarkdown/r-code.html).

_Relevant text/code here:_

>The value of a chunk option can be an arbitrary R expression, which makes chunk options extremely flexible. For example, the chunk option `eval` controls whether to evaluate (execute) a code chunk, and you may conditionally evaluate a chunk via a variable defined previously, e.g.,

    ```{r}
    
    # execute code if the date is later than a specified day
    do_it = Sys.Date() > '2018-02-14'
    
    ```{r, eval=do_it}
    
    x = rnorm(100)
    
    ```
    
I haven't actually had a need to try this out myself before, so can't answer more questions beyond this. It seems like it could be useful, though. Hope this helps!",1539220479.0
displaced_soc,"If I understand correctly, it might be similar to what I did at one point with survey results. There were several groups of persons (let's say different cities), and there was need for identical reports on survey results for each of those groups. I created a `template_00_test.rmd` report, with all tables/graphs/charts, and then called it from regular R script in a loop, which for me generated some 10 identical reports with different groups. If I wanted to tweak something in it, I would change just in template.rmd, and re-run the loop. 
        
    for (group_id in unique(survey_data$group))  {
      sdta_for_report <- filter(survey_data, group == group_id)  # data for report
      rmarkdown::render(""template_00_test.rmd"",
                        output_file = paste(""report_"", group_id, "".html"", sep = """"),
                        output_dir = ""reports"")
    }

The only thing with creating markdown reports this way is that all the variables that are used within the .rmd template are from global environment (thus, creating `sdta_for_report` which is used in template).

There is probably a better way to do this, and for your particular issue, but it solved my problem easily. 

edit: just some formatting 
edit2: Was bored, in your case it would be:

`template.Rmd`

    ---
    title: ""Scenario number `r scenario`""
    author: ""Krynnadin""
    date: ""October 11, 2018""
    output: html_document
    ---

    ```{r}
    print(x+y)
    ```

And then, from regular script you would call

    
    df <- data.frame(x = c(1, 2),
                     y = c(2, 3)
                     )

    for (scenario in (1:nrow(df)))  {
      x <- df$x[scenario]
      y <- df$y[scenario]
      rmarkdown::render(""template.rmd"",
                        output_file = paste(""scenario_"", scenario, "".html"", sep = """"),
                        output_dir = ""reports"")  
    }
",1539291730.0
Core_Four,COOL,1539197788.0
flyos,"Why don't you square your variables beforehand in your table. Something like (using dplyr):

````
data %>% mutate_at(vars(starts_with(""X_"")), funs(sq = .^2))
````

Then, you just create your formula like this:

````
form <- colnames(data) %>% str_subset(""^X_"") %>% str_c(collapse = "" + "") %>% str_c(""Y ~ "", .) %>% as.formula()
````

And call you model using `lm(form, data=data)`
",1539261973.0
millsGT49,"You can use the `poly` function on each variable or use the `:` or `*` command to create an interaction between two variables:

    y ~ poly(x1, 2) + poly(x2, 2)
    
    y ~ x1*x1 + x2*x2
    
    y ~ x1 + x2 + x1:x1 + x2:x2

Should all be equivalent but check the docs to make sure",1539186476.0
Pine_Barrens,"If you are using it for modeling purposes, can't you do (believe the same syntax works with model.matrix as well):

    fit <- lm(y ~ . + .^2,data=....) ?",1539195967.0
efrique,"Try  `x1+I(x1^2)` etc

or `?poly`

",1539236830.0
ch1ppos,It will overwrite no need to uninstall.,1539184760.0
Darth_Marrr,Why do you only have three rows of actual data? This is nonsensical then to have such a large data frame with NA's in all but three rows.,1539179384.0
Darwinmate,"It depends on what youre analyzing, what you want to learn what questions you are asking. 

Maybe first work with a full dataset that isn't missing millions of points and go from there. ",1539170601.0
halhen,"I'm in my second small (25+ people) company introducing a more data-driven approach to product and software development. Shiny is a key piece is making this happen.

I build many cheap apps (a few hundred lines of code each at most, each answering a specific set of questions), for internal use, with no regards paid to style, deployed on shinyapps.io. With a few exceptions, most apps are interactive, automatically kept fresh, more actionably aggregated replacements for Excel sheets being emailed back and forth.

The pros are too many to mention, and shinyapps makes for super convenient deploys. However, below are a few griefs even for internal use.

 * Authentication is a bit of a mess in shinyapps. AFAIK, I have to add every use individually to every app, with no UI help in selecting other users in my organization etc. I could of course hook that up via R in each app, but that takes away a lot of the convenience.
 * It is a bit slow; warming up an instance at start takes a little too long for it to be out-of-the-way. Once going, it is up to me to make is snappy enough, which usually is fine
 * Anything other than plain defaults in layout and look-and-feel seems an order of magnitude harder than sticking to defaults. Granted, this is due to the defaults being as good as they are. Still I wouldn't choose o fight the API and infrastructure to build something very custom.",1539158999.0
huessy,"The main reason is that it's a lot slower than a JS equivalent. There are rumors that RStudio is going to try to address this, but so far they can't really guarantee a ""business stable"" option when you're trying to do more than plot a few histograms. I've been trying to introduce it at work as a Tableau alternative but it has issues if you're connecting to multiple SQL sources and running multiple queries.

I agree, it's like magic, but it isn't perfect magic yet and that's part of the reason people aren't flocking to it in droves.",1539171350.0
Core_Four,"My experience with clients is they already have a power bi or tableau license. Selling them on R or Shiny is like selling them alien technology. This obviously isn't true for all customers, but it's the case for many large organizations.",1539197853.0
mouse_Brains,"There are companies that use shiny. I have a friend who's entire business model is building shiny apps for companies and individuals, he never lacked for customers so far.",1539292938.0
another30yovirgin,"It's a great idea, but it needs to be made more efficient. Right now it's just too slow. ",1539389984.0
Core_Four,"this got me MOIST

thanks for sharing",1539197991.0
Darth_Marrr,"This is the first time I am seeing this package and it is pleasantly interesting. I then went and took a cursory read of this link to find out more: 

[https://medium.com/applied-data-science/new-r-package-the-xgboost-explainer-51dd7d1aa211](https://medium.com/applied-data-science/new-r-package-the-xgboost-explainer-51dd7d1aa211)

I'm not convinced the explainPredictions() function you're using is providing you with information about the feature contributions in the manner you think it is. I could be wrong though, however the article above explains it simply. It is worth a read.",1539179894.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/learnmachinelearning] [FYI - For those of you using the xgboostExplainer package](https://www.reddit.com/r/learnmachinelearning/comments/9n2kq2/fyi_for_those_of_you_using_the_xgboostexplainer/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1539199425.0
brews,I usually just do 'd'.,1539120341.0
ryapric,"Word to the wise: `df` is the name of the function for the density of the F distribution. You might not even ever realize it until you hit some weird closure errors, trying to do something to your object named ""df"". I always suffix it like `df_0` (but only as function args, etc, because that's still not any better than the point this post is trying make haha).",1539114618.0
1337HxC,"Conversely, some of my colleagues don't believe in shorthand and have variables named shit like `Cell.line.filtered.no.xy.normal.tissue.control`",1539109459.0
metagloria,"My boss has the habit of putting ""tt"" at the beginning of otherwise nondescript vectors and utilities...ttx, tty, ttdf, etc. Meaningless, but at least uniquely conveys the meaninglessness.",1539106652.0
AllezCannes,"I use stuff like ""tears_of_my_enemies""",1539129227.0
Digging_For_Ostrich,"Then one month later when you've moved on to a project called something like ""Departmental Finance"" and used df as shorthand for that, reading that original code becomes hell on earth for a week.",1539104599.0
b00mIR,"I laughed way too hard at this
",1539164399.0
ichikaren,I would usually use something like \`data\_df\`. ,1542762444.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/rshiny] [Raincloud plots Shiny App](https://www.reddit.com/r/rshiny/comments/9mqs2y/raincloud_plots_shiny_app/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1539103526.0
gergness,"You can do this with the survey package. Or, if you're used to dplyr syntax, the srvyr package wraps the survey package with dplyr's syntax.


Commands should be something like this (though I'd recommend checking that you get the same estimates of variance as SAS because sometimes you need to set some of the options in svydesign/as_survey to match other statistical packages):

```
library(survey)
svy <- svydesign(~mycluster, strata = ~mystrata, weight = ~myweight, data = mydata)

svymean(~as.factor(year), svy) # Percentage of survey per year 
svyby(~myvar, ~year, svy, svymean) # Mean of myvar by year
```

or 

```
library(srvyr)
svy <- as_survey(mydata, id = mycluster, strata = mystrata, weight = myweight)

# Percentage of survey per year
svy %>%
  group_by(year) %>%
  summarize(pct = survey_mean())

# Mean of myvar by year
svy %>%
  group_by(year) %>%
  summarize(myvar = survey_mean(myvar))
```",1539096938.0
GildedFuchs,"Sure, here's one way to to do it. In all seriousness, you should probably look back at the notes from your lecture. Maybe you'll find the dplyr method useful, or you could do it in base R with a lot of reassignment and so forth.

    library(data.table)
    library(lubridate)

    dt <- fread(""file.txt"")

    dt[, `:=` (BUILD = ifelse(BUILD == 1, ""emaciated"", 
    					ifelse(	BUILD == 2, ""thin"", 
    						ifelse(BUILD == 3, ""average"", 
    							ifelse(BUILD == 4, ""fat"", ""obese"")))),
    			LEAVE = ifelse(LEAVE == 1, ""routine"", ""intensive care"")),
    			Birthdate = mdy(Birthdate))]

    dt[, .(Birthdate, Sex, BUILD, LEAVE)]

*edit minor syntax error",1539095928.0
notsoslimshaddy91,I used to get overwhelmed by the plethora of packages available on CRAN and what they do until I found this https://cran.r-project.org/web/views/ ,1539083952.0
currentaccount123,I think the package swirl has some nice introduction tutorials,1539093726.0
welshfargo,Anyone know where it is on a Mac? I'm not finding it.,1539114503.0
efrique,You never even explored the menu bar in R? Take a look at the Help menu. Examine it *carefully*.,1539141054.0
MadPhatFishKiller,"Linux User here.  I found mine in */usr/share/R/doc/manual*

However, I installed R with sudo.  So, these might be somewhere else if I had installed as a normal user.

Thanks for letting me know about these!",1539351264.0
Laerphon,"Why are you running 2000 lines every time you knit the document? If it is a book, you should be dividing the document into separate subdocuments (chapters). If it is a small document with a lot of code, you should be running that much code separately and calling on intermediary saved objects. If you want it to rerun all that external code on ""real"" reruns, then just write a simple data loading function that has a flag to rerun the full codebase when set to TRUE but otherwise just pulls from saved objects when set to FALSE.

Edit: I am going to assume you're writing a dissertation or book. [Use bookdown and follow the advice here](https://bookdown.org/yihui/bookdown/preview-a-chapter.html).",1539048674.0
Samazing42,You could run the code chunk in Rstudio? Otherwise I’m not sure you can see the result without knitting each time. I’ll be interested to see if someone has a way to do exactly what you are asking. ,1539047031.0
RememberToBackupData,You could save the objects that require long computation time as .RDS files.,1539066357.0
R6stuckinsd,"You could save the intermediate TeX files by setting keep_tex to true. After you generate just the plots you need, you can edit the TeX output directly to swap them out.

    ---
    title: ""Title""
        output: pdf_document: 
            keep_tex: true
    ---",1539067433.0
MortGillu,You could use the cache option. It only reruns the chunk that changed but helps save time if you haven't made edits to other chunks.,1539049969.0
efrique,"multiply the y-value by a constant will produce an up or down shift

multiply the x-value by a constant will produce a left or right shift

do both and you get ""diagonal"" shifts at various angles (depending on those constants)",1539078637.0
mbillion,"Yeah, use a log instead of a constant. ",1539040845.0
mattindustries,You can accomplish the same thing with just docker and nginx as well. Bonus that nginx can perform caching. RStudio even has a [tutorial](https://support.rstudio.com/hc/en-us/articles/213733868-Running-Shiny-Server-with-a-Proxy) for the latter. Pointing different subdomains to the same IP will do the trick. I haven't used the shinyproxy though. ,1539026796.0
kenderpl,Shinyproxy was used in a product I was working on. What happened in the end that it got rewritten in house due to the issues we were having with blocked connections after the process was finished.,1539029936.0
jowen7448,"Yes used shinyproxy. What's kind of neat is that deployed apps don't actually have to be shiny or even R at all. Just containerised applications. Plus if you/your team have docker/networking skills it's a cost effective solution. 

also made use of shiny server pro and Rstudio connect. What u don't pay for in license fee for shiny proxy you pay for in other ways. The overhead of someone looking after it, host server costs. Integration with authentication etc. The rstudio products have one-push deploy etc. You don't need someone with docker/ networking knowledge. ",1539027708.0
VincentStaples,"chisq.test usually wraps table, so, chisq.test(table(df$var1, df$var2)). Not sure what you're doing with rep(1/length), but seems like you're making life hard for yourself.",1538964820.0
efrique,"If you're doing a distributional goodness of fit test you need to bin the data first and apply an ordinary chi-squared goodness of fit test to that binned data.

Also if you're estimating parameters, you have to do it on the binned data or you no longer have a chi-squared test. 

But in any case, the chi-squared goodness of fit test applied to binned continuous data has terrible power. Why would anyone use it? 
",1538966865.0
ReusAlThor,Can I ask why you are doing a chisq.test? It seems like there are better solutions for your data than a chisq.test if you have continuous data that are approx. normal ,1539003037.0
efrique,"> the data was given to me by a municipal agency so it is public but I'm not sure if I am the ""owner"" to do with it as I like

If the data are available on the municipal agency's website you can always write code that gets the data from there each time.

If that's not feasible, *ask the agency* if you can make their data available.

Personally I'd be inclined to use Github to hold the data, code, documentation, etc.
",1538955088.0
willbell,"Once you put it out there, it might be hard to control who uses it.  My lab had a policy of citing people if we used their data or models, and only offering people co-authorship if their data or model was unpublished (unless it was obviously a collaboration in which they contributed to the project as a researcher does).",1538969362.0
avamk,"I have experience publishing the complete source code (in R) of my research plus the accompanying data. It has, so far, worked very well for me.

For R code, I tend to publish it in a git repository hosted on [Gitlab](https://about.gitlab.com/). *Always* remember to include an open source license such as the [GNU GPLv3](https://www.gnu.org/licenses/gpl). For legal reasons, simply including a statement like ""The model and code are available for public use and I welcome any solicitations for collaboration"" doesn't actually mean much. As a general rule, if you don't include an appropriate license, others are technically not allowed to touch your code at all.

For data, I suggest publishing it to the [Open Science Framework](https://osf.io/). Also remember to include a license for the data, such as the [CDLA](https://lwn.net/Articles/753648/). The Open Science Framework is great because it gives your data a digital object identifier (DOI) which is a fully citable item that you can include in publications, etc.

I don't know which jurisdiction you are in and IANAL, but for US federal government produced output, you typically don't need explicit permission for doing whatever you want. But if it's municipal government-produced stuff then it might be safe to check with them first.

In any case, huge kudos for publishing your code and data. This is sorely needed especially in light of the reproducibility crises. Publishing the full code and data is the best way to ensure not only transparency in research, but also to take responsibility for it.",1539357257.0
PM_ME_CAREER_CHOICES,"Havent tested it, but something like this probably
    
    plot_func <- function(df,n) {
    tmp <- na.omit(df[df$Cohort == n,c(""fit"", ""Survey.Year"")])
    tmp$Survey.Year <- as.factor(tmp$Survey.Year)
    p <- ggplot(data = tmp, aes(x = Survey.Year, y = fit)) +       
          geom_boxplot() + 
          stat_summary(fun.y=mean, geom=""point"", shape=23, size=4) + 
          ggtitle(paste0(""Cohort "",n, "": Fit vs. Survey Year"")) + 
          theme(plot.title = element_text(hjust = 0.5)) +
          labs( x = ""Survey Year"", y = ""Fit"") + 
          stat_n_text(y.pos = 1) + 
          scale_y_continuous(breaks=seq(1, 4, 1), limits=c(1, 4)) +
          geom_line(data=aggregate(O$fit, list(O$Survey.Year), mean), 
                    aes(x=as.numeric(Group.1), y=x), col = ""red"", size = 1) 
    }
    n <- 4 # number of plots
    plots_list <- lapply((1:n)-1, function(x) plot_func(Main_Q,x))
    nCol <- floor(sqrt(n))
    do.call(""grid.arrange"", c(plist, ncol=nCol))
           
Where the two last lines are from [this Stackoverflow answer.](https://stackoverflow.com/questions/10706753/how-do-i-arrange-a-variable-list-of-plots-using-grid-arrange#10706828)

Generally as soon as you start having chunks of almost identical code you should try to make it into a more general function.",1538949690.0
revgizmo,"https://eddjberry.netlify.com/post/writing-your-thesis-with-bookdown/

https://github.com/ismayc/thesisdown

http://tysonbarrett.com/jekyll/update/2018/02/11/r_dissertation/",1538884717.0
lucretiuss,Does your university have a latex template? That can be used in R studio. I have been writing my dissertation using my uni’s latex template in R. ,1538886111.0
mixedliquor,The only way I've found for OneDrive or SharePoint is to use the OneDrive app to map a share locally and then treat it like a local file. The OD app will sync the two. The drawback being it will sync everything in he directory which may not be ideal.,1538879617.0
Moody_Mudskipper,https://stackoverflow.com/questions/29579782/reading-onedrive-files-to-r,1538878959.0
Phoogi,you should talk to your sys admins about this.  Public vs. private will matter.  Especially when it comes time to automate.  They're the only ones who will know about the relevant configs.,1538881717.0
andrewcsq,"Download the file locally and read it from the local destination. If this is a 1-time thing, this is the easiest and will save you the most hassle.

If it's for automation purposes, mount the OneDrive folder as a network directory.

Either way, trying to load the file directly from the URL is going to be more complicated than either of the above methods, and exceedingly unreliable.",1538911981.0
hmt28,"You are attempting to use `sum` with a character vector when it only works with logical or numeric vectors. To add count character vectors use `n()`

Based on your question I cannot tell if you want the result to be aggregated or not. My assumption is not so I will use the ungroup() function, if you want the aggregated results, just leave that off your code.

    df <- df %>% group_by(Author) %>% summarize(Freq = n()) %>% ungroup()
    df$Publication_Productivity <- ifelse(df$Freq <= 1, 1, ifelse(df$Freq == 2 | df$Freq == 3, 2, 3))",1538872760.0
mrinalbh,I was talking with my professor and it seems I shouldn't be using sum but count instead. And thanks for the help guys. ,1538891948.0
7shitij,Thanks. This was something much needed. I’ll use it in one of my projects at work.,1538826834.0
Moody_Mudskipper,"A few Stack Overflow  questions I've answered using `cutr`: 

* [splitting-a-continuous-variable-into-equal-sized-groups](https://stackoverflow.com/questions/6104836/splitting-a-continuous-variable-into-equal-sized-groups/52333388#52333388)
* [cut-include-lowest-values](https://stackoverflow.com/questions/12245149/cut-include-lowest-values/52674283#52674283)
* [adding-column-which-contains-bin-value-of-another-column](https://stackoverflow.com/questions/5570293/adding-column-which-contains-bin-value-of-another-column/52674188#52674188)
* [create-categorical-variable-in-r-based-on-range](https://stackoverflow.com/questions/2647639/create-categorical-variable-in-r-based-on-range/52674078#52674078)
* [divide-a-range-of-values-in-bins-of-equal-length-cut-vs-cut2](https://stackoverflow.com/questions/5915916/divide-a-range-of-values-in-bins-of-equal-length-cut-vs-cut2/52673878#52673878)",1538824865.0
ProgramAllTheThings,"Hah, I wrote a much simpler command line utility called cutr for improved manipulation of delimited files over cut. Good work on the package!",1538827290.0
cgk001,"Looks very interesting, will give it a try at work",1538840187.0
,I like this.,1538848386.0
Moody_Mudskipper,"Here's the full README:

-----------------------

cutr extends `base::cut.default`'s possibilities, getting inspiration from existing alternatives such as `Hmisc::cut2` and the `ggplot2::cut_*` family of functions, but going much further.

Installation
------------

You can install cutr from github with:

``` r
# install.packages(""devtools"")
devtools::install_github(""moodymudskipper/cutr"")
```

default behavior + `closed` and `open_end`
------------------------------------------

We build a distribution and show the default behavior, in the rest of the document we'll show results using `table` as it's more telling.

``` r
library(cutr)
x <- c(rep(1,3),rep(2,2),3:6,17:20)
hist(x,breaks = 0:20)
```

[![histogram][1]][1]

  [1]: https://github.com/moodymudskipper/cutr/raw/master/README-unnamed-chunk-2-1.png

``` r
cuts <- c(0,5,10,20)
smart_cut(x,cuts)
#>  [1] [0,5)   [0,5)   [0,5)   [0,5)   [0,5)   [0,5)   [0,5)   [5,10) 
#>  [9] [5,10)  [10,20] [10,20] [10,20] [10,20]
#> Levels: [0,5) < [5,10) < [10,20]
table(smart_cut(x,cuts))
#> 
#>   [0,5)  [5,10) [10,20] 
#>       7       2       4
```

The output is very similar to what we get with `base` or `Hmisc` and `ggplot2` alternatives.

``` r
table(base::cut(x,cuts))
#> 
#>   (0,5]  (5,10] (10,20] 
#>       8       1       4
table(Hmisc::cut2(x,cuts))
#> 
#> [ 0, 5) [ 5,10) [10,20] 
#>       7       2       4
table(smart_cut(x,cuts,""breaks""))
#> 
#>   [0,5)  [5,10) [10,20] 
#>       7       2       4
```

A the difference with `base` is that we close by default the left size, this can be changed by setting the `closed` parameter to `""right""`.

Another is that the ends are both closed in `smart_cut`, this can be changed by setting the `open_end` parameter to `FALSE`

`closed` is borrowed from `ggplot2::cut_*` functions, and corresponds to the `RIGHT` parameter of `base::cut.default`.

`open_end` corresponds to the (negated) `include.lowest` parameter of `base::cut`.

The difference with `Hmisc` is due to the formatting function, which is `formatC` for `cut` and `smart_cut`, and `format` for `Hmisc` (which gives all labels the same width). The display of `smart_cut` is highly flexible thanks to the argument `format_fun` detailed further in this document.

`what` and `i`
--------------

The `what` parameter determines how cuts will be chosen, depending on the value of `i`.

``` r
table(smart_cut(x,cuts,""breaks""))                   # fixed breaks
#> 
#>   [0,5)  [5,10) [10,20] 
#>       7       2       4
table(smart_cut(x,2,""groups""))                      # groups defined by quantiles
#> 
#>  [1,3) [3,20] 
#>      5      8
table(smart_cut(x,list(2,""balanced""),""groups""))     # optimized groups of equal size
#> 
#>  [1,4) [4,20] 
#>      6      7
table(smart_cut(x,3,""n_by_group""))                  # try to get 3 items by group using quantiles
#> 
#>   [1,3)  [3,17) [17,20] 
#>       5       4       4
table(smart_cut(x,list(3,""balanced""),""n_by_group"")) # try to get 3 items by group using optimization
#> 
#>       1   [2,4)  [4,17) [17,20] 
#>       3       3       3       4
table(smart_cut(x,3,""n_intervals""))                 # intervals of equal width
#> 
#>     [1,7.333) [7.333,13.67)    [13.67,20] 
#>             9             0             4
table(smart_cut(x,7,""width""))                       # interval of equal defined width, start on 1st value
#> 
#>   [1,8)  [8,15) [15,22] 
#>       9       0       4
table(smart_cut(x,list(7,""right""),""width""))         # interval of equal defined width, end on last value
#> 
#>  [-1,6)       6 [13,20] 
#>       8       1       4
table(smart_cut(x,list(6,""centered""),""width""))      # interval of equal defined width, centered
#> 
#>  [-1.5,4.5)  [4.5,10.5) [10.5,16.5) [16.5,22.5] 
#>           7           2           0           4
table(smart_cut(x,list(6,""centered0""),""width""))     # interval of equal defined width, centered on 0
#> 
#>  [-3,3)   [3,9)  [9,15) [15,21] 
#>       5       4       0       4
table(smart_cut(x,list(7,0),""width""))               # interval of equal defined width, starting on 0
#> 
#>   [0,7)  [7,14) [14,21] 
#>       9       0       4
table(smart_cut(x,3,""cluster""))                     # create groups by running a kmeans clustering
#> 
#>     [1,3.5)  [3.5,11.5) [11.5,19.5] 
#>           6           3           3
```

`simplify`
----------

`TRUE` by default, when a value is the only one in its group, display it as a label, without brackets. Similar to `oneval` in `Hmisc::cut2`.

``` r
table(smart_cut(x, 5, ""width"", simplify = TRUE))
#> 
#>   [1,6)       6 [11,16) [16,21] 
#>       8       1       0       4
table(smart_cut(x, 5, ""width"", simplify = FALSE))
#> 
#>   [1,6)  [6,11) [11,16) [16,21] 
#>       8       1       0       4
```

`expand`
--------

expand makes sure all values from x will be in an interval by expanding the cut points. `base::cut.default` never expands, `Hmisc::cut2` always expands.

``` r
table(smart_cut(x,c(4,10,18)))
#> 
#>   [1,4)  [4,10)      17 [18,20] 
#>       6       3       1       3
table(smart_cut(x,c(4,10,18),expand = FALSE))
#> 
#>  [4,10) [10,18] 
#>       3       2
```

`crop`
------

`crop` is `FALSE` by default, if `TRUE` the side intervals are reduced to fit the data.

``` r
table(smart_cut(x,c(0,10,30)))
#> 
#>  [0,10) [10,30] 
#>       9       4
table(smart_cut(x,c(0,10,30),crop = TRUE))
#> 
#>  [1,10) [10,20] 
#>       9       4
```

`squeeze`
---------

`squeeze` is `FALSE` by default, if `TRUE` every interval is reduced to fit the data.

``` r
table(smart_cut(x,c(0,10,30)))
#> 
#>  [0,10) [10,30] 
#>       9       4
table(smart_cut(x,c(0,10,30),squeeze = TRUE))
#> 
#>   [1,6] [17,20] 
#>       9       4
```

`brackets` + `sep`
------------------

Different brackets can be chosen

``` r
table(smart_cut(x,c(0,10,30), brackets = c(""]"",""["",""["",""]"")))
#> 
#>  [0,10[ [10,30] 
#>       9       4
table(smart_cut(x,c(0,10,30), brackets = NULL, sep = ""~"", squeeze= TRUE))
#> 
#>   1~6 17~20 
#>     9     4
```

`labels`
--------

`labels` can be a vector just like in `base::cut.default`, but it can also be a function of 2 arguments, which are a vector of values contained in the interval and a vector of cutpoints.

``` r
table(smart_cut(x,c(4,10)))
#> 
#>   [1,4)  [4,10) [10,20] 
#>       6       3       4
table(smart_cut(x,c(4,10),labels = ~mean(.x)))   # mean of values by interval
#> 
#> 1.667     5  18.5 
#>     6     3     4
table(smart_cut(x,c(4,10),labels = ~mean(.y)))   # center of interval
#> 
#> 2.5   7  15 
#>   6   3   4
table(smart_cut(x,c(4,10),labels = ~median(.x))) # median
#> 
#>  1.5    5 18.5 
#>    6    3    4
table(smart_cut(x,c(4,10),labels = ~paste(
  sep=""~"",.y[1],round(mean(.x),2),.y[2]))) # a more sophisticated label
#> 
#>   1~1.67~4     4~5~10 10~18.5~20 
#>          6          3          4
```

`format_fun`
------------

With `cutr` the user can provide any formating function through the argument `format_fun`, including the package function `format_metric` (more info on value formating in the dedicated section).

``` r
table(smart_cut(x^6 + x/100,5,""g""))
#> 
#>                  1.01     [64.02,1.563e+04) [1.563e+04,2.414e+07) 
#>                     3                     4                     2 
#>   [2.414e+07,6.4e+07] 
#>                     4
table(smart_cut(x^6 + x/100,5,""g"",format_fun = format, digits = 3))
#> 
#>                1.01 [6.40e+01,1.56e+04) [1.56e+04,2.41e+07) 
#>                   3                   4                   2 
#> [2.41e+07,6.40e+07] 
#>                   4
table(smart_cut(x^6,5,""g"",format_fun = signif))
#> 
#>                  1         [64,15625)   [15625,24137600) 
#>                  3                  4                  2 
#> [24137600,6.4e+07] 
#>                  4
table(smart_cut(x^6,5,""g"",format_fun = smart_signif))
#> 
#>                  1         [64,15600)   [15600,24100000) 
#>                  3                  4                  2 
#> [24100000,6.4e+07] 
#>                  4
table(smart_cut(x^6,5,""g"",format_fun = format_metric))
#> 
#>               1     [64,15.6 k) [15.6 k,24.1 M)   [24.1 M,64 M] 
#>               3               4               2               4
```

more on `groups`
----------------

`groups` and `n_by_group` try to place cut points at relevant quantile positions, we won't get the required number of groups if several quantiles fall on the same value, to remedy to this we can use an optimization function (the default one will most likely be enough).

``` r
table(smart_cut(x,3,""groups""))
#> 
#>      1  [2,6) [6,20] 
#>      3      5      5
table(smart_cut(x,list(3,""balanced""),""groups""))
#> 
#>   [1,3)  [3,17) [17,20] 
#>       5       4       4
```

the second element of the list can be a string (which will be mapped to a predefined function) or a cusom made 2 argument function that is applied on all possible bin combinations : Bin size is the first argument and the cut points is the second. the combination that return the lowest value when passed to optim\_fun will be selected (or the first of them if the minimum is not unique). ""balanced"" minimizes the variance between groups and will work for most purposes.

`cutf` and `cutf2`
------------------

These are copies of `base::cut.default` and `Hmisc::cut2` with the difference that the formatting function can be used freely. All the features are contained in `smart_cut` but these functions allow users to keep the interface, and defaults of the function they know and to modify existing code easily, for example to leverage `format_metric` with minimal effort.
",1538823772.0
Loco_Mosquito,rstudio::conf 2018 was pretty well recorded. https://resources.rstudio.com/rstudio-conf-2018,1538782859.0
Moody_Mudskipper,"I love the plotcon videos, there's a bunch on youtube :

[https://www.youtube.com/results?search\_query=plotcon](https://www.youtube.com/results?search_query=plotcon)",1538825807.0
mathnstats,"What'd be *really* fantastic is if you could export the app itself as a web app or something, using data from a specified source (like a database view). Basically, an out-of-the-box Shiny App that you don't really have to customize for a wide user base",1538783168.0
epimeliad,Great tool. I think it does help lower the learning curve for new r users,1538788504.0
Darth_Marrr,"I have yet to try this, however at first glance this remedies a current issue I have with speed of execution. Often I lament the rigmarole that is required to assess bi-variate and tri-variate relationships in R (or Python for that matter). Currently I use R, but in a pinch I will use JMP. This will allow me to, not only, quickly assess what could potentially be important for a model or displays for a report, but it also generates the code for such a plot!!! Amazing! Additionally, I no longer require an extra cost of a JMP license once I graduate. To the developer: You sir, have my respect. A hat tip to you and your ilk! ",1538883499.0
keepitsalty,While these are always cool tools to see. I can't help but feel that point and click GUI apps take away from the experience of being immersed in code and data. It's the same difficulty of the learning curve to build visualizations that produce good data analysis IMO. ,1538752700.0
ran88dom99,looks better than ggraptr.  [added to R PnC list](https://alternativeto.net/list/2063/guis-to-save-from-typing-r-code/),1539158070.0
millsGT49,"I think this is less an ordinal variable problem and more a general model fitting problem. 
> glm.fit: fitted probabilities numerically 0 or 1 occurred.

implies that you have a variable in your model that perfectly separates the data. What we mean by that is that the regression can basically make the coefficient of this value as big as it wants and it will always be ""rewarded"" in the sense that it's fitted probabilities will get more and more accurate. You should check if any variables in your model, ordinal or not, have this issue where  none or all of the levels have values of 1's or 0's. ",1538752062.0
s3x2,By using ordinal variables that don't split your data perfectly. Drop the ones that do or get more data if you do care about those variables and know that they shouldn't be perfect predictors of the outcome.,1538748436.0
AlisonByTheC,"We use the Azure DevOPS to hold our git repositories for version control.  It works very easily with RStudio.  

If you have Office 365, accounts are integrated seamlessly from Office to Azure.  

",1538707193.0
nonfamous,There’s a big list of R-enabled services in Azure here: https://docs.microsoft.com/en-us/azure/machine-learning/r-developers-guide,1538742002.0
mr-datascientist,Azure with hdinsights? MRO with MLS?,1538706197.0
kinow,"Hey Ryo! I had a quick break and could quickly have a look at the article, and it looks super (and geek-ly) interesting!!! I've always liked Japan's culture (food/language/movies/etc) and being a programmer, your posts are always super interesting to me.

I'll try to use some of your code later to experiment with local data here in New Zealand, and see if I can find any data from Brazil too. And then compare what it looks like. Will let you know if that works :)

Keep up the good work!",1538693315.0
hodos_ano_kato,"Yes, I was there for a couple of weeks in July and it was SWELTERING! Great post 👍🏻 ",1538709499.0
mongooseondaloose,"You may need a VPN service to get around this. When you ""log in"" are you doing this in the rvest::html_session? Or in your browser?",1538598629.0
ConwayPuder,"I probably found the exact same tutorial to web scrape with R as you did. I was hacking at thst earlier this afternoon. 

I think the library has a langugage command. You might need to type 'En-US' or something. ",1538605759.0
Phoogi,like @mongooseondaloose mentioned a VPN might be the best choice.  You may also consider using something like selenium to make the browser-side selections to change the language before attempting your web scraping... let me know how it works out!,1538614020.0
aizheng,"IMDb also publishes their database dumps, or at least large parts of it as TSV. ",1538669371.0
pearsonchi,"Yep, in the end I used VPN. I thought this will work out, but also wanted to learn about other ways. Thanks !",1539124907.0
forever_erratic,df[[i]],1538590654.0
jackbrux,"Lots to say here. Is there any reason you have a list of dataframes and not a single large dataframe? Also, conventionally people would recommend a ""functional"" approach over a for loop. i.e. using something like `purrr::map`.",1538590803.0
RememberToBackupData,"Put your transformations into a function, and then call the function using [desiderata](https://github.com/DesiQuintans/desiderata)::apply_to_files().",1538602455.0
shujaa-g,"Gather combines everything by default. If you don't want it to combine everything, you tell it what you want combined. Have a look a `?gather`, specifically the `...` argument. It is described as:

> A selection of columns. If empty, all variables are selected. You can supply bare variable names, select all variables between `x` and `z` with `x:z`, exclude `y` with `-y`. For more options, see the `dplyr::select()` documentation. See also the section on selection rules below.

So, your options would be any of the following:

    gather(df1, key = ""key"", value = ""value"", -x, -y)
    gather(df1, key = ""key"", value = ""value"", z1, z2)
    gather(df1, key = ""key"", value = ""value"", z1:z2)
    gather(df1, key = ""key"", value = ""value"", dplyr::starts_with(""z""))

etc.

 The **Rules for Selection** section of the help page provides more details on the selection mechanisms, and the examples at the bottom of the help page demonstrate several of the options. I'd highly recommend running through the examples at the bottom of a help page when you're having trouble with a function. It's usually much faster than a reddit post ;)",1538589297.0
tdunn12,"    df1 %>%
      gather(key, z, z1, z2) %>%
      select(-key) %>%
      filter(!is.na(z))
    
Something like that.
",1538589550.0
2strokes4lyfe,"If I remember correctly, you can use the ‘-‘ operator to prevent a variable from being gathered. I’m not at my computer but give this a try. 

    df1 %>% gather(key = x, value = z, -y)

Edit: This doesn't work, sorry!",1538589311.0
AprimeAisI,"I was a big reshape2::dcast() fan, but recently switched to using gather and spread. gather works so much better in my pipe heavy analysis scripts. ",1538597365.0
RememberToBackupData,"Firstly, your reproducible example creates the wrong data frame (z1 is 1, 1, 2 instead of 1, 1, 3).

Here is one way to merge and then separate columns into rows:

    df1 %>% 
        tidyr::unite(z, z1, z2, sep = ""_"") %>% 
        tidyr::separate_rows(z, sep = ""_"") %>% 
        dplyr::filter(z != ""NA"")  # It's not NA_character_
    
      x y z
    1 1 a 1
    2 2 b 1
    3 2 b 2
    4 3 c 3

`unite()` mashes multiple columns together with a separator:

      x y    z
    1 1 a 1_NA
    2 2 b  1_2
    3 3 c 3_NA

`separate_rows()` creates a new row containing the stuff after the separator (it can create more than one row if there is more than one separator):

      x y  z
    1 1 a  1
    2 1 a NA
    3 2 b  1
    4 2 b  2
    5 3 c  3
    6 3 c NA

And then you filter by `""NA""` to get rid of NAs if you don't want them.

      x y z
    1 1 a 1
    2 2 b 1
    3 2 b 2
    4 3 c 3

I use this a lot for columns that have mixed data types in them because I can work with them as text and then convert them after the fact.",1538606447.0
statsnoobie,I know that the read\_excel() function from the readxl package has a range option. The difficult bit is identifying the ranges.  It also has column and row options.,1538568076.0
thunderdome,check out the package excel.link,1538569940.0
Core_Four,the only correct solution is to slap whoever is sending you these excel workbooks right in the face. ,1538589491.0
ryapric,"This issue was prevalent enough that someone started working on a package to automate most of it. I hadn't checked in a while, but according to the CRAN release, it's now in v1.0+.

I've not used it myself, but it's called [tidyxl](https://github.com/nacnudus/tidyxl).",1538579687.0
thaisofalexandria,"I couldn't do this with read_excel().  A quick browse to see what other people had come up with led me to this

https://stackoverflow.com/questions/47000380/splitting-dataframes-in-r-based-on-empty-rows

and I think the answer is in some combination of this technique and read_excel().",1538746380.0
chocolateandcoffee,"I maybe don't understand the question fully. Are you not at allowed for some reason to use other packages? You can build the tree and then use other packages to build the plot. See e.g., [http://rstatistics.net/decision-trees-with-r/](http://rstatistics.net/decision-trees-with-r/)",1538598487.0
,[deleted],1538548298.0
YepYepYepYepYepUhHuh,"I believe the easiest way would be to create a package with the function in it (with proper documentation, see Hadley's [guide](http://r-pkgs.had.co.nz/)) in order for ? and help() to work.",1538517901.0
mouse_Brains,"You might be able to hack something in with a help parameter. If help = TRUE, display document on viewer. It's not standard but can't think of anything else. It is indeed frustrating that functions can be returned without documentation.

I believe the correct way to do this would be to document the output within the generator, in the @return section

Edit: alternatively you can make your own help() function that returns help for those functions, or (and this is hacky as hell) concatenate a new class name to your functions so you can differentiate between a regular function and something you return, if class matches return a help manually, if not, proceed with the regular help function ",1538524501.0
Moody_Mudskipper,"This functions is never used in a loop or in any code so it doesn't really matter if it's efficient, i'd just do a messy override :

&#x200B;

>z <- mean  
>  
>class(z) <- c(""myclass"",class(z))  
>  
>\`?\` <- function(e1, e2) if(""myclass"" %in% class(e1)) ?median else help(as.character(substitute(e1)))  
>  
>?z

&#x200B;

This will display the help from `median`",1538830161.0
pm_me_yo_waifu,"The r-sig-phylo mailing list has lots of useful questions and answers. 

https://www.mail-archive.com/r-sig-phylo@r-project.org/

Also, Liam Revell's phytools blog has lots of useful tutorials and demonstrations. I've found a lot of useful code snippets related to plotting results here. 

http://blog.phytools.org/

Finally, I would try to find a copy of ""Analysis of Phylogenetics and Evolution with R"" by Emmanuel Paradis (the developer of ape). This will give you a thorough breakdown of how phylogenetic data is stored and structured in ape (and by extension most other phylogenetic packages as most of them use ape's data structures). 

 
",1538527562.0
Drewdledoo,"Sorry can’t comment on your specific needs, but try /r/bioinformatics as well if you haven’t already",1538520204.0
flyos,"To the list given by /u/pm_me_yo_waifu, I'll add the (relatively new) book ""Modern Phylogenetic Comparative Methods"" which has nice online tutorial for many chapters:
http://www.mpcm-evolution.com/practice",1538552079.0
YepYepYepYepYepUhHuh,"Have you looked into this chapter from the lme4 package? 

http://lme4.r-forge.r-project.org/book/Ch2.pdf",1538508976.0
lil_meep,"    test_data %>% 
    	ggplot() + 
    	aes(x = noiselevel, color = gender, group = gender, y = wdsrecalled) + 
    	stat_summary(fun.y = mean, geom = ""point"") + 
    	stat_summary(fun.y = mean, geom = ""line"") + 
    	scale_color_manual(values=c(""#FF3333"", ""#0066CC"")) + 
    	labs(title = ""Interaction Effect Between Noise Level and Gender"") + 
            labs(y=""Mean wdsrecalled"")

This looks at the mean wdsrecalled for each gender at every noiselevel.

Inferences:

* As noiselevel goes up, wdsrecalled goes down for both genders
* Gender 1 has a higher wdsrecalled than gender 2 for each noiselevel, except for noiselevel 3

You may want to consider a 3 dimensional scatter-plot (plotly has examples). I'm not saying the above is the prettiest graph by any means - just a leaping off point. ",1538512885.0
agclx,Isn't using `while` and manually counting up the same as using a `for` loop?,1538493210.0
,Why use gradient descent instead of iteratively reweighted least squares?,1538484016.0
mghoff330,"u/atreadw I'm struggling to get your code to complete and would love some help as I find this really interesting. 

* Your `lr()` function takes three args: `x`, `y`, and `theta_arg`. However, inside that function lies: `x_arg`, `index`, and `theta_arg`. Am I correct to assume that `x` -> `x-arg` and `y` -> `y_arg`?
* When you create `train_x`, there is no `x` already created for the `Reduce` function to take. I believe `Reduce` should take the initial `train_x`.
* When running the `lr()` function on the training data/model, which should take 3 arguments, you only provide 2. Can you elaborate on this?

Thank you in advance!

EDIT: a word",1538760945.0
avamk,"Thanks for the link.

Can anyone recommend a good introduction to *what* random forests are and how to use them? The linked article looks nice but after reading it I still have no idea what the mechanics of random forests are...",1538404229.0
too_many_splines,"Health research and social work are good places for that. Epidemiology is happily still abiding by most statistical and scientific principles whereas most of the empirical research in industry now has shifted toward a ""whatever works and has the smallest error"" attitude - which I find deeply unsatisfying. Besides research (ala Fred Hutchinson institute) healthcare providers/networks and health tech are okay on this front. Also firms that are involved in survey design I've found are quite disciplined on proper methodology and more structured observational studies.

Sadly most statistical programming is heavily geared towards business interests. If it's really not you're thing, it's best to avoid SaaS type firms, startups and anything that says ""pipeline"" too much (yes I'm aware how this includes an overwhelming majority of statistical oriented jobs) ",1538364538.0
DrOddcat,Government. State and federal. Both have data collection and analyst positions.,1538358619.0
xubu42,"I've worked in a few different data science jobs. Not all are about marketing, though there are plenty of those available. I've worked for the government in order to help figure out fraud in public benefits problems and how to best forecast and plan around issuance of new applications. I've worked for tech companies improving product decisions, which usually isn't about making more money but making it easier to use the app or more enjoyable. My brother in law works for the government on sensor data from rivers. My friend works for United airlines on problems about capacity planning and forecasting. I think you might be jumping to conclusions after reading a few job descriptions. My advice is not to search by job title as it's became meaningless for the work you stated you like to do. Instead use an advanced search for keywords in the job descriptions that match what you are looking for.",1538349592.0
dtrillaa,"I work for FinTech company and what my team does is write data pipelines in python that are consumed by our clients. I take raw input data and output data sets that include various financial statistics for given securities. My title is Data Scientist, but really what I am doing is data software engineering since I’m developing automated processes that will produce these data sets daily in morning for consumption.",1538352871.0
actuaryal,"Take a look at non-profit cancer centers or other large hospitals. I've worked in two cancer centers and loved my job in both. I'm programming in R most of the day, working on various projects. 

Of course you'll probably make 10-20% less than in pharma, but the types of projects and tasks are more varied I'd say and there is less pressure. ",1538407259.0
baladeplata,Prop trading.,1538370633.0
Core_Four,"biostatistics

lol",1538508554.0
ClickableLinkBot,"##r/vim
---------------------------------------------
^(For mobile and non-RES users) ^| 
[^(More info)](https://np.reddit.com/r/botwatch/comments/6xrrvh/clickablelinkbot_info/) ^| 
^(-1 to Remove) ^| 
[^(Ignore Sub)](https://np.reddit.com/r/ClickableLinkBot/comments/853qg2/ignore_list/)",1538349089.0
KopfJ4ger,"Start with [r4ds.had.co.nz](www.r4ds.had.co.nz).

It'll get you some basics in R and a thorough introduction to the Tidyverse. After, you should know enough to decide which direction to go.",1538322735.0
egoweaver,"I am from a medical background and learned R from scratch 2 years ago, mainly analyzing NGS data.  There's a quote about R that has been valuable to me: You don't learn R *per se*; you learn how to do a particular thing with R. I think this is particularly true for us biology people. Thus, anything that familiarize you to the syntax and basic control flow of R will serve you well as the first step, and a hands-on project and someone who can help you when your troubleshooting is in a dead end are the most critical things IMHO.

If you are in an institution/company who provides workshop or training course, I would recommend that.

Then, (Using NGS as an example) find papers that you are interested in and shares their data. Go to GEO Dataset or other data repository to find if they also share semi-processed data.

Analyzing semi-processed data helps you to learn how to transform and filter the data to a format that makes sense to you and your field.

As for raw data, like quality parameters for NGS itself or alignment, these things sometimes require more computational resources and are often done with individual software packages outside of R. If it interests you, learn it when you are familiar with data handling, but don't do it first.

I arrange thing this way because you will always want to compare your analysis to the published result to ensure you are doing something consistent with accepted pipeline of analysis. If you do raw data first, you might find it difficult to organize your results so you can't really compare to what is published because you are not yet familiar with organizing data. 

Additionally, if the result is somewhat awkward compared to published analysis , you'll need to examine every details in each step of your analysis to find out why, and it's usually harder for raw data processing. Of course the authors could be wrong, so you still might want to ask someone capable in the end.",1538327272.0
GoodAboutHood,"DataCamp is 100% worth it. If you’re trying to learn R it’s an awesome resource. Do the data analyst track, but skip the second ‘importing data’ course for now.

Also use that Hadley Wickham book the other user posted, it’s pretty handy as well.",1538325024.0
kenderpl,DataCamp is ok for getting your feet wet but if you really want to get into the thick of it - go through Hadley's books plus some books on statistical methods like Statistical Rethinking. The books will teach you much more than the online courses.,1538327977.0
I_just_made,"Hey there;

Really cool, looks great for a first Shiny app! Mine looked... well... let's not go there. And the code behind it was pretty abysmal. 

I don't know if you want to continue with Shiny, but if so, I highly recommend looking into using modules!  It isn't so intuitive at first, but it will be a massive help as your apps get larger and more complicated.

If I could offer a suggestion; things are very busy in your graphs.  With so much going on, I had a difficult time finding out what I was looking at, despite legends being there.  Check out what [Overbuff](https://www.overbuff.com/) does, it is very similar I imagine.  They got right to the point though, and must have clearly spent a lot of time tweaking things to be easily digested when visualized. *This isn't easy!*

My suggestion is only a suggestion; for a first app, this is a great job! You took an idea, not just a basic ""show this df"" either, and shaped it into a functional thing that others can use.  This is excellent, hope to see more as you learn!",1538325964.0
coip,"This is amazing! Honestly, I'd love to read a full-on tutorial on how you did this or learn more about how you did this. I signed up for the Halo 5 developer API years ago, managed to get their data imported into R, but the data structure was an absolute mess of lists embedded in lists embedded in lists, and so on. It was clearly designed for something other than R and I gave up on trying to figure it out. How'd you make sense of it all?",1538349967.0
13ass13ass,Love the use of background graphics! I hadn't thought to do that but it makes it look so much better.,1538366751.0
Core_Four,neat use case - expand it for other games :),1538508684.0
ExcellentOdysseus,Why not just post problems as the come up? Pull from the whole community ,1538268962.0
sparkysparkyboom,"Hey man I've struggled through the same thing in school before.  Try to make use of any office hours if you can. That's one of the biggest things I would have done differently.  I rarely went either because I didn't want to seem like I was falling behind and look bad, or I was so lost I didn't know what questions to ask. But some of the most successful students were ones that went to most of the available office hours.  ",1538285934.0
wouldeye,r/learnrstats,1538267975.0
Altruistic_Camel,Have you tried talking to your professor? Or a classmate?,1538329232.0
edimaudo,I don't think you need a mentor. You need to revamp your learning style.  ,1538303450.0
mouse_Brains,Do you have office hours you can exploit? A one to one with TAs could be more helpful than you anticipate,1538528264.0
Phoogi,"What am i missing here?  Seems to fall into camp \*duh\*... installing Shiny server or RStudio server isn't hard, i don't see why doing it on Azure is special at all.... and you still need expensive licenses in order to authenticate individuals (which is the only thing that makes this platform worth anything to an enterprise).... and they would no doubt have access to on prem or cloud based servers....  


So... why does this matter at all?  Why is doing this on Azure special?

&#x200B;

It seems like all that has been done is a VM was stood up (which any idiot can do) and we ran some commands...",1538283219.0
Deto,"For Python specifically: The [scipy.stats.hierarchy](https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html) module has this functionality.

First create the linkage matrix with the [`linkage` function](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage)

Plot it (optional) with the `dendrogram` function.  And cut it with the [`cut_tree` function](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.cut_tree.html#scipy.cluster.hierarchy.cut_tree)

Example code:

&#x200B;

    from scipy.stats.hierarchy import linkage, cut_tree
    Z = linkage(my_data, method='average', metric='euclidean')
    groups = cut_tree(Z, n_clusters=5) # could specify height instead

&#x200B;",1538178841.0
o-rka,Thanks!  I will definitely use this.  Have you come across any more robust methods that don’t cut by a specific number of clusters or a global height ?  Trying to figure out all of the cut methods that are out there to figure out which ones I could possibly use. ,1538180421.0
mouse_Brains,You'll still be waiting though... ,1538245949.0
doggie_dog_world,"Where/what is the UnitPrice variable?  If it's in another dataframe, you could use a JOIN (left or inner) provided there is a common ID among dataframes.  If you are calculating within the dataframe, you can use MUTATE.",1538178965.0
mouse_Brains,Shinyjs has a function logjs to print what you want on js console,1538528458.0
ZZ-TOP,"Nice post, except the pie charts.. ",1538205499.0
shaqerd,"Why are you fuzzy matching IDs? They should be exact matches, right? Fuzzy matching names would be a normal use case, but not IDs.",1538138930.0
,[deleted],1538110804.0
Adamworks,This isn't really a fuzzy join. Just convert the ID variable to a single string which contains both the numeric and character variable.,1538145997.0
shujaa-g,"One other comment on your code. The biggest thing killing your speed is ""growing an object in a loop"". Compare:

    n = 1e5
    time_x1 = Sys.time()
    x1 = c()
    for (i in 1:n) {
      x1 = c(x1, ""a"")
    }
    time_x1 = Sys.time() - time_x1
    
    time_x2 = Sys.time()
    x2 = character(n)
    for (i in 1:n) {
      x2[i] = ""a""
    }
    time_x2 = Sys.time() - time_x2
    
    cat(time_x1, time_x2)

The code is doing the same thing 2 ways. The first way grows the object 1 item at a time (like your code), the 2nd way pre-allocates the object with the correct length, then fills it in 1 item at a time. The pre-allocation is about 500x faster.
",1538153616.0
shujaa-g,No sample data? A little sample input and corresponding desired output would make it so people could try it out and try to speed it up. I'm pretty bad at improving code just by reading it...,1538091452.0
Atheriel,"It's worth keeping in mind that if your slow code works and this only needs to be done once, it's probably not worth spending a lot of time trying to figure out a fast solution. Sometimes ""better"" is the enemy of ""working"".",1538103053.0
laboranalyst3,"Can you [parallelize](https://www.r-bloggers.com/how-to-go-parallel-in-r-basics-tips/) your code? Adding a repeating column from 1 to the number of cores you want to use and then using foreach to parse your data in parallel might be helpful. For example, if you're using 10 cores you'd be able to go 10 rows at time instead of one. (although I don't think the speed increase is linear). ",1538108565.0
shujaa-g,"Okay, so

-  `toJSON` will ignore `NA`s by default, so if we replace the 0s with NAs, it's *very close* to what you want. We just need to replace the empty results `{}` when a row  is all NA (all 0) with your `""null"":null` pattern.
-  Working on the whole data frame at once and then editing the result will be much faster than going line by line. (Didn't test this assumption, but it feels right.)

Using these insights, this should be quite fast. (To edit the result I used `stringi` for speed.)

    library(toJSON)
    library(stringi)

    result = toJSON(replace(input[, -1], input[, -1] == 0, NA))
    result = stri_replace_all_fixed(result, pattern = ""{}"", replacement = ""{\""null\"":null}"")
    result = substr(result,  2, nchar(result) - 1) # omit leading and trailing square brackets
    # next we replace the comma between rows with another character (""|"") 
    # so we can easily split rows into separate strings
    result = stri_replace_all_fixed(result, pattern = ""},{"", replacement = ""}|{"")
    result = unlist(stri_split_fixed(result, ""|""))
    input$result = result
    input
    #    uid v1 v2 v3   v4                           result
    # 1    0  1  0  2    0                  {""v1"":1,""v3"":2}
    # 2    1  0  0  0    0                    {""null"":null}
    # 3    2  5  5  5    5    {""v1"":5,""v2"":5,""v3"":5,""v4"":5}
    # 4    3  1  0  0    4                  {""v1"":1,""v4"":4}
    # 5    4  2  0  3    0                  {""v1"":2,""v3"":3}
    # 6    5  0  0  0    0                    {""null"":null}
    # 7    6  3  5  6 1000 {""v1"":3,""v2"":5,""v3"":6,""v4"":1000}
    # 8    7  2  2  2    0           {""v1"":2,""v2"":2,""v3"":2}
    # 9    8  0  4  0    0                         {""v2"":4}
    # 10   9  3  0  4    4           {""v1"":3,""v3"":4,""v4"":4}
    
Using this sample data:
    
    input = read.table(text = '""uid"",""v1"",""v2"",""v3"",""v4""
    0,1,0,2,0
    1,0,0,0,0
    2,5,5,5,5
    3,1,0,0,4
    4,2,0,3,0
    5,0,0,0,0
    6,3,5,6,1000
    7,2,2,2,0
    8,0,4,0,0
    9,3,0,4,4', header = T, sep = "","")
",1538152849.0
northernwildling,Check out purrr,1538097155.0
willbell,"I have practiced implementing the DBSCAN algorithm and I don't think it is meant to solve your problem.  DBSCAN is meant more to avoid unnecessary inclusion of outliers.

You should consider how strongly you are worried about the false clustering rate.

I will try to think about your question more.  Maybe try to find a bunch of shortest paths from one year to the next?",1538165914.0
Belsaga,"Did you try with DBScan? It separates the ""noise"".

&#x200B;

[https://cran.r-project.org/web/packages/dbscan/vignettes/dbscan.pdf](https://cran.r-project.org/web/packages/dbscan/vignettes/dbscan.pdf)",1538100555.0
shujaa-g,"As the comment on stack says, can't tell if anything is right without knowing what your data looks like. Post a few rows of sample data (in a copy/pasteable way, please!)

I will say, your many-nested `ifelse`s are terrible to read. I would suggest making a look-up table, something like this:

    result_lookup = data.frame(
      Match_Type = c(""Normal"", ""Semi-final"", ""Final""),
      home_wins = c(0.6, 0.6, 3),
      tie = c(0.4, 0.4, 0.5),
      away_wins = c(0, 0, 0)
    )

You join to this simple table based on the `Match_Type`. Then you can replace all your mess with

    case_when(
      home > away ~ home_wins,
      home == away ~ tie,
      home < away ~ away_wins
    )

It's easy to tell what's going on, and it's easy to inspect or edit the `result_lookup` table.",1538082974.0
n0gc1ty,"Hell yea, doing some cool stuff with R! ",1538086044.0
mirzaceng,Check the case_when function from dplry to use instead of nested ifelse statements. ,1538119788.0
WitJakuczun,Yes we use to access https://RSuite.io functions.,1538073280.0
crazy_gambit,"Is there really a market for financial models in r?  

I work in investment banking and right this second I'm auditing a project finance model from a major bank in Excel and let me tell you, it's the worst, but even so I can't imagine how you would present the sheer amount of information contained in them using R.  

Honestly I don't think Excel models are going anywhere and you might be much better served learning to model using that rather than R. Though I have to say I'd love to see one such model just to satisfy my curiosity.",1538076625.0
BaronPampa,"I'd be glad to help out, but I'm not sure what's precisely the thing you have issues with.  It would help if you could maybe make a list of stuff you want to learn? A couple of leading questions:

- Until now, you obtained your data from local files. What would be the new source? Sql database? Document store, like MongoDB? Also files,  but maybe uploaded automatically? Or maybe your employer wishes to have possibility to easily run analysis on new file specified by him?

- What's the pipeline you're refering to? Who's maintaining it?

- Are you documenting your code properly? So when somebody green sees it without any verbal explanation, he/she can study it and understand whats going on,  i.e. why are you doing it, what's the purpose, and how can it be run again?

- About version control,  I'm only familiar with git. I think the best way is to start using it in your projects on a day-to-day basis.  Start small. Make a development branch, commit once or twice a day. When code gets stable, move it to master branch. A day will come when you'll screw something and want to go back.  Youll know how to google it by then ;).",1538070990.0
mattindustries,"RStudio has a server version which is fantastic. Both the desktop and server versions have the ability to integration with git repositories. I would watch some youtube videos so you get the general understanding, and then read through [some documentation](http://www.geo.uzh.ch/microsite/reproducible_research/post/rr-rstudio-git/). 

As far as automation, you can use cron. There is an [RStudio add-in](https://cran.r-project.org/web/packages/cronR/README.html) for that, or you can look into crontab. 

If they want on-demand instead of scheduled, you might want to look into writing a Shiny script for them to click on what reports they want, and what range/filters. 
",1538083801.0
madmongoose1,"I uploaded an example report to github a couple of days ago. It shows how to package your R-scripts in a Docker container and set a cron-schedule. This container can then be started on a server. Check it out [here.](https://github.com/filipmoren/report-dummy) Using docker is a really nice way to keep your projects stable over time. 

Regarding version control, do you have any other developers in the office that can introduce you to what they do? If not, sign up for a [Bitbucket-account](https://bitbucket.org/), its free for small teams and let you have private repositories. Then read about the commands `git clone` `git status`  `git add` `git commit` and `git push`. From there you can learn the rest.",1538114453.0
triple_dee,"I kind of learned git by doing but if you're on your own and don't have people to depend on, I understand it's a little intimidating. Maybe try to start a repository with some of your current work and track what you're doing. It's pretty nice to keep track of your work.  [One link that I've kept](
http://happygitwithr.com/) Disclaimer: I haven't used this resource much and it seems rather in depth for basics, but maybe some bits and pieces here and there may help you.

What does ""create reports dynamically"" mean? And also if you're getting your data from spreadsheets and other files that people send you, maybe it'd be good to look into something like shiny. Cron would be good if you have a database or some place that always has your data, and will always output the same report format but with different data. Something like setting up a Shiny server (you can start with a free version I think) would be helpful maybe if you need users to submit files.

edit: I'll also add that if you're not used to command line stuff, you can maybe looking into something like Github desktop which does make the process a little more clear to see...not sure how other people feel about using it, but I use it at work honestly.",1538086866.0
Atheriel,"I'm not sure of your specific work environment, but if people are asking for you to do this you might not be the only one. It's probably worth asking your coworkers if there are existing processes/approaches to automated report generation, and taking advantage of them if so.",1538102838.0
BillCarney,Full disclosure I work at RStudio...RStudio Connect is professional product that can help with this. You can try with out with full & free evaluation [https://www.rstudio.com/products/connect/](https://www.rstudio.com/products/connect/).,1538415879.0
Fueld_,This might help. [https://stackoverflow.com/questions/24440258/selecting-multiple-odd-or-even-columns-rows-for-dataframe](https://stackoverflow.com/questions/24440258/selecting-multiple-odd-or-even-columns-rows-for-dataframe),1538066619.0
flyos,"Something like this?

    library(tidyverse)
    df <- read_tsv(""filename"", col_names = FALSE)
    df %>% 
    # Create a new ID each time ""|"" is encountered
    mutate(ID = X1 %>% str_detect(""\\|"") %>% cumsum()) %>% 
    # Remove the useless ""|"" rows
    filter(!str_detect(X1, ""\\|"")) %>%
    # Replace (first only) "":"" for nonambigous character (here @)
    mutate(X1 = str_replace(X1, ""\\:"", ""@"")) %>%
    # Now, separate key from values
    separate(X1, c(""key"", ""value""), ""@ "") %>%
    # Finally, spread into a tidy dataframe
    spread(key, value)
",1538073140.0
GoodAboutHood,"Here's an awesome free book:

[https://otexts.org/fpp2/](https://otexts.org/fpp2/)

One of the authors (who also created the *forecast* package) teaches a DataCamp course that goes over some of the topics in the book:

[https://www.datacamp.com/courses/forecasting-using-r](https://www.datacamp.com/courses/forecasting-using-r)",1538073155.0
efrique,"Hyndman and Athanasopoulos' *Forecasting: Principles and Practice* (""FPP"") has already been linked; that would be a good first text on forecasting. I'll mention Shumway and Stoffer's book *Time Series Analysis and its Applications* as a good full R-based text on time series. It's a bit more comprehensive.

see the book's page at https://www.stat.pitt.edu/stoffer/tsa4/

[If you look around Stoffer's pages there you should be able to locate a link to the 4th edition of the book, but I'd recommend that you actually buy it; it's easier to learn with a physical copy.]

There's also an ""easy"" version of the book that's more introductory in level.
",1538093803.0
cogsbox,"I would say start with  *Forecasting: Principles and Practice 2.* Then read Principles of Business Forecasting. Also, you may want to work with time series tibbles = [tsibbles](https://pkg.earo.me/tsibble/)",1538095422.0
Belsaga,"This is a very good material:

&#x200B;

[https://a-little-book-of-r-for-time-series.readthedocs.io/en/latest/](https://a-little-book-of-r-for-time-series.readthedocs.io/en/latest/)",1538099749.0
AlisonByTheC,"Facebook’s Prophet package is pretty good. I go between it and the forecast package everyone is mentioning.  

",1538105913.0
noahpoah,"This article is a very weird mish-mash of good and bad information.

From the first paragraph (emphasis mine):

> As a data scientist, I want to describe the design principles of A/B tests based on data science techniques. **They will help you ensure that your A/B tests show you statistically significant results** and move your business in the right direction.

You shouldn't want to ensure statistical significance. If there is no effect, or a very small effect, statistical significance is bad.

This paragraph from later on is a real emotional rollercoaster:

> The P-value (our statistical significance) is the probability of observing a statistic at least as extreme as those measured when the null hypothesis is true. If the p-value is less than a certain threshold (typically 0.05), then we don’t reject hypothesis H0.

Hooray for a correct definition of a p value, sad emoji for mis-stating what you are supposed to do when the p value is less than the threshold.

Point 2 at the end is another emotional rollercoaster (and is inconsistent with some of the good parts of the post):

> Never rely on your intuition, and **don’t stop the experiment until achieving statistical significance.**

There are also some other good points about things like determining what you will measure ahead of time, avoiding data peeking, and so forth, and there is an interesting, and I believe, subtly incorrect take on randomization.

Like I said up top, it's a weird mish-mash...",1538069952.0
bvdzag,"I disagree with the premise here. The goal isn't ""statistical significance."" The goal is to establish whether or not a change has an effect on the dependent variable. If your intervention doesn't have an effect, no matter your N statistical significance will not be reached. Good A/B tests should have designs with quotas or timeframes established before they are launched. They shouldn't be just let to run until you stumble upon some standard errors that happen to be below some arbitrary value. Calling the methods described in the piece ""science"" is a little bit of a stretch.",1538070405.0
BaronPampa,"On mobile too, but basically:
-Find the paths to desired files(os.listdir)

-lapply data.table::fread to the paths to get a list of dataframes. I believe that fread is the fastest way to read csv atm.

-rbindlist(also data.table??) allows you to merge a list of df's into one df, provided they have the same columns. Otherwise youll have to combine Reduce/reduce function with the ""merge"" function. ""By"" argument specifies which column to use in merging

-drop unwanted cols

-fwrite",1538024941.0
addfunr3,"I've been using the Rio package, it wraps fread and can import multiple files. Function is rio::import_list(), it even adds a handy extra column to tell you which file a row comes from.",1538037475.0
blankepitaph,"Super rough cause I'm on mobile, but there's a purrr based method that allows you to read in multiple csvs at once into a list of data frames, described [here](https://github.com/STAT545-UBC/Discussion/issues/398). 

Following that, running purrr::map_dfr(~dplyr::bind_rows) on that resulting list of data frames is a useful way to combine them into one data frame. Finally, readr::write_csv is a will allow you to write that object back to a csv file. 

Edit: I realized I didn't address the column consolidation! I don't know what the data looks like, but is there a common column that can be used to join the two datasets? If so, dplyr::left_join would be your best bet, followed by dplyr::select to just keep the four columns you care about. ",1538017774.0
YepYepYepYepYepUhHuh,"Couple things:

1. It's generally not a good idea to use ```data``` as a an object in R, since R already has a function called data.

2. You're trying to define a data frame using a variable and a subset of that variable. When constructing data frames all columns need to have the same number of rows, which is why your code is not currently working.

3. You can subset a data frame (I'll call it ```data``` as you have by doing this 

``` df <- data[data$factorvariable != ""A"",]```

This gives you a data frame ```df``` without any factor level ""A"". 

I'm not sure why it worked before, it's hard to tell without a reproducible example.",1538001525.0
Lifebyrd,"""R is named partly after the first names of the first two R authors and partly as a play on the name of S"" - [From Wikipedia](https://en.wikipedia.org/wiki/R_(programming_language\)).

R is an implementation of S so they wanted to reference that.

The name was chosen by the creators Ross Ihaka and Robert Gentleman.",1538000207.0
AMessyDatum,[https://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-is-R-named-R\_003f](https://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-is-R-named-R_003f),1538002713.0
where_is_the_mustard,"All I can say is it makes googling things much harder! Then again, C, Julia, and even Python aren't much better.",1538017503.0
phobonym,"https://twitter.com/_ColinFay/status/1037648854206824449?s=19

That Twitter thread has some background info on it!",1538002487.0
Belsaga,"What kind of plot do you want?, a bar plot?, a boxplot? If i'm correct you have only 1 numeric variable right?",1537982110.0
esotericish,This is really easy to do with ggplot. Just create a grouping variable based on infected/uninfected. If you give sample data I could write this up -- it'll be like three lines of code.,1537983639.0
OPdoesnotrespond,"It'd be cool if this David Robinson were that David Robinson.  Like, he retired from the NBA and got his PhD and now works in statistical mumbo-jumbo.

(I kind of want to wikipedia this, just to make sure.  I mean, that David Robinson went to the Naval Academy--he's surely got some brains.  And he's got time to get a PhD after making millions.......)",1537979989.0
not_rico_suave,Cool posts. Reminds me of David Robinson's Introduction to Empirical Bayes.,1537981195.0
Kickuchiyo,"Can someone explain the differences between full bayes, empirical bayes, etc?",1538002390.0
bob_arnold,Nice work. I am doing Bayesian stats at uni this semester and my final project is on Bayesian hierarchical modelling like this. It gets even more interesting when you start playing with the dependence of the player averages on the overall average. ,1538038685.0
krinya,Where is the data comming from for this? I would use i lt as well.,1537998423.0
Stuporficial,"Some form of: 

gather %>% group by row name %>% mutate n_distinct == 1 %>% spread",1537971701.0
Ax3m4n,"Lot's of `gather` solutions here, which I think are the way to go. Basically, if you are doing a calculation like this, it likely means your data isn't tidy. You have values of the same variable distributed across columns. `gather`ing will help most analysis and all plotting from here.",1537975111.0
esotericish,"Here's a really easy way of doing it that works:

    df %>%
      rowwise %>%
      mutate(nunique = length(unique(c(P1, P2, P3)))) %>%
      mutate(same = ifelse(nunique == 1, TRUE, FALSE))",1537983459.0
brindlekin,"The apply function lets you specify if you want the function to apply across rows or columns .  You can apply length(unique()) to all the rows and that should give the number of unique values in each row. In your case, you would want a boolean function if length(unique()) == number of columns is what it sounds like from your description. 

https://www.rdocumentation.org/packages/base/versions/3.5.1/topics/apply
",1537970707.0
infrequentaccismus,"Here are two options, one using a group\_by as mentioned below and one using a nest (Which I think is more versatile and clear:

&#x200B;

`#Using nest()`

`tibble(Item = c(""iPod"", ""iPad"", ""iPhone""),`

`P1 = c(500, 1000, 2000),`

`P2 = c(500, 1500, 2000),`

`P3 = c(500, 100, 2100)`

`) %>%` 

  `nest(-Item) %>%` 

  `mutate(long_data = map(data, ~ gather(.x, key, value))) %>%` 

  `mutate(n_distinct = map_dbl(long_data, ~ n_distinct(.x$value))) %>%` 

  `filter(n_distinct == 1) %>%` 

  `select(Item, data) %>%` 

  `unnest`

  

`#Using group_by()`

`tibble(Item = c(""iPod"", ""iPad"", ""iPhone""),`

`P1 = c(500, 1000, 2000),`

`P2 = c(500, 1500, 2000),`

`P3 = c(500, 100, 2100)`

`) %>%` 

  `gather(key, value, -Item) %>%` 

  `group_by(Item) %>%` 

  `mutate(n_distinct = n_distinct(value)) %>%` 

  `filter(n_distinct == 1) %>%` 

  `select(-n_distinct) %>%` 

  `spread(key, value)`",1537972984.0
Awkward_Newt,On my phone apologies but collect the three end columns then use group by on the first column and do a distinct and then count ,1537972696.0
blankepitaph,"Extremely hacky solution, but perhaps you could create a new column with mutate() that concatenates each row's contents into a single character vector (regardless of individual column content) and then use dplyr::distinct on that column to see whether its nrows matches that of the original data frame?

(I wouldn't be surprised at all if a super tidy method for this exact problem exists though - it just eludes me right now and I'm away from a computer so I can't trawl through documentation...)",1537971492.0
MindlessTime,"Writing this on my phone so I can’t test it, but... try this?

```
# Some dummy data.

d_rend <- data.frame(
    foos = c(1,2,3,4),
    ro = c(1,3,5,7),
    dah = c(1,4,3,2)
)

# You’ll need dplyr, or at least it makes it easier.

require(dplyr)


d_rend <- d_rend %>%
    mutate(
        row_all_equal = (foos == ro) & (foos == dah)
    )

print(d_rend)

```

This works because:

1. Things which are equal to the same thing are equal to each other. So if the second and third row elements are equal to the first, then they’re all equal.
2. The `&` and `&&` functions/operators do slightly different things in R. The `&` function tests for truth between vectors on an element-wise basis and returns a logical vector of the same length. The `&&` function does the same thing but then checks that it’s true for all elements, then returns TRUE or FALSE depending on that result.

",1538004026.0
deadcaribou,"A tidyverse one-liner:

&#x200B;

`df %>% mutate(is_equal = pmap_lgl(.l = list(P1, P2, P3), .f = ~isTRUE(length(unique(c(..1,..2,..3)))==1)))`

&#x200B;",1537987571.0
guepier,"What do you mean, you ""can't use any additional packages""? This requirement makes no sense.",1537961427.0
gnusmasa,"The base plot function has a `plot.function` method that could be of help. 

You can use it like this:

`plot(function(x) x^2, -10, 10)`",1537967471.0
samclifford,You could make a heatmap of the function using `image()`,1537968918.0
dartkite,"Are you asking how to create a surface plot in base R?

[https://www.r-bloggers.com/creating-surface-plots/](https://www.r-bloggers.com/creating-surface-plots/)

&#x200B;

&#x200B;",1537969017.0
kxgq,"Here's the [CRAN Task View on optimization](https://cran.r-project.org/web/views/Optimization.html), which might prove useful to you.",1537923835.0
zdk,"check out CVXR 

[https://cran.r-project.org/web/packages/CVXR/index.html](https://cran.r-project.org/web/packages/CVXR/index.html)

&#x200B;",1537923932.0
KingDuderhino,[Free](http://web.stanford.edu/~boyd/cvxbook/) book on convex optimization.,1537958593.0
minorsecond,Yes. I have an RStudio server (Linux) that mounts a Samba share and runs just fine.,1537889314.0
DeuceWallaces,"They're either lying or they don't know what they're talking about. R can access local network, S3, and a whole slew of virtual drives like Box with certain packages.",1537895855.0
shaqerd,R can pretty much access data anywhere. ,1537893439.0
bc2zb,"I have no issues with RStudio server seeing network storage, or my local R seeing mounted storage on mac, linux, or windows. However, because R does like everything in memory, if you are working with a large dataset on a server that is mounted via smb, it can take a long time to load and write out data, especially if the network connections are garbage. I would encourage you to setup a RStudio server with the data right there if at all possible to minimize their frustrations.  ",1537895568.0
revmachine21,"I suspect they are confusing data storage with data in memory.

They are correct that desktop R must have enough onboard local RAM memory to hold the data object they import into R. The exception to this is if R is running on a server instance like this... Then the server RAM must be sufficient to hold the data in RAM memory.

https://www.rstudio.com/products/rstudio/download-server/


But importing data, as long as the data is stored on a mounted / mapped object accessable by the R instance (whether it be local or server R instance), R can import it from anywhere.

Edit: after a bit more thought, they may have run into issues with network latency. If the data they are accessing is on a remote server (let us assume you have people in location A, server farm in location B 50 miles away) they could be having issues transferring large data objects over the pipe between location A and B. If this is the case, I would look at a process to store data remotely at location B, and copy that data to your statisticians' HDD/SSD in location A overnight so the data is available to them locally the next day. This also might make your non-statistician user base happy because if the statisticians are pulling a ginormous data object during normal business hours, they could be impacting the wider network user base. Maybe...",1537908943.0
mattindustries,"There are a wide variety of use cases for people using R. The biggest two factors are probably 

* Data size
* Frequency the script runs from start (load data into memory)

If you are closing the session and running once an hour, pulling in that 1gb over the network, and into memory is going to be a pain. There are practices that can mitigate the issues though.

* Fetch data only if local file does not exist.
* Have a server subset data (hadoop, mysql, etc) so less information has to be transfered.
* Set up a cron process to copy data from network storage, either on a hook for data being changed or an interval. 

Alternatively you can make a system call to backup once the script is finished running. I prefer using rsync. 

**R scripts should be version controlled using subversion, git, etc**. I usually just add *.csv to my gitignore.",1537909628.0
Ringbailwanton,"Yes, R also has packages to connect to S3 storage, can manage docker images as well.  One issue is often that academics are not as versed in network management and connections.  If you can set up simple functions for them that is probably all they need.

A second challenge is that R really needs slashes to be escaped.  It’s a huge pain, but on Windows you need either \\ or /, so if people aren’t familiar with this it could also cause problems.

The CloudyR project: https://github.com/cloudyr",1537891918.0
Hoelk,"Once the data is loaded into R its in your system memory anyways, doesn't matter where it comes from. It starts to get tricky when you wave to work with data bigger than your RAM, but that has nothing to do with where the persistant data is saved",1537904791.0
shujaa-g,"Not a full answer, but 

A) You can use, e.g. `set.seed(42)` to set the random seed. This isn't specific to getting bootstrap estimates, you can use `set.seed` anywhere.

B) I'm not sure your issue here. Would you like to paramaterize the bootstrap (how? why?) or just stop it from telling you that it is non-parametric?

C) You might prefer the `simpleboot` package, have a look [at this example](https://www.rdocumentation.org/packages/simpleboot/versions/1.1-3/topics/lm.boot).

D) See the bottom of the example in the `simpleboot` link. 

> I find so far that R is really bad at giving you all the statistics you need in one go 

That reminded me of this fortune:

    fortunes::fortune(184)

> You must realize that R is written by experts in statistics and statistical computing who, despite popular opinion, do not believe that everything in SAS and SPSS is worth copying. Some things done in such packages, which trace their roots back to the days of
punched cards and magnetic tape when fitting a single linear model may take several days because your first 5 attempts failed due to syntax errors in the JCL or the SAS code, still reflect the approach of ""give me every possible statistic that could be calculated from this model, whether or not it makes sense"". The approach taken in R is different. The underlying assumption is that the useR is thinking about the analysis while doing it.
>
>   -- Douglas Bates (in reply to the suggestion to include type III sums of squares and lsmeans in base R to make it more similar to SAS or SPSS)
>
>  R-help (March 2007)

You can try to ignore some of the preachiness (there's quite a bit of vitriol in the R-help community against type III sums of squares in particular), but you're right, R doesn't try to give you everything you might want all at once. Because the list of ""everything you might want"" differs quite a lot based on domain and situation. And as soon as you start to add more things into the output, you also have to begin making choices of defaults for things where opinions differ quite a bit. Like for standardized effect sizes, how do you standardize categorical variables? I'm not sure what SPSS does, but I've seen literature and can make arguments for multiple different methods.

(Edit: had a couple incomplete sentences about `boot` before I started recommending `simpleboot`)",1537878546.0
,[deleted],1537878208.0
,[deleted],1537876794.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/askstatistics] [Bootstrapping a regression](https://www.reddit.com/r/AskStatistics/comments/9ir7rn/bootstrapping_a_regression/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1537874110.0
ReimannOne,"rollapply() from the zoo package should be able to do it, especially since you already have the function written.


https://www.rdocumentation.org/packages/zoo/versions/1.8-4/topics/rollapply",1537842534.0
loady,"You're a data scientist.  SQL is pretty important in almost every corporate environment so might want to move that forward a bit.  It's fun to learn what is more easily accomplished in sql than other data processing languages like R or Python.

In general I'd recommend approaching marketing analytics with caution.  Marketing loves to advertise itself as ""data driven"" but often the people making decisions are not numerate and really would like you to justify their instincts, rather than be guided by numbers.  But marketing ""wants"" ""smart people"" so there are entry opportunities there.

>>I am knowledgeable in standard regression, GLM, Bayes and also have skills in survey design and analysis. I fully acknowledge that I do not have a stat PhD level skill in these but I would feel comfortable teaching a course in all of the above mentioned areas.

That's awesome.  I've worked with PhDs who didn't know dick about stats



Everyone wants to hire for ML expertise, even though almost nobody hiring for it understands what it means.  ML is often much easier than GLM, as you don't really have to interpret it as much, but even still depending on the application the results will probably be better and then someone can take credit for your magic.

If you want to do software development, it's not that big of a leap from doing programming in R/Python.  I think some people don't realize it's more boring than doing analytics though.",1537867374.0
noahpoah,"My understanding is that there is pretty high demand for people (possibly) exactly like you. Your technical knowledge is relevant and important, but, depending on which part of social science you're working in, so is your knowledge about, well, social science.

I have heard good things about bootcamps that are designed to help academics move to industry. For example, I recently met two people who went through the [Insight Data Science](https://www.insightdatascience.com/) program, and they are very happy with what they are doing now (very good data science positions outside of academia).",1537878279.0
CabSauce,"You probably want the arules package, if you're not using it already. The hardest part was structuring the dataset correctly for the algorithm. I'll do a quick googling if I get a minute.",1537835749.0
universityncoffee,You could try converting it to csv and data wrangling with the tidy library package to make it work easier in R.  ,1537833586.0
shakkyz,You could use str_replace_all() and then some type of aggregation. ,1537834089.0
GoodAboutHood,Do you have an example dataset?,1537829302.0
imidan,"I didn't do much to your plot statement, but here's an example of the rest of it pretty similar to your code. I used some dummy data, but it shouldn't matter what data you use it on. Anyway, this looks long, but I put a lot of comments in... it's really only about 10 lines of code.

The most obvious way to shorten up the code is to not repeat your loop for computing mean and sd... just do them both in the same loop. Another thing is that R is great at applying functions to vectors, so instead of a `for` loop to add up a list of numbers, just use `sum()` on them instead.

    # The next few lines are just to generate some dummy data
    df = data.frame()
    for (i in 1987:2016)
      df = rbind(df, cbind(i, 1:365, rnorm(365, sample(6:12,1), 2)))
    colnames(df) = c(""year"", ""day"", ""obs"")
  
    head(df)
    # end dummy data generation
    
    # set up a starting and ending year for the loop
    startyear = min(df$year)
    endyear = max(df$year)
    
    # yearly will be the data frame containing the yearly statistics
    yearly = data.frame()
    
    for (currentYear in startyear:endyear) {
      # for all rows in df for the current year, get the daily observations
      daily = df[df$year==currentYear,]$obs
      
      # compute mean
      meanObs = sum(daily) / length(daily)
      
      # compute standard deviation
      # this is the sample standard deviation to match the R sd() function
      # to make it the population standard deviation, just remove the -1
      stdevObs = sqrt(sum((daily - meanObs)**2) / (length(daily)-1))
      
      # compute CV
      cvObs = stdevObs / meanObs
      
      # add the values to the output data frame
      # also add the values computed by built-in R functions for comparison
      yearly = rbind(yearly, cbind(currentYear, meanObs, mean(daily), stdevObs, sd(daily), cvObs, sd(daily)/mean(daily)))
    }
    
    yearly
    
    # just a sample plot
    plot(yearly$stdevObs, type=""o"", col=""red"", ylim=c(1,3), xlab=""Years Since 1986"", ylab=""Standard Deviation"")
    title(main='Standard Deviation')    
",1537831847.0
willbell,"Your primary code will be shorter if you define functions to hide away some of the loops.

e.g.

     Mean <- function(x) {

      if (length(x) == 0) {

         return(NA)
    
      }

      total <- 0

      for (i in seq_along(x)) {

          total <- total + x[i]

      }

      result <- total/length(x)

      result

    }

And then you can 'use' mean and standard deviation like usual without actually using the default functions.",1537838062.0
Soctman,"You can export the dataset to a CSV file, which is easily opened in Excel. Let's assume that the dataset is called `MyData` in R. The code would be:

`write.csv(MyData, ""/file/path/MyData.csv"")`

Where the statement in quotation marks is the path to the newly-created CSV file on your machine.

Beware, though - I am sure that a CSV full of historical climate data will be very big!",1537818742.0
One808,"A little poking about reveals:

Try `View(ec_climate_locations_all)` to see the locations.

Use `View(ec_climate_params_all)` to see all the parameters.

Then you can do something like `activePassData <- ec_climate_data(""ACTIVE PASS"")` to gather data from the relevant location.

And then finally you can `write.csv` as mentioned.

Hope this helps.

&#x200B;

&#x200B;",1537822208.0
TrueBirch,Real talk: I run a data science department and most of my projects are not this well organized. Well done!,1537792111.0
tdts,"I saved this post since I am an academic researcher and I want to know how the things are done in the private sector.

Btw, I really impressed with the way you're working. Hope you can get fruitful ideas!",1537788450.0
Sosewuta,I don't see you mention any kind of version control. Any software project is incomplete without it. I recommend git. It has a bit of a learning curve but is really helpful.,1537789723.0
Hector_Belascoaran,"This is very impressive, man. Can you show an example of how you use Docker to deploy a report?",1537800537.0
ryapric,"Your current repo organization is pretty solid. However, you would benefit (especially if containerizing the deployment) by making your R folder a formal R package.

To address your points:

[Please read this whole book](http://r-pkgs.had.co.nz/), which it sounds like you've already found. It answers several of your questions, including the ""why"" of using packages. You can search for other opinions on the matter, but they will mostly echo the author's thoughts. You can never have too few functions in your workflow to warrant using a package; even *one* function can be packaged! If you package up your R workflow, you can benefit from all of the formal tools that R as a software provides for software development (testing, linting, integrity checks, documentation, etc, etc).

That being said, you mentioned testing. ""Testing"" can mean different things here. If you want to *unit* test the functionality of your helper functions, etc., then absolutely put them in a package, to take advantage of the `testthat` library.

If you want to *validate* the data going into the reports (i.e. confirm their validity), then you should write functions that do that for you (and also put them in a package!). I worked in a position where all of our data validations were done by hand, for *years*, before we automated the checks. Your future self will thank you immensely. Shameless plug of my [logging library on CRAN](https://github.com/ryapric/loggit) for logging out failed data validations for review/conditional deployment (e.g. if data validations fail, you can prevent deployment, send an email/Slack message, etc).

Your question on code documentation [can be found in this chapter of the same book](http://r-pkgs.had.co.nz/man.html). `roxygen2` is a fantastic dynamic-documentation tool that feels super natural. In addition, your repo overall should have a `README.md` written in Markdown, which documents the whole project.

I've been in almost the exact same boat as you, as the only analyst on a team working to deploy regular analytical/reporting results (and in the end, containerize them for super easy & reproducible deployment. People love to shit on Docker, but it's usually for very different use cases than what you & I would use it for). You're already hitting many of the best practices I'd describe, but again, please look into packaging your R workflows, and you'll also benefit from learning how such a system can fit into the broader software ecosystem.

Good luck!",1537807496.0
,"> Testing is done manually which takes a lot of time. How do you usually setup automatic testing for automatic reports / markdown documents?

Ideally only the core abstracted functionality should be tested. There should be nothing of that in rmarkdown documents - rmarkdown documents just call the functions defined in your source files or a package.

Separate the computing/plotting/saving/etc into separate functions - don't put everything under one big function. This will make it easier to test.

I personally only test sensitive and complex functions. Plotting functions are never tested - they are typically quite simple. And often time when they go wrong you will see it visually (some line is not there, etc).

> My helperFuns are simply kept in an R-script which is sourced. They should probably be put in a package? But it seems to be a lot of work to just keep a couple of functions.

Writing a package is a lot easier than testing. Also takes a lot less time. There are multiple packages now that will automatically make a skeleton for your package. `devtools` is most popular of those.

> Profiling: I stay away from to usuall pitfalls, make sure to not grow bjects in loops etc., but apart from that I really don't know which lines takes a lot of time. Some of my Dockers generate up to 10 000 reports per day so there could be alot to save. How do you usually setup profiling in projects like this?

Only care about this if it's a problem. If something really takes a lot of time. Otherwise leave it be. I feel like this is the least important from all your concerns.

But try the `profvis` package. It produces a nice interactive plot of your code and assigns each part the time it took to run it. In a few cases I used it it make it quite easy to see where the bottle neck was.

> The tidyverse way to deal with bigger projects seem to be to package them like an R-package. I have yet to see the pros of packageing everything up. Further, if you know of any resources of how to create an R-package (other than Hadleys R packages), the bennefit thereof etc., please let me know.

I am not sure about packaging projects. But I've seen people do it and work with one guy that does this for his project. My impression that it's not so much of a benefit. Personal opinion: make a package out of the functions you use a lot, and make them as abstract as possible. Don't put the whole project inside a package. Everything will change often and you will get tired of running CRAN checks, tests and changing documentation everytime you change one plot.

Also putting your entire project inside an R package will make it a lot harder to use other tools (not written in R) when needed.

> Documentation: Apart from the commit messages, there's none. How do you keep your documentation up to date? A resource to a general introduction on how to write code documentation would also be nice.

Making a package for your base functionality will take care of this. You can do this with a `roxygen2` package which let's you put nice tags in a form of comments above the functions you put in a package and generate the R man pages from the. Then the documentation will be in R's `help()`.

---

In summary - I think all your concerns would be addressed my making a package for your core functions.

---

Few additional things that might bring you benefits:

1. Versioning system like `git`.
2. Automation tools like `Makefile`.",1537806168.0
Economist_hat,"I'm probably one of the most organized data scientists at my company, and I'm not that organized.

Excellent job!

My usual project:

- top level: Entry point and report RMDs
  - R/
    - Cleaning.R, data connectors.R, models.R... 
  - SQL/ (as yours)
  - data/
     - input/
     - output/
  - img/

And git. With data/ usually ignored.

I'm curious to know exactly how you're incorporating docker since my ""deployment"" of reports to my coworkers could use improvement. We're an MS shop so bonus points if you have an inkling of how to integrate report generation to a onedrive.",1537805382.0
Orthas_,"Regarding automatic testing, set up unit tests for different stages. For example for a script which performs data filtering and aggregation you would have a test which feeds the script some simple data and then check if that is processed as it should be. Then you activate the tests whenever you change something. Some testing can make sense to run periodically or each time (eg. see if data looks like it should). 

Yes make a package. That provides easy way to do version control and documentation etc. Biggest benefit of course is if you eventually will share it to someone. 

For profiling simplest way is to just measure system time. There are probably better ways. 

&#x200B;

I would love to see an example of a report you have done. That could maybe generate more useful comments as well. ",1537789598.0
shujaa-g,"Best package resource is Hadley's book on it. Search and you will find. That said, your workflow is solid enough that the additional benefits of packages isn't so much more. 

The main thing a package would get you is a nice framework for documenting and testing code. You would put all your common functions in the package (and common constants too - package data is a great place to keep, e.g., branded colors and a custom ggplot theme).

You can realize just about all of the package benefits without a package, but a package gives you a common structure for all of it, makes it easy to share/install, makes your documentation easily accessible and pretty, will make sense to a broader audience, has lots of examples to pattern after and learn from...
",1537797234.0
madmongoose1,"Hi again!

&#x200B;

I've created a repo with a dummy-report. It is really a MWE but is shows the the major parts of how i do things today.  [https://github.com/filipmoren/report-dummy](https://github.com/filipmoren/report-dummy)

&#x200B;

Feel free to check it out and please comment with suggestions and questions. And thank you for your comments so far!",1537874705.0
ZachForTheWin,Please post examples?,1537791195.0
Bruce-M,"Thanks for sharing your workflow!

I will echo the others and say that I'm doubly impressed at how organized you are.

My structure is similar:

    /case/
        /code/
        /data/
        /report/ 

I would really like to use Docker to containerize each case. I don't need to run it continuously like you do, but rather I'm thinking of using Docker for archival so that if it needs to be run a year or so from now it can be done and have the results be the same. Also for sharing the data and scripts if someone else wishes to replicate it.",1537821162.0
Laerphon,"With regard to using packages to organize projects, I do it using [rrtools](https://github.com/benmarwick/rrtools). Each article I write is a package, though the structure varies substantially depending on my needs. For fully reproducible work, knitting the article PDF will fetch data, clean it, run models, generate output, and then dump the doc. For things with big data or models with run times over a few minutes it will only fetch existing output when it knits.

I wouldn't say it is inherently a better way to build things out than the above, but the constraints on R packages force me to do things in a clean and consistent manner.",1537823627.0
anondasein,"I built my first package for R after studying this book, [Mastering Software Development in R](https://bookdown.org/rdpeng/RProgDA/).  It walks you though how to setup the package files in a very straight forward way.",1537884673.0
setyte,"Reading about your workflow was a pleasure, and I can't contribute a thing. I definitely have some learning to do to get to your level. Personally I think being a solo analyst would be a dream. Then again I spent 6 months sort of as a solo analyst and it was a nightmare of people expecting me to pull rabbits out of hats. Your situation doesn't seem to have the component of other people expecting you to quickly turn shitty data into magic reports when given faulty instructions on what they want.",1537935065.0
Moody_Mudskipper,"It's a neat workflow. You can have some ideas about workflows here : https://stackoverflow.com/questions/1429907/workflow-for-statistical-analysis-and-report-writing/47238105#47238105, including mine a few months ago, though it evolved a bit.

> My helperFuns are simply kept in an R-script which is sourced. They should probably be put in a package? But it seems to be a lot of work to just keep a couple of functions.

Packages are really not complicated to make, in Rstudio :

    # create new devtools package project
    # stuff all functions in R folder
    # make sure non base functions are called with somepackage::some_fun notation
    # put cursor on function body
    # Ctrl+Shift+Alt+R to create template Roxygen skeleton, fill it
    devtools::use_package(""somepackage"")
    devtools::document()
    pkg_path  <- devtools::build()
    install.packages(pkg_path, repos = NULL, type = ""source"",clean = TRUE)
    # tada

> Profiling: I stay away from to usuall pitfalls, make sure to not grow bjects in loops etc., but apart from that I really don't know which lines takes a lot of time. Some of my Dockers generate up to 10 000 reports per day so there could be alot to save. How do you usually setup profiling in projects like this?

https://www.r-bloggers.com/profiling-r-code/

> The tidyverse way to deal with bigger projects seem to be to package them like an R-package. I have yet to see the pros of packageing everything up. Further, if you know of any resources of how to create an R-package (other than Hadleys R packages), the bennefit thereof etc., please let me know.

Projects don't have to be organized like packages, but it's convenient if they're organized so it's very easy at some point to create a package out of the reusable functions. It can be just storing the functions that are not project specific in different files or in a separate folder.

> Documentation: Apart from the commit messages, there's none. How do you keep your documentation up to date? A resource to a general introduction on how to write code documentation would also be nice.

Hadley's R packages really has it all, but Ctrl+Shift+Alt+R gives you 90% of what you need to know.

",1538878600.0
triple_dee,"I think you're doing a better job than I do...A lot of my work seems somewhat similar to yours, except I use shiny rather than RMarkdown. I'm also sort of a one man analyst group, but I fell into a workflow that already existed which was a rather poorly put together. I'd like to fix things up to be more standard and have thought about using docker, any tips or links to articles that were helpful to you? 

I also get all my data from a SQL server directly from R which is annoying because I don't quite have DB resources to help me nor authorization to make temp tables...I'm just kind of curious how you work with SQL and R? Or anyone else for that matter.",1537837067.0
revmachine21,"I'm an R novice and am trying to improve my workflow habits too. Here are a collection of links assembled to incorporate into my processes. You may find some of the contents helpful. You do already have a well defined work flow so some of these may be a bit basic. My best link has since gone 404 grrrrrr. 

https://ntguardian.wordpress.com/2018/08/02/how-should-i-organize-my-r-research-projects/

https://gist.github.com/jennybc/362f52446fe1ebc4c49f

https://github.com/jennybc/here_here#readme

https://www.tidyverse.org/articles/2017/12/workflow-vs-script/


In case this returns to the net....

https://chrisvoncsefalvay.com/defensive-programming-r/#1",1537842669.0
Alytia,"Awesome, I've been thinking about workflow recently. I'm solo as well, so it's really interesting to see what other people do. My thesis is a one-off project rather than an ongoing job, so my needs differ from yours.

I've been experimenting with [R Project Template](http://projecttemplate.net/index.html) for structure. I think there are benefits to approaching any analysis in the same way every time (although I'm struggling a bit with getting it to recognise .accdb files). I'm trying to integrate this with [Github flow](https://guides.github.com/introduction/flow/). Even though I'm not collaborating with anyone, this helps keep me on track with my project. For each TODO I take out an issue and then generate a new branch to work on. Once I think it's solved, I merge it back into the master.

Prior to starting this, my directory structure was:

* project
   * data
      * raw
      * interim
      * processed
      * external
   * source
   * notebooks

I'm still not fully settled on this, so I'll be keeping an eye on this thread with interest.",1537854130.0
malditobarbudo,"dplyr has the `case_when` function for this case to use inside a mutate statement:

[https://www.rdocumentation.org/packages/dplyr/versions/0.7.6/topics/case\_when](https://www.rdocumentation.org/packages/dplyr/versions/0.7.6/topics/case_when)",1537784965.0
flyos,"What about this:

     df0[df0$id %in% id_fix, ] <- 
          df0[df0$id %in% id_fix, ] %>% 
          mutate_at(vars(matches(""^p"")), ~ NA)",1537776243.0
coffeecoffeecoffeee,"It’s simple:  I see Data Science Central, I downvote.",1537771266.0
DemonKingWart,"If you're using R's scale function, all observations are divided by the scaling factor. This means the scaled mean will be the original mean divided by the coming factor. ",1537761230.0
too_many_splines,"You'd need the actual value of cigarettes smoked and then take the difference between that and your prediction to get the residual for that particular datum. But that's not very useful. More broadly speaking, a linear regression will give you an unbiased sigma estimate and that will tell you the spread and thus the entire distribution of the model's residuals as a whole (remember ols says its normal centered around 0)",1537757095.0
blozenge,"For 1 and 2 you need to sort a factor. In my experience it's usually easiest to do this outside ggplot. Use relevel or similar so that for your factor `level(TEST1$Block)` return the levels in the order you want. You can always put in a dplyr mutate and relevel before passing to ggplot then it doesn't change your data. Once the factor has the right levels ggplot will order it right.

For 3 you can probably just put `fill=ALC.T3` in the aes for the bar. You then need to play with `scale_fill_gradient2()` to get the diverging colours.

For 4 there is an example which does this in this cookbook: [http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html](http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html) look for diverging lollipop chart. It should just be adding a geom_text on top of a geom_point.

",1537738602.0
slammaster,"It's really just a much scarier version of the birthday problem.  It's not about sharing 9/13 *specific* loci, it's about sharing any 9 loci.

I just taught probability last week, I might share this example.",1537728040.0
,But R overtakes Python in my heart,1537720853.0
dulldata,"I think using Google trends to identify popularity in search is fine. But they can never been said whether it's for good or bad. For example the search about Bitcoin would have been higher while the coin was falling down because of the curiosity the news generated. The same way more people might be looking for Python on Google but not necessarily translated to use Python in their day job or work. 

Having said that, I don't deny the fact that many new comers prefer Python to R because of its popularity among CS, I think as R developers we have to make it a case that R is as good as Python in the tasks Python is used in Data science",1537724783.0
normee,I don't get why Vincent Granville keeps trying to spam old links to his shitty site under so many alts to all the data-related subs,1537731023.0
Hoelk,"I think that is ok, though are you sure you do not want to export them? You usually should not need to use `:::`, even across your own work. If the testing is important for you, look into unit tests with testthat, instead of relying on the execution of the examples",1537695709.0
fabiofavusmaximus,"[Data](http://ireports.wrapsnet.org/Interactive-Reporting/EnumType/Report?ItemPath=/rpt_WebArrivalsReports/MX%20-%20Arrivals%20by%20Nationality%20and%20Religion)

[Code](http://www.favstats.eu/post/refugee_dat/)

[Github](https://github.com/favstats/usa_refugee_data)


[Static Graph](https://github.com/favstats/usa_refugee_data/blob/master/refugee_analysis_files/figure-gfm/unnamed-chunk-3-1.png?raw=true)

[Total Number of Refugees Graph](https://github.com/favstats/usa_refugee_data/blob/master/refugee_analysis_files/figure-gfm/unnamed-chunk-2-1.png?raw=true)

Tools: `R`, `ggplot2` and `gganimate`



",1537623833.0
remil_,"Nice job, thank you!",1537630068.0
tragicsolitude,Very clean. Thanks!,1537641895.0
Samazing42,Really cool thanks for sharing.,1537657486.0
anti-gif-bot,"[mp4 link](https://g.redditmedia.com/GBqrAnFJGBRHsyHn0dL80CP5gXWgBLG0vp1piSS-mGs.gif?fm=mp4&mp4-fragmented=false&s=a2469f46451c0ce1bd6c6e7257e9c193)

---
This mp4 version is 56.87% smaller than the gif (1.2 MB vs 2.79 MB).  


---
*Beep, I'm a bot.* [FAQ](https://np.reddit.com/r/anti_gif_bot/wiki/index) | [author](https://np.reddit.com/message/compose?to=MrWasdennnoch) | [source](https://github.com/wasdennnoch/reddit-anti-gif-bot) | v1.1.2",1537623845.0
chandaliergalaxy,"It's nice to see someone who appreciates the language for itself as most reviews of R are along the lines of great libraries; terrible language.

However, I would say that Cabal and CRAN are more about the ecosystem rather than the language itself. It would seem that while not trivial, it is possible for someone to write a separate package manager for Haskell without fundamental changes to the language.

And how ""general-purpose"" are we talking about here? I do use it for some text processing and shell scripting - the vectorized operations, particularly for strings, is super nice - but typically I would turn to python for these tasks. Part of it is the libraries (`os` and `subprocess`, etc.), but also the fundamental data types and operations built for them - (tuples in particular; you can use tuples for dictionary keys). And anything hierarchical (e.g., involving tree-like structures) I tend to reach for python. 

The functional ""support"" for R (vs. python which seems to be reluctant in embracing this paradigm or does so begrudgingly) and recent introduction of pipes (`magrittr`) is very nice though. I know about 7 languages but R does come near my top.


",1537578486.0
keepitsalty,Obviously time spent with a language has a lot to do with things but I agree. I can pretty much do any general purpose scripting I need with R. Especially since the advent of the tidyverse and things like rvest. Make doing short simple scripts super easy for R. ,1537578493.0
coip,"As someone who uses R solely for statistical analysis, what are the non-data things you're using R for that make you love it as a""general-purpose, high-level language""? 

Unless I misunderstood, you don't seem to really state what your general purpose usage is or explain why it has ""many advantages over [ruby, python, and perl]"". Do tell!",1537580354.0
Eleventhousand,"It's personal opinion, really.

For general purpose simulations, I like C.

For general purpose utility (the protypical automate the boring stuff), I like Python.

If I boot into Windows and need a more complex OOP simulation with a GUI, I like VB.Net.

For data analysis, I like R (and of course SQL).  Strangely, other than taking some MOOC courses utilizing it, I've never really done any data work in Python.



",1537582755.0
AlisonByTheC,"We get nothing but “blocked by group policy” errors with non-admin accounts with library installations via pip for Python at our work. 

I don’t dislike Python but my god it’s so painful to do anything with it with such a locked down environment we have in my workplace. Anaconda? Blocked by group policy.   It’s absurd. 

R/RStudio on the other hand, simple and painless.  Packages install with zero issues save for the rare Rtools requirement.    

With tidyverse available, I’ve watched at least four other employees with zero experience jump from nothing to munging, modeling, and forecasting within six months.  

I love it, and I evangelize it as much as I can.  Especially over that devil language, SAS.  

",1537586804.0
xubu42,"I love R for a lot of reasons, but most of which have to do with it's inspiration from Scheme (and this Lisp). R is a really practical language because it provides a ton of structured flexibility. Creating methods using multiple dispatch makes programming generic tasks fun. This is why you can call the summary or plot or whatever function on different types and get different, relevant, useful results. That's not a thing in a lot of languages. I like that R has OOP, but it's philosophy is basically, ""here if you need it, but you probably don't and even if you want to try it we'll keep it to a bare minimum"". Lastly, creating packages in R is lightweight and easy and the tooling is excellent.

I didn't feel this way about R until I was mostly done reading Advanced R by Hadley Wickham. It's a great book, free and open a html (though you can buy a hardcopy if you prefer), and made me a better programmer. http://adv-r.had.co.nz

I don't always prefer R and find it easier to use other languages for certain tasks, but I'm find myself in admiration of the R language because it is just so different and interesting.",1537588402.0
ExcellentOdysseus,Yes,1537604352.0
setyte,Write a game in R and I won't think you are crazy. Or a photo editing program or something significant.,1537596890.0
EquipLordBritish,"I'm an amateur, but I thought you were supposed to use a width value with position, not a text value...

Most tutorials online say to use position like the following:  

    >pd = position_dodge(width = 1)
    >ggplot(...) + geom_...(position = pd)

I've always had to play around with the width; If your x values are numeric and they are much larger or smaller than 1-10, you may have to change the width variable to something comparable to your x range.",1537570666.0
chandaliergalaxy,"I don't know if this fixes it, but have you tried using the group argument?

    ggplot(aes(x = course_number, y = mean_enrolled, fill = year, group=year)) +
      geom_bar(stat = 'identity', position = ""dodge"")",1537571704.0
displaced_soc,"I assume your year is listed as numeric, and you need to change it to

     fill = as.factor(year)

This should work.",1537622747.0
bunsenthebeaker,"The simplest method would be to convert the image to black and white then calculate the number of black pixels.  You would need to create a calibration curve with a known number of eggs and use this curve to find the unknown number.  

The raster package would have the necessary tools to do this.",1537551061.0
cu29co,"I dont know that there is not a way in R, but you should also investigate how to do this in Fiji/imageJ. I have used it in the past to count large numbers of small seed on a paper. You can also easily figure seed/egg size if you care. I am also interested if there is a way to do this in R or maybe a package that uses imagej from r. ",1537551167.0
_Wintermute,"Is there a particular reason this has to be done in R? Image processing is really not R's strong point.

However, you might be best with [EBImage](https://bioconductor.org/packages/release/bioc/html/EBImage.html), it's designed for fluorescent microscopy images, but finding blobs is finding blobs.",1537551094.0
fencelizard,"edit: I see u/bunsenthebeaker already had this suggestion - seconded.

Check out the raster package - it's mainly for spatial data but should have all the tools you need. If you can do a few counts manually to get an idea of the average number of pixels each egg takes up something like this should work:

    r <- raster(""~/Desktop/eggs.jpg"")
    eggs <- r < 100
    eggs <- reclassify(eggs,matrix(c(0,256,1),nrow=1,byrow=T))
    egg_pixels <- sum(values(eggs))
    egg_pixels/pixels_per_egg",1537564418.0
mouse_Brains,"I think I'll have the motivation to write some code of mine for this because it reminds me of the old days but here's what I think you should do:

Turn the image black and white

Try to determine a threshold for the image. Some basic clustering should be enough for this. This is necessary because different images have different background levels and background isn't uniform.

Detect contiguous objects based on adjacent pixels. Count their sizes. (matlab has great functions for this)

Filter based on size to remove dust and noise. If zoom level is different in each image this also have to be done dynamically.


Profit ",1537570472.0
biggulpfiction,"If you have a matlab license, I would definitely recommend doing it in matlab instead as there is a ton of resources/infrastructure already built in for this. With a bit of googling, you could probably find a matlab script already written to do it for you ([example](https://www.instructables.com/id/Image-Processing-and-Counting-using-MATLAB/)). The straightforward part of it, as others pointed out, is to [convert the image to black and white](https://blogs.mathworks.com/steve/2016/05/09/image-binarization-im2bw-and-graythresh/), and then count the number of black items. With overlapping/touching items, watershed transforms will help you out here, also with [tons of resources in matlab](https://www.mathworks.com/company/newsletters/articles/the-watershed-transform-strategies-for-image-segmentation.html)",1537626610.0
Squeezie91,"Cut out some small Images each containing one egg. Open them as rgb matrix/array and set a threshold so you detect all pixels over a certain ""black"" threshold. Mean those numbers, load the original image as rgb matrix and again count all the black pixels above the same threshold and divide by the mean number you retrieved earlier. Voila. ",1537555929.0
nerdsarepeopletoo,"My first instinct would be to first play with some color/contrast transformations to really distinguish the egg colors from the background. If you can create a data frame or matrix of all the x-y coordinates of ""black"" pixels, you could use something like DBSCAN to detect clumps of black pixels which could be eggs or clumps of eggs or noise. 

Each cluster would be sized by a number of pixels and you could try to do some learning on the distribution of cluster sizes to guess the number of eggs in a multi-egg clump - I'm thinking you would see a large number of clusters with similar pixel counts, and then a number of outliers with large (clumps) or small (noise) counts.

You could isolate these groups maybe using like kmeans or some other method to get the rough distribution of pixels in single-egg clusters and apply some math the estimate the number of eggs in larger clumps.

I dunno, maybe it wouldn't work but it would probably be fast and pretty straightforward if it does",1537595965.0
kgmeister,"Are these the eggs of Aedes aegypti, presumably from the Wolbachia project? ",1537556155.0
baracapy,"`c(2,4:7)` gives you a vector of all column numbers, excluding the third:

    plot(data[,c(2,4:7)])",1537543245.0
Oshobooboo,"Replace 2:7 with c(2,4:7)",1537543256.0
grasshoppermouse,"You can select specific columns a few different ways:

    data[c(2,4,5,6,7)]
    data[c(2,4:7)]
    data[c('mysecondvar', 'myfourthvar', 'myfifthvar')] # replace names with your column names",1537543409.0
dulldata,What do you mean by reproduce a table ? You just need to print a table? ,1537489205.0
ohheyitsdeejay,"Do you mean create a new data.frame?

new_df = data.frame(x, y, z)",1537490296.0
Babahoyo,Use Rstudio to install packages. Rstudio adds some defaults like where to look for online for the package. Jupyter doesn't do that. ,1537747551.0
Cezoone,"Found this the other day, and it's really great for when you don't need a whole IDE but still want to do some R. ",1537484933.0
thefringthing,Hey look! I'm credited for the name. :),1537528953.0
lcota,Thanks for this.  No more JGR...,1537490077.0
guepier,">  but of course these tests compare normal distributions

[Not quite](https://en.wikipedia.org/wiki/Student%27s_t-test#Assumptions): *t*-tests compare data where the *sample means* follow a normal distribution. That’s certainly true for normal distributions proper but — in the limit case — it’s also true for a lot of other distributions.

You can test whether this is the case for your distribution with the help of a [Q–Q plot](https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot) — and lots of means of randomly sampled distributions:

    means = replicate(1000L, mean(rbeta(30, 20, 10)))
    qqnorm(means)
    qqline(means)

This gives me [something like this](https://i.imgur.com/o1sQ2N7.png), which is actually pretty good, despite the tail.

More formally you could apply a [Shapiro–Wilk test of normality](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test). Applying this to my test data gives a *p*-value on the low end (i.e. a relatively low probability that the data is normal):

    > shapiro.test(means)

        	Shapiro-Wilk normality test

    data:  x
    W = 0.99709, p-value = 0.06602

 You’ll need to see whether this is good enough for you. Given the Q–Q plot, I wouldn’t be too worried. If you’re still concerned about non-normality, you can substitute the *t*-test with a [Mann–Whitney *U* test](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) (known as `wilcox.test` in R), which is a nonparametric test and has no assumption of normality.",1537527640.0
efrique,"1. If the distribution is beta, as long as it's not going to be pretty skew a t-test wouldn't be terrible

2. If you have beta responses, you could easily have the situation where the distribution changes a lot but the mean may not change (you could have a case where P(male score<female score) exceeds 50% substantially but the means are equal. You need to be very clear about what kind of difference you want to detect!

3. If you make necessary assumptions that make a difference in distribution imply a difference in mean, you could do a likelihood ratio test. If your assumption is that the change will only be to the first parameter, so much the easier.

3. Personally, if you are primarily interested in means, I'd be inclined to use beta regression


(orthogonal to all the above, would the real data be ""paired"" in any sense?)",1537490462.0
SemanticTriangle,"If you want to compare means on the null hypothesis that the classifier value is the same between the two sexes, you could use a bootstrap/permute method. As long as you have enough data, there's no assumption that the data is distributed in a particular way.",1537492434.0
titaniumcartographer,"Original blog post is here [https://www.displayr.com/chances-hit-by-meteorite/](https://www.displayr.com/chances-hit-by-meteorite/)

Made in Displayr: [https://www.displayr.com](https://www.displayr.com)

Made in collaboration with my colleague (disclosure: I work for Displayr)",1537483890.0
questionquality,`gather()` and then probably `separate()`,1537480062.0
VincentStaples,Show your data.,1537495872.0
mkuehn10,"This is what it says

Package ‘GenABEL’ was removed from the CRAN repository.
Formerly available versions can be obtained from the archive.
Archived on 2018-05-25 as check problems were not corrected despite reminders.

You can try downloading from https://cran.r-project.org/src/contrib/Archive/GenABEL/ and building it locally I think.",1537459125.0
mkuehn10,https://fivethirtyeight.com/methodology/how-fivethirtyeights-house-and-senate-models-work/,1537458719.0
Alrik,"    plot(1:5, xaxt = ""n"", xlab='Occuptation') #xaxt suppresses the numbering
    axis(1, at=1:5, labels=c('Bank Teller', 'Astronaut', 'Porn Star', 'Retail Clerk', 'Salvation Army Bell Ringer') # custom axis tick labels",1537461486.0
DeuceWallaces,That was a nice paper with lots of applications beyond psych.,1537471356.0
engti,"what did you try? sharing your code, even if it doesn't work, usually helps shed some light on what needs to be done.


",1537436868.0
Playblueorgohome,"dplyr is the right call.

group\_by(Isolate, Temperature) %>%

summarise(new\_col = mean(Germinated))

&#x200B;

That will give you the mean germinated value across the three reps for each isolate and temperature. I think that is what you are asking for.

&#x200B;

Warning could come from two functions conflicting in the name space run group\_by and summarise explicitly from dplyr with dplyr::group\_by. 

&#x200B;",1537438433.0
MrLegilimens,You should download rStudio and use it.,1537441124.0
misterkoko,Is it just R vs RStudio - two separate programs?,1537435039.0
william12323,RStudio is awesome. It will make R much easier to use.,1537443634.0
mkuehn10,https://www.tidyverse.org,1537412580.0
GildedFuchs,"I find data.table faster, more intuitive, and more flexible than dplyr",1537463332.0
ashwinmalshe,For actual ML modeling check out caret and mlr. ,1537451124.0
umbrelamafia,"Are you using ggplot?
if so
https://ggplot2.tidyverse.org/reference/geom_text.html",1537413364.0
umbrelamafia,"    da = data.frame(
      country=c('brazil', 'guatemala', 'namibia')

      ,x=c(1,2,3)

      ,y=c(3,1,2)

    )


    plot(y~x,data=da)

    text(y~x,labels=country, data=da)

",1537413652.0
hiranomi,Shinyapps.io might be the cheaper option.,1537413697.0
SemanticTriangle,"Am I interpreting your problem correctly in saying that your null hypothesis is:

H0: There is no difference between P(easy) for all of the reporters

?

And that you are trying to calculate the probability that H0 is incorrect for essentially each reporter?",1537419515.0
NinjasInTheWind,"A couple of things here: First, you want to group_by all of your grouping variables at once, rather than one at a time, though I don't think that's what you really want to do here anyway. Second, you can't summarize a variable that you have grouped by. I'm on my phone, so I can't give this the full treatment. It should be something like:

   Dataset %>% dplyr::filter(Month %in% c(""Jul-18"", ""Aug-18"", ""Sep-18"") %>% dplyr::group_by(ID) %>% dplyr::summarize(Velocity = mean(Velocity, na.rm = TRUE))

If I'm a little off-base, I may be able to help you more if you give me the output from `dput(head(Dataset))`.

Good luck!",1537382520.0
alphafishing,"Without a [reproducible example](https://www.tidyverse.org/help/), it's hard to answer definitively. Instead, I've put together a pattern using mtcars that I believe you can adapt for your purpose. Let me know if this isn't what you are trying to accomplish.

&#x200B;
```
mtcars %>%

filter(cyl %in% c(4, 6)) %>%

group\_by(carb, gear) %>%

summarise(mean\_hp = mean(hp))
```
&#x200B;

&#x200B;

&#x200B;",1537421119.0
deanat78,"I'm not familiar with all those subs, but I do agree. I may not be 100% unbiased, but I think rstats should be the unifying sub as it's the largest and is also the stackoverflow/Twitter tag people use most commonly. Someone needs to reach out to other subs mods and ask them if they agree to make the necessary changes to their subs",1537371546.0
notsoslimshaddy91,I think we can unify it to rstats since rstats is used as hastag on twitter and is also a tag on stackoverflow. ,1537355459.0
danderzei,"`rbind(Rlanguage, rstats, RStudio, rprogramming, R_Programming, RStatsProgram)`",1537348593.0
KingDuderhino,"/r/RStatsProgram is dead and /r/R_Programming isn't really active either, but there is also /r/rshiny. Still leaves us with 5 active R-Related subreddits. rstats is also linked from the /r/statistics so maybe this could be the main one. But are the mods from other R-related subreddits ok with that? Unfortunately you can't force an owner of a subreddit to close their sub or redirect traffic to another sub.",1537348805.0
thefringthing,"Step one: Contact the moderators of the other subreddits.  
Step two: For each subreddit that agrees, add their moderators as moderators of this subreddit, then have them close their subreddit.",1537361743.0
keepitsalty,"Mod of /r/statistics and /r/rstats and I agree. To me, rstats should be the main sub. It’s the recognized hashtag people use on twitter and is more informative than other sub titles. ",1537365017.0
william12323,Good idea. Lets do it!,1537349175.0
PM_ur_good_deeds,"Great idea, mods please make it happen!",1537362681.0
danderzei,"In all seriousness, this is a great idea for the mods of these subreddits to consider. How could this be practically facilitated?",1537381836.0
fnord123,"As a mod of r/R_Programming, I'm on board.",1537441581.0
natched,"Unity! As one, come together!",1537379968.0
Darwinmate,The r/R_programming subreddit has been restricted to approved posters only. i.e. it's been closed. ,1537489770.0
_Wintermute,Can we keep /r/RStudio separate as a decoy to contain all the homework related questions from people who don't know RStudio != R ?,1537361178.0
hopeyesperanza,[LOL this thread] (https://www.youtube.com/watch?v=hyquiA8RL1Q),1537383169.0
shaqerd,I like having a separate forum for Shiny questions. They are usually more CSS / HTML / markdown oriented and don't necessarily fit well in a typical R forum.,1537360796.0
fabiofavusmaximus,So what about [r/rstatsmemes](https://www.reddit.com/r/rstatsmemes/)?,1537652115.0
s3x2,">However the documentation for confint() says that it assumes asymptotic normality

That's base confint

lme4 includes a method that extends confint to detect lmer objects and presumably does the right thing instead

See the example code:

https://www.rdocumentation.org/packages/lme4/versions/1.1-18-1/topics/confint.merMod",1537318185.0
efrique,"The asymptotic normality it refers to would be for the sampling distribution of the parameter estimate; the shape of a population distribution for  conditional response doesn't change with the size of the sample you draw, so clearly it can't be a reference to the conditional distribution of Y (let alone its marginal distribution)",1537320271.0
Mooks79,"Before anything you need to stop and remember that a confidence interval is an interval around an estimate. That may be an estimate of a slope/intercept, or a prediction of a dependent variable’s value.

The point is that you could imagine running your estimation a thousand times using a thousand different datasets of measurement data. In each case you’ll get a slightly different estimate, so what you’re doing is estimating the mean of that distribution of estimates. 

What you care about is whether the estimates are normally distributed, not whether the variable is normally distributed - they’re actually two different distributions.

Thanks to the central limit theorem - the distribution of your estimate will tend towards normality, provided you have enough data. That’s even if you’re estimating something about a variable that is *not* normally distributed. 

It’s a bit tricky to get your head around first, but the tl;dr is that confint() may well be exactly what you need, after all. ",1537340261.0
mkuehn10,You could look into bootstrap confidence intervals.  ,1537316326.0
zap1000x,"A lot of this depends on how that data is stored (I'm not familiar with the dataset). You should absolutely be able to use [dplyr](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf) to subsect the data, using simple boolean logic to subsect it as needed by both x and y (which I assume are yardage and...something?). 

Alternatively, use dplyr's mutate funcitons to create a variable for each ""zone"" that you can call in your analysis. 

Like everything in R, the answer is probably something Hadley Wickham built. tidyverse is killer. ",1537302140.0
jpiburn,Can you link to an example of the data? ,1537324721.0
eldenv,"The data OP is referring to is here (""soccer"" for any North Americans): https://github.com/statsbomb/open-data/tree/master/data

You might have seen, but StatsBomb have also put out an R Package to make it even easier to access their free data: https://github.com/statsbomb/StatsBombR

Using this, we can load the event data for the world cup final (France vs Croatia) like so:

    match <- FreeMatches(Comp$competition_id) %>%
      filter(match_id == max(match_id)) %>%
      get.matchFree()

If you see `match$location`, the location data is given as a `c(x,y)` coordinates, 0:120, 0:80 within a column, which is tricky to work with.

It might be easier to split those into separate x and y columns, but I'm not sure how to do that.

Working with the coordinates in the same column, however, you could create a function to categorise them. It would be more complicated to do this for say a grid, such as a 6x4 square (which I imagine is what you're after), but it would be simpler to work with just one value of the coordinates. As such, here's a function to determine which third of the pitch a coordinate is (ignoring for the moment the fact that attacking and defending would be reversed for one team.)

    pitch.thirds <- function(x){
      
      x <- x %>%
        extract(1) ## this takes the x value from the coordinate pair
      
      case_when(x <= 40 ~ ""defensive"",
                between(x, 41, 80) ~ ""middle"",
                x > 80 ~ ""attacking"")
    }

Then back to the dataframe, you can use `map` to apply this function to every pass in the dataset, and create a new column with the result:

    match %>% 
       filter(type.name == ""Pass"") %>% 
       mutate(third = map(location, pitch.thirds))",1537381636.0
ChrisYee90,"unique(df\[df$Haz\_Risk > 1, neighborhood\])

&#x200B;

try that!",1537301256.0
Samazing42,Have you tried the [dplyr](https://dplyr.tidyverse.org/) library? Specifically the `group_by` or `filter` functions.,1537299155.0
cruyff8,"hoods[hoods$Haz_risk > 1,*column number of neighbourhood name*] will sort your problem.",1537303097.0
shujaa-g,"Using base R: `with(your_data, unique(neigborhood_column_name[Haz_risk > 1]))`. 

In general, you might want to look into using `dplyr`. The [Introduction to dplyr](https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html) vignette is a good place to start.",1537299168.0
Holophonist,"The simplest way to do this would be something like:

    unique(df[df$Haz_risk >1, '[neighborhood_col_name]'])",1537306125.0
eldenv,"As others have said, the `tidyverse` and `dplyr` in particular are packages that make this really easy.

Rather than having to nest fuctions within functions, you can use the pipe operator `%>%` to 'chain' functions together.

With your example, it would be as simple as:

`data %>% filter(Haz_risk > 1)`

If you wanted just a list of the neighbourhood names, then you can add another function to the end:

    data %>%
        filter(Haz_risk > 1) %>%
          pull(your_names_column_here)",1537380349.0
asuliman,"1) I would create a new column:

Data$V4 <- abs(Data$V2-Data$V1)

2) I would then sort by V4, create a new data frame, and take the first 10 records:

Data1<- Data[order(V4),] 

Data1 <- Data1[1:10,]

3) Finally, plot using both data frames:

ggplot(Data,aes(V1,V2,)) + geom_point() + geom_point(Data1,aes(V1,V2,col=V3))
",1537300795.0
maijts,"man this is exactly what I am looking for right now, good stuff!",1537271204.0
jackbrux,Isn't this what RMarkdown is for ?,1537224473.0
ecdf,https://github.com/ksint/pasteLastVal,1537217963.0
pan_paniscus,"This is amazing, and a trick I've been looking for! Thanks!",1537221004.0
squirrellina,"Perfect, just what I was looking for! ",1537299878.0
NTGuardian,"In general, complexity is *bad* and you want to avoid and control it as much as possible. So to that end, I recommend two Hadley books: [*Advanced R*](http://adv-r.had.co.nz/) and [*R Packages*](http://r-pkgs.had.co.nz/). *R Packages* teaches you how to write a package, which then guides you how to organize your code. You should be writing functions doing common tasks and separating them into themed files, then have separate analysis scripts (in a separate directory) that actually executes the analysis. *Advanced R* teaches you how to write *powerful* code as opposed to *complex* code, which is then more easily understood.",1537198148.0
kenderpl,"In general you want to organize your code so that it is easier to reason about. Part of it is properly organizing your files so that are responsible for a distinct part of what you are writing - it lowers to cognitive load.

Also the future you or someone else will have easier time figuring out how the pieces fit together if you have split the code sensibly.

I would say that this is not a data analytics thing but a general programming thing, it's important to make the code readable and easy to reason about. I believe that most sensible people realize the importance of this once they go through something (usually their own code from the past) that did not adhere to those ideas.
",1537198223.0
Lareine,"Jenny Bryan, queen of RStudio workflow, has [this nice overview](https://www.tidyverse.org/articles/2017/12/workflow-vs-script/) of how to structure an R project.

&#x200B;

I personally have just converted to the following principles, an amalgamation of advice from conferences, blogs, and chats with people like Dr. Bryan.  For context, I'm a stat professor, and I do my best to develop shareable code but would never call myself a software developer.

* Every project I begin starts with a well-named, empty folder in Dropbox.
* This folder is immediately turned into both a GitHub repository and an R Project.  See [this page](http://happygitwithr.com/) for an amazing walkthrough of using R Projects and GitHub in tandem. 
* Each function gets its own .R script and has no global dependency
* Any large simulation I plan to re-run is also a standalone .R script
* Any exploratory analysis, such as cleaning a dataset, pre-plotting to see the structure of the data, checking out simulation results, etc. goes in RMarkdown.  That way I can remind myself of each step, and go back to chunks.
* Final writeups of results are typically RMarkdown + pdftex.  (This is mainly because my collaborators don't usually use R, so I can easily copy the tex source code over to a pure LaTeX writeup when I need to.)

&#x200B;

Basically, my guiding principle is, what will make it easier to come back to this work.  R Projects with clean source scripts convert to R Packages easily.  Markdown files convert to papers and blog posts easily.  Using Dropbox and GitHub makes me never lose my work and is great for collaboration.

&#x200B;

Hope this helps!",1537214585.0
shaggorama,"The README file of [this project](https://github.com/dmarx/make_for_datascience) discusses the benefits at length and advocates a particular project layout demonstrated in the repo. Skip down to the ""Motivation"" section.

If you'd like to get into the weeds, a great book on the topic is [Guerilla Analytics](http://guerrilla-analytics.net).",1537198577.0
DataDouche,"I feel like R Markdown is the way to go instead of writing multiple scripts, because you can basically split your workflow into code chunks to help separate your ideas, as well as write prose in between chunks to describe your methods/results. Ultimately it is personal preference though.

Using rmd in conjunction with RStudio makes my work feel much more organized and I feel like my productivity is at its peak when I use it. Again though, this is just what I prefer and it might not work as great for others.",1537198096.0
UbiquitinatedKarma,"For me it's about the mental overhead needed to comprehend a program. I find that somewhere between 100-200 LOC I start struggling to remember what all of it is doing. By breaking the analysis up into several scripts you can fit each part into (your own) memory.

Additionally it sequesters things that are ""done"". Once you've read in the data you shouldn't need to touch that code again so you can subset it out into a separate file. ",1537198338.0
abecker93,"For my projects I have that same structure, but I typically will write bits and pieces so that I don't have them replicated throughout my code. Instead of writing a load and clean of a particular data set 10 times across many analyses, I write that in a script, and then source that script in each. This does 2 important things.

1- Reduces the amount I actually have to type

2- It means that if there's a bug in that code, it's only in one place, and I only have to fix it in that one place.

",1537202137.0
WitJakuczun,"Maybe my experience with large scale R apps can be helpful. Please find my presentations:

http://www.slideshare.net/WitJakuczun/r-software-development-how-to-write-and-maintain-30k-loc-in-r-and-survive

http://www.slideshare.net/WitJakuczun/large-scale-machine-learning-projects-with-r-suite",1537246449.0
IdEgoLeBron,"If you're writing an R script over 200 lines, it might be a better idea to do that part in a language better suited for it, and do the essentials in R.",1537200654.0
tacothecat,https://stackoverflow.com/questions/38441732/changing-the-font-size-of-valueboxes,1537184636.0
dhawk312,"As a psychologist also (and former SPSS but now devout R user), here is my two cents: 

1. I think we (as in psychologists, maybe even all behavioral scientist who always used SPSS) use PCA because it's the primary option in SPSS's 'data reduction' category. I believe the SPSS output even mixes PCA and EFA analyses without explicitly labeling these for less savvy users to know whether they're looking at PCA or EFA output, IIRC. That's why people will refer to this SPSS analysis as factory analysis still, even though it's primarily PCA. Most users don't seem to care so I'm not sure if it would help to label these better. This doesn't help for you doing these analyses but I thought this oversimplified summary might ease your mind a bit about not knowing PCA vs EFA. 
2. If your colleagues use SPSS and you want to obtain the same output between R and SPSS, then use the 'flipDimensionReduction' package. I think it's based on the 'psych' package and extends the 'princomp()' function. It allows for Kaiser Normalizations, which SPSS does automatically while there isn't an easier way to do them in R. To install: 

* `require(devtools); install_github(""Displayr/flipDimensionReduction"")`
* `library(flipDimensionReduction)` 
* Then, the basic function call is: `PrincipalComponentsAnalysis(your_measure1_here, rotation = ""none"", select.n.rule=""Kaiser rule"")`
* Using this package, PCA and EFA are dead simple to run, visualize, and report. Of course, be sure to check out the help file to see the functions.

3. Elizabeth Page-Gould, at UToronto, has some great resources for psychologists getting started in R. All of her materials are written in an accessible manner...and they're all freely available with creative commons licensing. She even has a workshop on data reduction, including a very simple explanation of PCA vs EFA and how to conduct each. I highly recommend checking out her (workshop materials here: [http://www.page-gould.com/r/](http://www.page-gould.com/r/) 

&#x200B;

Hope these things help you to run PCA and EFA. At the end of the day, don't sweat the PCA vs EFA issue. Rather, I'd say, learn when to conduct EFA/PCA vs CFA as it sounds like CFA might be more appropriate in your current situation. 

&#x200B;",1537196444.0
master_innovator,"Lavaan package, they have a tutorial on their website for conduction confirmatory factor analysis.",1537127253.0
Deto,"I looked this up when seeing this question and now I'm more confused than ever.  I almost never use Factor Analysis, but I thought the difference was that the factors could be correlated (but that usually, the maximum likelihood fit minimized this to some extent).  However, in the wikipedia definition of it, they constrain them to be uncorrelated.  So then what is the difference between PCA and Factor analysis?  The few articles I found claim they're ""very different"" and go on to explain them using different words, but it appears the math is the same.  Vague BS like ""PCA uses \*components\* but Factor Analysis uses \*Factors\*.  The \*Factors\* cause the observations while the components describe it!"".  

&#x200B;

Can someone explain how they are different mathematically?",1537128782.0
dhvalden,"For an EFA use fa(). For a confirmatory factor analysis, use lavaan.",1537144544.0
where_is_the_mustard,I found this useful: [https://stats.stackexchange.com/questions/123063/is-there-any-good-reason-to-use-pca-instead-of-efa-also-can-pca-be-a-substitut/123136#123136](https://stats.stackexchange.com/questions/123063/is-there-any-good-reason-to-use-pca-instead-of-efa-also-can-pca-be-a-substitut/123136#123136),1537154372.0
guepier,"What the presenter calls “R”, and contrasts with RStudio, **isn’t R**. It’s the “R GUI” (called `R.app` on macOS), which is another (fairly barebone) IDE for R, and which ships with the R binary installation from CRAN. But it’s not R in the same way that RStudio isn’t R.

Given that the whole point of this presentation seems to be to contrast R and RStudio, this inaccuracy is pretty crucial, and more than a little annoying: the presentation gets its core claim fundamentally wrong. It also makes it sound as if RStudio is the only way of using R with the features shown in the video. In reality RStudio is obviously a very powerful and user-friendly IDE but obviously not the only one (ESS, Vim-R and RKWard are just some of the alternatives). And many of the things shown in the video don’t require an IDE at all.",1537188165.0
AnIncredibleMetric,"Can't recommend these enough, Marin does a great job of taking you through R.",1537149924.0
ZZ-TOP,"I assume that x is some measure of distance between the distributions, like difference of mean. It would have been helpful if you linked to the post you are trying to replicate.

Power can be calculated using a simulation, Im not sure if an analytical formula exists for wilcoxon, but if it does, it should be easily available.
Just create some bivariate samples from the distribution, conduct wilcox.test() and save the results. Then for each x, you'll have the estimated power (proportion of rejections). ",1537082892.0
galileuk,Where did you obtain that dataset? Usually there is a codebook accompanying it indicating what the values stand for.,1537059740.0
VincentStaples,"If you don't have a codebook, often the label is stored in the variable if you just print it (i.e. run df$gender). If desperate, look for known gender differences across your variables.",1537061097.0
jcmoney18,"The dataset was obtained from an online bio stats class. It is a .csv file, I don’t know if that helps. Where would I find the code book?",1537072330.0
ZZ-TOP,"I made myself a rule: the variable indicates the number of y chromosomes, therefore I always set male = 1, female = 0.",1537083066.0
geocompR,"I haven’t watched the video yet, but glad to see more random forest out there. I build a huge amount of models in my line of work, where my clients value predictive power over explanatory power. More often than not I find RF to perform better than SVM or Neural Nets, yet still retain some explanation of what is happening in terms of variable importance.",1537073898.0
aladyinsane42,"how about something like this?

    cor.matrix <- cor(df)
    strong.cor <- names(abs(cor.matrix[abs(cor.matrix[, 1001]) > .25, 1001]))
    new.df <- df[names(df) %in% strong.cor]",1537052101.0
infrequentaccismus,"    df %>% 
      summarise_at(2:1001, ~abs(cor(.x, y)) > 25) %>%
      as.matrix %>%
      t %>%
      as.vector%>%
      {df[,.]}",1537067249.0
Tarqon,"I've been using Udpipe a bunch lately. It has by far the best lemmatization and part of speech tagging for non-english languages I've found. 

Be warned though, running it takes a very long time on large datasets.",1537106861.0
I_just_made,"I keep meaning to learn more about NLP.  Thanks for sharing, I will have to check it out!",1537185945.0
Jerome_Eugene_Morrow,"Broad outline of steps (you should be able to find the specifics by Googling):

1. Load the text as a string

2. Split the string at spaces

3. Remove punctuation

4. Use ```sample``` to select your random eight words without replacement",1537033020.0
efrique,"You should clarify what you mean by ""random"" here

Does every word have the same chance? If I have ""a"" and ""subdermatoglyphic"" in the article once should they each have the same chance to be drawn even though the second word takes up a larger fraction of the article?

What happens with repeats? If ""a"" appears 16 times and ""subdermatoglyphic"" appears once,  what happens then? ",1537061084.0
cyril1991," You likely won’t get something online for free but you could install Jupyter server on some computer of yours (it doesn’t have to be a beast). 

You install Docker and use it to install one of the Jupyter notebook server stacks (the R one) from https://jupyter-docker-stacks.readthedocs.io/en/latest/index.html and manage it with a tool like Portainer. Add some free letsencrypt certificate to secure it, or configure a VPN server and connect to it. 

You get an online R notebook you can access whenever you want, and it is free and open source.

",1537015436.0
smortaz,"Notebooks.azure.com. Free. Python, R, F#. ",1537018926.0
dartkite,You can run R in Jupyter notebook using Sagemaker on AWS,1537033827.0
grrrck,Not sure it fits your use case but have you looked at [Binder](https://mybinder.org)? They have a [few examples using R](https://github.com/binder-examples/r/blob/master/README.md). ,1537034634.0
shujaa-g,"Sounds like she's having trouble with the features of the RStudio editor. I'd recommend looking for a video introduction to RStudio, or [this presentation](https://dss.princeton.edu/training/RStudio101.pdf) might help.",1536960346.0
ChestnutArthur,"More info would be needed to say for sure, but it sounds from the description like she wants to view the contents of the file and is using the View() function to do so. This brings up a spreadsheet in a separate window (or RStudio tab). 

If this is the case and she instead wants the contents to print to screen she just needs to type in the name of the object and hit enter.",1536960517.0
mkuehn10,"How are you trying to load it?  You should just be able to do load(""file.Rdata"") and then you should see the object in the environment pane if you're using RStudio.

If you're not using RStudio try same thing and then try ls() to see if you see the object.",1536959095.0
abecker93,"If you want to load the file into your workspace in R, you can use load(). Then once the object is loaded into your workspace, you can access it by typing the name of it into the console and hitting enter. A more helpful way is using the head() function which prints the top ten lines of the object.

For example, if you had an .RData file located in your documents, named ""data.RData"" which contained a data frame called 'data' you could load and view it with the following commands (assuming a mac/unix machine).

load('~/Documents/data.RData')

head(data)

",1536961909.0
I_just_made,"/u/abecker93 has good advice here; you are loading it into the environment, then using a command after that is confusing maybe.

    view(yourdataset)

is what is causing that spreadsheet to show up; its fine to use, though I never do.  If you are coming up with your script, then just typing

    yourdataset

Will print the information from your dataframe in the console.

    head(yourdataset)

Will give you the first 6 lines (more manageable). Hope it helps!",1537186467.0
shujaa-g,"This seems like a `tex` question, not an R question. `knitr` calls pandoc which calls `lualatex` to compile the PDF, so the only relevant question is how to use `lualatex` with your requirements. You'll probably get a quick answer if you ask at <https://tex.stackexchange.com/>.
",1536949891.0
Hasnep,"I don't have a solution, but if you want to do multiple lines of code in Reddit, start each line with 4 spaces.

    ---
    title: ""Test Report""
    author: ""me""`
    geometry: left = .5cm, right = .5cm, top = 3cm, bottom = 3cm
    output:
      pdf_document:
        keep_tex: yes
        latex_engine: lualatex
    header-includes:
      -\usepackage{wallpaper}
      -\usepackage{fontspec}
      -\pagenumbering{gobble}
      -\setmainfont[Path = ~/Desktop/Typography/ , UprightFont =  font1 , ItalicFont = font2 , BoldFont =  -Bold , Extension = .otf]{common_filenames}
    ---

Good luck with your question. :)",1536945680.0
klo99,"\\fontfamily{lmss}

\\fontsize{10}{14}

\\selectfont",1536976990.0
fonzy6,I would check out the openxlsx package. You can create an excel file and use that packages commands to format it. I’m pretty sure you can create a table to look like that using the package.,1536939105.0
guepier,"There are many small inaccuracies and suboptimal solutions here. I’m sorry: this is pretty sloppy for somebody with so much R experience under their belt.

* “Transposing a dataframe” actually creates a *matrix*, not a data.frame. This is important, because unlike in a data.frame, in a matrix all columns have the same type (`character`, in the example given). This is rarely what you want. Use `melt` instead (or, better yet, the tidyverse equivalents)

* “New Function Creationg *[sic]*” contains a redundant space in the pasted values (either use `paste0` instead of `paste` or, more likely, remove the trailing space in the string literal).

* “Boolean to Integer Typecasting” uses the discouraged `T` and `F` instead of the literals `TRUE` and `FALSE`. Don’t do this: `T` and `F` can be reassigned (`TRUE` and `FALSE` can’t). Furthermore, coercing logical values to integers via prefix `+` works (and is efficient!) but is obscure. Be explicit and use `as.numeric` instead.

* “Extracting Words starting with 's' in a string” would normally be written more idiomatically (and shorter, more efficiently and more readably) as

        grep('^s', unlist(strsplit(sentence, ' ')), value = TRUE)

  or:

        stringr::str_match_all(sentence, '\\bs\\w+')[[1L]]

  If using `lapply`, keep your anonymous function short; in particular, the braces in this case are simply redundant and decrease readability; instead, write

        unlist(lapply(unlist(strsplit(sentence, ' ')), function (x) if (startsWith(x, 's')) x))

  And, on the subject, please use spaces consistently (as shown here, and unlike in the original link).

",1536937412.0
ukraineisnotweak,"Sorry, minor error/typo in one line of code:

`names(new_df) <- b1:10` should be `names(new_df) <- 1:10` 

under the Matrix to Dataframe conversion.",1536895290.0
0_to_1,"First, start with benchmarking your actual process - if you use RStudio, you can get quick timing on your functions using the Profile menu (""Profile selected lines""). 

Not to contradict you but R can be fairly fast. More than likely, it's something else such as a callout, file retrieval or something of that nature which is making your app ""run"" slowly. I have noticed that feather, while very useful for crossing to python, is relatively large and unwieldy. Start with RDS files for caching if you can. ",1536818762.0
gds506,"Even tough this is not directly related to your R/Fortran question, but have you already tried with data.table? In one of my Shiny apps it reduced the processing lag from around 6 seconds to less than one (vs dplyr). After that improvement I use it in every Shiny app I build. ",1536842284.0
efrique,Did you read the documentation? ,1536820022.0
chrisbeeley,"To be fair, a cheap cloud VM running Shiny Server Open Source is a lot less than $15K a year",1536832191.0
vaguely_specific1,"looks cool, how does it differ from shinyapps.io?  Does it allow for more active hours for free?",1536800235.0
rosshalde,"Very cool! Are you involved with the company/site? If so, I have some questions about a project I have been considering.",1536805658.0
nobits,"A question like this is probably best asked on stack overflow, where there's better formatting for detailed answers with code/graphs etc.  
One way to do this using ggplot2 (a popular visualization library for R) is [this answer here](https://stackoverflow.com/questions/47273193/r-plot-over-a-background-image-with-coordinates)",1536768374.0
I_just_made,"Check out [The Magick Package](https://cran.r-project.org/web/packages/magick/vignettes/intro.html).  This is a package designed to work with images in R; of note, you can do composites like you are asking for I believe. They specifically show a demonstration of drawing one image on top of a scatter plot.  Let me know if you have any questions about it; I have worked with it a bit as I come up with functions for annotating electrophoresis gels.  They aren't the best, but the package is pretty handy and easy to work with.",1537186808.0
dhvalden,"What about mousetrap: https://cran.r-project.org/web/packages/mousetrap/index.html

Also, mousetraker http://www.mousetracker.org/?",1536796224.0
mick14731,"I use statscan data alot, what do you need tested? ",1536758023.0
moosejock,Looks interesting. Will be looking at StatsCan data in a few months - bookmarked!,1536844656.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/rlanguage] [Cross-post - a package for Stats Canada API](https://www.reddit.com/r/Rlanguage/comments/9f7bpy/crosspost_a_package_for_stats_canada_api/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1536755781.0
moosejock,"I'm trying to access the 2016 Census data, but don't see at CANSIM or the Table number. For example, there's the page with Population Dwelling counts: https://www12.statcan.gc.ca/census-recensement/2016/dp-pd/hlt-fst/pd-pl/Table.cfm

",1540308272.0
Deto,"Great post and some very thoughtful comments.

One thing I'd like to mention is that you actually can create code blocks that aren't supposed to be executed in a Jupyter notebook.  The trick is to just define the cell as a markdown cell and then inside it, write the code surrounded by the three backticks opening/closing line to mark it as code.

So in your markdown cell you can then mix your text and code and when you run the cell, it just formats the result - it doesn't run the code in the code-formatted blocks.",1536696448.0
guepier,"Yihui makes a few good points but I think on balance I agree a lot more (like, close to 100%) with Joel than with him. This is mainly due to three factors:

1. Yihui downplays just how different R Notebooks and Jupyter notebooks are. Yes, there’s a section where he looks at how R Notebooks stack up against the criticisms. But he’s maybe being a bit too modest here: R Notebooks are quite a *fundamental*, rather than just an incremental, improvement over Jupyter Notebooks (due to how they are just plain text and yet well integrated into an IDE).

2. Yihui maintains that, while some of the criticisms are true, notebooks have tremendous value for exploratory data analysis (EDA). This is true, but he ignores that there are other ways of performing EDA which work just as well: [EDA actually works *great* in an IDE with an embedded REPL.](https://twitter.com/klmr/status/1033127518293970944) I switched all my EDA to knitr/Rmarkdown/notebooks a few years ago, and switched back some time later. I now mostly use [`spin`](https://deanattali.com/2015/03/24/knitrs-best-hidden-gem-spin/) rather than `knit`, which allows me to write literate programming documents that are at the same time reusable code modules.

3. Yihui stresses that data analysis isn’t the same as programming and therefore doesn’t need to follow software engineering best practices. I think this is (mostly) wrong, and harmful advice. It’s true that not all software engineering best practices translate well to EDA. But it’s a cop-out to therefore throw them out of the window. For instance, I agree that useful unit testing is very hard and probably unnecessary for strict data analyses. However, the answer to this shouldn’t be “ignore unit testing”. It should be, “think about the code you write, and refactor as much of it into modules, which you then test”. Notebooks invariably discourage this approach, and I think Joel’s examples show this very well, contrary to what Yihui says.

  In fact, if you’re performing data analysis in R or Python, *you’re programming* — whether you like it or not. There are nuances between library development and EDA using programming tools, but I claim that the commonalities are much bigger than the differences. And in the R and Python community, it’s become fashionable to vastly overstate the differences. Joel doesn’t, and this is why he comes to a different conclusion regarding the usefulness of notebooks for EDA.

R Notebooks are *great* for some things. For example, authoring presentations or articles/books and generally for making rerunnable examples. But they’re not particularly great for EDA, and data scientists tend to rely on them way too much.",1536748878.0
nobits,"I really like how R Markdown is integrated with RStudio, so you get the benefits of a good IDE, despite the debugging functionality in notebook being not that good.  
Do people generally use the Jupyter's default editor for notebooks or do they prefer writing in an IDE like PyCharm?",1536697996.0
quantnerd,The Great* Notebook War. It's not the first until there's a second.,1536739627.0
eric_he,"I don’t understand why people are always up in arms about the fact that a notebook can run cells out of order or ignore previously run all cells. I was always under the impression that that was a feature rather than a bug.

When I’m performing an ad hoc analysis I make a lot of mistakes in my coding, or I’ll make a transform which I want to modify. Why would I want to redownload the data, redo other transforms or rebuild graphs just so I can normalize one feature in my latest transform? Nobody asks a journalist to retype an article if (s)he decides to change the flow of the last three paragraphs!

Whenever I think I’m done with the analysis, I’ll make another notebook which has all the code cleaned up and in order to rerun the code and see if I messed anything up (or I’ll keep two notebooks, one for messing around and he other to hold completed work). Maybe not the best workflow, but making me have read only cells or rerunning previous cells would not be any kind of solution to my needs.",1536719110.0
Darwinmate,"Checkout this:

[https://cran.r-project.org/web/views/HighPerformanceComputing.html](https://cran.r-project.org/web/views/HighPerformanceComputing.html)

&#x200B;

and the default/base \`parrallel\` package",1536722500.0
keepitsalty,"We can leverage `gather()` and do something similar to the code used in the [README](https://github.com/tidyverse/purrr) of the purrr library. 

    boston_list <- boston %>% gather(key = ""var"", value = ""value"", 2:14) %>% split(.$var)
    
    boston_pred <- names(boston)[-1]
    
    map2_df(boston_list, boston_pred, ~lm(crim ~ value, data = .) %>% tidy() %>% mutate(var = .y))

Now you don't have to utilize `paste()` and you end up with a similar table.
",1536665250.0
jbuhr,"I don't have access to R at the moment, but I think the best solution would involve keeping everything inside one datafram/tibble. Try to use gather() to gather every column except for your response variable. This should give you a tibble in long form, where every row is some combination of the response variable and one of the predictors. Then, you can group_by() your newly created column (say you called it ""predictor""), pass it to mutate() inside which you call map().

Sorry I couldn't provide proper code, but this should get you on the right track :)",1536653572.0
tacothecat,"Here are a number of other methods:

    ## Use paste
    library(tidyverse)
    
    tibble(var = names(boston)[-1]) %>% 
      mutate(lm = map(var, ~lm(paste(""crim ~"",.), data = boston) %>% tidy)) %>% 
      unnest()
    
    ## Use glue
    library(tidyverse)
    library(glue)
    
    tibble(var = names(boston)[-1]) %>% 
      mutate(lm = map(var, ~lm(glue(""crim ~ {.}""), data = boston) %>% tidy)) %>% 
      unnest()
    
    ## Use sym
    library(tidyverse)
    
    names(boston)[-1] %>% 
      map(sym) %>% 
      map(~expr(crim ~ !!.)) %>% 
      map(lm,data=boston) %>% 
      map_df(tidy)
    
    
    ## Use sym3
    library(tidyverse)
    
    tibble(var = names(boston)[-1]) %>%
      mutate(results = 
               var %>% 
               map(sym) %>% 
               map(~expr(crim ~ !!.)) %>%
               map(~lm(data = boston)) %>% 
               map(tidy)) %>% 
      unnest()

&#x200B;",1536705434.0
IMainlineMemes,Which exercise is it?,1536643288.0
guepier,"Your code isn’t bad. The only changes I would make are cosmetic: `purrr::map` supports a shorthand syntax for anonymous functions; and don’t use pipelines in small sub-expressions. The result is:

    tibble(predictor = names(boston)[-1]) %>%
        mutate(map(predictor, ~ tidy(lm(paste('crim ~', .x), boston)))) %>%
        unnest()

Same as yours, just slightly shorter.

If you want to go the extra mile, don’t use string manipulation on R expressions: it’s conceptually unclean and error-prone. Unfortunately *in this particular case* the alternative is slightly longer, and not obviously superior. Here it is anyway for completeness’ sake:

    tibble(predictor = names(boston)[-1]) %>%
        mutate(map(predictor, ~ tidy(lm(bquote(crim ~ .(as.name(.x))), boston)))) %>%
        unnest()

",1536665766.0
chrisbot5000,"I hate for loops as much as the next guy, but this is one idea. The nice thing about this, it'll store your models in a list called models that you can index by feature name. So linear regression of crime on number of rooms in dwelling is just `models$rm` for instance.

```
library(MASS)
library(dplyr)

boston <- as_data_frame(Boston)

crime <- boston[[""crim""]]

boston_features <- boston %>%
  select(-crim)

models <- list()

for(i in 1:ncol(boston_features)){
	feature <- pull(boston_features[,i])

	models[i] <- lm(crime ~ feature)
}

names(models) <- names(boston_features)
```

edit:markdown fix",1536669602.0
flyos,"You could use a function like this:

    do_lm <- function(var, df) {
         formula <- as.formula(str_glue(""crim ~ {var}""))
         lm(formula, data = as.data.frame(df))
    }",1536651964.0
VincentStaples,"Best to do it visually at a first cut if you have a large number of dependent variables. Assuming you're not in a field that's anal about treating well-structured ordinal data as numeric...

df.x <- subset(df, select=c(""catvar"", ""ordinal1"", ""ordinal2"", ""ordinal3"", ""ordinal4"", ""ordinal5""))    

df.melt <- melt(df.x, id.vars = c(""catvar""))  

ggplot(df.melt, aes(catvar, value, col = variable))+  
stat_summary(fun.data = mean_cl_normal, geom = ""pointrange)+  
facet_grid(~variable)",1536655099.0
schwerbherb,"Multinomial regression does exactly this. However this quickly gets unsightly when you have a lot of categories or independent variables.

Sounds like you are trying to evaluate a questionnaire? Usually psychologists have pretty good tools for evaluating this. Think factor analysis maybe or look up Cronbach's Alpha. But I doubt that this will work with a categorical dependent variable.",1536617441.0
thaisofalexandria,"You can ignore the fact that some of the data are ordered and simply do a chi-square test between your five value column and one of the others - this will tell you whether they are associated or independent.  You have to do this pairwise so you should think about whether it introduces an increased likelihood of type I error.

Alternatively you could use a non-parametric test like kruskal-wallace and test them all out pairwise.  You will still have to think about distributing the error but it's easy enough to do it manually.",1536671310.0
moon_patrol,"Normally, you would use a Kruskal-Wallis with a mix of categorical variable and numeric variables. Here, you treat the level of the categorical variable like a numeric variable, I'm pretty sure this violate some assumption behind this test. In my opinion, khi square would be more appropriate. ",1536671187.0
moon_patrol,"Do you know which association measure (or test?) you want to use? It's something I would clarify first: 

Do you want to do a statistical test, for example to test the independence between two variables? 

Do you want to measure the effect (like a correlation) between two variables? 

Do you want to modelize the effect of many variables on a dependent variable? ",1536613384.0
AlisonByTheC,Wouldn’t the baruta or vip packages do the job?,1536638124.0
ZZ-TOP,"Very nice post, I might introduce this subject in a course im TAing. ",1536603805.0
SemanticTriangle,"Is it generally good practice to ever leave a tibble grouped like this? It's a kind of non-intuitive property to just 'leave hanging' on a tibble. It's not a sort. It's just an association which affects how subsequent operations will act.

From a workflow point of view, shouldn't groups almost always be undone after the relevant processing steps are performed?",1536589263.0
Stuporficial,https://dplyr.tidyverse.org/reference/groups.html,1536585303.0
aridf,"I've never used that particular plot before, but usually you can add lines to a ggplot with + geom_vline(x intercept = [insert x value for the line here]. That's how I would try and solve this. ",1536497943.0
DrHampants,"Use the geom_vline(aes(xintercept = value)  for that. An easy way to find the 1st and 3rd quartiles would be to use the quantile(data$variable) argument, so you might do something like:

    geom_vline(aes(xintercept = quantile(DDT)[2])) + #first quartile
    geom_vline(aes(xintercept = quantile(DDT)[4])) #third quartile

Try it and let us know if it works!",1536506484.0
HelloiamaTeddyBear,"You have a dataset (of a sample), if describing it is all you want, then by all means don't bootstrap. However, if we want to 'generalize' into a population (say we want to know the mean of the population) there are many ways to go about it:   
1.) We can assume certain things ^^^like ^^^the ^^^sampling ^^^distribution ^^^is ^^^normal, and create a confidence interval around that, say 95% confidence interval. This means that in a hundred identical repetitions of the experiment/study, then then the confidence interval would contain the true mean of the population 95/100 times.   
2.) However, some people are not comfortable with this assumption, so they create other assumptions ^^^like ^^^the ^^^sample ^^^is ^^^representative ^^^of ^^^the ^^^population, and this is where the bootstrapping comes in. Because all we have is the data in front of us, this is all that we have going on to infer about the larger population. By resampling we basically re-do our experiment/study over and over and over again, and over the course of this repetitions we get the mean over and over again (because it changes more or less in each iteration). Then we  can create an interval in which we can be pretty sure is the location of the true population mean. Why does this work? because we assume the data we have representative of the population, then resampling from it is like getting different subsets of the population every iteration.

",1536495300.0
Pneumatocyst,"When you sample a population, you are hoping that you have sampled enough that your results will represent the 'true' population. 

But... what if you're not too sure? What if you haven't sampled enough? What if you can't sample enough? How can you take your 'small' sample and draw conclusions about the larger population?

Bootstrapping! 

If you were to repeatedly sample the 'true' population, you would get a distribution of means (in your case). Do it enough times and the 'true' population's mean should fall within the range of your sample means. 

Bootstrapping re-creates this process, but uses your 'single' sample as a proxy for the 'true' population. You're creating a range of means from your sample, and if your sample  is representative of the 'true' population, the range of means you produce should cover the 'true' mean. 

One major issue you should always keep in mind about bootstrapping, is that you are only actually finding support values of your sample. And therefore, you're always assuming your sample is representative of the 'true' population.",1536510773.0
rojowro86,"Why wouldn't this just produce a fuzzy version of the sample?  When n=infinity, won't the bootstrapped mean be equal to the initial sample mean?",1536531298.0
avamk,"Follow up question, if anyone is still on this thread:

How do you relate this to the concepts of standard deviation vs standard error? AFAIK you use the standard deviation when referring to the variance of that one, original sample, but standard error when referring to the variance of the means from each bootstrapping sample. But... why?/how?",1536926483.0
,[deleted],1536498187.0
Soctman,"Does anybody know if the `Stylo` text analysis package works well for identifying ""cross-genre"" writing, e.g. comparing a short story to a newspaper article by the same author? It seems unlikely that the Op-Ed author would have any editorial pieces similar to this floating around the internet, making a data comparison of this sort almost impossible.

On a side-note - I do not think data scientists should be attempting to reveal an anonymous source, especially in the name of data science. But it is out there for the general public, so who am I to judge. ¯\\\_(ツ)_/¯",1536548231.0
floridawhiteguy,"Using the wrong dataset for comparisons won't help - Start instead by sourcing text from every person who's ever written an editorial there. That will (at least) tell you the most likely person to have edited the work, because it almost certainly didn't come direct into its published final form without some tweaking and stylistic changes.

Speaking of which: IMHO it's all just too neat, and too clean, and too consistent with the paper's style to be anything other than masturbatory fan-fiction.",1536513692.0
markedmondson,"Do you want the output as a list?  Then this works, alter the 1:6 to the length you need:

lapply(1:6, function(x) seq(from = 15, by = 15, length.out = x))

[[1]]
[1] 15

[[2]]
[1] 15 30

[[3]]
[1] 15 30 45

[[4]]
[1] 15 30 45 60

[[5]]
[1] 15 30 45 60 75

[[6]]
[1] 15 30 45 60 75 90
",1536418503.0
allumallu,Try using Map with seq() function.,1536418164.0
dhvalden,Use the subset() function.,1536359330.0
run__rabbit__run,"What you want is to do is make two new dataframes, each being a subset of your original dataset, and for that you can use the subset function, like so:

&#x200B;

>df1 -> subset(df, df$year < 1990)

 

Where df is your original data. Then do another subset but for year > 1989. Also, check out the dplyr package and the filter function, which does the same thing. 

&#x200B;

 ",1536359607.0
sovietsatan666,"Thanks y'all! I've got it now. subset was exactly the right thing to do! 

&#x200B;",1536360874.0
guepier,"One issue is that the style of tweets is by necessity quite different from other forms of writing. Having absolutely zero expertise in NLP, I wonder whether it would help the analysis to account for this: e.g. scrub all hashtags and Twitter handles, and only include tweets in the analysis that exceed a certain length/use at least two sentences/have a minimum sentence length.",1536313739.0
RememberToBackupData,You gotta change your writing style for throwaway accounts!,1536315677.0
chris_conlan,Someone was on /r/datasets yesterday asking how to approach this problem haha,1536324849.0
hodos_ano_kato,I’m just happy to see how `widyr` is used—definitely want to start incorporating it in the workflow!,1536340912.0
pina_koala,"This is good, and I like the stats, but tweets and op-eds are totally different writing styles. If he'd used books written without ghostwriters (co-authors) then we'd really be cooking. ",1536331219.0
BeerSharkBot,"Just another example of people doing things because it technically can be done, not because it should. Seems like thats the primary driver of most projects. Learning when to implement something is the hard part, implementing it is trivial in comparison.",1536366016.0
Thats_not_magic,Nice! Saved and will be going through as soon as I have some time and bourbon. ,1536282272.0
Volatilityshort,Very nice!  I would love to see more tutorials like this. ,1536284278.0
mowshowitz,Thank you so much for making and sharing this! Can't wait to dig into this.,1536512597.0
VisuelleData,Put every project in a folder and keep similar files in a subfolder,1536274616.0
peepplayingbyme,"[https://peerj.com/preprints/3192/](https://peerj.com/preprints/3192/) 

I recommend the above paper as a guide. Your use cases could be entirely different, but it provides a general framework.",1536349238.0
RememberToBackupData,"Here's what I do for my thesis:

1. I put every dataset in its own project/folder, where the data gets cleaned and reshaped.
2. I put those cleaned datasets into a package whose job is to contain all of these datasets plus some other thesis-related stuff, e.g. functions to convert factors from a central list of my study species etc.
3. The analyses for each chapter are contained in their own project/folder, and within those projects are separate Rmarkdown files that ask and answer one question. Because I store my data in a package, I don't need to maintain any kind of directory relationship to a folder where I keep the data, I can just attach my package and do `data(pod)` to get the pod dataset.",1536278258.0
shaqerd,Huh?,1536273695.0
KingDuderhino,Learn git. ,1536301496.0
flyos,"That's because mutate_all() with several function calls like this will create new columns. Check the number of columns after your call, it should have tripled.

If you really want to condense into one call, you need to create a function beforehand like:

    convert <- . %>% str_conv(encoding = 'utf-8') %>% str_replace_all('[^[:ascii:]]', '')
    df %>% mutate_all(convert)",1536261415.0
0_to_1,"Here's a pretty definitive resource for base r:
[easy r programming](http://www.sthda.com/english/wiki/easy-r-programming-basics#basic-arithmetic-operations)

Feel free to reach out with more questions or also, search on [stackoverflow](https://stackoverflow.com/questions/tagged/r)",1536253611.0
Crypt0Nihilist,"[Twotorials](http://www.twotorials.com) Anthony Damico is an absolute legend.

You'll want to use RStudio rather than the way he does it, but he'll help you lots with the concepts.",1536254888.0
Quasimoto3000,http://r4ds.had.co.nz,1536278680.0
TheArtilleryMan,"Datacamp do an intro to r programming course. Also as an useful intro to stats, I always recommend laerd statistics. They use spss but if you complete the instructions and select paste, itll put it into syntax which is similar to R and could be useful for comparisons and general stats knowledge. Also don't be afraid to tell your teacher if he is assuming knowledge that was not a specified prerequisite. Left brained stats nerds often forget not everyone is as literate with lanaguages. In the meantime, sort a study group. Saved my life. ",1536256453.0
russian_botka,"datacamp.com was a life saver. While it can feel kind of grindy, it teaches you what you need to know, especially for social sciences. ",1536276944.0
wouldeye,r/learnrstats,1536282053.0
aaronsaunders,"I followed these instructions and they worked for me. It was not clear from the question whether you have R installed but that it a prerequisite.

https://chrisconlan.com/installing-r-python-anaconda-biologists/",1536263584.0
Darwinmate,"First Go here:https://cloud.r-project.org/

You are probably on windows so you want this link: https://cloud.r-project.org/bin/windows/base/R-3.5.1-win.exe if not go here to select your OS: https://cloud.r-project.org/

Install it in default location. 

Once you have downloaded R, download Rstudio: https://download1.rstudio.org/RStudio-1.1.456.exe
if you are not on windows use this link to select your OS: https://www.rstudio.com/products/rstudio/download/#download

Use Rstudio, don't use base R software directly. ",1536197058.0
xubu42,"Do you have a + sign between geom_bar and ggtitle functions? Think of each function for ggplot2 as adding a layer to the chart, which is why you use + sign. Next time, instead of copy/paste from the R console, put into a text editor (or script pane of RStudio) and copy/paste that so the new R lines (> sign) don't show up ",1536204628.0
bvdzag,"Question isn't reproducible; would need more info on the data and objects being manipulated.

But, for one, once `class_survey` is set as the data, in `ggplot()`, you no longer need to include it when calling variables from that data. So `class_survey$Religious_Importance` can instead be `Religious_Importance`.",1536186921.0
Hoelk,"dates are internally stored as numbers, the 17713 is number of days since some reference date. use `format(i)` and you should be fine. if that doesn help, the problem is likely how the `for` loop accesses the variables. Then you can do `for (i in seq_along(foo)) ... foo[i]`

Style advice: `i` should only be used for integer indices anyways.",1536161756.0
SemanticTriangle,"Can't you just do it?

    df %>%
      mutate( dummy= min(c(AppointmentDate-`1`, AppointmentDate-`2', AppointmentDate-`3`), na.rm = TRUE),
        Min_difference = ifelse(
          dummy >= 0, dummy, NA
      ),
    dummy = NULL
    )

Alternatively, you could work on a tidy data set then revert back to your untidy ways afterwards,

    df %>%
      gather(key, value, -PatientID, -AppointmentDate) %>%
      group_by(PatientID) %>%
      mutate(
        dummy = min(AppointmentDate - value, na.rm = TRUE),
        Min_difference = ifelse(dummy >= 0, dummy, NA),
        dummy = NULL
      ) %>%
      ungroup() %>%
      spread(key, value)

I haven't tested any of this code but if you know any dplyr you should get the gist and be able to correct formatting errors.

If you don't know dplyr, go learn dplyr, and you won't have to ask data wrangling questions.",1536141637.0
anonemouse2010,"    df$MinDiff <- apply(df[,23:262] - df$AppointmentDate, 1, min)",1536162167.0
samclifford,"If you really need to run multiple paired t tests (to compare against some baseline) and want to do it in a tidy manner you would probably use gather to have a column of repeated baseline values, a column of treatment labels, and then a column of treatment values such that each row was a pair. Then split on label, and use map to run the test, then extract the p value and return as a data frame with `map_df` with the `.id` argument named sonyou can keep track of where the p value came from. ",1536129351.0
ron_leflore,"I think it's extracted out off here
https://www.transtats.bts.gov/ONTIME/
",1536106240.0
zorp_,"lost a week of my life just working on transit data (roads in Minnesota) the data are so rich, also super easy to get in csv",1536118681.0
The_Old_Wise_One,"Not exactly answering your question, but why are the information criterion not working out well for you? Also, have you tried LOOIC (loo package) or Bayes Factors (bridgesampling package)? LOOIC is generally more stable than WAIC. 

I would think that a logistic regression is straightforward enough that most of these metrics would be fine, so I'm curious to know what's going on.",1536114451.0
AspiringGuru,"This is good. will be rereading this a few times.  


I'm constantly looking to upgrade my skills. Too many tutorials/courses stop at 'how to do X' widget and don't follow through on how to structure complex systems.  Any pointers, resources, materials in this area appreciated. It's waay to easy to build bad habits.  


Case in point: recently inherited a system built haphazardly, thousands of lines of code, near zero documentation of function, zero error trapping or management. zero staging points to enable debugging, tuning, improvement without re-running the entire process. ",1536100369.0
singularperturbation,"Just did a hacked-together Shiny app, 'multipage' through use of navbarpage.  Really wish I knew this stuff (+ Shiny modules) before.

This definitely looks useful.  Also tried a bit with [plumber](https://www.rplumber.io/), but it's a little finicky, especially when trying to upload non-text files.",1536109802.0
BehindBrownEyes,"I would say its better to use latex with editor like texstuio and save outputs from R (plots = eps, tables = xtable package). Its hard to find any real advantages of rstudio and if there are any they are more difficulties that you will face just because rstudio is not build for such a task. ",1536047056.0
PM_ME_QUOTE,"You can try this [cheat sheet](https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf)

Alternatively, you can see this rmarkdown [book](https://bookdown.org/yihui/rmarkdown/) ",1536017218.0
huessy,"In order to make them completely customizable you need to load a case template that you write. Sadly that's all the info I know about it, start out by googling ""RMarkdown custom layout""",1536017451.0
RememberToBackupData,"    > char <- ""10""
    > char
    [1] ""10""
    > class(char)
    [1] ""character""
    > as.numeric(char)
    [1] 10
    > class(as.numeric(char))
    [1] ""numeric""

If you're trying to learn R, you should try Swirl. https://swirlstats.com/",1535961430.0
danderzei,Use as.numeric. If the character value is not a valid number you get NA.,1535961390.0
goddammitbutters,"Howdy. When posting a problem, try to be very specific in what you start with, and what you want to end up with.

I'm assuming you're starting with some variable x that contains a string, like this:

    x = ""3.14""

If you want to obtain a variable `y` that is equal to `3.14`, i.e. a number without the quotes, just do this:

    y = as.numeric(x)

Now you can do `y * 2` and all that fun stuff.",1535999889.0
efrique,"Do you mean ""how do I get a factor for an ANOVA/regression model"" or do you mean something else? ",1535963067.0
PM_ME_QUOTE,"May I know detail of the character variables? 

Is it a name, string of texts or group variable such as (gender, group a,b,c)

If it is group variable you can changes it to numeric by dummy coding it using dplyr package. For example:

    > library(dplyr)

    > a = c(""a"", ""b"", ""c"")

    > b = c(""1"", ""2"", ""3"")

    > df = as.data.frame(cbind(a,b))

    > df

     a b

    1 a 1

    2 b 2

    3 c 3

    > df$a <- recode(df$a, a = ""0"", b = ""1"", c = ""2"")

    > df

     a b
    
    1 0 1

    2 1 2

    3 2 3",1535961651.0
Fireflite,"You can handle imputation for MNAR data as long as you have an appropriate imputation model. What ""appropriate"" means depends on the causal structure of your data and the level of rigour you're after, but it's going to be challenging to do worse than interpolating the average value (or linearly) or excluding incomplete rows.

Take a look at [MICE](https://cran.r-project.org/web/packages/mice/index.html) and see if it meets your needs.",1535935663.0
bbbeans,I am wondering how the user experience is when interacting with this. I've been staring at it for so long it is hard to get the perspective of someone coming in for the first time. Any suggestions for improvement are appreciated!,1535912750.0
mkuehn10,What are you going to use this for?  I used to teach AP stats and this would be a pretty good tool to show sampling distributions. ,1535914890.0
Nschnock,Have u tried on ipad ? It displays not well (portrait),1535949764.0
Darwinmate,"The `file` (for zipping) referred to in the docs is a path. So `test1.txt` either needs to be in the current working directory or somehow accessible by R. My suggestion is to explicitly state the path: 

    workingDir <- ""C:/UsersBATMAN/foldder/folder/folder/""
    filestozip <- c(""test1.txt"", ""test2.txt"")
    zip(""test.zip"", paste(workingDir, filestozip))

Or something similar, the implementation might be off, can't really test on mobile!
",1535858137.0
,[deleted],1535777071.0
s3x2,"I recommend starting with online courses before books since you get a lot more hands on practice and the material is easier to engage with IMO. Suggest you check out the Data Science track from Johns Hopkins over at Coursera. It's the best free resource I've come across (good expert advice, realistic use cases) and it has a 2x speed setting so it's not that tedious to watch.",1535777563.0
,"If you want to do some quantitative finance stuff in R, you can consider reading to documentation for the package rquantlib. It was written by Dirk Eddelbuettel, who one of the greatest open source statistical software contributors in recent years.",1535777981.0
PopeRaunchyIV,"It depends on your style of learning. I've found it hard to read a book and slog through examples; I do a lot better when I'm passionate about a project and just start working. Lots of roadblocks and lost time, but I judge myself much more likely to actually get somewhere if I shoot first and read the book later.

Honestly, it sounds like you know enough about programming in general to just take your project and dive in. Some intro tutorials might help, Hadley's book at the link you posted is prob a fine place to start reading, not sure haven't done it. The `help` function and google + stack exchange will get you the rest of the way there on the language. Getting back into the stats side will probably be the harder part.

As for R vs Python. Do what feels right to you. If you're just building this model for fun and to sharpen your skills, it won't matter which, the more important thing is just getting the work done. If you want to parlay this into a job, Python is probably better but don't worry too much about that. And yeah, unless you're addicted to EMACS or something, RStudio is practically mandatory.",1535779834.0
klo99,"Books and courses are good, but given that R is free and widely used I would suggest to start and learn by doing. After you learn the basics from books without spending much time you start google your questions. You will find a lot of your answers from stack overflow which you should try to understand and learn.",1535814995.0
lakenp,`tidyquant` might be a helpful package for such analyses. Here's a list with all kinds of R materials you could browse: https://paulvanderlaken.com/2017/08/10/r-resources-cheatsheets-tutorials-books/,1535816912.0
dankwormhole,"You really need to provide more information when you ask questions:
1. Give us the smallest possible subset of your data that shows the problem
2. Give us the relevant code that you have tried already
3. Tell us why it doesn’t work

But anyway, here’s an example of how to use plot()

x <- 1:30

y <- sample(30)

plot(x,y)",1535758661.0
standard_error,"No, you can't interpret the R^2 like that. I don't know how plm calculates the R^2 for these models, but if you think of the dummy variable formulation of a fixed effects model (which is equivalent to the demeaned formulation used by plm), the two-way model is simply the time-f.e. model with the addition of group dummies. R^2 can never fall when you add variables, so an R^2 computed in a comparable way between the two models must *always* be higher for the two-way model. 

This is also one of several reasons why R^2 is very rarely a good model selection statistic. ",1535742208.0
amstell,"It's important to not think about regression analysis in 'the best' way to model in terms of statistic significance or r squared. Instead think about your identification and what you need to control for. You obviously need to control for group effects in order to remove unobserved heterogeniety, if you have them. Now the question is do you need to include a time trend or time fixed effect. A trend removes the no stationary component of the data. Is your data non stationary?  Then add a trend if you want to establish averages, especially in a linear model. If not, then you probably want a year fixed effect. This will control for differences in each year, such as annual season differences.

The take away is to spend time thinking about what you need to model instead of what the best r squared is. Honestly the r squared is irrelevant if you can't predict out of sample, which is the go to method for establishing a 'best' model.

Hope this helps

",1535761536.0
s3x2,"    library(tidyverse)
    
    iris %>%
      mutate(out1 = Sepal.Length < 6,
             out2 = Sepal.Length < 5) %>%
      group_by(Species) %>%
      summarise(p1 = mean(out1),
                p2 = mean(out2),
                pdiff = p1 - p2) -> d
    
    t.test(d$p1)
    t.test(d$p2)
    t.test(d$p1, d$p2, paired=T)

You'll get confidence intervals as part of the output.

[This tutorial](https://www.cyclismo.org/tutorial/R/confidence.html) explains the details behinds the calculations.",1535729305.0
,[deleted],1535699204.0
apaniyam,"For those coming across this same issue in the future, make sure you have not only installed but loaded all the packages you need. One of the supporting packages wasn't loaded, so jtools wasn't loading after installing. 
",1536192245.0
StephenSRMMartin,"I know that the brms/rstanarm/stan set of tools can do BMA and model stacking, but it's with a Bayesian bend to it of course. Those all fit random effects models just fine. They also have better methods that LASSO (e.g., horseshoe, horseshoe+) and better variable selection techniques (e.g., projpred).",1535695241.0
zdk,"You might find the `fs` package easier to work with than `file.path`
https://cran.r-project.org/web/packages/fs/index.html",1535680778.0
bluestorm21,"You might look at `normalizePath` from base as well, setting the `winslash` argument to ""/"". You can also have it check that the path is valid as well with the `mustWork` argument.  

`file.path` and `normalizePath` are both very useful for constructing system-agnostic and relative paths ",1535687907.0
puck_it24,"Ggplot doesn't like making legends out of lots of elements. Bring everything into one dataframe with a variable for category, and then add fill = `category` in the aes (). Dplyr::bind_rows is a good function for contating dataframes, there's a base r function too.",1535672755.0
BillCarney,[https://stackoverflow.com/questions/10349206/add-legend-to-ggplot2-line-plot](https://stackoverflow.com/questions/10349206/add-legend-to-ggplot2-line-plot),1535725450.0
biledemon85,"Ggplot, and most of the tidyverse tools use non-standard evaluation by default, in other words it's expecting you to pass on a bare, unquoted name of one of the columns of the data.frame. I.e. `aes(columnx)` rather than `aes(""columnx"")`, which is what you would see in almost all other languages and is called standard evaluation in R. This is what aes() expects, unquoted names.

It might help to read the documentation for [aes](https://www.rdocumentation.org/packages/ggplot2/versions/3.0.0/topics/aes).

If you pass in the actual vector from the data.frame it can still use it, but it's not the [idiomatic](https://en.wiktionary.org/wiki/idiomatic) way to do it. It might seem a bit strange to write code in non-standard evaluation but the tidyverse is fairly opinionated on how you should code and as much as possible encourages you to forget about the structure of your data (e.g. `dataset[2]`) and focus on the concepts you're trying to evoke in your analysis (e.g. `ggplot(dataset, aes(variable_x))`). 

If you're interested in using ggplot in a programmatic way, such as creating a large amount of similar plots for a report then the recommended way is to use tidyeval and quasiquotation (check the `aes()` docs for a starting point) but be prepared to be very confused for a while. It's fairly abstract stuff.

Edit: I was incorrect about `aes()` not accepting raw vectors, it does, my bad. My other points still stand, the idiomatic way to use `aes()` is using non-standard evaluation (`ggplot(dataset, aes(variable_x))`) and you should prefer it unless you have good reason not to.",1535669277.0
Deto,"What other people are saying is true and useful, but I think it'll work if you use two brackets (e.g., y=`dataframe[[3]]`)

dataframe[i] gives you a dataframe with one column

dataframe[[i]] coerces that column into a vector",1535678938.0
conor_tompkins,"I see two issues.

First, the whole point of ggplot is that you don’t need to deal with vectors within the aes() function. You just need to type the name of the column in the data frame.  Like so:

ggplot(data= your_dataframe, aes(column1, column2))
+ geom_

Second, I think your indexing is incorrect. The first number is the row, the second number is the column. The numbers are separated by a comma.

This selects the first row and second column from dataframe: dataframe[1, 2]

If you just want the entire third column, you omit the first number, but you still need to use the comma: dataframe[, 3]",1535670063.0
neuro99,    data[!a %in% ex],1535654023.0
ejoran,"The != operator is only for comparing two vectors of the same length or comparing against a length-one vector. What's happening here is that R is recycling the ex vector to be the same length as the a column, then comparing element-wise.

What you want instead is the `%in%` operator, or data.table's character optimized version `%chin%`, to see whether each element in column a is a member of ex.

    data[!a %chin% ex]",1535654176.0
MrLegilimens,"Yeah, that's definitely the problem. Because you'd actually want to have:

data[data$a != ""cat"" | data$a !=""dog"" | data$a != ""donkey""]

",1535654067.0
kaaswagen,Looking forward to the next part! ,1535660629.0
NinjasInTheWind,"Here's a tidyverse solution:

>    df <- read.table(header=T, text='
>     Subject Day Correct1 Correct2 Correct3  Percent1 Percent2 Percent3
>                     1	1	1	0	1	50	25	70
>                     2	1	1	0	0	75	30	80
>                     3	1	0	1	1	70	45	90
>                     4	1	0	1	0	80	50	100
>                     5	1	1	1	1	90	60	100
>                     1	2	0	1	0	30	75	90
>                     2	2	0	0	1	45	70	80
>                     3	2	1	1	0	50	30	90
>                     4	2	1	0	0	60	45	100
>                     5	2	1	1	1	80	45	90
>                                      ')
>

   df %>%
      tidyr::gather(Correct, CorrectValue, starts_with(""Correct"")) %>%
      tidyr::gather(Percent, PercentValue, starts_with(""Percent"") %>%
      dplyr::mutate(Correct = as.numeric(sub(pattern = ""[C][a-z]+"", replacement = """", Correct)), Percent = as.numeric(sub(pattern = ""[P][a-z]+"", replacement = """", Percent)))

You will have to install.packages(""tidyverse"") first, though. Sorry about formatting, I'm on my phone.",1535658032.0
sohaibhasan1,https://www.r-bloggers.com/melt,1535641929.0
Hoelk,"Going from SAS to R is going to be a big change. I never used SAS, but I once had a one week course on it, and it was ...strange... to me as an R user.  I cannot help you with your analysis but I can give you some pointers. 

>I just simply can't type all this every single time I run one of my dozens of ANOVAs..

Honestly, its really not a lot of typing, but you probably should not type nonetheless. If you just want less typing and have to run several anovas with the same settings, you can define a function as a ""shortcut"" for yourself.
```
hangman86anova <- function(formula, data){
  Anova(lm(formula, data=data, contrasts=list(topic=contr.sum, sys=contr.sum)), type=3)) 
}

result <- hangman86anova(blah~blubb, mydata)
```

But you shouldn't type it dozens of times, you should either write a loop or look into `lapply()`. It would look something like this (assuming your formula always stays the same, but you can also loop/lapply over a list of formulas)

```
inputs  <- list(data1, data2, data3, data4, ...)
results <- lapply(inputs, function(.x) hangman86anova(formula = time ~ topic * sys, data = .x))
```
This calls your function once for every element of `input` (your data sets) and uses them as the first argument for the function you supply. The syntax above might look a bit strange for you because with the anonymous function we are defining, but if you google for a lapply tutorial and try it out a few times, you should be able to figure out whats happening.

(I did not post an example for a simple loop, if you are not familiar with for loops in R, look into that first)

",1535651011.0
Darwinmate,"> I just simply can't type all this every single time I run one of my dozens of ANOVAs..

That is honestly not a lot of typing... 

Depending on how you've setup your objects/dataframes and what you want to test, you can automate most of this process. Without knowing what your setup is, I can't really give you advice. ",1535636100.0
SemanticTriangle,">I just simply can't type all this every single time I run one of my dozens of ANOVAs..

Then write a function, and call the function every time.

If you're completely new to R, you're trying to do too much, too quickly. I suggest going over DataCamp's relevant lessons. It's $30 for a month and you can easily learn everything that you need to know inside of a week or two. Money well spent.",1535636190.0
Babahoyo,"Are you saying you want to be able to do 

```
foo <- function(y, x) {
lm(y ~ x, data = df)
}

x = ""age""
y = ""income""
foo(y, x)
```

IIRC this abstraction is tough because of the way R parses formulas. It's been a real pain point for me in R and if you solve this I would appreciate the solution. ",1535645195.0
fairmantium,"What I believe you're looking for and will help you the most is to create a Shiny application where you can load in your data, have it automatically go through your chosen ANOVA analyses and then output interactive graphs/tables.  I've set this up where I work where people were previously typing things into excel sheets by hand from a database and then loading all of that into JMP to do their analysis.  Now it just loads into Shiny and outputs everything they need.

[https://shiny.rstudio.com/](https://shiny.rstudio.com/)

Shiny  Dashboard makes it even prettier:

[https://rstudio.github.io/shinydashboard/](https://rstudio.github.io/shinydashboard/)",1535652194.0
brbecker,"Just use ifelse since that is vectorised.

Use apply for functions that aren't vectorised or when you map over a list of vectors.",1535614528.0
flyos,"What about:

    narows<-which(is.na(temp_sub$V2))
    temp_sub[narows,""V2""]<-interp_pt(temp_sub[narows,""lon""],temp_sub[narows,""lat""],
       temp_full,lons2,lats2)

Or doesn't this function allow vectorisation?",1535614507.0
NinjasInTheWind,"Can you give us a small sample of your data or something like your data? You could select a few rows, then use dput so we can work with something similar to what you’re seeing.",1535627108.0
jiminie,"purrr::map* functions FTW.  https://jennybc.github.io/purrr-tutorial/ helped me a lot.  Her webinar, pretty sure it's on rstudio site, helped me a ton!",1535684179.0
Bruce-M,Did you try sink()?,1535580589.0
2strokes4lyfe,"If you're trying to pipe a dataframe into a text file then you can add this to the end of your script:

    library(magrittr)
    df %>%
        write.csv(file = ""~/foo.txt"", row.names = FALSE)

The `%>%` from the `magrittr` package is a pipe-operator much like the `|` in unix. ",1535603411.0
brbecker,Use the sink() function to initiate console output to a file instead and when done call sink () again.,1535614639.0
flyos,"Is this what you look for?
https://stat.ethz.ch/R-manual/R-devel/library/utils/html/capture.output.html

FYI (and without any judgement), it took just a few sec to find out using this website:
https://rseek.org/
You should give it a go!",1535614204.0
2strokes4lyfe,"I'm not seeing an issue with the above code, but it looks like you introduced some `NA`s into your seeds dataframe. When you call `summary(seeds)` do any of your vectors have `NA` values?

Is the seeds data included in an R package? I'd be willing to play around with this if I had the data.

edit: spelling",1535603017.0
devaaa23,"I want to flag anything past deadline1. However, you'll notice that anything on the same date (but different time) as the deadline also gets pulled up. I am currently using a ifelse function to do this. 

Full disclosure: This is for an assignment. Please just guide me in the right direction. ",1535572809.0
KoolAidMeansCluster,"#Easy Solution:  
#I'm assuming you want to flag anything past January 3rd, 2017  
deadline1 = deadline1 %>% as.Date(origin=""1970-01-01"")  

grades %>%  
mutate(submit_time = submit_time %>% as.Date(origin=""1970-01-01"")) %>%   
mutate(FLAG = ifelse(submit_time > deadline1,""LATE"",""ON TIME"")) %>% #make the flag  
group_by(FLAG) %>%  
tally()  ",1536086915.0
TyrannosaurusBrexit,I search for python jobs. Most jobs that use R have something like “experience with python or R preferred” or similar.,1535552746.0
ciarogeile,"R is a great language, but a terrible title, isn't it?",1535554393.0
pettyferrari,"I've found that on Indeed, you can put R in brackets, which will work. So I'll search for something like this: data [R]",1535556887.0
s3x2,"R jobs are almost a subset of all jobs mentioning ""data"". You can also use a specialized service, like Stack Overflow, which includes specific tags for that.",1535553063.0
brews,"""rstats""",1535555894.0
another30yovirgin,Seriously! Why did they do that to us?? Could someone please create R <- R + 1 so we can put that on our resumes?,1535555791.0
Mooks79,"Given that the letter “r”, when it’s part of a word, does not have a space on both sides, but probably will when used in a job description, why not force your search to look for “ r “ - literally using quotes?",1535574263.0
huessy,"Try ""R analyst"", ""R Programmer"", ""R language"", etc.. there is also a site called something like jobs for r users that posts mainly part time freelance stuff but sometimes has real jobs.


Spoiler alert, people want Python but will settle for R... If you're willing to learn Python.",1535578492.0
seeellayewhy,"Ive found the same problem. One workaround I've found is to include (among other relevant terms) the phrase ""statistical software"" (or somethign similar, like ""statistical programming language""). 

I got to the point where I was seeing a lot of irrelevant results and upon paying attention to what made the relevant ones stand out, I came across that phrase. Often it'll say ""experience with a statistical software like R, Python, SPSS, etc."". ",1535570985.0
imse82,Search for keywords like data science and analytical in addition to the letter R,1535557142.0
Jeroniimo,"Construct some related keywords; R statistics, R programming, rstats as someone's already mentioned, R Python, R SQL, R analyst and probably many more related to specific domains or jobs",1535564443.0
shoretel230,Statistical software...  rstats,1535580574.0
patriotto,"can you search for a ""crantastic job""",1535596436.0
Eleventhousand,just search for data science jobs.,1535596566.0
,"I haven’t worked with that package, but have done lots of mediation analyses with other statistical software, but I’m assuming that the package you’re using provides some estimated coefficients that will help answer question 1. Standardized coefficients will help you to answer to your question. Also, double check that everything is coded correctly in your data. I’ve wrestled with lots of weird results... only to realize I forgot to reverse code a survey item. But if what you think is happening is actually the case then it’s possible that the two variables are covariate outcomes of something and there isn’t a mediating relationship between the two. 

For question 2, again it’s hard to answer without the output. Are any of the new relationships significant? Part of the problem with throwing more variables into an analysis is that what you’re finding simply tends to happen anyway, even if the model isn’t actually any better. ",1535543252.0
KingDuderhino,"Actually, just using plot(seeds) should do the trick.",1535488920.0
s3x2,"You don't need to learn about big databases or machine learning for the dataset you've described.

After any cleaning you have to do, transpose your dataset to get one sample per row and [follow this tutorial](https://www.r-bloggers.com/partial-least-squares-in-r/) to obtain a cross-validated estimate of the number of components you need for PLS.",1535475608.0
haechunlee,"I love this video by Hadley Wickham about building a dataframe with a model for each row.  he even addresses your thoughts on the apply function. 

https://youtu.be/rz3_FDVt9eg

good luck!",1535450507.0
Schallabeer,"Here is a link to a video: [https://www.rstudio.com/resources/webinars/working-with-big-data-in-r/](https://www.rstudio.com/resources/webinars/working-with-big-data-in-r/). 

I would definitely recommend looking into the dplyr package that is very useful for data wrangling. 

R for data science is also a very handy book: [http://r4ds.had.co.nz/](http://r4ds.had.co.nz/). 

&#x200B;

Good luck with your masters!",1535458722.0
efrique,"     results <- predict(sis.model, newdata=samples, interval=""prediction"")

where was ""samples"" defined? ",1535417989.0
dhvalden,"I think that you should take the ""supervised learning"" approach in your code, to make it simpler to the reader. You have two well defined proceses: you first fit a model (or train a model) and save their fitted parameters in a new object. Then you have the prediction stage, where you predict an outcome based in that model, using predict(). I would Keep those processes separated, just for clarity. Also, I'm not sure why do you save the summary of your models in your function, since you don't return them. Finally, you have a sie.model and sis.model, but you only use the sis.model for the prediction, why? Best!!!",1535427182.0
RememberToBackupData,"If you turn the code you’ve cobbled together into a package, then you and everyone else will have this available to them any time ;)",1535416708.0
dulldata,Please clarify. Is this like an alternate for any knowledge base CMS or SharePoint knowledge base site ?,1535416329.0
Darwinmate,Side bar links dont work in chrome...,1535457395.0
AnonIMoust,"The help function provides citations to inform you where the ideas behind these calculations come from. In this case: Nakagawa, S., Johnson, P. C. D., & Schielzeth, H. (2017). The coefficient of determination *R*2 and intra-class correlation coefficient from generalized linear mixed-effects models revisited and expanded. *Journal of the Royal Society Interface*, *14*(134), 20170213. r/http://doi.org/10.1098/rsif.2017.0213. This provides a detailed overview of the different methods implemented in the MuMIn package.",1535430802.0
TS_Sama,This looks like it could be really fun to play around with. ,1535399106.0
Bjornwahlroos,"You can write 
    dplyr::mutate 
to show which package the function is called from.",1535392350.0
biledemon85,"Data analysis with R in a project of any reasonable size creates a tension when you're a few days / weeks into it and the few scripts you cobbled together at the start to prototype a solution are feeling a bit clunky to use (managing dependencies, environments, passing way too many parameters around, etc). Some teams will simply move to python/julia/whatever at this stage to make handling these things easier, before the R scripts become too large to justify converting wholesale to another language.

The alternative is to seriously consider the structure of your solution, shift the development into an R package, and use the appropriate tools provided in R. As the author mentions, NAMESPACE and the S3 class system are pretty damn powerful even if they make ""serious"" programmers wince a bit when looking at them. And of course when you make this shift (which you should probably have been doing from the start) lo and behold you find the `@importFrom` tag in `roxygen2`. The tools for serious development are (mostly) all there in R, it's just when you first come across the language from a data analytics perspective, it really doesn't seem that way. ",1535400708.0
,[deleted],1535396027.0
Cosi1125,[Shameless plug...](https://cosi1.github.io/texts/008-python-flavored-imports/) ;-),1535468482.0
hdgdtegdb,"The NeuralNet package is old.  The package hasn't been updated in 2 years, and machine learning is advancing at speed. 

Couple of good suggestions from /u/[millsGT49](https://www.reddit.com/user/millsGT49) and /u/[yordanivanov92](https://www.reddit.com/user/yordanivanov92)

To elaborate on these for you.  H2O is a simple-to-use library, aimed at quickly getting results from data.  Definitely aimed at 'business users' or people who don't want to get too much into the detail.

&#x200B;

MXNet is one of the top NN packages:

[https://mxnet.incubator.apache.org/api/r/index.html](https://mxnet.incubator.apache.org/api/r/index.html)

&#x200B;

I'll add my own recommendation: The guys at RStudio have made it possible to use Tensorflow through R.  An insane wealth of functionality and loads of tutorials and documentation on the web.  I'd start by learning how to use the 'Keras' wrapper library, which provides a simple way to interact with Tensorflow to get results more quickly.

[https://tensorflow.rstudio.com/keras/](https://tensorflow.rstudio.com/keras/)

&#x200B;",1535403006.0
millsGT49,"try the `h2o` package, they have a number of speedy machine learning algorithms including basic neural nets. ",1535384609.0
yordanivanov92,you can try mxnet,1535384831.0
questionquality,Or do the base computations in a separate .R script and write a Makefile for the whole project.,1535381416.0
KingDuderhino,Write a function that does the output...,1535378664.0
Inner_Dragonfruit,Why not knit with parameters?,1535406149.0
flyos,"Have a look at the package `cowplot` and its function `plot_grid`. It does exactly this!

EDIT: Damn, I'm blind, I didn't notice the last part of the post. What do you mean ""pass all objectives""?

EDIT2: Does this work for you?

    do.call(""plot_grid"", allfigs)
",1535377105.0
enilkcals,"Give [patchwork](https://github.com/thomasp85/patchwork) a whirl, very neat and concise compared to `cowplots::plot_grid`.",1535388375.0
Crypt0Nihilist,"It might not be R or R Studio. 

As has been suggested, run the script on the command line, see if it still happens to rule RStudio in or out.

My guess it that it's more likely that you're soaking up your memory and processing power which is causing another process to screw up or timeout which borks the network.",1535369530.0
geocompR,"What happens when you disconnect from the network (turn off WiFi, unplug Ethernet)? What if you run it in R’s default GUI or command line (NOT R Studio)?

Edit: this question is probably better suited to /r/RStudio",1535362979.0
pmgurman,"I think your correct, the following example should help

[http://gallery.rcpp.org/articles/using-rmath-functions/](http://gallery.rcpp.org/articles/using-rmath-functions/)

Another option might be to try the boost library if speed is important, which it probably is if you are thinking about RcppParallel

[https://www.boost.org/doc/libs/1\_45\_0/libs/math/doc/sf\_and\_dist/html/math\_toolkit/special/bessel/bessel\_over.html](https://www.boost.org/doc/libs/1_45_0/libs/math/doc/sf_and_dist/html/math_toolkit/special/bessel/bessel_over.html)

&#x200B;

&#x200B;

&#x200B;",1535362545.0
DataDouche,"There is a package called [ngram](https://cran.r-project.org/web/packages/ngram/ngram.pdf) that is another good package for Markov Chain text generation. I'm not sure if it has the functionality you're looking for exactly, but it might also be worth examining.",1535330419.0
abecker93,Base R isn't really built for data sets this large. There are several packages designed to make working with such data sets easier but the syntax can get complicated.[ Here is a pdf on working with such data sets in R.](http://www.columbia.edu/~sjm2186/EPIC_R/EPIC_R_BigData.pdf) Hope this helps!,1535294855.0
grasshoppermouse,"With only 6 variables and 5.4 million rows, you could probably get pretty good estimates of your coefficients with a small subsample:

    subsample <- sample(1:nrow(df_original), 54000) # 1%
    df_new <- df_original[subsample,]

EDIT: as /u/KingDuderhino notes, he should subsample on individuals. Maybe the code would look something like:

    ids <- unique(df_original$id)
    sub_ids <- sample(ids, 9000) # 1%
    subsample <- df_original$id %in% sub_ids
    df_new <- df_original[subsample,]",1535297115.0
amstell,Check out the 'lfe' package. Its made for larger fixed effects models because it demeans the data first then estimates the model. ,1535296243.0
KingDuderhino,"The problem is that objects returned by statistical estimators in R have often, in addition to the estimated coefficients and some additional information, the original data and estimated residuals.Therefore, if you have large data these objects can become very large. My suggestion would be:

1. Estimate one model, do statistical tests

2. Extract relevant estimations and delete the rest from memory (with rm)

3. Estimate next model and do statistical test

4. Delete unneeded objects

5. And so on

You won't be able to use stargazer with this method but other packages like kable or xtable will work.",1535306339.0
MrLegilimens,It can't have 6 observations but 5.4 million rows. Each row is an observation.,1535294010.0
huessy,"I'm not sure how you're reading in your data, but it may (big ""may"") help to use read_csv() from the readr package (part of tidyverse).

It helps minimize ram usage when reading datasets and could help processing time of the lms.

There are some theoretical flaws in my logic but it never hurts to try.",1535327716.0
iconoclaus,"first, plot the two variables you are interested in, with dependent variable on y axis. then you can ask questions :)",1535284450.0
Nschnock,"Correlation does not implies cauation. Even though you find a correlation, that does not mean its directly linked.

I would try a regression line (firstly a stupid trend line in excel).

(It reminds me a data science course on edx.org or coursera, where they took a base ball data set and calculated the number of home run needed to be in the play off (like in the movie moneyball) I can try to find the name, if interested.)",1535285753.0
SemanticTriangle,">Most results of searching for how to do this include things that seem unnecessarily complicated.

With respect, this is because you have not bothered to learn how base plot and/or ggplot2 work.

As a ggplot user, for example, I can see from your sentence

>I would like to plot sis and sie as the two x variables

that what you *actually* want is markers on the x axis at those locations. The x axis appears to be just be a normal axis.

Because this is what you want, your data frame may or may not be tidy. My approach would be to first join those two sets of sie and sis points into a single vector `sie_sis_vec' for later, then run something like

    gathered_df <- dplyr::gather(counting.df, key = ""key"", value = ""value"", -efficiency)

This will convert your data frame into one that looks like

    efficiency                key                        value
    <efficiency_value>  <sie_or_sis_flag>  <sie_or_sis_value>

Then I would run a

    gathered_df %>%
      ggplot(aes(x = value, y = efficiency)) +
      geom_point(color = key) +
      geom_smooth(<arguments) +
      scale_x_continuous(limits = c(<min>,<max>),breaks=sie_sis_vec)


I'm not that familiar off the top of my head with the geom_smooth arguments for the quadratic equations, but it's easy enough to look them up ?geom_smooth.

This might seem 'needlessly complicated', but each line of code is trying to instruct R to give you one of the returns you want: the points categorised by sie/sis family, the points laid out on the x axis, the two familes separated by color, and a smooth fit function for each.
      ",1535250941.0
Pneumatocyst,"If I'm understanding you right, you want to:

* Plot two sets of data on a single graph
* add text to the graph

Alternatively, are you looking to plot a 'trend' line through your points? And/or are you looking for the equation of that trend line?",1535254566.0
mkuehn10,Maybe http://www.r-tutor.com/elementary-statistics/goodness-fit/chi-squared-test-independence,1535207993.0
pikacool,"You could run a panel regression :

y = a + b*x + c*z

Y is proportion of people who use public transit in a tract, x Is proportion of non-white residents, z are other controls.

If b is significant you could infer that it is an important determinant. You should actually estimate changes of y and x rather than the stock.",1535209800.0
s3x2,Why exactly do you need a test? You have census data. Whatever difference you see is real and exact.,1535210133.0
efrique,"I don't know what you mean by ""test ... against"". What's the question of interest that you're trying to address?",1535249432.0
engti," 

After some experimenting, it seems to work as expected when I set the Python path manually:

    ## load library 
    Sys.setenv(RETICULATE_PYTHON = ""C:/Users/uname/.julia/v0.6/Conda/deps/usr/python.exe"") 
    library(reticulate) 

I am not too sure why I have multiple Python paths, when I only installed Anaconda once, and this is a laptop has been only imaged recently. And why it exists within the Julia path, I don't know.",1535302399.0
luckylukey,"Tomorrow Night 80‘s all the way! 
Still liking the general „dark“ theme you are going for + the contrasts seem nice!",1535113170.0
030Economist,This would actually make me consider using a dark theme. I hate the dark blue windows. ,1535102276.0
randy3k,"Just submitted a pull request to RStudio

[https://github.com/rstudio/rstudio/pull/3365](https://github.com/rstudio/rstudio/pull/3365)",1535094047.0
ramgorur,This is a proper dark theme. ,1535115027.0
Darwinmate,"This looks awesome and I hope it gets into the next release.

&#x200B;

Can you answer a question for me, how do I modify the font type? I'm a bit dyslexic and prefer a large bold font. I can change the size of the font but Rstudio doesn't have font face option.",1535105098.0
youcanbroom,Dark theme is the way to go! I sleep beter ever sence i set my cpu to dark,1535131911.0
Boot329,Silly question.. how do you apply this theme to Rstudio? Loving the theme.,1535654910.0
ReimannOne,"Try using `seq.Date()` to get the dates you need, and then run a loop using those dates.


    dates <- seq.Date(from = as.Date('2018/07/01'), to = Sys.Date(), by = 1)
    for(i in dates){
      #your code here, replace hardcoded dates with i.
      } ",1535058864.0
Reggaepocalypse,Definitely going to play with this.,1535065743.0
H4CKTHEPL4NET,Huh. This is really cool.,1535064507.0
hyperfocus_,Haha that's really quite neat. ,1535070286.0
sydn,"We really are living in the future. While I can't think of anything I ~need~ it for, I'm totally going to play with it",1535072864.0
030Economist,"RemindMe! 1 month ""That was easy!""",1535079819.0
oHenry12,I can't wait to play with this later. Thanks for sharing! ,1535126368.0
LogicalRisk,"This sounds like you have created your football pitch in base r. 

Assuming that is true, the command that you want is points(). See ?points in R help

It will look something like the following

    plot(THE CODE FOR YOUR FOOTBALL PITCH HERE)
    points(X_COORDINATES OF SHOT, Y_COORDINATES OF SHOTS)

where X,Y are vectors of x and y shot location data. ",1535056711.0
sydn,"Thanks, this was an easy to follow case study, and I picked up some new r knowledge",1535071951.0
Thaufas,"Personally, I think a package made just for plotting Six Sigma/Measurement Systems Analysis results is overkill. Regarding using ggplot2 vs R's basic plotting functions, for a long time, I resisted using ggplot2 because I had become very proficient with the base plotting functions, and a lot of early R packages had a terrible maintenance record.

However, after I could clearly see that ggplot2 was here to stay, I started using it and have been very pleased with it. Even now, I'll still use the base plot functions for very simple graphs, but I always use ggplot2 for any publication quality charts or in-depth data analysis.

Regardless of which one you choose, creating a plot panel for Six Sigma/MSA is fairly straightforward. If you want to create multiple plots on a single page with the base functions, you have several options. The simplest is the [layout() function](https://www.rdocumentation.org/packages/graphics/versions/3.5.1/topics/layout), which is fairly flexible. However, for more complex layouts, use the [split.screen() function](https://stat.ethz.ch/R-manual/R-devel/library/graphics/html/screen.html).

If you want to create multiple plots on a single panel using ggplot2, which I do recommend, there is an [excellent vignette](https://cran.r-project.org/web/packages/egg/vignettes/Ecosystem.html) that gives a great overview of the many ways to accomplish your goal.

Please don't hesitate to let me know if you want help with specific examples.",1534944972.0
DeuceWallaces,"It's not really necessary. Take a day or so and learn to make them with ggplot. Those are very simple graphics and you'll want to customize ticks, ranges, labels, colors, etc. ",1534948480.0
revgizmo,"No, but I’m looking forward to hearing from others",1534913237.0
freedomakkupati,"Create a new variable in which you include the year and month of the timestamp, bind it to the dataset and then table the new variable you created",1534887741.0
ReimannOne,"Assuming your data is named `mydf` and the date column is `date`:

    library(dplyr)
    library(lubridate)
    mydf %>% group_by(year = year(date), month = month(date)) %>%
      count() %>% arrange(year, month)

Should be about right.  The zoo (or maybe xts) package has yearmon() or as.yearmon() function that can be helpful when the month and year are all that you're looking at from a full date.",1534941005.0
tacothecat,Afaik conditional matches are still a work in progress in dplyr.   You could look into the [fuzzyjoin](https://github.com/dgrtwo/fuzzyjoin/) package ,1534890542.0
chandaliergalaxy,How should they be joined?,1534890791.0
bottomfeeder_,"It's pretty straightforward. First, make a data frame with columns ""Country"" and ""Year"" so that the rows cover all the combinations. There are lots of ways to do that, such as this: https://stat.ethz.ch/pipermail/r-help/2011-July/282834.html 

Then join the new data frame with your existing one so now each row for a given country that same start and end years listed. 

Mutate using case_when() which takes logical arguments and assign your outputs as desired (1 or 0) based on the arguments. 

Finally select the columns you want and finish with unique() ",1534900181.0
champdave,"Convert the second dataset to yearly observations, then join them. This can be done with dplyr:

dataset2\_yearly <- dataset2 %>%

  rowwise() %>%

  do(data.frame(Country:=.$Country:, Date:=seq(.$Start:, .$End:))",1534917655.0
KingDuderhino,"I don't know much about RStudio Server Pro, but something about Microsoft R Server. Your existing R code should run on Microsoft R Server.

Advantages:

* Well integrated with other with other MS products, e.g. their SQL Server but it also works with other databases. Also easily scalable with Azure

* Can also be deployed on a Hadoop/Spark system

* Comes with a bunch of functions designed to work with large data

Downside

* Biggest downside in my opinion is a vendor lock-in. The RevoScaleR package is not available on CRAN. If your work relies a lot on the function from that package than switching to another system is difficult.

",1534882590.0
daanzel,"No experience with Ms ML server, but RStudio server is pretty great. You probably do want pro, since the free version doesn't allow multiple logins (if I'm not mistaken). 

Other option would be to keep developing locally and use git to deploy code to a server running plain R. Much cheaper en works just fine. We have deployed such a setup with 3 r-servers and run DKRON to distribute load and make sure jobs run, even if one server is down. Such a cluster can be scaled-out with little effort and cost.

Internally, our company uses Databricks. If cloud is an option, check it out. It's available on both azure and aws and is a full SaaS solution, so no hassle maintaining servers. You develop and run code in a notebook and attach as many resources to it as you want, paying only for compute time. As a bonus, cloud offers easy to use storage for your data. 

Talking about storage, think about that as well when choosing a setup. Letting everyone dump csv's and other datadumps on a server gets messy real fast.. 

Which solution is best depends on more than just ""want more compute power"". Make a list of all requirements and compare what's best; you won't be the first to invest in some software only to find out it doesn't do everything you want.

So I'm afraid I can't really answer your question, but hope this helps :) If you have any questions feel free to ask. ",1534887134.0
InflationSquare,You could use the ordinary open-source RStudio server and host it yourselves if that's an option? ,1534885120.0
tacothecat,I'm sure if you reached out to them they'd both be glad to give a sales pitch,1534889373.0
hawairob,"So... maybe something like this...?

    data$columname <- data$columname[data$columname != 0] #remove values with 0's
    data$columname[data$columname < 0] <- 0 # anything below 0 turns to 0
    data$columname[data$columname > 0] <- 1 # anything above 0 turns to 1

Hope it helps.",1534880912.0
GildedFuchs,"A data.table approach:

    library(data.table)
    setDT(data) #convert to data.table
    data[colname != 0, colname := ifelse(colname < 0, 0, 1)]

I think this is a more idiomatic way to go about this. This selects everything that isn't 0 and recodes it like you need. The advantage is that you aren't making a bunch of memory intensive re-assignments (don't know how big your data is), and the ':=' function inside of a data.table updates by reference (doesn't make a copy in memory). 

Even if you don't want to use data.table, use the ifelse() function. It's designed for this kind of task; you can also nest them:

    data$colname <- ifelse(data$colname == 0, 0,   ifelse(data$colname  < 0, 0, 1))",1534904778.0
030Economist,"Here you go. 

http://rstudio-pubs-static.s3.amazonaws.com/6975_c4943349b6174f448104a5513fed59a9.html",1534861758.0
midianite_rambler,"At this point going straight to any particular package or software is premature. My advice is to back up and think through what you are trying to do.  Software can then help you do what you figured out.

In general the right way to handle a problem like this, with an uncertain parameter, is to average the thing you're looking for (time to event or whatever) over the possible values of the parameter, weighted by your degree of belief in each value. This is the Bayesian approach. In this case you can derive a degree of belief from the given mean and s.d. (making some assumption about the form of that distribution along the way).

But I wonder from where the mean and s.d. are coming from, and for what purpose you want to compute probability of non-event over a time range. I'd like to suggest that you could say more about that here.",1534871182.0
efrique,"Get a suitable model clear in your head before you touch a computer.

Take a look at the Poisson process and see if its assumptions fits with what you're supposing. 
",1534892080.0
,"Hey you wanna do a survival analysis. It’s called that because it was developed to model the probability of death over time. But any binary variable will work.

Edit: I’ve only done it in SAS and this was a long time ago so I can’t be much more help. But I bet you can find some good resources if you start looking into survival analysis. ",1534853742.0
ercmrn,"The `melt` function from `data.table` might help with your problem, or alternatively you could use a combination of `tidyr`'s `gather`, `separate`, and `spread` functions.",1534817164.0
s3x2,Beyond telling you what functions might work there's not much more that can be done without a sample of data. Use `dput(head(yourdata))` if you feel like sharing something others can work with.,1534820757.0
BeerSharkBot,"The most common answer (for good reason) is going to be tidyverse related.
You can just filter for the correct restrictions on state and date and then summarize to take the max of the correct variable. Plenty of easy intros to dplyr (part of tidyverse).
Or you can use a subset function etc. Lots of different combinations to solve this one",1534795224.0
BeerSharkBot,Have you tried creating your pattern string with paste or paste0? ,1534796463.0
YepYepYepYepYepUhHuh,"Building on my dplyr solution from your previous thread:

    get_sd <- function(df, word) {
    df %>%
      select(contains(word)) %>%
      gather(key, value) %>%
      summarise(sd=sd(value)) %>%
      mutate(wordSelected=word)
    }

    get_sd(df, ""a"")

In this case the word argument is whatever word or expression you are filtering your columns by, and df is your data frame.",1534801261.0
YepYepYepYepYepUhHuh,"I don't think R likes having identical column names.It doesn't seem to throw up an error, but it doesn't know how to distinguish one column from another in operations. 

A better way might be to use something like dplyrs matches() to help the select() call.

    a <- rnorm(100)
    b <- rnorm(100)
    c <- rnorm(100)
    d <- rnorm(100)
    e <- rnorm(100)
    df <- data.frame(a,b,c, d, e) # use df instead of data

    colnames(df) <- c(""a1"", ""b1"", ""a2"", ""b2"", ""c"")
    
    df %>%
      select(contains('a')) %>%
      gather(a, a_value) %>%
      bind_cols(gather(select(df, contains('b')), b, b_value))


edit: just reread the question. In order to gather multiple columns you can use the gather() function also in dplyr. This method will preserve the key for the original columns the data came from,but those can be excluded later if desired.",1534787858.0
YepYepYepYepYepUhHuh,"Shiny has a lot of interactive plotting options. As far as getting all three years on the same plot, it might be easiest just to assign them each a day (1-365) and then keep the year as a factor. You can always assign custom labels to your plot later.

You can make use of lubridate's day() function to get the day value.",1534826702.0
D4rkyFirefly,"Good job there! And thx for the info, seems you invested a lot of time into it!",1534805091.0
HimmelLove,I appreciate you sharing these materials. I’m beginning to use R in the classroom and it is extremely helpful seeing how you have utilized it!,1534825370.0
VincentStaples,data.table(),1534769344.0
dhvalden,As.data.frame.matrix(),1535300063.0
KoolAidMeansCluster,"You’re looking to transform long data into wide data.  
  
I think you’re looking at the spread function (dplyr or tidyr).
  
Not at my computer to try but, it looks something like:
DF %>%  
spread(ArrivalDate)
  
If it gets spread as numeric, then do:
DF %>%
spread(ArrivalDate) %>%  
mutate_if(is.numeric, as.Date)  

Good luck!




",1534762994.0
guepier,"This works:

    library(dplyr)
    library(tidyr)
    
    table = read.table(…)
    result = table %>%
      group_by(Index) %>%
      mutate(Col = 1 : n()) %>%
      spread(Col, ArrivalDate)

It only adds three columns, not four, though. Not sure why you’d want a fourth column that’s all NAs but it’s easy enough to add.",1534773355.0
engti,"the following worked for me:

    ## load library
    library(reshape2)
    library(dplyr)
    ## get data and transform - data.csv is where i saved your data
    tmp <- read.csv(""test.csv"") %>%
    ## group by index so we can find out how many entries there are
    group_by(Index) %>%
    ## for each group create a numeric index
    mutate(
    cnt = 1:n()
    ) %>%
    ## spread the numeric index to be the column header
    dcast(Index ~ cnt,value.var = ""Arrival.Date"")",1534764820.0
StephTheChef,"Something like this might work
    
    col1 = c('A','A','B','B','B')
    
    col2 = c('2013-02-01','2013-03-01', '2014-02-01', '2015-04-01', '2015-02-01')
    
    col3 = seq(1,5)
    
    df = data.table::as.data.table(cbind(col1,col2,col3))
    
    df = data.table::dcast.data.table(df, col1~col3, value.var = 'col2')
    
    as.data.frame(t(apply(df,1, function(x) { return(c(x[!is.na(x)],x[is.na(x)]) )} )))    
    
    ",1534764687.0
Goldgeg,"You can also use base R implementations such as aggregate() or by() or for simpler things tapply().

Just wanted to let you know that these exist too, although functions from dplyr package will be easier to handle.

And sorry for no example, am on mobile phone.",1534777435.0
sohaibhasan1,This question is nonsensical without more information on the kind of data that will be in the table. You're not asking for a script or package. You're asking for a magic wand.,1534662256.0
MrLegilimens,What kind of data vis could you even possibly do with just a 2x2 beyond a bar chart with no standard errors because you only did a 2x2 of what I’d assume are means.,1534683147.0
sydn,"This is the first result on search engines for ""r descriptive stats for each column in data frame""

https://stackoverflow.com/questions/20997380/creating-a-summary-statistical-table-from-a-data-frame

The second answer is probably sufficient",1534723673.0
efrique,a 2x2 table is exactly four numbers. If you say more than 4 things about it you're repeating yourself! ,1534810964.0
SemanticTriangle,There was a recent post here linking to a post on bootstrap in mixed models [which may be useful](https://datascienceplus.com/introduction-to-bootstrap-with-applications-to-mixed-effect-models/) to you.,1534657988.0
webbed_feets,"I don't know what deltas are. I don't know what cLDA is but you probably don't need it.

You need to use a mixed model, but not for the reason you think. If you don't use a mixed model for repeated measures, you're essentially doing a nonsense analysis. Mixed models are the statistical way of saying that each person is correlated to themselves, not independent observations.

A mixed model on its own will not test for a change in a continuous variable at two time points. One way to test this is to add an indicator variable for time 0 and 6, and see if these indicators are significant.",1534815593.0
throughthekeyhole,"Sure. You're sort of right. private/public/active elements cannot have the same names. So, when I'm creating active bindings, I can't just use the same name as the private element. I've actually seen it as private$..age, but the idea is still the same. Now when you see 2 dots further in your code, you know you're accessing the private element, but you can also tell that it ""goes with"" your active binding age.

You could call it something else, but the dots are an easy convention.

You could just as easily use private$pvt\_age, and that would work just fine. Not sure how much you wanted to know beyond the notation, but here's a quick block of code to illustrate. Assuming you've installed the assertive package, you can copy-paste this and play with it.

`library(R6)`

`television_creator <- R6Class(`

`""Television"",`

`private = list(`

`..size_in_inches = 50,`

`..body_color = ""black""`

`),`

`active = list(`

`size_in_inches = function(value) {`

`if(missing(value)) {`

`private$..size_in_inches`

`} else {`

`assertive::assert_is_a_number(value)`

`assertive::assert_all_are_in_open_range(value, 15, 100)`

`private$..size_in_inches <- value`

`}`

`},`

`body_color = function(value) {`

`if(missing(value)) {`

`private$..body_color`

`} else {`

`assertive::assert_is_a_string(value)`

`private$..body_color <- value`

`}`

`}`

`)`

`)`

`newtv <- television_creator$new() #creates a Television object with black body_color and 50 size_in_inches`

`newtv #Look at the private elements - they're the default of 50/black`

`newtv$body_color <- 123 #Will error because body_color should be a string`

`newtv$body_color <- ""yellow"" #Works`

`newtv$size_in_inches <- 120 #Nope, outside of range`

`newtv$size_in_inches <- 55 #Let's make it a bit bigger. Works! Shame it's yellow`

`newtv #And now the private elements have changes`

Edit: Lost all my indents when I pasted. Made a half-assed effort to make it a bit more readable.

Edit2: Nevermind, apparently I don't know how to indent code in comments!",1534694460.0
chrisbeeley,"My own book has received mixed reviews:

Web Application Development with R Using Shiny - Second Edition: Integrate the power of R with the simplicity of Shiny to deliver cutting-edge analytics over the Web https://www.amazon.co.uk/dp/1782174346/ref=cm_sw_r_cp_apa_uIkEBbAKX5Z2Y

I'm working on a third edition with better and more advanced examples, should be out by the end of the year. Some people found the second edition helpful, others as you can see did not, so, maybe wait for the third edition and draw your own conclusions.",1534634556.0
revgizmo,https://shiny.rstudio.com/tutorial/,1534634549.0
moravak,"Hi, I'm not sure if there is something that comprehensive, I liked [Building Shiny Apps](https://www.amazon.es/Building-Shiny-Apps-Development-English-ebook/dp/B077X15NT6)",1534664649.0
Ryo-N7,"I don't think there are many (if any) books on Shiny out but there have been tons of workshops by Joe Cheng, Dean Attali, etc. that are really useful, especially since they put all their material online!

https://github.com/jcheng5/shiny-training-rstudioconf-2018

Otherwise, the way to go is just keep reading blog posts and try recreating/modifying them yourself!",1534648124.0
grasshoppermouse,"Zelig wraps lots of r functions, and it can automatically do the pooling:

http://docs.zeligproject.org/

http://docs.zeligproject.org/articles/using_Zelig_with_Amelia.html",1534821223.0
alphafishing,"It looks like you might have solved it but this [dplyr programming](https://dplyr.tidyverse.org/articles/programming.html) pattern should help:

    myFun <- function(df, ...) {
      vars <- quos(...)
      res <- df %>%
        select(!!! vars)
      return(res)
    }
    
    myFun(mtcars, hp, disp)",1534624193.0
kzbigboss07,"Why not introduce a delimiter into your surrogate key ?

ie: 12000220-1-1-11 vs 12000220-11-1-1",1534620058.0
ArturBotarelli,"I am a beginner, so I am very insecure on my ability to help. But recently I had a similar problema and a used a loop with the assign() function.in order to import all the files. ",1534619922.0
grasshoppermouse,"Check out DT:

https://blog.rstudio.com/2015/06/24/dt-an-r-interface-to-the-datatables-library/",1534620868.0
,[deleted],1534564455.0
AGINSB,What level are you looking for? If you want entry level you would probably have good luck reaching out to local universities. I know DePaul Masters in applied stats students would fit your description pretty well,1534561951.0
jplank1983,You should post this on /r/actuary,1534862998.0
sausagepizzaguy,I am very interested in this. I've sent you a PM,1534621926.0
GildedFuchs,Sent a PM,1534791390.0
Ader_anhilator,Will this role be seeking objective answers or cherry picking data to help someone's narrative?,1534525596.0
MrLegilimens,"This is the basis of all cleaning and managing of data. You would NOT EVER want to touch the excel file. That is 10 types of bad. Instead, look into rbind.fill, rbind, cbind, and manipulating multiple datasets within R itself. 

Also, all regressions will be case-remove if not all information is present. So You'd be looking only at 1982-1999. ",1534515430.0
PA_Irredentist,"I started trying to write out an explanation, but it's pretty contingent on your data frames and what they look like. Generally, you'll want to load the csvs into R and join them. I usually use left_join from the dplyr package for what it sounds like what you're trying to do; left_join will keep all observations in the left dataframe and matching observations in the right dataframe based on a unique key between the two dataframes. ",1534516712.0
,[deleted],1534517134.0
Economist_hat,"You have to join the files together.

1. Read in one xls(x) file for every different format you have.
2. Manually assess the format (standardize column names as necessary, rename variables as necessary, reparse/correct read in errors as necessary
3. Automate the read-in/parse
4. Join (merge() in R) on the common IDs

For a very simply process it might look like this

    cleaner <- function(data) { # Your code to clean raw data here} 

    files_df <- lapply(list.files(), read_excel)
    cleaned_df <- lapply(files_df, cleaner)
    do.call(merge, cleaned_df, <merge parameters>)

Or in a more tidy esque way

    list.files()                     %>%
      read_excel(<params>)            %>%
      cleaner                         %>%
      do.call(what = merge) 


",1534520052.0
orcasha,"Nice writeup on bootstrapping! Just be aware that the percentile method tends to give inaccurate CIs when the resulting bootstrap distribution is skewed. It's recommended to use bias corrected, ""accelerated"" (BCa) bootstrapping instead (see [this article](https://projecteuclid.org/euclid.ss/1032280214), which corrects for this.

Thankfully you can just change the type to ""bca"" and be done! ",1534499100.0
SemanticTriangle,"I'm struggling a little to interpret the function of the code

    new_dat<-data.frame(x=seq(-2,2,length=20))
    mm<-model.matrix(~x,data=new_dat)
    predFun<-function(.) mm%*%fixef(.)
    bb<-bootMer(m,FUN=predFun,nsim=200) #do this 200 times

This is used somehow in the next step to grab the 2.5 and 97.5% limits, but I can't quite follow. What is this code doing?

Edit: ok, it took me some time to work it out. But it's actually pretty straight forward. The summary function applied for the bootstrap is the matrix multiple of (1,x) with (intercept, gradient) from 'fixef' of each bootstrapped data set. So instead of the summary function being the intercept,gradient output of 'fixef', you get a data frame of points. These are the points plotted later, from what I can tell.

These points get ordered at each x later, then the 5th and 195th point taken out later. As expected, the edges of the confidence interval aren't actually straight, even though they're close to straight with this data set.",1534480597.0
Statman12,Am I the only one who [cares](http://adv-r.had.co.nz/Style.html) about [writing](https://google.github.io/styleguide/Rguide.xml#spacing) code to make it easier to read?,1534509897.0
SemanticTriangle,"[Shiny has got you, fam](https://shiny.rstudio.com/tutorial/written-tutorial/lesson1/).",1534424363.0
DeuceWallaces,"I think with 0 coding and R experience it will be a bit work before anything productive comes  out of Shiny for you. Not trying to discourage, just saying it will take some time.",1534431187.0
taskhomely,"Step 1 is to get RStudio. Step 2 is to familiarize yourself with Tidyverse (makes data processing a breeze). 

Shiny isn’t that hard, you just have to get the hang of it. I spend about twice as much time picking colors and working on CSS than actually building the app itself. 

Also, DataCamp has a good course on Shiny but presumes you know the basics of Tidyverse. ",1534433738.0
CJL_LoL,"0 coding experience? I'd try tableau unless your interest is learning to code (in which case welcome to R, it's great). Unsure of limitations on free version though ",1534440724.0
Lareine,"Data Camp!!!

There are a ton of free courses, including [Intro to R](https://www.datacamp.com/courses/free-introduction-to-r) and [Building Web Apps with Shiny](https://www.datacamp.com/courses/building-web-applications-in-r-with-shiny).

If you are willing to pay, there are fabulous paid courses as well.  I highly recommend [Intro to the Tidyverse](https://www.datacamp.com/courses/introduction-to-the-tidyverse) - if you are just starting out with R, you should absolutely learn it the tidyverse way, IMHO.  The first chapter is free, in case you want to check it out before paying.

",1534456026.0
brazzaguy,"There's a coursera class ""building data products"" reviewing it. It also comes along with a book from Brian Caffo named the same way ",1534457819.0
Modmanflex,"There are numerous tutorials out there on building apps in Shiny.  It will take a little getting used to how projects are built in SHiny, but it won't take you long.  Plus there are lots of great examples out there with full code you can look at tweak, etc...",1534612482.0
GoodAboutHood,"Post some example data and a bit of example code - we’ll be able to help you better that way.

My first guess is purrr but it’d be good to see what you’re trying to do",1534423636.0
efrique,"> I've read that R for loops are often inefficient because they make a copy of the entire dataset you're working with in each iteration.

The problem is not the for loop, it's growing an object. If you make the object large enough to hold its final size that loop is not really an issue.
",1534470677.0
Darwinmate,"\> I've read that R for loops are often inefficient because they make a copy of the entire dataset you're working with in each iteration. 

This is not true. See this: [http://predictiveecology.org/2015/09/04/Is-R-fast-enough-04.html](http://predictiveecology.org/2015/09/04/Is-R-fast-enough-04.html)",1534429421.0
jowen7448,"Without knowing exactly what you're trying to do you could potentially create a list of all the combinations of parameters you want to try. The `expand.grid()` function might help if there is regularity to the parameters. Then use something like `lapply` or `purrr::map2`to calculate the vector of outputs. You could then `do.call(rbind,<list of outputs>)` to row bind them all into a data frame. This would stop the iterative growing of the resultant data frame and remove nested looping. ",1534416400.0
Ader_anhilator,"Use data.table if you really want speed and memory efficiency.

    j <- 0
    store <- data.table(par1     = rep(1234.5678, length(par_1) * length(par_2)),
                        par2     = rep(1234.5678, length(par_1) * length(par_2)),
                        results1 = rep(1234.5678, length(par_1) * length(par_2)),
                        results2 = rep(1234.5678, length(par_1) * length(par_2)))
    for (a in seq_along(par_1)) {
      for (b in seq_along(par_2)) {
        j <- j + 1
        set(store, i = j, j = 1, value = a)
        set(store, i = j, j = 2, value = b)
        set(store, i = j, j = 3, value = runif(1))
        set(store, i = j, j = 4, value = rbinom(1,1,a))
    }",1534575865.0
drblobby,You need indentation on html_document.,1534421901.0
NinjasInTheWind,Just to check: are you indenting those header options for code folding properly? I’m pretty sure that matters.,1534415558.0
henkar91,"You need to put your code into tags,

\`\`\`{r}

dummy <- c(""Why"",""Am"",""I"",""So"",""Dumb"") 

dummy

\`\`\`

Have you done that?",1534406762.0
MadPhatFishKiller,"I believe that the C-P method produces consistently conservative intervals.  I'm interpreting conservative as 'cautiously wide '.

[http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=BF0333B82FD5B5A73FD66C40CA464E37?doi=10.1.1.326.1772](http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=BF0333B82FD5B5A73FD66C40CA464E37?doi=10.1.1.326.1772)",1534378337.0
efrique,"> Why is Clopper-Pearson the standard

I wouldn't necessarily say Clopper Pearson was standard in the stats community. To my mind wikipedia's coverage gives a good overview of the main approaches used https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval

It's possible Clopper Pearson is the most widely implemented for a default, but people do all kinds of things. 

>  getting the confidence for binomial distribution?

You're not getting a confidence interval for the *distribution* here, you're getting a CI for the parameter



>  why not use standard errors (e.g. mean +/- 1,96 * (sqrt(pq) / sqrt(n)))

tons of people do, and when npq is large it's perfectly reasonable

But you're dealing with a discrete distribution, for which the normal approximation is just that - an approximation - and on occasion not such a great one. 

Because the distribution is discrete you can't actually achieve the desired coverage -- your actual coverage depends on the true parameter value and has ""jumps"" in it, whatever rule you use.

The various tradeoffs people make lead to different properties. ",1534382206.0
_nt2,"Estimating a binomial proportion is very interesting! 

If you are interested, I strongly recommend this fairly readable and classic review (as of 2001) of the situation: Brown, L. D., Cai, T., & Dasgupta, A. (2001). Interval Estimation for a Binomial Proportion. Statistical Science, 16 (2), 101-133.

It is available free to the world here [PDF](https://repository.upenn.edu/cgi/viewcontent.cgi?article=1440&context=statistics_papers)",1534437525.0
stevejow88,Can’t you also see if the eigenvalues of matrix X are close to 0 to determine if you have multi-collinearity?,1534377799.0
SemanticTriangle,"Running through this working example made me wonder if there exists a straightforward way to feed this collinearity test information into a principle component analysis, as a way to human-guide that process to a more interpretable outcome. Anyone have any suggestions about this?",1534385248.0
smolzie,"Are you familiar with tidyverse? If not I highly recommend this book for learning purposes: [http://r4ds.had.co.nz/](http://r4ds.had.co.nz/)

To answer your question, if all your have the same columns I would combine them as follows (given that the multiple is not a high number, if it is, you can still loop the binding).

    library(tidyverse)
    df2001 = data.frame(matrix(rnorm(20), nrow=10))
    df2002 = data.frame(matrix(rnorm(20), nrow=10))
    df2003 = data.frame(matrix(rnorm(20), nrow=10))
    your_new_df <- dplyr::bind_rows('2001' = df2001, '2002' = df2002, '2003' = df2003, .id = 'Year')",1534355250.0
HoberMallow90,"Create a function that takes a dataframe as an argument and uses dplyr::mutate to add year.  A more concise method would be to incorporate this step into the next step (map) without explicitly adding a function first.

Then use purrr::map to apply that function to each element dataframe in a list (set) of dataframes.

Then, assuming the variables other than year are also the same, use dplyr::bind_rows to combine the dataframes.

Oh you mean years are sequential between dataframes, right. Ok then just iterate over the length of the list (number of dataframes instead of the list) and use the current index to increment years.  So map(seq_length(yourList), function(x){yourList[[x]] %>% mutate(year = initial year + (x-1))}). Then bind_rows the adjusted dataframe list elements.  Double check my code cause I'm typing from my phone.",1534510797.0
smolzie,"Yes, if you want to treat the columns as a literal string. As an example, if you want to select all the columns that have dots it's a lot more readable with `""contains('.')""` than the regex version which is `""matches('\\.')""`. There are plenty of other cases where the matches can become even more messy whilst contains remains easy to read.",1534344108.0
flyos,"Another thing is that fixed matching (as does contains()) is usually a lot faster than regexp matching (as does matches()). But if you have enough columns that computation time matters for select(), you probably have other things to worry about! ;)",1534353860.0
efrique,"> I cannot make any assumptions about the mean or standard deviation 

You have to assume something in order to simulate from it. Any procedure that simulates will simulate values from some distribution with a particular mean and variance, which is what you're then assuming. 

You don't have to only use one mean and variance or only one distribution -- you can look at behaviour across a wide variety of them -- but each simulated sample will have a particular one.

What's the specific purpose of the simulation? (What are you simulating in order to find out?)

You may need to keep in mind typical characteristics of Likert-item responses (tendency to bunch up the ends for example, to polarize (a proportion at both ends at once) and a common tendency to head toward the middle. So you may, for example, have to consider the possibility of having three modes (indeed with 20 possibilities, perhaps more than three). ",1534289883.0
_westcoastbestcoast,"Given no prior information on the distribution. I think you would be OK with assuming a non-informative prior, i.e. the uniform distribution, and continue working in a Bayesian framework.

That being said, if you just want 10,000 samples from a uniform 1-20, the code is the following:

sample(20, size = 10000, replace = TRUE)

",1534289195.0
grandzooby,"I'm not quite sure what you're going after, but you might consider an approach where you try different generation procedures and run your analysis process against each set.

As others have said, any particular generation procedure in your simulation has to assume *some* distribution - that's how simulations work.  But nothing says you can't try one set of 10,000 using a discretized normal distribution, another set with 10,000 with a uniform, another set using a discritized beta distribution (maybe several, with different parameter sets)... and run each of those through whatever analysis you're trying to develop.

In any case, you can start from pretty much any distribution (discrete or continuous) and if needed, discretize/bin it into your 0-20 integer values. 

It might also be useful to see what other people have done, such as this: https://www.researchgate.net/publication/285928169_A_procedure_simulating_Likert_scale_item_responses

or
http://r.789695.n4.nabble.com/how-to-simulate-Likert-type-data-using-R-td3625664.html",1534315534.0
AllanBz,"Are you doing this to test someone’s skills at model identification?

     model.mean <- runif(1, min=0, max=20)
     model.sd <- runif(1, min=0.3, max=5.8)
     data <- pmin(20,pmax(0,round(rnorm(10000, mean=model.mean, sd=model.sd), 0)))
    
If you are testing identification, I would save the mean and sd somewhere.

If you *really* don’t want to assume normality, I would generate a bunch of moments and uniformly select from whatever distributions you think would be challenging.",1534317698.0
metagloria,"There may be more appropriate ways to do it, but you could simulate normal data, expit-transform it – i.e., exp(x)/(1+exp(x)) – to get it between 0 and 1, multiply by 20, and round to the nearest integer. 

Or you could draw from a binomial with size 20. ",1534289220.0
Drewdledoo,"I think you might be missing a call to `library(data.table)`, but here's a solution using `cowplot`, specifically the nifty `get_legend()` function:

    library(cowplot)

    plots <- plot_grid(p1 + theme(legend.position = ""none""), 
                       p2 + theme(legend.position = ""none"") + ylab(""""),
                       align = ""vh"")

    legend <- get_legend(p2)

    plot_grid(plots, legend, rel_widths = c(2, 0.2))

[Check out this vignette from the `cowplot` package for more detail](https://cran.r-project.org/web/packages/cowplot/vignettes/shared_legends.html).

Edit to page /u/Economist_hat",1534288786.0
balanaicker,"facets in ggplot.
http://www.cookbook-r.com/Graphs/Facets_(ggplot2)/
",1534287991.0
Economist_hat,"Responding to subscribe... when you figure that give me a heads up :(
",1534287484.0
ZZ-TOP,Where the content at? ,1534246283.0
Stuporficial,How does it compare to zoo?,1534274915.0
picardIteration,Never use K-means! Use the E-M algorithm! ,1534255871.0
keepitsalty,"    library(readr)
    
    nyc_dat <- read_csv(""https://data.cityofnewyork.us/api/views/ebb7-mvp5/rows.csv"")

should work for you just fine. ",1534205269.0
featherfooted,"I'm not explicitly familiar with ggplot's version of jitter, but my thought would be to just create your own values of jitter and make them all negative.

e.g. given data vector `y`, then let `j = runif(length(y), -0.05, 0))` and plot(x, y + j)",1534204000.0
_Shipwreck,Did you try it? I'm pretty sure that points don't go over the maximum value if you define the correct scale in ggplot.,1534204577.0
displaced_soc,"Would this work?

    geom_jitter(aes(x = x, y = y - 0.025), height = 0.05)

Depending on other elements of the plot, you might need to go with `inherit.aes = FALSE` and pass data directly.",1534237377.0
hmt28,Have you tried that in conjunction with setting the ‘width=0’? otherwise width jitter will automatically be set to 40% the res of the data by default. ,1534224220.0
chocolateandcoffee,"The short of it, if I understand your data, is that you can't do this as a violin plot. A violin plot is essentially a suped up box and whisker plot which shows distribution of data, not counts or factors.

I'm sure there are better guides out there, but [look here](https://blog.hubspot.com/marketing/types-of-graphs-for-data-visualization) for an explanation of different chart types.",1534185424.0
jackbrux,You need long format,1534188825.0
MrLegilimens,You need the full dataset to pull off violin plots.,1534193227.0
flyos,"It seems you are using R on another computer using ssh, so R cannot access your computer's graphic system and thus cannot display anything. It should create a PDF named Rplots.pdf on the current folder however, though I'm unsure whether that works when R is run in an ssh session.

If your ssh serveur allow, you can request a redirection of the graphics toward your computer using the -X option, though it'll be slow and might be a bit blurry:

    ssh -X [username]@XXX-YYY-ZZZ.whatever.ku.dk",1534179843.0
mattindustries,"http://www.cookbook-r.com/Graphs/Output_to_a_file/

Does this help at all? Use `list.files()` to show the files. You should have a new one with your plot. In a new terminal (not connected to the server) use 

`scp username@host:/path/to/hosts/file/host_file.txt ~/Desktop`

to download the file from the server to your machine. ",1534190647.0
new_in_R,"This is not going to help you directly, but I thought myself how to plot data using the free Udacity course. Good luck!",1534411405.0
jfcantin,I ended using a lot tidyverse. But again what type of data are you using? ,1534112300.0
brainfuckguru,"Why is this being downvoted?

This is a good intro to the tradeoffs one faces when choosing a framework for data cleaning. I stick with tidyverse but I have had to resort to using `data.table::fread()` for reading in big flat files like Census data. `read.csv()` and `readr::read_csv()` just aren't gonna cut it sometimes.

data.table is pretty illegible so unless you really care about compute time you'll save way more writing-code-and-reading-documentation time.",1534184649.0
MrLegilimens,Definitely! On mobile but you can do this through facets on ggplot.,1534078289.0
ruslankl,"You can try this code:

    library(cowplot)
    
    x <- data.frame(x= rnorm(100, 0, 1))
    y <- data.frame(y= rnorm(100, 1, 2))
    
    plt1 <- ggplot(x, aes(x = x, y =..density..)) +
    geom_histogram() +
    geom_density()
    
    plt2 <- ggplot(y, aes(x = y, y =..density..)) +
    geom_histogram() +
    geom_density()
    
    plot_grid(plt1, plt2)

https://image.ibb.co/fEgiJ9/Rplot.png",1534086403.0
Peter-Cottontail,"`par( mfrow = c(1, 2))` will kind of get you there. You might need to adjust the ylim to get them equal.

If you're familiar with ggplot, then I would suggest you use either `facet_wrap()` or `facet_grid`. There's a lot of documentation about ggplot, but you will need to have your data organized together in a single dataframe.",1534093677.0
gxvxnxc,"The plot you're trying to emulate was made with the 'lattice' package.
Check out this link, 
https://stats.idre.ucla.edu/r/modules/exploring-data-with-graphics/
",1534120710.0
Quasimoto3000,This is pretty cool! I’m looking for a way to have my python using colleagues reuse my R code and this seems like a nice option. ,1534062677.0
fasnoosh,Interesting name. Seems a bit rapey...,1534385332.0
king_curry,"Good read, thanks for this. Your first bullet point has an unfinished sentence at the end of it.",1534030490.0
danielw29,"This is what you need

https://stats.stackexchange.com/questions/10838/produce-a-list-of-variable-name-in-a-for-loop-then-assign-values-to-them#10839",1533963447.0
Ringbailwanton,"This is how I’d do it.  First, all surveys go in a directory with only that data.  Also, uses dplyr & purrr

library(dplyr)

library(purrr)

readin <- function(x){
  
  Read file
  
  return(data frame)

}

filevar <- a string vector of file names using list.files, with setting full.path=TRUE

filevar %>% map(readin) %>% bind_rows()

Sorry, on mobile.  But I think that should be enough to show the pattern.

EDIT: damn, that looks terrible, sorry.",1533986825.0
RememberToBackupData,"Use `apply_to_files()` in [my personal package](https://github.com/DesiQuintans/desiderata), and use readRDS() or readr::read_rds() as the applied function. It’ll do what you want in one line. ",1534021191.0
metagloria,"Are you sure it's giving 97.5% intervals, and not just showing the 2.5% and 97.5% confidence limits (which form a 95% CI)?",1533927841.0
LogicalRisk,I'd recommend the [estimatr](https://declaredesign.org/r/estimatr/) package assuming you do not want to simply code up the procedure of finding the ATE yourself. ,1534472571.0
aeburnside,"Well, the content and function seems very similar. Is it not just a different bootstrap theme?

https://bookdown.org/yihui/rmarkdown/html-document.html#appearance-and-style

edit: or custom CSS/HTML to modify the TOC box appearance?",1533922442.0
MrLegilimens,"No. **No** one should ever provide help over private message. It discourages open discussion, and if others have the same problem, they cannot find your solution by searching. 

If you want help, post the output of your regression, and ask what you have questions on, what you understand, what you don't understand. ",1533908580.0
mossguire,"If you’re uncertain about the interpretation, are you certain that you didn’t violate OLS assumptions? The interpretation of OLS is pretty straight forward. I’d recommend thoroughly checking your results against OLS’ assumptions before beginning any interpretations. ",1533914555.0
,"This is really interesting. Are confidence intervals easy to obtain from the nls model object in R? Like, can I just get the SE from the predict function?",1534286085.0
AnscombesGimlet,Log will help stabilize the variance not make it stationary. Auto.arima will take the differences necessary to make it stationary automatically.,1533906296.0
alexpnx,"You take the difference of your time series as many times as required to achieve stationarity (in the wide sense)

If the time series has a seasonal component, you difference at the seasonal period as well",1533897766.0
Economist_hat,"Difference the series and use auto- and partial-correlation plots to see which differences you should take.

Whichever lags in the auto-correlation plots have non-zero correlation, should be differenced out.

If you have a broad, slope in your AC plot, then you might want to try smoothing with an MA term.",1534267977.0
PM_ur_good_deeds,"You could fit a smoothing spline and subtract it from your original data (see e.g. `?smooth.spline` in R).

Edit: I could be very wrong about this suggestion. If you downvote this suggestion, please let me know why it is not a good suggestion so I can learn!",1533897587.0
sparkysparkyboom,What do you mean by duplicates? There's some pretty simple R functions that allow you to dedup.,1533883529.0
Quasimoto3000,I’d charge 60 an hour. Pm me. ,1534064087.0
mxdata,Can you specify that ?,1534263449.0
Darwinmate,"yeah
https://stackoverflow.com/questions/40941467/r-studio-change-cursor-color",1533901769.0
,Great read. Wickham is one of the greats. Always enjoy reading what he says. Thanks!,1533834546.0
yaylindizzle,He was the head of the stats department at my university! But left during my first year,1533834598.0
VincentStaples,"ggplot(df, aes(x=x, y=y))+  
stat_summary(fun.data = mean_cl_normal, geom = ""errorbar"")",1533766652.0
ColinQuirk,"Use `stat_summary`

[This](https://stackoverflow.com/questions/19258460/standard-error-bars-using-stat-summary) should get you started.",1533762153.0
unclognition,"picking up after your first code block, and using dplyr (i don't really use data.table, but generally dplyr is awesome for data wrangling):

    library(dplyr)
    data <- data %>% 
      as.data.frame() %>%
      group_by(Subject, Condition) %>%
      summarize(mean = mean(x)) %>%
      spread(Condition, mean)

but I would recommend (for most purposes) leaving off the final `spread()` step and keeping Condition as a variable, to keep your data [tidy](http://r4ds.had.co.nz/tidy-data.html).",1533756001.0
AlisonByTheC,"I noticed this was from 2015.  It’s a great article but are there better examples?   

I have been using the package forecast for NNAR on time series data but ARIMA and ETS always seem to do better.  ",1533781496.0
ercmrn,"This was a really fun one - here's my approach.

- flatten the original df down to three columns: `Priority`, `date`, and `index`
- expand the df so that it contains all days each `Priority` belongs to
- remove rows where a lower `Priority` overlaps with a higher `Priority`
- group by `Priority` and `index`, use the lowest `date` as `Start` and the highest `date` as `End`, then remove `index`

So:

    library(tidyverse)
    
    df = data.frame(Priority = c(""Priority_2"",""Priority_1"", ""Priority_2""),                 
                    Start = as.Date(c(""2018-01-01"", ""2018-01-03"", ""2018-01-08"")),                 
                    End = as.Date(c(""2018-01-04"",""2018-01-05"",""2018-01-09"")))
    

    expand_dates <- function(df) {
        purrr::map(1:nrow(df), function(x) {
            row <- df[x,] 
            data.frame(Priority = row$Priority,
                       date = row$Start:row$End,
                       index = x,
                       stringsAsFactors = FALSE)
            }) %>% 
            bind_rows() %>% 
            mutate(date = as.Date(date, origin = ""1970-01-01""))
    }
    
    
    enforce_priorities <- function(df, higher, lower) {
        df %>% 
            filter(Priority == lower) %>% 
            anti_join(df %>% filter(Priority %in% higher), by = 'date') %>% 
            bind_rows(df %>% filter(Priority %in% higher))
    }
    
    
    fill_date_gaps <- function(df) {
        date <- as.Date(min(df$date):max(df$date), origin = '1970-01-01')
        data.frame(date) %>% 
            left_join(df, by = ""date"") %>% 
            fill(index, .direction = ""up"")
    }
    
    
    df %>% 
        expand_dates() %>% 
        enforce_priorities(""Priority_1"", ""Priority_2"") %>% 
        fill_date_gaps() %>% 
        group_by(Priority, index) %>% 
        summarise(Start = min(date), End = max(date)) %>% 
        select(-index) %>% 
        arrange(Start)
",1533803692.0
millsGT49,"This is the closest I've seen to something of an update: http://wesmckinney.com/blog/announcing-ursalabs/

They don't mention Feather in R specifically but there seems to be a focus on improving interoperability between R, Python, and Arrow so you'd assume they would focus on Feather more. ",1533666660.0
Runner1928,Feather is super interesting. I use sqlite for the moment: very high interoperability between systems and existing support.,1533678836.0
Ader_anhilator,I'd much rather see a data.table connect version. ,1533682982.0
SeveralBritishPeople,"There’s some very recent activity spinning up: https://cwiki.apache.org/confluence/display/ARROW/R+JIRA+Dashboard

Wes seems very enthusiastic to get a real R interface going, but there aren’t a ton of people with the R and C++ knowledge to do it well, so it’ll take some time. I’m confident that it’ll happen, though. Having Romain Francois on board is a good sign, and it helps that there are C++ bindings for RCpp to use, and RCppR6 may be able to reduce boilerplate grunt work. ",1533684528.0
noviceProgrammer1,"The talks of interoperability among tool chains and serializations of data has really caught my fancy lately. I've been an avid R user for a few years, but for work I use mostly python. Due to the ML hype I was curious with the development of ONNX. 

Now I'm really pondering if proto3 or arrow would be better as a base tool to communicate between languages and/or operating systems.

I include operating systems, because the ease of which I can install all the necessary tools affects my adoption rate and how easily I can sell coworkers on the toolchain as well.  ",1533706392.0
thaisofalexandria,"No.  One data set (frame, tibble), with columns for score, location and occasion.  That's tidy.",1533655548.0
guepier,"`setNames` is the perfect answer in this specific case, but there’s a more general answer:

**You can rewrite *any* statement of the form `foo(bar) = baz` into a function call!**

Simply call it as follows:

    `foo<-`(bar, value = baz)

(note the backticks around `foo<-`!)

Or, with a pipe:

    bar %>% `foo<-`(value = baz)

Because under the hood that’s exactly what such “assignment functions” do. The same is even true for subset assignment:

    x[1] = 5
    # results in the following internal call:
    `[<-`(x, 1, value = 5)

So you *could* use ``df %>% `names<-`(c('newvar1', 'newvar2'))`` but most people would probably find `setNames` more readable than `` `names<-` ``.",1533657188.0
too_many_splines,"Yup. On my phone but it should look something like this :

    Mydf %>% setNames(c(""newvar1"", ""newvar2"")) ",1533647808.0
manwithoutaguitar,"    mydf %>%
      names(c(""newvar1"",""newvar2"")) 

equals

    names(mydf, c(""newvar1"",""newvar2"")) 

Why don't you try out if it works? 

[If you want to use the pipe use dplyr::rename](https://dplyr.tidyverse.org/reference/select.html)",1533647654.0
guepier,Apologies for not having anything more positive to say but I strongly suggest you forget everything in that post and instead [use the **{fs}** package](https://www.tidyverse.org/articles/2018/01/fs-1.0.0/). It provides a modernised and vastly improved API for file system functions in R.,1533657456.0
willbell,"I would have to check but I think in the boot library in R, if you set a stratum, it samples each stratum up to the number of individuals in that stratum.  This is likely your best choice unless you have a strong statistical background and know what you're doing.

The alternative would be to write your own function (if you do know what you're doing), I'm going off the assumption that what you want is a bootstrap (if you only want a sample once then ignore the very last line) where you take up to some maximum number from each stratum, e.g. something like:

(This code might not be great, it is made on the go)

    appliedMin <- function(n, maxnoofindividuals = 5) { ## For use in the apply function below

             min(n, maxnoofindividuals)

    }

    plantSingleBoot <- function(strata, measurement, maxnoofindividuals) {

            Levels <- levels(strata)

            Table <- table(strata)

            Table <- sapply(Table, appliedMin, maxnoofindividuals = maxnoofindividuals)

            n <- sum(Table) ## How many you'll ultimately sample

            Resampled <- rep(NA_real_, n) ## Preallocate memory

            x <- 1

            for (i in 1:nlevels(strata)) {

                    groupMeasurement <- measurement[group == Levels[i]] ## Include only the individuals in a given level for this resampling

                    Resampled[x:(x+Table[i]-1)] <- sample(groupMeasurement, size = Table[i], replace = T) ## Sample the measurements of those individuals

                    x <- x+Table[i]

            }

            ResultYouWant <- mean(Resampled) ## Or whatever statistic you want.
    
    }

Then do something like this:

    BootstrapOut <- replicate(plantSingleBoot, 1000)

You're better off I think if you want to remove the effects of geographical location constructing a model that includes location as a random effect and taking the residuals or something like that.",1533612208.0
s3x2,What do you mean stratified by their observed values? Do you want to sample each location until you've either exhausted the available individuals or gotten at least X observations per level of your measured variable? Do you need to sample every location?,1533613280.0
cardboard_dinosaur,"If you have at least n observations in every segment then it’s fairly straightforward it do it with the tidyverse - group by location and category and then sample n. 

If you don’t have at least n observations in every segment then you could separate the dataset into those where you do (and then sample them) and those where you don’t (and then select them all), or write a function to handle the segments differently. ",1533622896.0
Babahoyo,"Not sure about that, but you can create a lambda function (I don't know what its called in R) to wrap the label to a certain width. ",1533583554.0
secret_tacos,"Not sure what you’re ultimately doing, but you can try using ggrepel if you have overlapping labels.",1533585358.0
Quasimoto3000,Why don’t you just use coord_flip?,1534064142.0
AddemF,So ah ... what's the recommended course of action to correct this?,1533507732.0
RememberToBackupData,"There’s a dirty trick that some journals do. They’ll review your paper, reject it, and send it back with “Make minor revisions and submit again.” You make the changes, you resubmit, and they accept you in short order. The resubmission starts the timer anew so that all the time leading up to your first rejection is ignored, but the bulk of the actual peer review happened in this period. With your resubmission, the reviewers are likely to be appeased and wave it through and *boom* a fast received-to-accepted time for the journal. ",1533540274.0
umib0zu,"Correct, or avoid?

Is there anyone not working at a journal who actually thinks the journal submission system works? I would hope there's a point when people realize the formality only benefits journals and not the researcher, so researchers will stop playing the system.",1533513680.0
efrique,"First thing: Take a look at the journal's information for authors, and what it already publishes. ",1533429888.0
leonardicus,"I can't really answer your question directly, but I'll give you my opinion. Were these tests already implemented in some fashion with existing packages, and your code offers some convenient wrapping functions? If so, then it may be hard to convince the editors/reviewers of their importance.

However, if the package translates mathematical or statistical equations into code, or implements novel procedures, then it should be an easy appeal for submission.

Certainly you can publish your package without a formal publication, it happens all the time. You can also write an article and submit to a preprint server.",1533429699.0
BirthDeath,"I've refereed for both J Stat Soft and the R Journal in the past.  R Journal papers are generally very short and simply describe the package and a few example implementations.  I wasn't given a rubric or guidelines for acceptance/rejection. 

Journal of Statistical Software is much more formal and generally results in longer and more novel papers.  I've typically found that the underlying methodology is described in much greater detail than in R journal papers.  They had a more formal rubric and requested a lot more feedback than the R Journal.  

Both journals require the user to submit R code and as a reviewer, I evaluated it as if it were a part of the paper.  

In my opinion, Journal of Statistical Software papers are generally much higher quality and more useful to me than the R Journal (it also has a much higher impact factor).  

Based on your brief description of your idea, I think that your best option would be to submit one paper to Journal of Statistical Software detailing your methodology and implementation or submit two papers: a methodological paper to an applied statistics journal and an implementation paper to the R Journal.   ",1533497424.0
YepYepYepYepYepUhHuh,"Most peer-reviewed journals would require significant updates or expansion of the scope of a project to warrant a second publication. In the submission form for R Journal, for instance, you have to check a box to indicate that 
> The manuscript has not been published in a peer reviewed journal, and is not currently under review for another journal

So if you feel there are significant differences between what is published in the stats journal and what would go in to an R journal, you might be able to get two publications out of it. However if you do build a package you can reference your original stats paper for citations, which might boost the visibility and impact of just one paper.",1533430020.0
Darwinmate,"For highest impact and betterment of science you should publish both the paper and the implementation together. For most end users your statistical test will be too esoteric and essentially meaningless. The best papers I have seen have published both the tests and the package as a single paper. This allows both theoretical and practical assessment of your test.

IMO, trying to stretch what is essentially one paper into two is really fucking lame. You want good papers that are self contained. Providing the implementation of the statistical test in an accessible format raises your publication from theoretical to practice, opening it up to a wider audience. 

The only time I have seen what you describing is when several packages are brought together and a paper is written on the practice usage of these packages in concert. I've also seen statistical tests that right at the end will say ""and this has been implemented in the R package X"". ",1533439558.0
huessy,How's that tenure track going?,1533445261.0
Darwinmate,post your code and an exmaple data. ,1533425208.0
_Shipwreck,merge() is the native way. ,1533419098.0
RememberToBackupData,"dplyr::left_join()

If that doesn’t work, you might have to dplyr::bind_rows() and then do some rearrangement, it really depends on the shape of your data and how tidy it is. ",1533415073.0
mearlpie,You could use sqldf to join the files using sql inside of R. ,1533421248.0
bek2113,"If you’re trying to do them all in a single loop, look into Reduce(). ",1533448010.0
hurhurdedur,Can you post an example of your files and the desired end result? E.g. the first three lines of two of the files and then an example of how the contents from the files appear in your desired end result? This will help determine which kind of join or other operation needs to be used.,1533490041.0
,"List all the files, read them using tidyr::read\_csv then do a bind\_rows to merge them all together",1533464492.0
danderzei,"range results in a vector with the minimum and maximum value of the input values 

diff((range()) substracts minutes from max.

Best way to learn is to try stuff out with simple data.",1533382366.0
s3x2,"`identical(c(min(1:3),max(1:3)),range(1:3))`

`identical(c(max(1:3)-min(1:3)),diff(range(1:3)))`",1533383676.0
xubu42,"One thing you should know about R is the CRAN task views. They are essentially reference guides for pretty much anything you might want to do using R. https://cran.r-project.org/web/views/

There are a few task views that might be obvious, such as multivariate and time series, but you should also check out econometrics and machine learning as regression is important to both of those areas.

Next, Rob is currently developing a replacement for the forecast package called fable.

https://github.com/tidyverts/fable

It's not fully speced out yet or a final product, but it should provide an easier to use and more consistent API for time series forecasting. 

Some packages you might find interesting complements to forecast are sweep (for cleaning up models from the forecast package), anomalize ( uses Twitter's AnomalyDetection algorithm), and prophet (Facebook's automatic time series forecasting algorithm). 

For regression, there are an overwhelming amount of resources available in R. Classics include car, lme4, e1071. 

One of my favorite packages and most practical in real world settings is glmnet, an implementation of the elasticnet regression. Elasticnet is not deep learning or related to neutral nets. It is a technique for reducing the variables in your regression formula to only the ones that contribute significantly and do not inhibit other variables. It's the safe, practical choice for feature reduction and has been very good in production grade forecasting over alternatives like ridge regression or lasso regression. That's because it is designed to be a in between of those two methods. glmnet is great also because you can implement and compare all three at the same time through the package. 

Lastly, you should check out the caret and mlr packages. They have a similar purpose, but different feel and feature set. They are designed to wrap a lot of predictive modeling and forecasting packages into a single, consistent code style (API). This makes it much easier to learn new techniques and compare methods to each other. 

Good luck on your journey through predictive modeling!",1533345035.0
Ruffie1234,Thank you for putting this together.,1533395298.0
BirthDeath,"Tsay's book [Multivariate Time Series Analysis](https://www.amazon.com/Multivariate-Time-Analysis-Financial-Applications/dp/1118617908) is a good starting point.  Its companion package MTS can be useful for learning purposes, but it doesn't have the most efficient implementations.

Another book that I really like is [Lutkepohl's New Introduction to Multivariate Time Series](https://www.amazon.com/New-Introduction-Multiple-Time-Analysis/dp/3540262393/ref=sr_1_1?s=books&ie=UTF8&qid=1533497667&sr=1-1&keywords=lutkepohl).  It's a bit dated, but it offers a great introduction to vector autoregression.",1533497730.0
HeyItsRaFromNZ,"If you want to indent a single line—or a block after visual selection—you can hit `>>`.

If you want to set different defaults to how Vim auto-indents R code, check out the help documentation: `:help ft-r-indent`.

By default, function definitions and any line after one containing a `<-` ought to be indented. Is this not the case?

 Hope this helps. That help command is not easily guessed! ",1533405067.0
Demortus,"Check out the [margins ](https://cran.r-project.org/web/packages/margins/vignettes/Introduction.html)package. It will let you predict values of z at various levels of a, once you have fit your model.",1533268761.0
Digging_For_Ostrich,"Ggplot on its own won’t be able to do that, because the value of b matters based on your model.

However, because it’s a simple additive model what you can do is specify b as a constant value e.g. = 0, then plot the effect of a on z, based on the coefficient of a in your a+b model.

The graph will look weird, but it will explain the effect (note, you must specify it is only the effect when b = 0) of a on its own. Just grab the coefficient of a, plot that on a graph independently of your model and you should be good to go.

Edit: /u/Demortus has linked a package I didn’t know about which will do just this and probably much more! Very useful, thanks!",1533266773.0
NeckbeardsDelight,"You could also use a conditional plot from the package 'visreg' (pbreheny.github.io/visreg/contrast.html)

This plots the dependent against one independent variable while holding all other IVs on a constant value (by default on their median, but you can also change the value of the other variables)",1533304851.0
infrequentaccismus,"Yes, that is the normal convention. Often you’ll have functions.R and then the script that uses (like eda.R) will call source(“functions.R”) as the first line. ",1533260525.0
Darwinmate,"Another option is to create a custom package for all your helper scripts. It seems relatively straight forward. Here is a short tutorial:
https://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/

This enables you to call functions directly without importing or sourcing everything. So you can do `MyHelpers::plotPanda()`.",1533263524.0
RememberToBackupData,"There is a convention, it’s called making a package! The package is the standard unit for sharing code amongst several projects. It’s not too hard to make a personal package that holds all of the helper functions that you’ve written. Here’s mine: https://github.com/DesiQuintans/desiderata. I read [Hadley's book about making packages](http://r-pkgs.had.co.nz/), of course.

I’ve also made packages to do stuff like hold the cleaned data for my thesis, because it means that I can organise my analysis scripts in a sensible folder structure and access the actual data from anywhere, without needing to maintain up-to-date addresses for the data in a CSV or RDS file or whatever. ",1533284029.0
balanaicker,"Two unconventional patterns I use are,

**Anonymous function:** Use a nameless function to return an object with all the sub functions needed similar to a nodejs package. The main advantage is that you dont litter your default namespace with lot of function names. eg.
Your arith.r file should have,

    (function(){
        arith<-list()
        arith$square <- function(a){return(a*a)}
        arith$cube <- function(a){return(arith$square(a)*a)}
        return(arith)
    })()

and in your main file you call this script by calling for its ""value"",

    arith <- source(""arith.r"")$value

now you can use the functions by using,

    arith$square(2)
    [1] 4
    arith$cube(3)
    [1] 27

Note that the object returned from anonymous function does have state (internal variables) making the function similar to a class definition in Oject oriented programming.

**Rscript environment**: You can read from and write output to standard IO in linux/unix from Rscript. So for big data pipelines, I make small Rscripts which read from stdin and write to stdout and chain them together with pipes. e.g

""square.r"" file contains,

    !# /bin/Rscript
    suppressMessages(library('tidyverse'))
    read.table(file('stdin'),sep="","") %>%
        mutate(square=value*value) %>%
        format_csv %>%
        cat

""cube.r"" file contains,

    !# /bin/Rscript
    suppressMessages(library('tidyverse'))
    read.table(file('stdin'),sep="","") %>%
        mutate(cube=square*value) %>%
        format_csv %>%
        cat

data.csv contains,

    value
    1
    2
    4

Now we can do this in shell,

    cat data.csv | square.r | cube.r > result.csv

The result.csv will have,

    value, square, cube
    1,1,1
    2,4,8
    4,16,64

The advantages are that you can encapsulate problems into tiny processing components and you can wire these individual components with other system programs. like curl a file from web, filter it using grep, process it with R, write it to sql database with a mix and match of different tools which are really good at what they do.",1533283959.0
bottomfeeder_,"There isn't a universal standard, but you can look up ""R Style Guide"" for some options. Here is an example from Hadley: http://adv-r.had.co.nz/Style.html",1533259674.0
willbell,"My planned workflow for my next time organizing a project in R will be using [this system](https://robjhyndman.com/hyndsight/workflow-in-r/), it seems like a useful method.",1533618728.0
RememberToBackupData,May I ask why you want each day in a separate dataframe? Are you exporting data for another program to use?,1533252271.0
galileuk,"It is not necessary to create a new object (variable) for each dataset. In this situation, lists are your friend, and they can store multiple dataframes. You can also name each element in the list. I would do this:

# loop to subset
data <- list()
for (i in 1:30){
    data[[i]] <- subset(original_data, day == i)
}

 # name list
names(data) <- paste0(""day_"", 1:30)
",1533253347.0
infrequentaccismus,"It sounds like you actually want to nest the data frame for each day.  

    nest(df, day_column) %>% 
      mutate(day_column = paste0(“df_day_”, day_column)",1533271003.0
PM_ur_good_deeds,"I would use `split()` which is a function in base R which splits a vector or dataframe according to some grouping variable. In your case (making up some data as I go), you could do something like:

    dataframe_by_month <- data.frame(day = c(rep('01-01-2018', 3), rep('02-01-2018', 3), rep('03-01-2018', 3)),
                                                        x1 = rep(1:3, each = 3))
    split(dataframe_by_month, dataframe_by_month$day)

which gives a list of dataframes with each dataframe containing the observations of an individual day:

    $`01-01-2018`
             day x1
    1 01-01-2018  1
    2 01-01-2018  1
    3 01-01-2018  1
    
    $`02-01-2018`
             day x1
    4 02-01-2018  2
    5 02-01-2018  2
    6 02-01-2018  2
    
    $`03-01-2018`
             day x1
    7 03-01-2018  3
    8 03-01-2018  3
    9 03-01-2018  3

",1533283420.0
eldgrimr,You can use `file.exists()` in an if statement and then use `download.file()` in the body. Both of these are already in your base installation and don't require loading a package  Check the documentation of these functions using `?file.exists` and `?download.file`. Good luck!,1533239473.0
hurhurdedur,"From reading the post and hearing about misplaced files, I'd recommend for one that you adopt Git/Github (as an academic, you can get a cheap/free private Github repo for your project).

Once your project is safely version-controlled, you can feel free to just take a morning and organize your project into neat folders with clearer names, e.g. (Simulations, Simulations/Inputs, Simulations/Output, Functions, Functions/R, Functions/Cpp).

Btw, this is a nice, well-written post! I hope you follow it up after you try some reorganizing or come up with some good solutions.",1533232656.0
Mooks79,"You can also think about checkpoint instead of packrat for dependency issues. packrat is great if you actually want to store copies of the packages - eg for distributing your work. But, if all you need is a way to specify and manage the specific versions of the packages - then checkpoint will do this very well without having to make you take up space storing lots of different packages in their entirety.",1533278881.0
Babahoyo,"> Academic code often was produced during research. My understanding of professional programming is that often a plan with a project coordinator exists, along with documents coordinating it. While I’m new to research, I don’t think research works in that manner.

Why do you say that research doesn't work in this manner? Good project management is a way of atomizing changes to things. Git is a way to have a history of incremental changes. Coding an API and sticking to it in your analysis will *always* save headaches down the line. Research isn't that different from anything else. ",1533325512.0
,"Sorry, it is a long blog post. Are you asking the sub for advice?",1533227694.0
questionquality,You can also just pass the dates you want as breaks with the breaks argument: `scale_x_date(breaks = my_data$date)`,1533216931.0
_Wintermute,Probably easiest converting your dates to factors and ordering them that way.,1533187082.0
jowen7448,Have a look at parameterised reports section of rmarkdown material on the rstudio collection of pages. ,1533186876.0
paddedroom,Docker containers with specific deployments per script or necessary analysis packages.,1533156850.0
bc2zb,"When I had to do this, I used conda to create specific R environments, and then pointed RStudio to use the R for that particular environment.  ",1533142236.0
guepier,"> This doesn't work for open source contribution because I don't want to push the modified `.gitignore` to upstream.

You don’t need to check in or push your local `.gitignore`. For that matter, you don’t need to add the library to your local `.gitignore` either.

> then specify `lib.loc` in `library()`

Yeah, don’t do that. Instead, specify it via `.libPaths()` in the `.Rprofile` file in the project root.

So both solutions “work” to some extent.

Alternatively you could just use Anaconda and set up a completely separate environment. I find this approach a bit too heavy-handed but for many use-cases it’s appropriate.",1533147649.0
infrequentaccismus,Rstudio connect is the professional product designed partly for this. ,1533141607.0
hjkl_ornah,[https://github.com/ankane/jetpack](https://github.com/ankane/jetpack),1533146149.0
jimbean66,You could always have different RStudio dockers for each version. ,1533151508.0
RememberToBackupData,"Install packages to the `temp()` directory by specifying the arg `lib = temp()` in `install.packages()`. The packages will be uninstalled at the end of your R session (because the temporary folder will be deleted). 

This argument is not available in `devtools:install_github()`, but I make it available for both CRAN and Github packages in my package management package [*librarian*](https://github.com/DesiQuintans/librarian). *librarian* also lets you install and load multiple packages from both repos at once, so that also solves your second problem. ",1533151705.0
grasshoppermouse,"Check out dev_mode in devtools:

https://www.rdocumentation.org/packages/devtools/versions/1.13.6/topics/dev_mode

EDIT: use it like this:

`> devtools::dev_mode()`

(install and load bleeding edge stuff to develop against)

`> devtools::dev_mode()` 

(back to my normal library)",1533144446.0
privlko,"Still learning about ggplot2 but two great resources I think everyone should try are;

1. The dataviz chapters on [r4ds](http://r4ds.had.co.nz/).
2. Kieran Healy's resources on [dataviz](http://socviz.co/).

Healy's course is especially good, with lots of chopping, changing and evaluating the best way to build a graph.",1533136902.0
MrLegilimens,Swirl has multiple courses that use ggplot2.,1533130247.0
CJL_LoL,"The cheatsheet can be really good for quickly answer questions of ""how do I do this""",1533147754.0
dopadelic,"I learned ggplot2 with a coursera course. They went in depth with many of the [renderer techniques.](https://www.rdocumentation.org/packages/grDevices/versions/3.4.1/topics/x11) I don't remember almost all of what I learned. I just refer to the [ggplot2 cheat sheet](https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf) and/or I google a description of the plot I need and it hasn't ever failed me.
",1533142184.0
Sosewuta,"Are you just starting out with ggplot2 or do you already know the basics? If you want to learn the basics I can recommend not a course but an online book: [R for Data Science by Grolemund and Wickham](http://r4ds.had.co.nz/)  
Chapter 3 ""Data visualisation"" is what brought me up to speed in ggplot2 within one day.",1533211723.0
SemanticTriangle,?lm,1533116922.0
Hoeschel,"edit: did not see this was posted in rstats. My answer assumed that you only have excel available.

Very basic answer that will get you what i think you want:

If you want to measure the strength of association between a question and the overall experience you can do so like this:

multiply the coefficient by the standard deviation of the responses to that question and divide it by the standard deviation of the responses to the ""overall question"":

coefficient * sd(question_1) / sd(question_overall)

This is called a standardized regression coefficient and similar to a correlation. Values go from -1 to 1. 0 means no association.

The P-Value tells you how reliable that association is. Smaller values mean more reliable. The easiest way to use them is to only consider coefficients with p-values <.05 as reliable enough. If very few people answered your questionnaire this might be all of them.",1533118283.0
Economist_hat,"The blog left out the best part:

William Sealy Gosset was the chief brewer for Guinness and was using the t-distribution to improve the beer brewing process for Guinness!",1533106623.0
localoptimal,I love encountering bits of historical context in technical posts. Thanks for the share.,1533101774.0
ercmrn,"Yep, pretty close: a dataframe is a list of lists, so one row is still a list of single-element lists, and a list is not numeric even if it only contains numeric values. Try out `is.numeric(list(1, 2))` and `is.numeric(c(1, 2))`.

All this to say that if you do `matrix(unlist(df[1, c(""start.x"", ""start.y"", ""x"", ""y"")]), ncol = 2)`, you'll get a matrix that works for the `Line()` function.",1533108216.0
,[deleted],1533078100.0
s3x2,"Assuming you loaded your file as a `data.frame` called `data` and that all columns are numbers:

    mdata <- as.numeric(as.matrix(data))
    matrix((duplicated(mdata)|duplicated(mdata,fromLast = T)),ncol=ncol(data))

That gets you a matrix with `TRUE` values wherever there's a duplicate. If your data aren't numbers then `as.character` instead of `as.numeric` should work. If you have data of mixed types then it's probably a good idea to split by types before comparing them.",1533078967.0
neuro99,"If I understand your question correctly, there can be duplicates in a row and you want to eliminate those rows. In the following example, line 3 of df has the number 3 duplicated. Using apply on a row basis (1), you can sum the result of the duplicated function (true or false) and keep only the ones that sum to 0 (no duplicates or all false).

    df <- data.frame(matrix(c(1:5,3,7:9),nrow=3))
    df
      X1 X2 X3
    1  1  4  7
    2  2  5  8
    3  3  3  9
    
    df[apply(df,1,function(x) sum(duplicated(x)))==0,]
      X1 X2 X3
    1  1  4  7
    2  2  5  8
",1533081212.0
mbillion,"I'm not even sure what you are asking.  Are you asking if each row is a unique combination of the ten columns.. i.e a ""unique record""

",1533086738.0
dankwormhole,"Here is my approach. Hope this helps:

`library(tidyverse)`

`# df1 contains a single duplicated value of 9`

`df1 <- as_data_frame(matrix(1:100, ncol = 10))`

`df1[9,9] <- 9L`

`# df2 does not contain any duplicates`

`df2 <- as_data_frame(matrix(1:100, ncol = 10))`

`are_there_duplicates_in_columns <- function(df){`

`df %>%`

`map(unique) %>% # gets the unique values in each column`

`unlist() %>% # converts all the values from all the columns into a single vector`

`duplicated() %>% # determines if there are any duplicates`

`sum() # if 1 then there are duplicates, 0 if there are none`

`}`

`are_there_duplicates_in_columns(df1)`

`are_there_duplicates_in_columns(df2)`",1533334819.0
efrique,"continious -> continuous (several places)
",1533020528.0
ecdf,"Use the interval package or the icenReg package. See

page 13 in this vignette [https://cran.r-project.org/web/packages/interval/vignettes/intervalCensoring.pdf](https://cran.r-project.org/web/packages/interval/vignettes/intervalCensoring.pdf)

or page 6 in this vignette for icenReg [https://cran.r-project.org/web/packages/icenReg/vignettes/icenReg.pdf](https://cran.r-project.org/web/packages/icenReg/vignettes/icenReg.pdf)

The first uses the diagonal drops to indicate the uncertain event times for the interval censoring.

The second vignette mentions that "" This is because with interval censored data, the NPMLE is not always unique; any curve that lies between the two lines has the same likelihood.""

Hope this helps!

ksint",1532994209.0
ecdf,"Intuitively, the mean of 7.23 is not unreasonable. Assuming there is not interval censoring and we assume that for each individual, the survival time is exactly between T\_L and T\_R, the mean survival time is 8.22. Also, there is more information from a smaller interval of \[6,8\] than a wider interval of \[6,12\]. Hope this helps intuitively.",1532999668.0
guepier,"> Surely in the NAMESPACE it would need to import from the local packrat repo.

No, it just checks the locally configured `.libPaths()` (which is set by Packrat via the folder-local `.Rprofile`) when compiling the package, same as when loading packages.",1532974230.0
ReimannOne,"This stackoverflow answer looks like what you might need:

https://stackoverflow.com/a/8097519/7547327


`Reduce(function(...) merge(..., all=T), list_of_uneven_dfs)`",1532965211.0
DataPseudoscientist,"Hey, you could try importFrom(""dplyr"", bind\_rows) in your NAMESPACE when building your package. They don't need all of dplyr.",1532965264.0
questionquality,"It depends. You present two different ways of combing data yourself: by row `dplyr::bind_rows`, and by some common columns `dplyr::full_join`. It just happens to be that your example could be solved with either. What happens if more than one df in your list has a row with `a = 1`? Should that row be added as extra columns? If so, your reduce+merge solution looks good. Or should those be added as extra rows? If so, the base r function `rbind` should do.",1532968185.0
030Economist,"My first suggestion would be to turn the labels vertical in order to have more space. 
In ggplot2 you can add this: 
+ theme(axis.text.x = element_text(angle = 90,
hjust = 1))

In combination with the preceding, you can also change the size of the x-label text size. 

If that fails, [this post on Stackoverflow](https://stackoverflow.com/questions/33529116/ggplot2-stagger-axis-labels) discusses the option of staggering x-axis labels. 
",1532981211.0
BillWeld,"Validate function parameters!  Dynamically typed languages don't do this for you so you've got to.  R idiom:

    f <- function(a, b, c)
    {
        if (getOption(""warn"") > 0) {
            stopifnot(
                is.numeric(a),
                is.vector(a),
                length(a) == 1,
                is.finite(a),
                a > 0,
    
                is.character(b),
                ...
    ",1532958226.0
dm319,"This is excellent, and something I think most R-users (coming from a statistical rather than coding background) don't but definitely should think about.",1532951417.0
guepier,"Amazing article.

I am happy you included point four, and strongly agree with it. I wouldn’t make it a hard and fast rule, of course: having a {dplyr} pipeline of ~10 lines, where every line contains `dplyr::`, isn’t very readable. But especially for rarely used speciality packages, explicitly qualifying the name should be the default.

Similar for `require`, except I’m even more stringent here: I think `require` is not a good fit even for the attach-or-install case; [I (along with Hadley, if I understand correctly) recommend instead using `requireNamespace` in this case](https://stackoverflow.com/a/51263513/1968).

On the other hand, I’m no fan of Packrat. Don’t get me wrong, I definitely like the *idea* of Packrat. Unfortunately it falls short in practice because it fundamentally doesn’t (and can’t!) support multiple package versions inside the same library (something that Pyenv/Bundler/… have supported for ages).

I’m no fan of point 8 either because … well [the alternative is easier and technically superior](https://github.com/klmr/modules). tl;dr: Write modules, not packages. For small utilities, packages consist of > 90% irrelevant cruft, and this encourages inappropriately aggregating unrelated functionality into “super packages” (see {Hmisc} etc.). Modules, by contrast, are agile, self-contained and hierarchically combinable.

But point 9 is where you’re quite simply wrong, sorry:

> never, ever, ever use = to assign. Every time you do it, a kitten dies of sadness.

This is FUD, please don’t spread it. [**There’s nothing wrong with `=`.**](https://stackoverflow.com/a/51564252/1968) It’s *purely* a question of personal preference. In fact, *if anything* [*`<-` is more error-prone*](https://stackoverflow.com/questions/1741820/what-are-the-differences-between-and-in-r#comment14293879_1742591) (granted, this is a *very* slight chance but it’s still higher than the chance of making an error when using `=`).",1532955423.0
giziti,"I would've expected something about defensive programming to include something about error handling, try/catch, etc. ",1533011257.0
RememberToBackupData,"%in% is used by offering a vector as the right-side argument. So 

    “moo” %in% c(“moo”, “baa”)

would return TRUE.",1532917192.0
LogicalRisk,"There are two different issues here. First, your filter command does not have the data frame as an argument, which may be why you get an error. In addition, you're passing the list of numbers one at a time instead of a vector, so dplyr will not understand that they are all one argument. This may be another reason you are getting an error.

    set.seed(1)
    library(dplyr)
    
    # Sample data frame with your setup
    df = data_frame(INK_KEY = c(1:20), P_CODE = LETTERS[1:20])
    
    # Randomly generate some numbers in the INK_KEY range
    x = sample(1:20, 5, replace = F)
    # filter command in dplyr
    # If you want to keep this, which you probably do, assign it to an object
    filter_df = df %>% 
        filter(INK_KEY %in% x)
    
    # Assuming you have these as a list just read them into R. 
    # This code just shows you that dplyr will work with two data frames for your future use
    y = data.frame(x = x)
    df %>% 
        filter(INK_KEY %in% y$x)",1532927445.0
infrequentaccismus,"The easiest way to do this is to do an inner join between the data frame that represents the dataset and the data frame with the numbers you want to keep. 


(I apologize in advance for code typed Into a smartphone)

First, import both data frames like this:
    library(tidyverse)
    df <- read_csv(“your_dataset.csv”)
    df_filter <- read_csv(“your_list.csv”)

Then you can return the dataset you want like this:

    inner_join(df, df_filter, by = c(“col_from_df_with_ids” = “col_from_df_filter”))",1532919123.0
StephenSRMMartin,"Honestly, I love vim.

But RStudio is \*excellent\* for working with R. **You can use vim keybindings in Rstudio.**

Go to tools -> Global Options -> Code -> Keybindings: Vim",1532893434.0
green_tealeaf,"I use neovim with NVim-R and I absolutely prefer it to RStudio. I wouldn't hold yourself back from using NVim-R in this case as it's needless pain. If I didn't have a good R plugin, I would almost certainly be using RStudio (and sadly missing all my vim editing power).",1532893006.0
guepier,"I'll second the Nvim-R recommendation. It's excellent. And I actually think that Rstudio is very good as well, and its Vim bindings are better than in most IDEs.

But it's not the same (by a long shot) as regular Vim: there is noticeable lag after some keystrokes, and not all common Vim actions and movements are implemented correctly. And the slight differences between real Vim and Rstudio make it error prone to experienced Vim users — to the point that I have to disable Vim mode when I'm using Rstudio.

On the other hand, Nvim-R does *not* provide feature parity with Rstudio either. Personally I don't miss those features but be aware that support for debugging R code is somewhat better in RStudio. Same for working with R Markdown documents: Nvim-R actually supports this quite well but it doesn't support interactive Rstudio's R Notebooks. These don't offer additional *functionality* but they're a nice gimmick.",1532896128.0
hjkl_ornah,"I use to use Nvim-R, but have now switched to Iron. It’s perfect since it works for both Python and R. I only use RStudio when working on packages. ",1532910954.0
standard_error,"I switched from RStudio to Neovim with Nvim-R, and don't see myself switching back anytime soon.",1532946785.0
_Wintermute,"I use vim with vim-slime, tmux and rice/rtichoke.

* I can use any language with a repl
* It doesn't crash every few hours
* It works over ssh
* It's proper vim, not just keybindings (though RStudio's vim emulator is not bad)
* I don't use 99% of the stuff in RStudio",1532977480.0
Playblueorgohome,"I use nvim-r like a lot of people has mentioned, check my recent ish post on unixporn for dotfiles if your interested:)",1532898024.0
,"Interesting thread. I use to exclusively program in VIM prior to learning R, but have pretty much stuck with RStudio since I began programming in R. Lately I find RStudio to be rather clunky - like when I type it is all laggy like MS Word 2016. I think I'll give this neovim and nvim-r a shot.",1533076742.0
,"I work with R exclusively and only use neovim. Also I often advocate against Nvim-R and vim-R plugin. It's bloat with too many configurations in it, maybe for people who don't mind adding a hundred or so snippets/mappings to their editor that they never use.

Can you write more about the features you are missing from R-studio?",1533163285.0
,Emacs+evil+ess is my r toolkit. Extremely happy with it. ,1532961246.0
grumpymonk2,"No. I can see the appeal of vim, but only a sadist would prefer it to rstudio!",1532906847.0
SemanticTriangle,"While this is useful for a basic example, I'd be really interested in seeing some more advanced examples of variable selection and definition that have some kind of generalisable lesson to them. When one follows a 'good result' kaggle kernel, there's usually just some arcane step: ""I define this new variable and improve my ROC by 5%.""

What? Why? Why that variable? What gave you the idea? Why can't the algorithm run on the interaction of several other variables and discover that relationship? Why is it special?

And so on.",1532874747.0
efrique,The binomial GLM model is inherently heteroskedastic. ,1532859154.0
bwcampbell,Binned residuals are what you should be looking at if you really care.  ,1532861930.0
SSID_Vicious,"[Modern Dive](https://moderndive.com/) is a book which uses a Tidyverse approach. It isnt finished yet but far along enough to be useful.

[This course](http://www.math.smith.edu/~bbaumer/sds220-f16/syllabus.html) is also based on libraries like ggpot2 and dplyr. It has all materials, including labs and assignments, on the site. Uses the Openintro Statistics book.

[This course](https://beanumber.github.io/sds192/index.html) is an introduction to data science course, also with most assignments and labs and such available.

[Learning Statistics With R](http://compcogscisydney.org/learning-statistics-with-r/) uses regular R and is written specifically for psychology students, but hits most of the expected subjects.

There is also [this introduction to stats and probability course](https://02402.compute.dtu.dk/Frontpage) from a Danish university, with all materials, including the book, in English. 



",1532858065.0
wouldeye,Andy Field’s book is pretty good. ,1532872551.0
El-Dopa,I thought [Applied Predictive Modeling](https://www.amazon.com/Applied-Predictive-Modeling-Max-Kuhn/dp/1461468485/ref=mp_s_a_1_1/133-2235462-4630633?ie=UTF8&qid=1532915957&sr=8-1&pi=AC_SX236_SY340_FMwebp_QL65&keywords=applied+predictive+modeling) was great for practical machine learning lessons in R. ,1532916096.0
lakenp,"There are several open access R books listed here that could help you further 
https://paulvanderlaken.com/2017/08/10/r-resources-cheatsheets-tutorials-books/",1533069744.0
chocolateandcoffee,"I really liked [Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/). It's a free book, does go kind of in depth, but also fully explains all of the concepts rather plainly. There are also exercises to help you learn.",1532896426.0
efrique,"You may need a bit more basic theory (possibly at about the level of Wackerly, Mendenhall and Scheaffer) to make good progress.

What topics are you looking to cover? Regression? Generalized Linear models? Time series? survival analysis? etc etc? ",1532915249.0
sghil,"It sounds like it's an appropriate method - are you controlling for participant ID by using it as a random effect? 

It sounds appropriate enough and if you're not seeing any significant relationships between your variables then that in itself is meaningful - null results are still results and can sometimes be even more interesting. Has there been previous research that's shown a very clear link? Is there something different about your research that could explain the link not being evident? (etc.). Don't get suckered into just looking for something that shows p < 0.05 with some ***s next to it!

Also it's better practice to do the power analysis before starting collecting data so you know how much data you need to collect, but you can always do a power analysis and that should let you know if it's appropriately powered or not. ",1532807507.0
amstell,"How did you select your sample? Do you have a control and treatment group? If your identification is weak, which might be the case, you can't test your hypothesis. Or it could be your identification is fine but your hypothesis is not correct, which is a result too!  ",1532806123.0
malaise_forever,"More information might be helpful. What is your random effect, is it individual? What is your response variable, is it BAC? 

Typically you would choose your random effect(s) a priori, based on a study design choice.

If you suspect alcohol consumption being influenced by day of the week, why not include that as a fixed predictor in the model? 

Since you cannot transform the data and it is gamma distributed, a glm or glmm is likely your best bet. ",1532834918.0
Hoelk,"You are looking for the packages **dplyr** or **data.table**. **dplyr** has become really popular in the last few years and you should find a lot of examples and documentation for it. It's made by the same guy who made ggplot.

**data.table** fills a similar purpose and has been around for a long time, but it might be a bit harder to learn for beginners. 

p.s: You should avoid using `apply()` with `data.frame`s, though it really shines if you work with matrices or multidimensional arrays.",1532803944.0
another30yovirgin,"I know everyone's using dplyr now, but I really love plyr. It just seems really intuitive to me in the way that dplyr doesn't. And it fills in some of the gaps in the apply family (particularly splitting data frames). It's also faster than loops (though not light-years faster), and you can do nice things like progress bars and parallelization.",1532822795.0
willbell,"If you preallocate memory, often it is no faster to use an apply family function than to use a loop.  Apply is written in R so it is not much faster than a loop (compared to an equivalent function written in C++ which many vectorized functions like colMeans, colSums, etc are).  Like you say, loops are often just as clear, so don't be afraid to use loops.  Just make sure you preallocate memory first.

dplyr is written with a similar aim of following a general sort of philosophy of data entry.  dplyr aims to make it easy to put everything into a very particular format and work with data in that format (tidy data it is called).  I am less impressed, but if you liked ggplot2 you might like it.",1532819951.0
Darwinmate,Did you load the library \`car\`? Make sure you don't have another package that is masking or unloading \`ncvTest\`. ,1532780573.0
Crypt0Nihilist,"It's good that the source of the code for retrieving the text was linked. I was a bit confused skipping to the code to see two different styles. For instance, Michael Toth knows to use ""library"", not ""require"".

Sorry if I'm being daft, but where is the unsupervised learning?",1532783306.0
avamk,"Question for the pros out there:

Can generalised linear models be applied to answer this question? If so, how might one do it?",1532692712.0
Darwinmate,"What proportion test did you perform? 

I really like Handbook Of Biological Statistics: http://www.biostathandbook.com/linearregression.html

especially because the examples are great.

Because of the number of variables, I think a multiple regression suits your analysis (which is very similar to an ANOVA). The above site will give you information on how to interrupt the results.

A Chi-squared (or I prefer Fishers Exact Test) test of independence works well if you are a 2x2 table of variables. For example Male/Female X WoodBox/MetalBox.

Edit: Looking at your data, you might have to combined box type and size into a single variable to model the interactions in a reasonable way.",1532684211.0
sophiepeachie,"Managed to figure out how to do the Chi-squared test for species/site!

    >       Pearson's Chi-squared test
    > data:  bats$Site and bats$Species
    > X-squared = 573.22, df = 6, p-value < 2.2e-16",1532695034.0
guccibling,Um what other species are using your website?!,1532696095.0
efrique,"ANOVA would not be appropriate

chi squared might be okay depending on the model/hypothesis

You might be better with a generalized linear model suited for count data. ",1532764291.0
dtrillaa,"The correct syntax is

    thing <- “thing.rdata” %>%
      load()",1532654478.0
sparkplug49,"Rstudio is going to run server.R and ui.R when you hit run app so your data wont be pulled.  

I would put your data pull in server.R before the server function (or even at the top of it).  [This article](https://shiny.rstudio.com/tutorial/written-tutorial/lesson5/) will probably help a lot. If you really want the helper.R stuff separate that is fine but call it from server instead of from an app file. ",1532629803.0
tbdbabee,"One solution seems to be changing my directory structure to:

base folder:
    |-ui.R
    |-server.R
    |-global.R

and referencing source('global.R', local=T) in both files, while laso calling

    df <- read.data()

in both files, I had previously only been calling this in the server file.

",1532636641.0
cxavier,"Any reason why you're keeping the ui and server separate?

    library(shiny)

    # setwd(dirname(rstudioapi::getActiveDocumentContext()$path))  # set your working directory

    getData <- function() {
      # read the data file
    }

    # Set the ui section.
    ui <- fluidPage(
      # ui elements
    )

    # Set the server section.      
    server <- function(input, output, session) {
      # server code
    }

    shinyApp(ui = ui, server = server)

Typically I structure my shiny app with data, ui, server, all within one app.R file.",1532717157.0
sghil,"If you don't split the data, you can just use

> table(mtcars$vs, mtcars$am)

to get the table. ",1532616983.0
thaisofalexandria,"    mtcars %>% 
      split(.$vs) %>% 
      map(~pull(.,am)) %>% 
      map(~table(.)) <- alist

Then do what you will with the result in the variable alist.",1532616885.0
hurhurdedur,"Why not just use the `count` function  from the dplyr package?

`mtcars %>% count(vs, am)`",1532640801.0
tacothecat,"I absolutely don't recommend this, but you can do the following:

    mtcars %>%
    split(.$vs) %>%
    map(pull, am) %>%
    map(table) %>%
    {set_rownames(reduce(.,rbind), names(.))}",1532749683.0
byronhout,"You can model this with the multinomial distribution. You can think of this distribution as an extension of the binomial distribution, where you now have more than 2 outcomes (in this case, there are 3 outcomes).",1532582115.0
gammadeltat,"Uhh I'm not sure, but because the order of the games doesn't matter I think you can treat each game as an independent event. Then you just multiple the likelihood of each event (27 for jimmy, 33 for robert, etc) with the likelihood of each other event. I think you can treat it as JIMMY WINS **AND** ROBERT WINS **AND** ROBERT WINS **AND** TOMMY WINS **AND** TOMMY WINS **AND** TOMMY WINS. Unless the answer is to find the probability of each of the three scenarios you presented... 

If I'm wrong someone correct me!

I think your answer is just 
.27 * .33 * .33 * .40 * .40 * .40

E: I WAS WRONG.",1532578110.0
Alrik,"Not the right sub unless you're looking for R code to the solution, but basically, you'll construct a tree of all possible outcomes.

Multiply the odds of winning at each step (so, e.g., the probability of Jimmy winning two games in a row is 0.27 * 0.27 = 7.29%). Then add up the probabilities of similar outcomes (i.e., where they've won the correct number of games).",1532576811.0
bcrossman,11.3%,1532603380.0
zdk,"Is the problem just in compiling the C code and installing the package?

In that case, what may work is to use pre-compiled R packages. For example, try to install

    install.packages('purr', source='binary')

Though I guess if you're using a windows machine, this is already the default option.

You can also try installing precompiled R packages via the conda ecosystem. conda-forge has pre-built, up to date mirrors for most core CRAN packages.
https://anaconda.org/conda-forge/r-purrr",1532576995.0
denzelswashington,Used day length in geosphere -- looks similar. What are you trying to do?,1532571246.0
bwcampbell,DataCamp!,1532566510.0
oreo_fanboy,R4DS,1532576667.0
another30yovirgin,"You can't learn R in 14 days. That's absurd. If you could learn it in 2 weeks, why would I bother putting it on my resume?",1532561660.0
metagloria,R Primer.,1532555006.0
I_just_made,"Why do you have to know R within 14 days?  That seems very... specific.

Anyways, if you already know some programming and stats, I'd look at what the others have recommended; but if I were in your shoes, I wouldn't kid myself; 14 days is not enough to be competent in a computing language.  Take your time to learn it, the end result is good.",1533126829.0
Geothrix,Check out swirlstats,1532615777.0
millsGT49,"I renamed your `data` dataframe to `df` since `data` is used by R elsewhere:

    library(tidyr)
    library(dplyr)
    library(stringr)

    tidy_df <- gather(df, key = ""measure"", value = ""response"", -part_id, -auc) %>%
      separate(measure, c(""words"", ""statement""), sep = ""_"") %>%
      spread(words, response) %>%
      arrange(part_id, statement) 

Gives us

    head(tidy_df)
      part_id auc statement both happy happysad i sad they
    1       1 0.2      econ    0     0        0 1   1    0
    2       1 0.2       soc    0     0        0 1   1    0
    3       2 0.3      econ    1     0        1 0   0    0
    4       2 0.3       soc    1     0        1 0   0    0
    5       3 0.4      econ    1     0        1 0   0    0
    6       3 0.4       soc    1     0        1 0   0    0

Which is two observations per participant with each row either `econ` or `soc` response. 

Problems with this:

* Not sure what the `auc` value represents so I just included it with the `part_id`
* This is super ugly since each response is its own column, lets see if we can do better

code:


    tidy_df2 <- gather(df, key = ""measure"", value = ""response"", -part_id, -auc) %>%
      separate(measure, c(""words"", ""statement""), sep = ""_"") %>%
      arrange(part_id, statement) %>%
      filter(response > 0) %>%
      group_by(part_id, statement) %>%
      mutate(response_total = cumsum(response)) %>%
      mutate(response = factor(response_total, levels = c(1, 2), labels = c(""pronouns"", ""feelings""))) %>%
      select(-response_total) %>%
      spread(response, words) %>%
      ungroup %>%
      arrange(part_id, statement)

Which gives us

    head(tidy_df2)
    # A tibble: 6 x 5
      part_id   auc statement pronouns feelings
        <dbl> <dbl> <chr>     <chr>    <chr>   
    1      1. 0.200 econ      i        sad     
    2      1. 0.200 soc       i        sad     
    3      2. 0.300 econ      both     happysad
    4      2. 0.300 soc       both     happysad
    5      3. 0.400 econ      both     happysad
    6      3. 0.400 soc       both     happysad

Don't have time to comment everything but hopefully this showed you whats possible and you can tweak it to your specific needs",1532553438.0
featherfooted,"When I saw this post this morning, I thought ""man, I have way more other complaints than that"". I'm more often bothered by vague questions or unreproducible code examples, than I am by reposts. As long as the question fits the subreddit, then I'm fine with it. Syntax error? /r/rlanguage. Using a particular statistical package? /r/rstats. Setting up a workflow? /r/rstudio.

And so, I ignorantly looked away from this post and didn't comment. Then, later in the day, I stumbled on [this monstrosity](https://www.reddit.com/r/Rlanguage/comments/91v33z/recommend_the_book_to_learn_r_in_14_days_for/e30yc9j/). I'm sorry OP, you were right.

I would still stand by my statement - it is possible to put the same link in multiple places while adhering to the rule that says put the right things in the right places. Crossposting can help generate additional views and more readers, however discretion must be applied. Here's my expectations of a crosspost:

1. Do not duplicate thread content. Posting the same body in multiple places is bad, and it should just be one post in one subreddit and all subsequent posts are a *link* to that thing in the other subreddits (i.e. how /r/bestof would work). In this way, the ""monstrosity"" I linked is actually doing it in the technically correct way, but is abusing rule #2...
2. Make sure each crosspost fits the rules and expected content of each subreddit it gets reposted to. Think critically about which subreddits it best fits.
3. Even if it would fit multiple subreddits, refrain from posting to more than 2 subreddits at a time. This is my application of the DRY principle in this context.",1532563054.0
VincentStaples,"Alpha is a gradient by nature, so it's a bit weird to do with categorical data. There's no logical gradation for it to set alpha to, so col/fill would be more usual. There are use cases (ordinal data, or manually specified alphas if you have subgroups in your categories), it's just warning you that it's a bit odd I think. ",1532524665.0
spraynard,"Wouldn’t using your  p-values to set alpha accomplish the same thing? You could do 1-p so that the significant effects are darker, and the less significant ones are lighter. ",1532531362.0
_Wintermute,"It's just package authors being overly opinionated, feel free to ignore it.",1532547287.0
AnInquiringMind,"Assuming that your toy example is representative of your actual data, an easy way to do this would be to create a logical variable as the grouping variable:

    df %>%
    group_by(X < 4) %>%
    summarize(mean_Y = mean(Y))

This would group your observations by whether or not the value of X was under 4 (i.e., 1 to 3) and then provide the mean summary statistic by group.",1532483611.0
bvdzag,"The function group_by works with functions of variables in most cases, so you can write an expression like X >  4 & X < 6 and group_by will evaluate as TRUE or FALSE and group by the result.",1532483878.0
grasshoppermouse,"Put all the values in a data frame (with variables x, xend, y, yend) and then use `geom_segment`.",1532476605.0
AnInquiringMind,"What do you mean by ""better way to run""?

I mean, if your goal was to reduce code, then at first glance, you could replace the options x, xend, y and yend with vectors containing all elements. You could even move the elements to a dataframe and just call them by reference.

But not sure what it is you intend to do?",1532476193.0
shujaa-g,"[You posted this on Stack Overflow at the same time](https://stackoverflow.com/q/51508913/903061). Don't do that. It wasted people's time because you ended up getting the same answer from multiple sources. 

Use one forum at a time for help. If you are dissatisfied with the response from one, feel free to move on to another. But cross-posting at the same time should be avoided.
",1532480559.0
grasshoppermouse,"Your loop is replacing the data in `dataset` 49 times (not what you want).

Something like this might work for the first part:

    library(purrr)
    df <- map_df(file_list, read.delim)",1532467452.0
bubbles212,"Theres the sdf_quantile() function that you can use to get the median, maybe you can try using the spark_apply() function to apply the weights first? I'm on mobile and also a Spark noob, so I'd need some time to play around with this before knowing if it would actually work.",1532521391.0
KingDuderhino,It depends on where the bottleneck is but don't expect miracles. If it is some matrix calculation then Intel mkl may use more cores to speed up calculation but due to overhead doubling the number of cords doesn't mean doubling the speed. ,1532506702.0
iconoclaus,"can’t speak for mro, but independent iterative algorithms (loops) like bootstraps benefit enormously.  matrix math would see a bigger boost from using the gpu. ",1532626597.0
030Economist,"Hi Ryo, 

Thanks for the post. I managed to get gganimate working today after some [troubles ](https://www.reddit.com/r/dataisbeautiful/comments/90rnun/animated_arrest_data_for_narcotics_in_chicago_oc/e2slyks)with RTools for Microsoft R Open 3.5.0. 

In your experience, how capable is the anim\_save function of this package? In my case, even if I change the dimensions of the gifski\_renderer width & height parameters within the anim\_save function, the saved .gif always ends up being 400 x 400.  ",1532465715.0
TheArtilleryMan,"checkout laerd statistics, saved my life during a stats degree. Also data camp do a good r programming course.",1532468360.0
gnusmasa,"I think [Peter Daalgard's Introductory Statistics with R](https://www.springer.com/us/book/9780387790534#aboutBook)  might be what you're looking for. 

It has a nice mix of theory and extensive code examples.
",1532474605.0
SemanticTriangle,[The basic 6-sigma stuff is at least a good reference](https://datascienceplus.com/six-sigma-dmaic-series-in-r-part-3/).,1532440372.0
efrique,"You seek ""how to do chi-square tests, regression & anova, and manova in R""? Well, the first thing is of course the help:

`?chisq.test`
`?lm', `?anova`, `?aov`
`?manova`

You might find Quick-R helpful:

https://www.statmethods.net/stats/frequencies.html

https://www.statmethods.net/stats/regression.html   
https://www.statmethods.net/stats/rdiagnostics.html  
https://www.statmethods.net/stats/anova.html  
https://www.statmethods.net/stats/anovaAssumptions.html

etc. hope those links are useful to you

",1532441810.0
VincentStaples,"This is a fairly vague post, so here's some vague help:  
**ANOVA**  
summary(aov(y ~ x, data=df))   
**t-test (one kind of way)**  
t.test(df$var[df$group==1],df$var[df$group==2])  
**Regression**  
summary(lm(y ~ x1 + x2, data=df))  
**chi2**  
chi.sq(table(df$x, df$y))
",1532440207.0
Schrodingers-Human,"Ok I couldn't find a way to do it using html\_text() so I hacked this together:

    library(rvest)
    library(stringr)
    
    ppm <- read_html(""http://lyrics.wikia.com/wiki/The_Beatles:Please_Please_Me"")
    
    ppm %>%
        html_node('.lyricbox') %>%
        as.character() %>%
        str_sub(start=23, end=-39) %>%
        str_replace_all(""<br>"", ""\n"")

I tested it on a bunch of different songs and it seems to work.  You can mess around with the string replacement if you want something other than a newline character.",1532413844.0
revgizmo,Looking forward to seeing any responses. Keep us posted. ,1532380329.0
,"I first learned using “Hierarchical Linear Models: Applications and Data Analysis Methods” by Raudenbush and Bryk. This was in 2011, and I’ve used the methods a few times in my own research. Things may have changed. I know that one of the applications I’ve used multilevel modeling for (age period cohort analysis) has fallen out of fashion.

From my own experience the base concepts will start to make sense quickly, but problems with fitting the model can be hard to diagnose and solve. Unfortunately, I only have experience doing this type of modeling in SAS. I’d recommend checking that text out of a library if you get a chance, although I believe they use SAS and SPSS for examples.

You might have a hard time finding some R tutorials, but I wish you luck!

Edit: is your dependent variable continuous or fucked up?",1532384561.0
Slabs,Applied Longitudinal data analysis is a very accessible book for applied researchers (and code for different packages is available on the UCLA sire) as is Linear Mixed Models: a practical guide using statistical software.,1532383013.0
HimmelLove,"I have a resource, but I'm not by my computer. Send me a pm and I'll reply in ~1 hr. ",1532397403.0
questionquality,[Here](https://peerj.com/articles/4794/#)'s a really nice guide to the whole process of using multilevel models.,1532431119.0
gloverpark,"I think you may be using the wrong search terms here. What you have is good old fashioned panel data, and you need a course on how to work with panel data. Start with learning basics of fixed and random effects models, then you can move on to higher dimensional modeling ( ie adding the time dimension and/or interaction terms) from there. A graduate level Econometrics course may be sufficient for what you  need and you could probably start by looking at some videos on YouTube.

Ps. It would be a very similar situation to calorie count data, or time-use data which is popular in dev econ",1532387351.0
Quasimoto3000,How well does this scale? What are the best practices for multiprocessing requests and using promises?,1532387442.0
BillCarney,"[https://pages.rstudio.net/July25thPlumbingAPIswithplumber\_Registration.html](https://pages.rstudio.net/July25thPlumbingAPIswithplumber_Registration.html)

Companion webinar by Jeff on July 25th!",1532367795.0
zdk,"I'm not familiar with the moving average function here, but I image you can turn your list of matrices into a 3d array, and then `apply` the moving average function across the 'z' axis for each vector.

e.g. `apply(x, 1:2, function (x) ... )`",1532360276.0
atroiano,"I haven't used sparkylr in a while but you could create a weight DF in Spark join/bind that back to the table with the data to do the multiplication and then take the mean of that.

something like 

sdf\_bind\_cols(data,dim\_table\_with\_weights)%>%mutate(weighted\_avg\_col = data\_col\*weight\_col) %>% summarise(weighted\_mean = mean(weighted\_avg\_col))",1532358720.0
perfilibe,"You could just clone the source code from the package repo, add your modifications, load the package with ```devtools::load_all()``` (must be inside the package folder) and test it. If it works, you can run ```R CMD INSTALL .``` to overwrite the old version with the modified one.",1532313881.0
flyos,"To complete the responses, a (very very hackish!) solution to ""patch"" a loaded package on-the-fly is to replace the body of the function to patch with patched code.

Say I want to fix the function thisfunction(). I can access its code by typing
   
    thisfunction

Then do the edits I want, and save it to e.g. thisfunction.new:

    thisfunction.new <- function(args) {
       ...code...
    }

The problem is that this new function is not in the good environment and is the not the one that would be called by other functions of the package relying on `thisfunction()`. A solution to that is to ""plug"" the code into the package function by doing:

    body(thisfunction) <- body(thisfunction.new)

This, again, is **very hackish** and will only work for pure R functions. But it's quick and dirty enough to check your fix before looking at the whole source code and recompile of the package. Compiled non-R code (like e.g. C/C++) *will* require meddling with the source code and, well, recompilation, of course...",1532328347.0
questionquality,"[`devtools::install_local(""/path/to/package"")`](https://rdrr.io/cran/devtools/man/install_local.html)",1532340361.0
fflores97,"If you use Rstudio and have devtools installed, enable Build as part of the tabs where the environment is (top right pane by default). There is a button says Clean and Rebuild and it will re-install with your updates. This can also be done under Build at the top bar",1532314960.0
Darwinmate,"I think you're over complicating it. You don't need 3 scripts, you just need 1 that is split into different chunks. This is how I would do it:

* R can create folders using `dir.create()` 
* Put all input files in a folder named appropriately. 
* Create an `output_graph` and `output_csv` folders. 
* Put your script in the same location as `input`/`output_x` folders. 
* Write `R1` functions that will do what you describe, it takes input (eg: csv) and output location. 
* Write `R2` function etc. Use `here` or similar package to detect location of script and set workding dir (using `setwd` function) to this location. 
* Use `list.dirs` function to return a list of files in `input` folder. 
* Apply `R1` to this list.
* Apply `R2` to the outputs of `R1`

You can skip writing functions all together if you want and write a script that looks in `input` folder, gets a list of all files and applies R1. Once this is completed, R2 is executed.

IMO, you've done the hardest thing: thought what you needed to do. Start writing the R1 and make sure it runs on all your files. Don't worry about creating folders/moving things around. Just make sure the script works for 1 file.

Good luck!",1532306477.0
picardIteration,"I would save each output in a separate folder. Then set the iteration number as a variable (say iter), and add one each time. Then you can write to the file using write.csv(paste0(""iteration"", iter,""/file.csv""))",1532306062.0
,[deleted],1532209182.0
Darwinmate,"... where do you live where your local pawn shop uses probability statistics to model risk?

That's corporate level modelling. ",1532149883.0
MrLegilimens,"Lol. Called intuition modeling, or the GUT package. Not hosted on cran though.",1532172304.0
regionjthr,"I'd say they use the ""do you look like a crackhead"" method",1532203037.0
bvdzag,r/Rstats needs more good shitposts like these. Take my upvote,1532184665.0
geauxcali,What risk?  They have collateral worth way more than they are lending.,1532207950.0
ibepeer,RMarkdown is awesome! I also would recommend ReporteRs (https://davidgohel.github.io/ReporteRs/) as it also can format and build docx files. ,1532164135.0
vaguely_specific1,"I'm a little confused, where does it say how to do it in RMarkdown?  It mentions some packages and then a bit of visual basic code but not how specifically to do it.",1532189577.0
ColorblindChris,"Thanks for posting. I've been working on related issues this week. I posted a question on the blog, but I'll throw it here too:

Have you found a way to have these macros available in the docx file right after you create it in rmarkdown? I was looking for something like excel's personal workbook for Word the other day, but I came up empty. Or do add the macros each time you create a new doc? #stupidWordquestions",1532152001.0
,"I was about to say, I always just have a macro in the template I point to that I run after the document is done to fix all my figures and tables, and that is basically what the article says to do...",1532186517.0
Thaufas,"Earlier this week, I spent a full day writing a table parser for PDFs. I'm looking forward to trying Tabulizer.",1532157929.0
umbrelamafia,can you send a print screen?,1532109942.0
biledemon85,"Just a tip to help in future: 

If you have an issue you want help with, share any steps you took in order to replicate the issue. In this case a list of precise steps taken in the GUI, with screenshots of possible. For any written code sharing the code and datasets would be useful. You can edit your original post here on Reddit if you like.

It just means we can more easily help you and your more likely to get a useful response, although it does take more effort on your part.  

I'm afraid I don't have experience with R Commander and cannot help you in this instance. Good luck!",1532114913.0
ChoaYuraNaeun,"Based on your last error, I'd guess that you might not have some of the base R packages loaded.",1532120372.0
atroiano,Why don't you use R Studio?,1532113157.0
Kroutoner,"There's also tolerance intervals, the still lesser known and still wider sister of both",1532056588.0
masher_oz,"How can I calculate prediction and tolerance intervals for heteroskedatic data? I want to be able to draw a straight line through my data with upper and lower bounds representing the CI, PI, and TI.",1532080673.0
SemanticTriangle,Philosophy of data science question: what is a true mean? Is this merely the mean that you would measure if you could sample the entire distribution?,1532063503.0
Deto,Was the algorithm set up to take advantage of multiple CPUs?  Otherwise I wouldn't expect the more powerful instances to improve much.,1532068954.0
Darwinmate,"Would mom and pop shop have the data in an accessible format? I think this is the biggest issue you might face, how do you get access to the data? Do they track anything at all? ",1531962179.0
EveningTap,Not a bad idea. I've always wondered if you could arbitrage a subscription to DataRobot or something similar. ,1531949915.0
dtrillaa,I know some people who do this. Guy charges $100 an hour. I know it’s a viable business model because there are lots of people who would want this sort of information but can’t afford the major firms,1531957309.0
guerisimo,It’s certainly possible.  Most of your questions basically depend on your own preferences and tolerance for risk,1531961659.0
tikeshe,"require(rgdal)
shape <- readOGR(dsn = ""."", layer = ""SHAPEFILE"")

Try the above to see if it loads correctly.

If this doesn't work, then it may be due to your shapefile.

For more help, post the code you're using.

",1531944894.0
Ringbailwanton,"Can you try it with, 1. A shp file you know is properly formatted, and 2. Possibly link to a similar shapefile and include the code you are using?",1531942201.0
DeuceWallaces,Make sure it's a valid shapefile. You could try some of the other functions like sf::st\_read.,1531944827.0
lxs_10,"Should load the package first.

library(IATscores)",1531931187.0
d4rkride,"Look into the tidyverse, specifically using dplyr will help you filter and manipulate your data

    library(dplyr)
    
    ## Filter on Nets
    nba.BN = nba %>% filter( team == ""Brooklyn Nets"" )
    
    ## Filter on Nets, keep only c(""name"", ""salary"") columns
    nba.BN = nba.BN %>% select( name, salary )
    
    ## Can do this inline
    nba.BN = nba %>% filter( team == ""Brooklyn Nets"" ) %>% select( name, salary )
    
    ## Sort Nets data on salary then name
    nba.BN.sort = nba.BN %>% arrange( desc(salary), name )
    
    ## Find the top salary for each team
    nba %>% group_by( team ) %>% filter( salary == max(salary) ) %>% arrange( desc(team) )

Not having access to the same data.frame I can't guarantee it will all work, but it should be a close enough template for you to tweak to get the right results.

EDIT: Updated top salary filter",1531941559.0
adiamb,"try this data table way

`require(data.table)` 

`setDT(nba)` 

`nba[, .SD[which.max(salary)], .SDcol=""salary"", by=team]`
or
`nba[, list(max_salary=max(salary)), by=team]`",1531935841.0
votetrump_jk,"you want the `tidyverse` package, which has a `group_by` function that will do what you want. Check out this [cheat sheet](https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf)",1531933083.0
haroldcourt22,"1 


Thanks all for the response. With some googling I was able to find the solution using BASE R:


    v1<- nba1[nba$team==""Brooklyn Nets"",]
    v1
    v2<- v1[order(v1$salary,decreasing = TRUE),]
    v2[,c(""name"",""salary"")]



Is this the norm? That it requires this much code for a 2 step process-- filter (get 1 team in this example ""Brooklyn Nets"") AND then sort by highest salary?


Genuine question, as I am very new to R, so my honest thoughts are something along the lines of ""this is a lot of code and syntax to remember for 2 quite simple operations""



2


A lot of people suggested dplyr and I did a lot of reading on R and it was mentioned but I know nothing of it except that is it sort of 'add-on'. Now, I'm wondering -- do I learn how to get things done with BASE R or dplyr?...or both. Syntax looks simpler and more intuitive in dplyr, but would it be recommended to learn how to get things done in BASE R first?


Looking for some general direction. Thanks in advance again.



3


Is there a BASE R solution for the 2nd question in my original post: Find the top salary for each team 



For example, a table with the highest paid player from each team.
",1531943271.0
efrique,">  the intercept is significant while the other two years aren't since they are on either side of the intercept.

What is being tested is different. The intercept is significantly different from zero but the year-factor coefficients represent mean differences *from that intercept*. I expect the equivalent intercepts would all be significantly different from 0, but not from each other.


",1531890971.0
MrLegilimens,So how is this better than your competition - aka Swirl?,1531879345.0
IshwarHegde,Good job OP. Will henceforth recommend this to anyone seeking to learn R.,1531905567.0
doggie_dog_world,I spent a few minutes looking through the site and it's really well-done!  Congrats!  I'll be sure to share with others since I regularly get asked about R resources.,1531941357.0
FuriousFrodo,started with the basics. interesting so far. thanks for all the work you have put in to make this! :),1536170138.0
polished_iconoclast,"Cool project!

What would help students (and SEO) is a list of concepts and skills that your tutorial covers, e.g., you will learn: vector multiplication, string manipulation, etc.

I am viewing it on mobile. Maybe there is such a list in the web version.",1531920272.0
iCouldBeDoingDrugs,Any plan to add tutorials using ggplot?,1531959997.0
HappyEggplant,"Hi, I'm a complete newbie on R and I'm following your tutorials right now. Having a good time! Thanks for putting this up, I really appreciate your effort!",1538689655.0
Ader_anhilator,Is this for middle schoolers or elementary schoolers?,1531883073.0
jbuckets89,I’m a little confused. You are writing your own function that effectively just subsets complete cases and saves the files - why do you need a package for that ?,1531877473.0
throughthekeyhole,"I think... OP, you're learning R, and you're writing a blog post as a way to showcase what you're learning?I can't imagine using a function like this, but I'll add something I think will help with some understanding:

    cleanfile <- write.csv(cleandata, file = ""cleanfile.csv"", row.names = FALSE, na = """")

There's no benefit in trying to assign output to an object. \*cleanfile\* isn't actually used anywhere else in the function, which is good because it's just a NULL object. I see the same thing with saveRDS later on in the function. They'll still work, but the cleanfile <- does nothing of value.Also, after cleanfileread <- read.csv() you're using as.data.frame(), though the object you've read in is already a data frame. If you look at the help for read.csv(), the first line is:

>Reads a file in table format and creates a data frame from it

As a general rule, it's better when single functions do fewer things well rather than many things poorly. I understand the thinking behind having a single function that does all your data exploration, but it doesn't work well in practice, not least because data is not that pretty. For example, your function calls View() on every subset of data you create. For me, that would probably crash RStudio altogether given the data sets I work with, but even with a smaller data set there's not much benefit to printing \*all\* of the data separated into 6 different subsets. The closest you get to doeverythingplease() functions are things like describe(), which you use in your function.

I've never had much luck with Data Exploration packages... but maybe check out the DataExplorer package... someone posted that on reddit a few weeks ago, and though it's new and a bit rough (a lot of duplication of functions... i.e. same function with different names), I like a couple of the functions from that because each function does one nice thing, and I can pick and choose what I want for my data rather than trying to throw everything at it all at once. Are you using RStudio? If so, load DataExplorer and type DataExplorer:: to see the list of included functions. Type the function name to see the code and you'll see they aren't terribly complicated, but can still save some time. Some examples:

* BarDiscrete: This function creates frequency bar charts for each discrete feature
* PlotMissing: This function returns and plots frequency of missing values for each feature
* HistogramContinuous: This function creates histogram for each continuous feature",1531881643.0
Volatilityshort,What is the variables don’t load as the right type and need to be recoded?  I encounter this nearly every day at work. ,1531880040.0
YepYepYepYepYepUhHuh,"Your code to generate your dataframe resulted in some NAs for me (probably because of the 00-00 month-days) and also didn't match the data you have up top, however something like this could work. It's a bit messy because of the NAs, and there's probably a neater way of doing this, but it should return a list of Days that are not present in the dataframe that are between the first and last days listed in the dataframe.

    df %>%
    mutate(start_date = as.Date(start_date), end_date=as.Date(end_date), 
    row=seq(1:21), Date=as.Date(NA)) %>%
    group_by(row) %>%
    filter(!is.na(end_date)) %>%
    complete(row, Date=seq.Date(start_date, end_date, by=""day"")) %>%
    ungroup() %>%
    select(Date) -> dbDates

    df %>%
    mutate(start_date = as.Date(start_date), end_date=as.Date(end_date)) %>%
    summarise(minDate=min(start_date), maxDate=max(na.omit(end_date))) %>%
    mutate(Date=as.Date(NA)) %>%
    complete(Date=seq.Date(minDate, maxDate, by=""day"")) %>%
    select(Date) -> allDates

     anti_join(allDates, dbDates)

edit: formatting",1531857756.0
halhen,"Not the most efficient code perhaps, but as long as you're in reasonable volumes of data, the below might work for you?

    library(tidyverse)
    
    data <- tribble(~start_date,  ~end_date,
                    ""2006-04-01"", ""2006-09-30"",
                    ""2004-04-01"", ""2004-09-30"",
                    ""2007-04-01"", ""2007-09-30"",
                    ""2003-04-01"", ""2003-09-30"",
                    ""2001-04-01"", ""2001-09-30"",
                    ""2008-04-01"", ""2008-09-30"",
                    ""2002-04-01"", ""2002-09-30"",
                    ""2009-04-01"", ""2009-09-30"",
                    ""2010-04-01"", ""2010-09-30"",
                    ""2010-10-01"", ""2011-07-01"",
    ) %>%
      mutate(start_date = as.Date(start_date),
             end_date = as.Date(end_date))
    
    with(data,
         tibble(date = seq.Date(from = min(start_date), to = max(end_date), by = '1 day')) %>%
           filter(map_lgl(date, ~ !any((. >= start_date) & (. <= end_date))))
    )
",1531861446.0
pagan_sinus,"If non overlapping, a data.table solution looks like

    # change to a data.table
    library(data.table)
    setDT(df)
    df[,start_date := as.Date(start_date)]
    df[,end_date := as.Date(end_date)]
    # order by start date
    setkey(df, start_date)
    # add column for previous end date
    df[,prev_end_date := shift(end_date)]
    # find any gap
    df[,gap := as.numeric(prev_end_date - start_date, units=“days”)]
    # aggregate by year
    df[,.(“missing_days” = sum(gap, na.rm=T)), by=.(“start_year”=year(start_date)]",1531865888.0
Darwinmate,"`?` means 0 or 1. Said differently: once or none at all. But `*?` means something slightly different in regex.

Use a regex builder like https://regexr.com/ (also checkout the cheat sheet) to see the difference visualised. 

Here is a stackoverflow answer on `*?`:
https://stackoverflow.com/questions/3075130/what-is-the-difference-between-and-regular-expressions

`*?` is what they call a lazy quantifier. It will match as few characters as possible. eg:

Searching in `101000000000100`
`1.*1` will match `1010000000001` while `1.*?1` will match `101`.

Also the use and meaning of `?` varies. eg:

    (?:abc)     non-capturing group
    (?=abc)	positive lookahead
    (?!abc)     negative lookahead",1531828422.0
RememberToBackupData,"`.*?` is one of the holiest patterns gifted to us by Our Lord Kleene. It is probably the best way to capture any kind of text before/after/between landmarks. 

In the case of your regex, it’s looking for names with the pattern “Doe, Mr. John” or something, right? The landmark that `.*?` is matching is “anything between “, “ and “.”, so it’s no surprise that it gives you a bad result when it comes upon a name like “Mrs. Martin, Elizabeth L.”",1531829587.0
4rtemisia,"I can also recommend [regex101](https://regex101.com/) as an intuitive online tool that helps you (or at least, helps me) understand exactly what is being selected and how different expressions work on different inputs. I write all my regex for R there and then modify it for R (usually by adding backslashes until it works lmao). I prefer it to regexr.",1531829974.0
emiltb,"Depending on what you are doing, you might save yourself a lot of work by looking into the lubridate package. Hadley has a nice chapter on it in R for Data Science: http://r4ds.had.co.nz/dates-and-times.html",1531835527.0
CappyFlowers,You should look into the Chron package if you have to do much  with the time/date. It makes dealing with time so much easier.,1531832517.0
mbillion,"In general this is likely caused by not using a recognized format.  Make sure to encode all fields using a standard format.  R can read most of the accepted ""standard"" formats",1531838104.0
mighty_conrad,"Looks like R recognized field as a string, check up levels of result factor. ",1531855041.0
insufferablemoron,"Seems to be because I had the excel format in a custom “hh:mm” format. Changing it to excels ‘time’ format has worked although im not sure if this is going to work how I want it too.

Anyway thanks everyone who had a look",1531827357.0
BustedEchoChamber,Oh boy. Well you're in luck there are a ton of ways to do that. There are a whole suite of tools in R based on what's called GDAL (for geospatial data abstraction language). Off the top of my head you could look into the package sp & its vignette. Convert your GPS data to a spatialpointsdataframe and go from there.,1531807350.0
ashwinmalshe,"If I understood your question correctly, you are asking why we need to specifiy .SDcols = c(""x"",""y"",""z""). If you drop it, effectively you are running:

    DT[, c(lapply(.SD, sum), .N), by = x]

When I ran it, I get the following output which isn't the same as you have written above.

    > DT[, c(lapply(.SD, sum), .N), by = x]
        x  y   z  N
    1: 2 26 30 4
    2: 1 23 26 3

But it is almost exactly what you want. The reason why you don't get the second column for x in this output is because when you use ""by"", data.table calculations are not done on the variables in ""by"". However, your exercise requires that you report the values for ""x"" as well, which is specified in ""by"". Therefore, you need to declare the columns that you want to return using .SDcols.   ",1531787767.0
triple_dee,"I'm probably not the best person to solve this for you but I'll try to explain a little bit.

If you have a list in `j`, data.table recognizes each element as a column. When you do `.(lapply(.SD, sum), .N)`, you're essentially returning a list(list(), .N), which is why I think you get the long table. data.table seems to be taking the lapply result as one element and trying to make it all one column.

With c() you can avoid that unintended consequence. How it happens I don't really understand, but perhaps someone else somewhere might have better understanding. For what it's worth the c(lapply(), [value]) exists in the [data.table documentation](https://rdocumentation.org/packages/data.table/versions/1.11.4/topics/data.table-package). ",1531786722.0
master_innovator,I would not use this post to try and get a job... it will likely have the opposite effect.,1531789515.0
umbrelamafia,"I would use an excel template, or even better, a google form, since I could predefine the field types (text, number yes/no) and avoid input error. Also, I would save everything in a simple database like sqlite and just insert the new data using a package like DBI.
",1531801387.0
Babahoyo,+1 to using a google form. Then you can use the package `gsheet` (or something like that) to download the data from there. I am currently using it for one of my workflows.  ,1531840049.0
chonggg511,How about setting up a desktop that you can ssh into and doing the analysis using rstudio server?,1531730049.0
polished_iconoclast,"For R:


If you do processing and analysis locally, and don't parallelize (e.g., using doParallel or foreach), then I might need to prioritize clock speed. You could install R on a USB stick and benchmark laptops in the Apple Store to estimate performance per $. 


If you parallelize, then clock speed is less important.


GPU doesn't matter that much unless you use some esoteric package that compiles to GPU code. 


Personally, I would certainly get 32 GB (this is what I am using now) to avoid any slow downs due to swapping, and see if you can afford getting a faster CPU. Benchmarks would tell you if a faster CPU makes sense. My feeling is that running analysis in the cloud is more feasible then getting a faster CPU.
",1531747320.0
Darwinmate,"The only piece of software that really needs power is photoshop, but are you doing real photoshop work or are you modifying graphs/plots? You should use illustrator for that.

Either way, a macbook would do the job but might struggle a little bit with photoshop. The rest will work fine. 

16gig of ram is huge amount imo. id see if you can get dedicated GPU for photoshop.",1531733268.0
joetheschmoe4000,"Per the Wikipedia article on confidence intervals:

> A 95% confidence interval does not mean that for a given realized interval there is a 95% probability that the population parameter lies within the interval (i.e., a 95% probability that the interval covers the population parameter).[10] Once an experiment is done and an interval calculated, this interval either covers the parameter value or it does not; it is no longer a matter of probability. The 95% probability relates to the reliability of the estimation procedure, not to a specific calculated interval.[11] Neyman himself (the original proponent of confidence intervals) made this point in his original paper:[3]

The article says that a 95% CI means there's a 95% chance that the interval will contain the true mean. 

Now I understand that in a normal distribution, the confidence interval and credible interval are the same so you CAN make that statement, but I think for the purpose of introducing people to the idea of CIs it's a good idea to make this distinction to avoid confusion. ",1531694763.0
RememberToBackupData,"## Installation

**From CRAN:**

    install.packages(""librarian"")

**From GitHub:**

    install.packages(""devtools"")
    devtools::install_github(""DesiQuintans/librarian"")

## A quick tour of _librarian_

|      Function | Example                                  | Description                                                                        |
| ------------: | :--------------------------------------- | :--------------------------------------------------------------------------------- |
|     `shelf()` | `shelf(cowsay, DesiQuintans/desiderata)` | Attach packages to the search path, installing them from CRAN or GitHub if needed. |
|   `unshelf()` | `unshelf(cowsay, desiderata)`            | Detach packages from the search path.                                              |
|   `reshelf()` | `reshelf(desiderata)`                    | Detach and then reattach packages, helpful for refreshing a personal package.      |
| `lib_paths()` | `lib_paths(""C:/new_lib_folder"")`         | View and edit the folders where R will install and search for packages.            |",1531655546.0
bmkred,"Well done, I look forward to using this.",1531685856.0
king_curry,This is awesome! What differentiates between this and [pacman?](https://cran.r-project.org/web/packages/pacman/index.html) If you're looking for any help whatsoever with this package I'd love to contribute/work on this with you :),1531695385.0
wouldeye,this. this is a useful package. ,1534552581.0
ran88dom99,Ignore the stupid picture. This is the best explanation package in R. ,1531675470.0
LiesLies,Great integration with caret! I'll give it a spin.,1531777362.0
mulderc,I have been looking forward to a book like this for some time. Can't wait to dig in. ,1531701775.0
KingDuderhino,https://cran.r-project.org/web/packages/plm/index.html,1531611987.0
craoloro,Google for glmmTMB,1531613256.0
030Economist,"Check out the [lfe package](https://cran.r-project.org/web/packages/lfe/index.html). 
It achieves the within-transformation by using the demeaning process, rather than using dummy-variables. 
This is great if you have a dataset with a high number of levels for your fixed effects. 

There is also the [bife package](https://cran.r-project.org/web/packages/bife/vignettes/bife_introduction.html) for modeling fixed effects in probit and logit models. ",1531722310.0
iconoclaus,"Thanks for posting your exploration. I don't do as much predictive work as I'd like to, so I have a couple of questions for you:

1. Is the goal of this project to understand what factors are most important, or to predict the outcome accurately? I ask because you seem to conflate the two by using predictive methods (holdout/training, confusion matrix) while simultaneously trying to interpret coefficients.

2. I'm not yet clear what the value of interpreting a decision tree is — they are notoriously unstable (unlike regression techniques). Have you tried a single resample to see if the tree remains the same? I realize that the random forest attempts to bootstrap results and produce more stable results, so why not go straight to the random forest?",1531581054.0
Hadamard1854," So, there is this: https://www.youtube.com/channel/UC_R5smHVXRYGhZYDJsnXTwg",1531514567.0
samclifford,"A lot of the presenters are posting their slides and sharing on Twitter, in case the YouTube video isn't high quality enough for you. ",1531565983.0
shujaa-g,"This is a precision problem. Have a look at [Why are these numbers not equal?](https://stackoverflow.com/q/9508518). This is a deep concept that effects all programming. Read it, think about it, and read it again to make sure it sinks in.

An easy solution in this case would be to set a tolerance, say `tol = sqrt(.Machine$double.eps)` and use `dat$painTE20 + tol <= -20`. Or perhaps `round`ing `dat$painTE20`, depending on your needs.

Seeing your `.6666...` decimals, I'm guessing you are computing these numbers and dividing by 3 at some point? A robust and exact solution would be to put off the division, keep things in terms of integers for comparisons.",1531496774.0
Hoelk,"a quick check of the github page shows that multiplyr hasn't been updated in two years. That is a very long time considering how much has changed in dplyr during this time. Looks like a pretty abandoned project to me. If think you should stick to **future** for now, it's already great and unter active development. The only advantage you would get from using something like multiplyr is (probaly minor) syntax sugar, not worth depending on an abandoned package that's not on cran anymore (apparently it was on some point).",1531519979.0
CadeOCarimbo,You can directly ask him on Twitter or Rstudio Community. ,1531499041.0
DataPseudoscientist,/u/hadley,1531495933.0
TeslaIsAdorable,"I've used both, but furrr/future is much easier to use imho. ",1531530533.0
kazi1,Just use it. Just because something isn't on CRAN doesn't mean it's bad (all CRAN does is check that a package builds correctly...).,1531499259.0
Ader_anhilator,Is it faster than data.table?,1531498327.0
FitzKaos,"I would also like this, for similar reasons.",1531496371.0
RememberToBackupData,"He’s probably already trying to get it there, but it takes a long time. It took me three submissions over like 1 month. ",1531516591.0
ashwinmalshe,I’ve never used Chinese characters so can’t comment on what might be going wrong. But I know that it’s not easy to show Chinese characters on plots. Check out showtext package which can tackle this: https://github.com/yixuan/showtext/blob/master/README.md,1531487800.0
LogicalRisk,"guess\_encoding() likely spit out that error because you passed it something that was not a file. 

Using your sample file 

    library(readr)
    
    guess_encoding(""path/to/YOURFILE.csv"")
    
    # will spit back 
    # encoding   confidence
    #  <chr>           <dbl>
    # 1 ISO-8859-1       0.37
    # 2 ISO-8859-9       0.28
    # 3 ISO-8859-2       0.23
    
    # Try with the appropriate locale encoding
    data = read_csv(""path/to/YOURFILE.csv"", locale = locale(encoding = 'ISO-8859-1'))

should work to read in the data. 

I'm not sure what you mean by merge in this context, but if you are trying to join two different files, make sure both of them have the right encoding as that example code. ",1531441250.0
VincentStaples,"Assuming you have 80 matches... 


df <- subset(df, matchno > 50 )  
**Correlation**    
cor.test(df$victory, df$aces)  

**Regression**  
summary(lm(victory ~ aces + shoesize, df))",1531450410.0
LogicalRisk,">I target tennis players. I want to know if there is a correlation   between a tennis player's victory and the number of ACEs made during his   last 30 matches. Can you help me write this in R?

People are willing to help when you post a reproducible example **and** what you have done to try and solve the problem. A reproducible example would include some example data, in your case your data on tennis player(s). You have done neither, which makes it nearly impossible to provide assistance. 

>What distribution should I choose?  Poison? What to put on the axis  of  the abscissa and what to put on the axis of the ordinates? What are  the  variables in this case? How to write this in statistical terms? what   distribution to use?

In the nicest way possible, this sounds like you do not actually know what these terms mean. Correlation is defined as covariance(x,y)/(standard deviation of x \* standard deviation of y). You do not need a distribution for that. 

The rest of these questions are all questions you should answer about your own data. They are not rstats specific question. 

>ALSO: How to have a model that predicts what would happen if the number  of aces were below a certain threshold? Thank you so much if you could   answer to me

Google is your friend. The search term that will help to start with is lm() R. I would also recommend working through online resources. Hadley Wickham's ""R for Data Science"" is free and quite accessible. ",1531423024.0
iamdelf,"I have done this before with apache, nginx and haproxy.  The key parts are you need to set things up as a reverse proxy, the server needs to rewrite URLs, and you need to change the timeouts for open connections.

These configs from the Rstudio people are a good starting point https://support.rstudio.com/hc/en-us/articles/227449427-Running-RStudio-Connect-with-a-Proxy

For the https part, you need a certificate which probably will mean getting one from letsencrypt.  Before you can do that, you will need a domain that you can use.  You can have a look at this example.  https://gist.github.com/cecilemuller/a26737699a7e70a7093d4dc115915de8

If you really want to just do it locally, you'll have to set up a CA(have a look at cfssl).  Making a CA will be like this: make a cert and self sign it.  This is now your own CA.  Make a certificate and CSR on the server, have the CA sign it and then distribute the CA's certificate to every machine you want to use Rstudio Server on.",1531412270.0
Caswe,Have you looked into using Traefik? I use that with Letsencrypt to serve a Rstudio docker. I could never get going with nginx but found Traefik quite easy.,1531428835.0
DoItForTheGild,DataCamp has an in-browser R prompt where you practice the code learned in the lesson. ,1531402725.0
BillCarney,Still in alpha but a very helpful spot to learn. [https://rstudio.cloud](https://rstudio.cloud),1531410427.0
PaperCrepes,"Install it on a flash drive? That's what I did to run R at my work. Though we do have R available for other departments, but I don't have access to it. It runs from the flash drive well. ",1531406132.0
pagan_sinus,"Anaconda can be installed locally (by default I think it installs in your user documents folder). If you then do

`conda install r jupyter r-irkernel`

You should be able to run jupyter notebooks, which are a pretty decent alternative to Rstudio. There’s also jupyter lab (would have to install another package), which is a bit closer in look and feel to rstudio. ",1531410723.0
Nschnock,"Install it on a digital ocean droplet (or amazon), for  a few bucks a month",1531405491.0
bvdzag,"Any chance you could convince your employer that learning R will make you more valuable to the company? Find some enterprise applications, pitch how you would implement in the context of your work, and request R Studio for your box. Might give you a bit of extra motivation when teaching yourself too. I always feel I learn better with a specific application in mind.",1531404299.0
dm319,"R can be installed without administrator privileges on Windows machines that I've tried it on, but RStudio can't.",1531409149.0
rflight79,"So u/PaperCrepes already mentioned installing R to a flash drive, a google search also brings up Portable versions of [R and RStudio](https://sourceforge.net/projects/rportable/) that I'm guessing run under the portableapps.com framework, and there is also an installer less version of RStudio (see the zip/tarball versions at the bottom of the download page).",1531412202.0
huessy,"I think you can use www.jupyter.org. They say they have R support, and I've seen it in action with Python. 

Basically an in-browser IDE, but I'm not sure if you can get all the same outputs as with RGUI/RStudio (plots, interactive things, etc.)",1531403373.0
jackbrux,Or just give yourself admin rights [https://imgur.com/gallery/H8obU](https://imgur.com/gallery/H8obU),1531412792.0
Kennedy76112,"So I found this just on my phone, haven't tested it but worth a shot

https://rstudio.cloud",1531402695.0
DataPseudoscientist,It's a long shot but do they have Docker?,1531402802.0
drmissmodular,"Good on you! Could the EBI Rcloud server work for you? I used to use this quite a lot:

[http://www.ebi.ac.uk/Tools/rcloud/](http://www.ebi.ac.uk/Tools/rcloud/)

I also hate to miss an opportunity to introduce folks to [swirlstats](https://swirlstats.com/students.html), which looks like it has tutorials to run outside of R studio. Though I really like Rstudio, so if you can run this on the web, or from a flash drive, your experience will be better. ",1531413227.0
joetheschmoe4000,"You could set up an RStudio server on your home computer and access it through a browser. Alternatively, you could configure a Jupyter notebook server and create notebooks with an R kernel. ",1531418536.0
smalldjo,have you tried to download the zip version of Rstudio?,1531423442.0
CohoCharlie,If you absolutely have to you can use Python's Anaconda for this. If you install just for your account you won't get promoted for an Adminstrator password on Windows.,1531425382.0
xubu42,"Have you tried asking your IT team to install R/RStudio on your computer for you? I've had luck in the past just nagging the IT team until they do it to shut me up. 

Otherwise you can install RStudio on a USB drive and have a portable setup. You'll have to be careful about where you stash your packages and data to not overfill the USB, but you should be able to run the software without much issue.

https://support.rstudio.com/hc/en-us/articles/200534467-Creating-a-Portable-Version-of-RStudio-for-a-USB-Drive

I don't believe there is an RStudio as a service option outside of RStudio Server Pro (probably due to licensing). If you can install a text editor like VScode or atom and R, then you don't actually need RStudio -- you can get by with the plugins. If you can't install anything, then I'd look into DataCamp.com as they have a lot of interactive R tutorials. You can try a few free ones and see if they help. ",1531448854.0
mm8999,"Yup, look at ngram package",1531347509.0
guepier,"That’s a bit too little information. That said, if you copied the error message verbatim then the problem is one of spelling: {Rcpp} is spelled with a capital R, and package names are case sensitive.

On the other hand, the error message you’re encountering is usually triggered by packages that are missing a `NAMESPACE` file. And {Rcpp} is *definitely* not missing that file.",1531328309.0
fonzy6,Can you copy and paste the code from R? That may help.,1531349007.0
lemur78,"Something like:

    library(lubridate)
    library(stringr)
    string <- ""Fri Nov 13 23:22:13 +0000 2015""
    ymd_hms(paste0(str_sub(string, -4, -1), "" "", str_sub(string, 5, -6)))

gives:

    [1] ""2015-11-13 23:22:13 UTC""",1531301696.0
_Wintermute,"This is easy enough in base R.

    x = ""Fri Nov 13 23:22:13 +0000 2015""

    strptime(x, ""%a %b %d %T %z %Y"")

Returns:

    ""2015-11-13 23:22:13"" 

To apply this to a whole column:

    df$new_datetimes = strptime(df$old_datetimes, ""%a %b %d %T %z %Y"")",1531380656.0
webbed_feets,I was that person! This is great. Thank you ,1531666632.0
TroyHernandez,"There's nothing inherent in R or Python that makes one or the other better for deep learning.  In the end, they're both just wrapping C++ code.  The difference is in the size of the community.  

Python folks tend to be more engineering savvy and R folks tend to be more statistical.  Understanding deep learning requires no more than an undergrad level of mathematics, but it used to require some engineering chops... so that appealed to the Python folks and that community grew more quickly.

That said, I've seen multiple times where more in depth statistical understanding is super useful for deep learning projects so I would encourage you to join the R community in pushing this forward.

There are two paths of note:

1. [Tensorflow in R](https://tensorflow.rstudio.com/)
Rstudio put a ton of effort into this and it works well!
The downside is it calls Python which then calls the C++.  This can create a mess of Python environments, but if you don't care about your Python environments, it doesn't really matter.  The upside is that it's well supported and works.

2.  [MxNet in R](https://mxnet.incubator.apache.org/api/r/index.html)
The downside is that it used to be less well supported, but it appears to have come a long way since I last looked.  The upside is that it directly calls the C++ from R, as it's being maintained by the Rcpp core team.  That means the code feels more R-like and is a bit more straight-forward to debug.",1531265642.0
umbrelamafia,"R-keras runs on Python via the reticulate package. So in the end you are ""two layers away"" from the core C++ implementation. Is it wrong to use a wrapper for python? No. Do wrappers suffer from delayed upgrade and increase the probability of bugs? Yes.
In one day of reticulate use I discovered that reticulate converts dates to int64 when transferring an object to Python. It's not convenient.",1531343523.0
dtrillaa,"JJ Allaire has put great effort into bringing the Tensorflow and Keras library’s to R. As always the only benefit to doing it in python is it’s easier to productionize your models. That being said I’ve built models in both and there is little to no difference.

If you’re interested check out the Deep Learning in R book by Allaire and Chollet (the creator of Keras)",1531279733.0
syrahshiraz,"If you're currently working with the R stack, you should try out the R-TensorFlow ecosystem (specifically [https://keras.rstudio.com/](https://keras.rstudio.com/)) to see if it fits your needs. Addressing a couple other comments:

1. There's no difference in ease or performance in model deployment, because the saved models are the same; the model serving service doesn't know/care how your model was fit. (As an aside, this is also true for other ML frameworks, such as h2o and Spark.)
2. MXNet isn't used much outside of Amazon, and the R API for it isn't at parity. Much development lately has been focused on the gluon interface but whether it'll gain traction is yet to be seen. TensorFlow/Keras, on the other hand, are much more popular https://twitter.com/fchollet/status/971863128341323776.

There's value in staying in the ecosystem, especially if you're integrating with shiny or rmarkdown, and if you/your team is more comfortable with R. Even if you're doing (applied deep learning) research, the R APIs for TF and Keras are at parity (modulo a few days lag at new releases), so you wouldn't be losing functionality. That said, the R deep learning community is currently smaller in numbers, and the code accompanying new research today is going to be in Python. PyTorch, which is preferred by some researchers, is only available in Python. Also, if you want to work at a self-driving car startup, they're going to be Python shops. ",1531319055.0
edimaudo,There are a few libraries but if you do want to do deep learning you should go with python libraries,1531261956.0
Quasimoto3000,Check this out: https://raw.githubusercontent.com/rstudio/cheatsheets/master/keras.pdf,1531322477.0
I_just_made,"Absolutely not! It just depends on what your goals are.

Keras now has a package in R, which makes the creation of neural nets with tensorflow much more ""intuitive""; Another package, H2O, works pretty well and tries to make it more like a gui.",1531398623.0
efrique,"https://cran.r-project.org/bin/windows/base/R-3.5.0-win.exe

It's odd that there doesn't seem to be a direct link to it that I can locate.",1531264266.0
too_many_splines,"[This practical guide](https://github.com/dselivanov/text2vec/blob/master/docs/similarity.Rmd) from the excellent `text2vec` package should give you a place to start. In my experience, I get the most sensible results by comparing the LDA-generated topic distributions of each document using some similarity measure (like Jensen-Shanon)",1531263302.0
dungareejones,Google the title and pdf and you can read it ,1531253678.0
infrequentaccismus,Caret can call a variety of svm packages. You have all the same control that you have built into any package you prefer. I suggest looking at the caret vignette online. You can search svm and see all the packages it can call. From there you can assess which one fits you needs. ,1531236840.0
Digging_For_Ostrich,"R is much better at data flows, statistical processing, modelling, flexibility, and integration with other advanced analytical products.

Excel is easier to use for the vast majority of simple data needs an office worker has and allows people to manipulate data, but you go into your office tomorrow and ask them to download and join millions of rows of data from multiple sources, script repeatable transformations and conditional feature extraction, build a series of predictive models explaining that data, put together a neural network, then produce publication quality graphics explaining it all, and you’ll see why all of that is easier in R.

Excel is a spreadsheet for simple data manipulation.

R is an statistical engine that can power or be integral to huge data processes.

They are aimed at different groups of people. R will never be in as common usage as Excel, and Excel will never be able to do everything R can.",1531170904.0
dm319,"One reason is errors/reproducibility.  It's easy to make a copy-paste error in excel with a formula that can be hard to spot, and it's also hard to reproduce the process - as usually copy/pasting of data and formulae is required to analyse data.

With R you can always go back to the raw data and script file and check what you've done.",1531174846.0
sparkplug49,"One thing I've said to people in the past is that for most tasks, Excel puts the thing you dont care about in front of the thing you care about and R does the opposite.  Most excel sheets I've ever used are just endless fields of numbers that I could not possibly ingest in any meaningful way until the 'end' when they are properly summarized or graphed.  But usually those calculations are too complicated to do in one formula so they are split into a bunch of columns doing different things.  But what I really care about is what is happening to the numbers and what the end results are.  With R you see what you care about (what is happening) and then publish the end result.  

Also Excel occasionally does weird things, like changing values of cells without telling me (usually with dates).  I'd be fine showing me what is there in a different format but if just opening and saving a document causes changes that I didnt make, that is a huge problem from a data integrity standpoint.  

The biggest reason is probably reproducibility and just the fact that R is designed to be a data science workhorse from the ground up where excel is a ton of different things that are not exactly its core competency which leads to people using excel sheets as databases etc.  ",1531171769.0
huessy,"Excel is like a Swiss army knife. It has a bunch of tools all in one package but those tools are limited for the sake of mobility.

R is like owning a tool factory. You can make custom precision tools for any event, you just have to know the way to make the tools before you use them.",1531172909.0
Eleventhousand,"There's a tipping point where implementing something in code (R) instead of a GUI (Excel) becomes more efficient.  This is not just an R vs. Excel thing.  You could say the same thing about a bash command prompt vs. a file explorer.  

Anyways, if you want to run a regression analysis one time against a single spreadsheet, it may be fast to do it with built-in Excel functionality.  However, if you want to do the same against a bunch of csv files, it's much, much faster to do this in R.

Also, R is better for reproducible research - you can code up a whole solution including downloading a file, performing data cleansing, analysis and visualization all within a single R script.  To do the same with Excel would require a VBA macro, which at the point, Excel becomes just your IDE and runtime container anyways.",1531188594.0
030Economist,"When I did my MSc thesis I came from a STATA/SPSS-and-Excel heavy background. I immediately started learning R for a number of reasons. 

A: Excel simply couldn't handle the dataset I required for my hypothesis. The basic version of the dataset I had collected was over 5 million rows. 

B: SPSS was extremely picky about merging my datasets to expand upon it with new variables. It requires matching keys being sorted in alphabetical order, and it was slow to boot. 

C: I knew that despite the higher learning curve, I had six months to write my thesis and this learning curve would payoff with time savings in the end. 

Merging datasets? 
- Done with with one line and under a minute. 

Collecting weather data for 100s of airport codes in my data?
 - Just provide a list of the codes in my data and go through this list by making use of a loop to feed into a scraping function. 

Making multiple graphs in the APA-theme? 
- Just complete a theme in ggplot2 and change a few lines to the respective variables and titles each time.

You will see a couple of characteristics in the advantages I just mentioned. Speed of working with large(r) data, automating repetitive actions, producing reports in a consistent format, and the statistical analysis in R which in unrivaled. 

Finally, I would say that working with R prevents human error to a higher degree. Repetitive tasks that might get changed by hand in Excel might cause mistakes to slip through, whereas a some syntax in R might achieve the same task. 

To give you another example. A friend of my mine analyzed his thesis using Excel, and never touched R or Python. Perfectly valid of course, but he is missing out in my opinion. He ended up working as a consultant at one of the ""big five"" companies, where he creates Excel reports at clients. 

The monthly updates he creates takes about a month's work of time to deliver. Between his machine crashing at random times and having to re-do work, rather than having the option to re-run commands through an R notebook. 
Hell, having to re-do each report on a monthly basis without any code ready to execute is just repeating your own steps. 

Besides that, having to verify data through h&v-lookup or index match functions in Excel, rather than a one-liner in R is cumbersome in comparison. 

Then there is a lack of documentation at his department. People were uncertain how certain systems were linked, certain calculations performed, etc. The lack of any codification means that all that knowledge gets lost when certain employees leave the department. Working with R, or a similar tool makes you conscientious about writing things down. You can pass along your r markdown documents in your onboarding of new employees. 

Although, there is perhaps one area at which Excel exceeds at: making accounting spreadsheets. 

In short, I have urged him to take the time to learn R. Not only because it might give your career a new perspective, but also to keep enjoyment in your job. You can automate repetitive tasks to a large degree, giving you more time to explore parts of your job which are fun and stimulating. 

I would like to end with an important caveat though. It's not as much as which of the tools is the best, it is probably about the tools which fit the best to you: 

If you are working with small data, then you are perfectly fine working with Excel.
If you don't do extensive statistical analysis or data manipulation, then working with Excel is fine.
If you do a lot of accounting or financial modeling, then Excel is probably perfect for the task.  
And in the end it's all about what you find enjoyable. If you can really enjoy the challenges of working with code, and in the end make work easier, then it's a win-win. 

",1531178633.0
ohheyitsdeejay,"Unless you know some serious VBA, R makes everything a lot more reproducible. Have dozens of CSV that need to be merged and produce summary stats every day? Want to connect directly to an API? Dplyr alone is worth learning R. ",1531171648.0
aliaschick559,"I just spent a month or so in a stats class that offered explanations of how to use software to solve problems in R, Excel, and others. I didn't realize I could toggle between R and the others, so I learned R. A few days I finish the course and went back to compare the two. R's formulas are sleek and offer visualizations that can be cumbersome in Excel. If I need the five number summary, I can do that with one formula in R rather than five in Excel on top of having all the rows manually entered into the Excel formula. I really wanted to learn Excel, but now I don't think I will given how much easier I find R to work with.",1531171589.0
IdEgoLeBron,"Even without going in to why R is great at all, Excel starts getting fucky around like 10-15k cells depending on the data types",1531171632.0
Bruce-M,"I just want to add that, it's hard for you to find why R is preferable because your workplace/workflows are most likely very simple. I think you won't really be able to win anyone over to R until you start tackling more advanced problems.

Maybe you can expand a bit on what you are currently doing and what you want to do?",1531181671.0
kaaswagen,"Whilst not fun to say on an R subreddit, Excel is better at 99% of workloads you'll typically see in an office environment. However that 1% is more than enough to warrant R being a top programming language and R Studio an A-list program.

R is great at collecting, analyzing and reporting on vast amounts of data. Just as a really simple example, whilst Excel can handle about a million rows of data max, the limit in R is more theoretical than realistically achievable in any workflow. 

R also has a really rich library of add-ons ('packages') that enhance it's functionality greatly. There are [many](https://support.rstudio.com/hc/en-us/articles/201057987-Quick-list-of-useful-R-packages) that make R do what you need it to do, whereas in Excel it's Microsofts way or the highway (although you can do a lot with VBA I would consider this bad practice anno 2018).

  Let us know if you have any more questions!",1531171865.0
jiminie,"R is faster than Excel.  I was using Power Query and Power Pivot for heavy lifting in Excel, rebuilding something in R takes me a while, but R executes it much faster than Power Query and Power Pivot.  For example, a script to merge 1,000s of Excel files is much faster in R.",1531177393.0
giziti,"Excel is great at what it's built for: making spreadsheets. What is a spreadsheet? A kind of augmented computerized version of old-timey paper accounting worksheets. A lot of the time, that makes sense and there's no reason to do anything else. If all you're ever doing is that kind of stuff, some easy types of charts, and maybe some Pivot Tables, why do anything else?

Sometimes, however, you have to do some heavy data processing or some actual statistics (this is called ""rstats"", so we do statistics here!). You *could* maybe do something in Visual Basic or save a lot of intermediate variables on other worksheets, I guess. You'd be amazed at what you can torture a workbook into doing - but it might explode. Or you can use a general purpose programming language (like R!) to write the functions you need and then apply them to your data set. And do some actually decent graphics.",1531178464.0
fonzy6,"Basically what everyone else said. Most of the people on my team use Excel, SQL Server/Teradata/SAS to pull data, Word or Powerpoint to create a report or presentation. With R+RStudio, you can do it all in one place. I don't need to pull out 5 different tools that have no connection with each other to run an analysis. I can just do it all in one place with no copying/pasting between tools.

Basically everything you want to do, R is better.

R is way better at reading in data or pulling it from any source whether it's a text file, excel file, coming from a database. Ever try to import a table into Teradata or SAS? You have to manually use a ""wizard"" and it's difficult to automate. Want to read in every tab from a 50 tab Excel file? Much more difficult in these other places but not difficult in R. 

Manipulating data is much better with tidyverse or data.table than Excel. Of course, for small data sets and simple things, Excel is better and easier to understand. However, when you have a large data set or many steps, it's a mess doing it in Excel.

Statistical analysis--Excel can only do simple things and SAS costs thousands of dollars. R is free and far better at statistical analysis.

Graphs--can't compare. Only way to get ggplot and ggthemes quality is a lot of manual tinkering that won't reproduce for different graphs. SAS has graphs but guess what, that's more $.

Database--for many things, R is comparable to SQL Server/Teradata/SAS for Select queries. Other kinds of queries, maybe not but for most of what you want to do, it's just as good. Also, you can do all of it in the same script and not have to switch between programs (also these programs cost money).

Reporting--you don't have to copy/paste tables and graphs into Word or PowerPoint. You can use Markdown to make a report and presentation right away.

Reproducibility--code tells you exactly what you did to get the final analysis. Want to change part of the data pull and re-run the same analysis and produce the same report/presentation? Or go back and change a part in the beginning of the process (like exclude a certain group of people)? All it takes is changing a few lines of code and re-running the R scripts. No need to copy/paste anything.

Now R does have some downsides. Many people don't like to code and prefer Excel. Also, some companies may be skeptical of open-source software. Also, if you need to give someone an Excel file that has formulas to play with different inputs and see what the output is, unfortunately you have to build that in Excel. Also, Excel may be easier to show what's going on when someone isn't trained in coding and it's a simple calculation.

Also, if you're passing work along, it may be much easier to create an Excel file for someone to update if they don't know R. R does have a decent learning curve for someone who hasn't coded before. 

Sometimes, running or installing R can give weird errors and require some fixes and googling, especially if you're on a machine without admin rights. You have to find work-around's for some things. Sometimes, changing aspects of a graph or a Markdown report may be a little weird and require googling around some and trial and error.

In general R is better but the user has to be ready to learn and be ready to google to fix coding errors or just weird errors that may pop up. Over time, you get better at it though so it's less of an issue. Once you find the way to build a graph or a report that looks the way you want, then just hang on to that code and re-use it.",1531186010.0
samclifford,"When I started my PhD I was given a 130MB Excel worksheet that, when you copied the data in (a time series of histograms representing the size-fractionated particle number concentrations from an aerosol experiment) it would calculate things like the count median diameter, geometric standard deviation, total particle mass concentration, and total particle number concentration. It'd also spit out a few graphs showing the time series of the CMD, GSD, PMC and PNC.

To use it, you delete what's already there, copy the new data in, and trim the plot axes if necessary. It took 30 seconds to load because it had to recompute everything using the drag-filled formulae and cross-sheet references. I replaced it with a 2MB CSV file for the data, and about 30 lines of code to read in the data and perform all the calculations (which took less than 30 seconds). This was in MATLAB rather than R, but the point still stands. Separating the analysis from the data makes it far more general and easier to follow.",1531188364.0
drunkferret,"If you're a regular joe then Excel is going to be 'better'.

If you're inclined to learn programming then R is much better. You can export results to Excel so there's no difference in the end to 99% of the people who need to see what you're working with. Plus it'll open faster because no formulas and/or vba built into the workbook. R also lets you define functions, which, if you're into programming...it's just more fluid feeling than vba subroutine and being hacky with the built in formulas. Easier to manage and debug overall.

I really feel like R is the data analysis version of Python. It's just very easy with the syntax and layout. Easy to learn and then once you get the hang of it, pretty damn powerful. I really like R for graphs and such personally...but I've been forced to use Tableau at work so I've been using it much much less. I'm still a big proponent of it though. 


R also has libraries for a lot of what SAS can do. So if you work at at a smaller company that can't afford those licenses then you can use that instead. I did find issue with hadley's sas7bdat library though and I did not find the same issues with Python's libraries for the same situation...so if that is a case you have to deal with, check out Python, at least for getting data out of sas files. Throw it into csv and go back to R or whatever you want to do. No way in hell Excel open sas files so there's another one up either way...at least R can open them and get an idea.",1531189037.0
manwithoutaguitar,"Three reasons.

1. Reproducibility. A good coder leaves comments so when you face a similar problem the next year you don't have to start from zero. In excel this is more difficult and you will find yourself doing the same job from scratch again and again.

2. Statistics. Because of the awesome work of thousands of others, R is simply great for statistics while in Excel everything that even comes close to statistics is difficult.

3. Advanced plotting. Plotting in Excel is easier and faster, but you are limited to only a few graphs that Microsoft allows you to make. 

To he fair, most office workers don't really care a lot about any of these three.",1531197120.0
PM_ME_UR_TECHNO_GRRL," I am of the opinion that Excel will die as machine learning becomes more mainstream. It can do everything a spreadsheet can, but automated. 

It might be R, or some other language that takes over Excel's market. But Excel is probably done.",1531207114.0
koi-koi,"* Excel is limited to 1 million rows or just over. R handles larger data sets.
* Excel changes strings to numbers to dates and back again to be ""helpful"". R doesn't do this.
* Excel is much slower at computing anything in general.
* Excel is a spreadsheet, it has no concept of columnar data types. R is a good balance: not as rigid as a database, not as fallible as a spreadsheet.
* Excel can't connect to much. R can connect to most things (ODBC, XML, Spark, spreadsheets, graph databases, etc).
* R has thousands of packages to extend the language in lots of ways (modeling, APIs for Google Analytics and Dropbox, anything really)
* R has implemented lots of Unix-style services (rcurl, grep, etc) - Excel doesn't have this.
* Excel uses VBA as a scripting language which is completely horrible and should burn in a fire.
* You can't interop with Excel between different languages. You can with R.
* You can build interactive dashboards, notebooks, GUIs, client-server architectures with R, and things like enterprise scale BI services and security if you want to pay for it.
* You can build functions in R and make them as weird or complicated as you want.
* Since R is an application just like any other, you can run a script in Task Scheduler every half hour if you want to.
* If you use PowerBI or Tableau you can integrate R with those products.

An example: you are in a property development company. You want to build an interactive map of developments so that you can aggregate to states or counties. You want to connect to government data sources to assess where is good to build. You want to run a model to estimate profit using a Scalable Vector Machine. You want to put it on a server and create an HTML dashboard so that people can run it on their phones. You want to refresh all the data every day. You want this to happen even if you're on holiday for a month.

Automation, speed, reproducibility, and flexibility.",1531231230.0
edimaudo,"It depends on what you are doing.  If you are going to be doing heavy data analysis, statistical analysis or some machine learning prototyping then R is the better option.",1531174753.0
hyperplanes,"Literally the only thing I can think of where Excel's better than R is when I need to view/browse a dataset that Excel can fit, because View() on Rstudio is pretty slow. And most times I have to do that I end up using Stata's browse instead. ",1531177954.0
Mooks79,"Excel has its uses, quickly chucking data into a simple table, for example. But really there’s so many reasons R is better, you only need to make a graph once (then trivial to repeat if you have to do lots of the same), reproducible research, R can simply do things that Excel can’t - or at least not without a lot of wrangling, and I’m sure many other things people have listed. Note, also there’s packages that can read/write to excel too so you can always give an excel file to someone after. One even can put in tables, and another graphs. Shame not the same package!

But I always come back to what was said to me by the person that introduced R to me, after having used only Excel/Origin for data analysis.

When you’re explaining what you want done to someone, is it better to speak to them, or prod them and point at stuff?

That’s the fundamental difference between Excel’s point and click approach and an actual language like R. ",1531205849.0
willbell,"To take the mean of multiple columns in excel (a relatively simple operation): click a box, type mean() and then highlight the column you want that box to be the mean of, pull that mean box over so you get the mean for the nearby columns.  I hate scrolling so this might take me about a minute for a largish dataset (100x100 say).

In R: 
> apply(data, 2, mean) ## Took 5 seconds to write or better

> colMeans(data) ## iirc

(ignoring the need for data cleaning, etc in R if the info is already in an excel file, but often that will come up in excel too)",1531225659.0
Ruffie1234,R doesn’t remove leading zeros.  ,1531226863.0
ReusAlThor,"Pretty much everything about R is better than Excel. Once you get very comfortable with it, some people even use it for data entry/ formatting data. It is much faster to manipulate data and big data sets. ",1531229408.0
YeahILiftBro,"I think it depends on what exactly your workplace is doing to be able to weigh the two.  It looks like a lot of answers have come in regarding the technical aspects so I'll focus on people processes.

I'm currently on a team where everyone uses some BI application to access data, then send it into Excel to do their analysis and report build.  The issue here is really 1) reproducibility - where we need a human being to do this each month, and if that person leaves, we need to train a new person to do the exact same thing.  Because of this we have crappy, and likely dated, instructions floating around that someone needs to follow.  If you use a programming script, you can easily add in comments to each and every step you took.  2) timing - if you need a person to do this work each month, that is eating at your organization's ability to spend time on really digging into the data and research why things are happening.

Now the real issue here is after showing people why a programming language is helpful, is incentivizing people to change their ways and learn something much more difficult than Excel (but more valuable in the long run). Or you end up like me, have more advanced technical skills than your group, and be in a position of frustration and start looking for a job elsewhere.

Edit: Excel is probably fine if all you're doing is dropping a set of data into an Excel table and then summarizing with a pivot table, but then again you can do this with R.  So sometimes it's a matter of what your audience needs.",1531243278.0
tjen,"Okay, so you asked for reasons why r is better than excel and people have answered, and they're all kinda right, but a lot of the benefits listed are misrepresenting modern Excel. 

So please allow me to answer from the perspective of an Excel/corporate/BI shill! 

R is better than Excel at statistical analysis, it's literally what it was built for and it excels at this (hurrhurr). 

It's also better than Excel at doing particular graphs that look like you want them to, and it's better at producing academic publication ready work. 

In addition to having better capabilities in the above two areas, faceting across aspects of your data is also much easier. 

It is code - you can make it do pretty much whatever you want with enough time/effort, and it's better to code in than VBA. 

If you work involves this (that is, you're in a data scientist kind of role), and your colleagues perform this kind of work, then you oughta be proselytizing R.

And now to shill Excel a little, you mentioned you've used R for about a year, in this time you've probably been exploring all the different functionality, best practices, trying out new packages, and you've probably gotten a lot more data-savvy/conscious. Most people who use excel haven't done this with Excel, because the core functionality is what people know and don't think of data in the same conscious way when using it - it's usually taught by people who learned the ropes \~10-15 years ago. 

Anyway, in 2016-onwards you have built in ""Get & Transform"" which was previously an add-in called ""Power-query"". This is basically a script'able ETL GUI that can easily handle massive amount of data, and includes all the same benefits in terms of reproduce'ability, transparency, etc. in data cleanup, and enables joins/appends/etc. of multiple disparate data sources (online/offline, HTML, XML, JSON, CSV), API Connectivity, and so on and so forth. Combine this with Power-Pivot, DAX formulas,, new key measures, etc. and from a ""standard"" data analysis perspective, you've got a great tool even for >100mb datasets. Built into your standard Excel 2016. 

Most people are still using VLOOKUPS when they should be doing left joins. 

You also have to take your environment into perspective, where R may be difficult to deploy in a larger enterprise environment, if you're not working in IT. You also have a significantly smaller pool of people who can actually work with your stuff afterwards - if you can do the same job in two tools, then management will need an argument for why you're increasing the number of tools required to do the job.

And that's when you need to tell them about the great statistical, graphing, and analytical capabilities of R, and how all the big ERP-software providers are incorporating R into their analytical solutions, so it's totally the future in that area and you may as well begin developing statistical/operational models using it instead of more expensive alternatives :P ",1531244134.0
efrique,"Ultimately it's about what you're trying to achieve and why, but Excel has some inherent issues that make it a dubious choice for many mission-critical applications.

Here's a couple of them that come to me while my brain is still half asleep (there are many other good reasons, though):

1. If you have some collection of large excel spreadsheets that someone else has created (or even that you created a year ago), how do you make sure they're correct? How do you debug them if they're not?

2. If you're not sure if a function that comes with Excel is working correctly how do you check what it *actually* does (rather than what little the help says about what it does)? With R you can actually look at the code. 



",1531343261.0
brocialism,"When you say the list itself, are you referring to the variables used? These are also stored in the coefficients list (along with their coefficient values).   


I'm sure there are better answers, but I've found that call is sometimes handy to pass itself or its components (like its formula) to other functions (like stepwise). ",1531225217.0
shujaa-g,"It's also really cheap to store the call, and good record keeping. If you're running up against memory issues, you might set `model = FALSE` to save space, in which case the model frame is not saved, and but the `call` is nice to have as a record of how the model was created.",1531232702.0
flyos,"You can use mutate_at() for this. Something like

    df %>%
        mutate_at(vars(contains(""Q14"")),
                  funs(helpful = ifelse(. == ""Very helpful"", ""Helpful"", ""Not helpful"")))

I didn't tested this code particularly and YMMV, but you get the philosophy I guess.",1531155520.0
BaronPampa,"How about:

df %>% mutate(
Q14r1 = fun_helpful(Q14r1,
...,
Q14rX = fun_helpful(Q14rX))

Where fun_helpful(x) would be something like ifelse(...)?

Edit: You could also create second function which takes column names as argument and does stuff above for each of them,  so the overall code is cleaner",1531153218.0
guepier,"The tidy way of solving this problem is to forego the loop and instead restructure the data into [long format](https://en.wikipedia.org/wiki/Wide_and_narrow_data). This can be done with `gather` from the {tidyr} package.

Afterwards you’ll be able to mutate all values in a single statement. Something like this:

    df = df %>%
        gather(Key, Value, matches('^Q14r\\d+')) %>%
        mutate(Helpful = ifelse(Value == ""Very helpful"" | Value == ""Somewhat helpful"", ""Helpful"", ""Not helpful""))

This example will create a new character (rather than factor) column, `Helpful`. You can make it into a factor by surrounding the `ifelse` by `factor`. the `matches(…)` expression inside `gather` causes it to gather all columns starting with `Q14r`, followed by a number. This *seems* to match your loop but you might need to tweak it to fit your actual data.",1531149832.0
WorstCapitalist,"Is this as horrible as it sounds?

Edit: Not that the technology is horrible or misguided, just that it sounds like something that would take a lot of work,",1531145203.0
lemur78,What about pictures in post? Uploading and embeding... ,1531149627.0
TroyHernandez,knit2wp is the function I've needed for some time! Great post! Thank you!,1531141950.0
samclifford,"ggplot2 has geom_violin. If you aren't using that, you can't say it's easy. ",1531120556.0
Ax3m4n,"    ggplot(d, aes(x, y)) +
      geom_violin() +
      geom_boxplot(width = 0.1, fill = 1)",1531127103.0
grasshoppermouse,"Was just looking at this:

https://github.com/thomasp85/lime",1531005344.0
tacothecat,You might also consider [SHAP](https://medium.com/civis-analytics/demystifying-black-box-models-with-shap-value-analysis-3e20b536fc80),1531057814.0
ran88dom99,"[pdp](https://cran.r-project.org/web/packages/pdp/pdp.pdf)
[datarobot](https://cran.r-project.org/web/packages/datarobot/vignettes/PartialDependence.html)
Two partial dependence packages. I am not sure if datarobot's can be used on other models. and [Unified Interpreting Model IE SHAP] (https://arxiv.org/pdf/1705.07874.pdf) paper tacothecat suggested.
[mlr's pdp](https://mlr-org.github.io/exploring-learner-predictions-with-partial-dependence/)
mlr has a built in one but mostly its ICEbox. has ""Functional ANOVA""

And [pdp's explanation](https://journal.r-project.org/archive/2017/RJ-2017-016/RJ-2017-016.pdf) suggests plotmo,ICEbox,

[DALEX](https://www.r-bloggers.com/dalex-how-would-you-explain-this-prediction/) is really good. 
",1531407629.0
bmkred,Simply excellent. ,1531006391.0
Bruce-M,"I see StarCraft. I see R.

I upvote.",1531066943.0
Deto,"This could be really cool for gathering analysis on a large number of games.  If it were annotated to where unit types were included, then you could look at questions like ""What units tend to move out together? (same player)"" and ""What units tend to engage each other?""",1531012903.0
KopfJ4ger,WTF am I looking at?,1530996932.0
jwiener9,This is absolutely incredible. I’m going to try and do this with some HOTs replays.,1532699286.0
ryanmonroe,"You can just use the ""any"" function, then convert back to numeric.

In dplyr, you can do

>df &#37;>&#37; group\_by(Smith.tri) &#37;>&#37; summarise\_all(\~as.numeric(any(.)))

Or, using data.table

>df\[, lapply(.SD, function(x) as.numeric(any(x))), by = Smith.tri\]

I prefer data.table, but to use it on df you have to run setDT(df) beforehand to convert df to a data.table.",1530993565.0
willbell,"I think your original method makes sense, maybe post some code of it and we can critique it.

Another thing you could do is convert the values to logical values and then take an 'or' statement of all of the values associated with a given variable, since the only requires one TRUE in order for the whole thing to be true.",1530991310.0
keepitsalty,"Here is a way using the tidyverse method. There is a more concise way using pre-built functions but I did this quickly:

    library(tidyverse)
    
    df<- data.frame(Smith.tri=c(""cb1"",""cb1"",""cb10"",""cb10"",""cb1041"",""cb1041"",""cb1045"",""cb1046""), 
                   FR=c(0,0,0,0,1,1,0,1), 
                   Stor = c(0,1,1,1,0,1,0,1))
    
    df %>% 
      group_by(Smith.tri) %>% 
      summarise_all(sum) %>% 
      mutate(FR = case_when(
        FR >= 1 ~ 1,
        TRUE ~ FR), 
        Stor = case_when(
          Stor >= 1 ~ 1,
          TRUE ~ Stor
        ))

This should give you what you want, and while I understand ""tidy"" data isn't always required, I imagine it would make your analysis easier to make your data ""tidy"" (ie long format)

you can then do this:

    df %>% 
      group_by(Smith.tri) %>% 
      summarise_all(sum) %>% 
      mutate(FR = case_when(
        FR >= 1 ~ 1,
        TRUE ~ FR), 
        Stor = case_when(
          Stor >= 1 ~ 1,
          TRUE ~ Stor)) %>% 
      gather(key = ""type"", value = ""value"", 2:3)

This way each row contains only 1 observation and follows the [tidy data](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html) methodology. ",1530993775.0
Drewdledoo,"Not sure what you’re asking without a reproducible example, but just to throw some guesses out there:

You can have the line layer render on top of the points by doing `geom_point() + geom_vline()`. 

You can add some random or defined scatter in the x- or y-directions to each point by doing jittering/dodging. I like to use `geom_jitterdodge()` for this. 

If you’re asking about a way to clip the point shapes so that they don’t overlap the vline, then the only idea I have is to use a thicker white (or whatever your background color is) vline and then your normal one. 

That said, you should do your best to provide a more full explanation with a reproducible example so that we can better help you. Good luck!",1530978063.0
funkentelchy,How about drawing a geom_rect with the same fill colour as your background? Just add that after your points and it should cover the parts that overlap your line,1530979270.0
RoughPebble,"try 

    coord_cartesian(xlim = NULL, ylim = NULL, expand = TRUE, default = FALSE, clip = ""on"")",1530987752.0
grasshoppermouse,"I don't think you need to subset columns (but you will need way more than 3 rows). Instead:

    ggplot(sampleData, aes(x = x, y = y, z = z)) + geom_contour()",1531005706.0
willbell,"Since someone already explained what a truncated mean in general is, note that a 100% truncated mean would be the median, and that speaking broadly, more truncated means are less susceptible to outliers.  So for instance, the mean income is higher than the median income because there are some outlier extraordinarily high income people, but the lowest income is capped at zero, giving it a positive skewness.",1530995984.0
,[deleted],1530905923.0
erlo,"I don’t know tidyverse, but on base R, you could do something like this 


lapply(
lapply(yourNestedList,functionThatExtractsInfo)
,otherFunction)


And if the first list is very nested, rapply might work.

So lapply(rapply())


Not the exact answer you were looking for but I thought I would share. ",1530927373.0
Demortus,"The worst though is when you have functions that work fine the first time you run them, but you load a package later that masks them with their own \*identically named\* functions. Then you have to figure out why everything stopped working and then do package::function for everything that was masked..",1530886799.0
worldisfascinating,"Jesus, that's true.

I'm having a horrible time trying to understand map and do and the whole purrr package, have you all struggled with this as well?",1530892117.0
lolercakesmcgee,"Things like this happen to me all the time and it is crazy frustrating. I know it’s just some silly errors on my part, but finding it is just really frustrating. ",1530885193.0
neuro99,"We don't have enough info to see where the problem comes from. If i were you, I would try 'fwrite' from package 'data.table'. Considering you need to save several files, 'fwrite' is by far the fastest way to save csv files. ",1530829380.0
Bruce-M,"Not too sure what you mean by ""R as an API""? Do you mean you have made available an R process via an API like plumber and it's not writing the files correctly?

What happens when you export it from R instead of through the API? Does it work then?",1530846047.0
westaby,"For speed, why not use [data.table::fwrite()](https://www.rdocumentation.org/packages/data.table/versions/1.11.4/topics/fwrite) and [data.table::fread()](https://www.rdocumentation.org/packages/data.table/versions/1.11.4/topics/fread) instead? Does this result in the same problem? It's difficult to say without a reproducible example.

You could try creating a test data frame consisting of just the prevlost column and seeing if you get the same result. That will at least let you know if one of other columns is bleeding into it causing an issue.",1530815035.0
ilovetheuniverse,Try using write.csv instead? ,1530813779.0
_Wintermute,"I've had nothing but trouble with the readr package, as it's already been mentioned `fread` and `fwrite` are better alternatives.",1530817100.0
iamdelf,Does the csv written really contain NAs or is it the read_csv function that is mangling the input?,1530815447.0
fonzy6,I would focus on the lines giving you errors and see what makes those lines different than the rest. You can also just write those rows to a csv and see what they look like since you’re dealing with a large table. Should be obvious once you look at those lines in particular.,1530819421.0
codemasta14,"Original post: sorry I deleted it. Also, I couldn't recover the screenshot photos, and I'm not going to recreate them, so you get terrible glare phone picture with my working from home bed head. sorry.

So right now I'm working with large datasets, and to streamline everything I pull info from a sql server, do my data wrangling, and then save it to a csv. However, I've found something interesting. If I pull the data, run this code, and then view it, I get the appropriate values in the prevlost column. However, if I take that same dataframe, and save it to a csv, and then view it, it's full of seemingly random NA values. I've triple checked the code, and literally the only thing that is different is saving it to the csv before looking at it. Does anyone know why this is / know of a solution? (sorry for glare, and phone picture) 

[What works](https://i.redd.it/8veaowl446811.jpg)

[What doesn't work](https://i.redd.it/fgig0tm446811.jpg)",1531420345.0
mmoneyinthebank,Codemasta needs to learn from screenshot-san. Or better just paste your code. ,1530813512.0
deanat78,"That's a great looking website theme! :)

And nice post, let's hope for more drama!",1530863740.0
ctnl,"Have you tried doParallel?

    library(doParallel)
    cl <- makeCluster(4) # how many clusters 
    registerDoParallel(cl)
    allJson = foreach(url=jsonUrls) %dopar% fromJSON(url)
",1530801170.0
,Convert the x axis label column to a factor in the input data frame. Order the factor as appropriate. Then plot.,1530799190.0
fasnoosh,"Check out the function  `forcats::fct_reorder`

Here’s a good explanation from R For Data Science, Section 15.4, “Modifying Factor Order” http://r4ds.had.co.nz/factors.html",1530806506.0
phobonym,"Hey, since you have a very specific order in your data, that is not ordered by the frequency values or alphabetically I think providing the frequencies as a named vector that stores the values in the order you want them to be plotted is the way to go here. The example below shows what I mean. There are probably other ways to do this, but this just works.

    # create some random data resembeling the data categories in OP's plot
    cats <- c(""Baumholz I"", 
              ""Baumholz II"",
              ""Baumholz III"",
              ""Dickung"",
              ""Stangenholz 1"", # btw, why are arabic and latin numerals mixed here
              ""Stangenholz II"")
    
    Datenerfassung <- tibble(Entwicklungsstufe = sample(cats, 5000, replace = TRUE))
    
    # Doing the actual thing OP asked for.
    ve_vec <- table(Datenerfassung$Entwicklungsstufe)
    ve_vec <- ve_vec[c(4,5,6,1,2,3)]
      
    barplot(ve_vec,
            xlab = 'Entwicklungsstufe',
            ylab = 'Anzahl Bäume',
            main = 'Verteilung der Entwicklungsstufen aller Bäume')
    
Edit: using table() instead of count() makes more sense here. Updated the example accordingly.",1530798438.0
kazi1,"It's ordering the columns as if they are factors. The order of the levels in the factor determines which order the bars get plotted in (by default, alphabetically). To change the order, convert that column to a factor, then reorder the factors levels how you want it. (Yes, factors are ""fun""...)",1530802372.0
nickatick,"I think I managed to do it. Thanks to everybody who tried to help out!

If anybody else is having a similar Problem, here is how I solved it:

\#This is the old code in which the bars are not sorted how I want it

Verteilung.Entwicklungsstufe <- count(Datenerfassung$Entwicklungsstufe)

barplot(Verteilung.Entwicklungsstufe$freq,

names.arg = Verteilung.Entwicklungsstufe$x,

xlab= 'Entwicklungsstufe',

ylab = 'Anzahl Bäume',

main= 'Verteilung der Entwicklungsstufen aller Bäume')

\# This is the code with the correct order of the bars

Verteilung.Entwicklungsstufe <- count(Datenerfassung$Entwicklungsstufe)

Verteilung.Entwicklungsstufe.sorted<- Verteilung.Entwicklungsstufe \[c(4, 5, 6, 1, 2, 3),\]  # This is the line I had to add, the numbers correspond to the rows in the data frame. I just had to write them in the order I need them in  

barplot(Verteilung.Entwicklungsstufe.sorted$freq,

names.arg = Verteilung.Entwicklungsstufe.sorted$x,

xlab= 'Entwicklungsstufe',

ylab = 'Anzahl Bäume',

main= 'Verteilung der Entwicklungsstufen aller Bäume')",1530815581.0
PA_Irredentist,"You're mixing up two different methods of subsetting a data frame. Here are some ways of doing what you're asking: 

     v2 <- data$variable2 
     v2 <- data[, 2]
     v2 <- data[, ""variable2""]

You can use any of these to subset the data frame. The first makes use of the dollar sign operator to identify the column you're interested in. The numerical indexing in the second allows you to refer to it by the column number. You can also use the name of the variable, as indicated in the third example, but it always needs to be enclosed in quotes. R doesn't know what to do with your subsetting request because a) it would need to be enclosed in quotes and b) even if it was, you would need to refer to it by the variable name and not using the alternate dollar sign operator.

",1530750209.0
asiatownusa,"to add some follow-up to the correct answer, this is one of the more confusing parts of R. When subsetting a dataframe, you can select by index, name, or logical vector

```
v2<-data[,c(1,2)] # this selects columns 1&2 by index
v2<-data[,c('variable2')] #this selects variable2 by name
v2<-data[,c(FALSE,TRUE)] # this selects variable2 by logical vector
```

So what you were passing in originally was the vector `data$variable2`, which is a bunch of repeated numbers 1-5. So you are trying to select via an index that is likely out of range

```
v2<-data[,c(1,5,1,3,4,2)] #selecting via some random numbers 1-5 
```",1530762935.0
LogicalRisk,"In the embed plots chunk setup, try adding results='asis'",1530745484.0
,[deleted],1530667548.0
Stripedpajamas,"That's up to you. All of the code in R packages is open-source, so if you're concerned about what a package does, just look into its source code.  

R packages aren't peer-reviewed. Publications that arise from those packages may be, but the package itself only has to pass computerized checks within CRAN (and if you're downloading packages from Github, they don't even have to do that).",1530667649.0
fang_xianfu,"For reference, CRAN's policies regarding package submission are available here:

https://cran.r-project.org/web/packages/policies.html

It is not CRAN's responsibility to protect you from malicious code.",1530685444.0
_Wintermute,"libstableR has a peer-reviewed publication associated with it [[link]](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=8&ved=0ahUKEwi3jMed_ITcAhVEjiwKHTEADsoQFghPMAc&url=https%3A%2F%2Fwww.jstatsoft.org%2Farticle%2Fview%2Fv078i01%2Fv78i01.pdf&usg=AOvVaw1fWtaBL_J6gXqagtXVK2_u), so it's likely to be one of the more trustworthy packages on CRAN.

But as others have said, even if a package on CRAN, there are zero checks that methods are implemented correctly.",1530691449.0
grumpymonk2,"A few things to look for

- does the library have a GitHub repo? If so
    - is it active? Are people raising issues, and are the authors replying?
    - how many contributors are there? 1 man bands are fine, but more people is generally better
- does the package contain unit testing? Look for code in /tests/testthat. Do they mention test coverage in the GitHub readme?
- how many releases have there been? More is good.
- how many downloads does it have? You can find download stats in a few places online
- Google it. Can you find evidence it's widely used.

It's not an exhaustive list, but basically try to get a feel for the health of the project. 

Then balance with the risk - depending on what your data is, what could go wrong? You have further options, such as using a computer (or container) with no access to the internet for extra security.

",1530694275.0
guepier,"In your specific case you’re probably fine, since [Bob Rudis forked and edited the package at some point](https://github.com/hrbrmstr/libstableR). This is relevant because Bob Rudis (*aka* hrbrmstr) is (a) a prominent member of the R community, and (b) an IT security researcher.

Generally your scepticism is somewhat warranted (given that there have been countless occurrences of malicious code on NPM though not, as far as I know, ever on CRAN) but in the case where a package [can be tied to an author who’s a real person](http://www.lpi.tel.uva.es/~jroyval/), the risk is manageable: virtually all malicious code is published anonymously or under a fake name. This isn’t the case here.",1530711979.0
,[deleted],1530679455.0
AnonymousGourmet,"A data.table solution:

    library(data.table)
    database <- data.table(database)
    database[, lapply(.SD, sum), by = .(ID1,ID2), .SDcols = c(""1"",""2"",...,""n"")]",1530652424.0
EdinDevon,"Will n be in a finite range (e.g. it'll either be 5 or 12 but nothing else) or not?

I ask because you could have an if on the number of columns. It might be possible to dynamically generated the sum (vn) clauses but I'd need to play and test and I don't have R to hand. ",1530645315.0
ilovetheuniverse,"This should do it
    
    library(dplyr)
    df%>%
    group_by(ID1,ID2)%>%
    summarise_all(funs(sum))
",1530650166.0
infrequentaccismus,"You are right that data table is slightly faster, but this may not matter depending on the size of your data.  Here, I create a sample dataset similar to yours.  You can experiment with different numbers of columns and rows.  On 100 columns and 1M rows, data.table completed the operation in .35 seconds and dplyr completed the operation in .8 seconds. I prefer the readabilty and simplicity of dplyr.

    #library(tidyverse)
    #library(microbenchmark)
    #library(data.table)
    
    
    
    #Create large sample dataset
    num_cols <- 100
    num_rows <- 1000000
    
    matrix(data = c(sample(letters, num_rows, replace = T), 
                    sample(LETTERS, num_rows, replace = T)
                    ),
           ncol = 2
           ) %>% 
      as_tibble %>% 
      bind_cols(matrix(runif( (num_cols - 2) * num_rows, 1, 10) %>% as.integer,
                       ncol = num_cols - 2
                       ) %>% as_tibble
                ) -> dftest
    
    names(dftest) <- c(""id1"", ""id2"", (1:(num_cols - 2)) %>% as.character)
    
    # Create data table out of tibble
    dftest %>% 
      as.data.frame %>% 
      data.table -> dftest2
    
    
    
    # Benchmark results
    microbenchmark(
      #dplyr solution
      dftest %>% 
        group_by(id1, id2) %>% 
        summarise_at(-(1:2), sum),
      #data.table solution
      dftest2[, lapply(.SD, sum), by = .(id1,id2), .SDcols = names(dftest2)[-(1:2)]]
    )
",1530667743.0
AnonymousGourmet,"Have you tried this?

http://igraph.org/r/doc/as.directed.html",1530653567.0
Statman12,"Define the letter names as a variable. Obtain the numeric ranges, and subset the proper letter names.

For example, if you don't need to get into the double-letter columns, then just use `LETTERS`. Want columns 3-7? `LETTERS[3:7]`, or if you have 3-7 as an index `LETTERS[my_index]`

Edit: I realize this keeps you at using the column indexes. You could define a helper function to generate the sequence of letters based on the start and end letter. Something like:

    seq_char <- function( from=""A"", to=""Z"", set=LETTERS ){      
      idx1 <- which( set==from )
      idx2 <- which( set==to   )      
      return( set[ seq(idx1,idx2,1) ] )
    }
        
    cols_to_apply_style <- seq_char( from=""C"", to=""F"" )

If you do need to get into the double-letter columns, just define the `set` argument appropriately, such as `col_names <- c( LETTERS , paste0(""A"", LETTERS ) )`",1530630949.0
infrequentaccismus,"Predict will return either the prediction (1 or 0) or the probability of the positive class, depending on which outcome choose in the arguments. If you give it newdata, it will apply the model function to the model matrix (apply the equation that represents the model to the matrix that represents the predictor variables) and generate a vector of predictions. How glm is different from lm is a more involved question. Is that what you are wondering?",1530573906.0
efrique,"> If I simply run predict(my_model), it gives me a set of values but I don't understand what those values mean

The help for `predict.glm` (what `predict` calls when you supply a `glm` object) explains

1. the default setting of what `predict` produces in the help (`?predict.glm`) under the `type` argument; you get the linear predictor, Xß (for the estimated value of ß)  

  since you're doing logistic regression, this would be the logit of the fitted value

2. If you omit the `newdata` argument, you get fitted values for the original data

so you will get fitted values of the linear predictor


>  I know I can also run predict(my_model, newdata = test_data) which produces another set of values. Is this predict() using the B, C, and D values to predict A?

See 1. above

> What if test_data only have B, C, but not D or some other combination? Would the predictions work?

No. You need a complete case (and the newdata data frame needs the same variable names). What do you want it to do if one variable is missing?

>  I have a nagging impression that there is a transformation applied during the process, if so at what stage? Does it mean I have to transform the predicted values to the original scale of A? What is the use of the transformations (if indeed there are any)?

See 1. above. (If you don't read the help, you won't know what its doing nor how to get it to do what you want!)

>  What is the use of the transformations

see any reasonable quality text on generalized linear models; or there are a number of suitable sets of notes on the internet.

>  Is there a relatively ELI5 conceptual explanation of how predict() works with glm() and how to interpret the results of both??

Can you identify what is not clearly stated in the help for `predict.glm`? I'm having trouble seeing what it doesn't tell you. Do you just need a reference to an explanation of generalized linear models or is there something it's not telling you? 
",1530584118.0
Digging_For_Ostrich,"You either try and complete that data, or exclude that input variable. Otherwise, it will absolutely include that bias into the output.

Whether or not it is significant is another question, does it actually matter about that bias, is that factor even a significant predictor? The answer depends on both the stats, and the actual process you’re modelling, and only you know that!",1530578407.0
VisuelleData,"This project started with the function 'is.output.same()' which I made to test my code after refactoring it to ensure that it produces the same output. This led to some research into metaprogramming, because I wanted to make 'is.output.same()' pipeable.    
  
  

  
From there, I started writing 'tictocify()', which generates a function identical to its input function that provides its runtime in addition to the normal output.  
 
 
Finally, I decided to put all of the techniques I discovered and used, into their own functions and later into a package. 
 
'frite' stands for **f**unctions w**rite**, since this is a package for functions that write functions. ",1530548577.0
dartkite,This looks really interesting. Will try some experiments later,1530551444.0
AFreudOfEveryone,"Looks interesting! FYI, a typo here:

These types of _can_ modifications can be difficult to make sequentially, so there are a couple of helper functions to allow you to see what you’re doing.",1530565107.0
Mooks79,"Search internet, copy existing code and modify to your needs. Search internet when errors form. Ask questions (with a minimum working example) here or stackoverflow. Repeat for next topic.

Generally you’ll learn faster and better than online courses (although dip in and out of them if you want). R Bloggers isn’t a bad website to search specifically as has a huge amount of examples. 

Also, don’t forget to read about the stats behind the programming. Learn about PCA in general, read a book, search internet, Wikipedia, whatever. 

First place to start:

1. Install R
2. Install R Studio
3. Read data into R (depends on what format your data is) eg data <- read.table(“file location”)
4. Do PCA eg pca_result <- prcomp(data)
(There is also princomp that does PCA, basically they’re the same but the way you specify the control parameters and the way they output result is slightly different)
5. Read prcomp help pages using ?prcomp
6. Read about PCA to try to understand results.

Or, as I mentioned originally, search internet and copy existing code.

Oh and a good place to start if you’re really at step zero is R for Data Science by Hadley Wickham. It’s online. But start doing as soon as you can, you don’t have to finish the book first. ",1530522119.0
lolercakesmcgee,"Coursera specifically had one section of an R course focused on PCA that involved using an image. I would look for that if I were you. 

Unrelated, but this is excellent http://setosa.io/ev/principal-component-analysis/",1530531577.0
chimpbucket,"I learned PCA and other multivariate statistics using these two books:   
 https://www.amazon.com/Introduction-Applied-Multivariate-Analysis-Use/dp/1441996494. 

https://www.amazon.com/Numerical-Ecology-R-Use/dp/3319714031/ref=mp_s_a_1_1?ie=UTF8&qid=1530532199&sr=8-1&pi=AC_SX236_SY340_QL65&keywords=numerical+ecology+with+r&dpPl=1&dpID=41H5vKMNY4L&ref=plSrch. 

Good luck!",1530532441.0
VincentStaples,"That's a very long way of writing: summary(lm(y ~ x1 + x2, data=df))",1530509457.0
Reggaepocalypse,Samples systematically underestimate population variance.  N-1 adjusts for this in many stats equations,1530505026.0
coip,"I'm always amazed that we can do this kind of stuff in R (and by ""we"" I mean people who aren't me). ",1530370100.0
,Which additional grid or viewport function was used to add the two corner images / captions?,1530373023.0
vintage2018,Any way to slow it down?,1530395488.0
RememberToBackupData,"`data()` calls a built-in dataset from a loaded package. If you’re trying to import some data that’s in a file, you need the use some other function that’s appropriate for the file type. 

Otherwise, if simCalls is in the overlap package and it’s not being loaded, then it means that overlap is not actually loaded. The working directory doesn’t matter for loading built-in datasets.",1530307306.0
LogicalRisk,"On my machine, the following works straight out of the box.

    rm(list =ls()) # Get a clean environment to confirm no package confusion
    library(overlap)
    library(sp)
    library(maptools)
    
    data(simCalls)
    str(simCalls)
    
    # Output to console
    # 'data.frame':	100 obs. of  2 variables:
    # $ time : num  2.38 4.07 1.17 2.26 1.86 ...
    # $ dates: chr  ""2017-01-05"" ""2017-01-09"" ""2017-01-13"" ""2017-01-14"" ...",1530310150.0
MrLegilimens,Are you sure you’ve loaded the two libraries?,1530357410.0
BenderBG,"Have you tried `geom_abline(slope = yourSlope, intercept = yourIntercept)`?",1530282329.0
halhen,"A useful, more general tool is `stat_function`


    library(tidyverse)
    # Generate some test data
    data <- tibble(x = seq(0, 10, length.out=100),
                   y = 0.1 * x + 0.5 + rnorm(length(x), mean=0, sd=0.2))

    data %>%
      ggplot(aes(x, y)) +
        geom_point() +
        stat_function(fun = function(x) { 0.1 * x + 0.5 })",1530310756.0
TripKnot,"[geom_segment](https://ggplot2.tidyverse.org/reference/geom_segment.html)

geom_segment(aes(x=0, y= 0.5, xend=10, yend=1.5))

or [geom_path](https://ggplot2.tidyverse.org/reference/geom_path.html)

geom_path(aes(x=1:10, y=0.5*1:10+0.5))",1530278469.0
luckylukey,"Aws is (nearly) free, depending on the task. I used an Aws solution last year to periodically scrape twitter data for me and it cost like 5€ / 3 Months IIRC",1530629721.0
hummingbirdz,"You can't run something without your computer being on...

You could use a free or paid cloud computing service to run a script (note though that this still requires a computer to be on its just an Amazon/Google/DigitalOcean owned computer).",1530271654.0
Pramodprk,Code goals be like this !! Awesome job man ,1530245953.0
zerodel,nice job !,1530257175.0
Bruizeman,Great work man,1530276846.0
dartkite,"Do you mean

which (df$var2 == 0)",1530211024.0
slammaster,"This usually happens when a function is trying to build too large a structure, do you have a particularly large matrix?

Google is usually your friend in situations like this, there's a stackexchange thread that helps navigate how you might solve the problem: https://stackoverflow.com/questions/5171593/r-memory-management-cannot-allocate-vector-of-size-n-mb",1530190376.0
Jandro93,Found this very helpful ,1530138954.0
NSAkela,"In plain language you have something like this:
""Okay, calculate average of all values (~intercept)
Then find regression slope on DV-intercept for the first predictor
Then substract predictions based on X1 from DV-intercept
Repeat procedure for every fixed predictor
Okay, now you have residuals after all fixed variables and it is time for random variables
Calculate average of residuals for every subject
Then for every subject subtract average of residuals and find slope of X1 then subtract predictions and find slope of X2 in remaining residuals. Present results as size of fixed effect and between-subject variation of it""
So if you swap X1 and X2 you get different results, because between-subject variation calculated on different residuals. ",1530130306.0
efrique,"Oh, SPSS has screwed you up there.

1. There are two Kolmogorov-Smirnov tests. The first tests a single variable against a fully-prespecified distribution (i.e. one you have to specify!) and the second tests whether two variables could have come from the same distribution (where you don't specify what that is). R does both of these using the `ks.test` function.

2. Beware; SPSS uses a single name for the Kolmogorov-Smirnov and Lilliefors tests (calling them both Kolmogorov-Smirnov). R doesn't; they're two different things. 

 I expect you're probably after a Lilliefors test, but even then there's more than one. If you're trying to test normality without specifying the distribution, you would be asking for *Lilliefors' test of normality*. (Even Wikipedia gets this part wrong, asserting that the Lilliefors test is a test of normality. Not so; Lilliefors did one for a test of exponentiality as well and both are called Lilliefors tests). Several packages implement Lilliefors test of normality. I'd recommend reading Conover's Applied Nonparametric Statistical Methods (the coverage in 2e is pretty good, I expect it's still in 4e)

3. `ks.test(x)` doesn't do anything; if you test a single variable you have to tell it what the specific distribution function is, like `ks.test(x,'punif')` (one sample K-S test of standard uniform). If you give it two variables, `ks.test(x,y)` does a two sample K-S test.

4. If you're testing residuals you have already estimated the coefficients which generally leads people to think Lilliefors, though strictly Lilliefors test is not suitable for that purpose. However, it comes a lot closer than the actual KS test does.

5. To test each group (level of a factor) with a KS test (which you don't want here, but for completeness' sake), you need to supply `ks.test` as an argument to a function like `by` or `aggregate` or `tapply` to get it to be applied across levels.

6. If you really want to test normality, don't choose a test with fairly low power (like Lilliefors, which has poor power against deviations in the tail); choose a better test -- but only choose one. I'd suggest  that if you must test normality, you just use the Shapiro-Wilk as a default - though there are slightly better options, the Shapiro-Wilk is available without loading additional packages and is quite decent.

7. However, the entire exercise is misplaced. Checking your assumptions via formal testing is a bad idea. e.g. for the case of testing an assumption of normality, see Harvey's answer here: https://stats.stackexchange.com/questions/2492/is-normality-testing-essentially-useless/2501#2501

 In many cases, testing normality ends up leading you to do exactly the wrong thing whether you reject or you fail to reject (frequently leading people to abandon normal-theory procedures when they're fine and to think they're okay when they're not). It also impacts the properties of whatever original tests you were choosing between, so your p-values no longer mean what you think they do.
",1530146487.0
pfunkman,"Short simple hack : change fun.a to not use subset 

    fun.a <- function(indata, var1, var2) {
        indata[indata[,var2]==1, c(var1,var2)]
    }

If you don't like this approach, you can read about R's [non-standard evalution](http://adv-r.had.co.nz/Computing-on-the-language.html), which subset uses, and perhaps come up with a better solution. ",1530114733.0
CapaneusPrime,"You're sending a character vector for var1 into fun.a, then using it as your subset argument.

The select argument can handle character vectors fine, while the subset argument is looking for a logical expression.

Right now, you're grabbing all the rows where ""event"" == 1. That's no rows because one is a character vector while the other is a number.

To fix this, inside your function, wrap var2 in an expression() call.

`data1=subset(indata, expression (var2)==1,select=c(var1,var2))`",1530114097.0
saltwound,"You're using the $ syntax inside of your dplyr functions, even though you are also using the pipe syntax to pass in the dataframe. Try drop the df$ in front of the column names and that should solve the problem. Dplyr is special in that it works with unquoted column names directly. ",1530072999.0
tacothecat,You didn't ask a question,1530060324.0
mkuehn10, https://dplyr.tidyverse.org/reference/coalesce.html,1530060554.0
guerisimo,In each column or the whole df?,1530061051.0
huessy,"which(is.na(table$column) == TRUE))

That will give you the positions in a given column that are NA. You could even try a for loop that looks at the whole df. If you want the positions of the non-NAs, just change the logic to ""== FALSE"". Again, just column by column, but you could find out the positions in the whole df with a for loop or probably another smarter way that I'm not clever enough to think of right off the top of my head.",1530062574.0
moosejock,Look at dplyr::mutate,1530032056.0
synchoresis,Never mind! Figured it out with all=TRUE :),1530028573.0
Bruce-M,"This is a x-post from [r/dataisbeautiful](https://www.reddit.com/r/dataisbeautiful/comments/8m9ha6/i_created_a_tool_to_automatically_extract_the/dzlsmfg/)

The short version is, I made this little tool in R and R shiny to summarize text. Let me know if you try it! =)

**[Link to tool: autoSmry](https://www.brucemeng.ca/project/autosmry/)**

**[Link to post testing autoSmry on various movie plots, a novel plot, and a few blog posts](https://www.brucemeng.ca/post/autosmry-test/)**

",1530023407.0
GoodAboutHood,"This should work.

    library(tidyverse)
    library(lubridate)
    
    dfx <- dfx %>%
      mutate_if(str_detect(colnames(dfx),""date""),mdy)

So mutate\_if() allows you to specify ""if this returns true, then execute this function"". So in this case, if you detect the string ""date"" inside the column names, execute the function mdy() on those columns.

mdy() comes from the package lubridate - if you haven't heard of it, it's the tidy tool to deal with dates, I'd highly recommend it. mdy() is basically just an easy way to point the month/day/year as.Date() transformation. There's a ymd() (year/month/day), ymd\_hs() (year/month/day/hour/sec), and pretty much any combination you can think of along those lines.

Let me know if you have any questions!",1530020793.0
PickledOilCured,Awesome!  Thank you,1530026895.0
OnlyBikesOnLSD,I've been looking for something with a bunch of data sets forever just for some practice! Really appreciate it! ,1530035608.0
The-Credible-Hulk79,"This is awesome, I'm brand new to R and this will help me practice. Thanks OP!

 Everyone in this sub probably already knows this but just in case you're a noob like me; make sure you change the windows default single backslash to double backslashes or forward slash when you input the file path.  ",1530080302.0
singularperturbation,"There's [RCloud](https://rcloud.social/index.html), which might fit the bill.  Can import Rmd documents, and share / fork notebooks.  No idea how complicated setup / hosting is, though.",1529980632.0
,"What about Microsoft's Machine Learning Server? It apparently runs R and Python. I've never used it but just saw it online

[https://www.microsoft.com/en-au/sql-server/machinelearningserver](https://www.microsoft.com/en-au/sql-server/machinelearningserver)",1530009302.0
jbraids1421,"Is there a reason RStudio server doesn't fit the need? If it's (RStudio server) architecture is the problem, then I don't personally have any recommendation as you'd have to share what your achitecture situation/options are. 

If it's just a pricing issue, anyone can set up and host their own server instead of paying RStudio to do it for you. Again, depending on what your stack is at work would determine your best path forward, but we run one through our IT department that only requires a user to click a URL, which we then allow a user to log in to to be able to point to a user workspace for saving code/data files. This is hooked up to multiple SQL server instances as well as an IT maintained Hadoop cluster with Spark. These external data sources are hosted on AWS, though they were originally hosted in our own on premise data center. 

tl;dr You can build to suit which will offer you the best way to customize the solution and hardware to your needs. This can be entirely on premise or in a private or public cloud.",1529979655.0
KopfJ4ger,"What's wrong with Rstudio Server? Based on your needs it seems like that would be the best option.

>users must not be required to install R or any other programs just to run scripts.

... but when you say this it makes it sound like even logging in to a server would be too much for these people. Maybe you need a front-end that calls the scripts in the background?",1529979664.0
noelsusman,"RStudio Server is by far the most popular option, and it fits all of your needs.  I don't think you're gonna find an option that also has SQL built into the web interface, but RStudio has a lot of features for connecting to databases (http://db.rstudio.com/).  You can easily run queries from within R.

Maybe look into [Jupyter](https://jupyter.org/)?  It supports R (plus many other languages) and might even be a better interface for ""just need results"" people.  The multi-user server version is called JupyterHub.",1529980835.0
another30yovirgin,"Seems like RStudio Server is exactly what you need, so I'm not sure why you've ruled it out.",1529984842.0
fairmantium,"If you're doing the programming and analysis and just need to display dashboards of information, Shiny is the right thing for you:  [https://shiny.rstudio.com/](https://shiny.rstudio.com/)  [https://rstudio.github.io/shinydashboard/](https://rstudio.github.io/shinydashboard/)

I'm a scientist at a pharma company and I use Shiny Dashboard to display interactive plots and data analyses.

Also use Rstudio Server and Jupyter on a cloud instance for development work.",1529998148.0
,[Use google](https://stackoverflow.com/questions/4862178/remove-rows-with-nas-missing-values-in-data-frame) please.,1529949619.0
Laerphon,"I figure this could be a homework question, but if so it is also on [stackexchange with formal proofs](https://math.stackexchange.com/questions/1417274/connecting-noodles-probability-question). I was pretty lazy here, so the code could be shorter and more efficient. I use some dplyr calls because they're what I usually use, not because they're required in any way. Edit: Note I use pipes here, so you will want to do library(dplyr) to use the code below.

    data       <- data.frame(noodle = rep(1:100, 2))
    draws      <- 1000
    loop_count <- numeric(draws)

    for(i in seq_along(loop_count)){
      sampled_data      <- dplyr::sample_n(data, 200, replace=FALSE)
      sampled_data$pair <- rep(1:100, each=2)
      pair_loops        <- sampled_data %>% 
                               dplyr::group_by(pair) %>% 
                               dplyr::summarize(loop = length(unique(noodle))==1)
      loop_count[i]     <- sum(pair_loops$loop)
    }

    summary(loop_count)

My results agree with the formal proof of E(n) = 1/((2*n)-1). For high precision, increase draws to 10000+.",1529950044.0
Deto,"Give it a shot?  Right now your question is just ""code this random thing for me please"".  ",1529947996.0
,Look into using [this sort of thing](https://stackoverflow.com/questions/31592139/select-list-of-columns-from-a-data-frame-using-dplyr-and-select).,1529939568.0
navidshrimpo,glm(),1529898589.0
frinkahedron,"I am just about to try my first logistic regression model. This is perfect timing! Very nicely written article, thanks for posting.",1529898505.0
,[deleted],1529795835.0
I_just_made,Thanks for posting! It is nice to see some variation in how people explain various concepts; the added benefit of seeing how you coded your examples in R was a very nice touch as well!,1529813517.0
green_tealeaf,"This is good advice for the plot itself, but won't allow you to add a background image over the entire panel. (You're left with no image behind the axes and legends.) If you want to add a background image to the entire panel then you'll want to use (the amazing) [`cowplot`](https://cran.r-project.org/web/packages/cowplot/vignettes/introduction.html), which also gives you a handy `save_plot` command that handles these slightly hacked-around charts better than `ggsave` does.

I use cowplot when making thoroughly-styled output plots like these: [http://www.weirddatascience.net/blog/index.php/2018/04/10/mapping-paranormal-manifestations-in-the-british-isles/](http://www.weirddatascience.net/blog/index.php/2018/04/10/mapping-paranormal-manifestations-in-the-british-isles/) \-- there's a code example at the bottom of that post that includes a couple of other fun tricks with `cowplot`.",1529784605.0
TrickyRiquey,you might want to check in at r/Rlanguage too,1529802993.0
tacothecat,"here you go.  requires purrr and matrixcalc passages.

    block_lower_triangular <- function(n) {
        m <- purrr::map(n, ~(lower.tri(matrix(NA,.,.),diag=TRUE)*1L))
        purrr::reduce(m, matrixcalc::direct.sum)
    }

example:

    > block_lower_triangular(2:4)
          [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]
     [1,]    1    0    0    0    0    0    0    0    0
     [2,]    1    1    0    0    0    0    0    0    0
     [3,]    0    0    1    0    0    0    0    0    0
     [4,]    0    0    1    1    0    0    0    0    0
     [5,]    0    0    1    1    1    0    0    0    0
     [6,]    0    0    0    0    0    1    0    0    0
     [7,]    0    0    0    0    0    1    1    0    0
     [8,]    0    0    0    0    0    1    1    1    0
     [9,]    0    0    0    0    0    1    1    1    1

EDIT:  here is a version that uses base R functions.  returns a sparse matrix of type dgCMatrix.

    block_lower_triangular <- function(n) {
        bdiag(lapply(n,function(x) lower.tri(matrix(NA,x,x),diag=TRUE)))*1L
    }",1529771007.0
LbrsAce,"Here's the answers for ya: https://jrnold.github.io/r4ds-exercise-solutions/

According to the above: Note that in dep_time, midnight is 2400, not 0",1529683320.0
fonzy6,I would print the first few rows of the column to see if it’s doing what you think it should be. You can also create your own hour column and compare it with that one. ,1529681560.0
Kromleech,"I can't contribute to your question, but I take the opportunity to ask for your opinion: Is the book any good?",1529687787.0
brindlekin,"Hey this is super cool ! Looks nice and works well, great job. ",1529683785.0
I4gotmyothername,https://www.reddit.com/r/statistics/comments/85hxgt/bayesian_statistics_courserecommendation/dvy1efq,1529615807.0
rheidzan,"A bit unrelated, but you're a Medical Doctor that works with R? ",1529619145.0
dtrillaa,Datacamp covers it [here](https://www.datacamp.com/courses/foundations-of-probability-in-r),1529621859.0
Atheriel,"Although you don't seem keen on books, I do heartily recommend the ""Statistical Rethinking"" book and its [accompanying R package](https://github.com/rmcelreath/rethinking). It takes a very learn-by-doing-in-R approach to Bayesian methods, and is specifically targeted at those coming from a frequentist background.",1529641975.0
Darwinmate,"There's bayesian statistics in the `swirl` package? Sweeeet. Swirl is how i learned R to begin with, it was great. 

IMO (someone please correct me) bayesian statistics is fairly specialised part of statistics. Most statistics you will encounter in medical journals will probably be inferential statistics, so sharpen this side and you will be better for bayesian down the line. 

Also if you are after a specific book, send me the name and I will see if I can track a PDF for you.",1529625180.0
ndha1995,"Not sure if you still need this, but a simple search on datacamp returns 3 bayesian courses: https://www.datacamp.com/courses/q:Bayesian",1538621866.0
Digging_For_Ostrich,"    MyData1 <- read.csv(file=""c:/file1.csv"", header=TRUE, sep="","") 
    MyData2 <- read.csv(file=""c:/file2.csv"", header=TRUE, sep="","") 
    MyData3 <- rbind(MyData1, MyData2)

File contents:

    Name,ID
    Dave,1
    Barry,2

and

    Name,ID
    Ian,3
    Kevin,4
",1529592803.0
LogicalRisk,"u/Digging_For_Ostrich answer works if the csv files have the exact same column names. R will throw an error if they do not.

If they have different structures one solution using dplyr

    library(dplyr) 
    
    df1 = read.csv(file = ""~/file1.csv"", header = TRUE, sep = "","", stringsAsFactors = F)
    df2 = read.csv(file = ""~/file2.csv"", header = TRUE, sep = "","", stringsAsFactors = F)
    
    dfs = bind_rows(list(df1, df2)) # gets every column from both dfs, place NAs for non-overlap
    
    # OR if you just want the overlap
    # return all rows from x where there are matching values in y, and all columns from x and y
    dfs2 = left_join(df1, df2, by = c(SHARED_COLUMNS)) 

You could certainly do this in base as well. Search the merge() function.",1529593341.0
Oct8-Danger,"Docker makes this super simple with the docker/rocker container.

You Can spin up the virtual environment in seconds on any vm with docker installed

https://hub.docker.com/r/rocker/",1529580848.0
RememberToBackupData,"Easy using `ifelse()`, which works very well in a dplyr pipeline. dplyr also has `case_when()`, which is a switch statement that will go inside a pipeline also. ",1529528996.0
grasshoppermouse,"Hmm, not sure I completely understand your question, but does this work?

    A <- rbinom(n = 100, size = 1, prob = 0.4)
    B <- rbinom(n = 100, size = 1, prob = 0.42) # Because many 1's will get set to 0
    C <- rbinom(n = 100, size = 1, prob = 1.0)
    B[A==1] <- 0
    C[A==1 | B==1] <- 0
    df <- data.frame(A, B, C)",1529532474.0
Alrik,Tidyverse::mutate will let you create columns that are functions of other columns.,1529530030.0
emiltb,"You might want to take a look at [http://r-pkgs.had.co.nz/r.html](http://r-pkgs.had.co.nz/r.html)

A common approach is to make changes to you local version of the packages and then use \`devtools::load\_all()\` and test your functions. You can also choose to build the package on you system - that will also install the local version. It is not necessary to push your changes to github and install from your branch online. ",1529522115.0
millsGT49,Devtools also has an install local option (on mobile so not sure exactly which one) so you can try out your local changes before pushing it.,1529521214.0
LogicalRisk,by\_row() was moved to purrrlyr. [See the reference here](https://cran.r-project.org/web/packages/purrrlyr/purrrlyr.pdf). ,1529524233.0
maltiv,"I think by_row was just moved to «purrlyr», so as a quick fix you can just install that. ",1529524464.0
zdk,"Hadley is working on multidplyr but it's not in CRAN yet
https://github.com/hadley/multidplyr

I personally like `future` package for generic parallelization as well as the `batchtools` implementation.",1529525522.0
ran88dom99,"(set up pipelines in r)[https://www.reddit.com/r/rstats/comments/8pnv45/how_to_setup_data_pipelines_in_r/] parallelization

I need to run dimensional reduction on datasets far bigger than my ram.",1529632657.0
prashanthsriram,"There is an interesting new package on CRAN based on the ""purrr"" tidyverse package for working with lists and the ""future"" package for parallelization, called ""furrr"":

GitHub - https://github.com/DavisVaughan/furrr/blob/master/README.md 

CRAN - https://cran.r-project.org/web/packages/furrr/index.html


I haven't tried it yet, so this is just ""Hey, I saw this on Twitter recently and it looks cool"" and not an endorsement (yet).",1529570589.0
Quasimoto3000,"What’s the use case? For normal operations, the overhead involved in passing data around multiple processes doesn’t add up to speed improvements.

For long running operations that can be done in parallel I’ll use foreach. 

Most model interfaces, such as XGBOOST, have an nthreads parameter.

If I have enough data for parallelization to be worth it, I’ll use Spark and SparklyR.",1529585460.0
iconoclaus,"so.. rtichoke is invoked from CLI using `r`? Does it just alias over r or is it installed as a plugin?

wouldn't Rstudio thus load up rtichoke by default?",1529504315.0
,[deleted],1529522211.0
mguzmann,"What's the point though? If I need to call a python function from R I can already do that, same if I need to call an R function from python. There is also babel in Org mode already. Also, ESS is much better than rstudio (and than whatever this is) anyways.

Btw, the main issue with this kind of approach where you mix languages is how you share objects between them.",1529485009.0
ELKronos,"Well, there are density plots, and there are stacked bar charts. I am not too sure what you are looking for, so here are some sources

1. [Stacked bar charts](http://ggplot2.tidyverse.org/reference/geom_bar.html)
2. [Examples on editng coloring in stacked bar charts](http://www.sthda.com/english/wiki/ggplot2-barplots-quick-start-guide-r-software-and-data-visualization)
3. [More examples onf editing the formatting of stacked bar charts](http://rstudio-pubs-static.s3.amazonaws.com/3256_bb10db1440724dac8fa40da5e658ada5.html)
4. [Density plots](http://ggplot2.tidyverse.org/reference/geom_density.html)
5. [Density histograms](http://ggplot2.tidyverse.org/reference/geom_histogram.html)
6. [Here is an excellent library of different charts](https://www.r-graph-gallery.com/48-grouped-barplot-with-ggplot2/)
7. [Other samples](https://rstudio-pubs-static.s3.amazonaws.com/228019_f0c39e05758a4a51b435b19dbd321c23.html)",1529456236.0
Drewdledoo,"Thanks for updating with the extra picture. What do you want the colors to correspond to? I'm guessing the `Infected` variable?

If I'm guessing your research question correctly, you want to see how the distribution of the `Length` differs between infected (`df$Infected == 1`) and uninfected (`df$Infected == 0`) subjects over time?

If so, I think there is a much easier alternative to seeing the trend over time than trying to do this with `ggridges`. I think if you instead use `facet_wrap()`, you will also get the effect of seeing the distributions spread out vertically (so you can easily compare), but a much better look (IMO) than with `ggridges`. 

Here are two plots I made using the `df` you pasted in your other reply to me, one using `geom_density` (not `geom_density_ridgelines`) in case you wanted a smooth density representing frequency, and another using `geom_histogram` in case you wanted the density as bars representing counts:
    
    library(ggridges)
    library(tidyverse)
    
    # Change Infected to TRUE/FALSE scale
    df <- df %>% mutate(Infected = ifelse(Infected == 1, TRUE, FALSE))
    
    # Plot a smooth density representing frequency
    ggplot(df, aes(x = Length, fill = Infected)) +
      geom_density(alpha = 0.7) +
      facet_wrap(~SampleDate, ncol = 1) +
      scale_fill_manual(values = c(""green"", ""black""))

[Here's the density plot.](https://imgur.com/FpF5Pah)

    ggplot(df, aes(x = Length, fill = Infected)) +
      geom_histogram(alpha = 0.7) +
      facet_wrap(~SampleDate, ncol = 1) +
      scale_fill_manual(values = c(""green"", ""black""))

[Here's the histogram plot.](https://imgur.com/jT1gjO3)

Hope that helps!

PS: For some weird reason, I can only see your reply to my other comment by going to your history. Not sure what that means (shadowbanned? error on reddit's side?), but just wanted you to know!",1529514000.0
Drewdledoo,"Can you post some sample data (you can paste the output of `dput(your_data)` from within your R session) and tell us how the pictures you posted are similar/different than what you want? The code you posted (which I'm guessing creates the second plot you posted) doesn't have a `fill` aesthetic argument, which might just be the ticket to get what you want.

If you edit your post with some data for us to play around with, we can tinker with it on our own until we get what we think you want and then tell you how we did it!

Separately, I think `ggridges` are more suited towards plotting several distributions on their own ""lines"" rather than on the same line as in the stacked density plot you posted.",1529460491.0
bob_arnold,"https://cran.r-project.org/web/packages/ggridges/vignettes/introduction.html

Should contain all you need. I recently made of bunch of these plots with no experience and everything I needed was on that page ",1529461577.0
YepYepYepYepYepUhHuh,Are you using the [geom_raster()](https://www.rdocumentation.org/packages/ggplot2/versions/1.0.1/topics/geom_raster) function maybe? This uses linear interpolation...,1529438544.0
Darwinmate,"Your question is a bit confusing, define ""method"".",1529452064.0
Binary101010,You mean the geom_tile() function in the ggplot2 library?,1529436247.0
mjwalds,Do you mean your data is lat/long and you're trying to combine with coord_map to use a map projection?,1529449081.0
azdatasci,Git,1529412710.0
ychinenov,"I second git. R studio, indeed, makes using git very convenient. Depending on your preferences you can either use GUI in R studio or do it from a command line in Rstudio terminal. ",1529414008.0
hyperplanes,"A couple of things:

- Your sample code is missing the family argument, so as it is it seems to me you're running a linear regression (default argument is gaussian).

- How is workload coded exactly? I'm asking because you said you're running a logit, but your dependent variable ranges from 0-50. That can't be right, it needs to be a binary variable. Is workload binned into categories and you're running a multinomial logit? If so, that also needs to be specified in your code. There's also the fact that workload (I'm assuming in hours) is a variable that would make the most sense and be most easily interpreted as a continuous variable in most contexts. In that case you'd just run OLS.

- McFadden's pseudo-R2 is a fairly common measure of explained variation in logit models.",1529409298.0
NSAkela,"You can try relative goodness of fit, e. g. Akaike Information Criterion or Bayesian Information Criterion. You can compare your model with null workload ~ 1 using AIC(fit0, fit1).  And workload doesn't look like binary variable, maybe you should transform it into binary workload less than x hours or workload more than x hours, then logit model would be more meaningful. ",1529402190.0
StephenSRMMartin,JASP is another. Its backend is R.,1529426426.0
grasshoppermouse,https://rstudio.cloud,1529533046.0
ran88dom99,stagraph has 2 discrete variable graphs?,1529631200.0
samclifford,"For starters, use case_when when you've got multiple options for matching on the conditions.

Edit: actually I suggest looking at the forcats package to see how factor relabelling can be done. ",1529395972.0
ararelitus,"I'm not sure what you mean by variable labels. If you are referring to row labels, these are removed by most dplyr functions and there is no way to prevent it. Just do something like df$id = row.names(df) first.",1529403993.0
oggesjolin,"Are you saying that you all losing labels for ""dAge"" when creating ""dAge2"" with mutate, or that you were expecting labels in dAge2? I'm assuming you're reading the SPSS data with haven::read\_sav(). Are the ifelse lookups done on the value labels from column dAge? Do you need labels on dAge2?

Have a look at sjlabelled::set\_labels (in case you want labels on dAge2), maybe:

library(sjlabelled)  

df <- df &#37;>&#37;   

  mutate(dAge2 = case\_when(dAge == 1 \~ 1,  

dAge &#37;in&#37; 2:4 \~ 2,  

dAge == 5 \~ 3,  

TRUE \~ 4)) &#37;>&#37;   

  set\_labels(., dAge2, labels = c('GEN Z', 'GEN Y', 'GEN X', 'BABY BOOMERS'))  

I have never lost attribute ""labels"" in labelled classed columns in a data.frame read by haven when adding columns through dplyr::mutate(), but I don't really think that is what you're saying?",1529430558.0
efrique,"you should be able to refer to factor levels as factor levels

    > myf<-factor(sample(letters[1:5],9,replace=TRUE))
    > myf
    [1] c e c d e d d e e
    Levels: c d e
    > myf2<-factor(sample(1:5,9,replace=TRUE))
    > myf2
    [1] 3 4 5 3 4 1 5 2 4
    Levels: 1 2 3 4 5
    > myf3<-factor(sample(1:5,9,replace=TRUE))
    > myf3
    [1] 4 3 2 3 3 1 2 4 2
    Levels: 1 2 3 4
    > myf3[(myf==""c"" | myf2==3)]<-1
    > myf3
    [1] 1 3 1 1 3 1 2 4 2
    Levels: 1 2 3 4
    

",1529366102.0
,"You can refer to the values, and not the levels? If you're going to be using the levels as handles in your working, why are you using factors and not just integers?",1529352240.0
displaced_soc,"Coming from social sciences (with a lot of categorical variables) it seemed extremely burdensome to use factors and not simple numbers like in SPSS. Close to one year after, it’s a blessing, code is always clear, sometimes it needs a line or two more writing but is waaaay better.

For example

    # immigration background
    GSS.14$pcitizens <- Recode(GSS.14$parcit, “’BOTH WERE CITIZENS OF AMERICA’=‘YES’; NA=NA; else=‘NO’”)
    GSS.14$immigrant <- !((GSS.14$citizen %in% c(“YES”)) &
                           (GSS.14$parcit %in% c(“BOTH WERE CITIZENS OF AMERICA”)) &
                           (GSS.14$born %in% c(“YES”)))
    GSS.14$immigrant[is.na(GSS.14$citizen) & is.na(GSS.14$parcit) & is.na(GSS.14$born)] <- NA
    GSS.14$immigrant <- as.factor(GSS.14$immigrant)

(It’s old code, I switched to “_” in names.) Also Recode from car package was great for me (although with factors I often defolt to simple base functions).

I also have some samples of more complex recordings, I can share them later  if you want (main thing I use is  iterating between char variables and factors if I need to do a lot of add/remove etc.)",1529369582.0
vogt4nick,forcats,1529373317.0
SamSamSammmmm,Hi! It's kind of hard to understand what you're saying. Could you please give us a snippet for the part of code for this and a console output when you run it? Thank you.,1529350177.0
jimbean66,What package are you using?,1529355417.0
DrLionelRaymond,Sounds like your dealing with a dataset that had information added to it via the [labelled package](https://cran.r-project.org/web/packages/labelled/vignettes/intro_labelled.html).  Look at the section on mutating to factors to remove the labels. You should be able to coerce into a dataframe once the labels are removed. ,1529375333.0
crieo,"Sorry for my late response. The issue was disappearing by starting from scratch again. So i bet i made an error whilst extracting.

But thanks for trying to help me :)  
",1529393854.0
sigriv,Found your post through search since I'm having the same error! I solved it using tibble(),1533172855.0
efrique,">  I know closer to -1 means no correlations, whereas closer to 1 means a high likelihood of correlation. 

Neither of those is a correct statement. Let's start with a general sense:

Positive correlation: if I am high, you tend to be high, if I am low, you tend to be low

Negative correlation: f I am high, you tend to be low, if I am low, you tend to be high

(in each case high and low is relative to our own average)

generally, by correlation (with no other adjective) we mean linear correlation and specifically Pearson correlation


1 is perfect positive correlation; our values are linearly related (if we look at a plot they always lay on a straight line sloping up)

-1 is perfect negative correlation  (if we look at a plot they always lay on a straight line sloping down)

>  is there a range that is accepted as, somewhat of a correlation, range of ""high correlation"", range for ""low correlation"", etc

Not generally. Within a certain discipline, you may have a conventional sense of what's high or low, but a high correlation to a psychologist may be uselessly low to a chemist

If you look at the wikipedia page on [correlation and dependence](https://en.wikipedia.org/wiki/Correlation_and_dependence) it has a picture that shows a range of correlation values when the variables are linearly related, as well as various different kinds of nonlinear relationship that would produce 0 correlation:

https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Correlation_examples2.svg/400px-Correlation_examples2.svg.png

You may also find it helpful to read the article on the [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)",1529341163.0
julialert,"Uh, an r value closer to -1 should mean it has a negative correlation. Closer to 0 regardless of negative or positive means no correlation. Likewise, closer to 1 means positive correlation.

Usually above .50/-.50 is good but it really depends on the situation, as sometimes .40 or .35 can be an indicator of something. There is no right answer for all situations. The ranges for low correlation and high correlation can be easily debatable but generally the closer you are to 1 or -1, the ""better"" (in terms of being correlated).",1529336298.0
,"If you look at the cor documentation, you'll see that the default 'method' is [Pearson](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient).",1529335062.0
LiesLies,"I would take a look at this page, it seems like a good overview at first glance:
 http://www.statisticshowto.com/probability-and-statistics/correlation-coefficient-formula/

A correlation of 0 implies no linear association between a and b, and a correlation of 1 or -1 implies a strong linear association in either direction. There's a lot to explore once you have grasped this basic (Pearson) correlation, for example: 

- what if the data aren't linearly associated? 
- what if I don't want to weight things like the Pearson formula does? What if I don't want to use the raw values directly at all? (Hint: Spearman!)
- what if I have more than 2 variables?",1529339726.0
TheArtilleryMan,There is a really good site I use called laerd statistics. Helped me massively with my stats degree and has very good explainations on everything from t tests and correlations to regression. Hope it helps!,1529343868.0
Ax3m4n,"For all intents and purposes you can ignore the backticks. But it is weird that they appear in this case. Perhaps a newly introduced bug in `list`?

`x` is not particularly the problem, since you can switch `x` and `y` around and `y` will get the backticks.",1529317297.0
Economist_hat,"Bizarre.  


Backticks not present in 3.4.1 (Windows MS R server version)  
Backticks present in 3.5.0 (Windows CRAN version)",1529341072.0
Er4zor,[Reproducible on Windows](https://stackoverflow.com/questions/50387825/unnecessary-backticks-in-r),1529333376.0
kenderpl,"    > list(z = c(1,2,3))
    $`z`
    [1] 1 2 3

    > list(a = c(1,2,3))
    $`a`
    [1] 1 2 3
    > list(a = c(1,2,3), b = 1)
    $`a`
    [1] 1 2 3
    
    $b
    [1] 1
         
    > list(b = c(1,2,3), c = 1)
    $`b`
    [1] 1 2 3
    
    $c
    [1] 1

It seems that first element of the list will have its name in backticks . Backticks do not change the meaning of what is between them so you should not worry about it. I don't know why this has changed in 3.5 though
",1529356988.0
GreyZephyr,I have also reproduced this on Windows 10 with R 3.5.0,1529379159.0
KopfJ4ger,"You're more likely to get help if you post your specific sticking points with code and example data. Kind of like StackOverflow. If you're not willing to do that, at least give us an idea of what you're asking for help with.",1529297887.0
,www.datacamp.com.,1529297165.0
hopeyesperanza,"Do your readings, attend lecture, and pay attention in lab.",1529282946.0
MrLegilimens,What do you think? What do you have so far?,1529284084.0
Dmicke,"Is this in a data frame?  Is drgcode a factor?
The following will only work if it is a data frame and everything is a numeric.  use str(data) to see if anything has been set up as something else.

you could try something like 
AmtReim.mean<-mean(data$AmtReim[data$drgcode == 293 && daata$dischdest == 62 && data$Age >=5,])

I haven't actually tried the code, i might be a bit off.

",1529282211.0
,"From the 'pairs' documentation
>Logical and factor columns are converted to numeric in the same way that...

So pairs requires a numeric. You're going to have to reassign the factors to the levels after the fact, unless 'pairs' has an option to not act like this.

The numbers you're seeing instead of the factor are the levels of the factor, obviously.",1529253722.0
,[deleted],1529252290.0
,Why not just use termux to ssh to a machine with R on it? ,1529215431.0
ryapric,"This is a very thorough write-up; but why not just run on something like [GNURoot Debian](https://play.google.com/store/apps/details?id=com.gnuroot.debian)? Installing R (and any other deb software) is already well-documented, and that app doesn't actually require root access, as the name implies.

Edit: I'd also add that compiling the tidyverse needs *at least* 1GB of *spare* RAM, so less-equipped phones may or may not be able to accomplish this by any means.",1529248107.0
Stolwe,Works great. Thanks a lot,1529239755.0
ToxicPennies,Thank you for sharing! ,1529201735.0
rudeevil,Though I'll not really try this right now as I don't think I really need it at the moment. But very useful post and who knows maybe I'll soon need this. :) ,1529264799.0
mouse_Brains,Sorry to revive this after all this time but question: Can you run shiny apps locally using this?,1537409029.0
AllezCannes,"`group_by(ID) %>% # Group by ID`

No shit.",1529164224.0
TS_Sama,ooo how did you achieve this?,1529147507.0
,I can't wait to fat finger parentheses in addition to losing and adding them as usual.,1529153138.0
jsgrova,This absolutely does not need to happen,1529161836.0
seitgeist,the horror,1529159986.0
statbro,Blood rushed to my penis,1529157983.0
guerisimo,I already feel like my monitor at work is too small though,1529161665.0
avamk,Cool... but why? What's the use case?,1529160363.0
GGG_Dog,Why tho?,1529176166.0
htrul18,Is that a Pixel?,1529164369.0
goddammitbutters,Is that... emacs and ESS? How..? I want this!,1529148677.0
Neuro_88,WOW!! 😮 ,1529167233.0
Arsonade,Yes! Been wanting this for ages,1529181484.0
rflight79,"Probably using GNUroot Debian or Wheezy?? Basically installs a Linux distro on your phone, then install r-base",1529183928.0
karafili,"Hmmm, like it",1529204227.0
brbecker,Could you sample the dataset?,1529122309.0
WhosaWhatsa,A sample size calculation relative to the effect size you're looking for and the variance could work. Then sample that many at random a few times to get an expected value. That work? ,1529125193.0
dm319,"Could dimension reduction help with reducing your measured variables?  Clustering might help with finding interesting populations to look at.

Another approach might be to make sure you are using performant code.  I recently compared some simple code that used a loop in Julia (0.5s) vs split-apply-combine in R (10+s) versus a loop in R (nearly 1000s!).  Have a look at this [article](https://www.reddit.com/r/programming/comments/8ljjzm/commandline_tools_can_be_235x_faster_than_your/) for example.  Different packages and techniques in R can have 100x differences in speed.  Maybe checkout the data.table package which focusses on speed.

Failing all that, consider renting some heavy metal from someone like Amazon.",1529246752.0
fonzy6,"There’s the biglm package for R that you can try. Supposed to run regressions on data too big to fit in memory. 

There’s also a spark database although I’ve never tried it. 

http://spark.rstudio.com
",1529248097.0
pettyferrari,"Are you saying you don't care which points are connected with a line? If so, you can create a random grouping variable in your dataframe and use that with the group argument. Otherwise you can manually create a grouping variable to render the lines you want.

Either way, if you want to do this entirely within ggplot the group argument is probably your best bet.",1529083087.0
SteveDougson,">  I would like to avoid having to make a bunch of temporary columns if possible

I still feel like there's a more elegant solution out there but I was able to get my desired output with just one temp column.

    ... %>%
    mutate(temp = case_when(str_sub(.$Airport1, 1, 1) == 'Z' & .$Airport1 != 'ZBF' ~ .$Airport2,
                            TRUE ~ ''),
           Airport2 = case_when(str_sub(.$Airport1, 1, 1) == 'Z' & .$Airport1 != 'ZBF' ~ .$Airport1,
                            TRUE ~ .$Airport2)) %>%
    mutate(Airport1 = case_when(.$temp != '' ~ .$temp,
                                TRUE ~ .$Airport1)

There are two mutate calls because I was getting an ""Unknown or uninitialized column: 'temp'"" error when they were all one.",1529074512.0
Ax3m4n,"depends on your tree and your preference, `igraph`, `ape`, `ggtree` & `ggraph` are good options.",1529068830.0
lasagnwich,"[lme4](https://cran.r-project.org/web/packages/lme4/lme4.pdf) is the go to package for mixed models and there is also a very useful JISC mailing list (r-sig-mixed-models@r-project.org) that help you with queries that is run by academics in mixed modelling / authors of the package. I wouldn't email the group unless you can't find the answer in the archive of the mailing list, or in the lme4 vignette. 

Also Bristol university run a[ free online course](http://www.bristol.ac.uk/cmm/learning/online-course/course-topics.html) in multilevel modelling which I used before I started.

I'm no expert but I have done a research project using MLM in R so I can offer some help / advice to another novice.",1529056467.0
StephenSRMMartin,"[https://cran.r-project.org/web/packages/FactMixtAnalysis/FactMixtAnalysis.pdf](https://cran.r-project.org/web/packages/FactMixtAnalysis/FactMixtAnalysis.pdf) May be one option.I don't know of any others, unless you want to get your hands dirty in rstan (which requires a fairly complicated stan model; doable, but not easy as a first project in Stan).

As for 'opinions'; They're just used for different things, or are special cases of one another.

FMM and LCA are both mixture models. Latent class model = A latent group exists; can we recover these groups from the responses. Factor analysis = A continuous latent variable exists; can we recover these scores from the responses. FMM = A continuous latent variable model exists; are there latent groups for which this model varies?

They don't really compete with each other. You can sorta think of a factor analysis as just a FMM, where you assume only one group exists. A latent class model is sorta like a factor analysis, except you assume latent groups exist instead of latent scores.",1529910028.0
,[deleted],1529049126.0
zdk,"Since you're applying the LASSO, the r^2 isn't that meaningful. 

You'd should prefer to assess model fit using some post-selection inference test, which also corrects for the variable selection.

Here is some software: https://github.com/selective-inference/R-software
a method designed for cross-validation + LASSO: https://arxiv.org/pdf/1511.08866.pdf",1529028212.0
NSAkela,"Predict values of dependent variable with model, calculate Pearson r of predicted and observed values and take square of r. Alternative approach is to calculate variance of observed values (total sum of squares, TSS) and sum of square roots of (observed - predicted)^2 (residual sum of squares, RSS). R^2 = 1-RSS/TSS. You can use RSS to calculate information criterions (AIC or BIC) in order to prevent overfitting. ",1529056198.0
bc2zb,I found this [gist](https://gist.github.com/jennybc/e9e9aba6ba18c72cec26#file-2015-03-02_plot-next-to-table-rmd) which seems to handle what you want. ,1528998732.0
anotherep,"You can try using CSS to create a page column layout. 

For the CSS, I add this to the start of the markdown:

    <style>
      .col2 {
        columns: 2 300px;         /* number of columns and width in pixels*/
        -webkit-columns: 2 300px; /* chrome, safari */
        -moz-columns: 2 300px;    /* firefox */
      }
    </style>

Then surrounded anything I want in columns with:

    <div class=""col2""> 
    ....
    </div>
",1528992569.0
mattindustries,What do your tables look like? There are some responsive tables that go from horizontal to vertical you could use. ,1528997068.0
Owz182,"I had to do this, I used tablegrob to output the table to a png, then used imagemagick to overlay the table in to some white space at the side of a graph. There might be a more efficient way to do it, but it worked for me. Not at my pc for a few days but can post the code up if you like?",1529037155.0
Rhyoung3,I use the htmlTables package - takes any matrix or data frame and fully wraps it as an HTML table. Also has extensive CSS scripting options. ,1529060373.0
I_just_made,"Something you could try is using the kableExtra package if you are using kable to make the table.

You would have something like:


this is some filler text to show Rmarkdown and positioning of a table blah blah.

```
kable(df,format = ""html"") %>% kable_styling(full_width = F, position = ""float_right"")
```
followed by:

```your plot output```",1529063038.0
lucibelloj,Have you tried dbSendQuery()?,1528988340.0
RememberToBackupData,"I would just make a new column using, like, `paste(year(Timestamp), week(Timestamp), sep = ""_"")` and aggregate on that.",1528940211.0
sparkplug49,"I would use mutate(., date = floor_date(date, ""week"") as you're aggregation variable.",1528948813.0
jiminie,Check out tibbletime package.,1528974412.0
masskodos,"Combine the data sets and include an interaction term for sex and degree, and potentially sex and each characteristic.",1528914441.0
NSAkela,"I recommend you not to use logarithms here because it is hard to interpret coefficients in such cases. If log(wage) equals k + b2×y + b3×z + e then wage equals k × y^b2 × z^b3 × e, but it is very unlikely that multiplication is defined for your variables. I don't know physical meaning of degree^-1 for example or degree^0.5 so better keep things clear. If the wages' distribution isn't normal then you can transform wage in ""below threshold / above threshold"" and solve the problem with logistic regression (and take an advice from masskodos about an interaction term). ",1528925349.0
efrique,">  I ran the same model twice: one for men and one for women. 

*why*?

Much easier to deal with in a combined regression.
",1529886617.0
,Why does anyone use this? I never quite understood. ,1528916713.0
dm319,"Very nice, thanks for posting this.  Could you apply it to other road races, like the NW200, and compare?  Could you control for number of entrants?  Or even do a survival analysis by miles of track covered?",1528890495.0
,[deleted],1528888024.0
DeuceWallaces,"Nice post. I learned bayesian in JAGS, but man STAN has some nice packages to do some simple to intermediate stuff. I think I'm going to have to force myself to pick it up.",1528901790.0
RememberToBackupData,"I use knitr::kable and kableExtra for extra styling options. Works really well in my Rmarkdown docs, and kableExtra even lets you add more formatting like colouring cells conditionally and stuff. 

See here for a nice tour of what kableExtra can do: https://haozhu233.github.io/kableExtra/awesome_table_in_pdf.pdf",1528846138.0
mghoff330,"The DT package for shiny apps, specifically, is great. 

Can’t speak to Rmarkdown, sorry. ",1528845841.0
ohnodingbat,"I swear by Huxtable for html tables. But have not tried it in Rmarkdown

Caveat: Any search brings up a boatload of hits for that other Huxtable, Cosby",1528846456.0
VisuelleData,Stargazer has been recommended for this before. Not sure if it has shiny support. ,1528849993.0
grandzooby,"I've used kable, but Pander does a nice job too.",1528879659.0
lemur78,"DT for Shiny apps, sometimes kableExtra for small tables (expecialy when I need to extra styling for cells) and kableExtra for PDF/HTML from RMarkdown.",1528882242.0
eggplantae,"I like using ""stargazer"". However if you want to exclusively display tables, then I would recommend using ""xtable"".

Simply google ""stargazer"" or ""xtable"" and it will show you the syntax. Stargazer is very simple and it's particularly good for displaying regressions. Xtable is particularly good for printing stuff onto Latex.",1528910164.0
fonzy6,Kable for easy implementation and huxtable for more options.,1528914647.0
,"I use kable and kableExtra unless I need to output to Word. If I'm outputting to word I just use kable and then I use a word macro that re\-formats all the tables in the document. Trying to get nice looking tables in Word directly from RMarkdown I found to be a herculean task, but writing a macro isn't too bad and I can use a .dotm template that includes the macro code I need.

For HTML, definitely kableExtra.

I also recommend using a wrapper function for all of your tables so you can easily tweak the format in one place in your code.",1529002061.0
responsible_dave,I've liked using pixie dust.  ,1529037345.0
DemonKingWart,The forecast package has a tsCV function that effectively backtests a model. ,1528856454.0
whogan,"I found it.
Here is a demo:
https://twitter.com/Rami_Krispin/status/997909832387575808",1528860351.0
BeerSharkBot,"Hey maybe this link will help
https://www.reddit.com/r/rstats/comments/8qmzum/time_series_backtesting",1528845600.0
mattindustries,"You can put your DFs in a list and then use

    lapply(names(df.list), function(df.name) {
        df.list[[df.name]]$WAVEN[df.list[[df.name]]$WAVEN %in% c(""0."",""99"")] <<- NA
    })

EDIT: Made it work",1528838154.0
shujaa-g,"You should fix the problem upstream. However you read these data frames into R, there is probably an `na.strings` argument which you can use to specify what values get converted to `NA`. Put these values in that argument so the conversion is automatic as the data is loaded.",1528840584.0
chonggg511,"Yeah check [Advanced R](http://adv-r.had.co.nz/Functional-programming.html), The functional programming chapter has you create a function then looping through the columns with `lapply()`. Good luck!",1528840432.0
allid84236151,Thanks everyone! Ill let you know how it goes when i get to work tomorrow!,1528850813.0
allid84236151,correcting in the read in for the df using na.strings worked wonders thank you!,1528902754.0
Shushani,"It’s good practice not to use spaces in column names. In your source file, separate your headers with underscores instead. Then replace all dots in the ‘query’ with underscores also :)",1528832479.0
questionquality,"""Reshaping from wide to long"" means to go from a dataframe where you have some data in the column names and want to move that data into a column itself ([r4ds on tidy data](http://r4ds.had.co.nz/tidy-data.html#spreading-and-gathering)). That's not what you have here: Neither VALUE, QTY or ID are really data points, and they're column labels as they probably should be. Instead, you want to make multiple rows out of each row in another way. I found the best way to do this is often two-fold: First, find a function that will transform the data from one row into the multiple rows you want it to. In this case, `rep(VALUE / QTY, QTY)` will do. Then vectorize that with the `map*`family from `purrr` (part of the tidyverse) to produce a list column and `unnest()` that list column.

    library(tidyverse)
    mutate(DF,
           VALUE = map2(VALUE, QTY, ~rep(.x / .y, .y)),
           QTY = 1) %>%
        unnest()

([r4ds on list columns](http://r4ds.had.co.nz/many-models.html#list-columns))
",1528835499.0
questionquality,">  I do have an AC level of 0 which does not have a corresponding gam...might this be the issue?

Yes, this might be the issue. `gs[0]` will return a numeric with length 0, which will give an error when you try to call `predict()`on it. Try debugging by running your for loop one round at a time with `       for(i in c(1){` etc.",1528817382.0
constorm,"This is exactly what peer review is for. You're more likely to get reliable feedback from experts through the peer review system of a good journal than looking for help from strangers on Reddit.

Good peer review isn't an obstacle to overcome, it is a tool to improve your work",1528806467.0
TalkterWho12,"You can try it out as a pre-print! I have yet to put something up there, but it would address your concerns. Look for an appropriate resource for one given your area of study. ",1528838080.0
MrLegilimens,"Hope your title isn’t as vague as this one was.

You should have put the topic in there.",1528804763.0
jeremymiles,Mabye: https://www.rapidtables.com/math/symbols/Set_Symbols.html,1528764255.0
efrique,https://en.wikipedia.org/wiki/List_of_mathematical_symbols,1528802323.0
mowshowitz,"This might be overkill, but I have a similar level of formal math background and I found [this course](https://www.coursera.org/learn/datasciencemathskills/home/welcome) very helpful in helping me get up to speed.",1528809730.0
RememberToBackupData,"Given a dataframe (`raw_data`) that looks like this, with multiple rows per date: 

    datetime                tempC
    11/08/2016 5:06         7
    11/08/2016 6:06         7
    11/08/2016 7:06         6.5
    11/08/2016 8:06         8
    11/08/2016 9:06         10.5
    11/08/2016 10:06        13
    11/08/2016 11:06        14
    11/08/2016 12:06        16
    11/08/2016 13:06        16
    11/08/2016 14:06        17

You can simply use `dplyr` to add a new column for the date component only, group by that column, and then summarise the table to get rounded daily mean temp:

    library(lubridate)
    library(dplyr)
    
    mean_data <-
        raw_data %>%
        mutate(datetime = dmy_hm(datetime),
               date     = date(datetime)) %>%
        group_by(date) %>%
        summarise(meanTemp = round(mean(tempC)))",1528759984.0
LogicalRisk,"The dplyr way given by /u/RememberToBackupData should work just fine, but it is worth answering why you are getting that particular error message.

R is telling you that you are trying to pass an argument that is not a numeric or logical object. My guess without seeing your data frame is that TempT$Temp is a character vector. e.g.

    mean(""e"")
    [1] NA
    Warning message:
    In mean.default(""e"") : argument is not numeric or logical: returning NA",1528767207.0
Spudjnr123,"For context, here is the full script

     # generate mean temps for each day
    
          Year <- c(rep(2018, 155))
          DateX <- c(as.Date(as.Date(""1-8-18"", format = ""%m-%d-%y""):
                            as.Date(""6-11-18"", format = ""%m-%d-%y""), 
                            origin = ""1970-01-01""))
          Date <- format(c(as.Date(as.Date(""1-8-18"", format = ""%m-%d-%y""):
                            as.Date(""6-11-18"", format = ""%m-%d-%y""), 
                            origin = ""1970-01-01"")), format = ""%m/%d/%Y"")
          Day <- c(1:155)
    
          TeTv <- rep(NA, length(Day))
          for(i in 1:length(Day)){
    
            TeTv[i] <- round(mean(TempT$Temp[which(as.Date(TempT$Date, 
                              format = ""%m/%d/%Y"") == DateX[i])]),2)
          }
    
          TeT <- data.frame(Year, Date = DateX, Day, Temp = TeTv)",1528748620.0
Laerphon,"Your format string specifies year as a two digit year while your actual values are four digit year. Use format=""%m/%d/%Y"" instead.",1528747334.0
esotericish,"also, if you're working with dates, check out the lubridate package! it's one of my favorites.

https://lubridate.tidyverse.org/",1528752288.0
zdk,"You're looking for polynomial regression. Another option, to stay with a linear model with a simple coefficient, is to log transform the data.

that is:

    y = k x^a
    log(y) = log k + a log x

Then you can use lm to get a. ",1528745280.0
efrique,"There are a *bunch* of different ways to fit such a model. 

Why so many? In part because your model is incomplete. 

Which you would use depends on the rest of the model, or at least some aspects of it - hopefully things you know/assume about the variables (or alternatively on your loss function).

What's missing is the distribution about the relationship, of which the most important aspect is typically the variance.

consider if Y is necessarily positive -- then small values have less capacity to vary than large ones and you find the spread increases with the mean; often in that case working with a linear regression on the log-scale carries the double advantage of linearizing the relationship and making the variance more stable (which results in appropriately putting less weight on the data that is on average further away from the center).


But sometimes that's not the case. For count responses it tends to put too much weight on the smaller-mean values, for example. You may be better off with a count GLM.

If your y's may be negative (and sometimes even when that's not the case) the spread may be almost constant as x changes. In that case you may be better off with nonlinear least squares.

Even when taking logs does work well, it may not be the best choice; it depends on what you want to use it for. If you exponentiate back you don't get a model for the conditional mean of y for example (if you want that, you might be better off looking at another GLM perhaps, or something similar).

So ... can you say something about your y-variable? What kind of a thing is it? Have you seen similar sets of data before this one?  


[Someone mentioned polynomial regression; that might be okay if you know *a* but I wouldn't suggest it as a particularly good way to estimate *a*]",1528754502.0
LbrsAce,"You forgot to update this paragraph:

>A confusion matrix is used to determine the number of true and false positives generated by our predictions. The model generates 13 true negatives (0's), 19 true positives (1's), while there are 8 false positives.

Great post!",1528803766.0
yoyomac,"Nice! I find the 92.5% accuracy to be quite high actually!

Besides the features you mentioned another important factor would be how much the company paid a dividend in the previous year (so that the investors ""expect"" a dividend). I wonder if that will increase the accuracy even more or is it correlated with your factors.

",1528807082.0
accidentlyporn,"I feel like more than 3 features should've been used, but I don't know enough about economics to make a hard call on it. 80% seems pretty low for a supervised boolean classifier.",1528740209.0
master_innovator,Won’t work..,1528745490.0
spannbowser,"I don't use them too often, but recently I used one for what felt like a cleaner way to chunk up larger SQL queries.  The factory returns a function that can be called to return the next `n` rows of a big query result.  [This gist](https://gist.github.com/AdamSpannbauer/b04c1f6243ce07a5d2e0c9eb78502a55) shows the logic used.

*Note it is written for SQL server; different SQL flavors use different OFFSET/LIMIT syntax*

-------

I'd be curious to hear if anyone has any other strategies for chunking queries in R.",1528724543.0
StephTheChef,"Could this work? R seems to ""auto-pick"" different linetypes here but I think it should solve your problem
> graph.1 <- ggplot(data = data.frame(x = 1:1000, y = cumulative.sum.of.means), 
                  aes(x, y)) + 
  geom_line(size = 1) + 
  geom_hline(aes(yintercept = mean(means.random.numbers),linetype = ""NameOfLine1""),
             color = ""blue"", 
             size = 1) + 
  geom_hline(aes(yintercept = 1/0.2,linetype = ""NameOfLine2""), 
             color = ""red"", 
             size = 1) +  scale_linetype_manual(name=""Title of legend"", values = c(1,2),
                                        guide = guide_legend(override.aes = list(color = c(""blue"", ""red""))))

>graph.1
",1528706990.0
,"How do you define the RMSE for a classification problem? You should be using Accuracy or ROC, yes?

My understanding is that you should (or at least can) use glmnet for lasso in caret::train. I don't know if 'lasso' is intended for classification problems. Maybe try glmnet instead.",1528659546.0
jzyeo,"`gsub` works:

    x <- data.frame(date = c(rep(""0062-09-23T00:53:28+00:53"", 5)))
    gsub(""^([0-9]{4}-[0-9]{2}-[0-9]{2}).*"", ""\\1"", x$date)

result:

    [1] ""0062-09-23"" ""0062-09-23"" ""0062-09-23"" ""0062-09-23"" ""0062-09-23""",1528673294.0
gauchnomics,"Are all your dates just the first 12 chars? If so, you can use [substr](http://rfunction.com/archives/1692): ```df$var_new <- substr(var, 1, 12)```.

 Otherwise you and use the strsplit and to grab the date and then vectorize the first element. 

Ex: 

```
df$var_new <- as.numeric(t(as.data.frame(strsplit(as.character(df$var), ""T""))[1, ]))
```

I'm sure there is a cleaner way to split a string, but I've used the latter code successfully before to transform data from ""value (s.e.)"" to ""value"".",1528662268.0
zdk,"Other answers here are way overcomplicating things, IMO. This is a simple regular expression that works with gsub.

    gsub(""T.*"", """", ""0062-09-23T00:53:28+00:53"")

^ substitute the T and everything that follows with an empty character.
",1528683504.0
StephTheChef,"This should do the job
> sapply(strsplit(""0062-09-23T00:53:28+00:53"", ""T""),""["",1)

[1] ""0062-09-23""",1528705725.0
,Can't you just use stringr::str_replace in a series of dplyr::mutates?,1528656854.0
Runner1928,dplyr::separate creates new columns separating a string column by some separator you give. Its reverse function is unite.,1528658172.0
grasshoppermouse,"Would this work?

    library(lubridate)
    df$myvar2 <- as_date(df$myvar)
    
    # Or:
    df$myvar2 <- as.character(as_date(df$myvar))",1528671469.0
Tarqon,"I'd use a lookahead. 

    mutate_at(data, vars(starts_with(""var"")), funs(stringr::str_extract(., ""^.+(?=T)"")))

Untested but I think that should work.",1528668027.0
I_just_made,[Maybe this link can help?](https://www.rdocumentation.org/packages/DescTools/versions/0.99.19/topics/Winsorize),1529063395.0
shujaa-g,"You don't list ""R programming"" in a ""Degrees and certification"" section. You list it in a ""Skills"" or ""Programming"" section. Do link to your blog. No need to call out the ""self-taught"" aspect.",1528641941.0
Bruizeman,Where's the link to the blog?,1528640328.0
another30yovirgin,"I've been using R now for several years and in a few different jobs and contracting assignments, but I never took a course. I think that's probably more common, really, than having a formal course. My strategy with it has always been two-fold: (1) list it in the overview section; and (2) explain how I used it in different jobs.

So for example, in the overview section, just below my name and contact info, something like ""Experienced in data analysis using R."" along with other things I do like stats and analytical writing. Then, under jobs I've done, I'd say ""Developed a blah blah blah using R.""

That way, R is clearly a part of your resume from the beginning, but more importantly, you're highlighting the actual work you've done with it. ",1528643122.0
patriotto,"most people are self\-taught, so you're not at a disadvantage for not having a degree or certificate",1528646621.0
Slabs,"I wouldn't put ""self-taught"", but I also don't think it's goes under degrees and certifications. Can you not add a ""Technical Skills"" section?",1528654371.0
Zippityzinga,"Do you have any projects under your belt? I think if you list them, that would be sufficient and mentioning under your skills that R is one of them. ",1528640545.0
L00ph0l386,"This is what a cover letter is for! This exact thing, where you are creating a narrative of graduate\-level quantitative analyst with skills in multiple areas who has written extensively about R! 

Damn doesn't that sound good. Though I would definitely suggest including some sort of experience that is built to accompany this, as I'm sure you've done something somewhere.",1528654897.0
Fenr-i-r,"I'd say it's a hobby/interest, not a qualification.  
That way you have no expectation to be good at it. If you are good, mention why you are good, what projects you've done, how skills in it may be relevant. Having it as a hobby also shows that you are interested in it outside of your career.

I only just got my own job, and that's what I did. Stuck using excel at the moment, but it's easier to share my work and collaborate... So I'll keep R up my sleeve for when shit hits the fan.",1528640736.0
AlisonByTheC,"I would breaks it down into small tiny area calculations.  

If the length of the track is fixed, or you can at least measure it going horizontally (per the picture going left to right) then you have your y0 to y1 distance for each  unit of x.   Then you can create a tiny area area calculation from x0 to x40, assuming x has a maximum size of 40.  Then summarize all of your areas together.  

I’m sure there is a better method somewhere but this would work.  

Edit: that’s what you already did. Lol, I should read the full statement next time.  ",1528638363.0
Tarqon,"Add ```expand = c(0, 0)``` inside ```scale_x_date()```",1528623364.0
Oct8-Danger,Rstudio is an IDE for R. Download and install R then install rstudio,1528622565.0
cavedave,Might be worth looking at /r/datasets if you need to find a dataset,1528623945.0
joftius,"I'm back to explain more about a point I made in an earlier post. If you are really concerned about picking a good value of lambda, you should know that the choice will also depend on how many folds are used in cross\-validation. Start reading from the last paragraph on page 242 of ESL: [http://web.stanford.edu/\~hastie/ElemStatLearn/printings/ESLII\_print12.pdf](http://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf) \-\- it begins ""What value should we choose for"" so you can search for that text to get to right spot",1528640584.0
throughthekeyhole,"Your fit object includes both lambda.min and lambda.1se, which is what you want. Just grab that within your loop, so fit$lambda.1se",1528585584.0
Alex_Pan,"This [StackOverflow](https://stats.stackexchange.com/questions/91462/standard-errors-for-lasso-prediction-using-r) link may help, as well as this [vignette](https://cran.r-project.org/web/packages/penalized/vignettes/penalized.pdf) (see page 18).",1528588350.0
NSAkela,"Looks like predictions plotted as a function of time, so if independent variables do not change linearly over time - so does not dependent variable and prediction. You can get straight lines with plotting dependent variable as a function of the first predictor (plot(x1, y)) , then residuals as a function of the second predictor (plot(x2, y-b0-b1×x1) and so on or with plotting actual values as a function of predicted values (plot(b0+b1×x1+bn×xn, y)). ",1528542298.0
guepier,"> Closure: A function that returns a function.

That’s not what “closure” means. A function that returns a function is just *also* a higher-order function:

First-order functions are functions that work on *values* (as arguments, return values); i.e. something with a signature (in mathematical notation) *f*: *X* → *Y*, where *X* & *Y* are sets/domains of values. A simple example is the [successor function](https://en.wikipedia.org/wiki/Successor_function) *S*(*n*) = *n* + 1, with *S*: ℕ→ℕ. This can be generalised to sets of sets, or tuples; i.e. multiple arguments/return values. For example, *f*(*x*, *y*) = x + y, *f*: ℝ×ℝ → ℝ. You get the idea.

By contrast, a *higher-order* function simply contains multiple arrows in its signature. For example:

1. *f*: (*X* → *Y*)×*X* → *Y* is a function that takes another function as an argument, applies it to a value, and returns another value; a (useless) example in R: `call_func = function (f, x) f(x)`.
2. *g*: (*X* → *X*) → (*X* → *X*) is a function that takes another function as an argument and returns *yet another* function, with the same input and output domain.

---

A *closure* is a function that carries a wrapping environment around with it, and can access the values in that environment: it *closes over* the values in that environment. In R, *every* function is associated with the environment it’s defined in (you can access it explicitly by using `environment(f)` or, inside a function, via `parent.env(environment())`). Hence, *in R every function is a closure* (well, almost: certain builtin functions, such as `base::sum`, aren’t closures). Consider:

    make_add = function (n) function (x) n + x

`make_add` is a function that returns another function. The returned function is able to access `make_add`’s argument `n`, because it remembers the environment of `make_add` that it was called with:

    add_2 = make_add(2)
    add_5 = make_add(5)
    add_2(1) # => 3
    add_5(1) # => 6",1528558821.0
VisuelleData,"I wrote a higher order function that checks if two functions produce identical output.
 
 
I use it when cleaning my functions, to make sure they still work as intended after the code is changed. ",1528540948.0
RememberToBackupData,"I wrote a function that applies a function (usually read_csv or some other importer) to all of the files in a directory that match a regex pattern (usually a pattern looking for a file extension).

    rain <- apply_to_files(path = ""Raw data/Rainfall"", pattern = ""csv"", 
                           func = readr::read_csv, col_types = ""Tiic"", 
                           recursive = FALSE, ignorecase = TRUE)

    dplyr::sample_n(rain, 5)

    #> # A tibble: 5 x 5
    #> 
    #>   orig_source_file       Time                 Tips    mV Event 
    #>   <chr>                  <dttm>              <int> <int> <chr> 
    #> 1 BOW-BM-2016-01-15.csv  2015-12-17 03:58:00     0  4047 Normal
    #> 2 BOW-BM-2016-01-15.csv  2016-01-03 00:27:00     2  3962 Normal
    #> 3 BOW-BM-2016-01-15.csv  2015-11-27 12:06:00     0  4262 Normal
    #> 4 BIL-BPA-2018-01-24.csv 2015-11-15 10:00:00     0  4378 Normal
    #> 5 BOW-BM-2016-08-05.csv  2016-04-13 19:00:00     0  4447 Normal

I also wrote a function that evaluates an expression and suppresses all console printing except for the returned value. So if the author of a package liked to use `cat()` to print debugging text, just use `shush(loud_func())` and it stops all of that. 

https://github.com/DesiQuintans/desiderata",1528547132.0
clbustos,"I need to do several related factorial analysis, deleting sequentially items on a survey. The main methods returns a list. One on the elements of a list was a function that launch Libreoffice and shows a representation of the factorial analysis for better viewing. ",1528560795.0
VisuelleData,"You can do it and it's not too difficult, but it may be a little hard for a beginner. 
 
 
Can you read/write tidyverse code? 
 
 
Off of the top of my head, here's are some rough steps that should work:  
- Fit a model, such as lm(), to the data using your categorical variable as the only predictor  
- Use the summary() and broom::tidy() to get the p values in their own data frame  
- left_join() the p values to their respective category in the original data  
- Use mutate() to create a new variable called significant with if_else() assigning p values under a 0.05 to Yes and above to No  
- Make your   bar chart with ggplot2 with 'color = significant' in the aes() function  
 
 
This isn't the best way to do it, but it works and will do what you want. You'll probably also need to format the the p value data frame before you use left_join(). ",1528520287.0
,This is trivial. I am guessing no more than 10 lines in base R if you know what test you need to do and don't require the final picture to be too pretty.,1528585284.0
PM_ur_good_deeds,"Whether it makes sense or not depends on the data, I suppose. Depending on what data you're looking at, 0.03 can be either minuscule or huge. What kind of variables are you using? Are they on different scales, did you normalize them beforehand?",1528538957.0
pho1701,"There are no guarantees that any given model will fit some cause/effect logic that you have, or that a particular type of models will work well with a given data set. On the other hand, maybe this model is good - you did not share any prediction results for accuracy or errors, etc. Is your sample size very small?

The coefficients in Lasso are hard to meaningfully interpret. 
You could try doing a Ridge Regression if you want to keep all the variables, same package, same code, just add 'alpha = 0'.


",1528545019.0
SataMaxx,"There's [drake](https://ropensci.github.io/drake/).

You can also use [nextflow](https://www.nextflow.io/), which is language agnostic and can be used with R.",1528496208.0
xubu42,"I think Jenkins is a nice, easy to get started with replacement for cron. You get the monitoring, trends of execution timing, can log to stdout and keep that history in its own run file, and can be more flexible with how you schedule jobs to keep them from all starting at the exact same time. There are a ton of plugins that can make working with git, AWS, docker, you name it, easier. You can use Jenkins pipelines in much the same manner as Airflow, though you have to write them in Groovy instead of Python, though there are also GUI plugins you could use to build pipelines instead. The best part of Jenkins is that most devs are familiar with it and can probably help you out if you get stuck. But I highly recommend managing your own instance and keeping your data jobs separate from any Jenkins builds the other dev teams are doing.

I've used Jenkins and Airflow both in production at work. They're both pretty nice. Airflow is a bit easier to write pipelines, IMO, but it's not that big of a difference. Google Cloud just released Airflow as a managed service. Would be awesome if AWS had to something like that. 

The senior R people I've worked with are fond of make files, so maybe there's something to drake as mentioned previously. I find make to work well, but more for installing packages or reproducing steps exactly rather than running a script on a schedule. Maybe I'm just missing something though. ",1528521886.0
Runner1928,"I use airflow to call R scripts. I use a mixture of R and Python tasks inside the airflow DAG, just whatever language gets the specific job done best.",1528507923.0
Hoohm,I highly recommend snakemake,1528537520.0
joftius,"fit$cvm will only work if fit is a cv.glmnet object, not if it's a glmnet(... s = something) object.

Why are you re-running cross-validation 100 times?

BTW, whether you use RMSE or R^2, you should know that they are going to be optimistic. 

edit: also if you generate random numbers for an example use set.seed(some number) if you want to example to replicate with the same random numbers for someone else",1528505877.0
beyondcaffeinated,I'm seeing the same issue -- did you resolve?,1538700178.0
EuclidsPimposaurus,"Crazy, I was just looking through my location data on google maps. If you go to [Google timeline](https://www.google.com/maps/timeline?pb) you can see all the locations and routes you've taken.",1528461943.0
Squeezie91,"Monnem assozial.
Nice map tho. Google data is awesome and frightening at the same time. ",1528472747.0
Im_int,OP is stuck in a https://en.wikipedia.org/wiki/Potential_well,1528473533.0
Fenr-i-r,"I would imagine google increases the location polling when it sees you are moving, and during active hours of the day.

Regarding places you've never been, the phones location accuracy (GPS or cellular location) also affects it. Often I'll be ""at"" a local restaurant on the other side of the cell tower to me.

I enjoy the usefulness of Google, but the amazing amount of information it has on me does worry me. Not that it changes anything, but I don't use Chrome (but I am logged into my Google account while browsing), partly (and only half seriously) because I don't want Google to have the whole jigsaw puzzle of who I am.",1528511080.0
dastram,How did you create the first map?,1528496331.0
Jingelheimersmidt,Nice blog!,1528518404.0
Popsandchips,"This is fantastic. I always knew the extent to which Google monitored my whereabouts; however, I was always quite interested in what my 'average location' might be. The tool you used to generate this map seems to be exactly what I was after.

Thanks!",1528520425.0
efrique,"According to [*this*](https://cra.r-project.org/bin/windows/base/old/) page there was no release 1.1 of R (and if there had been, it looks like it would have to have been in late 2000, about 18 years ago).

Please show us a screen shot. I bet you're NOT running version 1.1 of R.
",1528408929.0
ryapric,"I pray that's a typo, and you meant ""3.1"". Heavens.

Either way, you can read all the release notes for R [here](https://cran.r-project.org/doc/manuals/r-release/NEWS.html). Though if you're really using v1.1, you're in for quite a read... I don't even see 1.1 on [CRAN's version list](https://cran.r-project.org/bin/windows/base/old/).",1528405018.0
CastorpH,Maybe you confused version of Rstudio with version of R,1528405845.0
RememberToBackupData,Are you sure the course doesn’t mean “R >= 1.1”?,1528405581.0
scbagley,"In each loop iteration, `cv$cvm` is not of constant length. If you look at the source for `cv.glmnet`, you'll see that `NA` values are removed from `lambda`, `cvm`, and related vectors, so the lengths vary. You'll need to line things up using the `lambda` vector, not just glue using `cbind`, assuming you need to save all those numbers.

Also, in your loop code, `x` and `y` are interchanged, and you left out `alpha` and `nfolds`.",1528416522.0
blozenge,"I think that this might work:


     animal_ids <- c(1,2,3,4)
     times_per_animal <- c(5,4,3,2)
     rep(animal_ids, times = times_per_animal)


You get a single vector which repeats each animal id the number of times specified in the second input (`times = ...`).

As long as you recode your animal ID to be it's integer position within `iextract` you should be golden.

This said, the length of your two inputs must match and the `times_per_animal` vector must consist of integers with no missing values (`NA`s).
",1528398825.0
StephenSRMMartin,"Doesn't a gamma\-distributed model assume the residuals are gamma\-distributed, not normally distributed?",1528401255.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/statistics] [Question on modelling a composition sensor for a chemical reactor.](https://www.reddit.com/r/statistics/comments/8pb4fl/question_on_modelling_a_composition_sensor_for_a/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1528384769.0
Statman12,"I'm not certain who ""they"" are or what other commands they are trying to use, but if you need to import a csv file into R as a matrix:

    x <- as.matrix( read.csv( ""path_to_file.csv"" ) )

See `?read.csv` for a variety of other arguments, such as `header` (is the first line the variable names?), etc.",1528382374.0
,"This is probably using grid based graphics or lattice. Try this:

    library(gridExtra)

    a <- plot(effect(""IV1"", m1), grid=FALSE, cex.title = 1)
    b <- plot(effect(""IV2"", m1), grid=FALSE, cex.title = 1)

    grid.arrange(a, b)",1528320620.0
ArchieHaddock,Alternatively use facet_wrap and plot with ggplot2.,1528378569.0
sinnsro,"As suggested, you can either use ```as.vector```, or create a ts object to handle the data.

Concerning forecast quality, I'd just like to mention that I'm unsure ARIMA is the best model here.

* Data is not normal. Which is not good for ARIMA models, as you violate a key assumption for the model.

* There are some wild fluctuations in the data (which you can see by either plotting the series, or by checking the histogram of the differentiated series). Then again, maybe ARIMA is not the best call here.

* The ```auto.arima``` function suggests a random-walk as the best model, but the AICc is quite high (3704) and you have a MAPE of 100. I'd be wary of such predictions.",1528316076.0
IMainlineMemes,"Although I do not know what causes it, one way to ""fix"" your problem is to use the as.vector function to pass newdata.

https://stackoverflow.com/questions/28618719/having-a-lot-of-issues-with-time-series-objects-in-r

Has an answer that suggests that time-series cannot handle irregularly spaced data.
",1528314701.0
GoodAboutHood,"Here you go.

    over100_df <- your_df %>%
      gather(key = key, value = value, -ID) %>%
      group_by(ID) %>%
      summarize(Over100 = sum(value > 100)) %>%
      left_join(your_df)

Just replace ""your\_df"" in both spots and the code will run.

Explanation: basically you take your wide data frame, make it long, summarize using group\_by, then join it back to the original data frame. Let me know if you have any questions",1528321363.0
CastorpH,"I would suggest this

     data %>%
      mutate(Over100 = rowSums(select(., -ID) > 100))",1528311989.0
PEG-8000,"In base R:

df$Over100 \<\- rowSums(sapply(df\[,2:ncol(df)\], function(x) x \> 100))",1528356400.0
dm319,"> while keeping it simple and tidy

not sure whether you mean tidy or ['tidy'](https://www.tidyverse.org/), so will give you two answers :)

Create the data:

    library(tidyverse)
    
    ID <- 1:500
    F1 <- sample(LETTERS, 500, replace = TRUE)
    F2 <- sample(LETTERS, 500, replace = TRUE)
    V1 <- sample(1:150, 500, replace = TRUE)
    V2 <- sample(1:150, 500, replace = TRUE)
    V3 <- sample(1:150, 500, replace = TRUE)
    V4 <- sample(1:150, 500, replace = TRUE)
    
    df1 <- data.frame(ID, F1, F2, V1, V2, V3, V4)

should give something like:

    # > head(df1)
    #   ID F1 F2  V1  V2  V3  V4
    # 1  1  F  Q  14  71   9   4
    # 2  2  T  D  74 118  89 140
    # 3  3  O  P   4  61 103   1
    # 4  4  B  N  51  52  53 125
    # 5  5  P  K 118  93 132 150
    # 6  6  S  D  42  44  44 146

(I threw in a couple of factor columns to show that it works ok with it).

'Tidy' way (keep it functional, keep hold of multiple factor columns):

    df2 <- df1 %>%
    	gather(key = ""V"", value = ""value"", -c(""ID"", ""F1"", ""F2"")) %>%
    	# rearrange data into 'tidy' format
    	group_by(ID) %>% # group by ID
    	summarize(Over100 = sum(value > 100)) %>% # add values more than 100
    	select(Over100) %>% # select results
    	cbind(df1, .) # rebind with original wide-format data

which should give you:

    # > head(df2)
    #   ID F1 F2  V1  V2  V3  V4 Over100
    # 1  1  F  Q  14  71   9   4       0
    # 2  2  T  D  74 118  89 140       2
    # 3  3  O  P   4  61 103   1       1
    # 4  4  B  N  51  52  53 125       1
    # 5  5  P  K 118  93 132 150       3
    # 6  6  S  D  42  44  44 146       1

Simple way (discard factor data, 'apply' over matrix):

    df3 <- df1 %>%
    	select(-c(""ID"", ""F1"", ""F2"")) %>% # drop non-numerical stuff
    	apply(1, function(x){sum(x > 100)}) %>% # apply over dimension 1 (rows) this function
    	cbind(df1, Over100 = .) # rebind results with wide-format data

which produces the same result:

    # > head(df3)
    #   ID F1 F2  V1  V2  V3  V4 Over100
    # 1  1  F  Q  14  71   9   4       0
    # 2  2  T  D  74 118  89 140       2
    # 3  3  O  P   4  61 103   1       1
    # 4  4  B  N  51  52  53 125       1
    # 5  5  P  K 118  93 132 150       3
    # 6  6  S  D  42  44  44 146       1

The tidy way seems more step-wise and logical, though a bit more verbose.  The downside is `summarize()` will only work if the function returns a single value.  If you need to work on table data in a tidy way and return more than one value, then after `group_by()` you need `nest()`.  `apply()` is a nice function, but it works with arrays rather than dataframes.

EDIT:

Inspired by /u/GoodAboutHood, this is a slight variation on the tidy version above, but is cleaner and more robust.

    df2 <- df1 %>%
    	gather(key = ""V"", value = ""value"", -c(""ID"", ""F1"", ""F2"")) %>%
    	# rearrange data into 'tidy' format
    	group_by(ID) %>% # group by ID
    	summarize(Over100 = sum(value > 100)) %>% # add values more than 100
    	merge(df1, by = ""ID"")
",1528325390.0
cqfvd,"Could you do something like `mutate(df, Over100 = (v1 > 100) + (v2 > 100) + (v3 > 100))`?",1528309732.0
Smutte,"Sorry for solution in Python with Pandas. Maybe it can help anyway by inspiring a similar solution in R.


I think this does what you want:

df['Over100'] = df.drop('ID', axis=1).apply(lambda x: sum([p>100 for p in x]), axis=1)",1528319512.0
Ordzhonikidze,"Have you tried an ifelse() statement inside your mutate() call?

        mutate(data, over100 = ifelse(V1 > 100 | V2 > 100 | V3 > 100, 1, 0))

Not quite sure on the inclusion of operators inside ifelse inside mutate, tho.     ",1528317064.0
fang_xianfu,"This is really a data engineering question. You need to think hard about the relationships between entities that you're trying to capture and consider how they can be represented with a normal form.

For example, if there is a many-to-one relationship between the crumb_3 and the crumb_2, that is to say that the information is in a hierarchy, with crumb_3 being more detailed than crumb_2, then the crumb_2 is unnecessary to uniquely identify the correct row to join on. You can join using only the crumb_1 and crumb_3. But it depends on what relationships your data is supposed to embody.",1528303291.0
Tr8ze,"Tried the fuzzyjoin package? 

From the package description:

“The fuzzyjoin package is a variation on dplyr's join operations that allows matching not just on values that match between columns, but on inexact matching.”

install.packages(""fuzzyjoin"")",1528331677.0
buckhenderson,"If I understand the problem correctly, you should be able to partition the datasets and do regular joins with non-na data and cross joins with the na data?",1528334959.0
,[deleted],1528300605.0
BustedEchoChamber,The intro vignette indicates that you use it to specify which variables are alternative-specific. So if you have repeat measurements by individual & a column in your data frame that varies by individual but not by alternative you can specify all the columns you want to include.,1528256870.0
revgizmo,How new to this are you? Do you have the data in a csv to import or are you trying to enter the data directly? What’s your actual question?,1528241220.0
sad_house_guest,"You're going to need to be a lot more specific \- ""enter data and get the graph"" is incredibly vague. 20 samples of what? What is the response and what are the things you need to plot it against? What file format is the data stored in? ",1528248461.0
MrLegilimens,What graph?,1528281933.0
,"You have to understand how factors work.

A factor is really two values. A face value, which you see, and an underlying level value. If you sort a factor, it sorts on the hidden, underlying value. In order to sort a set of factors, you need to assign the order explicitly. [An example guide](https://www.r-bloggers.com/reorder-factor-levels/).

You might also notice that when you call 'as.integer' on a factor, the values that come out aren't what you expect. This is because as.integer returns the hidden integer list of levels. If your factors look like c(""these"",""values"",""aren't"",""numbers""), then calling as.integer doesn't make sense to you. But it does to R. Likewise, there's no 'intuitive' ordering to that vector in terms of numbers, but there is for the underlying hidden integer levels of the factor.",1528225020.0
Ax3m4n,"    df2 <- df[order(as.numeric(as.character(Subject))),] ",1528270209.0
zdk,"    gsub(""IC Date Subj "", """", df$Subject)",1528221778.0
bubbles212,"/u/zdk and /u/BeerSharkBot covered how to solve your particular problem, but for general data cleaning and manipulation work involving character strings I've found the [stringr package](https://cran.r-project.org/web/packages/stringr/index.html) extremely useful. The ""Working With Strings"" [cheat sheet from RStudio](https://www.rstudio.com/resources/cheatsheets/) is a handy two-page pdf to use as a quick resource. On the first page it lists a quick summary of the useful functions in the package, and on the other it gives a nice breakdown of regular expressions with examples. The package page also has two vignettes (linked on the page): ""Introduction to stringr"" and ""Regular expressions"". Those will go into more detail than the cheat sheet does if you want to learn more.",1528232069.0
BeerSharkBot,"For versatility, lookup regular expressions and how you could use something like “//d{1}$“",1528222981.0
mbillion,What language? That's kind of an important part,1528247085.0
Deleetdk,**glmnet** can do this no?,1528217729.0
notrealsuredude,"cor(data) won't work because you have factors in the data. Because of that, the only variables R will let you correlate would be income, consumption, and age. Correlations between factors are usually pretty useless bits of information. You can get better information with something like regression, which is next. As for the others, you can just create another object with those selected columns to get those correlations, or you can install the ""lsr"" package and use correlate(data) to get a matrix that will give them to you while leaving out the factors. 

W/r/t regression, your formulas for reg1 and reg2 are largely the same, but with reg2 you won't need the data[,-1] since you're not inputting the family id variable into the function. Reg3 tests a model with just an intercept. This doesn't really tell you much unless you want to test reg1 against reg3 (use anova(reg3,reg1) to test them) and see if an intercept-only model (reg3) is a better fit than a model using predictors (reg1). The data[,-1] isn't necessary in reg3. 

As far as the output goes, what you're seeing is totally fine. R treats the first level of a factor as an intercept (since you can't have level 0 of a factor). So for continuous variables, you're seeing the intercept, then effect that each unit increase in the variable has on the response variable. For categorical variables (factors), you're seeing the first level as the intercept, then how each increase in a level of a factor influences the response variable.",1528215817.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/rstudio] [Need help with a multiple regression model (&tests)](https://www.reddit.com/r/RStudio/comments/8ormfs/need_help_with_a_multiple_regression_model_tests/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1528211950.0
MrLegilimens,"Wow. Slow down. 

You want family num to be a factor too though? Why...? 

>Ok but then how do i get a correlation matrix which doesn't consider 4 columns of data ?

Make a new dataset with only the columns you want.


IMO the most visually appealing is using dplyr's select and pipeline then just subsetting out. 

    data2008 <- data2008 %>% dplyr::select(support_torture, poliknow, tthreat, #3
                                           auth, republican, independent, #6
                                           democrat, weight, year, #9
                                           age, agegroup, male, #12
                                           female, famincome, individincome, #15
                                           culturalt, immigrantlevel, education, #18 
                                           immjobthreat, race, wiretapnowarrant, #21
                                           wiretap_powers, muslim_thermo, auth.both,
                                           auth.both.t, everything()) #25
    
    data2008_red <- data2008[,1:25]

>But I get an error: ('X' must be numeric).

Yes, and you're trying to correlate factors. That's not how correlation works. You made everything a factor. Do you know what factors are?

>reg1=lm(Income~.,data=data[,-1])

Why would you? Doing ~. is irresponsible. Be clear what variables you're including. 

>reg2=lm(Income~Consumption+Status+Qualification+Family.num+Age+Sex,data=data[,-1])

Dont have data[,-1]. Just say data.

>i get one less intecept for each factor:

This is a basic regression question. 

https://www.andrew.cmu.edu/user/achoulde/94842/lectures/lecture10/lecture10-94842.html#why-is-one-of-the-levels-missing-in-the-regression",1528216153.0
Babahoyo,"Souds like you should write a function, in R, or post to Statalist?",1528227624.0
ReusAlThor,"If you give us code, there should be a relatively easy fix. A possible work-around could be to make a manual scale with the right vector length, but only contain the strings you want as your labels (I've never tried to do that, so I don't know if it would throw an error).",1528205549.0
flrrrn,"I think the first step might be to create the plot. Then, you can modify your plot by changing the tick marks and labels. Instructions for such customization is easily googled. But I think the important part is: You are going to plot *all* your data, you only want to change the labelling of the axis. That's a cosmetic issue and doesn't require your data to be transformed or aggregated. 

It'd be easier to help if you already made the plot, shared your code, and then asked how to achieve certain cosmetic changes. This R package might also be very helpful for related modifications: https://github.com/calligross/ggthemeassist",1528195660.0
KoolAidMeansCluster,"This is close to what you're asking for.

I put it in a ""justpasteit"" website to keep formatting.

https://justpaste.it/63868",1528206195.0
dm319,"The tidy way to do this is to make use of the columns to describe the data.  Using underscores is necessary when working with named arrays - where you only have names running down rows and columns.  With dataframes you can describe the data in as many dimensions as you like.

The tidy way to store the data would be to have recordid, eventname, and measure as your columns, then a final column with the value.

You don't have to record the data like this though - use the functions gather() and spread() to shift the data.  When you want to work on the data, spread() it into the columns and use mutate() to manipulate the data, then use gather() to return it back to that default state.

EDIT: Just re-read what you've written on the bottom, I'll try to make up an example to show you what I mean.",1528123554.0
DeuceWallaces,"Wide form for just looking and entering the data. l'm a data.table person so when you start creating new variables for each subject like differencing, ratios, and rowSums I would use lapply like:

old.variables\<\-c\(""v1"", ""v5"", ""v8""\), or if you have lots of variables and consistent naming structure:

old.variables\<\-grep\(pattern=""timepoint"", names\(data\)\)

data\[,new.variable:=lapply\(.SD, function\(x\) x/ v2\), .SDcols=old.variables\]

 Makes it easier to see effects on a subject over time, but you need to melt the data into long format for modeling and graphics. Speaking of modeling, I would use a mixed hierarchical with your subject parameterized as a random effect.",1528142454.0
Darwinmate,"Can't answer your question directly, but most packages will write a method function for \`plot\` so it can handle specific object types. Check it out here:

[https://rdrr.io/rforge/partykit/man/party\-plot.html](https://rdrr.io/rforge/partykit/man/party-plot.html)",1528160319.0
bubbles212,"Have you tried using using the structure function str() on the cforest output? This should show you all the elements making up the actual output object, so you may be able to extract the information you want directly. ",1528174123.0
ran88dom99,You could run cforest through caret or mlr. Both have several methods to determine varImp. ,1528304521.0
Darwinmate,"Do you have admin rights/read\+write access to C:?

Check if \`cplexAPI.dll\` exists in that path. ",1528071688.0
questionquality,Check out the options in Tools -> Global Options -> Code -> Diagnostics,1528017829.0
huessy,"Also if you save the file as a .R file it will tell RStudio which ""spell check"" option to use and will then have little yellow exclamation point signs next to the lines with problems.",1528043532.0
ReimannOne,"DataCamp is well worth it.

Hadley's R ~~bible~~ book is free online: http://r4ds.had.co.nz/

",1527989561.0
singularperturbation,"AFAIK H2O's [automl](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html) is the only fully automated ML package for R (offers support for Python as well), but caret, mlr, etc. have support for specifying models with a common interface + hyperparameter optimization for a given task, and then you can select among them after training finishes.

I haven't found automatic ML packages to be too useful outside hackathons.  The biggest impact comes from cleaner data, better feature engineering, and better hyperparameter optimization (in that order, IMO).",1527990468.0
noelsusman,"""Fuck thinking just minimize MSE lol"" is more of a computer scientist thing, and computer scientists use Python to do machine learning.  People who use R are generally going to be apprehensive about automated modeling.

Anyway, I don't know of anything comparable.  `forecast` has some automated time series modeling functions, and `caret` provides the basic building blocks if you want to make your own automated ML function.  It wouldn't be that hard",1528006657.0
efrique,Hopefully it has a better understanding of why that might be a bad idea.,1528001064.0
cu29co,What is a automated ml package? ,1527999326.0
Runner1928,"I almost always plot using ggplot now, and its ggsave function is easy to use.",1527984720.0
toothless_budgie,Why Java? It's not a word I usually see in the same sentence as 'startup'.,1527973239.0
TroyHernandez,Factors in RF can be split in 2^(n-1) - 1 different ways for a given node. I believe it's capped to prevent it from running for an impossibly long time.,1527972239.0
,"Don't most wrappers, like caret's train, convert categorical variables with more than two levels into double value independent categoricals anyway?

So red/blue/green gets converted into

red 1

blue 0

green 0


red 0

blue 1

green 0


red 0

blue 0

green 1

? In that case, is this question functional, or for interest? Because you can either use train() as a wrapper for 'ranger', or spread the variables yourself before you train().",1527966725.0
Jordo82,"How do you determine the dates that are being used to calculate the difference?  It looks like the first value is the month prior to the Date column in table A, but the second month in the difference is... what?

* 2001-02 to 1999-09 = 17 months
* 2002-01 to 2001-02 = 11 months
* 2003-04 to 2002-01 = 15 months",1527951455.0
aned_,"Check out the “combine datasets” section of this data wrangling cheat sheet. This should provide you with a way to join the data depending on how you’d like the end state to look.

https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf

Ps. I always keep printed versions of the cheat sheets on my desk and it’s saved me a LOT of time ",1527944296.0
navidshrimpo,"I don't understand what you're trying to accomplish. If you merge two datasets that are not related into one dataset, imagine first what you would want it to look like. Which analytical techniques would you be interested in? Even if you ran something like multiple regression, what would be your response variable?  It sounds like you have no correlations or things to link them in a meaningful way.",1527961325.0
nevk_david,"I am guessing on your intentions here, but there are methods for handling heterogeneous data from multiple studies during meta analyses of individual patient data, including imputation etc (and all connected caveats). You may want to check out related literature. ",1527971704.0
Rubipy3,"Are these multiple datasets about the same observational units (ie. same individuals measured in two studies) or different individuals assessed in multiple studies. Joins will help more with the former than the latter, IIRC. 

You may also want to look into “imputing missing data.” If two datasets contain lots of the same variables, but some different ones, you’ll essentially have ‘NA’s where there’s no data. Imputation can help you “fill in” the missing data using the variables that are common to both datasets, while preserving the performance of your statistical test (ie. 95% CI will still have ~95% coverage). ",1527960838.0
buckeye2114,Do they generally have the same variables? You could use the rbind() function then. I'm by no means an R expert but I think that's what you're looking for.,1527963505.0
,[deleted],1527976042.0
VisuelleData,"There's not much to the theory.  
 
If there's a single common variable you can merge datasets. Whether or not this means anything, depends on the underlying relationship of the variables. 
 
  
Example: 
 
I have bitcoin prices for every day of 2015.   
 
I have weight measurements for my dog for every day of 2015. 
 
I can merge them using the dates as a common variable.  
 
 
R4ds.had.co.nz 
 
This has a nice section that describes types of 'joins' for merging by common variables. It covers technical details and a little theory. The stuff you're looking for should be under the 'tidy data' section I think. In fact, I recommend reading the entire section about tidy data. The author of the book has an academic paper about it. ",1528018438.0
maltiv,"Confusing question, but it sounds like you could just join the tables and create the new column using if_else ",1527946592.0
sinnsro,"I would guess that, even if you don't get that many answers here, there would still be people out there that would be pleased to find a package for their needs.

Just as an anecdote, this happened to me today, but with LaTeX. I needed to conjure some diagramas with ISO 14617 symbols, and while I was willing to start from scratch on Tikz, I found this github repo with a package which already implemented almost everything I needed. ",1527898525.0
-Astral_Weeks-,"Is this the package you have in mind? https://www.r-bloggers.com/rchess-a-chess-package-for-r/

I love both R and chess but never thought about using R to analyze chess. (that's what we have stockfish for?) What chess variant were you interested in analyzing?",1527889094.0
kebabmybob,Honestly the interesting part of what you’re describing is the actual research and code and problem setup.  I much look forward to reviewing that from a statistics and Game AI perspective. ,1527944312.0
kenderpl,R is tied to statistics. Even if you will land let's say a Shiny app developer role it will be very likely connected to statistics aka math stuff.,1527882083.0
KopfJ4ger,"R is mostly used in positions where a basic understanding of statistics is required. You have already learned R and don't want to throw that away. Python is similar enough to R that you can build on the foundation you already have to learn a new language. Python skills are used more broadly outside of analyst/stats positions. That might be one way to approach the problem you're facing. 

If you want to do machine learning, math is basically required.",1527881292.0
factotumjack,"Your options are Coursera, Khan Academy, Skillshare, and Lynda, all of which teach math.

You don't have to know all of math, but there's a few concepts on matrix algebra an in optimization/algorithms that are absolutely must-know essential for machine learning. Without them, you'll be doing little more than making neural networks by guessing.

Edit: That came off as snarky, sorry. The take-home message was 'yes you need math, but only specific things within math'.",1527880802.0
,"Don't feel intimidated by the other answers here. In my experience, you don't have to fully understand the math or their proofs. But you must absolutely understand the intuition and the reasoning behind it. 

No one's going to ask you to actually do linear algebra or work with matrices by hand. That's what the computer is for. Getting the math done is actually the easy part. However, people like to conflate this, suggesting that algorithms and math are the only way to get anything done in data science.

That said, sometimes I see algorithms and explanations in mathematical notation and get easily confused. Then someone explains it to me and I see how unnecessary all of that was.",1527883921.0
VincentStaples,Basic statistics is not complicated and almost all data visualisation is of non-complex models.,1527906325.0
BeerSharkBot,"If understanding the math, the statistics, or the actual workings of the techniques was a requirement, there would be a lot more jobs unfilled. Don't let people who apply a checklist to things intimidate you. ",1528243859.0
moozywoozy,"data science, lol",1527983953.0
linkaneo,"It’s important to note that math ≠ statistics. You’ll need a certain level of math to do anything in data science, but ask any professional mathematician and they’ll laugh if they hear you saying statistics and math are the same. You’ll be able to get away with high school level pure math in order to understand how most statistical tests and probability work. Which is enough to be an analyst or a BA in a lot of different areas. ",1527882281.0
Lifebyrd,"You don't need to solve them, it just means that when you use filter() it will call dplyr::filter(). If you want to use filter from the stats package you have to explicitly call stats::filter()",1527855396.0
_Wintermute,I still don't understand why clobbering the standard library isn't a CRAN error.,1527871716.0
,"You don't need solve them, surely. If you want filter from stats, call

stats::filter

and if you want dplyr's filter, just call filter.",1527855334.0
dtrillaa,"As a general rule (there are always exceptions), for pure Classification Or Regression, Xgboosted models and Random Forrests will yield the most accurate models (as stated earlier). On occasion neural nets will perform better, but neural nets really shine with stuff like image classification or Natural Language processing.

You should also look into logistic regression. Logistic regression is great because it normally performs comparable to Random Forrest’s and Xgboosted models, but it’s output includes interpretable coefficients that can be used for insights past the ROC or misclassification rate.

Look into using the `caret` package in R, it allows you to find the optimal model in the least lines of code.",1527877412.0
maltiv,"Xgboost or Random forest (try the «ranger»-package) will usually give the best performance. nnet is probably a waste of time for this problem. 

But the big gains will not come from model selection, but from intelligent feature engineering. ",1527848068.0
ron_leflore,"Try h2o.

It has an automl mode where it will stage a mini competition between different algorithms.

http://h2o-release.s3.amazonaws.com/h2o/master/3888/docs-website/h2o-docs/automl.html",1527958861.0
deck13,"We need to move away from comparative methods that do not think about the problem at hand, such as elo rating using vast amounts of historical data. The results of the 1872 match between Scotland and England has no influence over today's rankings between Scotland and England (caveat: perhaps the fact that both countries had soccer** teams in 1872 is a predictor of future success). I am a novice with elo, but there does not seem to be a way to do statistical inference with the rating system. You can provide inferences with predicted win probabilities using elo (see Section 2.4 in https://www.stat.berkeley.edu/~aldous/Papers/me-Elo-SS.pdf). 

The attached paper claims that elo weighs more recent matches higher. However, each match is context neutral so that a World Cup final has the same weight as a qualifying game. I think that this would bias the inferences substantially.

Perhaps this would do better: https://pdfs.semanticscholar.org/94cf/86588244cd9aae76449e570147d47fea6438.pdf

** Yes I am American, all attacks on me calling it soccer can be directed here: https://yourlogicalfallacyis.com/ad-hominem",1527853088.0
SSID_Vicious,"Nice post! I like these practical examples, it shows how ridiculously easy R makes everything.",1527846444.0
fourpita,Fantastic tutorial of an ELO implementation. (Disregarding if the ELO is actually meaningful in this case),1527874211.0
jeremymiles,"First, create gaps in the data:

    newData <- expand.grid(Name, Date)

Then merge this with your old data:

    d <- merge(oldData, newData)

(You might need to do a sort to make sure the data are in the right order.)

Now you've got empty rows. You can run down the column and do a check.

    for (i in 2:nrow(d)) {
      if (is.na(d$Value[i]) | is.na(d$Value[i - 1] {
            NewColumn <- NA
      }   else if(d$Value[i] -  > d$Value[i - 1] {
            NewColumn <- 1
     } else if  {
        if(d$Value[i] -  <= d$Value[i - 1] {
            NewColumn <- 0
    }
     
(Untested code. Post a reproducible example if you want others to test). ",1527886412.0
Darwinmate,okay,1527822405.0
,Can you explain how this package is better or different than the traditional methods of feature selection? (For those of us who don't want to read stats papers),1527878673.0
hummingbirdz,"I think what you want is a left join (dplyr::left_join)

Make a table in a csv as you show with columns for all of the variables used to classify (with only all unique existing possible combinations as rows (use dplyr::distinct and then write.csv) and create (by hand or via excel or R) a column that says what to classify each possible combination.

Then join that table to your data by the shared columns.

This was a super helpful way for me to solve a similar problem (and makes your classification rules easy to hand check and validate). I’m on mobile or I’d give a better response, feel free to ask questions and I’ll try to answer later.",1527795054.0
,"It's not clear exactly to me how you want this match to work.

What happens if you have two matches in a given row, say 'Apples (Red)' in column crumb_1 and 'Plaintain Banana' in crumb_3?

It really seems like this problem is made more difficult because your data is not tidy. You should probably use 'dplyr::gather' to turn 'crumb_x' into a column populated by 'crumb_1, crumb_2, crumb_3', and to have a 'value' or 'fruit' column which takes your actual type of fruit.

Then you can do a really clean match of your second data frame, by = ""ID"", using an inner_join or left_join.",1527793755.0
ohheyitsdeejay,"I think you're looking for the melt package. 

Or maybe arrange(y) followed by seq(1, nrow(x), by = 1)?",1527793012.0
YepYepYepYepYepUhHuh,"A simple way would be to use substr() to extract the last two characters of the string, for instance:

    data$AgeNum <- substr(data$age, 7, 8)
    plot(effect(""AgeNum:IQ"", model1))",1527794536.0
cu29co,I am not aware of a method to rename these on effects plots. You could just plot the actual data using base plots or ggplot2. If you have random effects that you want to account for in your plot it can be complicated without the effects plots though. Good luck. ,1527809574.0
zdk,"The error message seems pretty obvious. Check:

    length(row.names(phylo_tree_data))
    length(names(trait))
",1527784679.0
RememberToBackupData,"`visdat` is great for glancing at the quality of a dataframe. It will tell you what data type is in each column, how much missing data there is, and where in the dataframe the missing data occurs.

http://visdat.njtierney.com/reference/figures/README-vis-dat-typical-data-1.png

`install.packages(""visdat"")`

---

Also a cheeky mention of my own package, `librarian`, which automatically installs and attaches packages from both CRAN and GitHub *at the same time*.

    devtools::install_github(""DesiQuintans/librarian"")
    
    librarian::shelf(dplyr, DesiQuintans/desiderata, purrr)
                       ↑        ↑                      ↑
                      CRAN     GitHub                 CRAN

Ever since I made it, it's been so convenient to use in my RMarkdown docs. Very easy to just put a new package name into `shelf()` and know that it'll be taken care of.",1527766107.0
VisuelleData,"Mine are:  
[SciencesPo](https://github.com/danielmarcelino/SciencesPo/blob/master/README.md)  
- Has a lot of [wrapper functions](http://danielmarcelino.github.io/SciencesPo/Viz.html) for ggplot2. Such as title_align_right and no_x_axis. Disclaimer, I've never actually used any of the PoliSci functions. 
- It's no longer on cran, so you'll need to download it from GitHub or elsewhere. 
 

[naniar](https://cran.r-project.org/web/packages/naniar/index.html)   
- Can do anything you'd ever need for missing values. Visualizations, summaries, and other stuff all adhering to 'tidy' principles. ",1527768616.0
GoodAboutHood,"[pacman](https://cran.r-project.org/web/packages/pacman/vignettes/Introduction_to_pacman.html) is a great package manager tool. You can load multiple packages in one call, and if the package isn’t on your system it auto-installs it.

It also has a function to install packages from github easily.",1527936579.0
samclifford,"Wild seeing both responses so far be packages by Nick Tierney, who I worked with. I'm a big fan of hrbrthemes for plotting and INLA for modelling. ",1527778868.0
,"Mine is: [matrixStats](https://github.com/HenrikBengtsson/matrixStats)

Very handy if you work with matrices.",1527782732.0
ciarogeile,"ggbiplot

It does exactly what you'd expect. Provides a ggplot based method to create biplots ",1527797428.0
efrique,"Well ... actually, `acepack`, which implements the ACE (alternating conditional expectations) and AVAS algorithms. It goes a long way to simplifying a particular thing I do; it's not very important for my job or anything but it's a package that has a few functions that each do one thing pretty well (though I mostly focus on one of the functions, `ace`) and which makes doing something for a particular hobby considerably simpler.

In short, `ace` tries to identify (nonparametric) transformations of response and predictors to make a relationship between t(y) and sⱼ(xⱼ) additive. these transformations can be general (but smoothish) functions or can be specified to be monotonic or periodic. [Once I have a good t(y), the effort in doing a particular task becomes considerably simpler -- *mischief managed*.]

",1527813579.0
MrLegilimens,Wesanderson for color,1527817805.0
questionquality,"[tidybayes](https://github.com/mjskay/tidybayes) for extracting samples from bayesian (jags, stan, brms) models.",1527839953.0
DrErinERex,Catterplots!,1527876729.0
reamsofdata,"finreportr and fincal are quite handy libraries if you work with financial data regularly. The former allows for downloading of financial statement data, while the latter is great for calculating financial ratios.",1528067350.0
,[deleted],1527809971.0
featherfooted,"I can't honestly tell if this is OP trying to advertise an interview for ""one of the biggest tech companies"" in some form of weird casting call, or if OP is trying to *pass* such an interview by posting the question online and hoping someone else will provide all the answers.

Either way, this is quite pathetic. Even better, OP has spammed this in /r/rstudio and /r/datascience.",1527780148.0
Velm,"A little clunky to get things in a tidy format at the beginning, but the rest works okay with `dplyr`.

    library(tidyverse)
    merlin_votes <- c(""Kevin_Bacon"" , ""Kevin_Bacon"" ,""Morgana"" ,""Kevin_Bacon"", ""Circe"", ""Circe"" ,""Morgana"" ,""Circe"")
    circe_votes <- c(""Merlin"" , ""Morgana"" , ""Morgana"" , ""Morgana"" , ""Circe"" , ""Merlin"" , ""Morgana"" , ""Merlin"" , ""Merlin"" ,""Kevin_Bacon"")
    morgana_votes <- c(""Morgana"" , ""Kevin_Bacon"", ""Morgana"" , ""Circe"" , ""Merlin"" , ""Kevin_Bacon"", ""Merlin"")
    kevin_bacon_votes <- c(""Merlin"", ""Circe"")
    
    vote_vector <- c(merlin = merlin_votes,
                      circe = circe_votes,
                      morgana = morgana_votes,
                      kevin_bacon = kevin_bacon_votes)
    
    tibble(voter = names(vote_vector),
           vote = vote_vector) %>% 
      mutate(voter = str_remove(voter, ""\\d+$"")) %>% # remove numbers from voter names
      group_by(voter, vote) %>% 
      tally() %>% # n = count of each vote + voter combo
      add_tally() %>% # nn = count of number of votes per voter 
      mutate(weight_vote = n/nn) %>% # weighted vote for each vote + voter combo
      group_by(vote) %>%
      summarise(sum_vote = sum(weight_vote)) %>% # sum the weighted votes by votee
      arrange(desc(sum_vote))
|vote        |  sum_vote|
|:-----------|---------:|
|Merlin      | 1.1857143|
|Circe       | 1.1178571|
|Morgana     | 0.9357143|
|Kevin_Bacon | 0.7607143|",1527797896.0
Im_int,"Better solutions have been posted already, but here's mine anyway:

    library(dplyr)
    bind_rows(tibble(caster = 'mer', votes = merlin_votes),
              tibble(caster = 'mor', votes = morgana_votes),
              tibble(caster = 'cir', votes = circe_votes),
              tibble(caster = 'kev', votes = kevin_bacon_votes)) %>% 
      group_by(caster, votes) %>% summarize (num = n()) %>% 
      group_by(caster) %>% mutate(adj = num/sum(num)) %>% 
      select(votes, adj) %>% group_by(votes) %>% summarize(tot = sum(adj)) %>% 
      filter(tot == max(tot))
",1527805225.0
manwithoutaguitar,"I cheated a bit because I was struggling turning the vectors into a tibble (thanks u/shujaa-g for showing how it's done):

     tribble(
        ~voter, ~merlin, ~circe, ~morgana, ~kevin_bacon,
        #--|--|--|--|--
        ""merlin"", 0, 3, 2, 3,
        ""circe"", 4, 1, 4, 1, 
        ""morgana"", 2, 1, 2, 2,
        ""kevin"", 1, 1, 0, 0
    ) %>%
      mutate(strength = 1 / rowSums(.[2:5])) %>%
      transmute(merlin = merlin * strength,
                         circe = circe * strength,
                         morgana = morgana * strength,
                         kevin_bacon = kevin_bacon * strength) %>%
      summarise_all(sum)",1527839943.0
forever_erratic,Do it yourself,1527779224.0
,"That seems like an excellent reason for using R instead of SPSS, which I guess was your point. Or why exactly did you post this in a sub about R?",1527750496.0
Tokazama,Your computer is telling you to stop using SPSS. It feels that it deserves better and you too deserve better. This is never an easy time. You have some of your spreadsheets in it's format and and it has its icon on your desktop. Sure it'll be a little awkward when your with a slick new piece of software and see it wantingly trying to compute a simple mixed linear model or interaction effects. But when it's time to move on you have to move on.,1527823782.0
scbagley,"The function to call for variable influence in gbm is `relative.influence`.

To find this, I evaluated:

1. `varImp`. It calls `UseMethod`.
2. `getS3Method(""varImp"", ""gbm"")`. It calls `caret:::varImpDependencies`.
3. `caret:::varImpDependencies`. It calls `getModelInfo`.
4. `getModelInfo(""gbm"")` returns a long list.
5. `getModelInfo(""gbm"")$gbm$varImp`. It calls `relative.influence`.",1527810205.0
,"You could use 'caret', use the 'train' function to apply gbm, and then you can just call

varImp(model)

afterwards.",1527771709.0
ELKronos,Thank you for making such an informative and well written guide!,1527729831.0
CaramelTHNDR,Wow. I just finished compiling a big dataset \(big for me\) and can't wait to use your package to explore!,1527732545.0
irishfury07,This is great! Thumbs up!,1527736647.0
engelthefallen,"I wanted to come in and say don't we have enough EDA stuff, but this is really well done.  Odds are will become my go to for first pass data inspection.",1527762702.0
VPMACH,"Initial thoughts:  I really like it.

I currently find it a tad unwieldy with a data set I am working with that has a large number of attributes.  When using plot\_bar\(df\) or plot\_histogram\(df\) is it possible to control the number of plots per page without calling each field individually?

A zoom function on the plot\_str\(\) would be nice as well for datasets with large number of attributes.",1527778439.0
Owz182,"Those are some really handy functions, good job!",1527743811.0
sarcasticsobs,I discovered this package a couple weeks ago and have been loving it. Great work!,1527770178.0
VPMACH,This looks cool.  I'll have to give it a whirl and see how it stacks up against my standard home brew EDA.,1527777597.0
RememberToBackupData,I actually found this yesterday when I was looking for a replacement for the `xda` package that hasn’t been updated in a while. It’s good to see that this package is active. ,1527794574.0
bery322,I just tried to use this yesterday but got the data.table error. Has this been fixed?,1527796795.0
ran88dom99,Includes heat maps of 2 categorical vars?,1527974468.0
Trek7553,"Any thoughts on why I'm getting an error?

I installed and loaded the library without any issues. Here's my output:

    library(nycflights13)
    library(DataExplorer)
    data_list <- list(airlines, airports, flights, planes, weather)
    plot_str(data_list)
    Error in plot_str(data_list) : could not find function ""plot_str""

I tried some of the other functions and they all give an error that the function can't be found.",1528753976.0
efrique,"Unless you mean something quite different by 'competent' than I'd assume, no. R takes some extensive effort and practice to learn to what I'd regard as a competent standard.

However, you may be able to develop enough basic knowledge to look like you've at least used it a little before.

> How do you suggest I go about this? 


Step 0: Don't wait until you only have 4 days to ask whether it's possible to learn R in 4 days. If the answer is ""no"", you can't go back and do anything different.

Step 1: actually use R to do things. Real things, like what you'll be doing on the job. Yes, it will be slow and you won't get a lot of the way through it. But if the job will involve some regression (for example), make sure you can do one, and get out all the usual things you'd need to see once you've fitted it.

Step 2: make use of any courses or notes you have access to but beware many of them won't move quickly enough on the parts you need (I read about 5 times the speed people talk so for text is considerably more efficient than video most of the time)

Step 3: *do* learn the basic data structures in R and how to manipulate them (especially vectors, lists and data frames but also arrays and matrices)

Step 4: *do* learn how to write and use functions you write as arguments.

Step 5: learn vectorized approach including the *apply functions and relatives

That's more than you'll fit in 4 days already and there's a bunch of important concepts and added functionality (I haven't mentioned a *single* add on package yet, and there is a bunch of important ones to know about) and suchlike that I haven't touched on

Step 6: use stackoverflow for R coding questions. There's something like 200,000 R questions there (most with good answers) -- almost any conceivable basic question has been asked and answered. You'll need to learn how to search it competently (both the built in search and a site-search via a search engine). 
",1527727742.0
PhysioTheRapist,"R CheatSheets
https://www.rstudio.com/resources/cheatsheets/
R for Data Science
http://r4ds.had.co.nz/

Not sure what applications you will be using but the above URLs should be able to provide you with a basic understanding of R and with the end of chapter questions some practical as well.

Once you're in the job ask your team members what packages they use regularly or better yet call ahead and ask and try to familiarize yourself with those in addition to the material above.

Good Luck! ",1527770918.0
,">I've completed about 2 hours of Datacamp tutorials but that's it.

Pull out your credit card. Pay for a month of datacamp. Run every tutorial from the beginner, intermediate, data loading, data manipulation, and data visualisation skill trees. You can do this in four days, although recall might be an issue with such a cram.

I highly recommend saving off the code from every single exercise in your own workbook, so that you can refer back to it.",1527843102.0
Pseudo135,"sd(data$age)
sd(data[, 1])

Data is a table (actually data frame) so you need to specify the column either by name or number as seen above. Some of the DataCamp courses might be a good place to start if you are having trouble referencing objects.",1527725794.0
bubbles212,"Sorry to go RTFM on you, but you can find out how the functions work by looking at their manual pages. These will list the inputs and outputs to expect from the different functions in R. To get to them you can use ""?"" followed by the name of the function. For example, typing

    ?sd

in the console/terminal and hitting enter will take you to the [manual page for the standard deviation function.](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/sd.html) From here you can tell that the function takes a single numeric vector as its main input, not data frames (which are technically lists of multiple vectors, all of the same length). You can see the other comments here for how to isolate particular vectors of numbers in your data frame.

",1527726824.0
Darwinmate,"OP I think you've got an X/Y question, what you actually want is ""How do I apply a function on multiple columns"" but you are asking ""How do I parse results from `summary` function"".

So to answer your actual question that (I think) you are asking: `apply` is awesome! Learn `apply` family of functions.

    apply(mtcars, 2, sd)
        mpg         cyl        disp          hp        drat          wt        qsec          vs          am        gear 
    6.0269481   1.7859216 123.9386938  68.5628685   0.5346787   0.9784574   1.7869432   0.5040161   0.4989909   0.7378041 
       carb 
      1.6152000

The `2` in the `apply` function specifies columns, use `1` if you want to apply functions to the rows. `sd` is the function you apply to every column. ",1527739244.0
imguralbumbot,"^(Hi, I'm a bot for linking direct images of albums with only 1 image)

**https://i.imgur.com/5iZ8xVn.png**

^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&subject=ignoreme&message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&subject=delet%20this&message=delet%20this%20dzumnkw) ",1527721920.0
Darwinmate,"more info is required, whats \`effects\`? what's inside. what type of object is it",1527725163.0
cu29co,Google the effects package and that will give you more insite on its methods. It is a great package and worth learning. ,1527727892.0
sinnsro,"You have to override the function/method in order to change these.

I assume the function calls lattice::xyplot() for this plot (given the overall aesthetic), so you'd have to modify code accordingly to change the output. 

I'd advise against getting rid of the panel.rug() though. It looks cramped, but it is valuable information about the distribution.",1529027925.0
shujaa-g,"You should read through [The R Inferno](https://www.burns-stat.com/pages/Tutor/R_inferno.pdf) to improve your skills and learn about common inefficiencies. You are ""growing an object in a loop"" - initializing vectors with length 0 and then adding to them at each iteration. This is a classic example of how to make R really slow.

In this case, your loop is completely unnecessary. You can subset a matrix directly with another matrix of row and column names (or indices). For example:

	m = matrix(1:4, 2)
	rownames(m) = letters[1:2]
	colnames(m) = LETTERS[1:2]
	m
	#   A B
	# a 1 3
	# b 2 4
	
	ind = matrix(c('a', 'b', 'b', 'B', 'A', 'B'), ncol = 2)
	#      [,1] [,2]
	# [1,] ""a""  ""B"" 
	# [2,] ""b""  ""A"" 
	# [3,] ""b""  ""B"" 
	
	m[ind]
	# [1] 3 2 4

Using this method, your code can be simplified:

    result = list(
      control.weights = control.tm[union.exp.net],
      treatment.weights = treatment.weight[union.exp.net]
    )

Wrap it in a function if you want, but it's so simple it's probably not worth it. Depending on the size of `union.exp.net`, I would guess that this will be between 2 and 4 orders of magnitude faster.",1527689904.0
,Get to know the data.table package.,1527688846.0
fencelizard,"Nope. This is Bayesian statistics. It has a name, and it’s not “probabilistic machine learning”. Also, in practice, complex Bayesian inference methods are such a morass of interacting hyperpriors and  arbitrarily tuned MCMC mixing algorithms that they are not actually any more interpretable than a neural net. Giving this shit a new name is making everyone stupider in the service of chasing additional uninformed VC dollars. ",1527733790.0
Hefeystus,"yes, check out shiny. Might be better off invisibly automating it on your side.",1527684155.0
mtelesha,"There are many different ways and depends on your data size.

In the old days I would use ftp and I had a simple script that watched the folder and would process the data. If it isn’t sensitive or overly large you could even use email.",1527686945.0
thetitanofsparta,With shiny you can create a Dashboard o a web interface easily to run r code,1527687971.0
another30yovirgin,"Shiny would definitely work, or you could try OpenCPU.",1527705078.0
mattindustries,"You could probably just have a cron process check your email for the data, or wherever they house it.",1527715248.0
nextdrone,"I'm looking into how to do this also. Contrary to what is suggested here, I think Shiny is just a Rstudio like web interface. I don't think you can automate any serverside scripts.

I looked into Rserve, but this project appears to be out of date. I couldn't get the module to load. Last update looks like 2015. 

I'm now looking to Rscript, I think this is the way to go. 

https://stackoverflow.com/questions/3301694/running-r-scripts-with-plots

https://stackoverflow.com/questions/31201561/running-r-commands-using-a-bash-script ",1527784829.0
preacher37,"Check out the raster package, specifically ?extract",1527644619.0
proxyformyrealname,"Assuming your interpretations are of the class, raster, you can use sf::st_make_grid to create a vector 'fishnet' of grid cells that you can overlay.

Convert the sf to sp with:

fishnet %>%
as('Spatial') %>% 
as('SpatialPolygons')

Then use raster::extract get the mean interpolation per grid cell.

raster::extract(interpolation.rstr, fishnet, fun = mean)

Check out the data wrangling appendix of this markdown for an example.

https://pennmusa.github.io/MUSA_801.io/project_5/index.html#6_data_wrangling_appendix",1527688844.0
anecarat,The thing is I think you need more than 5 data in each section for the chi test to work properly. Check the requirements and assumptions of the test. ,1527636598.0
efrique,"The chi-square statistic is very small because the deviation from equal proportions is as small as you can get with 9 successes.

However this doesn't *confirm* an ""equality of population proportions"" null against a two-sided alternative. Indeed the null is almost certainly false. It merely means the effect is too small to tell from what you'd see due to random variation at that sample size. ",1527661868.0
mattindustries,"This might be a lot of work, but you could probably build up some features with tuneR package to do some machine learning. https://www.r-bloggers.com/intro-to-sound-analysis-with-r/. If you have the file separated into channels that could make it easy for you. ",1527625259.0
GelmansDog,I played around a little bit with this and concluded that using pyaudioanalysis. Even if you don't know python it's pretty easy to get started.,1527632810.0
Economist_hat,"Use data.table or dplyr

    library(data.table)
    
    my_datatable <- as.data.table(my_dataframe)
    
    # Sort
    setkey(my_datatable, Name, Date)
    
    # Sift
    my_datatable[Date - shift(Date) == 12 &
                 !is.na(KeyColumn) &
                 !is.na(shift(KeyColumn)) , , by = .(Name)]

This will not work off\-the\-shelf. You have to convert the ""Date"" column into a class where you can take differences. \(Check as.POSIXct\)

The flavor is all here.",1527616629.0
DeuceWallaces,"You only want rows where a 

1. KeyColums has an actual
2. NA as long as it's followed by a KeyColums that is not NA

Is this correct?",1527615444.0
infrequentaccismus,"It is a bit hard to parse what you are looking for.  Here, I create a second column called keycolumn2.  It is the value of keycolumn when it is not NA.  When it is NA, it takes on the last value of the most recent non-NA month up to 12 months ago.

    # Make a dataframe that looks like your sample
    df <- expand.grid(name = rep(letters[1:3]),
                      dt = seq(ymd(""2015-01-01""), ymd(""2017-12-01""), by = ""1 month"")
                      ) %>% 
          mutate(keycolumn = sample(1:5, nrow(.), replace = T),
                 dt = substr(dt, 0, 7)
                 ) %>% 
          sample_frac(1)
          
    df$keycolumn[sample(1:nrow(df), nrow(df)/3)] <- NA
    
    
    # Transform dataset as you suggest
        df %>% 
      mutate(dt = ymd(paste0(dt, ""-01""))) %>% 
      group_by(name) %>% 
      arrange(dt) %>% 
      mutate(l1 = lag(keycolumn),
             l2 = lag(keycolumn, 2),
             l3 = lag(keycolumn, 3),
             l4 = lag(keycolumn, 4),
             l5 = lag(keycolumn, 5),
             l6 = lag(keycolumn, 6),
             l7 = lag(keycolumn, 7),
             l8 = lag(keycolumn, 8),
             l9 = lag(keycolumn, 9),
             l10 = lag(keycolumn, 10),
             l11 = lag(keycolumn, 11),
             l12 = lag(keycolumn, 12)
             ) %>% 
      ungroup %>% 
      transmute(name, dt, keycolumn, keycolumn2 = coalesce(keycolumn,l1,l2,l3,l4,l5,l6,l7,l8,l9,l10,l11,l12)) %>% 
      arrange(name, dt) %>% 
      filter(!is.na(keycolumn2)) %>% 
      View",1527648799.0
ReusAlThor,Look into tidyverse. You could do most of this with some creative pipelines using filter and full_join.,1527629510.0
infrequentaccismus,Install latest version of dplyr. ,1527682122.0
Economist_hat,"data.table

dplyr

ggplot2

knitr

caret

And everything else is as needed.
",1527515023.0
soft-error,`base`,1527515763.0
Ax3m4n,"That tweet is less then useful. Ranking packages by GH commits is a bit silly. Also, they are packages, not libraries. `ggvis` is dead (for now).",1527520116.0
Katdai,"It’s never mentioned on these lists, but, to me, RStudio is an absolute requirement. And git, because everyone should be doing good version control, whether they work in a team or not. 

They’re not technically packages, but I feel they get lost sometimes. ",1527518220.0
BUCats,Pacman has p_load which allows you to call all needed packages in a single line.  It will install any packages you don't have and libraries automatically.,1527527891.0
TeslaIsAdorable,"One I haven't seen on here that is simple but useful is 'fcuk', which alerts you to pesky spelling errors in your code.",1527542710.0
sparkplug49,"tidyverse has become the first and often only library I load. Not suggesting everything should be written like that but that group of packages has really become a complete ecosystem in the last couple of years.

DBI, rmariabd, shiny, highcharter, pool",1527518407.0
CaramelTHNDR,"lubridate

If you are working with data that uses various date formats this really helps. ",1527562733.0
sdrakulich,"[I just made a post about this](https://sdrakulich.com/lil-bits/r-packages). My site is a WIP, but I hope the post itself serves useful. 

R Packages for data processing, exploration, model building, visualization, text mining, and miscellaneous others. Please let me know if you found it helpful :)",1527521303.0
dirtyfool33,Mosaic has some great things in it.,1527532795.0
another30yovirgin,"I need to get into dplyr. It's one of those things where I love plyr so much, and that warning comes on that I shouldn't use the two of them together, and I'm just like ""ok, maybe next project.""

Also someone needs to do for XML what they've done with jsonlite. There are at least a few XML packages (XML, XML2R) and you usually end up with an S4 mess that takes 3 hours to unwrap. ",1527564489.0
ran88dom99,[no need for coding in R](https://alternativeto.net/list/2063/guis-to-save-from-typing-r-code/),1527645164.0
Tarqon,"Good old KDnuggets reposting something that was on reddit days ago.

Anyone have experience with mlr? How does the API compare to Caret?",1527537993.0
NovemberTerra,"


    fcuk
for command typos. Found it to be very useful when I was a beginner. Maybe intermediate/advanced users also find some use for it",1527557840.0
Mooks79,"I’d also recommend this fellow’s package, which dramatically simplified package installation/loading. 

https://www.reddit.com/r/rstats/comments/8l7w0q/i_made_a_package_to_automate_package_install",1527538256.0
samclifford,"You want a logistic regression, not Poisson, as you're looking at the number of times something occurs out a possible number of times it could have occurred. ",1527499722.0
dontchokeme,Looks like B is a parameter. I guess you gotta fix it and then generate Z_t via rnorm. Then X_t is just Z_t * K where K = (1-B)/(1-0.5B+0.06B ^ 2) . Then you have your X_t series.,1527467041.0
KingDuderhino,"First, you have to rewrite the process in a form without the backwards operator and rearrange it. You should get something like

    X_t=....
On the right hand side of the equation are the past realizations of the Xt and Zt. Thus, in order to simulate the time series you have to keep track of two variables: the Xt and the Zt.",1527504755.0
Madeline1844,Thanks so much everyone!,1528072158.0
KopfJ4ger,[The answer you seek is one google search away](https://stackoverflow.com/questions/25961897/how-to-merge-2-vectors-alternating-indexes),1527378680.0
anonemouse2010,"Sin e you're alternating M and F, you can subset the men with their numbers and women with theirs... (and the list doesn't need to be longer than 2 * lcm(Mlen, Flen) in this case 2 * 5 * 3 = 30 (so equivalent to n = 15)

    n <- 15
    x <- rep(NA, 2*n)
    Mlen <- 5
	Flen <- 3
    x[0:(n-1)*2+1] <- paste(""M"", rep(1:Mlen, times = ceiling(n/Mlen))[1:n], sep = """")
    x[1:n * 2] <- paste(""F"", rep(1:Flen, times = ceiling(n/Flen))[1:n], sep = """")
	x",1527383279.0
efrique,3x2 chi squared test? ,1527320900.0
abecker93,"This is a clear chi-squared test of homogeneity if you're just comparing the failed vs not failed proportions among the three groups.

If you're using 'days until failure' or mean temp you can test those separately using paired t-tests. I'd use  to see if there's a correlation between those. Across all drives see if there's a difference in mean temp failed drives, then you can move on to individual tests for each drive compared to each other. You may want to use some sort of correction for multiple comparisons

You can then do the same thing for number of days-- split into two groups, failed and not failed, run a t-test on mean number of days, see if it's significant, then move on to pairwise comparisons.

If you do this you should be able to answer questions such as 'do drives fail more often from brand A, B, or C?' and 'does temperature effect drive failure? Does it effect drive failure for brand A, B, or C?' and 'is there a pattern between length of use and drive failure? Are some brands more likely to fail early or late?'

This looks like a grand total of around of 9-15 hypothesis tests, which isn't an incredible amount. If you're very concerned about false conclusions, use a Bonferroni correction. For each **group** of tests (so 1 test for proportions, between 1-7 for temp, and between 1-7 for time) divide the alpha level by the number of tests you *expect* to do. So if you're planning on doing four tests for temp (omnibus, and pairwise comparison for each group), you'd divide your alpha-level by 4, resulting in 0.01/4=0.0025 as your level at which you'd conclude that your result was significant.",1527321557.0
,[deleted],1527307459.0
shujaa-g,"Use `geom_area`. I think. Hard to be sure without seeing sample data or the code you used... you mention only two columns but it seems you need three, job, employment, and probability of automation.",1527305335.0
Quasimoto3000,This is the first I’m hearing of plans to make purrr parallel. What an awesome development. The R community never ceases to amaze me.,1527358234.0
robsalasco,"I'm running it but I don't have better results than other packages. In the near future purrr will support parallel functions. Another interesting package is https://github.com/HenrikBengtsson/future.apply.

",1527534832.0
dm319,"I've had a go installing this and comparing it to tSNE on a 200'000 cell 38 dimensional mass cytometry experiment (white blood cells).  The data was asinh transformed and zero values were redistributed normally - both of these transforms are fairly common place in the emerging mass cytometry literature.

Here is the [comparison](https://imgur.com/a/WctRiry).

Visually it appears much simpler than tSNE, but it does seem to 'cluster' into clear white cell subsets - T cells (CD4 and CD8), B cells (CD19) and natural killer cells (CD56).",1527690634.0
dm319,"Hi this looks very nice.  For very high dimensional datasets, like the scRNA seq data, it looks promising.  Do you know if this is comparing the barnes-hut modification of tSNE?",1527268444.0
infrequentaccismus,"What you are trying to create to create is called a pivot table. Most people prefer to visualize this sort of data to share instead of pivot table it or if they just need to quickly look up the number then they’ll aggregate it into a tidy table.  With that’s said, you can google “r pivot table” for many different options to create it if you are still interested. “rpivotTable”is a good option that creates a pivot table html widget like what you are used to seeing in saas ",1527255165.0
oggesjolin,Have a look at expss package. It is very comparable to spss custom tables,1527339599.0
DrLionelRaymond,The mice package can do this. Fully conditional specification (FCS) is also known as multivariate imputation by chained equations (MICE). ,1527225109.0
PM_ur_good_deeds,"FYI, Googling 'how to do <X> in R' will usually lead to you official documentation and tutorials.",1527231794.0
brbecker,Huh?,1527224640.0
SickSalamander,"Yes, absolutely. 

>I was wondering if R could do what I'm looking for natively? 

Oh? No. Nevermind. R doesn't do anything natively. You need a package like jmv to do that. And even then, its just the framework there and you'd need to code or copy a loop to run it multiple times and aggregate. Still a good (the best?) option.  ",1527179397.0
Slabs,"Check out the MplusAutomation package. 

I know you didn't mention stata, but there is a lovely package called runmplus that has saved me countless hours by allowing me to automate multiple models in batch.",1527186944.0
DonaldFagan,"Also, not that you asked, but I think that in Stata that the -sem- command supports 1) mixed effects model, and 2) FIML via the -method(mlmv)- option. That would be easier than stitching together Mplus. But, it would not be free like R. Hope this is helpful. ",1527221703.0
DonaldFagan,"I am officially a bad person. Stata has multilevel
sem and it has sem with ML with missing values— but it does not have both simultaneously. ML is in the sem command and multilevel is in the -gsem- command. My bad!!!!",1527280450.0
faelun,there's also lavaan which does ML SEM and can impute data with FIML and approximate Mplus' method,1527443272.0
ChurchonaSunday,"Personally, I see no reason why you cannot code these as dummy variables having configured control as the reference level. Thereafter the output would be an OR for each treatment relative to control.
",1527189218.0
ELKronos,"To clarify, are the treatments mutually exclusive? That is, it's impossible for someone who received treatment A to have received any other treatment (like a randomly assigned condition)?",1527171200.0
dirtyfool33,"Make the treatment variable a factor, and set the CTRL to the first level (CTRL =1).  When you run your glm (I am assuming a logistic regression here, based on the comments), your control group will be the reference, and the coefficients you get can be converted into odds ratios.  
Why don't you want to use dummy variables?  For things like this they make your life easier.  You don't even have to code them if you have a variable that says which treatment group the observation belongs to. ",1527192219.0
darknomad23,"reverse_normalized_value =  normalized_value * (max(x) - min(x)) + min(x)

",1527125043.0
efrique,"You need to have those max and min values available. 

If z = (x-l)/(u-l)  then x=z*(u-l)+l",1527127156.0
StephenSRMMartin,"Why don't you look at your data to actually see what is missing? You say ""there is no way it could be missing"", but obviously there is \*something\* substantial missing from at least one of these variables. Subset your data frame by every variable name included in this model, and look for missings. The mice::md.pattern\(\) function may help visualize missing data in your data set.  


There is at least \*one\* variable in this model that has missings. Given the... difficult variable names \(I'm guessing you're using qualtrics, and didn't rename the items\)... there's a decent chance you've included a variable you didn't intend to.",1527107742.0
MrLegilimens,Talk about descriptive variables names. Would hate to be in your lab. ,1527106661.0
faelun,It sounds like its missing data or its not reading your data correctly. ,1527269576.0
,"Interesting conclusion from this article. The author lists packages based on the number of contributors and number of commits, but after the table says that these are the ""most popular"" and ""time-tested"" libraries. Actually, number of commits and contributors is not the same thing as popularity and longevity in software. ",1527104397.0
Owz182,"I suppose take it as one person’s opinion, but there were a few on there I wasn’t using before but might check out now",1527137281.0
Digging_For_Ostrich,"RAM.

Once you exit your session, it will ask you if you want to save your session data, which would then write it to a temp file.",1527086519.0
c_saezm,Perfect!! Thanks a lot!,1527086934.0
LiesLies,"R stores everything in RAM. I save data as either .csv files with descriptive names for local use later, or I write/append my results back to the database within the same script.",1527095690.0
ELKronos,What is your specific research question?,1527033439.0
Crypt0Nihilist,"Only skimmed so far, but found his casual use of ""reproducible reports"" a bit concerning. The usage makes sense in layman's terms with the automation theme, but that's a misleading and dangerous conflation with the scientific meaning where you can get the same report out by running the script of the same data in.",1527033345.0
SataMaxx,"Granted the `Column` column in your dataframe holding the variable names is of type character (and not factors, as per default with dataframes and character vectors), and all variables exist in the environment, you can do

    df$Column2 <- unlist(mget(df$Column))

This will only work when called from the global environment.  
If you want to do this inside an function you'll need to add an `envir = globalenv()` argument to `mget`.",1527033337.0
efrique,"1. According to this:

  https://en.wikipedia.org/wiki/Hubbert_curve

 the Hubbert curve is a scaled logistic-distribution curve, *not* a scaled Gaussian curve:

 > The Hubbert curve is an approximation of the production rate of a resource over time. It is a symmetric logistic distribution curve,^([1]) often confused with the ""normal"" gaussian function. It first appeared in ""Nuclear Energy and the Fossil Fuels,"" geologist M. King Hubbert's 1956 presentation to the American Petroleum Institute, as an idealized symmetric curve

 Do you want a Hubbert or do you want a Gaussian? Or both?


2. It looks like you won't get a very good fit with either function; first, the data look ""shifted up"" compared to a scaled density; it might work okay if you add a constant term. Second the peak is too broad for either curve.

 Edit: I tried a Gaussian fit -- terrible. But a Gaussian + a constant, as I was thinking just from the look of it -- actually fits really well.

3. You'll have an issue of serial dependence; the usual ways of fitting such curves assume independence and you likely won't have something close to that here; if you try to construct any tests or confidence intervals or prediction intervals that will be a problem for the typical methods



Edits: added more info, removed a question already answered in the original post",1527031243.0
revgizmo,So what did you learn from this? What are you now doing differently?,1527032499.0
_Wintermute,"Probably sacrilege on here, but you could try not using dplyr/NSE in functions, base R can subset rows just fine.

Replace

    table <- table %>% dplyr::filter(iv==coefa | iv==coefb)

 with

    table <- table[table$iv == coef1 | table$iv == coef2, ]

and just pass the arguments as strings.",1527063665.0
RememberToBackupData,"You provide bare names as the arguments to your function, but you haven’t actually done anything to **lazily evaluate** those bare names — to tell R, “Don’t try to follow these names back to an object that’s stored in memory. I will do something else with them, I promise”.

Lazy eval is a total black hole that is fascinating and rewarding, but also will swallow up quite a few hours. For you, the easy way out is to use the deprecated standard eval version of dplyr functions (e.g. `dplyr::filter_()`) because these will take character strings as arguments instead of bare names, and then you can say 

    modeloutput2(model, “Petal.Width”, “Sepal.Length”)",1527026187.0
nobadchainsmokers,"How about this? It's not the prettiest but work on your different date formats

    library(dplyr)
    library(openxlsx)
    library(lubridate)
    
    df <- data.frame(
        obs = c(1, 2, 3),
        TimeVariable = c(41642.738842592589, '14JAN2014:19:02:00', NA)
    )
    
    df <- df %>% 
        # each date format as own column then select non NA
        mutate(x1 = dmy_hms(TimeVariable),
               x2 = convertToDateTime(as.character(TimeVariable)),
               NewTimeVariable = if_else(!is.na(x1), x1, x2)) %>% 
        select(-x1, -x2)",1527015933.0
sohaibhasan1,"Looks like you just need to apply two different functions in two different scenarios. Wrapping the function calls in Ifelse or case_when should do the trick. The case could be if it finds the abbreviated name of the month in the string, or a colon.",1527010236.0
brbecker,"I think you guys are on the right track. Id split this problem into 3 parts.

1. A function to handle format 1
2. A function to handle format 2
3. A function to determine if a string is case 1 or 2 (like how you're using grepl. You'll want to spend time to really make sure you get this right)

Then you can use a case_when inside a ""clean up ugly times"" function where you say

1. If na return na
2. If format 1 apply function 1. 
3. If format 2 apply function 2.

Then mutate_at with the ""clean up ugly times"" function. Any error you have you can test each part carefully until you find what's wrong",1527020138.0
Digging_For_Ostrich,"This links to a blog post that says nothing. Completely non-information posted by a spammer.

The answer is of course yes, and there are many very good and detailed tutorials to link R with all sorts of things on much better sites than this.",1526984337.0
PM_ME_QUOTE,Now I do not have an excuse to take a coffee break because I am installing packages. ,1526980925.0
RememberToBackupData,"    # To install and attach
    shelf(dplyr, DesiQuintans/desiderata, purrr, update_all = FALSE, quiet = FALSE)
    
    # To unattach
    unshelf(dplyr, desiderata, purrr)

**Key points:**

- Install and attach packages from CRAN and GitHub in one function call.
- Uses bare names so that you don't have to mess with quoting your character strings.

I'd like to get this on CRAN within a week or so!

---

**Hey, if you know stuff about lazy eval:**  I currently have a dependency on `rlang` for just one function:

    dots <- rlang::enexprs(...)  # Lazy eval of dots list, pretty important!

It's the only function I found that treats the GitHub `username/packagename` names as a single thing without breaking it up into an unevaluated call (`/`, username, packagename). 

I would like to get rid of that dependency if possible, so if you know a way to do this in base R, I would be very happy to know it. I'd like to get dots as a list of names or a list of character strings, either works for me.",1526970580.0
tacothecat,How is this distinct from [pacman](https://cran.r-project.org/web/packages/pacman/vignettes/Introduction_to_pacman.html)?,1526988511.0
Darwinmate,Post this on stack overflow i think you will get your answer.,1526960954.0
giziti,"    levels(x_cut)[[1]] <- new_level

The problem in `recode_factor` is that `old_level` isn't a part of `x_cut` or anything, it's just a string floating around.",1526966151.0
RememberToBackupData,"This is what the apply functions, but particularly `purrr::map`, are for.

    library(dplyr)
    library(purrr)
    library(janitor)
    library(readxl)
   
    data <- 
        map(c(""raw1.xlsx"", ""raw2.xlsx""),
            ~ read_excel(file.path(""Subfolder"", .x), na = c("""", "" "", ""NA""))) %>% 
        map(., ~ clean_names(.x)) %>% 
        map(., ~ select(.x, -starts_with(""notes""))) %>% 
        map(., ~ mutate_at(.x, vars(contains(""date"")), ymd))

Inside `map`, you use `.x` as an alias to refer to each entry in the list or vector you've supplied. For the first `map` call that's the names of my spreadsheets, but for subsequent `map` calls it's the dataframes that are being held inside the list that's being piped down with `%>%`.

Once they're fixed up you can use `purrr::map_df` and `bind_rows` to get them into a single dataframe. ",1526873239.0
B1llyII,I don’t get it,1526867880.0
Molozonide,"R, Notepad, Snipping Tool?  
Not sure I get it...",1526868635.0
paerb,It's an RNotebook. I don't understand the Snipping Tool being there though.,1526868018.0
,I dislike this chart because the way the bars are connected suggests that there is an element of time involved,1526833203.0
pollinguk,"One, latent growth curves are really cool. You should have some fun with this. The easiest way to implement this in R is to use the growth function that you can find in the lavaan package.


Two, as the name implies, latent growth curve models model the change in a variable over time as a latent factor. This is because they are a part of the general structural equation modelling framework. As a consequence, each respondent’s slope for your dependent variable is available to include kind of analysis you like. You might think, for example that the slope parameter mediates the effect of one variable on another, or wish to see how demographic variables predict the slope.


To get started, I’d suggest “Longitudinal Structural Equation Modelling” by Newsom and “Growth Modelling” by Kevin Grimm. There are also some good videos on YouTube by a company called Curran-Bauer analytics (I think).


Feel free to get in touch if you have any questions.",1526742080.0
TalkterWho12,"A faculty member at my university put together a workshop and actually included stuff about implementing lavaan for Latent Growth Curve Models \(LGCM\). Notes for that workshop can be found [here](http://www.yourpersonality.net/R/R_Notes_3.html). I would also second using anything by Grimm. I took a class on LGCM and it was taught with [this book](https://www.amazon.com/Growth-Modeling-Structural-Multilevel-Methodology/dp/1462526063) by Grimm and it was extremely helpful. 

I have become the go to person in my lab \(psychology\) for using lavaan and have helped my peers become more familiar with LGCM. If you have any specific questions, or would want someone to take a look at your code, I would be happy to! I really enjoy this type of stuff. Google and the lavaan Project will be super helpful! There are plenty of additional resources out there!",1526788569.0
WhosaWhatsa,"First, you would want some very clear criteria for putting certain specimens into the ""healthier"" or ""sicker"" categories. This part is arguably more difficult than the stats analysis because you're trying to account for nuisance variables that are not known. What characteristics count toward the ""sicker"" categories, for example? The same question goes for ""healthy"". So the experiment itself will need to flesh these issues out before the stats analysis will do its magic. From my understanding, a series of biological measures (continuous data) and surveys (categorical or ordinal most often) are usually given to specimens in experiments like these. My guess, however, is that ""sicker"" and ""healthier"" are not often used as classifiers. Perhaps a scale from 1-10 is used to gauge where specimens land on the healthy/sick spectrum. Perhaps ""has undergone chemo"" and ""has not undergone chemo"" indicate another important factor. But since you're trying to identify treatment factors, you'll need effective, accurate methods to do so. Find yourself a clinical expert!

My next question: is Pt (and its subsets) large enough to accurately put specimens into the 5 stages (0-4). This question would need sample size calculations. For example, let's say that you did your tests and the following proportions arose from your sample: 

0 - .15 (0%)
1 - .25 (25%)
2 - .27 (50%)
3 - .20 (75%)
4 - .13 (100%)

You could run an ANOVA (as long as certain assumptions are not violated) to determine if these groups have equal dispersion and equal means. This ANOVA would take into account the number of treatment factors you developed in your experiment (healthy vs. sick specimens, chemo vs. no chemo, etc.). Then, just because stages 1, 2, and 3 are numerically different doesn't mean they are actually different according to an F statistic. Let's assume that your ANOVA shows that 1, 2, and 3 do have the same mean proportions and that 0 and 4 are in categories by themselves regardless of other treatment factors in your experiment. Now, you have three groups: 0, 1-3, and 4. Each group has different point estimates:

0 - .15
1-3 - .24
4 - .13

But, how much statistical power do you have to make even this somewhat simple conclusion of point estimates? A power analysis (sample size calculation) can give you an idea. You may have grouped 1, 2, and 3 together as having the same mean, but you did so because you don't have a large enough sample to detect the true difference between .25 and .27 (groups in stages 1 and 2). 

Then, just because you didn't find any of your treatment factors to be significant doesn't mean that you've reached a conclusion. Perhaps there is another treatment factor out there that your experiment did not account for. 

Finally, how does the known incident rate (P) breakdown into the stages 1-4 (since only 1-4 are indicative of positive tests)? Do my sample proportions in each stage align with the known proportions? ",1526740745.0
ichikaren,I suggest you ask this to folks at r/statistics instead. They are more suitable to answer this question.,1526741826.0
PopeRaunchyIV,"Since your vector is all zeros and ones, you can use the `mean()` function.

Side note, the best practice in R is to assign with `<-` instead of `=` ",1526705523.0
navidshrimpo,"Am I alone in thinking that passing queries through dplyr syntax is never as intuitive as it's made out to be? I struggle with decently complex queries, whereas typing in T-SQL or mySQL syntax I can intuitive throw together a complex query with CTEs, window functions, or other stuff in minutes. It's not elegant passing it through RODBC as a long string, without the formatting stuff, so I either:

-copy paste the query from my main DB tool

-just have two windows open and wrap my query up in a view, and then query that with a simple select in R

dplyr just confuses me when I go beyond the basics. Any recommendations?
",1526701248.0
Runner1928,"I often use a sqlite database for persisted models. In your training session, save to sqlite. Then in the prediction script, which you can run from the command line, load the sqlite database to get the trained models, then from wherever on your machine load the data to score, and run the data through the model.",1526656565.0
featherfooted,"i feel like this depends way more on your mail client than it does on your R environment.

Does your mail client accept custom spam filters? ",1526680159.0
RememberToBackupData,"I don't really understand the data to begin with so you should provide a small example dataset for best results. If we go with the real estate listing analogy, then using ML to dedupe is wildly overkill and you could just, as an example, `dplyr::arrange(address, ...)` to sort the dataframe by address first and then by the other fields in order of preference (e.g. make a decision to trust the largest floor size with `arrange(address, desc(floorsize), ...)`). Then you can `dplyr::distinct(address, .keep_all = TRUE)` to keep only the first match and get rid of the rest. `dplyr::top_n()` is probably the same thing but in a single function call. In the case of advertising, if you had a record of the product or service being advertised then this seems like a pretty good thing to use as an index.",1526631477.0
OneLegAtATime,"I dont understand the thickness of the polygon borders, but you can get the scale of the map through ggsn::scalebar. Hope that helps?",1526601347.0
fencelizard,"Looks like the map is in decimal degrees (lat/long). The axes will show how far apart a degree is. If you want a different distance measure, for example a bar showing how long a kilometer is, you’ll need to use a coordinate system that has constant length. In lat/long a degree at the equator correspond to a different length on the ground than a degree close to the poles, so a single scale bar can only give an approximation and won’t be true everywhere (depending particularly on the latitude range). Check out the “sp” package for functions to convert coordinate systems (I think it’s spTransform() ). Fair warning that geographic projections and file systems are a mindfuck. If you have a small area UTM is a nice coordinate system where the units are meters, so that would make it easy to make your own scale bar. 

Otherwise maybe someone wrote a convenience function? If so it’s probably an approximation that will be less useful the larger the latitude range, but might work ok if your scale is close to the equator and/or less than a couple degrees. God speed.",1526608792.0
salgranon,"I don’t have much time to read the link at the moment, but from prelim scam it seems to be half of the puzzle, but correct me if it’s at the link. 

The last piece that I can’t figure out is finding the scale of my map in mm. Unless the scale stays the same even though I change the x/ylims 

So if I calculate the distance in decimal degrees how do I calculate that same distance in mm in my ggplot? 

",1526645961.0
dtrillaa,"Edit: I deleted my first comment because I think this is a better answer.

    grouped_data <- data %>%
      groupby(as.factor(y)) %>%
      summarize(count = n())

    ggplot(grouped_data, aes(x = x, y = y, size = count)) + 
    geom_point()",1526569158.0
Meesje,"Hello,

I want to make a scatterplot in R where the size of the dots is dependent of the amount of datapoints with this same value \(like in the picture I added\). I have been searching a lot but I can not find a simple solution. I found something like qsec but it does not work, but I don't know why. Can you help me with a simple script? \(I have 900 data points and sometimes more than 300 have the same value, I want to be able to see that in a graph!\)

Thanks alot,

Mees",1526567113.0
nobadchainsmokers,"Did you reference `qsec` in http://ggplot2.tidyverse.org/reference/geom_point.html? `qsec` is a variable name in the `mtcars` dataset. If you want to adjust the size of the points, you have to put the variable as `size = ""your_variable""`. 

So your geom_point will be like `+ geom_point(aes(size = ""your_variable""))`",1526569521.0
fencelizard,"Yep. Check out the foreach package. I usually do a for loop/apply/whatever to pull together a list of commands I want to run through system, then iterate in foreach.

Eg

`
    commands <- ...`

`    foreach(i=commands) %dopar% system(i)
`",1526564567.0
guepier,"Right, in that package it makes absolutely no sense. Maybe trying to find meaning in a package called {bizarro} is a futile exercise. More likely, we’re missing context or there’s more to come.

Generally, `UseMethod` is needed to implement S3 generics. Its use starts to make sense when you have multiple types of objects that all have related, but slightly different, behaviours when calling the `bizarro` function.

The most basic example of that is the `print` generic, which is used in R to display user-readable representations of R objects. The `print` function is defined in R as follows:

    print = function (x, ...) UseMethod('print')

And there as different `print` methods for different data types. And you can extend this by making your own types:

    fraction = function (numerator, denominator) {
        structure(list(n = numerator, d = denominator), class = 'fraction')
    }

    gcd = function (a, b) {
        if (b == 0) abs(a) else Recall(b, a %% b)
    }

    simplify = function (frac) {
        gcd = gcd(frac$n, frac$d)
        fraction(frac$n / gcd, frac$d / gcd)
    }

    print.fraction = function (x, ...) {
        simplified = simplify(x)
        cat(paste(simplified$n, ' / ', simplified$d))
        invisible(x) # Needed to conform to `print`’s documentation
    }

And you can use it like this:

    print(fraction(2, 4))
    # prints: 1 / 2

(`print` is also invoked automatically by R whenever you type an expression directly; therefore we could omit the `print` call, and just write `fraction(2, 4)` to get the same result. But that’s unrelated to generics.)

So, to answer your software engineering question: `UseMethod` is a tool to implement the concept of [data abstraction](https://en.wikipedia.org/wiki/Abstraction_%28computer_science%29) and, more concretely, [polymorphism](https://en.wikipedia.org/wiki/Polymorphism_%28computer_science%29).",1526554018.0
Thaufas,"`UseMethod()` is useful as an advanced programming construct to implement polymorphism in R. Hadley explains it well in [this article](http://adv-r.had.co.nz/S3.html).

>""...As you might guess from this example, UseMethod uses the class of x to figure out which method to call. If x had more than one class, e.g. c(""foo"",""bar""), UseMethod would look for mean.foo and if not found, it would then look for mean.bar. As a final fallback, UseMethod will look for a default method, mean.default, and if that doesn’t exist it will raise an error...""",1526545731.0
spaniard96,"I've seen this used a lot in R's standard library, so maybe they've copied the style from that. I'd be interested to know the actual reasoning behind it though",1526544635.0
zdk,"It's good practice to write modular code. Now str_reverse can be reused, perhaps in related S3 methods that are not characters.",1526558605.0
wafflegraphs,"Is there a key that is connecting all the observations somehow? To do something like this, I generally use joins from dplyr and subset with brackets if nec. Do you use dplyr at all? There's a great cheatsheet online: https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf

edit: Actually, I might be misreading you. Are you trying to add more observations from the other data sets to the master only? So the variables of interest exist in all data sets?",1526537334.0
RotundSlim,"Use sqldf package in R, sql made for this kind of job ",1526547089.0
paulusj8,"You can make a list of the dataframes, bind the rows, and select the variables you want to keep:

`library(dplyr)`

`master <- data_frame(a = sample (1:10, 3),`

`b = sample (1:10, 3),`

`c = sample (1:10, 3))`

`dataset2 <- data_frame(a = sample (1:10, 3),`

`c = sample (1:10, 3),`

`d = sample (1:10, 3))`

`dataset3 <- data_frame(a = sample (1:10, 3),`

`b = sample (1:10, 3),`

`e = sample (1:10, 3))`

`# variables to keep`

`master_colnames <- colnames(master)`

`# create a list of the datasets`

`total <- list(master, dataset2, dataset3)`

`# create a dataframe from the list by binding all rows`

`df <- bind_rows(total)`

`# select the variables to keep`

`df <- select(df, master_colnames)`

You can put the last two lines into one with the pipe:

`df <- bind_rows(total) %>% select(master_colnames)`",1526543449.0
InflationSquare,"Using purrr+dplyr something like

bind_rows(master, map(list(df2, df3, df4), ~ .x %>% select_if(colnames(.) %in% colnames(master))))

should work. You map across the list of non-master data frames to keep the columns of the data frame only if their names are in the colnames of master, then bind the resulting list of dataframes into master.

To unpack some of the syntax, you can define an anonymous function that is applied to every element of the list using ~, then refer to the element being mapped over with .x (or ..1, ..2, ..3 etc. if you're mapping over multiple things), when you use a pipe it sends .x into the select_if function, and you can refer to it within that by just using . on its own. 

Edit: check out the haven package, it might save you converting .sav files to csv manually",1526565348.0
Darwinmate,"this is called a left sided merge. `dplyr::left_merge` is the function you are after or for base R: `merge(df1, df2, by=”common_key_column”, all.x=TRUE)`",1526613941.0
wigwaml,"You can also cut down the variables of interest in the other sets with 

other.set <- data.frame(other.set(index, column.you.want, other.column.you.want))

Then merge as 

set <- merge(set, other.set)

",1526562058.0
10101010101111,Perhaps try `pryr::object_size()` and then extrapolate?,1526543007.0
Hoelk,"Usually the in memory size is roughly equivalent to the size of the data stored as .csv 

To get an exact estimate the easiest way is probably to the read the first few rows, use `object.size()` on the result (no need to install an extra package), and multiply it towards the number of total rows.

should be about three lines of code, you could easily write your own function for that :)

getting the number of rowsautomatically might be an interesting challenge though... ",1526549677.0
bc2zb,"In the bioconductor package flowType, there's a [calcMemUse function](https://rdrr.io/bioc/flowType/man/calcMemUse.html) that you might be able to adapt to be used for your tables. ",1526565145.0
Hoelk,"because i got interested in the question I started working on a small package for exactly that task. It currently only works with csv files and there is not documentation yet but you can take a look at it if you want. it seems to overestimate the in memory size by 1-2 %. I am not sure what the reason for this is.

https://github.com/s-fleck/readmate",1526566715.0
Hoelk,"Look at the documentation of `tryCatch()`, it's exactly for that kind of sitatuion",1526495820.0
thedukeofedinblargh,"I'd definitely include it in the portfolio.  A few thoughts, though:

1. The blog post is really long and involved and I just skimmed it.  However, the app itself doesn't mean anything if you don't read the blog post.  You may think of this as just a way to prove you can technically write R, but if I were the hiring manager, I'd wonder if you've ever heard of UX?  An hour adding instructions and context would do wonders.

2. You use very machine learn-y terms (e.g., ""feature engineering"", ""sensitivity""), but then don't do any machine learning or statistical modeling.  Again, I was skimming the post, and kept getting tripped up, thinking I'd skipped the modeling part, only to read back and find I'd just missed a summary table.  I suppose you're not wrong about what you're doing, but it feels weird.

3.  You use your username 13ass13ass on the doc, but your real name is everywhere else.  Make sure you remember to change it.

Bottom line, I think you've proved you can write R, but I'd spend a little more time proving you can be a succinct and engaging storyteller.

One additional thought: I've known a few people who came out of neuroscience or psychology into analysis/data scientist positions.  It can totally be done, and all of us are probably happier for it.  That said, it might also be good to show you can write SQL, assuming that you can.  It's totally optional (and frankly rare) in the parts of academia I came from, but it's critical in every role I've been in outside of academia.  I suppose showing that you can use R to interact with databases effectively (e.g., using dplyr and dbi) is second best.  And, of course, don't use it as an excuse to not get out there (""I have to be a SQL master before applying for jobs"").  Just be aware that it might be worth doing the datacamp or mode analytics courses.
",1526534910.0
localoptimal,"Cool app! I'll try to use it for dinner later.

Just a minor comment: I'd put the github link toward the top of the post. While reading through it I was thinking this is cool - wish there was a github so I could see it as a whole. And in fact it exists! But it's linked at the bottom of the page, and employers may not get that far if they do look at the blog.",1526498271.0
fourpita,Wouldn't it be cooler if you generated a graph for the input combinations?,1526505264.0
tacothecat,The app doesnt seem to do anything on my s9+ ,1526513458.0
shujaa-g,"At a glance - I don't like the first line ""as a project for my resume"", something about that puts me off. Maybe just call it a side project or a personal project. More than that, for me as a reader, that's a really boring first line. The rest of the paragraph gets there, but start by telling the reader why they should be interested in your app.

In the app itself, ""data"" is a terrible heading for the results. Use something more descriptive. It might also be good to have the default first entry when the app loads be something more common than the alphabetically first ACHIOTE SEEDS. 

It'd be nice to add a little color to the app as well, but coming up with an accompanying visualization is probably more work than you're looking to do.",1526504193.0
mattindustries,"I posted a comment on the blog, but also might as well add it here. I wrote up how I see the [future of machine learning](https://rlang.io/machine-learning-as-a-service/ ) being used by the masses. ",1526492575.0
mnesca,"Thank you very much, i just got accepted into grad school going towards a data science route in health care. Posts like these are invaluable!",1526511177.0
MrLegilimens,"Maybe seeing the problem would help. But this also isn't really an R problem, more like a stats problem, so /r/homeworkhelp. ",1526474144.0
efrique,"Indent your code 4 spaces (then remove all the blank lines between)... it will be more readable
",1526474501.0
amoonand3balls,"I encountered a similar error in the past and it was a result of the correction applied by R, but that does not seem to be your issue.

Your p-value is correct for that test stat and df. Do you know what answer you are looking for from the test stat?
    ",1526492829.0
alhoand,Let me know if you need ellaborations to help :),1526473668.0
thedukeofedinblargh,"In your case, ungroup B (if you want to).  The group_by doesn't permanently affect A as an object.  It only affects it for the purpose of that one set of piped commands.  (I'm sure there's a more precise definition in [Advanced R](http://adv-r.had.co.nz) having to do with scoping and lazy evaluation or something)

B, on the other hand, you can think of as a version of A with grouping attached to it.  Thus, B will be grouped if you inspect the object.

Of course, I almost never ungroup my tbl's, until it comes back to bite me 3 days later when I can't drop a variable, or calculations act unexpectedly and it takes me 20 minutes to realize why.",1526448535.0
Darwinmate,"Format your code.

What the hell is A and/or B? A is a variable but B is an object? Why do you refer to them together?",1526447091.0
Darwinmate,"The first step in any analysis is data cleaning. The `dyplr` and `tidyr` packages are your friends!

I would start with cleaning each of the 5 datasets individually then merging them into a super-set then make them long. Look at the `Tidy` data cleaning principles (https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html) and try to follow these. They will make your life easier. Stick with one type of nomenclature/style/abbreviation. You may need to include a supplementary file detailing each variable (column), how it was created (if it's computed from other variables), or what the abbreviations mean. 

The `tidyr` package has some great functions, such as `fill`, `spread`, `gather`. The `dyplr` package is great because it has `mutate` and `filter` functions and imports the `%>%` (pipe) operator making your life easier. I hear `lubridate` package is awesome for working with dates, personally I split dates into day/month/year variables. This works only if you don't need to compute time differences or time series analysis.  

Finally, `grep` family of functions in base R can be extremely powerful. I love `gsub` for finding and replacing or extracting importation. 

",1526430685.0
thedukeofedinblargh,"I agree with /u/Darwinmate that tidyr can solve this for you.  I suspect it will turn out not unlike the top comment [on this post](https://www.reddit.com/r/rstats/comments/8jpykf/how_to_take_difference_of_paired_columns_in_dplyr/), in which you gather your variables; use some mix of separate, mutate, and/or gsub or something to parse the names; and then spread back out to whatever you want to end with.

If when you get your data you want to post a sample, you might get lucky and have someone willing to work the example.  Someone who's done this sort of thing before should be able to do it in minutes, not weeks (assuming a reasonable level of renaming).",1526449106.0
,"In addition to the other responses here, if your data is truly messy, you may want to look into the package 'fuzzyjoin', which, as the name implies, allows you to join data frames based on keys that 'sorta' match.",1526473032.0
ryanw89,Here's a useful post you can check about converting data from wide format to long format: https://www.r-bloggers.com/converting-a-dataset-from-wide-to-long/,1526488998.0
scbagley,"Try this:

    library(dplyr)
    library(tidyr)
    d <- tibble(hits = 1, runs = 2, o_hits = 5, o_runs = 9)
    d2 <- d %>% gather(key = ""col_name"", value = ""value"", everything())
    d3 <- d2 %>% separate(col_name, into = c(""team"", ""metric""), sep = ""_"", fill = ""left"")
    d3$team <- ifelse(is.na(d3$team), ""a"", ""b"")
    d4 <- d3 %>% group_by(metric) %>% spread(key = ""team"", value = ""value"")
    d5 <- d4 %>% mutate(diff = a - b)


    ",1526428699.0
mongooseondaloose,"There's likely a cleaner/more elegant way to do this, but this is the solution I've used in the past when faced with a similar issue, using dplyr (also depends stringr and forcats, but `library(tidyverse)` should take care of that). Here I'm using the diamonds dataset and creating error-fuzzed ""opponent"" scores for the diamonds x, y, and z variables.

    # Set seed for reproducibility
    set.seed(1738)
    # Load tidyverse
    library(tidyverse)
    # calculate diffs
    diamonds %>% 
      head(5) %>% # Pull the first five obs, for eyeballing
      mutate(id = 1:n(),
             o_x = x + runif(5),
             o_y = y + runif(5),
             o_z = z + runif(5)) %>% 
      select(x:o_z) %>% 
      gather(metric, score, -id) %>%
      mutate(group = str_match(metric, ""x|y|z"") %>% as_factor()) %>% 
      group_by(id, group) %>% 
      arrange(id, group) %>% 
      mutate(i = seq_along(id),
             ref = ifelse(i == 1, score, lag(score)),
             diff = score - ref) %>% 
      filter(i == 2) %>%
      select(id, group, diff) %>% 
      spread(group, diff) %>% 
      {
        df <- .
        diffs <- str_c(names(df), ""_diff"")
        names(df) <- diffs
        names(df)[1] <- ""id""
        df
      }

Basically, the process is as follows:

1. Tidy the data (observations in rows, characteristics in columns): `gather` call
2. Pull a metric identifier via regex (here you'll probably want to use ""metric\\d"" as your pattern): group = `str_match` in the `mutate` call
3. Sort by a (contrived, in my case) identifier variable, assign the reference group to the first observation (i.e., the non-`o_` metric), and calculate a difference variable for each row
4. `Filter` to the desired rows (metric*X* - o_metric*X*)
5. `Select` and `spread` variables
6. Reassign column names, if desired

Hope this is helpful!",1526428626.0
infrequentaccismus,"The trick here is a clever multiple use of tidyr's gather and spread. Run the dplyr-tidyr pipeline line by line to see how each line works.  

    library(tidyverse)
    df <- tibble(metric1 = runif(100),
                 metric2 = runif(100),
                 metric3 = runif(100),
                 o_metric1 = runif(100),
                 o_metric2 = runif(100),
                 o_metric3 = runif(100),
                 )
  
    df %>% 
      mutate(id = row_number()) %>% 
      gather(key, value, -id) %>% 
      mutate(source = if_else(stringr::str_detect(key, ""o_""), ""Opponent"", ""Team""),
             key = stringr::str_replace(key, ""o_"", """")) %>%
      spread(source, value) %>% 
      transmute(id,
                key,
                metric_diff = Team - Opponent
                ) %>% 
      spread(key, metric_diff) %>% 
      select(-id)",1526445068.0
j4kem,"I may be missing something here, but why won't base R work with A-B, where A is your matrix of metrics, and B is your matrix of corresponding opponent metrics?",1526469330.0
questionquality,"Here's a different approach. It avoids gather and spread, and instead uses the (black magic) `rlang` package (still part of the tidyverse)

    
    library(tidyverse)
    
    
    example_data = data.frame(metric1 = 1:10, o_metric1 = 2:11,
                              metric2 = 10:19, o_metric2 = 2:11)
    
    
    diff_column <- function(data, col) {
        # rlang magic .. don't ask me why I used sym here and ensym below
        # it's what worked
        col = sym(col)
        
        opponent = str_c(""o_"", col)
        opponent = ensym(opponent)
        
        diff = str_c(""diff_"", col)
        
        # this is the crucial part: mutate a dataframe with programmatically defined names
        mutate(data, !! diff := !!col - !!opponent)
    }
    
    # use your list of ""base metric names"" here instead
    col_names = 1:2 %>%
        str_c(""metric"", .)
    
    result = col_names %>%
        # for each metric name ..
        #   apply the function
        map(~diff_column(example_data, .x)) %>%
        # map returns a list, which Reduce and *_join can then join together
        Reduce(full_join, .)
",1526465228.0
,[deleted],1526387799.0
Darwinmate,post your data. ,1526381616.0
grasshoppermouse,"You need the Obs var in your newdata:

    newdata <- data.frame(Temperature = 31, Obs = ???)",1526398195.0
Thaufas,"Anytime you see a construct such as the following in your model construction, that's a hint that you should really restructure your data before building your model.
`cbind(Failures, Successes-Failures)`

Without seeing your data, our ability to help you is very limited.",1526417501.0
frothdoctor,"https://i.stack.imgur.com/Ka8Zq.png

That link should be to the data.",1526455251.0
StephenSRMMartin,"First of all; what is with the R code?

```
fit1 <- glmer(fruit ~ elev + slope + ndvi + watshed + solar + (1|treename), data=ii, family=poisson())
```

As for the error - Your outcome variable isn't non-negative integers. It has real numbers, which doesn't work with poisson distributions, by definition.",1526358384.0
,"It looks like you are using treename as the group variable in the mixed model, but you have the same number of distinct trees as total observations. You need to have multiple obs for each tree to fit a multi-level model. I could diagnose this problem if you actually provided the data in csv format, but since you provided an image, I am disinclined to investigate further. 

Other than that, your model equation in R is strange: You specify a data set but also use dollar signs to pick variables from a globally defined dataframe (just do one or the other). The family argument is also strange, just write 'family = poisson' if you are not going to monkey around with the link function.",1526347881.0
pictocat,"Keep in mind that lme4 operates in S4 which has stricter rules than S3, so keeping your code S4 compatible is necessary to avoid errors.",1526357046.0
tacothecat,I think you'd have much better luck if you explicitly stated what isn't working and provided the code you're trying to run. ,1526349520.0
oggesjolin,"I had some luck with the rgdal package, downloaded shapefiles (I did maps for Sweden, but there are US county shapefiles on cencus.gov) and plotted with ggplot2. 

Check out: https://gist.github.com/lmullen/8375785  
or this? https://eriqande.github.io/rep-res-web/lectures/making-maps-with-R.html

(I noticed loading tidyverse (and thereby purrr) masks maps::map() which makes ggplot2::map_data() fail..?)
",1526500794.0
,"Sorry, corollary. The size of the list matters. If I only run 10 elements (10 data frames), the NAs will not appear, even if I run the elements which will produce NAs under the larger data set. Running 200 elements produces the problem. It's really strange.",1526328944.0
dm319,Are the types of the corresponding columns all the same prior?  Just wonder if it's a type conversion issue.  Like it assumes it's an integer or factor which causes a problem if really a double?,1526329772.0
,[deleted],1526346254.0
efrique,"Please make a small reproducible example -- a minimal subset of your data if possible

",1526346364.0
coffeecoffeecoffeee,Ooh this looks cool.  Why XML over JSON though?,1526330434.0
DeuceWallaces,Pretty nifty.,1526325050.0
TroyHernandez,Nice work.  Nice write-up.,1526338852.0
Digging_For_Ostrich,"Tyler this is really, really nice.  Great work!",1526402973.0
IMainlineMemes," You can use the line break command

""\n""   

So for the independent variable it would be:

independent = ""independent""

variable = ""variable""

test = paste(independent, '\n' ,variable)

cat(test)


I think having a line break would look better than a wider box. I am unexperienced with ggplot2 you can take a look at: https://stackoverflow.com/questions/20123147/add-line-break-to-axis-labels-and-ticks-in-ggplot/
which might help.
",1526311956.0
onewithbacon,"I know this sub is for R but if you envision path diagrams being a frequent thing in your analysis, it may be worth your while to make your diagrams in Latex. I use ShareLatex for my all my diagrams and SEM models. The Latex learning curve was noticeable for my first model, but the diagrams are visually the best I've seen of any software. Also the online help for Latex code is plentiful ",1526319024.0
oldschoolcool,Would be really interested in seeing the final Shiny product once it's done!,1526328055.0
,"Are you sure the supervisor said multilevel, not [multivariate](https://data.library.virginia.edu/getting-started-with-multivariate-multiple-regression/)? Using a multilevel model does not seem to make much sense here, particularly if you include a random affect for attitude.",1526299025.0
chocolateandcoffee,"The short of the answer is that by including both openness and openness | attitude (which I assume is a multiplication of the two?) your model is running into multicollinearity with the two openness factors. Because it is already accounted for in the multiplied variable, it is saying that openness on it's own is less valuable to the model. The very high level explanation at least.",1526323728.0
backgammon_no,What's A_scoresd?,1526299178.0
Ax3m4n,"Why do you have both and edge from 1 to 2, and from 2 to 1 if you don't want a directed graph?
",1526297227.0
neuro99,"Add ""edge.curved=FALSE"" like this: 

    plot.igraph( g, edge.width=E(g)$weight2,edge.curved=FALSE)",1526297714.0
aftersox,"I'm guessing not all of the edges are drawn as curves? It happens when you have multiple edges between a pair of nodes, such as you have between 2 -> 1 and 1 -> 2.

Try ```as.undirected``` to force it to be undirected, or ```simplify``` to remove multiple edges after you create the igraph object.",1526299073.0
backgammon_no,What do you want to do with it? ,1526288672.0
dm319,Can each owner own multiple buildings?,1526298135.0
RememberToBackupData,"A dataframe column can contain a list, so the intuitive format is a list of owners. 

Another way is to take a database design approach and break buildings and owners into separate sheets that are keyed together. So DF1 is a dataframe where owners get one row each, DF2 is a dataframe where buildings get one row each, and then DF3 is a dataframe where building ownership is recorded as a tidy dataframe (ie. long format data), with each building + owner getting a row. These sheets can all be joined together as needed to actually use them. The final format will be a long dataframe where every building and owner combo is represented on a new row. ",1526299789.0
BroodjeAap,"If you don't have to perform any complex operations on the owners I would make a new column with the concatenated owners, filter with grep.  ",1526293076.0
Darwinmate,"We can't help you unless you give us important information such as what your data looks like, how you are loading it in R, what you have tried. 

We're not here to do your homework. Unless you're paying, then we can do your homework.",1526280579.0
grandzooby,"If you already know how to use the tables about the normal distribution in the back of your statistics book, then this blog might be helpful:

https://www.r-bloggers.com/generating-your-own-normal-distribution-table/

The key R function here is `pnorm`.  This shows the proportions below (or is it above?) Z=0:

    > pnorm(q=0.0, mean = 0.0, sd = 1.0)
    [1] 0.5

",1526284169.0
efrique,type `?pnorm`,1526353557.0
Darwinmate,"They are all in minutes for me.

    id start selfreport time1 DIFF
    1 1 1/31/17 10:00 12.0 2017-01-31 10:00:00 NA
    2 1 1/31/17 10:02 20.0 2017-01-31 10:02:00 2
    3 1 1/31/17 10:45 10.0 2017-01-31 10:45:00 43
    4 2 2/10/17 12:00 0.0 2017-02-10 12:00:00 NA
    5 2 2/10/17 12:20 10.0 2017-02-10 12:20:00 20
    6 2 2/11/17 09:40 37.5 2017-02-11 09:40:00 1280
    7 3 2/11/17 10:00 NA 2017-02-11 10:00:00 NA

What R version are you using?

You should look into using the `Lubridate` package to work with dates. It will make your life easier",1526260956.0
,"Not pretty, but should work: convert `time1`  to integer (=seconds since epoch, UNIX time) and divide  the difference by 60.
```
df$DIFF <- unlist(tapply(as.integer(df$time1), df$id, function(x) c(NA, diff(x) / 60)))
```
2 Edits: incorrect brackets, stupidity",1526285463.0
ohnodingbat,"Try adding seconds to the as.POSIXct format:

df$time1 <- as.POSIXct(df$start,format = ""%m/%d/%y %H:%M:%S"")

",1526269148.0
khaberni,"Naive bayes classifier assumes independence among the input variables. For example, in a spam/not-spam naive bayes classier, the ""naive"" assumption is that the words are independent of each other, but of course that is naive because they are not. This assumption simplifies the mathematics a lot, since the joint probability of two independent events is simply P(A,B) = P(A)P(B)",1526252857.0
T1tanAD,"I was a bit thrown by the one line definition of Bayes' Theorem: ""Probability of an event given that another even has already been occurred"". To me, this is the Probability of A given B, which is technically not the same as Bayes' Theorem. I totally understand where the author is going with this, I just don't know if you can summarize Bayes' law so succinctly.",1526262986.0
the1whowalks,"A good summary, but I was definitely distracted by the phrase “aren’t you agree with me...”",1526224808.0
,"I don't think you should have to memorise ggplot. Once you know how the different layers work and how they're distinct, it becomes easier to work with. [An example](http://rstudio-pubs-static.s3.amazonaws.com/334136_686bd4fb71ca4bf1a239e9031afaf608.html) of a short tutorial. Even if you don't remember the exact parameter set for a particular layer, you can just grab the documentation, eg ?stat_smooth, and see the parameter set that you need.

Templating and building on your own code is normal. You shouldn't feel like you're doing something wrong when you use your own code again, or even code fragments that you don't understand. It's always worth *trying* to understand them, though.",1526195732.0
Darwinmate,"there is a cheatsheet:

https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf

If you're after a GUI tool go use graph pad prism. The whole point of ggplot **is** to program. Also you can create a list object with aesthetic arguments you can import and apply for ever graph if you want them to look consistent. ",1526196701.0
fasnoosh,I think this is what you’re looking for: https://github.com/gertstulp/ggplotgui/blob/master/README.md,1526201065.0
Cronormo,"I don't think most people memorize all of ggplot2's code. You should aim to menorize how the code is structured and with that you can use available documentation to reach what you need. Having some good code to re-use and modify is also a good thing. Save the ggplot2 cheat sheet, bookmark the ggplot2 master list and you have acess to examples of most basic things you'll need. For other stuff....praise google",1526202419.0
space-kitty,"try ggraptR, it's a gui and it outputs the ggplot2 code",1526215152.0
ohnodingbat,One of the more useful tutorials I found is [this](http://tutorials.iq.harvard.edu/R/Rgraphics/Rgraphics.html). I found myself using it as a reference so often I just downloaded the whole RGraphics folder from Harvard (for all the times when I don't have internet access) and have it open in the background. That and the cheat sheet.,1526216878.0
DrPineapple21,"When I teach my students graphics and `ggplot2` I first attempt to teach them the code. There have been some good suggestions for learning the syntax already suggested.

However, after familiarizing students with the code I show them [this](https://github.com/cardiomoon/ggplotAssist) (`ggplotAssist`). It's a `shiny` app that runs as a RStudio addin that walks you through how create a graphic and generates the code for you. You can even browse each layer that you create incrementally.

For what its worth, I still reference the cheat sheet or do a quick Google search quite a bit when I graph (even with 6+ years of working with `ggplot2`). I also wrap my commonly used graphs into custom functions so I can call my standard bar graph with confidence intervals, histograms with overlapped normal distribution, etc. without having to retype or copy/paste code every time I do exploratory analysis.",1526330163.0
actually_a_tree,It depends on the `wdman` package which has been archived because 'check problems were not corrected despite reminders.',1526133540.0
TeslaIsAdorable,You can still get it and it's dependencies from GitHub.,1526147689.0
infrequentaccismus,"I’m not sure I can help, but out of curiosity, why do you want it in this format?",1526130467.0
balanaicker,"20k is tiny btw for R. Just read the whole table in using
 
> data <- read.csv(""your_table.txt"")

filter by

> data <- data[data$column!=0,]

write to disk by,

> write.csv(data,""new_table.txt"")",1526128836.0
dm319,"R doesn't strike me as the best tool for the job here - you're doing text processing rather than numerical computing.  Lets generate some pretend sparse data:

    library(tidyverse)
    size <- 2000000
    
    sparsify <- function(x) {
    	n <- sample(0:3, 1)
    	if (n == 0){
    		x[1:length(x)] <- 0
    	} else {
    		i <- sample(1:length(x), n, replace = FALSE)
    		x[-i] <- 0
    	}
    	return(x)
    }
    
    matrix(data = rnorm(size*10), nrow = size, ncol = 10) %>%
    	apply(1, sparsify) %>%
    	t() %>%
    	write.csv(""data.csv"", row.names = FALSE)

That actually takes a while, but should give you a file with 2 million rows of 10 columns, which is around 85Mb on my system.  Some rows have nothing, others have 1, 2 or 3 random values in it.

So lets write a script in AWK to process this data:

    #!/usr/bin/awk -f
    
    NR > 1 {
    	split($0, a, "","")            #split the first line by commas
    	b = 0
    	for (i = 1; i <= length(a); i++) {
    		if (a[i] != 0) {           #check which values are not 0
    			b++                      #count numbers
    			c[b] = i                 #store index of number
    		}
    	}
    	if (b > 0) {                 #did we have any numbers on this line?
    		printf NR-1                #print line number -1 cos header
    		for (j = 1; j <= b; j++) {
    			printf "" ""c[j]"":""a[c[j]] #print index : value
    		}
    		printf ""\n""                #new line
    	}
    }
    
    END {
    	printf ""\n""
    }


Lets run it by:

    time ./process.awk data.csv > output_awk.txt
    
    real	0m10.151s
    user	0m10.005s
    sys	0m0.108s

So it does take a while, but it's processing a lot of data and produces a 69Mb file.

Let's take u/eaclv 's code to output into another file:

    #! /usr/bin/R -f
    
    m <- read.csv(""./data.csv"")
    
    sink(file = ""output_R.txt"")
    
    for (i in 1:nrow(m)) {
    	x <- m[i, ]
    	j <- which(x != 0)
    	if (length(j)) {
    		cat(i, paste(j, x[j], sep = "":"", collapse = "" ""), ""\n"")
    	} else
    		cat(i, ""\n"")
    }
    
    sink()

And we can run it by:

    time ./process.R data.csv

This kinda killed my computer.  I stopped it at 10 minutes.  By this point it had created a file that was 8.6Mb.  Assuming it was working towards a file size of 69Mb, this was going to take around 80minutes.  The output looks exactly the same apart from the difference in empty lines - the R script will print a line number for lines with all-zeros - the awk script doesn't.  I didn't bother changing that behaviour, but obviously fairly trivial to do so.

So if you have to do this a lot, or you have bigger files, IMHO better to use the right tool for the job - AWK.

EDIT: tried 200'000 rows instead.

    R: 1m25s
    AWK: 0.997s

TL:DR - use AWK :)",1526169001.0
ELKronos,[Try this](https://cran.r-project.org/doc/contrib/Farnsworth-EconometricsInR.pdf) or [this](https://www.amazon.com/Applied-Econometrics-R-Use/dp/0387773169),1526099349.0
Ryo-N7,"hey man,

this https://www.cengage.co.uk/books/9781305270107/ is what I used for my econometrics course back in college. you should be able to find a pdf version of the book if you google it. the data sets  should be in the above link too. ",1526120988.0
caPoliticalEconomist,"It’s unfortunately not R centric, but this free book is up to date and well written:

CAUSAL INFERENCE: THE MIXTAPE
http://scunning.com/cunningham_mixtape.pdf

You could reach out to the author (he is active on the socials) and ask if he has plans to add R.",1526132273.0
biledemon85,"I've been playing around with purrr recently and using it with [list columns in data frames](https://jennybc.github.io/purrr-tutorial/ls13_list-columns.html), based on ideas from a [lecture given by Hadley Wickham](https://www.youtube.com/watch?v=rz3_FDVt9eg). This is just a simple-ish use case demonstration of these ideas that I'm mostly doing for practice but I figured it might be useful for other people like myself who are starting to learn some of the more advanced tidyverse tools.

I hope you enjoy and learn something from it, and of course any feedback would be greatly appreciated.",1526070309.0
Belsaga,"Check this post

http://blog.revolutionanalytics.com/2013/06/plotting-classification-and-regression-trees-with-plotrpart.html",1526068385.0
chiv,"I mostly just use Rpart and tree packages. 

Your decision trees though really depend on a lot of factors of your data. What are you hoping to achieve with another package?

Are you just trying to make them easier to read/more presentable? Are you trying to make better predictions off of them (like with the ipred package for bagging)? Are you looking to use decision tree approaches but in different ways (like with boosting)?

",1526067977.0
mr_matzoball,Rattle if you can manage to install it ,1526090476.0
grasshoppermouse,"http://datastorm-open.github.io/visNetwork/tree.html

",1526226155.0
klo99,MICE is a package I prefer to impute data  [https://www.jstatsoft.org/article/view/v045i03](https://www.jstatsoft.org/article/view/v045i03),1526054442.0
dm319,"This doesn't answer your question, but [rfImpute()](https://www.rdocumentation.org/packages/randomForest/versions/4.6-14/topics/rfImpute) seems to do a pretty nice job for me.",1526052606.0
AlittleMisleading,"http://www.r-bloggers.com/missing-value-treatment/
  
https://www.reddit.com/r/datascience/comments/6rb7gv/general_practice_to_impute_missing_values/ 
   
  
https://stats.stackexchange.com/questions/265079/multiple-imputations-predictive-modeling  
    
  

 
https://www.statcan.gc.ca/pub/12-539-x/2009001/imputation-eng.htm
  
  
  

  
Some links  I found worth reading ",1526054431.0
matthewmerritt,"The VIM package is pretty nice. 

https://cran.r-project.org/web/packages/VIM/VIM.pdf",1526070546.0
rohan36,"You can try bPCA or imp4p packages, which are supposed to be more efficient. Links for comparison between imputation methods :  [https://www.omicsonline.org/open\-access/a\-comparison\-of\-six\-methods\-for\-missing\-data\-imputation\-2155\-6180\-1000224.php?aid=54590](https://www.omicsonline.org/open-access/a-comparison-of-six-methods-for-missing-data-imputation-2155-6180-1000224.php?aid=54590) 

 [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2827407/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2827407/) 

I haven't tried these packages yet, but do let me know if you have any success.",1526132203.0
bubbles212,"[The Bootstrap (pdf)](https://statistics.stanford.edu/sites/default/files/EFS%20NSF%20397.pdf) is a good place to start, it basically uses resampling to get better estimators. I believe there are some R packages available for this (just search ""bootstrap in R"" or something similar). If you do a Google Scholar search for ""data imputation"" you can find more recent methods, like [this for example (pdf)](https://pdfs.semanticscholar.org/f778/54f94c2f849ed93c018662bb3f0a3f8e38d2.pdf). That last paper compares a bunch of different imputation methods in the context of breast cancer research.",1526053295.0
jowen7448,"Just out of curiosity,  why is the python similar post worth $4 more per hour?",1526016159.0
spongebob,"The most valuable R freelance skill in my opinion is being able to advise someone when R is appropriate to their problem, and at the same time being willing and able to tell them when R is inappropriate.  If you *do* determine that R is appropriate, then advise them on the best statistical technique for their particular problem. Ideally at that point you know of someone who has just started with R who is looking for freelance work (and who you trust to implement a solution). Then introduce your client to your colleague for a commission. 

That kind of advice is worth at least triple the rates you mentioned, *and* you get the comission, *and* you're free to move on and advise other clients.  

If your freelancing advice is always ""you need R and here's what to do next and if you pay me I will do it for you"" or ""OK, I can run that function for you"", then someone else will likely be hired to give the advice I described above and you'll be stuck at $30 per hour. 

Nothing against R at all, it's perfect for many situations, but if you're aiming for maximum hourly rates you need to move up the food chain. 

Edit: I have a three year old, and I pay our nanny $30 per hour. ",1526028295.0
I_just_made,"I guess it depends on the end user's goal, but I would freelance Shiny apps.

There is a surprising amount you can do with those, just need to have a clear idea of what you want going in; also, a great methodology for keeping the code simple (using modules, etc).

On a side thought, has anyone done freelance R work? If so, how did you get started and what is some advice you can give?",1526042105.0
tenurestudent,Fuzzy matching and string manipulation ,1526017388.0
mudbot,parse excel files from various sources containing similar data but different layouts to a single data frame.,1526051858.0
dm319,Installing packages.,1526029700.0
DoomChicken69,"check out R Shiny https://shiny.rstudio.com/gallery/

It's interactive and looks a lot like tableau",1525953103.0
Eleventhousand,"You need to provide more info.  What do you use today for enterprise reporting, BI and analytics?  Or are you in a different job role and trying to add that to your responsibilities?",1525954227.0
azdatasci,"I use RStudio exactly for this. It's way more flexible than Tableau in my opinion and you can create any type of custom report using RMarkdown (PDFs) and Shiny for interactive dashboards. A couple years ago my company got some licenses of Tableau with the intention on allowing end-users (business folks) to create their own dashboards (and thus alleviate some of the requests to our BI team), but is was abandoned. Mostly due to cost and the fact it wasn't really adopted due to the challenges of getting certain data sets available in Tableau (and internal issue) and it was still technical enough that the business users didn't want to spend time doing that - or didn't have enough statistical or analytical knowledge to create meaningful dashboards or reports. 

While R/RStudio/RMarkdown/Shiny are going to need technical expertise, I believe it provides enough flexibility to create any type of report that the business or leadership wants. What I did was pitch my case and tell them I could create what they wanted with it and would do it on my own time to show the possibilities. Out of that I created some nice dashboards that allowed them to create a PDF report with the click of a button. They were impressed enough, we made it an enterprise offering and there are many areas in IT using it now to produce reports and perform data science products with it. We even have one group doing a lot of machine learning and other things that have provided tremendous IT and Business Value. 

I use the open source / community version, but there is talk of getting the commercial server version now that they see what it can do. I would say, pick a project, do a proof of concept, get that product to the right people. They will like it. Then tell them that these are the tools you need. ",1525975919.0
H4CKTHEPL4NET,"I’m interested in this discussion. RStudio connect seems to be a good reporting option, but I’ve never used it",1525951880.0
TeslaIsAdorable,"Sap4hana comes with r bindings. There's also an RSAP package that I just found yesterday. 

I used to work for a place that used SAP. I got the IT guy to get me HR data, and predicted which employees were likely to leave the company. I did analyses of compensation rates to determine compliance with the equal pay act. I also set up a bunch of ""dashboards"" in Shiny to save time generating reports on a regular basis... The applets just generate the report on the fly. ",1525955103.0
ilcapotasto,Saw some R+Microsoft Power BI crazyness which looked cool AF!,1525961522.0
economicurtis,"Ahoy - in terms of materials that might help on RStudio: 

- If you have existing R users in your organization, rstudio's pro products just make those data scientist's lives, and the lives of the IT staff supporting them, much easier. 
- [RStudio Server Pro](https://www.rstudio.com/products/rstudio-server-pro/) is rstudio server, with integration into most authentication methods, with admin tools for load balancing and scaling to many users. And has a few nice features so multiple people can collaborate in the same project at the same time. You also get support from [RStudio solutions engineering](http://solutions.rstudio.com/). 
- RStudio Server Pro comes with [RStudio Professional Drivers](https://www.rstudio.com/products/drivers/), ""RStudio Professional Drivers are ODBC data connectors that help you connect to some of the most popular databases."" 
- [RStudio Connect](https://www.rstudio.com/products/connect/) is a handy tool to automate scripts, deploy R Markdown docs, API using plummer, and shiny apps. It has nice features to automatically share that output with other people in your organization. And you can integrate that output into internal dashboards and newsfeeds. You could probably work around this with FOSS tools, but this saves a lot of time and makes everyone's lives easier.
- You might schedule a chat with a salesperson at rstudio as well; https://www.rstudio.com/products/rstudio-server-pro/. 
- I happen to work for RStudio. ",1525963270.0
cyran22,"For those that want enterprise features for Shiny, check out [Shiny Proxy](https://www.shinyproxy.io/)",1525988023.0
KopfJ4ger,Huh? They're both free...,1525952881.0
Spartyon,"They offer various levels of service for Shiny, i think the most expensive is about 20,000 per year, while the other options are between 4,000 and 10,000. Check here: https://www.rstudio.com/pricing/#ShinyProPricing",1525961180.0
goddammitbutters,"Check here:

https://stats.stackexchange.com/questions/5135/interpretation-of-rs-lm-output

First column is the regression coefficients, second column is their standard error, third column is the first column divided by the second column, and the last column is the coefficients' p value (these guys are < 2 * 10^(-16), i.e. close to zero)",1525938042.0
master_innovator,Education is positively correlated with earnings.,1525951037.0
DaedalusPuddlejumper,"Thanks for sharing! I'm curious to hear what NLP packages you have found useful. I have started to do more text mining and have been exploring different options for parts of speech tagging, abbreviation cleaning, and general text cleaning. I'm mainly an R user too but it seemed Python might have some more heavy duty options (Parsey McParseface?) so it's great to hear about your experience with reticulate!",1525969252.0
ELKronos,"[There are better ways to handle missing data IMO, but here is one package which has that](https://www.personality-project.org/r/html/corFiml.html)",1525912125.0
srsly_sam,Pretty sure you can do this in OpenMX.,1525916094.0
Slabs,Lavaan.,1526225559.0
abresler,"This is the best comprehensive r/data s ience/statics website I've found in sometome and I am a hunter for these sorts of things.

Brad is pretty good about doing blog posts covering more advanced topics as well.

It is a must bookmark.",1525870637.0
edimaudo,The mods should add this to the sidebar.,1525877916.0
oldschoolcool,"Honestly, what do you get by spamming these tutorial websites all over the analytic subreddits? Is it just as revenue? Is it really that much? ",1525899818.0
infrequentaccismus,Looks like your zomato dataset is a vector instead of a dataframe. ,1525873555.0
RememberToBackupData,Learning github added a good 4-6 hours to writing my first package. It's weird that so much can be written about the topic and yet it's still so mystifying.,1525830367.0
wafflegraphs,Nice! Thanks! About to start writing my first one in a few days. Pretty excited.,1525827954.0
tylermw8,Now get one on the CRAN for another extensive learning experience!,1525829490.0
ThisOnesForZK,"I do not know how to do this but am really wanting to find a mentor on Supply Chain/Purchasing analytics in R.

Are you someone that would be interested in that? Alternatively, do you know of good sources to find these kinds of analyses?",1525968628.0
undernutbutthut,Anyone?,1525907246.0
wouldeye,I’ve read this article and seen it around a bunch but I have no idea what it actually means for us in the future?,1525842821.0
dm319,"> In addition to improving R and Python, the group hopes its work will also improve the user experience in other open-source programming languages like Java and Julia.

I understand Julia, but Java?  I didn't think it had much to do with numerical computing.",1525815727.0
mattindustries,"When I jumped in from knowing some other languages, [this was a good resource](https://learnxinyminutes.com/docs/r/) for me. ",1525792812.0
MicoudLeChef,"I only really started to learn R when I stopped trying to translate Stata commands into R code. The underlying logic is just too different. 

That being said, if you start out by learning the syntax from tidyverse packages such as dplyr you will probably have an easier time, since they are - akin to Stata - verb-based  (generate / replace becomes mutate, keep becomes select and so on).

Good luck, it is very much worth the effort learning R!",1525797423.0
DoomChicken69,"I started with R, then switched to Stata for years and years, and when I wanted to go back to R, was rusty. This helped: https://dss.princeton.edu/training/RStata.pdf

It's frustrating because you think ""In Stata, I could do this by by typing this one line of code, but in R, it's a pain in the ass to approximate what I want"" but then that passes once you rack up more experience in R.",1525809553.0
MeanMrMustard92,"The 'R for Stata Users' book tries to teach you how to do stuff in base R, which you should avoid whenever possible. The tidyverse family of packages allows you to do everything you'd do in terms of data munging and more in stata. [Here](http://www.matthieugomez.com/statar/) is a good cheatsheet for that sort of thing. 

As for doing econometrics and estimation beyond linear models (with robust standard errors, which R does not do automatically, you should look into [estimatr](http://estimatr.declaredesign.org/) for robust linear models and instrumental variables, and [lfe](https://cran.r-project.org/web/packages/lfe/lfe.pdf) for fixed effects / GMM / clustered standard errors etc. [stargazer](https://cran.r-project.org/web/packages/stargazer/vignettes/stargazer.pdf) does latex/html exports as well if not better than `estout`.",1525810403.0
,[deleted],1525793128.0
efrique,"Sometimes, just asking how to do something is a good approach.

But if you want to avoid spamming lots of questions, start by searching stackoverflow with the `[r]` tag (plus whatever you're looking for)

Many thousands of R questions are answered there (237000 questions on R, 197000 with answers)",1525825494.0
questionquality,Any reason why `nlme::getVarCov(glss)` isn't working for you? ,1525863139.0
,"I'm not clear on how one is supposed to install this package. It says it is available from git but then the install code references ggplot2, not paletteR. Anyone know?",1525790329.0
bubbles212,"Are you using probability distributions for your model? You should be able to write a giant ""for"" loop and use R's built in functions if your distributions are from the common families (e.g. Normal, Poisson, Exponential, etc.).",1525738119.0
PM_ME_QUOTE,"Interesting, I was writing a report about time series simulation and this pop up. 

I came across a book called Simulation for Data Science with R, interesting idea in there.",1525768631.0
TroyHernandez,"In the time it takes you to find a library, learn its functions, and write that code, you could write your own very simple script using base R... as /u/bubbles212 suggested.",1525792691.0
dtrillaa,Maybe [this](http://r.adu.org.za/web/packages/CellularAutomaton/CellularAutomaton.pdf)  is what you’re looking for.,1525739883.0
Fireflite,A very simple approach: dissimilarity = 9-similarity,1525721404.0
shujaa-g,"Realize that you don't need 130.000 elements. Sample, aggregate, or reduce the number of elements some other way. It's hard to give advice without seeing even a sample image, but your plots probably don't even have 130k pixels, and your users certainly aren't tracking that much information. 

Also, minor pet peeve: the ""gg"" in ""ggplot"" should always be lower case, even in a title.",1525706623.0
p_hacker,I'd get it running well with doParallel and then migrate it to an EC2 instance on AWS. You can rent an instance with 64 cores and 256GB memory for not too much.,1525720642.0
mudbot,"If there is no particular reason to believe there is an outlier (detector defect, etc) than don´t classify a point as such. And certainly do not throw out data based on an outlier test. What you should do is rethink your model.",1525724368.0
AlpLyr,Subheaders are normally not a part of regular `data.frame`'s; what package are you using?,1525633558.0
AnInquiringMind,These look like labels out of the Hmisc package. You should be able to get them out by using the Hmisc::label() function.,1525636221.0
klo99,"You can definitely publish at https://datascienceplus.com and take advantage of the established audience (traffic) and simplicity of submission and publication process. 
If your projects are written in R Markdown you push directly to DataScience+; read how to: https://datascienceplus.com/posting-from-r-markdown-to-datascience/

[edited to add more info]",1525622190.0
infrequentaccismus,Please let us know where you post it. I’d love to see it!  Thanks!,1525662986.0
Elesday,"Maybe think about a blog using blogdown and netlify. Easy, quick and totally free !",1525632148.0
cavedave,I've talked to the main curator. They are very friendly and encouraging. If you've something cool to show send them your idea?,1525635197.0
ConCrim,"Maybe down the road, you should start a blog. Certainly a demand for what you're doing. ",1525623413.0
ElaboratedMistakes,If you have a blog you follow and that you think is relevant you could ask the author if you can make a guest post. Also gives you more audience.,1525635286.0
yaymayhun,You can publish on Rpubs.com,1525662218.0
brews,"Why not write a blog, yo?",1525628051.0
_Wintermute,"Or avoiding dplyr, you can just use `aggregate`.

    df <- data.frame(A = c(1, 1, 2, 3, 3), B = c(2, 3, 3, 5, 6))
    aggregate(B ~ ., df, mean)",1525612320.0
efrique,"Here's one way (of many) --

Set up some data:

    > yourdf
      c1 c2
    1  1  1
    2  2  2
    3  3  3
    4  2  4
    

    res <- with(yourdf,tapply(c2,c1,mean))
    outdf <- data.frame(d1=as.numeric(names(res)), d2=res)

    > outdf
      d1 d2
    1  1  1
    2  2  3
    3  3  3",1525614849.0
CrzySquirrel,"Edit: my bad... I thought this was the Python chat: 

R answer: 

require(dplyr)    
df <- data.frame(A = c(1, 1, 2, 3, 3), B = c(2, 3, 3, 5, 6))
df %>% group_by(A) %>% summarise(B = mean(B))

Python answer: 

Pandas has a group by function that will do this for you.  

Please use something like this sudo code: 

df.groupby(['col1']).mean()

Docs here: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html",1525610643.0
greenspans,I've reviewed your results but I'm not seeing Nicolas Cage in any of your research. You forgot to adjust for outliers,1525582895.0
accountsyayable,"You're running a large number of significance tests.  I would try using some kind of error rate controlling procedure (Benjamini-Hochberg, etc).",1525591718.0
keepitrealcodes,"So if you have ""2; 1"" and ""1; 2"" as separate entries in an edgelist, I'd imagine R is seeing that as a directed network. Have you tried using igraph's [as.undirected\(\)](http://cneurocvs.rmki.kfki.hu/igraph/doc/R/as.directed.html) function?",1525571905.0
singularperturbation,"If I'm understanding right, it's in ""edgelist"" format.

You can use [read_graph](http://igraph.org/r/doc/read_graph.html) to read in the data if it's written out to a file, or [graph_from_edgelist](http://igraph.org/r/doc/graph_from_edgelist.html) if you convert those two columns of the data frame to a matrix.

Then, you just have to specify that the graph is undirected:

    read_graph(""input_graph_edgelist.txt"", format = ""edgelist"", directed = FALSE)

    # Or
    mat = as.matrix(df[, c(1,2)])
    undir_graph = graph_from_edgelist(mat, directed = FALSE)

Or, you can convert after the fact, like /u/keepitrealcodes describes.",1525574008.0
mattindustries,If you show what you have so far it might be easier to help. You can bind mouseover events to mouseclick events if that is what you are trying to do. https://gis.stackexchange.com/questions/31951/how-to-show-a-popup-on-mouse-over-not-on-click,1525562436.0
oskonen,Your problem has nothing to do with processor.,1525521560.0
iamdelf,Windows or Linux?  I've used it just fine on AMD CPUs and Intel under Linux.  ,1525489667.0
scoetzee,"I have `foreach()` working on Windows with a Ryzen 1700 what issues are you having?

I am working with both mran, and R installed from conda with mkl.",1525498585.0
dm319,"might be easier to create the dataframe with the 1's in it, but I'm guessing that's not what you want so you could do this:

    take <- sample(0:9, dim(x)[1], replace = TRUE)
    
    for (i in 1:length(take)) {
    	x[i, toString(take[i])] <- 1
    }

EDIT: Removed one that I thought should work with mutate_at(), but it doesn't.

EDIT 2: It'd be nice if someone could provide a non-looped answer, but I'm struggling here.",1525474051.0
dm319,"Ok, second attempt - I realise that your 'x' turns out to be characters, which was confusing me.  Here's another way to get to your dataframe, but with integers:

    #create variables
    id <- 1:100
    variable <- letters
    variable2 <- 0:9
    
    #create array, melt and recast into dataframe
    array(0, dim = c(100, 26, 10), dimnames = list(""id"" = id, ""variable"" = variable, ""variable2"" = variable2)) %>%
    	melt() %>%
    	dcast(id + variable ~ variable2) -> x

Then I have a function that flips one of a vector and use the apply command to go row-wise down your data.  It seems to return the array transposed, hence the t().
    
    #function that takes vector of ints and flips one
    flip <- function(x) {
    	x[sample(1:length(x), 1)] <- 1
    	return(as.integer(x))
    }
    
    #take the columns of interest and pass them to apply then recombine
    x %>%
    	select(sapply(variable2,toString)) %>%
    	apply(1,flip) %>%
    	t() %>%
    	cbind(x[,c(""id"",""variable"")], .) %>%
    	head()

EDIT: I now see why your 'x' has everything in characters.  The 'gather' command is dealing with several factors - it can't assume that they represent the same values, so they are all converted to strings.  I'd also say best to avoid using underscores in names to represent two variables - just use another dataframe column.  You can get some nasty errors doing this if any of your variables have underscores in them.  Just use rep() if you need to repeat variable names in columns, or use a multidimensional array, above.",1525532302.0
dm319,"Ok this is getting ridiculous.  Third try.  It was irritating me that I don't have a good functional way of solving this one.  Using `apply()` and then attempting to splice your data back in is just not right.  I was trying to find a solution along the lines of `group_by()` then `summarise`, but it's simplistic in that it must summarise the data.  I came across this [very useful article](http://stat545.com/block024_group-nest-split-map.html) which walks you through more powerful methods of functional numerical computing, ending with the split-apply-combine paradigm.

To get started, we still need reshape2 as the `melt()` command creates the data quickly from a multidimensional array to dataframe, which doesn't seem possible with `gather()`.

    library(reshape2)
    library(tidyverse)

Then we need a function that works on a dataframe, rather than a vector, and returns a dataframe.
    
    flipdf <- function(x) {
    	x$value[sample(1:length(x$value), 1)] <- 1
    	return(x)
    }

So here goes:
    
    array(0,
    	   dim = c(100, 26, 10),
    	   dimnames = list(""id"" = id, ""variable"" = variable, ""variable2"" = variable2)) %>%
    	melt() %>%
    	group_by(id, variable) %>%           #this groups everything except the 0:9 variable
    	nest() %>%                           #this is the split - we now have a dataframe with a column of dataframes
    	mutate(data = map(data, flipdf)) %>% #we use map to apply our function and mutate the column of dataframes
    	unnest() %>%                         #recombine it
    	spread(variable2, value) %>%         #can use spread or dcast to return to an untidy layout
    	head()

I suspect that no-one is looking at this, but I'm glad to have found a solution that is functional, and it has been a personal voyage of discovery, which included several dead-ends (looking at you `sweep()`, `summarise()` and `mutate_at()`).  To be honest the loop is the simplest way of doing this, but if you want to do perform something more sophisticated, the split-apply-recombine paradigm is the most powerful.  Maybe it will help someone else who ventures here.",1525901013.0
dm319,"Here's a fourth way.  Might be the most elegant so far, it's functional, but it isn't a 'tidy' approach.  You can use `apply()` directly on a multidimensional array, and this might be one of the few times when it's better to do this.  You still need the tidyverse and reshape2 (which is more suited to converting between arrays and dataframes).

    library(tidyverse)
    library(reshape2)

And you still need the `flip()` function.
    
    flip <- function(x) {
    	x[sample(1:length(x), 1)] <- 1
    	return(as.integer(x))
    }

I've re-created the data using the multidimensional method, and left it as a 3D array.
    
    array(0,
    			dim = c(100, 26, 10),
    			dimnames = list(""id"" = id, ""variable"" = variable, ""variable2"" = variable2)) %>%
    apply(c(1,2),flip) %>% #this walks along dimensions 1 and 2 applying the function
    melt() %>%             #to the vector found in dimension 3
    dcast(id + variable ~ Var1) %>% #re-cast it and it's done
    head()

I think I'll stop it here.  Just to summarise the methods:

1. Loop.  The simplest way, but the most frowned-upon for R.  Actually, there is nothing wrong with a loop, and a lot of numerical computing is done in a loop in other languages.  However, loops are slow in R, and there is a risk of messing up if you get your parameters wrong.
2. Apply on a 2D array.  This is a sensible method, but it's awkward.  The reason it's awkward is because you want to operate on a series of vectors.  These should be arranged as a column to work the 'tidy' way, in order that you keep other columns for your variables.  Using `apply()` here drops your other variables and variable names.
3. Split-apply-combine.  This way you can keep your variables levels and names, and still operate on one column of vectors/dataframes using mutate.  This is the most 'tidy' and functional way of doing it.
4. Apply on multidimensional array.  This isn't a tidy way of doing it, but it is probably the most elegant one.  Because you have that extra dimension, you hang on to your variable levels (but not the names).",1525944543.0
Darwinmate,"Let's say the fastq files are in the same dir (this makes it easier) and the names are in a list:
    
    fqlist <- c(""1.fastq"", ""2.fastq"", ""3.fastq"", ""4.fastq"")
    #alternative automated way:
    fqlist <- list.files(getwd()) #getwd for current dir

###using `lapply`

Create a function, it's your essentially your code:

    qPlot_fq <- function(fastq) {
      s.fastq <- readSeqFile(fastq, hash=FALSE, kmer=FALSE)
      tiff(paste(fastq, "".tiff""), width = 7, height = 5, units = 'in', res = 300)
      qualPlot(s.fastq)
      dev.off()
    }

Then apply the function to your list using `lapply`:

    lapply(fqlist, qPlot_fq)

That should do the trick.

`for` loop is also doable but more finicky. In R, they're annoying if you try to use them as name variables or objects. ",1525477726.0
sohaibhasan1,"Generally either one is fine. I tend to go with for loops, probably more often than I should, because I find them easier to read, especially after some time has past since I last looked at the piece of code.

The one thing that can be tricky is that the iterator in a for loop is not localized to the environment in which it is created. This can lead to problems if your loop is inside of a function. This is solved for by the local() function, but that just makes it trickier to read again. 

I think, in your case, one is as good as another.",1525476991.0
murgs,"Not sure why you nest data.frames. A list of data.frames makes sense. A data.frame of data.frames less so.

Anyway, if the issue also exists running on the console then RAM is the issue and you should look at packages like data.table et cetera that keep stuff on the disc. If the console doesn't have issues, then it is Rstudio and there might be little you can do except not create large nested data.frames like that...",1525471199.0
sparkplug49,"How wide are your data frames? For a couple of versions of rstudio now using view() on wide data frames takes forever (and occasionally crashes) where viewing the same data after a gather() is no problem at all. I don't have a fix for you, I've taken to converting to tibbles and printing to the console or just converting to long format when possible.",1525531353.0
efrique,"`plot` is generic; what function is actually called (`plot.<whatever>`) depends on the class(es) of the object you supply to it. It's difficult to answer the question well without that information. 

What you want to do will almost certainly be doable but the best ways to do it depends on what plot particular function we're even talking about. 


",1525478112.0
vonkrumholz,"Best thing to do would be to feed `plot()` a data frame with just the variables you want to visualize. I use `ggplot()` because the language has advanced far beyond Base R functionality. You can pass the slimmed down data set as an ""anonymous data frame"" (a) or store it as a separate object (b) e.g. 


    library(ggplot2) # for plotting purposes
    library(dplyr) # to use filter function 

    # fake data since you didn't provide any
    mydata = data.frame(pred1 = c(""a"", ""b"", ""c""), pred2 = c(""d"", ""f"", ""f""), response = c(1,2,3))

    # filter to desired data IN the function i.e. the ""anonymous data frame""
    qplot(x = pred1, y = response, color = pred2, data = dplyr::filter(mydata, pred1 == ""c"" & pred2 == ""d""))

Alternative approach with stored object: 

    mydata_subset = dplyr::filter(mydata, pred1 == ""c"" & pred2 == ""d"")

    qplot(x = pred1, y = response, color = pred2, data = mydata_subset)",1525539579.0
millsGT49,"Use purrr::reduce, it will apply a function to each element of a list but instead of returning each result will keep returning an accumulated value. So you can merge the first two elements in your list, then merge that result with the 3rd, then that result with the 4th and so on. ",1525445111.0
MrSquat,"do.call(merge, listofdataframes)



",1525451339.0
GildedFuchs,lapply with a call to merge on the target column should do the trick. ,1525444329.0
Ordzhonikidze,"    reduce(df_list, full_join, by = ""your_key"")    

Using both purrr and dplyr. Assuming your key variable has the same name across the data frames. If not: 

    reduce(df_list, full_join, by = c(""key1"" = ""key2""))  ",1525501700.0
LegacyV1,Bind_rows() from tidyverse is your most efficient method,1525461589.0
samclifford,"You could use map with the gather function to put all the data frames in key-value format, where date and variable name form the key, bind them all together vertically with bind_rows, and then spread the result back to wide format with date as the key. ",1525444395.0
muchunu,"Use data.table::rbindlist(listOfDataframes)
Believe there's a similar dplyr function.",1525526544.0
FlyMyPretty,"Can you post your commands?

This can happen when you have missing data and do pairwise deletion.

",1525411665.0
PSJupiter2,"I apologize,

I did have a thorough question, but it must have gotten deleted:

When highlighting text in a formula, in Rstudio \(and other text editing programs\) duplicate words are highlighted or outlined \(as seen in pic\).

**What is this called?**  

Also, is there a keyboard shortcut to take advantage of these highlights.

In my example, i want to replace ""Huber"" with ""Smith"" in all outlined cases at one time.

Sorry for the confusion on the post.

Thanks!",1525364482.0
YepYepYepYepYepUhHuh,Could you provide a comment to give a bit more information about what you're asking for?,1525362427.0
millsGT49,"I don't know of a way to quickly switch your operations in-line. If you turn your operation into a function you can call it on any data frame and that can save you a lot of headaches like this. 

    mean_med_diff <- function(df){
      mean(abs(df$SaleRatioSMV - median(df$SaleRatioSMV))) / median(df$SaleRatioSMV)) * 100
    }

Then you can call 

    mean_med_diff(Huber)

Or whatever you dataframe is called. ",1525362786.0
VPMACH,This is making matching names in files without proper keys so much easier than it used to be.,1525360669.0
coffeecoffeecoffeee,This package is going to save me so much time.  Especially as I keep working with more lat/lng data.,1525368072.0
openclosure,"Damn, I literally spent like 2 hours today implementing a fuzzy match using the stringdist package. Tomorrow I'm telling my boss I need to be browsing /r/rstats at work. ",1525384736.0
flatus_maximus_,"Distance join looks amazing. I'm accustomed to fuzzy name matches including a certain percentage of errors, but adding additional columns could probably reduce those quite a bit.

    distance_join - Join two tables based on a distance metric of one or more columns

    Description
    This differs from difference_join in that it considers all of the columns together when computing distance. This allows it to use metrics such as Euclidean or Manhattan that depend on multiple columns.",1525374703.0
jimbean66,And a genome join function!,1525439732.0
spectrum_specter,"Loops! Be careful though, this part of R/coding in general can be a bit rabbit hole.  

df$Remaining_Balance  
Date1 = 01/01/2018  
for (i in 1:length(df$outstanding balance)){  
if (df$final_repayment_date[i]>Date1){  
df$Remaining_Balance = df$outstanding_balance[i] - df$annual_payment[i]*1  
}  
else{  
0  
}  

It's definitely not exactly right as it's hard to write things exact without the data, but something like that should work!",1525354253.0
,[deleted],1525360188.0
Opitmus_Prime,"There is no direct way but you can trick the data to align to the rows

Align the columns by inserting ""0"" in the first place of D$1
resultant column will have offset of 1 space


    df$D_mirror = c(0,df$D[nrow(df)-1])
    
That way now you will be working with all numbers in row 2 at a time


    D2: =if(D$1<$C2, ($A2-($B2*1)),0)


    Will change to 


    D2: =if(D$2<$C2, ($A2-($B2*1)),0)



Now use ifelse statement to utilize your formula


    df$D_updated <- ifelse(df$D_mirror<df$C, (df$A-(df$B*1)),0)
    
",1525360561.0
dm319,"This is a little bit *meta*, but bear in mind that dataframes or arrays in R are not spreadsheets.  Be wary of attempting to recreate how a spreadsheet *looks*, because that is generally not how R *works*.  It doesn't help if you have an assignment and that's what they want, but generally it's better to consider what final results you actually want, and then try to compute it using a dataframe paradigm.",1525962394.0
coip,"Is this the same QCA that was developed by Charles Ragin? 

I haven't used any of these packages (I didn't know there were QCA packages in R until your post--so thanks for that), but have done both crisp and fuzzy QCA in the past using [the standalone fsQCA software](http://www.u.arizona.edu/~cragin/fsQCA/software.shtml). In that vein, it looks like the QCA and QCA3 packages you linked to look promising and I'm definitely interested in checking them out.",1525353451.0
5Terre,"QCA or QCA3 should work. But note that, unlike the others, the QCAfalsepositive procedure doesn't do QCA estimation—it runs a test to determine whether your apparently-good QCA results are actually bogus. QCA's aggregation algorithm can produce artificially good results, and the procedures in QCAfalsepositive are designed to protect against that. See the original article (doi:10.1093/pan/mpv017) for more details.",1527284882.0
Grantmitch1,"SetMethods is a sound package. If you want a bit of support with QCA in R, I would highly recommend Adrian Dusa's upcoming book QCA with R: https://www.amazon.co.uk/QCA-Comprehensive-Resource-Adrian-Du&#37;C5&#37;9Fa/dp/3319756672",1530447495.0
edimaudo,Where is the link?,1525295613.0
scbagley,Try: `options(pillar.sigfig = 6)`,1525313146.0
,[deleted],1525286885.0
ColorsMayInTimeFade,"You could add an alias to your R profile.

    lenght <- function(...) {message(""'lenght' is not a function.\nDid you mean this?\n\tlength""); length(...)}",1525282767.0
doomgasp,I see why Excel shortens it to Len(),1525281905.0
penthiseleia,and otherwise it's widht(),1525282659.0
Ader_anhilator,What about sumamry?,1525281985.0
reddit_tothe_rescue,My nemesis function is `unqiue`,1525322408.0
dtrillaa,I never spell it correctly the first time,1525281000.0
triple_dee,I think RStudio has really helped me not get these errors as often,1525300596.0
efrique,"Now write a function called ""lenght"". That'll confuse 'em.

When I did my PhD I was consistently writing ""mutlinomial"" instead of ""multinomial"". (I knew how to spell it, it's a typo of teh same kind as ""teh"" instead of ""the"" -- which I do all the time.)

[My PhD supervisor (this is long ago now) used some of my functions. Instead of pointing out that error, he simply wrote a new function (with it spelled correctly) and called my incorrectly spelled  one (the function itself was good). I went back and fixed it (and removed his function), but I was tempted to leave it as he had, so that it didn't matter if I mistyped it again.]
",1525316988.0
szyy,I thought only I don't learn from my own mistakes ;) ,1525292488.0
ct0,Im always confusing glimpse and glance. Problem is glance will break the session!,1525460112.0
joelthelion,Python solves this issue very elegantly :),1525283279.0
taffyowner,This is the reason I started naming my data by single letters ,1525288945.0
zdk,"Start with Aitchison's 'Concise Guide'
http://www.leg.ufpr.br/lib/exe/fetch.php/pessoais:abtmartins:a_concise_guide_to_compositional_data_analysis.pdf

Most of the examples are motivated by geochemistry.

The best book after that is Theory & Applications, which contains more modern applications, plenty of R code and post-Aitchison theoretical developments https://www.wiley.com/en-us/Compositional+Data+Analysis%3A+Theory+and+Applications-p-9780470711354",1525267722.0
likenzombie,are you sure you don't have 4 levels for the IV C in your actual data where you're getting the issue?,1525257577.0
Darwinmate,"In your example, I am not seeing ""A:B2:C1"" and ""A:B3:C1"" \(edit: the reason I don't see this is because B1 is the intercept in my output, so this makes sense\). This is what I get:

    (Intercept) -0.05339    0.12790  -0.417
    A           -0.25464    0.14589  -1.746
    B2          -0.13303    0.19560  -0.680
    B3           0.03173    0.18777   0.169
    C2          -0.01114    0.17719  -0.063
    C3          -0.11125    0.18003  -0.618
    A:B2         0.28785    0.21668   1.328
    A:B3         0.23655    0.22490   1.052
    A:C2         0.42265    0.19782   2.136
    A:C3         0.21830    0.19677   1.109
    B2:C2        0.01959    0.27247   0.072
    B3:C2       -0.08620    0.25810  -0.334
    B2:C3        0.66935    0.27017   2.478
    B3:C3        0.23492    0.26142   0.899
    A:B2:C2     -0.25991    0.29605  -0.878
    A:B3:C2     -0.14241    0.28979  -0.491
    A:B2:C3     -0.20583    0.28014  -0.735
    A:B3:C3     -0.27039    0.29106  -0.929

I actually dont understand what the issue is. The interaction seems to have the correct output. Why would you not expect `A:B2:C2` `A:B3:C2` etc?",1525222479.0
,I'm confused. Why would you not expect your three way interaction to include all levels of C?,1525245228.0
umib0zu,"I'm not sure what the issue is. Does the package say that's how it handles multiple categorical predictors? In your first case, you have a single categorical predictor, so it's fine to make a reference level because mathematically you would have an overdetermined system (any third level is just ~(L1 + L2) ). For the two categorical case, it would make sense to see interactions between all levels and the second category's levels, so I wouldn't want to have reference encoding.

I just think everything is fine here and you just need a few more cups of coffee OP.",1525266763.0
Statman12,"I'm not sure we can diagnose your problem based on an example that does not replicate it. Can you post the results of `str( your_data )` ? That may provide some useful information to start digging into what might be wrong.

If you are concerned about confidentiality, set the variable names to the same generic A, B, C, etc that you used in the example.",1525267601.0
slammaster,"put four spaces before your code to make it look like code:

    > df.toprocess[,1]<- predict.preProcess(preProcValues, as.data.table(df.toprocess[,1])[,1]) 
    Error in predict.preProcess(preProcValues, as.data.table(df.toprocess[, : could not find function ""predict.preProcess"" 
    > df.toprocess[,1]<- caret::predict.preProcess(preProcValues, as.data.table(df.toprocess[,1])[,1]) 
    Error: 'predict.preProcess' is not an exported object from 'namespace:caret' 
    > df.toprocess[,1]<- caret::predict(preProcValues, as.data.table(df.toprocess[,1])[,1]) 
    Error: 'predict' is not an exported object from 'namespace:caret'

There's no function called `predict.preProcess`, there's a function `predict` and a function `preProcess`, as described here: https://www.rdocumentation.org/packages/caret/versions/6.0-79/topics/preProcess

As to why your third command doesn't work? I can't tell, the NAMESPACE file looks like it should be there.  If you load the library with `library(caret)` then it should work fine (the examples from the link above work for me).",1525195672.0
anonemouse2010,"You won't get an F test from this.  You can instead use anova.glm (just call anova on your glm object) and it will look at something analagous.

As for private and control... you should learn how to interpret the output of these models.

I will just imagine you have a model with public, private and a mean.

The fitted model would be

u + beta_public * indicator (public)

if the person is private then indicator (public) = 0 and so the model reduces to 

u.

Thus the reason you don't see private and control is because they were coded as 0 in the dummy variables.  They effect is inside the intercept variable.

I'd be very careful about running a glm when you can't even interpret linear regression coefficients... ",1525189671.0
pictocat,the “Display” function from the arm package is helpful for getting more details with GLM,1525210999.0
k027,"tried setting colour in geom_bar?

i.e.:

    ggplot(mydf, aes(x=x,y=y,fill=fill)) +
      geom_bar(stat=""identity"", fill = mydf$fill)",1525181483.0
seeellayewhy,"You need a fill manual. The `fill` argument to the `aes()` tells it what variable to fill on. When you create a fill manual you tell it how to fill.

    gg  + scale_fill_manual(values=c(""red"", ""orange"", ""yellow"")

Then, in the fill argument you can just map which variable you want to fill on. 

    mydf %>%
    ggplot(aes(x=x,y=y,fill=x))+  # assuming you want each bar a different color
         geom_bar(stat=""identity"") + 
         scale_fill_manual(values=c(""red"", ""orange"", ""yellow"")",1525181540.0
slammaster,"Just a suggestion for these kind of problems - you should keep needed rows, rather than dropping unneeded

    needed<-which(rownames(environment) %in% rownames(species))    
    environment2<-environment[needed,]

If you accidentally run your code twice, it'll throw an error the second time, while this code will always produce the same result even after the data has been sub-setted.",1525175105.0
quick_ben_,You could also merge the two dataframes (and keep only the common rows) - https://stackoverflow.com/questions/1299871/how-to-join-merge-data-frames-inner-outer-left-right,1525146298.0
,"A shorter solution that also makes sure that the rows in `environment` have the same order as those in `species`:
```
environment2 <- environment[rownames(species),]
```
Also, please use backticks to format your R code properly.",1525153511.0
,"If there is a unique identifying column in both data frames, you can use dplyr's left_join (with the first input as the smaller data frame) or inner_join (in either order, inner_join will throw out rows from both frames which don't have common key entries).",1525156934.0
Bruizeman,Use tidyverse and the anti_join function to quickly do what you want,1525187551.0
longshortdogsFTW,"Rmarkdown starts a new session when you knit, so if you are relying on anything in your global environment, it won’t be there. It has to be self-contained document. Not sure if that’s your issue, but throwing it out there!",1525142756.0
bubbles212,"Make sure at least one of the code chunks is importing your data set. As /u/longshortdogsFTW mentioned knitting doesn't use any objects you have saved in your environment. Instead, it uses its own environment affected by any of the code in your chunks.

Another thing that may be affecting it is if you've been using attach() to work with your data set (something that I wouldn't ever recommend). Your code may be calling ""MeanTime"" without referencing its actual data frame, but we can't know without more information about the exact functions you're trying to call.",1525145383.0
Darwinmate,"Can you please format your code properly? 

Can you please post a snip of your code? Have you loaded the packages you are using inside an R chunk?",1525139231.0
RonSconsin,"What do you mean? I’ve downloaded dplyr, lme4, and lmerTest in a chunk",1525140067.0
nkk36,"If you're just looking to practice you can usually find data sets from here:

[https://archive.ics.uci.edu/ml/datasets.html?area=&att=&format=&numAtt=&numIns=&sort=nameUp&task=clu&type=&view=table](https://archive.ics.uci.edu/ml/datasets.html?area=&att=&format=&numAtt=&numIns=&sort=nameUp&task=clu&type=&view=table)",1525138762.0
RememberToBackupData,"`iris` or any other continuous morphological trait data is a pretty good one. Try searching for some data from a field called ""cladistics"".",1525145658.0
cavedave,maybe try /r/datasets they have more datasets then you have had hot dinners,1525176123.0
DoomChicken69,check out the world values survey- it's freely available and easy to download. You can cluster countries by to how they respond on certain questions. You can see which countries group together based on values.,1525182583.0
biledemon85,"The [cluster](https://cran.r-project.org/web/packages/cluster/index.html) package has quite a few datasets built into it along with several different clustering methods that you can use. It's a safe bet that these are intended to be used with clustering algorithms!  

Once it's installed you can access them with cluster::name_of_dataset.

Happy clustering :)",1525293001.0
Ste_ler_2018,I had finished a course on this. That platform had something called as Solve mode where one can work on dataset to solve a business challenge. Let me know if you need it.,1528281370.0
edimaudo,"Have you tried googling ""k means clustering r""",1525138872.0
dkesh,"You can definitely make this work. 

You should begin learning R right away -- it has a very different emphasis than C/C++, with many of the low-level functions like memory management handled for you. The data structures are also a bit different -- you have to get used to the most basic data structure being a vector, not a scalar, and you should familiarize yourself with data frames.

But compared to statistics students who have a lot of experience with R, you'll bring something different to the table: a much stronger background in software development fundamentals. Hopefully, you will have some stats-trained interns working alongside you and you can help each other learn from each other's strengths.",1525128294.0
theonlyonedancing,"Chill, man. You're fine. Having a coding background gives you a solid leg up. It's a functional language and higher level than C and C++ so it's even easier to understand than C and C++. In addition to that, once you feel comfortable enough with R, you can even write your own R packages using C and C++. Many of the (imo) good ones are.

Learn how vectors, lists, and data frames work and what functions to typically use to manipulate them. Lists are (kinda) like JSON. Data frames are (kinda) like Excel spreadsheets or SQL tables. Vectors are basically arrays.

Coursera and edX have courses/course series for R. Example link: https://www.coursera.org/courses?languages=en&query=using+r&userQuery=using+r

Very reasonable to have baseline proficiency within a month if you already program professionally in C. Just give yourself maybe 1-4 daily hours of watching videos and fiddling around with data sets. You can get data sets from Kaggle or just from online courses. There are a few small ones that come by default with R as well. At the very least, you can learn enough to search your own SO answers or find the best R packages.",1525128844.0
yaymayhun,"I think Advanced R (http://adv-r.had.co.nz/) by Hadley Wickham would be a very useful resource for you. The description says:
> It should also be useful for programmers coming to R from other languages, as it explains some of R’s quirks and shows how some parts that seem horrible do have a positive side",1525136902.0
anotherep,"If you are a solid programmer in some language, you can probably learn what you need to learn in R with a month of dedicated time. You may not be fast, but you'll probably be able acquire the familiarity to tackle any problem.

As a first, you'll probably want to use the R Studio IDE instead of basic R, since that is pretty much standard.

Coming from a data analytics perspective, I would start by learning the packages that you will be most likely to use, which is essentially the tidyverse: `dplyr`, `tidyr`, and `ggplot2.` Those will probably be the starting point for any task you need to do in data analytics. The tidyverse also contains a very straight forward way to approach basic data manipulation problems that can look very convoluted in basic R to someone unfamiliar with the language. 

Finally, I probably wouldn't bother with `Rcpp` if you would just be using it to accomplish basic data tasks with C code inside R. If this internship is asking you to use R, it's so you can be on the same page with everyone else that works there. If you are writing C code inside R, then it's not like people using R are going to be able to work with your code unless they also are familiar with C, thus defeating the purpose. This is said with the caveat that I don't know C...",1525128637.0
navidshrimpo,"You'll likely learn the language no problem, but performing data analysis is not simply an exercise to create a bunch of text on the screen that we call code. 

The most important difference between software engineering and analysis, in my opinion, is your objective. Analytics is... an analytical field. I would recommend brushing up on research methods, introductory statistics, and some machine learning concepts. Generally, data analysts work closer with the business unit of an organization. Prepare yourself for what that might entail.",1525189664.0
BWrqboi0,"Just tell everyone you can make their coffee faster by rewriting it in Rcpp and your are done.

Edit: https://cran.r-project.org/web/packages/Rcpp/index.html",1525129392.0
DoomChicken69,"Dude, you got this. If you're dedicated, it should take about a week or so.

The best way to learn is by doing. I'd 
(1). find datasets (many are pre-loaded into R already), 
(2). manipulate/clean the data by merging, sub-setting, and collapsing, maybe with dplyr or tidyr, 
(3). summary stats and tables, 
(4). hypothesis tests like ttests and chi-square, 
(5). modeling (OLS, logistic, simple stuff), 
(6). use the broom package to do more with the model output, and
(7). visualize that data, try out ggplot2, 

Check out ""cheatsheets"" https://www.rstudio.com/resources/cheatsheets/
to get to the point, and 
look into UCLA's stats website (it's kind of old..) for code snippets, output, and interpretation: https://stats.idre.ucla.edu/other/dae/

eta: I'm assuming you have a stats background already. If not, then I'd focus on #1, #2, and #7 above.",1525134967.0
Thaufas,"Check out the [swirlstats](http://swirlstats.com) package tutorial. It teaches you *data science* and *R* as an interactive tutorial that runs right from within *R*.

I had a similar career path as you. There's some good advice in this thread. You need to learn the basic R data structures. The strangest and most flexible is the *list*, which has no real equivalent in C/C++. The *data.frame* is another you'll encounter a lot. Although I love R, my frustration with it is that the syntax looks *C* like, but it has lots of gotchas; check it R's `if`/`else` construct and you'll see what I mean as just one example.

Consider this job a great opportunity. There are many C/C++ developers on the market. Getting data science training will open up far more opportunities for you.",1525179224.0
cyran22,"Yeah, learning R wouldn't be as big of a concern as probably lacking more basic data analysis skills. You should probably stick to learning the basics of data analysis with R rather than just learning how to program in R. I think Hadley's [R for Data Science](http://r4ds.had.co.nz/) will be a gentle introduction to the typical workflow of loading a rectangular data set, summarizing and combining datasets, and exploring the data with visualizations and basic models. Definitely focus on learning these types of concepts and the code to do it rather than spending too much time on just ""coding in R"".",1525241469.0
oskonen,Asking you to pre-learn for a fucking internship? Refuse. That's what internships are for. ,1525131886.0
efrique,"> How fucked am I?

Mildly to badly depending on how quickly you are going to able to pick up a  language rather unlike C and C++. Add that it's somewhat inconsistent/clunky in design and has quite a few ""gotchas"", like silent argument recycling, and some hard-to-realize-what-you-did-wrong ways to shoot yourself in the foot (e.g. with attach).

Don't get me wrong, I like using R, but it has a few learning humps even for someone with several more similar languages already under their belt.

e.g. If you already had both Matlab and Python I'd say you'd probably be fine if you worked hard.

On the other hand, you won't have so many expectations from them that R will break, so you may have less of the ""*R does what??*"" reactions. That will help.

If you want to actually learn R, I would avoid Rcpp initially. Try to use R without leveraging your specific C/C++ abilities (but keep your more general knowledge, the stuff that would apply more generally) and then after you're comfortable with R you will get a lot more benefit out of Rcpp. I say this because using Rcpp to get stuff done will get in the way of learning how *R* works -- and make your R code harder to read and maintain for others. I'd wait quite a bit more than a month to start using Rcpp, to be honest; you want to wait until you'd only be using it to do things that would *naturally* make sense to do in Rcpp. Using it will slow you down, in the same way that trying to learn Japanese by reading English translations of Japanese novels wouldn't really get you there -- you really need to do things in *Japanese* and the best way to do that is immersion.

You need to learn to work with the fact that (almost) everything that does *anything* in R is a function; you'll be used to writing functions, but you need to get used to writing functions where you'd probably expect to do something else; supplying functions as arguments to things is the better part how to get stuff done. You need to learn R's data structures and the functions that work with them -- and the fact that the R way to do things is to work with vectorization rather than loops (you can use loops, and if you know what you're doing they'll be just as fast -- but the ""R"" way to do it will be easier to understand, debug and maintain... and once you're used to working that way, much faster to write.

MOOCs may help depending on your learning style, but either way you need to have a problem to work on, something that will make you do things you wouldn't do otherwise (something that will force you to figure out how to do something a MOOC won't necessarily even get to in a month).





",1525137298.0
kazi1,You can learn it in a weekend - there's a tutorial that covers most of the key bits here: https://jstaf.github.io/r-data-science/,1525129996.0
paerb,Datacamp would be a great place to start.,1525132069.0
2strokes4lyfe,Watch some Hadley Wickham live demos on YouTube!,1525133733.0
jjdonald,"Echoing some of the other responses.  I'd say go for it.  

Knowing R is a bit like knowing LaTex.  It's not an easy language to learn, but it's generally a sign of someone who's committed to the field.  I find R to be especially difficult for folks with a lot of conventional PL experience. There's a lot of unconventional patterns and usage. You can't write R the same way you write other languages (e.g. you generally avoid for-loops in R).  You'll have to unlearn some things :)  

There's a lot of R frameworks and GUI/Ide platforms.  The quality is wildly uneven, but there are several that are amazing.  I generally rely on the following combo of tools/frameworks/libs for 99% of the work I'm doing:

1. Vanilla R itself, directly from CRAN : https://cran.r-project.org/
2. RStudio, a third party project extension : https://www.rstudio.com/
3. Tidyverse, a third party collection of curated libraries : https://www.tidyverse.org/

Tidyverse is really more of a modernization effort for R.  Tidyverse introduces new syntax (the pipe operator : http://style.tidyverse.org/pipes.html), a new datatable format (tibble : http://tibble.tidyverse.org/), and a new plot environment (ggplot http://ggplot2.tidyverse.org/).

Generally, when people say they *love* R, they're talking about tidyverse.  There's thousands of other packages on cran that can provide specialized methods for specific contexts.  You eventually get good at evaluating which ones are good, which ones are outdated, which ones are too idiosyncratic, etc.


",1525152887.0
,">Datacamp maybe?

Datacamp is great. You'll be able to blast through the exercises because you're already a coder.",1525153257.0
sparkysparkyboom,"You're fine. R as a language isn't hard to learn, especially with programming experience. Just work on a few datasets to learn how to think in terms of structuring and organizing data.",1525155210.0
grandzooby,"R is for the most part a procedural language so it shouldn't take long to get up to speed on the basics... a lot of similarities to basic c.

Here are some resources I recommend to my students:

* http://swirlstats.com/students.html (one of my favorites)
* http://tryr.codeschool.com/
* https://www.datacamp.com/courses/free-introduction-to-r
* https://www.stat.berkeley.edu/~spector/Rcourse.pdf
* http://heather.cs.ucdavis.edu/~matloff/probstatbook.html (Norm Matloff)

A couple things that may help:

* indexes are 1-based (not 0-based like you're probably used to in C)
* lists are how you return multiple values from a function
* when you create a function, it won't be completely evaluated until runtime.  So if you create a function that refers to a variable that doesn't exist, you won't get an error until you actually call the function with the variable not existing
* R offers no true constants... just global variables
* students often struggle with when to use one set of brackets `a[...]` or a double set `a[[...]]`  (https://stackoverflow.com/questions/1169456/the-difference-between-and-notations-for-accessing-the-elements-of-a-lis)
* no ""switch""... just nested if/else blocks

On the upside, it's really good at shlepping data around and doing nearly any kind of analysis you can imagine.
",1525157189.0
brews,"Depends on how you learn...

I recommend a copy of ""The Art of R Programming"". It's one of the few resources for coders from other languages. It is a well written book.",1525175840.0
infrequentaccismus,We r users cherish the cpp programmers who join us!!! You will be loved and highly valued. This is a very good move for your career I think. ,1525183036.0
,"```r
results <- t(sapply(thedata, function(x) cor(x[,50], x)))
```",1525116843.0
Darwinmate,"Does this SO post help?

[https://stackoverflow.com/questions/50105760/how\-to\-use\-autoplot\-in\-r\-to\-plot\-rotated\-components](https://stackoverflow.com/questions/50105760/how-to-use-autoplot-in-r-to-plot-rotated-components)

If I'm understanding your question rotation = how the original points are reduced?",1525136936.0
ashwinmalshe,Thanks for pointing it out! I wasn’t aware of this one. ,1525090384.0
10101010101111,"What a nightmare! Knowledge of this bug should be much more widespread, I'd never even heard of it.",1525155715.0
slizb,I wonder if the problem persists when using dplyr via tidyverse...,1525096716.0
dtrillaa,"Not near a computer but I don’t see why

    expand.grid(data$col2[1:2], c(2, 3))

wouldn’t work",1525068158.0
blozenge,"You will need to split your Col2 variable into the sets you want represented in Col2/Col3.


    expand.grid(Col1 = c(""Item1"",""Item2"",""Item3""), Col2 = c(1,2), Col3 = c(2,3))

",1525072555.0
dankwormhole,"It’s difficult to assist you without much info.  Please provide a reproducible example: an except of the data you have, which package(s) you’re using, what code you have written so far, what you are trying to achieve, etc. ",1525056010.0
bubbles212,"Use the dplyr package. Assuming you have a data frame called ""dataset"" with columns ""occupation"" listing each occupation, ""gender"" indicating gender, and ""income"" with the person's income:

    new.dat <-  dataset %>%
      group_by(occupation, gender) %>%
      summarise(avgIncome = mean(income))
        
 This will give you a summary data set (new.dat) with three columns: occupation, gender, and the average income for each group.",1525035930.0
efrique,"For questions like these, the best approach is to create a small example data set (e.g. with 2 or 3 occupations) for people to show you their solutions on

    egdf <- data.frame(
                 occupation=factor(c(""doctor"",""doctor"",""lawyer"",""lawyer"",
                                               ""chicken sexer"",""chicken sexer"")),
                 gender=factor(rep(c(""M"",""F""),3)),
                 income=c(180,170,170,168,80,81)
                )

Is that the kind of data you mean?

",1525088335.0
alasdair_c,"sure.

df%>%
arrange(gender) %>%
group_by (occupation)%>%
summarise (difference=diff (income))

the arrange by gender is so each group has the same order of female then male. then when you take the difference itll be consistent. the diff function takes a vector of length n and returns the difference between consecutive elements, a vector of length n-1. so in this case we have the vectors of length two (each genders income) and the diff function will just return one difference.",1525121317.0
SeveralBritishPeople,"I’m not sure I’m totally following, but could you just nest the right data first on the join columns to make the join 1:1 instead of 1:many? You’d then manipulate the two list columns of data frames, but could pmap a combined function over those columns to avoid expanding the entire data set at once. ",1525036347.0
samclifford,Split the right data set and use an inner_join instead? ,1525033713.0
RememberToBackupData,"    sd(Weather$Rain)

    object 'weather' not found

Are you sure that you're using the correct name? `Weather` and `weather` don't refer to the same object.

If your capitalisation is fine, then the likely cause is that `weather` exists in your R environment, but it **doesn't** exist in your Rmarkdown file. See /u/imdrowning2ohno's [answer](https://www.reddit.com/r/rstats/comments/8fqfwm/r_runs_in_console_but_doesnt_knit_it_all/dy5mmwr/). Rmarkdown always knits from a new session of R, so it doesn't know about anything outside the Rmarkdown document.",1524996209.0
imdrowning2ohno,"Are you sure the line(s) of code where you imported the text file and/or renamed it and processed it are in the R Markdown file? If you imported the file using the button in RStudio, you may have forgotten to put the code in the R Markdown file. Therefore, in the html file, it's like the file never existed, and it has no idea what 'Weather' is. ",1524995935.0
huessy,"Don't do attach(). As people have noted already, Markdown operates sort of as it's own mini R session where you have to explicitly load all packages you're using and create all variables inside the document. Think of the console and markdown as two different sessions.

Also, not sure why you put a ""-"" in front of ""Desktop"". Try it with the full file path and see what happens, like ""C:/Users/You/Desktop..."". That way you can at least be sure that you're asking R to pull the file from it's location since it doesn't think that file exists. ",1525012871.0
bob_arnold,Are you knitting an R Markdown file?,1524995764.0
syrphus,"looks about right. `/etc/apt/sources.list` works. you can also paste the line into its own file in the directory `/etc/apt/sources.list.d/`. just call the file `cran.list`, or something.

after that, just run:

    sudo apt update
    sudo apt upgrade

to upgrade to the latest version of R.

edit:

you might also have to add the key that the packages are signed off with. to do that run:

    sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E084DAB9

before doing the apt update and upgrade step.",1524943123.0
ryapric,"If you have issues for any reason, you can use [a script I wrote](https://www.github.com/ryapric/install_R) that prompts you for various installations of the R ecosystem. On Ubuntu et al, one of the questions asked is whether or not you want the latest version of R. The underlying solution is the same as what /u/syrphus suggested.",1524950642.0
Mooks79,Been a long time since I used Ubuntu so apologies if this is outdated - but I think the issue is that you’re using the standard/default repositories - which only contain relatively heavily tested stuff. If you go into the options then you can tell it to use “multiverse” repositories as well - which contain all the latest stuff.,1524994458.0
cyran22,"when you use `caret::train()` you're looking into the caret package for the `train()` function and it succeeds (according to your edit). You probably want to call `library(caret)` in your Rmarkdown file because anything that goes on in the Rmd file is in a new environment by default. This means you won't have access to any variables in your current working environment and you won't have those packages loaded. 

The error message is confusing but I think it's because without loading the caret package, when you call `train()` you're calling some other train function from another package.",1524897214.0
wdanalytics,"Your R session's environment is not available to Rmd files, so they can use only the packages you load from within the file. The same goes for variables. I tried to use a variable in the global environment in an Rmd file and got an error, but it worked when I created it in the file.",1524901669.0
FlyMyPretty,Could it be a missing library() statement?,1524892355.0
revgizmo,This looks pretty awesome,1524932157.0
TheTrub,"I think there are a few important things to consider here. 

1.  For now you might want to omit the one row that includes status  as for content type.  You only have one observation, so you don't have any degrees of freedom to play with.  Getting rid of it will allow you to have a fully nested model.  


2. Since you are using count data, you'll want to consider using a *generalized* linear model, specifying a poisson distribution (if you're doing raw numbers/counts instead of numbers per 1000) or a gamma distribution (if you're going to be using the non-discreet values you have).  Since count data isn't normally distributed, your residuals aren't going to be normally distributed, so you're violating a critical assumption of general linear models. 


3.  As an extension to #2 and as an answer to your primary question, I would suggest using either the lm or glm functions in the lme4 library.  The script is fairly simple, but they will allow you to fit your data as either linear or generalized linear models.  the formatting should go something like this:


fit1 <- lm(Number.of.Interactions.Per.100~Content.Type*Promoted.Post.Detection, data=top10engagementhigh)


or for the generalized linear model:


fit2 <- glm(Number.of.Interactions.Per.100~Content.Type*Promoted.Post.Detection, data=top10engagementhigh, family=Gamma(link=""inverse""))

summary(fit1) # for your parameter estimates
anova(fit1) # for your f-tests

Good luck! I hope this helps!",1524862097.0
NormalCriticism,I'm not familiar with that exact problem but a few days ago a major update to the R software was released. You could try the earlier version. ,1524843608.0
,[deleted],1524809913.0
Darwinmate,"Different implementation of the same API. Choose whatever one you look. Personally I'd use `rtweet` because it has a nice vignette.

https://cran.r-project.org/web/packages/rtweet/vignettes/auth.html

Also its a more recent package that relies on a more recent base R version (2.X vs 3.X)",1524809904.0
grasshoppermouse,"You probably want something like this:

    boxplot(residuals(fit) ~ gender, data = mydataframe)
    boxplot(residuals(fit) ~ Subject, data = mydataframe)

",1524796436.0
forever_erratic,"Hey, a lot more detail (even a script) would be useful.

A couple things off the bat:

1.  What is leafTXT2.txt? Is that simply an image stored as a text file?  If so, that file, when read in and saved as the variable b, is not a list of points marked in fiji, it's the image itself.   You'll need to save the points separately (likely with the ""measure"" window in Fiji) and read these in using read.table or read.csv.

2.  I guess based on your use of owin, that you are trying to do stuff with the spatstat package?  If so, you'll probably be wanting to make a point process, with the ppp() function, which will then use the coordinates from Fiji as the input arguments.

3.  Based on just the code above, you're getting an error because neither W nor SP2 are ever created.  ",1524794697.0
huessy,"Google ""Twitter API in R"", you're going to need to register with Twitter as a developer and get an API key. Last time I looked at Twitter data, they only let you scrape the last 3200 tweets for any one user, so you might hit a snag there. I don't think they have a restriction on number of posts that share a hashtag though, so that might be the route you want to go down.

R-bloggers should have a Twitter API tutorial as well.",1524752843.0
Dokugumo,"This is a great tutorial for sentiment analysis of tweets :

http://varianceexplained.org/r/trump-tweets/",1524756612.0
huxleyan,"You want to use the Rtweet package. It's fantastic and the documentation is really good on the package website. 

[rtweet.info](http://rtweet.info/)",1524764951.0
zdk,"try converting the node names to character types, would be my first guess",1524781115.0
Darwinmate,"Have you looked at `MSBVAR`?
https://www.rdocumentation.org/packages/MSBVAR/versions/0.9-2",1524810646.0
openclosure,"If a project is small enough I'll fit the whole thing in an .Rmd file, but imo files get a lot harder to manage when they're more than a couple of full screen lengths worth of code. Better to separate things out and abstract the different parts of the process. I use a variation of the [cookiecutter](https://github.com/Satalia/production-data-science) format, usually trimmed down to match the scope of the project. Sometimes I will put the source files into a package, sometimes just a directory and source them as needed. To replicate the analysis pipeline, I'll have a scripts directory with files tagged in the order they need to be run. 

something like this usually:

    - project/
        - data/
             - raw/            # read only
             - processed/      # interim data
             - models/         # model objects
             - output/         # any raw output tables
        - source/              # sometimes a package
            - data.R
            - analysis.R
            - model.R
        - scripts/             # replication pipeline
            - 01_data.R
            - 02_model_train.R
            - 03_model_predict.R
        - notebooks/           # usually a record of the exploration/model building process
            - big_boy_analysis.Rmd
            - logistic_model.Rmd",1524701524.0
AlexBlackbird,"If you're at all likely to use the functions again I'd recommend wrapping them up in a package. I find having them in a package on GitHub that my analysis file installs from is very tidy and convenient. My personal experience has also been that a separate file for data cleaning is good, since you don't want to rerun it when you're done. I use RMarkdown for my analysis to have code + output + my thoughts all in one spot, which is another reason to separate them. If you use a project for the combination of data/cleaning/analysis/report it'll still be tidy and in one place.",1524700038.0
samclifford,"http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510

Keep raw data in a separate folder to everything else. Write a script that creates the processed data files in exactly the same way every time so that if someone reruns it it won't break the analysis work flow. Document the script that does this processing so that it's clear not just what it's doing but why.

If you're writing functions which are a particular to your data set I'd just dump them in an R script and source them. If they're generic things that may help others, put em on github as an R package. ",1524706161.0
chonggg511,"There are two packages that you can look into

    library(ProjectTemplate)
    library (workflowr)",1524717691.0
shaggorama,"This repo demonstrates or discusses most of my arguments for not putting everything in a single file: https://github.com/dmarx/make_for_datascience

To get the gist of it, read the ""motivation"" section of the readme.",1524728102.0
Stan125,"I mean, prcomp uses eigen value/ eigen vector decomposition to arrive at the variance maximising principal components, which you could also interpret as having rotated the axes such that the principal components have maximising and decreasing variance.

But what you probably mean is loadings rotation, which is a part of factor analysis. There, the loadings of factors on variables are rotated. If you just do factor analysis by the principal component solution (which you could also do with prcomp), then no, there is no loadings rotation.

But, if you use factanal, which does ML factor analysis, then yes, the loadings rotation (varimax) is already included.

Hope this answers your question.",1524693969.0
dm319,"Sometimes it's a good idea to post a very simplified version of what you're trying to do, so we can try to make it work.

I'm not sure what you mean by rotation?  Are you talking about the actual process of deriving the principal components?",1524693445.0
justamathnerd,I was just messing with this. The psych package has a lot of tools like this. The principal() function gives you the options you're looking for.,1524709899.0
Darwinmate,"Explain roll rate a bit more because i'm not following. your expected output doesn't really follow from your data. What's age 1 age 2? Why is Delinquent_2 have age1 = 2 and age2 = 1?

Try to not use jargon because it doesn't make sense to those not in your field. ",1524742362.0
jollyjellybeans,"Will they always only have 2 decimal points? Otherwise you could do some rounding but a basic way to do this is 

rep (c (1,0), times = c (100*decimal, 100*(1-decimal))",1524662302.0
ychinenov,as a demo for reticulate i suppose it makes sense; for visualization it makes no sense whatsoever - R has at least half a dozen visualization methods that produce plots like these and better.,1524660963.0
efrique,"Both are not quite correct in execution though the second approach will give the correct mean.

The first mean is wrong because you're choosing the d6 60% of the time and the d4 40% of the time. This makes 5 come up 10% of the time on average.

One correct way to simulate would be like this:

     outcomes3 <- replicate(turns,sample(1:sample(c(4,6),1),1))
",1524626168.0
fasnoosh,"Here’s one:

https://github.com/ropenscilabs/webrockets",1524686661.0
efrique,"You should probably ask for a more efficient way to create a vector with those characteristics rather than doing it the way it sounds like you're going to attempt. Actually creating the matrix would pretty much be a huge waste of space and time.

I'd consider looking at simulating an MA(1) series, then repeating that as many times as needed.


If you must create a tridiagonal matrix, look here: 

https://stackoverflow.com/questions/28974507/efficient-creation-of-tridiagonal-matrices/28974577

or there are several packages with functions for creating or solving tridiagonal systems that can be googled readily enough.
",1524617623.0
ohheyitsdeejay,Rvest and wordcloud ,1524621567.0
modestwitness,"Check out the various blog posts on tidy and text analysis. I think there's even a free ebook. Also, you might want to look at topic modelling + word clouds now you're at it. The stm package will have you covered on that front.",1524627868.0
biledemon85,"The amount of work that comes out of each release is kind of staggering, especially with the risk of breaking base API's and the kind of regression testing that I'm sure goes into it.

I guess I always imagined the R core team being like 2 bearded people in their 50's sitting in a basement in a New Zealand university. I presume the real picture is quite different :)",1524610438.0
PM_ur_good_deeds,Dumb question: does the fact that a package will be byte-compiled mean that each function in that package will run faster?,1524652321.0
SecretAgentZeroNine,"I'm hoping for an article that'll thoroughly pit R 3.4, R 3.5, RCPP, and Python against each other in a series of speed tests.",1524670085.0
Jake_JAM,What do we get in the new version?,1524646972.0
chonggg511,"; after dgamma(1,1) maybe?",1524545400.0
ejoran,"Use `lubridate` for easy vectorized date addition and `data.table` for fast assignment.

Starting from your `df`, this seems to work fine:

    library(lubridate)
    library(data.table)
    dt <- data.table(df)
    dt[, newdate := Date + months(Months)]

Edit: if you just want the numeric month rather than the full date, you can wrap it in `month`

    dt[, newmonth := month(Date + months(Months))]

I compared the `mapply` method with the vectorized version above, and even with the extra overhead of assignment in data.table, the vectorized method was around 100 times faster on a sample of 20000 rows.",1524532550.0
KopfJ4ger,"Could you do this?

    library(lubridate)
    df$newdate <- df$Date %m+% months(df$Months)


Source: https://stackoverflow.com/a/14182326",1524532659.0
maxpower63,"Try month() from the lubridate package. 

month(df$Xmonths) <- months(df$Date) + df$Months

This will give you a numerical value for the month. If you want a string with the name of the month set label = TRUE.

",1524532820.0
infrequentaccismus,"I benchmarked the solutions mentioned here as well as my solution and the results are below.

If you want the most performant solution, the strategy is to convert the date to the integer value representing the month first, then add months to it with a modulus of 12, rather than applying date algebra. 
 
Your solution slows down quite fast with larger datasets since it is calculating a whole new sequence of numbers for each observation.  The jump from 10k rows to 100k rows is noticeable. 

The datatable solution has equal performance to the lubridate solutions.  The %m+% operator is a very tiny bit slower because it is performing an extra day-of-month check that is not helpful for you.

On 10k rows, the mapply solution take roughly 1000 milliseconds, the datatable and lubridate solutions take roughly 30-50 milliseconds, and the solution I present takes ~1 millisecond.

Code:

    library(tidyverse)
    library(lubridate)
    library(data.table)
    library(microbenchmark)
    
    df <- tibble(date = seq(as.Date('1999-01-01'), 
                            as.Date('2018-01-01'), 
                            by = ""day""
                            ) %>% 
                   sample(10000, replace = T),
                 months = sample(1:24, 10000, replace = T)
                 )
    
    diff_function <- function(x,y) {
      return(seq(from = x, length = y, by = ""month"")[y])
      }
    
    
    microbenchmark(
      
      #Your function
      newdf <- mapply(diff_function, df$date, df$months),
      
      #data.table method
      newdf <- data.table(df)[, newmonth := month(date + months(months))],
      
      #lubridate
      newdf <- df %>% mutate(newmonth = month(date + months(months))),
      
      #lubridate operator
      newdf <- df %>% mutate(newmonth = month(date %m+% months(months))),
      
      #base math
      newdf <- df %>% mutate(newmonth = date %>% month %>% {(. + months) %% 12}),
      
      times = 30L
    
    ) %>% 
      autoplot",1524618560.0
SecretAgentZeroNine,Update to R 3.5 and see if the ALTREP changes effects the speed of your code compared to the version of R you're currently using.,1524670186.0
taskhomely,I recognize some of those words ,1524534710.0
revgizmo,It is now,1524485070.0
TKirby422,"It's **both** a great way to learn R **and** a nice on-ramp to learn machine learning.

Are you reading the 2nd edition?

P.S. Brett Lantz is also an instructor on datacamp.com.",1524598040.0
samclifford,"If you aren't absolutely wedded to using apply and grouping in this way, this is quite simple to achieve using geom_histogram and facet_wrap. ",1524476076.0
kenderpl,"1) Naming formal parameters of the function x and y is a bad thing to do - you are not providing any information and if the function gets bigger than this one you will have to keep in mind what x and y refer to which is an unnecessary load. Parameters can be descriptive and it's helpful when they are. Ie in your example they could be group and title.

2) The function should probably take the data frame as a parameter instead of taking it form the environment but that's probably not so important in your case.

3) Following code works on my end: 

    temp <- data.frame(Grp1 = c(1,0,0,0), Grp2 = c(0,1,0,0), Grp3 = c(0,0,1,0), Grp4 = c(0,0,0,1), Level = rep(sample(1:10), 100))
    
    graphing <- function(group, title){
           hist(temp[temp[group] == 1, ""Level""],
                breaks = 1:10, main = title)
    }

    mapply(graphing, Groups, Titles)

4) You might want to take a look at purrrr library - it has tons of functional applies that are a bit nicer to work with than base-r ones.",1524486109.0
deadsalle,"Not easily, because we  need to know most of m\^\^2 before we can compute m\^\^3. 

You can speed things up for high k by noting that, eg

    m^^7 = m^^4 %*% m^^2 %*% m

so only 4 matrix multiplications are required, rather than 6.

One can parallelize each matrix multiplication, but depending on the compilation of R you are using this may be happenning already:

https://simplystatistics.org/2016/01/21/parallel-blas-in-r/",1524473814.0
Vickerspower,Not sure how to solve your issue but sounds like you might be using the base R gui installation? I highly recommend using Rstudio when using R. It has numerous benefits over the base installation and coincidentally would probably solve your issue.,1524469767.0
SgorGhaibre,"In the Dock settings of your macOS System Preferences, if the preference is set to ""Minimize windows into application icon"" then the window will disappear into the application icon. To restore the window, Ctrl+Click on the application icon and the plot window should be listed above ""Options"". Click to restore it. If the preference is not set to ""Minimize windows into application icon"" then the plot window should be at the right hand side of the Dock.",1524476569.0
Mooks79,"Well, I would start with the author’s website for knitr!

Yihui Xie has also written a book about knitr. 

Karl Broman has a blog with a good intro on knitr. 

Then you’ve also got to think about the complexity of what you’re doing. If you are writing a relatively simple report in terms of layout etc, you can try writing in Rmarkdown and let RStudio do all the knitting for you in the background (will generate word, html and pdf). 

If you are doing something more complicated stick to knitr (although you could still generate a draft in rmarkdown and then manually edit the latex/word/html after to include whatever you want - e.g. referencing inside a document). 

My personal tip of the day with knitr (although I think this applies to all documents - especially latex). Generate your graph with the right size, the right font size, the right dpi etc in the first place - that you intend it to be in the final document.

For example, if it’s going to be 10 cm in the document, make it 10 cm to start with. If your document uses 10pt font, make your graph use 10pt (or some defined smaller size). If you want to change the line or font size - change it in the original graph, don’t do the thing most people do of just making a bigger first and then the scaling makes the font looks smaller. It’s very common but a completely arse-about-face way of doing it (one I was guilty of myself). 

This takes time at the start to set-up sizes etc right (once you have a std way it becomes quicker for all subsequent documents) but it saves a lot of fussing about of regenerating graphs of different sizes to make the embedded graph look right. Plus it keeps graphs looking very consistent throughout the document. In knitr there are chunk options to help you do this - e.g. out.width, fig.width etc. ",1524466828.0
CJL_LoL,"First brace isn't closed. Need to close it before closing the """,1524462848.0
dtrillaa,"It’s a syntax error, which normally means you forgot to close all your brackets, quotations, etc, but I couldn’t see any mistakes with the code you posted .

I think it’s with your writeLines call, but I can’t check",1524451206.0
itsintheletterbox,Is the use of a package going to make it any simpler than a giant if-else block?,1524453173.0
efrique,"> but my CI definitely don’t look like 95% ones

looks more or less like a 95% CI to me. What did you expect it to look like? 

",1524456472.0
Vickerspower,"limits <- aes(x = ‘whatever your x variable is’, ymax = ‘your upper CI’, ymin = ‘your lower CI)

Then you can add it into your ggplot code with: 

geom errorbar(limits, position = position_dodge(width = 0.75),
            color = ""black"")


I would also suggest selecting a smaller step size when creating your prediction dataset (prd). This way your prediction line will be a smoother curvy line that actually reflects the polynomial line that your model fitted rather than the zigzagging line that you have now. A step size of 0.1 should do it but going even smaller will do no harm and will improve the plot greatly IMO.",1524437829.0
efrique,"There's a *bunch* of ways to do it in R.

e.g. here's a couple of ways in vanilla R:

`mydata$v1 <- ifelse(mydata$v1== -999,NA,mydata$v1)`

`mydata$v1[mydata$v1== -999] <- NA`



https://www.statmethods.net/input/missingdata.html has a bit of information about missing values in R that may help for some other operations on missing values
",1524459421.0
,[Replace function](https://www.rdocumentation.org/packages/base/versions/3.4.3/topics/replace),1524400411.0
MeanMrMustard92,"Dplyr has a nice [replace_na](http://tidyr.tidyverse.org/reference/replace_na.html) function for this. Also, in terms of transitioning from Stata to R, I'd recommend looking into the tidyverse packages; they are far more syntactically coherent and sensible in terms of day-to-day working with data than their base R equivalents. IIRC, R for Stata Users focuses on Base R packages, I think you're likely to be better off working through [this stata-r translation](http://www.matthieugomez.com/statar/index.html) and [this book](http://r4ds.had.co.nz/) ",1524421315.0
standard_error,"In R, missing numerical values are closed as NA, and missing string values as """". [Here](https://www.statmethods.net/input/missingdata.html) are some simple ways to work with them.",1524400474.0
RememberToBackupData,"The `dplyr` function that others have not mentioned is `na_if`.

    data.frame(x = c(10, -999, 20)) %>% 
        na_if(c(""."", ""-999""))
    
    x  
    1 10  
    2 NA  
    3 20

`dplyr::replace_na` is not right because it takes `NA` and turns it into something else.",1524548998.0
biledemon85,"It's really hard to get anything out of this viz, like too much going on at once and the animation is too fast for me to read. I also can't tell the difference between the genders visually.",1524419137.0
TweetTranscriber,"📅 2018-04-19 ⏰ 04:44:19 [(UTC)](https://www.timeanddate.com/worldclock/converter.html?iso=20180419T044419&p1=1440)

>My first \#blogdown post on making animated directional chord diagrams using \#rstats  http://guyabel.com/post/animated-directional-chord-diagrams/

>&nbsp;

>Data from http://onlinelibrary.wiley.com/doi/10.1111/imre.12327/abstract 

>— Guy Abel 鄭蓋堡 ([@guyabelguyabel](https://twitter.com/guyabelguyabel))

>🔁️ 56 💟 102



📹 [video](https://video.twimg.com/tweet_video/DbDgfj5W4AA6QK3.mp4)



 &nbsp; 

^(I'm a bot and this action was done automatically)",1524391832.0
CJL_LoL,"Looks like you've came out of the loop in the second line you show when you put } which is the only place you have i defined. So when you try to use it in the line after you hit the break. I've not used JAGS in a while to know if it should be in the loop, but moving that brace down a lone fixes the code I believe ",1524378805.0
SeveralKnapkins,"> I know VS Code and Atom have extensions for this, but i am looking for something super minimal, whose only purpose is editing markdown documents. 

Why? Atom can be clunky, but vscode's performance is pretty good, and I'm not sure how you get much more minimal than a text editor unless you're going emacs/vim? ",1524356504.0
,"Screw Rmd. Just write plain R script in your editor of choice and use rmarkdown::render() to spin them into whatever output you want

link: https://rmarkdown.rstudio.com/articles_report_from_r_script.html",1524362590.0
agclx,I don't use it - but  Rstudios [notebooks](https://rmarkdown.rstudio.com/r_notebooks.html)  is the only one I know.,1524399298.0
RememberToBackupData,"In this large chunk of dense code, which function call is the one that returns the error? Have you tried doing a traceback in RStudio? Have you tried looking at whatever input you’re providing to see whether to see if it’s actually a numeric vector or not?

If it’s the `cut` call in your `GoF_Test` function, then it looks like you are trying to cut a matrix but `cut` only handles numeric vectors, or perhaps the matrix contains non-numeric characters. This is why R’s error message said, **“‘x’ must be numeric”**.

I also get the feeling that you’re rushing into R without really understanding how it works. You should have a look at http://swirlstats.com, which is an interactive R tutorial that runs in the RStudio console. ",1524342649.0
natched,"Various issues, first of which is you don't say what you are trying to do. You also don't say what is the problem that needs to be fixed.

I'm guessing you are evaluating the residuals for normality? Don't do that by using KS against random normal values and repeating it 100 times. Use shapiro.test or another test for normality.",1524340244.0
Negotiator1226,Oh man we can help you so much with all of this information you’ve given us ,1524342301.0
efrique,"You'd need to explain more about your problem (hint: a clear explanation of what you're trying to do and what the problem is, along with a minimal working example we can run ourselves would be a good start), but in any case what you appear to be doing makes little sense: 

1. If you're trying to test a single sample for normality, you introducing a huge amount of unnecessary noise by testing against *random samples* and then applying a two sample test; you're throwing away power for no obvious reason. Why would you not use a one sample test?

2. Note that the Kolmogorov-Smirnov test assumes a completely specified distribution but you've estimated (and then corrected for) the conditional mean and variance. There's two effects here. First the resulting variation from a standard normal is too small (the mean and variance are exactly 0 at 1),  so your p-values will tend to be too large. 
At the same time, even if the errors were normal you won't have actual normality of the residuals (indeed, in small samples they're quite distinctly non-normal). The KS test applied to residuals (either one or two-sample) won't give suitable p-values used this way. This is closer to a Lilliefors test (but it's a bit more complicated than that). 

3. It doesn't make sense to formally test normality when checking assumptions

 e.g. see [here](https://stats.stackexchange.com/questions/2492/is-normality-testing-essentially-useless/2501#2501)
",1524377205.0
calguy99,Never used R in Anaconda but more familiar with Python but good to know and Thanks for sharing!,1524330862.0
kron4,Your model sucks?,1524266164.0
WayOfTheMantisShrimp,"What sort of tools are you using? There's always a chance the inputs/outputs aren't being interpreted correctly.

What sort of variables are you using? Do you have a justification for why you expect those variables to show a linear relation?

Sometimes, there simply isn't a linear trend; you can then try non-linear models, or assume that there is no connection to be found.",1524265023.0
masher_oz,Have you tried visualising the data? Maybe 0.2 is correct. ,1524275520.0
dm319,Are you trying multiple univariate analyses to begin with?,1524263715.0
PM_ur_good_deeds,Can you post your code?,1524269840.0
,What do you mean by risk? You already have the probability of 'fraud' or 'not fraud' based on your logistic fit.,1524250435.0
Owz182,"I would suggest you look at PostgreSQL, it’s free and open source and can connect with R so you can pass data back and forth for analysis. There’s a lot of free online tutorials too!",1524284095.0
,"I’ve struggled to find good sources of information for what you are trying to do. Reading and using databases is a topic well covered. Creation and management is more often done in other languages/applications whereas in R the task is to flatten the data for processing. You can do it, I’m just saying there doesn’t seem to be a lot of resources online on the topic based on my own research. 

I have been using SQLite database files but they are just holding flattened tables for me. Not much better than using a csv file, currently. ",1524487462.0
avflinsch,"Try the 30 day free trial on lynda.com it should be enough to get you up and running.

",1524237103.0
team_morty,"This is a bit more advanced but what you want is actually a combination of a libraries that work in the tidyverse: purrr, dplyr, and tidyr.

    library(purrr)
    library(tidyverse)

    df %>% 
      split(.$receptor) %>% 
      map(~select(.,-receptor)) %>% 
      map(~spread(.,cell_population,value))

* split() turns everything into two lists, separated by the 'receptor' column.  

* map() applies the function to each list you created. 


* select() chooses the variable you want to use, but by using the ""-"" you're actually doing the opposite- eliminating the variable you don't want. 

* Then at the end you use the spread() function you mentioned originally.

Also, just to be totally clear for anyone reading this- the %>% is a pipe, which basically says 'take whatever I just created and pass it to the next thing.'  In this particular case, every time I use the pipe, the thing gets passed to wherever the '.' is in the next line.  In normal cases the pipe sends the previous object to the first part of the next function, or whevever you specify the '.'  It's more for readability than anything else.",1524227057.0
Stevo15025,Have you tried data.table?,1524283068.0
dm319,"Ok, so I set out to prove with a more complex example (which is a simplified version of what I used an MD array for) the value and utility of casting into an MD array, given the suggestion that things might be better done sticking to a dataframe (u/fourpita and u/Stevo15025 thank you for sending me on this journey of R discovery):

Lets create the data (I'm now a fan of data.table):

    > library(reshape2)
    > library(abind)
    > library(tidyverse)
    > library(data.table)
    > 
    > patients <- c(""X"", ""Y"", ""Z"")
    > cells <- c(""AA"", ""A1"", ""A2"", ""BB"", ""B1"")
    > receptors <- c(""three"", ""blind"", ""mice"")
    > 
    > patient <- rep(patients, each =  length(cells) * length(receptors))
    > cell <- rep(cells, each  = length(receptors), times = length(patients))
    > receptor <- rep(c(""three"", ""blind"", ""mice""), times = length(patients) * length(cells))
    > value <- runif(length(cells) * length(receptors) * length(patients))
    > 
    > df <- data.table(patient, cell, receptor, value)

which gives me this data:

    > head(df)
       patient cell receptor     value
    1:       X   AA    three 0.6232917
    2:       X   AA    blind 0.7829215
    3:       X   AA     mice 0.3712573
    4:       X   A1    three 0.4675860
    5:       X   A1    blind 0.5525423
    6:       X   A1     mice 0.1456692

So a number of observations of three receptors, on 5 cell populations (AA, A1, A2, BB, B1) on two organisms (X and Y).  Essentially data that forms three dimensions.

Two of these cell populations are controls (AA and BB) - I need to get the asinh-fold change from the control populations to get the final values of A1, A2 and B1, which are the results I'm interested in.

    > df %>%
    > 	acast(patient ~ receptor ~ cell) %>%
    > 	asinh() -> a1

Then I can subtract along the dimension of interest to get my results in the form of a 2D array.
    
    > A1_real <- a1[,,""A1""] - a1[,,""AA""]
    > A2_real <- a1[,,""A2""] - a1[,,""AA""]
    > B1_real <- a1[,,""B1""] - a1[,,""BB""]

And bind it back together again.
    
    > a2 <-abind(A1_real, A2_real, B1_real, along = 3, make.names = TRUE)

Which gives me this result:

    , , A1_real
    
            blind        mice      three
    X -0.19156970 -0.21805917 -0.1366609
    Y -0.04759030 -0.15563878  0.4489289
    Z -0.08597088 -0.05435828  0.1582733
    
    , , A2_real
    
            blind      mice       three
    X -0.67436331 0.2383068 -0.05206313
    Y  0.03597438 0.4736928  0.47929584
    Z -0.48968068 0.7697569 -0.19095280
    
    , , B1_real
    
          blind       mice       three
    X 0.4444238  0.4369615 -0.26818459
    Y 0.1631605 -0.1850383 -0.09902791
    Z 0.2557386  0.2120318 -0.06601929

Which can be easily melted back into a dataframe for further analysis.

    > head(melt(a2))
      Var1  Var2    Var3       value
    1    X blind A1_real -0.19156970
    2    Y blind A1_real -0.04759030
    3    Z blind A1_real -0.08597088
    4    X  mice A1_real -0.21805917
    5    Y  mice A1_real -0.15563878
    6    Z  mice A1_real -0.05435828

Unfortunately we've lost the column names which is a bit annoying.  But it's done, and in an elegant way.

But to show you how much harder this is to do without a multidimensional array, I'll attempt it the tidyverse way.

    > df %>%
    > 	mutate(value = asinh(value)) %>%
    > 	dcast(patient + receptor ~ cell) %>%
    > 	mutate(A1_real = A1 - AA, A2_real = A2 - AA, B1_real = B1 - BB) %>%
    > 	select(patient, receptor, A1_real, A2_real, B1_real) %>%
    > 	melt() %>%
    > 	head()

Darn.

    Using patient, receptor as id variables
      patient receptor variable      value
    1       X    blind  A1_real -0.1915697
    2       X     mice  A1_real -0.2180592
    3       X    three  A1_real -0.1366609
    4       Y    blind  A1_real -0.0475903
    5       Y     mice  A1_real -0.1556388
    6       Y    three  A1_real  0.4489289

It even keeps the column names.

TIL.",1524565133.0
Owz182,This is excellent! Caret is an awesome package,1524283882.0
jowen7448,"There is no need to combine the two objects to use them with the glm function if that's what you are asking. 

provided the order of x and y match and the number of obs match (which given the nature of the question is implied) then no issue. 

See ?glm (I think there should be an example, not on laptop right now ) ",1524206377.0
Owz182,"#Might not be best way but I might do this...

Data_train <- data.frame(“y” = Ytrain, Xtrain)

Mod <- glm(y ~ . , data = Data_train, family = binomial)

#the “.” In the glm formula means use all other columns",1524284573.0
shujaa-g,"Probably `nlme`, or use RStan.",1524189045.0
Disturbed_Capitalist,"I would recommend it. I was in the class that helped Dr. Leite finalize the book, and the class notes were basically the book chapters. If you are already moderately proficient in R, you'll find it very useful (otherwise you may want to brush up a little on your R skills).",1524188753.0
Busta-Reims,"What analysis/ordination method are you using? Either way, a useful resource may be ""Community Ecology"" by Mark Gardener. Maybe your library has it? It goes into detail on ordination methods in R.",1524173073.0
grasshoppermouse,"Check out the phyloseq package:

https://joey711.github.io/phyloseq/

There are lots of tutorials on that site.",1524240206.0
ombrerouge,Numerical Ecology with R is a great resource.,1524186346.0
Revanchist95,"This is something that I can actually comment on! (PhD student working in the microbiome as well). So as a comment prior pointed out, it is totally dependent upon what statistical method/ordination methods you are trying to do. Vegan can take absolute OTU abundance table as well as relative abundance data. For example, if you want a normalization method that is not making it compositional, i.e. rarefying -which is available in vegan, then you run the appropriate command with the absolute OTU abundance table. I think adonis2 works with absolute abundance rather than relative abundance as well. Cheers!
",1524178105.0
team_morty,"Try this, using lapply()

    lapply(df[3:5],FUN = as.numeric)",1524169549.0
jowen7448,You get the error because the columns of a data frame are actually components of a list. Df[3:5] returns 3 component list. Each component being a vector. As.numeric then tells you it can't convert that list to a double. ,1524174120.0
jowen7448,Are you running the R code as you go? Is the output being put into the editor or the terminal? It's an option you can change somewhere. Are you knitting  and leaving the result open in rstudio browser? This let's the document be reknit whenever you save the source which would slow things down?,1524164405.0
furryfairy,"Have you tried using the latest daily? If you are, I'd file an issue since that sounds like a bug.",1524191017.0
celestial_babe,"I am not sure if I have understood your question. Hudsonia$A85, Hudsonia$A86,... Would give u the 4 different matrices.

Do u mean 4 matrices with the same variable name? ",1524162754.0
autotldr,"This is the best tl;dr I could make, [original](https://ntguardian.wordpress.com/2018/04/19/a-recession-before-2020-is-likely-on-the-distribution-of-time-between-recessions/) reduced by 93%. (I'm a bot)
*****
> The longest previous period between recessions was the time between the early 1990s recession and the recession in the early 2000s that coincided with the collapse of the dot-com bubble; that period was ten years, and the only period longer than the present period between recessions.

> That&#039;s not enough to deter me from attempting to use the Weibull distribution to model time between recessions.

> These CIs suggest that while the probability of a recession before the 2018 midterm is very uncertain, my hunch about 2020 has validity; even the lower bound of that CI suggests a recession before 2020 is likely, and the upper bound is almost-certainty.


*****
[**Extended Summary**](http://np.reddit.com/r/autotldr/comments/8dpnh4/a_recession_before_2020_is_likely/) | [FAQ](http://np.reddit.com/r/autotldr/comments/31b9fm/faq_autotldr_bot/ ""Version 2.00, ~313521 tl;drs so far."") | [Feedback](http://np.reddit.com/message/compose?to=%23autotldr ""PM's and comments are monitored, constructive feedback is welcome."") | *Top* *keywords*: **Recession**^#1 **distribution**^#2 **time**^#3 **estimate**^#4 **financial**^#5",1524247394.0
Thaufas,Check out `?par` and look for the `mar` parameter.,1524119905.0
battlestarmetallica,Make sure that the variable is a factor! You can check by using the `str()` function,1524101808.0
WisOWis,I'd recommend fread (and fwrite) from the data.table package.,1524095368.0
mr-datascientist,"There are a few things you can use:

1. One read and save into the 'feather' format using the 'feather' package is HIGHLY recommended. Your read-write speeds will 18x-20x faster than using plain CSV files. Also the feather format is compatible with Python too, so you can play with both languages. https://github.com/wesm/feather

 - My benchmark comparison between base::save and feather::write_feather on my Intel Xeon 2.6Ghz, 128GB, with a Toshiba 2TB 7200 RPM SATA3 64MB Hard Drive which shows that feather is 18x faster than base for 10mil rows [10x faster for 1mil rows]. You'll see similar performance on the read functionalities as well.

2. To stick with csv, refer to some options here : https://statcompute.wordpress.com/2014/02/11/efficiency-of-importing-large-csv-files-in-r/

3. Check out the 'chunked' package. I haven't used it personally but had it bookmarked for investigation : https://github.com/edwindj/chunked",1524093406.0
Thaufas,"Some great suggestions have been offered in this thread. If you work with big data sets routinely and have access to an HPC or cloud resource, take a look at [sparklyr](http://spark.rstudio.com).

You won't beat the performance of a properly configured and provisioned instance of Apache Spark, and the R connector available through the *sparkylr* package makes working with Spark datasets just as easy as any other R dataset.",1524116990.0
spinur1848,"Put it into a database and then access that way.

If you have the skill and ability, postgresql and pgloader work wonderfully. If you need something fast and dirty, sqlite flatfile and/or python csvkit.

Once you've got the data in a database, dplyr for interacting with it.",1524098025.0
Atheriel,"There's no reason why `read.csv()` won't work, it might just be slow. If that's a dealbreaker, you can try `readr::read_csv()` or `data.table::fread()` instead.",1524095679.0
jazzlw,"I’m a big fan of read_csv from
The tidy verse. I do this kind of thing often with up to 8-10 gb files and it works pretty quick. ",1524097521.0
breunor7,"I've ran into this same issue on several occasions and found that the readr package offered me the best solution through 'read_csv_chunked'. It'll read in 5Gb+ files easily. You'll just specify how many rows to read in at a time, I normally do 20k, and set the progress flag to TRUE to get a status bar that lets me know it's still running.",1524154147.0
robsalasco,Try monetdblite https://github.com/hannesmuehleisen/MonetDBLite-R,1524161667.0
AnInquiringMind,"As a standard workflow, I'll use read_csv, do all the munge steps, then resave the dataframe to a binary .Rdata object (which loads almost instantly for future workflows). From then on you can just load that binary object in a fraction of the time it takes to reread the CSV.",1524102654.0
AGINSB,"I'm just going to add the additional note that, if your data is stored anywhere but locally,try moving it to your local machine before using fread or any of the other suggestions",1524145831.0
dejaentendu280,"Just to add to the list of possibilities here, I would check out the iotools package. I've gotten pretty great performance compared to base functions from the package. ",1524148688.0
tinhb,fst package is fastest imo,1524100810.0
csjpsoft,"It might not be freezing, it may just take a long time to rerun all your code and output the results.  It doesn't show any progress until it's finished.  Watch the console - it will report as each chunk is executed.",1524120861.0
disco_lama,"I've run into a lot of memory issues with R. You can increase virtual memory on your machine, but that won't prevent freezing. In some cases, it will only freeze temporarily (so you can try giving it more time), but if that doesn't work, you might need a different strategy altogether (it may be just too memory intensive, and Rstudio sets hard limits on the memory it uses). ",1524088074.0
jowen7448,"That would depend very highly on how long the R code contained takes to run. 

Lots of text and formatting would take time too, but most time will be running R code and formatting that output. 

you could look at setting up a chunk cache from the knitr docs if you have code in your document that takes a while to run each time you build it. ",1524085274.0
ryapric,"> ...WordClouds are what you should be looking at. They are simple to set up, provide stunning visualizations...

Oh, child, no...",1524266714.0
Hoelk,"This is exaclty the situation where you would want to write a custom function instead of copy and pasting the code :)

http://adv-r.had.co.nz/Functions.html",1524129353.0
Owz182,"Above is correct about writing your code up as a function. But, failing that, could you not just use Rstudio to Find/Replace your old file names for the new ones etc.?",1524284849.0
anderaaron,Take a look here this should help out: http://www.cookbook-r.com/Graphs/Plotting_means_and_error_bars_(ggplot2)/,1524106381.0
Humble_Monkey,"I predominately use RStudio for my work. I've downloaded RCode to test it out on my current project. There are some things I like and some things I don't like. 

I like being able to manipulate/edit/copy values and columns within the dataframe. This is useful for me to be able to quickly copy and paste data into other programs for external analyses or for other colleagues. 

I like the interface although it is a bit awkward being so used to RStudio. Unfortantely for now RCode is quite laggy, particularly when I have loaded large dataframes. Most of the time everything freezes and I am forced to close the program. So for now it is unusable for me and I will stick to RStudio. But I'll keep an eye out for future versions out of curiosity. ",1524059564.0
ShillingInTheName,"> Directly edit your variables

Will anyone here stand over this as a good idea? It's certainly bad on the reproducibility front",1524057023.0
webbed_feets,"The problem with Rstudio being so good is that any new IDE has to offer something really unique to bring in new users. I don't see any reason to switch.

Can you highlight features this IDE has that Rstudio is lacking?",1524057449.0
SmellyFartMonster,"Serious question, why should I use this over RStudio? ",1524057194.0
cntrl_,Looks good and I'd love to give it a try. Does it have a vim mode though?,1524068934.0
ballzoffury,Awesome that you're creating an alternative to Rstudio!,1524059460.0
yaymayhun,"Your IDE needs more testing. I am trying to create a new R script file by clicking File > New File but nothing happens. I also clicked on the New File icon ... nothing. 

After closing and opening again, I saw a notification ""Quick load RData successfully loaded"". Does that mean it is automatically saving the environment variables without asking?",1524063266.0
oscjm,Looks great! I'll try it!,1524059299.0
DataDouche,I will say it has a better night theme than RStudio,1524060569.0
macsmith,Opinions?,1524053865.0
huessy,I smell plotly,1524066322.0
metabyt-es,Looks pretty cool. Is there a way to serve it via browser?,1524068735.0
jambocurious,"Just from 5 minutes testing it. 
Idea of creating competitor to RStudio is great, but as someone said in this thread it is very very difficult. 
RCode is very laggy with big objects (and big i mean not even huge matrices, just couple of thousands columns data frames). It also was a bit uncomfortable for me to work in it, maybe because i am too used to RStudio. 
Overall it seems that this product is designed for people who are somewhere between Excel and R. And I think this is not how the competitor of RStudio should come to the market. ",1524080604.0
giaqui1,"My opinions:

 - The fast graph is the most interesting feature for me.

 - The project explorer and special help are very useful.

 - There are still some details like instantly close quotation marks, snippets and running a whole block of code when pressing Ctrl + R in the opening of a function that I'm used to it.

 - Showed an error once trying to access the settings.

 - I don't think that directly edit your variables is a good practice.

Seems very good for a new project, are you planning to do a server version like RStudio server? 
",1524083200.0
Owz182,I commend you and the team for trying to offer up an alternative to Rstudio. Some people are a little pious about editing values directly. I personally don’t do it (it’s good to have the code) but if someone wanted to use the feature I think they should be able to.,1524108803.0
br_shadow,Not open source = not interested,1524125803.0
Rylick,"""You shall have no other gods before Me."" -Lord Wickham

/Did you create it? I didn't mean to ridicule your input. Just did it for the lulz. Will look at it",1524076224.0
Quasimoto3000,Is there any notebook like functionality?,1524068111.0
dalaio,Is there really no code completion in the console pane or is it using a different binding than <TAB>?,1524078873.0
zeusjupiterlugh,"Does it have a knitr function? Does it handle rmd files? 

I rely extensively on rmd for writing drafts and sharing projects based on R. I'm not sure why someone in my situation would not switch another software that does not offer the same quality of life. It does look more esthetically pleasing though.",1524082308.0
iconoclaus,"Will it support more extensive editor tweaking and multicursor support, ala VSCode/Atom? (yes i know RStudio has some multicursor support already).  Another thing i miss from RStudio's editor:  the ability to copy code with formatting (color).",1524106231.0
Fugalysis,RStudio seems fine... Please go help Python get up to speed :),1524138883.0
HawkUK,Symantec Endpoint Protection **did not** like this.,1524480917.0
RememberToBackupData,mfw editing raw data is shown as a feature,1524079289.0
VincentStaples,mmm.... [familiar](http://www.jimmycomedy.com/wp-content/gallery/jimmy-o-yang-gallery/jian.jpg).,1524054200.0
HedgeTheRisk,"I'll give this a try but it'll have to be very, very, very, very good to get me to use it over Rstudio. ",1524069098.0
RememberToBackupData,"Never used this package, but I'll take a stab at an obvious weird thing:

    PModel.sim <- jags(
        data=c('data'),

You're taking a number vector and putting it inside another vector, in effect creating a list of named vectors with length 1. You're basically doing:

    data <- c(data = c(6, 1, 4, 4...)),

So your modelling function is seeing the named vector and thinking that it's a feature, but you haven't actually mentioned a feature named `data` in the `PModel()` call. So try just calling the data vector by name.

    PModel.sim <- jags(
        data = data,

Or else if this data is meaningful, put it into a named vector that matches one of the features that *is* mentioned in your model.

---

Edit: And also, `c('data')` isn't even your data lol, it's just a vector of length 1 whose only entry is the word ""data"".",1524031139.0
Kickuchiyo,"JAGS doesn't know what your Y variable is. Try 

bends <- list(Y = data)",1524052824.0
chalaila,Check out the [ifelse](https://www.rdocumentation.org/packages/base/versions/3.4.3/topics/ifelse) function.,1524023539.0
Ruffie1234,The ifelse is how the data that populated the column test was created.  ,1524024070.0
doughfacedhomunculus,"Please do not fork out for CMA. It's easy to use, but expensive, outdated, and incredibly limited in terms of the types of analyses you can do.

I haven't used 'meta', but I would highly recommend 'metafor'. I believe it's the most flexible meta-analysis package and [the documentation](http://www.metafor-project.org/doku.php) is fantastic. As far as I could tell, it could handle almost any type of meta-analysis you might want to do, perhaps with the exception of some more advanced multivariate problems or Hunter & Schmidt-style psychometric approaches for range restriction/measurement error (although [there's great packages for those](https://cran.r-project.org/web/views/MetaAnalysis.html) as well!). You can check out a tutorial [here](https://www.youtube.com/watch?v=d1pYHfCKhyA).

EDIT: Totally forgot - there's a recent JAMOVI module called [MAJOR](https://github.com/kylehamilton/MAJOR) that provides a really good GUI front-end for metafor functions, with point-and-click functionality much like CMA or SPSS. Video showing how it works [here](https://www.youtube.com/watch?v=7Wj9R_Qd4gs). I believe you can export the R code to make it reproducible/play around with it. Makes it extremely easy if you're just getting started and wanting to run analyses quickly.",1524013168.0
Darwinmate,"Fuck Stata. Use R. 

Here is a meta analysis of R meta-analysis packages:
http://journals.sagepub.com/doi/pdf/10.3102/1076998616674315
have a read and see which packages meets your expectations. If you can not access the paper let me know. 

Something else I would look for is good tutorials or vignettes. For me to use a package I would need a tutorial or some help navigating the package. So these things are important.",1524012033.0
,[deleted],1524015368.0
Slabs,"I prefer the metafor package, though I think meta does better forest plots, if I recall. Both are sufficient for anything you'd want to do.",1524036828.0
eldaym,See the rvest package ,1524000098.0
fasnoosh,"Check out the package ""Rfacebook""",1524024116.0
fencelizard,"data.table usually does well with big datasets. Shouldn't be too hard to implement bootstraps in data.table syntax if you can fit the full table in memory to start with. If not, you could use awk to print random rows to a temporary file through a system() call, then reading in the subsets and running the bootstraps. ",1524001568.0
Kroutoner,"What's the nature of the data you're looking at/how many group means are you computing? 

With such large sample sizes available to you, you would likely be fine relying on typical asymptotic normality assumptions and using conventional confidence interval calculations.",1523999659.0
Quasimoto3000,"How many cores do you have available? 

data.table + foreach could be your friend here.",1524008367.0
SeveralBritishPeople,"Poisson bootstrapping like you linked can avoid the slow copies involved in normal bootstrapping. It works really well when memory constrained, and plays nicely with mclapply and friends, again by avoiding extra copies that explode memory usage. It also works well on distributed data sets, or within sql. 

If you sort the vector you’re resampling in advance, you can estimate quantiles pretty efficiently by cumsumming the poisson vector until you hit the sum that corresponds to the desired quantile, and then recording the corresponding value. ",1524019108.0
jowen7448,Have you looked at modelr package as a potential solution? ,1524142004.0
minhhpham,"I don't see any function named jags in the rjags package. Also, `data=c('bends')` doesn't look right. It's usually `data = bends`",1524002448.0
blozenge,"So assuming you want speed measured as [euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance) per unit time the following may work:


    library(tidyverse)
    
    euclidean_speed <- function(lat2, lat1, long2, long1, time2, time1) {
      latdiff <- lat2 - lat1
      longdiff <- long2 - long1
      distance <- sqrt(latdiff^2 + longdiff^2)
      timediff <- time2 - time1
      return(distance / timediff)
    }
    
    df %>% 
      group_by(ID) %>%
      arrange(ID, time) %>% 
      mutate(speed = euclidean_speed(lat, lag(lat), long, lag(long), time, lag(time)))

This assumes that your time variable is numeric in that time[1] - time[2] gives a time difference in the units you want for your speed measure.

The logic of the code is to first group by ID and then sort by ascending times. The group_by is important because it means you don't calculate the speed between observations from different ID's. Sorting by time then means that the `dplyr::lag()` function will identify the temporally preceeding observation.

The mutate command then calls a function which returns the speed given the time difference and the distance according to equation (1) on the wikipedia link above.",1524003253.0
midianite_rambler,"See [Great-circle distance](https://en.wikipedia.org/wiki/Great-circle_distance) for formulas. If you don't need to be very exact, assuming the Earth is a sphere is probably OK. More complicated formulas take into account that the Earth is not exactly spherical.

I wouldn't be surprised if there were a library in CRAN for computing distances from latitude and longitude. Oh, it looks like `geosphere` is just that.",1524001308.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/rlanguage] [I have spatiotemporal movement data. How would I calculate speed travelled between coordinates?](https://www.reddit.com/r/Rlanguage/comments/8czu09/i_have_spatiotemporal_movement_data_how_would_i/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1523997528.0
cruyff8,Could you [add logging](https://github.com/johnmyleswhite/log4r) before line 64 and update your question with the output?,1523992706.0
minhhpham,"Your `GoF` matrix is all NA, because you forgot to assign values to `GoF` after calculating p-values from `ppois`. Also, the subscript of age3 is j not i

So in the nested loop, there should be:

    vals = ppois(c(y[j]-1, y[j]), exp(b0[i]+b1[i]*(age3[j])))
    GoF[i, j] = vals


",1524003301.0
,Don't give any credence to this dumb visualization  ,1524018967.0
ReimannOne,"`df_row_a_in_row_b <- df[df$row_a %in% df$row_b,]`


Then merge, or bind however you need.",1523974538.0
unrulyhat,What are you running the RStudio Server on? Ive run it on a few different platforms and have noticed slowness at time but only on certain platforms and/or with certain server specs.,1523970548.0
hummingbirdz,"Is it possible it's just a slow connection?

You could try testing it by running Rstudio in a Docker container on your PC: this takes the connection largely out of the picture and means you know the computing resources available to it. Then try browsers--I've never noticed a lag with Chrome (connecting to cloud instances though).  

Usually when Rstudio gets laggy for me on my laptop its because I've accidentally started to run low on RAM, maybe the server is not giving you a lot of resources.

",1523974175.0
kazi1,"The answer is no. If you really want to be hardcore, you can try Jupyterhub using the R kernel or just start the R CLI on the server directly over SSH. ",1523979969.0
metabyt-es,"Desktop RStudio is just HTML/CSS/JS under the hood anyway. If you use Chrome, you can get rid of the browser tab interface and address bar etc. by pinning a bookmark to your desktop that points to the URL (and port) of your server. If you have a bunch of extensions and stuff that are slowing down your browser, this may help with that.",1523980722.0
mtelesha,"I really like just using git and sharing the data a different way. I usually use resilio aka bit torrent sync on the data. Just secure your data especially if it's not yours or your work's network.

I like R Studio Server but I just see git as a more practical solution for me. ",1524000054.0
Viva_Uteri,Are you using Anaconda/Jupyter? This makes a big difference to me. Not the same environment but faster and easier to connect. ,1523967359.0
VincentStaples,"The other responses are correct and scalable, but you can just do it arithmetically if you don't have too many cols:

df$new = df$old1 + df$old2 + df$old3",1523953635.0
StephenSRMMartin,Is rowSums not the function you want?,1523952841.0
dm319,"It looks some people think you're looking for cumulative sums, others think you simply want to sum across columns.  I'm interpreting it as the latter.  Try this:

    apply(mtcars, 1, sum) # this means apply the function 'sum'
                          # row-wise (dimension 1) to the data
                          # mtcars.

That will give you the sum for each row.  To add it onto the end of the dataframe as a new column you can do this:

    mtcars$sums <- apply(mtcars, 1, sum)

Is that what you're after?

EDIT:

The '$' notation is for dataframes.

For matrices you can use 'cbind'

    matrixcars <- as.matrix(mtcars) # convert to matrix
    
    apply(matrixcars, 1, sum) # still works to sum the rows
    
    cbind(matrixcars, sums = apply(matrixcars, 1, sum)) # but you need to use a different
                                                        # notation to add a column
                                                        # the 'sums =' allows you to name the column
",1523956768.0
johnny_riko,"If you’re not already using it, then I can’t recommend it enough - tidyverse package. It includes a few packages which vastly improve data frame management in R. 

Once you have tidyverse, using mutate() from dplyr will allow you to easily do what you’re asking. I’m on my mobile, but I’m sure you can find a good guide somewhere. ",1523939287.0
AllezCannes,"I'm tired, so while this works I feel like there should be a much better way of doing this:

    iris %>% mutate(rowsum = iris %>% select(-Species) %>% pmap(sum) %>% flatten_dbl())

You basically have to make sure you remove any non-numeric variable in your data frame (in the case of the iris dataset, the Species variable).",1523940371.0
YepYepYepYepYepUhHuh,"I would also recommend using a tidyr approach, but both will likely rely on `cumsum()`. 

This will give you a running cumulative sum. Make sure your data are in the order you want before applying this function to your data frame.

This example doesn't make sense mathematically but this is how you would format the syntax.

`iris$cumulative.petal.width <- cumsum(iris$Petal.Width)`",1523940751.0
berf,"Except you don't want to do this.  It breaks the DRY/SPOT (don't repeat yourself, single point of truth) rule.  Don't put redundancy in what you store.",1523975248.0
PandaMomentum,"There are lots of ways to do this depending on what you mean by ""distance,"" ""border"" and ""from."" For example, what would you expect your function to return for Texas, Arizona, or California? Zero, or -- the distance from the geographic centroid of the state to an arbitrary point on the border, or to a specific set of border crossings, or the minimum distance to the geographic feature called ""border"" -- measured either as the crow flies, or, using driving time?

Point-to-point distances are trivial to calculate, either by a haversine function or, simpler still, calling the ggmap mapdist() function, which is a Google maps API call so you can set it to driving distance. You just have to decide what set of points you're using (you don't even have to geocode with mapdist() as it takes strings of placenames).

But. Point-to-geographic feature distance is MUCH more complicated because it requires actual GIS thoughts. You need a shapefile of the US-Mexico border, in some projection. Then you bring that into your map, and calculate min distances from your set of points to this feature. The rgeos library and the gDistance() function are your friends -- see this equivalent problem: https://stackoverflow.com/questions/21295302/calculating-minimum-distance-between-a-point-and-the-coast

And also this shapefile of the border: https://gis.stackexchange.com/questions/153514/us-mexican-border-data

You will need at least a basic understanding of what a projection and datum are to use rgeos.",1523966876.0
mongooseondaloose,"Maybe take a look at the [mapsapi package](https://cran.r-project.org/web/packages/mapsapi/vignettes/intro.html)? I'm not sure how you'd want to choose the endpoint in Mexico but this at least looks like a start. As far as the process goes (after setting up authentication with google) for each of the fifty states, you'll want to 

1. query the api with the `mp_directions()` or `mp_get_routes()` function
2. extract the set of routes from the response, find the minimum-distance route
3. assign this distance to some object that is linked to the state name (dataframe or list, probably)

Hard-writing all this for each state would be a pain, so after you've gotten it down for one state it might be a good idea to use a library like [purrr](http://purrr.tidyverse.org/reference/) to iterate through each state.

Note: I'm not sure how this will work with Hawaii. You might only be able to find the as-the-crow-flies distance.",1523929588.0
AlisonByTheC,"Am I missing something here or isn’t it simply the haversine distance from the geographic mean of each US state to each Mexican state?
",1523961658.0
shujaa-g,Sounds like you should be building a Shiny app that offers a download option.,1523926229.0
spinur1848,"Depends on whether you absolutely need to do everything in R.

Simple triggers are something you can do with Jenkins if it can see the web access logs.

If you want a pure R solution then Shiny is probably your best bet, but its kind of like carving a turkey with a scalpel.",1523964708.0
sixtyorange,"If Shiny is overkill, you could just run it as a CGI script. The simplest possible wrapper would probably just be a bash script that calls rmarkdown::render, writes the CGI header (see, e.g., [here](http://computing.dcu.ie/~HUMPHRYS/Notes/CGI/index.html)), then `cat`s the output.",1525246790.0
jeremymiles,"You're going to have to be a bit more specific than that, or no one can help you. 
",1523921713.0
galby_baby,slots in s4 objects are accessible via the @ symbol. So look at the str() of the object and then you can extract it via object@slot,1523901218.0
rvaducks,"The matrix is:

.4988  .4988  .0024

.4991  .4992  .0017

0        0          1

When it plots, the it rounds up to .5 and down to 0. I don't need it exact but two decimal points would be much preferable than this. Any ideas how to correct this?",1523879792.0
jowen7448,"Given OP username I would assume this is you dean? Is the source available for this. Interested in this being entirely shiny as my typical solution to something like this would involve a call to plumber or similar (read generic rest API) to add things to a queue to process to be added to database. How does this handle a high volume of requests?

Purely inquisitive for my own learning/understanding.

 love all the guides and stuff you do on shiny",1524142772.0
brbecker,Sounds like the researcher hates you.,1523857955.0
10101010101111,This is a perfect way for undergraduates to earn some pocket money.,1523860198.0
minhhpham,"I'm not an expert in mixed effect model, but I would try these:

    m1 = lm(Att ~ Genotype + Genotype:(Day*Cue))
    anova(m1)
    # or
    library(nlme)
    lme1 = lme(Att ~ 1, random = ~1|Genotype/(Day*Cue))
    summary(lme1)",1523847966.0
world__,Try the ANOVA and mixed model functions in package afex. they were developed to provide equivalent results. ,1523865930.0
,[deleted],1523870467.0
joelthelion,Did you find the answer to your question?,1524135918.0
engti,"I think your first problem is doing the analysis.

You could always use [rmarkdown](https://rmarkdown.rstudio.com/) to display your analysis. 

If you need a deck, you can always use [xaringan](https://github.com/yihui/xaringan)",1524023881.0
iconoclaus,"Nice course!  But I can't say I agree with the ""don't learn R the hard way"" mantra. Its stunting students abilities to actually learn the language they are using. They'll become tidyverse experts, which has great benefits of its own, but not actually get any better at coding.

Those of us who've coded for a while in other languages have seen this pattern repeatedly.  For example, Ruby on Rails coders who didn't really even know the fundamentals of Ruby.

There are at least a couple of downsides to become overly trained on a specific framework (like tidyverse or Rails). First, frameworks reflect an opinion in underlying design patterns. As our own understanding of these design patterns change, particular frameworks get abandoned and new ones emerge.  If you have a strong grasp of programming fundamentals (the ""hard way"" part), you'll be more flexible over time to switching frameworks.  Those whose fundamentals are weak tend (imho) to become more fundamentalist and cling on to their old frameworks and ways.

Second, having a strong grasp of fundamentals will help coders better utilize their framework of choice (e.g., tidyverse). There are elements in the coding in this blog post that reek of feature envy taken a bit too far.  For example, combining `<-` and `%>%`, instead of just using `->`, mixes metaphors and actually makes code harder to read for me:

    complete_specdata <- specdata %>% 
      na.omit() %>% 
      group_by(ID) %>% 
      summarise(nobs = n())",1523765755.0
DemiGodSuperNaked,"my.function <- function(var1){
 print(var1)
}
my.function(var1 = pepe) #This returns ""pepe"" since the function prints the variable you enter.

You can edit the insides or the required inputs to make anything inside the function. The examples provided by the R help are very useful, check them. Also, Google anything there are tons of tutorials.

",1523748558.0
thor-eauAway,What do you need help with?,1523753756.0
efxhoy,"I don't get it, why don't you post the questions? Are you cheating on coursework? Need help with a work project? ",1523916088.0
,[deleted],1523748107.0
discthief,Not that I know of. Can you speak to why you're asking? ,1523750256.0
jimmyjimjimjimmy,"Consolas with Tomorrow Night Bright theme.  Used Roboto for a while, not sure why I switched back.",1523741525.0
poopyheadthrowaway,"I used to use dark themes, but my work laptop has a reflective display panel (stupid choice of material, all you laptop manufacturers), so a dark screen makes it so that I'm practically staring at a mirror the whole time. After a couple days of looking at my face, I just couldn't handle it and went back to the default light theme.",1523764154.0
RememberToBackupData,"[Fira Code Retina at 10 pt, Merbivore theme.](https://i.imgur.com/wxSvAni.png)

I chose **Merbivore** because it has the most divergent syntax colouring scheme. Comments are purple and italicised, functions are orange, strings are green, built-in constants are gold. It's really easy to scan.

I changed from Consolas to Fira Code a while ago with basically no adjustment period needed. I tried Monoid but I found the narrowness of the letters harder to read, and I feel like a point size between 9 and 10 for Monoid would have been better for me personally.",1523743820.0
,"Emacs, Solarized theme with Dejavu Sans Mono font. Not Rstudio but very comfy.",1523738218.0
CJL_LoL,"I have no idea for the theme, I change it every few weeks out of boredom. Font size 10 
",1523738406.0
don_draper97,I use Fira Code Retina with Ambience. I like the dark theme and how the black is kind of grainy if you look closely. ,1523739400.0
tdawry,"Modern, Cobalt, 110% zoom, Input Mono @ 10 pt. There is surely some additional UI scaling going on, as I'm running in Windows at 4k.",1523743884.0
kenderpl,Fira Code Retina + Idle fingers. Font 9pt with 150% zoom.,1523746750.0
Darwinmate,"Is there any way to bold the font? 

",1523756020.0
ducnguyeenx,Roboto Mono with Dracula. Actually i'm using Dracula everywhere (VS Code and RStudio) https://imgur.com/a/FhkOs ,1523778888.0
ashwinmalshe,"[Hasklig-LightIt 10 pt with Vibrant Ink](https://imgur.com/a/UZ0o3)

",1523791951.0
navyjeff,"Modern, 110% zoom, Anonymous Pro with ligatures, 12 pt, Merbivore theme. I like dark themes, but I wish most of them had more contrast. [Example.](https://i.imgur.com/3yw8gqF.png)",1523832225.0
Sosewuta,"Font: Consolas  
Theme: Crimson Editor",1523866846.0
jackbrux,Roboto mono,1523738067.0
dontchokeme,"Anyone knows how to change the font while using RStudio Server version? It seems to default to OS specific fonts. On linux, it is alright, but on windows, it is hideous.",1523744212.0
biledemon85,"Don't try to set full file paths when you're expecting code to be shared across devices, just use relative file paths, e.g.:

    read.csv(""../data/my_data.csv"")

That way you don't have to worry about the actual current directory, just make sure the relative paths between the code and data are correct.",1523699929.0
agclx,"You are more or less describing how a R package finds it's data.  They are also just zipped directories.  If you really want that feature it might be worth going that way.  Sure it needs installation - but then it is robust.  Otherwise you totally depend how R is setup, the scripts are sourced and whatever happens in between.

",1523717461.0
Zmamo,Commenting to come back too later,1523723559.0
aprstar,"I always make my paths relative to the script itself. However getting the script path is not very elegant in R when compared to other languages like Python. To get the path of an R script, I tend to use a combination of rakensi’s solution https://stackoverflow.com/questions/3452086/getting-path-of-an-r-script -  and my own method mentioned in this discussion https://stackoverflow.com/questions/1815606/rscript-determine-path-of-the-executing-script.",1523710699.0
Owz182,"Sorry, don’t have time to check your code but sometimes I run in to this and have to use an ‘na.rm=TRUE’ argument in some functions. Is something similar needed with your correlation function? You mention NA cells in some rasters. ",1523681223.0
samclifford,"Cant you just take logs of both, use the log of the median as the mean of the normal distribution, you know how many standard deviations away the tenth percentile of a standard normal is from 0 so use that to solve for the standard deviation, simulate from a normal and take exponential values of the simulated draws? ",1523678420.0
samclifford,"Is this a homework question? It feels like one. R4ds.had.co.nz has a bunch of stuff on reading in data, modelling, etc and you may want to find a stats textbook that explains the interpretation of a linear model. ",1523672521.0
metagloria,Apply max(table()) to each row.,1523640131.0
koi-koi,"    library(modeest)
    
    answers <- data.frame(""Q1"" = c(4,5), 
                          ""Q2"" = c(4,5), 
                          ""Q3"" = c(4,5), 
                          ""Q4"" = c(4,5), 
                          ""Q5"" = c(4,2),
                          ""Q6"" = c(4,2),
                          ""Q7"" = c(2,2))
    
    answers$mode <- apply(answers, 1, mfv)
",1523640950.0
efrique,"So you want the mode for each question? 

What do you want to get if there's two or three equally popular answers?
",1523672295.0
jowen7448,"I found a good reference for this to be my supervisors book during my phd . Stochastic modelling for systems biology by Darren Wilkinson. 

I am probably biased but he is very intelligent and the explanations of concepts is pretty good. The systems biology bit isn't necessary for the stochastic models and poisson processes etc. 

this definitely won't be the most comprehensive reference on stochastic processes at all but I found it useful to learn a bunch. It also has code in (almost certainly R but over the last few years he has really got into scala, so maybe more recent editions have scala in)",1523950046.0
Eleventhousand,"That's very cool.  I assume that if you trained on late-model images only, you'd see higher than 80%, since all BMWs are just larger or smaller versions of the same looking car.

",1523641860.0
Engineer_Zero,"My very limited experience with Rstudio showed me that my hard drive’s read/write speeds made big differences. This was because my script was accessing data from various .cvs files. My home pc has solid states and it was very quick; my work laptop with the .csv files stored on its internal scratch disc was quite a bit slower and then my work laptop accessing the remote servers was very very slow. 

What hard drives does your new vs old pc have?

It’s probably not fair to compare your new vs old pc as I’m assuming they were running different OS/software configs. For example, I would say that 4gb of ram these days is insufficient, especially as your new pc is likely running a more modern operating system than your old laptop (and therefore requires more horsepower). 8gb is the bare minimum, 16gb is recommended if it’s your daily driver. 

Just my two cents, I use excel and power bi to do the vast majority of my heavy lifting and have only just discovered R. I’d be interested in learning what more experienced users say. I hope you figure out what your bottleneck is!",1523620173.0
agclx,"Your two machines sound almost the same.  Only higher frequency in the newer one - and that does matter very little nowadays.  The benchmarkme manual says something you should have 3GB Ram available - having just 4GB you can quickly fall into a region where the tests run worse than expected.  Plus on notebooks it may be they fall into some power saving or silent mode where they are slower.

It depends very much what you do - I have analyses that run happily a Pi0, other feel sluggish at the most expensive machine in the office.

Having said that R is not a good language for high performance computing.  There is quite some speedup waiting if one translates the code in a modern compiled language.  However for most applications it's just not worth the effort.

There are also various distributions around - Microsoft R, Oracle R, and some libraries for using graphics cards.  They all have different optimizations - some may be useful,  but all in all unless you have many cores or other dedicated hardware there is not much boost to be gained.

btw. I run the benchmarks on several machines with much higher specs than yours.  All of them came bottom of the list. It also seems number of cores weren't detected properly. I was originally willing to upload my results.  But although the script advertises it would let me review the to be transmitted data it started uploading stuff without confirmation. That's when it got banned from my machines.
",1523621178.0
amstell,"RAM. You need more RAM, especially if you are going to be doing ML in parallel. I think having 8Gb would be good enough and anything more take it to the cloud because it would be cheap to process. Google Cloud gives you $300 in credits if you sign up. ",1523768818.0
dm319,"That should be slightly faster - but not my huge amounts see this [comparison](http://cpuboss.com/cpus/Intel-Core-i3-5005U-vs-Intel-Core-i3-4130).

You might want to look into whether you are running single-threaded functions.  A lot of ML stuff calls C/C++ programs - you might be able to get a speed up by setting a parallel option.  Also look into your background OS (i.e. Windows? Linux - is the OS doing background work, like updating?), the speed of your harddrive (SSD vs HDD) and the speed/amount of RAM.

On my i7 5600u laptop (on battery power), I'm ranked 24/24 for both the tests.  Not sure what that says...

You could look at using a faster language - maybe the ML implemented in C/C++ or see if Julia would work better.",1523979944.0
dtrillaa,"In general if you’re doing machine learning, you should aim for a quad-core (like a intel i7), 64 bit, and 8 or 16 GB of Ram (the more the better). Examples of this are Mac Book Pros or Lenovo Thinkpads. These are not cheap though and normally cost around 2,000 dollars. R is not a fast programming language so the specs on your laptop or computer matter.

If that is too expensive, I would look into using RStudio on [paperspace](https://tensorflow.rstudio.com/blog/rstudio-gpu-paperspace.html)  (which can range from like 7 cents to 90 cents an hour)",1523634283.0
Petzval,Not a code-based approach but there is [Jamovi](https://www.jamovi.org) which is essentially a GUI implementation of R. It's free and open source alternative to SPSS. It's extremely easy to learn and great for quick exproratory analysis and report generation.,1523618848.0
tenurestudent,"This can be very challenging depending on the temperament of the staff. The last team I worked with, younger staff were more willing to fight the uphill battle where those closer to retirement resisted pretty aggressively. Be aware of this and be proactive in getting everyone on board. I personally feel, if you want to use a command-line based tool people will need to learn to program. It's easier to achieve than people think though. 
  
* To get everyone on board, find some examples of tedious jobs you do time-to-time and do it in R. If you can say, we'll work through the learning curve together, then our lives will be easier. It goes a long way. Also, if you need to sell the time to management, examples of error rates and reduced task times. Have plans on how to handle extra time at work.  
* Take time to learn the basics. Tidyverse is great, but you can't get away for long without understanding how indexing works.  
* Use examples from your data for training. Plan regular training sessions, an hour or two at the most. no one wins in 8-hr sessions. Make it hands-on.    
* Implement GIT/version control from day one. It provides robustness and security. We had one holdout who didn't quite actively sabotage our efforts... but their misunderstanding and unwillingness to learn resulted in a lot of introduced errors. Git saved us on more than one occasion.    
* Standardize your libraries. You'll need to make sure everyone is on the same version and updates won't invalidate your standard scripts (this is rarely a problem, but something management wanted us to be able to verify).  
* Code review sessions help everyone cross-train and learn new ways to address problems. It also helps verify there aren't any bugs.  
  
I came into the program about 4 years into struggling to implement R. I restructured how they trained and treated it more as a development team than analytics team. I think that really helped solidify confidence in the tools and members grew in their abilities (or I just happened to join as they crested the hill). I discouraged some of the helper packages, I feel like if you want to use R you should commit? Keeping a foot in both camps hampers learning; however, I've got more of an eye for coding then others...",1523622374.0
OldMans,"If you want to get your non technical colleges off excel and using programming concepts I’ve found that Alteryx is the way to go.  While it is expensive, it is usable by pretty much anyone that picks it up.  We have our accountants and finance people excitedly replacing all their current excel convoluted messes with work flows with relatively minimal training.  ",1523621695.0
koi-koi,Use simpler libs like flexdashboard or htmlwidgets. Actually ggplot2 is an implementation of the grammar of graphics and is as much about understanding that. Also your colleagues may have hidden talents. And if they don't you could use tableau instead. ,1523617724.0
efrique,One possibility you may want to consider is Shiny. It's possible to set up simple controls that allow various manipulations of what is seen  without requiring most users to write code. ,1523627564.0
RememberToBackupData,"Exploratory is a GUI front-end for R, it even lets you do some cool analyses. Costs money if you’re not a student or teacher, though. ",1523675767.0
nickjstevens,Thanks for the tips all - really good and much appreciated. I’ve got quite a few things to look into in more detail and try out. I’ll try and report back my findings for the benefit of future me and others with a similar question. ,1523725542.0
samclifford,"Write to a different CSV file every 100 rows, and write each file only once. Choose a well structured naming convention that allows you to read the files back in and combine them.

Edit: You might even be able to use a shell script to combine them without needing to open them.",1523617524.0
dtrillaa,"My colleague was just telling me about this, but for Python. I’m pumped they released the RStudio interface",1523593974.0
sohaibhasan1,The openxlsx package has straightforward functions to create workbooks and insert sheets. https://cran.r-project.org/web/packages/openxlsx/openxlsx.pdf,1523579984.0
zreeon,"Export it as a csv with write.csv() then open in Excel. 

Or, better yet, don't use Excel. ",1523577858.0
agclx,"It would help if you described the error you get.  I'm pretty sure you were able to copy the examples - so what went wrong?

Sometimes a dataset needs to get ""massaged"" into a form that excel understands.  Sometimes something small needs to be installed/setup correctly outside R for the packages to work.",1523599576.0
tdawry,It sounds like you want [WriteXLS](https://cran.r-project.org/web/packages/WriteXLS/index.html).,1523583215.0
minhhpham,"I had good experience with XLConnect. For instance, I have a data frame out.schedule that I want to write to xlsx, here's the code:
&nbsp;

library(XLConnect)

  outwb = loadWorkbook(""schedule output.xlsx"", create = TRUE)

  createSheet(outwb, colnames(Caps)[c])

  writeWorksheet(outwb, data =  out.schedule, sheet = 1)

Setting a single cell value/format/formula is possible too.",1523594039.0
soft-error,"Douglas Bates gives instructions on how to call Julia's `MixedModels.jl` package as a substitute to `lme4::lmer` for linear mixed modelling using CRAN package `JuliaCall`, which should be, overall, faster and allow more complex modelling approaches.",1523571746.0
questionquality,"Julia doesn't seem to output the convergence warning here, does that mean it magically manages to fit that complex models, or does it just hide the warning?",1523610630.0
questionquality,"I'm not sure you completely understand these functions. map runs a function *independently* on each element in a list, so you can't use map to call a function that depends on all the lists. map can be useful for t tests (see [this chapter of R for Data Science](http://r4ds.had.co.nz/many-models.html)), but in this case (does mpg vary by am?) what you want is just one t test in the first place: `t.test(mpg ~ am, mtcars)`, or almost equivalently (doesn't use welch's correction): `summary(lm(mpg ~ am, mtcars))`",1523559444.0
fdren,"Yep. Built a plumber API which is built off Rocker. https://www.rplumber.io/docs/hosting.html#docker
Load balanced the endpoints with nginx.",1523562861.0
ReimannOne,"There is probably a way to do it along those lines, but you might want to consider a dplyr join.

Make a dataframe with shortname column and longname column, and then `left_join(originaldf, shortnamesdf, by = c(longformA, shortname))`.  Then you can get rid of the columns you don't need any longer.",1523541376.0
questionquality,"Okay, you have some data

    d = data.frame(long = c(""Boston"", ""Boston"", ""Anaheim""))

and you can define how you want to translate them

    to_short = list(""Anaheim"" = ""ANA"", ""Boston"" = ""BOS"")

and you can translate them by subsetting this list with the data:

    > mutate(d, short = to_short[long])
         long short
    1  Boston   BOS
    2  Boston   BOS
    3 Anaheim   ANA
",1523563110.0
guepier,"First off, a *variable* isn’t a *file*. So you aren’t viewing a “converted file”, you’re viewing a variable. And the variable is indeed in the correct format: after assigning the `names`, the vector has the desired format. You’re done.

---

That said, if your file indeed looks as shown in your example, then `read.csv` will fail to read it correctly. Instead, you should be using something like the following code:

    df = read.table(filename, header = TRUE)
    phen = with(df, setNames(Phen, ID))",1523539565.0
dm319,"Have you tried sinply plotting principal component 1 vs 2 and colouring by group?  How many dimensions do you have for each data point?

Have a look also at LDA.  I'm afraid I'm not familiar with factomine,  I just use the built-in functions for PCA.",1523534170.0
Newraist,"I think you can use *sapply* to solve this problem.

    as.data.frame(sapply(X = 2:ncol(test), function(x){paste(test$TEST0, test[,x])}))",1523526202.0
Bruizeman,If that's a real stock price you trained on... Then that sounds about right,1523533747.0
galby_baby,how can you possibly expect someone to help you with what you provided here?,1523503338.0
mattindustries,"strwidth will tell you the width in specified units. You could loop through the position of spaces in your title and create substrings, checking if it is under the width of your plot.",1523487575.0
Mooks79,"If you set the size of the plot area and the text font, you could then relate the two and put that into str_wrap?",1523494085.0
qpdbag," you can also use ""\n"" within your plot title label to force a carriage return. Not exactly the best solution,  but it works.",1523506108.0
privlko,Can't believe I enjoyed a book review on time series. Pun tastic. ,1523470805.0
hummingbirdz,"Wrappers would work for a one off, but wrappers would add a function call and lower performance in general.

You can directly assign a function from any package to your own name for it

    Df <-data.frame


I’m in mobile sorry if that doesn’t work.

Probably doing this at scale would just lead to confusion especially because fixing the help pages would be tough.",1523456786.0
hummingbirdz,"I know it’s more of a should thing.  I think the work to automate all that in a way that’s flexible enough to deal with all the old and diverse packages is just not worth the gain.

Especially not worth it if all that you do is rename functions, when that can already be done user by user. And if you don’t just rename them and you change the function arguments with wrappers then you loose performance and have to layer on this automation of redocumenting and rebuilding the packages.",1523488793.0
Binary101010,"We have no idea what your data is, what it represents, or what you're interested in learning about it.",1523449487.0
AlisonByTheC,"Leaflet was the easiest option for me.  

Did you see these examples?
https://rstudio.github.io/leaflet/choropleths.html",1523443824.0
BetterGhost,I've used googleVis on previous projects. Its well documented and easy to use. You can find an example & code samples at the bottom of [this page](https://cran.r-project.org/web/packages/googleVis/vignettes/googleVis_examples.html).,1523458012.0
ChemiKyle,"I'm also a beginner in choropleth, many of the packages I've found have poor documentation or require APi access to google that hits query limits for me, I'm finding [the maps package far easier](http://eriqande.github.io/rep-res-web/lectures/making-maps-with-R.html).",1523535648.0
Crypt0Nihilist,"Not sure why you'd use SimPy when you can use Simmer, which is an awesome implementation in R. ",1523403220.0
cyran22,"I think [R for Data Science](http://r4ds.had.co.nz/) is a great way to learn about the basics of writing good, reproducible analyses in R. It doesn't really focus on biological analyses, but starting from here is a great place to learn to write readable, efficient R code.",1523397611.0
revgizmo,https://paulvanderlaken.com/2017/10/18/learn-r/,1523418849.0
hopeyesperanza,Verizanis simple r is good.,1523400739.0
efrique,"The analysis used in genomics is a bit specific, so you probably want to start with looking for resources by googling things like *genomics in R* perhaps supplemented by appropriate adjectives to reflect the level or nouns that reflect particular topic areas.

More generally googling *biology in R* turns up a few resources (sets of notes and some articles, some that look likely to be useful for getting started, like [this](http://users.monash.edu.au/~murray/BDAR/)). Somewhat related areas (like biostatistics, biomedical statistics and ecology for example) are also worth exploring via searches -- for example, I expect you might get some value out of Ben Bolker's book. 


 ",1523406782.0
gsmafra,Number 2 goes against [tidyverse style guide](http://style.tidyverse.org/syntax.html#object-names) and therefore it is indisputably wrong because Wickham is R Jesus.,1523387802.0
DataDouche,I wish there was some way I could passive aggressively show this to my project partners :) ,1523387362.0
jimmyjimjimjimmy,"I snake_case my nouns and camelCase my verbs (functions).  It's a great way to differentiate between tidyverse functions and my poorly written, buggy, personal functions. I picked it up from someone else's style guide, can't find the link right now, me thinks it was a msft employee.",1523414776.0
setyte,Man researching style guides a few months ago was so confusing. I found several that conflicted so I just stuck to the tidy one. As best I can tell the style guide may have changed over the last 5 years.,1523397050.0
RememberToBackupData,"I got sick of calling my temporary variables things like ‘df’ or ‘temp’ or ‘iris_temp’, so now I call them proper names like ‘bob’ or ‘alice’. You can’t dissuade me from this. Aside from being funny, it’s also a code smell that makes it clear when you’re referring to the wrong variable. ",1523423591.0
,"I mean, that's just like your opinion, man.",1523392477.0
,I've seen evidence that people who indent with space earn on average more money than those who indent with tab. But I've not seen a single-space double-space comparison. Is this the new rivalry that will last forever? Will I tell my grandchildren about the years I wasted fighting the double space infidels?,1523387858.0
,"I never get why such a large number of people agree with point **4**.

    doSomething(x, param1=""a"", param2=""b"", scale=TRUE)

looks much nicer and easier to read and compact to me compared with:

    doSomething(x, param1 = ""a"", param2 = ""b"", scale = TRUE)
",1523437693.0
Digging_For_Ostrich,"The exporting section of the tutorial might be useful for you:

http://adegenet.r-forge.r-project.org/files/tutorial-basics.pdf

You could create a data frame of the results and save that using normal r export commands, to an appropriate file format for you.

You can then read the data frame and convert back to a usable format with:

https://www.rdocumentation.org/packages/adegenet/versions/2.0.1/topics/df2genind

See also: https://www.rdocumentation.org/packages/adegenet/versions/2.0.1/topics/import2genind",1523376463.0
Usagi8P,"Usually when doing analysis in R, you'll want to use long format. There's a dplyr function to convert from wide to long: gather()",1523402847.0
mf_L,"Definitely check out [rvest](https://stat4701.github.io/edav/2015/04/02/rvest_tutorial/) for webscraping techniques, and then a combo of [jsonlite](https://cran.r-project.org/web/packages/jsonlite/vignettes/json-aaquickstart.html) and [httr](https://cran.r-project.org/web/packages/httr/vignettes/quickstart.html)  to work with APIs. Should help you along. ",1523334434.0
HawkUK,">I feel like there's a big blank void in my skills, since when I think about it, I probably couldn't even grab a table from Wikipedia and read it into R

Well, I certainly managed this recently. Did it in a similar way to this: https://cran.r-project.org/web/packages/htmltab/vignettes/htmltab.html

Though IIRC I found it easier just to extract *all* the tables and then choose the correct one from the list, rather than the way they did it.",1523359768.0
ct0,"internet explorer actually lets you export to excel, that might be of help until your scraping skills are up to par",1523481260.0
,"Ugh, the point of Debian Stable is to provide a solid base system. You can modify it if you want with other software repositories, even if that's not *the Debian way*. You just have a problem following instructions because I run Debian Stable and can install the latest version of R using those instructions just fine. If that doesn't work you can just install the latest version of R from Debian Unstable, there's not much difference. If you want to install from Debian Unstable you can follow this guide about apt-pinning:

http://jaqque.sbih.org/kplug/apt-pinning.html

edit: You run Gentoo and are bitching about someone not providing binaries....",1523327291.0
guepier,"> I can see that there are Windows binaries, though no Linux binaries are provided. Why one but not the other?

Because it’s fairly normal *and feasible* to install software from source on Linux (especially Gentoo). On Windows it’s simply not generally feasible, and has thus never been normal.",1523374146.0
Glasofeno0,"good post, will anxiously think on this one :D",1523325249.0
I_just_made,"Of course it would...

Every time I think I have some obscure thing that I need to do with a dataset, I find out Tidyverse already has something implemented.

Didn't know about this, thanks for bringing it up!",1523364371.0
,[deleted],1523328141.0
ohheyitsdeejay,"life_goals %>%
mutate(college = “complete”)",1523309915.0
Drewdledoo,"Just put the `magrittr` pipe (`%>%`) on there, or something longer like:


    > coffeenbagels %>% 
        take_classes(years = 4) %>%
        graduate(major = ""political science"", minor = ""data analysis"") %>%
        find_job(field = ""data analysis"")
        ",1523310228.0
Dave_,"from schoolname import degree

program/department = degree.DataAnalysis()

program/department.fit_transform(coffeenbagels)",1523310376.0
RememberToBackupData,"Not really. You can use `cbind` up to two values, but then it breaks down after that. (IMO the data structure you're describing is a matrix, so I use a matrix.)

    mat <- matrix(data = c(""AA"", ""AB"", ""AC"", ""BA"", ""BB"", ""BC"", ""CA"", ""CB"", ""CC""),
                  nrow = 3, ncol = 3, byrow = TRUE)
    
    print(mat)
    
    #      [,1] [,2] [,3]
    # [1,] ""AA"" ""AB"" ""AC""
    # [2,] ""BA"" ""BB"" ""BC""
    # [3,] ""CA"" ""CB"" ""CC""

    mat[cbind(c(1, 2), c(2, 3))]
    
    # [1] ""AB"" ""BC""
    
    mat[cbind(c(1, 2), c(2, 3), c(3, 3))]
    
    # [1] ""AA"" ""BA"" ""BA"" ""CA"" ""CA"" ""CA""


Writing a function to fetch individual entries in a matrix or dataframe is easy enough.

    getEntries <- function(data, ...) {
      output <- NULL
    
      for(entry in list(...)) {
        output <- append(output, data[entry[1], entry[2]])
      }
    
      return(output)
    }
    
    getEntries(mat, c(1, 1), c(2, 2), c(3, 3))
    
    # [1] ""AA"" ""BB"" ""CC""
    
    getEntries(iris, c(1, 1), c(2, 4))
    
    # [1] 5.1 0.2",1523308521.0
mf_L,"Hi all! I wrote the above as followup to a previous article I posted here ("" [A simple R script to quickly scrape crypto price data](https://www.reddit.com/r/rstats/comments/7yqbtl/a_simple_r_script_to_quickly_scrape_crypto_price/) ""). I've posted my full code [here on GitHub](https://github.com/MattLunkes/crypto-quick-scrape-2/) for those who'd like to skip straight to that. Thanks for reading, hope you find this useful!",1523298870.0
Whispersmith,This is a giant pain. see [this blogpost](http://www.owsiak.org/r-3-4-rjava-macos-and-even-more-mess/) for possible solution. ,1523314735.0
BirthDeath,"When I encountered this scenario, I incorporated the code into a shiny application.",1523276396.0
Hoohm,"You do need R to run your code.

What you could do is using conda and create an environment made for you script.",1523277570.0
Mirmalis,"You could do it with portable version of R, but its still 170MB+ for a simple script.",1523278688.0
aprstar,Perhaps creating a docker container (https://knausb.github.io/2017/06/running-r-in-docker/)? But then they would need to install docker...,1523272003.0
guepier,"> Is it possible [without requiring] a full install of R?

No. This is fundamentally impossible: each R script needs R to be executed. You *could* bundle an executable that comes with its own R but I’m unaware of a ready-made tool that would bundle such an executable for you. At any rate I’d generally avoid against it since it would lead to unnecessary bloat in the executable size.

That said, an example application that does exactly that is [SeqPlots](https://github.com/Przemol/seqplots). Unfortunately the public code base doesn’t seem to contain information about how the bundle (containing the code and the R executable) was created. But, in a nutshell, it uses Shiny and [Electron](https://electronjs.org/).",1523287291.0
Spartyon,"If you have a shiny instance stood up, run it on there. Apps can be designed to intake files, manipulate them and then allow a user to download them. If you don't have one already, [here](https://towardsdatascience.com/how-to-host-a-r-shiny-app-on-aws-cloud-in-7-simple-steps-5595e7885722) is a link to get going.",1523289084.0
jzyeo,Don't think there's a way to run R scripts locally without also having a full copy of R. Maybe make a quick shiny dashboard that allows your client to input their file?,1523271462.0
deadcaribou,"I would look into DesktopDeployR, ""a framework for deploying self-contained R-based applications to the desktop"". 
https://github.com/wleepang/DesktopDeployR/blob/master/README.md

If you want to package your code in a shiny app, RInno is a great solution.
https://cran.r-project.org/web/packages/RInno/vignettes/Introduction.html",1523299805.0
dtrillaa,"Try:

    acf(Y, rm.na = TRUE)

It appears there are missing values in your data set preventing you from calculating the plot",1523264696.0
tenurestudent,"When you run into errors, always start with the function's help file. They are sometimes difficult to read, other times not, but they should tell you what the function does and needs to work. Your answer is in there, see if you can find it. ",1523256316.0
suity1,"don't need to to mess with `Rselenium` or any of that more complicated stuff--to get the per game table:

library(rvest)
library(tidyverse)

     h1 <- ""https://www.basketball-reference.com/players/j/jamesle01.html"" %>% 
       read_html
    
     h1 %>% 
       html_nodes("".full_table .right , .full_table .center , #per_game .full_table .left"") %>% 
       html_text %>% 
       matrix(ncol = 30, byrow = T) %>% 
       as.data.frame

[returns](https://prnt.sc/j2whz7)",1523283687.0
SeveralBritishPeople,"I’m on mobile and can’t check the page source to confirm, but I suspect that table is filled in by javascript. If that’s the case, it’s probably pulling json or similar from an api endpoint, and you could just hit the endpoint directly. 

If that’s the setup and you watch the network traffic in chrome developer tools while you refresh the page, you can likely find the api request (sometimes labeled type xhr or something like that). ",1523256207.0
jzyeo,"I did it like this because I found the data in the html file itself, which was commented out. Probably not the most efficient way.

    library(httr)
    library(XML)
    
    url  <- ""https://www.basketball-reference.com/players/j/jamesle01.html""
    site <- GET(url)
    content <- htmlParse(content(site, ""text""))
    content <- capture.output(content)
    
    start <- grep(""<h2>Totals</h2>"", content)
    end   <- grep(""^</table>$"", content)
    end   <- min(end[which(end > start)])
    
    totals <- content[start:end]
    totals <- totals[grep(""Career"", totals)]
    points <- gsub('.*data-stat=""pts"" >([[:alnum:]]*)</td></tr>', ""\\1"", totals)
    points

Returns:

    [1] ""31002""",1523265851.0
eldenv,"I believe that as suggested, it's likely that the table is filled in by javascript - like you, I can't find the api request/end point/data source.

Scraping is still possible, and you've got to use something like RSelenium which will help generate a copy of the website. I've not done it myself but there's plenty of info out there about how to, and maybe someone on here can give some tips too.

https://cran.r-project.org/web/packages/RSelenium/vignettes/RSelenium-basics.html",1523275965.0
Darwinmate,"Seems like you've come over from python. You're confusing matrix with dataframe. 

If this is your vector:

    x <- c(""a"",""b"",""c"",1,2,3,4,5,6)

To return only numbers:

    x[!x %in% letters]

I'm still not really sure why you have a single variable (i.e a column) in a dataframe. 

Actually now thinking about it, have you posted in the wrong place?",1523232561.0
I_just_made,"+1 to /u/Darwinmate for his most direct answer; but it seems based on another post of yours that a,b,c are columns.  Take a look to see if this is more in line with what you want:

    df <- data.frame(a = c(1,2,3),
                     b = c(4,5,6),
                     c = c(7,8,9))
    df
    
    out_vect <- c(df$a,df$b,df$c)
    
    out_vect


df outputs:

    >df
       a b c
    1 1 4 7
    2 2 5 8
    3 3 6 9


out_vect returns:

    >out_vect
    [1] 1 2 3 4 5 6 7 8 9


In this instance, I created a dataframe with a,b,c as columns and vectors as their contents (that's the c()).

Any df column can be referenced using the (df name)$(column name).

df$a returns 1,2,3

by doing c(df$a,df$b,df$c), I'm just asking it to return a vector of each column's contents.",1523365133.0
rarity84,"Oh sorry, that’s the notation I usually do it in Matlab, and yes a,b,c are column names. So I just want to combine them and create a vector of length a+b+c",1523285743.0
efrique,"a matrix can only have values of one type; is ""a"" in your matrix a character or a variable holding a number? Or do you actually mean that x is a data frame? Note also that you appear to be using notation that would be unfamiiar to many R users (it's not how you specify a matrix in R ...  is that Python maybe? Or perhaps matlab? If not, what's it meant to be? Are a, b, c intended to be the first row or the first column?)

",1523273057.0
IMainlineMemes,"Alexios Ghalanos' ""rugarch"" package is pretty good and versatile.

http://www.unstarched.net/r/rugarch/

Is his website. 
",1523213229.0
Zoraxe,"In a rush, so can't talk much. Buy the book ""discovering statistics with R"" by Andy Field. It's the best book on R and one of the best books I've seen on learning stats. Go through the exercises and apply them to your personal data",1523212718.0
MicJaggs,"I learned R for psychology/statistics during my undergrad and it took a while to really master, but has been incredibly useful once I did. 

I'd recommend R's own intro manual and using the RStudio interface if you aren't already, I found it pretty useful. https://cran.r-project.org/doc/manuals/r-release/R-intro.html

Stack exchange also has some useful tutorials, resources, and is immensely helpful with troubleshooting. In the end it comes down to practice, apply the methods you read to your own data, try exercises that you can find, play around with it, and just try to enjoy it. ",1523213222.0
TeslaIsAdorable,"Learn R markdown and/or knitr. It will make writing papers so much easier. Git is also something you would do well to learn - version control isn't taught in psychology (or wasn't when I got my degree) but it makes things *so* much easier when you're coding. Neither of these things are part of ""core R"", but they will make your workflow when using R much more efficient.

Even if you're not ready for rmarkdown quite yet, the `stargazer` package and `xtable` package are nice for getting tabular results from R which are formatted nicely and ready for publication. ",1523218679.0
jazzlw,"Hadley’s newish book r for data science is great! And it’s available free online cuz he’s awesome. 

I think the url is r4ds.had.co.nz

Well worth a read. ",1523220893.0
ELKronos,"I think it depends greatly on your degree and concentration. For instance, if you are working in a larger lab which actually has big data, then R would be a necessity. I would recommend something like this guide on transforming and cleaning data because that is probably what most of your work as a graduate student would require:

https://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf

If you are a graduate student in a cognition or social psychology program with studies that have an N of 100 which are all trials run out of the lab at a single time point, well, you probably won't need R for a lot of what you would be doing. The data task is small enough and not repetitive that you do not need to automate the tasks because by the time you write and test your scripts you would have been finished.

Similarly, approaching R as a statistical tool is really based more on the types of analyses you will be using. If you primarily conduct one shot descriptive statistics and do simple tests (such as a t-test or even an ANOVA) I am not sure there is much added value to learn R to do this.

However, if you need to automate tasks and or create dashboards learning Shiny and ggplot within R would be great! Or, if you are in a discipline where you need to conduct more advanced analyses (SEM/HLM/IRT), then R is a very powerful tool, and because it is free, it makes a lot of sense to use it! Yet, even using R to this end is really package specific. It's simply a matter of finding the library you would need to conduct these sorts of analyses, and then memorizing the codes.

I do not mean to imply that some students should not spend time learning R. I just think it is important to highlight how R is used by different people in different ways and sometimes other programs are just going to be quicker and make more sense for your work.",1523214706.0
revgizmo,"https://paulvanderlaken.com/2017/10/18/learn-r/

When it comes to learning R (rather than understanding the underlying, stats), this article outlines an excellent path",1523228668.0
Whispersmith,try the [jamovi] (https://www.jamovi.org) interface,1523218411.0
,[deleted],1523220075.0
SkornRising,"I had a statistics professor during my Clinical Psychology graduate program who taught statistics using R instead of SPSS. I strongly recommend you continue using R. During your quantitative courses, request from your professor if you can do the homework with R instead of SPSS.

I couldn't finish my PhD program due to having a son but I'm making roughly six figures a year because of my skill with R, statistics, and general data analysis. It will make your a better researcher too.

Here is my personal recommendations;

1. Get a copy of Discovering Statistics Using R by Andy Fields. PM me if you would like a digital copy for free.

2. Go through the book from Chapter 1 all the way up to the general statistics you will likely learn in your first statistics courses (i.e., linear regression, logistic regression, and ANOVA). 

3. Get involved with research and reading research articles. You will start applying advanced models yourself for others and learn quickly.

",1523222585.0
Deleetdk,"Resources I recommend:

- http://compcogscisydney.org/learning-statistics-with-r/
- Andy Field's R based stats intro
- http://www-bcf.usc.edu/~gareth/ISL/
- http://r4ds.had.co.nz/ and http://www.springer.com/us/book/9783319242750
- start following the #rstats people on Twitter, many of which are psychologists",1523233379.0
BringANug,You guys are awesome!!!!!!! Thank you all so much ,1523239895.0
lebeer13,"Hey just started using R a few months ago myself.  This video really helped me to understand the work flow of data analysis in R

https://youtu.be/go5Au01Jrvs",1523241729.0
efrique,"I am not a psych student, but you might find Navarro's book Learning Statistics with R of some help.

http://compcogscisydney.org/learning-statistics-with-r/

It is designed to teach stats to psych students but it teaches R as well. 

You can buy the hardback or download the pdf (the above link has both options). What I have looked at of the pdf looks pretty decent (though not perfect). It looks like Navarro is in the process of updating it to a bookdown book (which would presmuably then be an online resource)",1523243611.0
midianite_rambler,"Aside from books and other documents, the best forum for asking questions and discussing R in general is the r-help mailing list. It gets a lot of traffic and many of the participants are very knowledgeable about R. See: https://www.r-project.org/mail.html",1523248563.0
sausagepizzaguy,"If you are a psych student, ask if a professor can get you started on a datacamp.com account. They are free for education/business programs. My professor got our research lab started on it and I feel like it has been crucial for short-medium game learning.",1523248962.0
Cognitively-Speaking,"I'm late to the party, but I haven't really seen the route I've taken talked about. I'm in psychology myself, and I've used R since my master's. I have a few pointers that have helped me throughout my time to gain an advanced level of understanding with R. 

1. One of the best things I was forced to do by my professor was to do all of my statistics homework in Excel, R, and SPSS. While the Excel part is a bit extreme, it taught me how the formulae work and where each output stat came from. This taught me a high level of detail on statistics that has been *extremely* valuable to me. It also allowed me to understand when and where I made a mistake. This is important within R as it doesn't have robust error correction if your numbers are wrong - only if the formula won't compute. This allowed me to hone in on the more basic components of R in a secure fashion that allowed me to know where I went wrong if I did. 

2. I've also used [Stack Overflow](http://www.stackoverflow.com) a lot to find specific answers, as well as Googling the errors and specific problems I have (just always saying 'in r' at the end). 

3. Reading books will help you, but I'd suggest that if you aren't using them in a classroom setting - use the books for specific needs and to answer specific questions. The book from Andy Fields is fantastic at times - and a bit spartan at other times. The one from Hadley Wickham is one I've looked at for minor data wrangling, and proved really useful. 

4. Do random exercises to increase your R understanding on your own. I often go and grab data from the [U.S. Government](http://www.data.gov), [Kaggle](http://www.kaggle.com), [Github](http://www.github.com), or other open source data sources and just analyse it on my own. It's increased my capabilities as a researcher and data scientist.  

5. Occasionally set goals of random R things to learn that will be at least somewhat useful for you at some point. For instance, I'm learning text mining at the moment. I'd probably suggest starting with ggplot2 as it is an impressively powerful visualisation tool that starts out simple, and can be made completely complex by your own doing to resemble complex yet aesthetically pleasing images. 

6. Get [RStudio](http://www.rstudio.com), and the packages [installr](https://cran.r-project.org/web/packages/installr/index.html) (for Windows) or [updateR](https://andreacirilloblog.wordpress.com/2015/10/22/updater-package-update-r-version-with-a-function-on-mac-osx/) (for Mac). Updating R can be a pain in the ass - these packages will help with that. You'll also probably want a handful of supporting packages pretty quickly but definitely get [psych](https://cran.r-project.org/web/packages/psych/index.html).

7. Using RStudio, make scripts and save **everything**. Be sure to also notate your script files. This will make referencing your materials easier, and you'll understand what has happened. If you're like me - you end up running the same tests repeatedly after multiple data cleanings, outlier removals, etc. I've learned that having a script is an amazing thing. Having notes in my script - life saving. This also can help you show development. In a few years if you go review old scripts you'll see how far you've really come. To notate: the line you want a note on, use the '#' symbol on that line. For instance: 

        #This is a random comment/note.
        print(""Hello World."") #Code and then a comment/note.         


8. Use the R Notebooks in R Studio, its written using R Markdown. I use them to help draft up the results sections but I use dynamic calls on the data listed within so that it's always referencing correct information even if I change the dataset a bit. By this I mean that when I mention a mean, I don't put in the number, I put in the function - \`{r} mean(df$variable)`. 
9. Eventually learn how to make your own function. It's not hard - but when you repeatedly conduct the same three tests it'll become easier to make this a function that just calls the same steps over and over.

I personally use R on datasets ranging from a 30 cases up to a couple hundred thousand, and I still approach it the same way. I believe R is quite powerful, and while the 30 case datasets don't need advanced parallel processing, I still think the advanced capabilities are appropriate. For massive datasets, it's pretty much a requirement - so might as well just use it all the time and be super comfortable with it. But it comes at a price - it will get frustrating at times, but it's worth it.  

Anyway, hope this helps! I'm happy to help more if you want to PM me. ",1523265336.0
dubidak,Looks good. I like Fira code retina. Not as condensed as monoid but worth a shot I think. ,1523159118.0
rz2000,"While I'm sure it would drive other people crazy, I always like to use an oblique or italic monospaced font. First, I used a Computer Modern ""CMU Typewriter Text Oblique"", then ""Consolas Italic"", and just recently I switched to IBM's open source ""IBM Plex Mono Light Italic"".

Since IBM's official sites are terrible for the Plex font:

- [Samples on Google](https://fonts.google.com/specimen/IBM+Plex+Mono)
- [OTF downloads on Github](https://github.com/IBM/plex/tree/master/IBM-Plex-Mono/fonts/complete/otf) or [here in a single zip](https://github.com/IBM/plex/releases)",1523165446.0
koi-koi,Installed regular bold and italic. I can only get the bold version show up in RStudio. Any ideas?,1523182626.0
reddit_tothe_rescue,"Check out the ‘rvest’ package, it works pretty well when all the data you need is in text on the page. Otherwise, mess around with the developer tools to find the url of the json data behind the webpage, then use fromJSON() to load it directly",1523149652.0
nkk36,"Shiny should be able to do that. Here is an example that lets users plot points and then performs clustering on those points:

https://shiny.rstudio.com/gallery/dynamic-clustering.html",1523148572.0
mr-datascientist,Sure. I'll participate. I've saved the link to go to by the weekend.,1523115412.0
RememberToBackupData,"You don’t necessarily need help for R, you need more help for statistics, and then the R stuff becomes straightforward to learn. Try [the Handbook of Biological Statistics](http://www.biostathandbook.com/analysissteps.html), it has a table of what tests you can use with what kinds of input data to arrive at a particular answer. ",1523044067.0
,What is your hypothesis? ,1523039350.0
2strokes4lyfe,"It sounds like you need to run an ANOVA instead of a t-test since you have multiple different groups to compare. This will tell you if at least one category is significantly different from the others. In order to determine which specific groups are significantly different, some form of a multiple comparison test comes to mind (iterative t-tests with a bonferonni correction, or Tukey HSD). Brush up more on the statistics before trying to translate everything to R. [Mathew Clapham](https://youtu.be/VWWbAQy5USk) provides some great statistics tutorials using R that also might be of use to you. Make sure you really know your data before moving forward with the analysis. Often times, it’s necessary to meet the test assumptions (equal variance, normal distribution, VIF scores etc.) before deciding which test to perform (parametric versus non-parametric). ",1523060836.0
,"Please provide an example input data frame and reformat your post with spaces between the lines so that looking at it doesn't make a person's mind explode.

Edit: that said, on first glance it looks like you're manually defining some kind of fit. Is there a reason that you're not using R's inbuilt fit functions, with the final data excluded from the input frame, then predicting from that fitting function?",1523035296.0
CadeOCarimbo,Try using a Recursive Feature Elimination method. The caret package will help you with that. ,1523021619.0
RememberToBackupData,"Try using a plot from the `forestFloor` package. These give you visual information about the contribution of each feature in how the forest makes its decisions. Together with the importance scores ([be wary of importance metric bias](http://parrt.cs.usfca.edu/doc/rf-importance/index.html)), this should help you pick features. ",1523021493.0
RaggedBulleit,"I use caret and gbm for this. Similar n, about 2k features. I do feature selection within categories of features, then join the top performers from each category. Rank those with vip from another gbm, then build an additive model  until performance plateaus on training data.",1523023447.0
dmuney,"My first inclination was to include an interaction effect, but I had a suspicion that this was unnecessary due to the nature of random forest, which was confirmed with a quick google search. The model is most certainly accounting for the interaction, but that may not be reflected in the variable of importance.  
  
While measures like the variable of importance are useful for selecting features, they should not be the only consideration when specifying a model. It is important to use your own intuition and understanding of the domain, along with some common sense,, when specifying a model. If you are relatively certain that a feature is important, even though it may not be reflected in the afformentioned measure, you should probably include it.  Models like random forest do well with large numbers of *relevant* features. One caveat would be if that variable is **highly** correlated with another variable ( > .90) then it could cause inconsistencies in cross-validation. Hopefully that helps.",1523021280.0
mapItOut,"Treat x as a factor and specify all levels, whether present or not. In your example:

    > mydf$x <- factor(mydf$x, levels = c(1, 2, 3, 4, 5))
    > x
    [1] 1 2 3 5
    Levels: 1 2 3 4 5
    > table(mydf$x)
    1 2 3 4 5 
    3 2 1 0 2 
",1523019076.0
scbagley,"A dplyr-based solution:

`mydf %>% count(x) %>% right_join(tibble(x = 1:5), by = ""x"") %>% mutate(n = coalesce(n, 0L))`",1523037792.0
efrique,"You don't say which tests you want to run on which variables nor how you have your data set up.

What exactly are you expecting people to do in response to this?

Are you asking for a book recommendation? If you seek specific help, ask a specific question about what problem you've got.

Also see the help: `?t.test`, `?lm`, `?aov` and `?anova`

",1522972790.0
RememberToBackupData,"The only thing I’m unsure about is 4. Magrittr already has a pass-through pipe (`%T>%`) that allows you to explicitly ignore any side-effects. I feel that it would make no sense for an entry in a forward pipeline (`%>%`) to *not* change the piped data in some way, because that would mess with the internal logic of the whole thing. ",1522994459.0
shujaa-g,"Plotting with ggplot should never be a side effect because the plotting function should return the `ggplot` object - otherwise you require users to hack your function to customize the plot.

Base plotting, of course, can't be anything but a side effect, but the data returned should be as close as possible to what is actually plotted, again, so that if the user wants to customize the plot you've helped them out as much as possible.",1523027304.0
jzyeo,Have not written functions with pipe operators in mind so though this was a pretty good read. Thank.,1523237062.0
shujaa-g,"Join first by ID, and then join the rows without matches by name.",1523124195.0
millsGT49,"Haha this is great, it does get easier though! As I learn new languages I'm always disappointed and frustrated that its never as easy to find documentation and internal help as it is in R. Hopefully coping with memes helps you keep going!",1522934793.0
shujaa-g,"It does get better! Once you learn you way around, R documentation is some of the best.

Get to know the most important sections for what you're looking for:

- usage, for how you should be calling the function, and a glance at the argument names and default values
- args, for details on each argument (what class should it be? what does it do?)
- value, for what is returned by the function
- examples, if I know more-or-less what the function is supposed to do but I don't quite get it, I go here. (Don't just look at the examples, run them! One line at a time, and look at what's happening!)
- description, more or less what the function does
- details, when you really want to know what the function does, or your thought you understood it but now something weird is happening
",1522937957.0
masher_oz,I love the way the java documentation is written.  Nothing ive seen cones close to matching that niceness.,1523013847.0
sparkplug49,r/rstatsmemes,1522983650.0
,Truth!!,1522995942.0
Thaufas,"Although R's help system is very nice, the actual help documentation is terrible.",1522996629.0
Surf_Science,You need to look at the strip chart function documentation as it’s overriding the base axis labeling ,1522915287.0
RememberToBackupData,"You need to display your code as preformatted text by putting 4 spaces in front of each line

    like
    so

You also need to provide the code in a format that people can just copy and paste into R.",1522911720.0
enilkcals,"If you want to see how predictive the factors are of your success in betting then you could...

    glm(Win.Fail ~ Odds + Home.Away + Confidence, 
        data = whatever.your.df.is.called,
        family = binomial)

",1522917037.0
BustedEchoChamber,What do home.away and win.fail mean?,1522913675.0
AnonymousGourmet,"I think this works:

    library(dplyr)

    x %>% group_by(UserID) %>% 
    arrange(DatePurchased) %>% 
    filter(row_number()==1)
",1522879485.0
DrOddcat,"First, likelihood is used to interpret the original coefficients of logit before covering to odds ratios.

Second, for a predictor variable that is binary, you can interpret the odds ratio as the difference in the odds of the 1 outcome in the DV between the 1 and 0 group.",1522868610.0
deltron__zero,trying to edit the table,1522866247.0
VincentStaples,"For really real those are awful plots. Use ggplot, it's easy.

ggplot(df, aes(x=x, y=x))+   
  stat_summary(fun.y = mean, geom = ""bar"", fill = ""white"", col = ""black"", alpha = 0.899)+  
  stat_summary(fun.data = mean_cl_normal, geom = ""errorbar"", width = 0.1) +  
theme(legend.position = """") 

Then tweak to your aesthetic preferences as you see fit (i.e. change ""width"", ""col"", ""alpha"", ""fill"", then look through the options in *theme*)",1522883079.0
,"Dynamite plots are pretty horrible, but can't you just [use this](https://www.r-bloggers.com/dynamite-plots-in-r/)? Why don't you just use ggplot?",1522859133.0
agclx,I may be not seeing it - what are the benefits against running `new.packages`?,1522913295.0
Usagi8P,"From what your lmer says I assume your DV is Att and IV are Group, Day, and Cue
You'll want random slopes for variables which vary within group (Day and Cue?) and random intercepts for between group variables (Group?).
You may want to look at the way your `family=` and check if that fits
Here's a useful article:
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3881361/",1522876089.0
Soctman,"What do your fit statistics (e.g., AIC) look like for the linear vs. non-linear versions of the model?

Also, 89k data points might be a little large for exploratory analyses. You might try testing different models using a random subset of the data and then bootstrap the entire dataset at the end to save computational resources.",1522844236.0
samclifford,"You could try pasting the variable names and coefficient values together

    data(mtcars)
    lm1 <- lm(data=mtcars, mpg ~ wt + qsec + am + vs)
    
    paste(paste(""E("",names(lm1$model)[1], "") ="", sep="""" ),
          gsub(x = paste(round(coef(lm1),2),
                         c("""", names(coef(lm1))[-1]) ,
                         collapse="" + ""), 
               pattern="" \\+ \\-"",
               replace="" \\- ""))
    

    > E(mpg) = 9.58  - 3.92 wt + 1.23 qsec + 2.94 am - 0.02 vs",1522823529.0
frinkahedron,very useful!,1522808026.0
efrique,"There's an infinite number of distributions with those properties! 

If you consider location-scale families, pretty much *any* continuous distribution would work. 

What are you trying to achieve here? What's this for? ",1522796863.0
Ader_anhilator,"rriskDistributions, quantile matching estimation",1522810923.0
metagloria,"If you have three specified percentiles instead of two, that corresponds to a unique Generalized Gamma distribution (assuming one exists with the three specified percentiles). But that's just the GG; as someone else said, you can find any number of distributions that satisfy what you want. The GG just has the advantage of being pretty easily calculable. ",1522804651.0
RememberToBackupData,"You could try making a QQ plot with different sorts of distributions. That would at least let you know which kind of distribution gets you closest, and then you can start using the built-in distribution generators to get what you want.",1522801173.0
samclifford,"Are you feeding it a data frame of x, y, z values or a matrix where each row is a y and each column an x? ",1522790812.0
TripKnot,"Using this `/etc/shiny-server/shiny-server.conf` you should be able to place your ui and server files right into `/srv/shiny-server/` and be able to access them via `http://example.com`.  All other apps should be able to put into another `/apps/` subdirectory.  I've confirmed this works on my test server. 


    run_as shiny;
    server {
      listen 80;
    
      # primary app
      location / {
        app_dir /srv/shiny-server;
        log_dir /var/log/shiny-server;
      }

      # other apps
      location /apps {
        site_dir /srv/shiny-server/apps;
        directory_index on;
    }
",1522777014.0
shujaa-g,SQL is usually code - not rectangular data. I don't have access to your download link so I can't look at it. I would recommend opening it in a text editor (or using `head` in a terminal window) to see what you're actually dealing with.,1522724213.0
BetterGhost,"I use SQL to bring data into R daily, via the [DBI](https://cran.r-project.org/web/packages/DBI/index.html) & [odbc](http://db.rstudio.com/odbc/) packages. That 2nd link has code samples, but here's the gist to save you a click:

    con <- dbConnect(odbc::odbc(),
                     dbname='myDB',
                     host='host.com',
                     port=8888,
                     user='spectrum_specter',
                     password='abc123')
    data <- dbSendQuery(con, ""SELECT * FROM myTable WHERE DATE >= '2018-01-01';"")
    dbDisconnect(con)
",1522725485.0
GOD_Over_Djinn,"A .sql file is a sequence of SQL commands for talking to a relational database. I don't think that's what you're meaning to import into R. You probably want the result of a SQL query itself. It is possible to connect R to a database directly if you have credentials using the DBI package or one of its cousins and the right database driver, but you'll probably be better off finding another way to download the dataset that you're looking for and importing the result of that into R.",1522724250.0
spinur1848,"First you should probably learn a bit more about the data you want to work with. I didn't have too much trouble finding this documentation:

https://www.yelp.com/dataset/documentation/sql

Next, understand that this is relational data. It doesn't come as one table, it comes as a bunch of tables. It also looks like it could be quite large, although I wasn't able to get filesizes without signing up for the download.

R keeps everything in RAM. It sounds like this dataset might not fit easily into RAM, which means you'll probably need a separate database system to store the files on disk. In the documentation link they suggest two different open source databases, MySQL or Postgresql. Both would work fine and can talk to R through the DBI connectors. You could also use sqlite and a file.

But do try to understand what you're working with before you end up with out of memory errors.",1522794656.0
metabyt-es,Classic X-Y problem going on here. ,1522724474.0
chalaila,"Al though this doesn't solve your question, maybe try with the JSON file.",1522726454.0
ychinenov,make one argument but define it as a list. parse the list inside function's body. ,1522749383.0
efrique,"There are numerous functions in R that have a number of parameters that are not fixed; you may want to investigate how those work.
",1522763148.0
cxavier,/r/cringe,1522672326.0
efrique,"I did not enjoy the style of it at all, but there are some good links in there.",1522654393.0
Amishjohnthomas,Anyone wanna post a version that has the key tips normally? ,1522677175.0
alt_account_for_pork,"Fuck that was annoying. Great links though, thank you.",1522660287.0
ZZ-TOP,"I love it when you call me big poppa
I only analyze data if it's proper :) ",1522653203.0
toothless_budgie,That was a much more interesting read than I first thought it would be.,1522631017.0
Resquid,Straight fire,1522633904.0
LoganR84,"I would be pretty shocked if rcpp gave you much of a speed increase.  Expm is not just some simple R script, it is written in Fortran or calls directly on some of the BLAS libraries (as is my understanding anyway).  If you want a real speed increase you might look into using a better BLAS/LAPACK package.  Ive had students that played around with this and reported some pretty impressive speed increases.

In general, going to rcpp really only seems to be worth it you cant get around running a lot of loops or if there is some specific C++ library/script you want to use.",1522542447.0
Mooks79,"LoganR84 is on the right track - switching to a faster BLAS library can give massive gains in R. I was playing around with this recently and found out that the easiest switch is also the fastest (at least for what I was doing). 

I tried a combination of matrix manipulations using Rcpp, RcppArmadillo, RcppEigen and - surprisingly - none were as fast as base R, when using Microsoft Open R. 

MOR is an evolution of Revolution R since MS acquired them. They’ve basically done all the hard work of speeding up R - for example they use the Intel MKL as their linear algebra library - automatic parallelisation for many tasks etc. 

Of course it will depend on exactly what you’re doing, but I’d try installing that and running your code in base R, before spending too much time with anything else. Provided you don’t mind the different license - of course. ",1522571263.0
,"Rcpp has two popular linear algebra packages: RcppEigen and RcppArmadillo. Both are very high quality in my opinion. There is a good chance that you will get a modest speed boost from re-writing code in one of these packages, but I can't really guarantee it because I don't know how expm is implemented. ",1522525123.0
tylermw8,RccpArmadillo will certainly speed up your code. It's not even that hard to use--check out the documentation at http://arma.sourceforge.net.,1522534247.0
guitarelf,"Did you measure only once? When you say different individuals - do you just mean two groups? Some in a control and the others in an experimental condition? Why was your data non-parametric? Are you trying to understand mean differences or predictive values of your conditions? How large was your sample?
",1522515818.0
wdanalytics,"The function wilcox.test() performs either a Wilcoxon signed rank test if you pass the argument paired = TRUE or a Wilcoxon rank sum test if paired = FALSE. The latter is equivalent to the Mann-Whitney according to R documentation. 

Paired is FALSE by default, so I would say that you performed the correct test if you had non-parametric data and wanted to compare two groups.

Edit: Corrected wrong T and F",1522517152.0
natched,"You need to answer the questions from the other comment to figure out what test you should use, but wilcox.test() does perform the Mann-Whitney test if you give it two samples and don't say paired=TRUE. So you'll almost certainly be using that function either way.

The main question is whether your data are paired, which is probably the same question as whether you measured the same individual twice or all the measurements are from different individuals.",1522517279.0
samclifford,Data aren't nonparametric. The test is nonparametric. ,1522536250.0
samclifford,How are you creating which?,1522493303.0
Crypt0Nihilist,"You have the same attitude as me - the wrong one.

If you're writing scripts which do more or less what you want them to, you are a programmer. Perhaps 90% of programmers are better than you, but it is a moniker to which you are entitled. Own it and be proud of it. 

I was given a terrible job at work this week. It took my manager five minutes of telling me how great an opportunity it was, how I'd be in important meetings etc etc before he worked up the courage to tell me I had to collate data from Excel sheets and our corporate data store and send out emails to people who made the naughty list. He was worried that I'd shout at him. I can't blame him, it's a crappy manual job you'd give an intern. 

Then he was afraid that I had gone silent so he was filling the dead air with more guff about how important it is and how I'll benefit from it. I wasn't silent because I was angry, I was silent because I was already working out that I can automate the task.

My job isn't to code, but I use code to help me do my job better and more efficiently. That makes me a coder. You're doing the same.

Anyway, back to your question, don't just look at books, you'll get better tips on coding from looking at blogs where you will see people write an entire piece of analysis. Then you need to cast a critical eye over what they've done and ask yourself what they were trying to achieve, how you'd have tackled it and whether their approach was better. You'll find a list of blogs if you do a quick search of this sub. Alternatively go to rbloggers which tends to publish quality content and find a contributor who is doing cool stuff in a cool way and go to their blog.

Finance really needs R. Spreadsheets are great for things like NPV and keeping an eye on cash flow where it's useful to eyeball the data, but it's painfully obvious that it's over-used when you have a 20 tab spreadsheet that creaks when you change a value. People love Excel because they understand it and you can do mad hacks to get it to do what you want and it makes you feel like a genius. I've been there. Learning R made me realise how impoverished Excel users are. 

As a recovering Excel user, the main thing you need to do is break from the eye-balling your data comfort zone. You should be dealing with too much data for that to be practical. Therefore you need to use charts and statistical summaries to understand what's happening in your data and what might be wrong with it. That's more of a niche analytical skill and something your programmer friends probably don't deal with. ",1522494068.0
refined_compete_reg,"https://www.coursera.org/specializations/jhu-data-science 

This course can be taken for free if you don't want the certificate or the assignments graded. Highly recommended!",1522460943.0
Usagi8P,"Hey, 
I learned through instruction and practice. Your friends seem skilled, at least one of them might have the time to help you out. 
Ask them to talk you through their code. Ask them what it does and why they chose to do it that way as opposed to another way. Maybe keep the code to sample for later projects. Practice applying what they did.
R like Excell is a toolbox and many of the skills you learn from one project can be applied to later projects. Learn what things are called and learn how to ask questions/Google.

I hope that helps.",1522450991.0
questionquality,"If you already read through the R cookbook and like dplyr, you will probably learn a lot from [R for Data Science](http://r4ds.had.co.nz/)",1522455616.0
,[deleted],1522459449.0
pipie314,"I skim read the code, but this can count a game as both a win for both players?",1522476749.0
AllezCannes,"Thanks for sharing, this is very detailed and useful.",1522437031.0
linyeah,it looks interesting thanks - do you have a brief run down of what you would consider required knowledge to follow this? ,1522507482.0
tomQ11,"Are there any missing values in the column you're taking the mean of? You can ignore them by adding the parameter na.rm = TRUE to mean(). So mean(data, na.rm= TRUE). ",1522366482.0
klo99,"Do you want to obtain the mean of Trial_Accuracy in the group with Prediction==1?

library(tidyverse)
average600_0 %>%
filter(Prediction==1) %>%
summarise(mean(Trial_Accuracy))",1522370835.0
malenkydroog,"In the coda package, there are the ""as.mcmc"" and ""as.mcmc.list"" functions. Use those to convert your output arrays to mcmc objects that coda can deal with. IIRC, you should first convert each of your arrays to an mcmc object using the first function, then coerce a list of those to an mcmc.list structure that you can use to look at convergence, etc.",1522357314.0
,[deleted],1522367370.0
questionquality,"In general, the *apply functions in base R are quite inconsistent in what formats they return, which is why [purrr](http://purrr.tidyverse.org/) (part of the tidyverse) exists. In purrr, functions like map (or map2 with 2 inputs or pmap with more inputs) are very similar to *apply, except you always know what type they'll return. For instance, map_dbl always returns a numeric vector. In this case, since you want to return a row, you can put that in a [list column](http://r4ds.had.co.nz/many-models.html#list-columns-1) with just map2 (no extra suffix means return a list).

    library(tidyverse)
    x1 <- mutate(x1, map2(pcorr, steps, run.rcwalk)) %>%
      unnest()
",1522337547.0
sat1vum,"You could use lapply instead, you just have to move stuff around a bit. 

    t <- 10000
    sims <- 10
    p.corr <- 0.7
    
    run.rcwalk <- function(sim, p, t) {
      x1 <- 2 * as.integer(runif(t) < p) - 1
      x2 <- c(0, cumprod(x1))
      pp <- cumsum(x2)
      data.frame(""trial"" = sim,
                 ""pcorr"" = p,
                 ""steps"" = t, 
                 ""end"" = pp[t+1],
                 ""min"" = min(pp), ""max"" = max(pp),
                 ""minstep"" = min(which(pp==min(pp))) - 1,
                 ""maxstep"" = min(which(pp==max(pp))) - 1
      )
    }
    
    x1 <- do.call(rbind,lapply(1:sims, run.rcwalk, p.corr, t))
    summary(x1)
        ",1522353608.0
oggesjolin,What software are you using?,1522354966.0
Hasnep,"The word to Google is ""limits"". ;)

You can do this in ggplot with:

    + scale_y_continuous(limits = c(0, 20))

or with the shortcut

    + ylim(0, 20)",1522322408.0
efrique,Indent your code 4 spaces. ,1522321765.0
mr_matzoball,"Lists may help here which you can index by the variable names. They'd operate similar to a dictionary in Python. 

E.g.

Var.names = list(X1 = ""question 1"", x2 = ""question 2"")

Var.names['X1'] would return ""question 1""",1522319748.0
refined_compete_reg,"You could make a key then swap out the veriable names near the end after most of your mumging is done.

Something like:

names(data) <- key$more.complicated.names

As long as you are carful to create your key such that it is the same length and order as your veriable names this should work.

OR you can rename in dplyr

OR I find it is easier to make a desriptive key and then name my variables shorter than your example but more descriptive than x.1.2.  this way your code is more understandable and you can add consistent and desriptive language to any output such as graphs or summary tables.",1522321340.0
KallerTobias,I don‘t know if I unterstand your Problem. But did you tried to use factor variables? ,1522314218.0
agclx,The [attr](https://www.rdocumentation.org/packages/base/versions/3.4.3/topics/attr) function could help. You could just add a field of full names and then in your functions flip them around as needed. Of course you‘d need to take care when colums are altered that the field stays in sync. ,1522397153.0
metagloria,Plot a white polygon over the lower ones. But why? This seems like a fishy thing to do in almost any context...,1522332628.0
RaggedBulleit,"Plot violin, plot your quantiles (or box) without outliers, then plot your outliers filtered from the data as points.

Bonus points: call a box - in - violin plot a box-in-box plot.",1522335179.0
arvi1000,"plot a boxplot with no outliers (`outlier.size = NA`), then plot the upper outliers (which you can get from `boxplot.stats()`) as an overlay with geom_point. 

but i agree this is a fishy use case :)",1522338413.0
,[deleted],1522300998.0
ran88dom99,"You will probably need 
coord_cartesian(ylim=c(highest,lowest))

Change highest max() and lowest to 75% see quantile().",1522426866.0
singularperturbation,"I don't know of a good turn-key, ""just use swap"" solution, unfortunately.  You might want to check out some of the packages in the ""Large memory and out-of-memory data"" section of https://cran.r-project.org/web/views/HighPerformanceComputing.html , though.  I think it depends on your task, mostly.

TIL about big.pca, I'll keep that in my back pocket. :)",1522299220.0
Deleetdk,Use `MASS::mvrnorm()`.,1522284451.0
,"Why do you assume that demand is normally distributed (unless I'm reading your code wrong)?  

This sounds like it could be a service level calculation (https://en.wikipedia.org/wiki/Service_level).  You may also want to learn about safety stock calculations see (https://en.wikipedia.org/wiki/Safety_stock).  

I've never done these in R, but I imagine there is a library to do all of this for you (if not, feel free to write one).",1522283748.0
DrHampants,"I'm a little confused as to what the main question is. It seems like you have two things going on here:

1. Find an estimated stock that will fully satisfy demand 95% of the time (You found this to be 115).
2. Test the stocking level to see if, given a random sample of demand, this amount will achieve the target fill rate. In other words, if you stock 115 units, will this meet actual demand 95% of the time?

If I'm reading this right, it seems question number 2 is really more of a binary question: in each instance, you either meet demand or not. If that's what the question is asking, wouldn't mean(fillrate4) give you the answer?

Any chance you could rephrase the question so it's a little more clear?",1522283517.0
JackOneill,"    subset(data, cname %in% c(""Germany"", ""Greece"", ""Italy"") )",1522272443.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/rstudio] [Choosing specific countries from a large dataset](https://www.reddit.com/r/RStudio/comments/87vqa1/choosing_specific_countries_from_a_large_dataset/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1522272538.0
dastram,"I would do it with the dplyr package. it let's do you some stuff in an easier more intuitive way.

    library(dpylr)
    
    filter_countries=c(""Denmark"", ""Finland"")    
    filtered_dataset=filter(dataset, cname %in% filter_countries)

filter() filters everything where the condition is TRUE. The thing after the comma is the condition. It checks for every row of cname if the string can be found in filter_countries. 

Does it make sense? let me know if it works",1522272777.0
elbevee,I figured it out - I didn't have the parenthesis after `studyModel` in my `predict` function!,1522268524.0
fonzy6,I know Stan can do this but unsure how.,1522237731.0
,[deleted],1522219700.0
oscjm,Well done! Thank you,1522197927.0
infrequentaccismus,Excellent work. You’re my hero. ,1522204391.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/rstudio] [coxph case weights](https://www.reddit.com/r/RStudio/comments/87w9lr/coxph_case_weights/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1522276999.0
JaceComix,"Look up how reactive elements work in Shiny, then create a reactive subset to build the plot from instead of building it from your complete data set.  
Reactives are a little tricky at first, but it shouldn't be too bad for your project.  
Let me know how it goes!",1522190700.0
kenderpl,Sure - you would need to filter the data frame according to your needs and plot that. If you put the data as reactive you will have the plot recreate itself when the underlying data gets changed.,1522189503.0
spinur1848,"That would be a pretty tall order. HL7 isnt one standard, its a whole family of standards.

What kinds of messages are you interested in parsing? How do you plan to handle authentication?",1523965047.0
colorlace,"This doesn't answer your question exactly, but I've found this to be the best visual explanation on the web.

http://setosa.io/ev/principal-component-analysis/",1522120083.0
dtrillaa,Boston Housing (which is super easy to find) is a good data set since a high amount of collinearity  exists between the predictor variables ,1522121969.0
anotherep,"[Here are my notes, which uses both random data and the iris dataset to work through PCA](http://rpubs.com/bds1217/PCAtutorial). I tried to show manual linear algebra calculations alongside the basic R PCA functions to show how PCA actually works. ",1522161215.0
anthracene,mtcars if you have a basic understanding of how cars work.,1522178767.0
dtrillaa,"It’s much easier to classify one of your outputs as 1, and the other as 0. (EG Male = 1, Female = 0)

The reason being that GLM regressions will output the probability of being a 1. R will then classify your data such that p<.5 are 0’s and p>.5 are 1s.

If you want to use a GLM I suggest logistic regression. The code in R to do so is:

    model <- glm(sex ~ ., family = binomial (link = ‘logit’), data = yourDataSet)

This will produce the output for you, and give you your coefficients when called.

Here’s a link to see a demo and explanation [in R](http://r-statistics.co/Logistic-Regression-With-R.html) ",1522129735.0
shujaa-g,Maybe just add an index to your data? `facebook$id =1:nrow(facebook)`. Then you can use that as your x-axis? Hard to tell without seeing any sample data...,1522100703.0
scbagley,"1. What happens if you remove the call to `facet_wrap`?
2. Are you sure that `year` is a column name in your data.frame?
3. Is `year` bound in the global environment?
4. Also, you can format your code when posting here to make it easier to read. Like this:
   
        ggplot(orlando_combined, aes(x = (dist/1000), y = lq)) +
            geom_smooth(span = 0.3, method = ""loess"", color = ""blue"") +
            geom_hline(yintercept = 1, color = ""black"", linetype = ""dashed"") +
            theme_minimal() +
            facet_wrap(~year) +
            labs(x = ""Distance from Downtown (km)"",
                 y = ""Concentration relative to metropolitan area"",
                 caption = ""Data source: 1990 Census and 2012-2016 ACS"",
                 title = ""Home Owners in Greater Orlando Area"") +
            theme(plot.caption = element_text(size = 6))
    ",1522111975.0
_nt2,"You mention the error has kept you up all day. Aren't you normally up all day? Perhaps you are nocturnal? 

Anyway, your code contains quite a bit that is irrelevant, and you don't give us any idea as to what your dataset (`orlando_combined`) has in it. Your code is syntactically valid, so it's nearly impossible to help.

If I make up some nonsense data with the relevantly named variables as follows:

    orlando_combined <- data_frame(dist=rnorm(100), lq= 1 + dist + rnorm(100, 0, 0.1), year = factor(sample(1:2, 100, replace = TRUE)))

then your code runs just fine. 

So perhaps it is something with your data. ",1522170335.0
ReimannOne,/r/homeworkhelp,1522118280.0
efrique,"Understanding the output is not a programming-related question, it's a stats question; it's off topic on stackoverflow. 

It's on topic on stats.stackexchange.com (but has very likely been asked and answered there already -- search first).

For example, these may be of at least some use to you:

https://stats.stackexchange.com/questions/82532/logit-link-glm-summary-interpretation

https://stats.stackexchange.com/questions/185391/interpretation-of-two-glm-model-summaries

but a more targeted search than I used may find more useful things.

some example searches:

https://stats.stackexchange.com/search?q=interpret+glm+%5Br%5D

https://stats.stackexchange.com/search?q=summary+glm+interpretation

You can also do a site search via google that sometimes turns up things the built in search doesn't (e.g. sometimes you'll locate a helpful pointer in comment)

You may want to flag your SO post for migration to stats.SE (don't cross post)
",1522098251.0
Pine_Barrens,And RStudio instantly becomes probably the best IDE editor for Python....,1522081268.0
efrique,I need this ... for reasons. ,1522101288.0
Arsonade,I picked a very good time to start learning python,1522108330.0
fasnoosh,"What are the key libraries in Python that don’t have an R equivalent that would make this package indispensable? Was talking about it with some guys at work today, and curious to get started w/ it, but I’m not very well versed in Python",1522118243.0
vsonicmu,Wow....The rstudio folks are absolute rock stars. Especially JJ Allaire. ,1522088061.0
venoush,Does anyone know of a Python distribution which allows installation on a shared network drive (without write access granted to users) and user package libraries... like R does?,1522139523.0
HawkUK,I've found this incredibly useful when needing to interface with some services that require a Python library.,1522146138.0
AlisonByTheC,"Will reticulate install Python itself or do I need to have a conda or Python installation already? This part is kind of unclear to me.  

I see it trying to latch onto existing Python installations with my personal PC but I have a situation on a work machine where I can’t control what gets installed without a need for an admin to get involved every time.   


Edit: autocorrected package name. ",1522150102.0
metabyt-es,"I frequently go the other route, using rpy2 and the %rmagic command in Python-kernel Jupyter notebooks. Gives me the best of both worlds.",1522163923.0
RememberToBackupData,"One key aspect where Markdown-based solutions cannot (easily) replace Word is with reviewing, commenting, and change-tracking. I am aware that there are solutions for this out there, but it requires buy-in from everyone involved in terms of installing and learning the new workflow. This is the biggest reason why only my first draft gets done in Markdown and all the editing and revision is done in Word, because when I distribute the document to co-authors, I need to be respectful of their time. ",1522037695.0
spectrum_specter,"I think that it's possible, but entirely too early and pretty unlikely.  

As you said, all the functionalities of the office suite can be more or less replaced by R.  

However, R's popularity has only recently skyrocketed to the point where a random person may have a good chance at knowing what it is.  

Given that, the amount of learning and aptitude necessary to master R to the point where it replaces the office suite would be prohibitive for many.  

There are many individuals in my office that have been doing things the same way in Excel for 10 years. It works for them, and the added benefit of doing it a new way in R simply wouldn't be present.  

Not everyone needs to know how to build a car given the requisite parts, some people simply need to know where windshield wiper fluid goes, and some simply need to know how to turn the key, press the pedals, and turn the wheel (oversimplification). The latter's operation of the vehicle wouldn't be aided by knowing how to put the car together.  

I'll use R to clean dirty data for my coworkers to use. Does it matter to them how I fixed their car, so long as it functions properly? Not really.",1522038474.0
samclifford,"I work in a stats group that is very heavily using R. Not everyone uses RStudio but a number of the early career researchers are using R markdown for blogging, writing teaching material, producing package vignettes, etc. Using github and R Markdown knitting to PDF and natbib referencing is certainly preferable to Word and Endnote. ",1522067168.0
brews,"You could follow the R bloggers planet. It is an aggregate of posts, including underdog sites:

https://www.r-bloggers.com/",1521995483.0
factotumjack,"Eric Cai, the Chemical Statistician
http://www.statsblogs.com/author/eric-cai-the-chemical-statistician/

Andrew Gelman
http://andrewgelman.com/

Quick-R. Not a blog per se, but with short, readable pages like one.
https://www.statmethods.net/

My own blog, filtered for R posts.
http://www.stats-et-al.com/search/label/R",1522040260.0
lakenp,"http://blog.revolutionanalytics.com/ 
and
https://paulvanderlaken.com/category/r-2/",1522055293.0
IlyaKipnis,I have a blog on systematic trading: http://quantstrattrader.wordpress.com,1522088963.0
vmsmith,"When I first started getting into R, circa 2103, most of what was written in blogs and such was written about base R. Maybe those writers have stopped writing or have moved on. But if you Google, you can certainly come up a lot of those old posts. 

And I would guess that if you searched R-Bloggers you could find a lot. 

Furthermore, StackOverflow is full of old posts addressing base R issues.

I have 24 dedicated R blogs feeds in my Feedly. When I get a few minutes I'll take a look and see if any of them might be appropriate.  ",1521917817.0
BehindBrownEyes,Magic of R is its packages. Why ignore them?,1521917589.0
RememberToBackupData,"If you ask for a base R answer in Stack Overflow, people seem to oblige you. ",1521927120.0
rustydrd,"You can check out Dirk Eddelbuettel's blog. Often Rcpp related, but generally using base R. http://dirk.eddelbuettel.com/blog/",1521933794.0
tylermw8,"There are bloggers out there who just use R to do interesting things and don't explicitly make it about the tidyverse, even if the tidyverse is used. Is that what you're looking for?",1521933905.0
GracelessMethodology,Karl Broman ,1521938503.0
insomegucciflipflops,Richard Landers just came out with a (free) datacamp course wherein the first 6 or so videos talk about base R (you may very well already know this more “foundational” material though),1521938941.0
AllezCannes,"There's Flowing Data, although I think lately Nathan Yau has been migrating away from R towards D3 and JS.",1521962017.0
ryapric,"I think it depends on if whether you're asking explicitly to learn about other uses of base R, or you're looking to learn more about base R in the first place.

If the latter, then the incredible irony is that [Hadley Wickham actually wrote a book on base R](http://adv-r.had.co.nz/), and it covers a lot of its inner workings, tips & tricks, functionals, etc.

I actually use the tidyverse in production quite a lot, and have had my share of woes when updates break things. Though I'm now better prepared for if that happens, I've gotten into the habit of using dependency-free code where possible.",1522002702.0
IlyaKipnis,I use the R/Finance stack and data.table occasionally. I do not use tidyverse because it screws up my analysis.,1522089916.0
Help_Quanted,"Why would you use a for loop when you can use a vectorized option that’s much faster? Build a scraping function, run the function through lapply, do.call rbind into a data frame. ",1521945760.0
edimaudo,You could look at rvest library,1521906353.0
prashanthsriram,"I got a bit tripped up by ""EPS next Y"" appearing twice - tidyr::spread threw an error because of that. But I finally got it working. Note: I tend to use a lot of tidyverse packages.

    library(rvest)
    library(purrr)
    library(dplyr)
    library(tidyr)
    
    stock_names <- as.list(c(""AAPL"", ""MSFT"", ""GOOG""))
    stock_pages <- stock_names %>% map(~ paste0(""https://finviz.com/quote.ashx?t="", .)) %>% map(read_html)
    stock_tables <- stock_pages %>% map(html_nodes, ""table"") %>% map(`[[`, 9) %>% map(html_table, fill = TRUE)

    tidy_it <- function(stock_table, stock_name) {
      stock_table[5, 5] <- ""EPS next Y %""
      return (cbind(tibble(""stock"" = stock_name),
                    stock_table %>% select(1:2) %>% spread(X1, X2),
                    stock_table %>% select(3:4) %>% spread(X3, X4),
                    stock_table %>% select(5:6) %>% spread(X5, X6),
                    stock_table %>% select(7:8) %>% spread(X7, X8),
                    stock_table %>% select(9:10) %>% spread(X9, X10),
                    stock_table %>% select(11:12) %>% spread(X11, X12)
      ))
    }

    tidy_tables <- stock_tables %>% map2(stock_names, ~ tidy_it(.x, .y))
    all_stocks <- bind_rows(tidy_tables)


Here are the first few columns of the result:

      stock Book/sh Cash/sh Dividend Dividend % Employees Income       Index Market Cap Optionable Recom   Sales Shortable Current Ratio Debt/Eq
    1  AAPL   27.42   15.15     2.52      1.53%    123000 53.13B DJIA S&P500    839.87B        Yes  2.10 239.18B       Yes          1.20    0.87
    2  MSFT   10.16   18.62     1.68      1.93%    124000 25.39B DJIA S&P500    668.29B        Yes  1.80  98.86B       Yes          2.90    1.14
    3  GOOG  219.50       -        -          -     80110      -     S&P 500    695.60B        Yes  1.50 110.85B       Yes             -       -",1521910657.0
bobbyfiend,"PCA is for reducing dimensionality in multi-item datasets, like finding which questions in a questionnaire statistically cluster together, or identifying which stars in an astronomical search might be from the same galaxies. I'm having a hard time thinking of using it this way, though maybe that's just my limitation.

Your situation seems much more straightforward: you have the dataset, and each case either has the molecule or not, and either has a disease or not. This is a classic setup for what you want to do, I think. You can approach it multiple ways:

1. Use an ANOVA or regression variant with binary predictor (because the molecule y/n variable is binary) or flip it around and have a binary outcome (i.e., the molecule). That would be one way to approach an answer.
2. Signal-type analysis, probably involving a receiver-operator characteristic (ROC) curve and calculating area under that curve (AUC), as well as determining optimal cutoff points. Probably you'd have to do this once for each disease versus control pairing.

I'm sure I'm missing something; my brain is saying I'm not thinking about this carefully enough and it's really late. But that, at least, seems to be a reasonable little list of suggestions.",1521866564.0
RememberToBackupData,"Decision tree is most obvious and practical solution; a doctor can look at the tree and follow it down to a possible diagnosis. You can do a random forest for better predictions, but then the doctor needs to be able to use a computer to interact with the model, or you need to extract a representative tree out of it. ",1521956845.0
thaisofalexandria,"Multinomial logistic regression.  There is a nice example at

[Multinomial logistic regression](https://stats.idre.ucla.edu/r/dae/multinomial-logistic-regression/)",1522070903.0
samclifford,This would normally fall into the remit of the broom package but I'm not sure if it's been done for rpart models. Perhaps https://github.com/njtierney/broomstick? I used to work with Nick and he's a big fan of broom tidiers and tree models. ,1521850251.0
Ader_anhilator,Sounds like you need someone with more experience to solve the problem.,1521865122.0
Hefeystus,"Ok first of all,

You explained your data badly. Instead of writing "" the important variables are: x y z"" you tacked on literal gibberish, which made it harder to read. Additionally, you didn't define what you meant by branch. Supposedly that would be ""obvious"" but some context would make this much less annoying.

Who knows? If you gave an example of your data and described it beyond ""daorj owjdo 123 asdf"" then maybe we'd be able to figure that out. The answer is ""I guess so"" if there is some trend/seasonality. 

There's a lot to not like about this post but tl;Dr /tl;dw: Don't ask questions assuming the anonymous person on the internet knows the context for which you're using something. 

If you had taken the time and effort to phrase this is an easy to digest way, where since you imply a degree of mastery of the exact implementation and process, you should be able to reduce this to a much more specific question.

Finally, google the models you used. They've been documented and discussed to death, if you still have a question after really working with all the easily available material about your models, then you should come back and pose a better question.
",1521862996.0
Er4zor,"Maybe you're using cached chunks?   
Try deleting the cache, if any, or force rebuild by adding this chunk at the start:

    ```{r setup, include=FALSE}
    knitr::opts_chunk$set(cache.rebuild = TRUE)
    ```",1521847803.0
huessy,"Try clearing your environments.sometimes if you run stuff in console,it refers to a variable with the same name that you created earlier. As I recall, code run in markdown doesn't overwrite things you ran in the console. Could be wrong, but worth a shot.",1521848201.0
Katdai,"Are you sampling? Did you set a seed? If you run it twice in the console, do you get the same result?",1521859868.0
infrequentaccismus,"Give me a small reproducible example of what you are trying to accomplish.  What exactly are you trying to combine?
",1521843288.0
infrequentaccismus,"I pasted together the 4 columns that represent the project essay, which is what I think you meant.  I used the traditional paste() function, unite() from tidyr, and str_c() from stringr.  str_c is MUCH faster than the base function and unite is about the same (because it uses paste() and adds a small amount of overhead to make it slightly simpler to use).  The stringr function took about 0.05 seconds on this dataset while the other functions took about 0.8 seconds.  See my code and the benchmark results below:


    library(tidyverse)
    library(microbenchmark)
    
    df <- read_csv(""~/Downloads/train.csv"")
    
    microbenchmark(
      df %>% unite(project_essay, project_essay_1, project_essay_2, project_essay_3, project_essay_4, sep = "" ""),
      df %>% mutate(project_essay = str_c(project_essay_1, project_essay_2, project_essay_3, project_essay_4)),
      df %>% mutate(project_essay = paste(project_essay_1, project_essay_2, project_essay_3, project_essay_4, sep = """")),
      times = 100L
    )
    
     #      min        lq      mean    median        uq       max neval cld
     # 835.8428 838.78284 861.72850 841.92676 842.02309 950.06699     5   b
     #  48.4862  49.57353  49.66089  49.74793  50.23766  50.25912     5   a 
     # 816.9082 826.70634 853.25693 832.06367 847.91377 942.69268     5   b",1521846058.0
clamiam45,"Sorry I have no knowledge of an R interface to Redis, but if you need a stopgap then it's possible you can wrap your work in [reticulate](https://github.com/rstudio/reticulate).",1521863233.0
shujaa-g,"I'd recommend looking at the table of proposed projects for ones identified as ""need students"", and contact the mentors.

https://github.com/rstats-gsoc/gsoc2018/wiki/table-of-proposed-coding-projects",1521824403.0
ossicones,"I'm in graduate school for applied economics and had to resort to Stata when I was taking an econometrics class. D:

There are/were lots of econometric functions missing from R, but one thing that springs to mind is that there's not a good replacement for the `nlcom` command in Stata. ",1521835094.0
guepier,"> Is there a way where I can make the program execute without ever opening R

No, because it’s an R script. It by definition needs to open R. However, you can make the script self-contained so that the interaction with R happens invisibly to the user. How this works depends on the operating system.

On UNIX-like systems, the usual way is to [make the script executable](https://stackoverflow.com/q/8779951/1968), and [add an appropriate shebang line](https://stackoverflow.com/q/43593272/1968).

On Windows, you can use the system registry to configure the `.r` file extension such that opening an R script by default runs the R interpreter (rather than the Rgui.exe, or an editor). However, this would change the behaviour for *all* R scripts on that system. An alternative is to create a command script (`.cmd` file extension) that invokes `Rscript.exe` with your script as its argument, and forwards further arguments. Then, dragging a text file onto the script will execute it correctly.

You can also wrap the script into an executable bundle — this is especially common on macOS but [it’s *a lot* more involved](https://mathiasbynens.be/notes/shell-script-mac-apps), and not really necessary.",1521821641.0
fencelizard,"You could build a shiny app (https://shiny.rstudio.com/), run it locally in a web browser, and bookmark the local address. You'd still have to open rstudio to start the app every time you shut down the computer though. ",1521826387.0
coldflame563,There's a way to construct it as a batch file so it'll be easy for users to just click and it'll go. ,1521844928.0
CJL_LoL,"We use batch files for a couple of our scripts that don't run from azure, still requires the user to have r installed of course",1521897264.0
jowen7448,Check out Rinno,1521899592.0
jjdonald,"It looks like an improved set of system libraries for R.  The endgame seems to be to wrap some of the new cross-platform async libraries (libuv), and start deprecating some of the old custom cross-platform code. 

Something similar is happening over at neovim, FWIW.  They're cleaning up the old vim codebase, and relying a lot more on libuv rather than maintaining their own fs/async functionality.",1521838964.0
valen089,It is primarily for package developers.,1521808836.0
RememberToBackupData,"Yes you are: A function can only return one object.

> If value is missing, NULL is returned. If it is a single expression, the value of the evaluated expression is returned. (The expression is evaluated as soon as return is called, in the evaluation frame of the function and before any on.exit expression is evaluated.)
> 
> If the end of a function is reached without calling return, the value of the last evaluated expression is returned.

 So either:

1. Put both plots into a `list()` and return the list, storing it in a variable and accessing each entry to get each plot, or
2. Use a package like [gridExtra](https://cran.r-project.org/web/packages/gridExtra/index.html) to stitch both plots into a single plot, and then return that.",1521777241.0
thedukeofedinblargh,"/u/RememberToBackupData is right, assuming you want to see them in the session.  However, if it would make sense for your workflow, you could also save each of the plots to an image file using [ggsave](http://ggplot2.tidyverse.org/reference/ggsave.html), all within the same function.  Then you'd have them for embedding in other documents or flipping through them with Preview or something.",1521778336.0
shujaa-g,"u/RemeberToBackupData covered the misconceptions well. I would recommend having the function take a print argument and then in the body
    
    your_function = function(..., print_plots = TRUE) {
      ...
      if (print_plots) {
        print(plot_1)
        print(plot_2)
      }
      invisible(list(plot_1, plot_2))
    }

~~invisible works like return~~ but it won't try to auto-print the result  (which won't work for a list of ggplots). But you can still assign the result to an object and modify, save, print later, etc. **Edit:** See guepier's comment below - `invisible` isn't like `return`, but it will prevent the object from being auto-printed.",1521783846.0
psychfi,"The psych package has ""describe"" and the pastecs package has ""stat.desc."" Both of them work pretty well for my purposes and can be used with dataframes. For the pastecs function, include arguements of basic = FALSE and norm = TRUE to get it to be more like the SPSS output. ",1521762759.0
ING_Chile,Just base R functions like summary() or quantile(). Those are enough for basic descriptive stats,1521763520.0
Darwinmate,`skimr` package is really good as a substitute for `summary` function. It prints out a really nice summary. ,1521768661.0
slammaster,"Hmisc has a describe function that I quite like, it typesets in LaTeX well too",1521764322.0
RememberToBackupData,"This is not descriptive statistics, but I just wanna give a mention to [visdat](http://visdat.njtierney.com/index.html). It returns a ggplot that summarises observations, variables, variable types, and missing values. I have been using it *a lot* since I started modelling.

http://visdat.njtierney.com/articles/using_visdat_files/figure-html/load-data-1.png",1521777711.0
berf,"Sometimes you just have to reinstall all the packages.  This is CRAN policy.  If they never broke any package compiled long ago, they could never use any new features.  They would be stuck back in 1990.",1521756551.0
agclx,This can be tricky to track down.  I found it very useful to use the checkBuilt parameter like: `update.packages(checkBuilt=T)` to track down problems.  But I still had a few odd ones I had to recompile manually.,1521794185.0
coip,"> When I use the aggregate command...I get the error that the replacement has fewer rows than the data frame itself. 

If you want to use base R, you can get around this by first storing the aggregate results in a separate object, say `Agg` and then merging the results back into the original data frame via `merge()`.",1521776161.0
hummingbirdz,"dplyr is a good place to start.

    data <- data %>% 
       group_by(identifier) %>%
       mutate(Agg = mean(var))

In general, for package development and some performance edgecases you may want to avoid dplyr and the pipes, but for basic processing before running a model, or just interaction exploration and graphing dplyr imo dominates the base R tools for data cleaning/aggregating.
    
",1521757813.0
mr_matzoball,"data.table

DT[,average := mean(.SD), by = identifier, .SDcols = c('vars')]",1521765999.0
RememberToBackupData,"The most important thing to remember about writing comments:

# Comments should explain, not describe.

This is a waste of time and space:

    # Multiply Tips by 0.2
    mutate(mmRain = Tips * 0.2)

This is useful. In particular, it is always a good idea to comment on 'magic numbers'.

    # Each bucket tip marks 0.2 mm of rain.
    mutate(mmRain = Tips * 0.2)

You could negate the need for comments by using an extra variable. I would do this if I wanted other people to edit the script easily, but not for my own use.

    bucketCapacity <- 0.2  # mm
    mutate(mmRain = Tips * bucketCapacity)

And of course, functions can also be self-documenting:

    tips_to_mm <- function(bucketTips, bucketCapacity) {
        return(bucketTips * bucketCapacity)
    }

The reason why newbs keep making the first set of comments is that these kinds of comments are often used in crappy books to explain code line-by-line to the reader. The books don't explicitly say that this commenting style is bad, so the hurt is passed on to the next generation.

---

Other good reasons to leave an explanatory comment:

1. **To explain why you've done something in a non-obvious way.** For example, I used `paste0()` to concatenate two numbers, and I explained that I'm using the numbers as the levels for a new factor, and multiplying or adding them would create duplicate levels.
2. **To explain your strategy.**  Like /u/shujaa-g, I also outline the steps of a script or function. I'll often start a function by pseudocoding its workflow in steps, and then keep each step to describe the goal of each code block.
3. **To explain where external objects come from.** Datasets, magic numbers, the reason why the plants that I identified as *P. tuberculata* are now being changed to *P. ferruginea*.
4. **To give credit.** If I've gone to Stack Overflow with a question, I'll paste a link to the answer in a comment. You can also add them to the footnotes of an R Markdown doc.",1521745510.0
shujaa-g,"All functions should be commented in a Roxygen-compatible style - nice and consistent, easy to package up later. At a minimum, have a one-line description of the function, identify the data types for the inputs and the returned value.

I like to do a rough outline of the goal of a script and the general steps it will take at the top, and then reference those steps (sort of like a table of contents and chapters).

Other than that, hopefully your variable names and coding style is clear enough that not too many line-by-line comments are needed. Call out exceptions with a brief explanation, flag incomplete or future tasks with `#TODO`.

If you find yourself annotating your script with output (pasting from the console into a comment) you should probably be writing an Rmd document instead.",1521736276.0
IamGrabear,Use docstrings from the start.,1521813529.0
ReimannOne,"You might want to take a closer look at your goal data.  Is the math right?

If not, this will work and you don't need to do any joining.

    df1$period <- floor(df1$day/30)",1521736656.0
infrequentaccismus,"Obviously u/reimannone has the best answer for this use case, but if the bins were not equally sized, then you could use a case_when() statement to create a new column with the join information on it. If there are too many bins to make that practical, then you can mutate a lag column to test for the condition of being between one column and another ",1521783787.0
,Usually R is good at halting operations that are written in pure R. If your code makes calls to compiled binaries then they are harder to halt,1521733707.0
tbc21,"I've noticed that R(studio) is DIRE when it's running a script (or interacting with data) from a non-local location.
It's generally under these circumstances that quitting an action causes R to terminate for me. ",1521738761.0
picardIteration,"The trick is to never have to stop it by only writing correct, efficient code",1521734260.0
dtrillaa,Which IDE are you using? I’ve never had such a problem in R Studio,1521733899.0
guepier,"This isn’t R’s fault (and as explained in other comments, R actually handles this very well). You *fundamentally cannot* simply abort a computation without crashing the process. This is simply how processes work.

In order to gracefully abort a computation without crashing the process, that process actively needs to do extra work (namely, it needs to listen for specific interrupts and handle them accordingly). R provides the necessary tools for this but if library implementors that write C, C++ or Fortran code don’t use these tools, there’s literally nothing R can do, short of crashing itself.",1521800247.0
greenspans,"Run it from the command line as an R script rather than R studio. Or have Rstudio run a script that immediately forks the process and captures the process ID. Or have it just spawn a bash script in another window.

The problem can also be memory. If your script eats the rest of your ram, R studio is going to die. 

RStudio is java based. Java will die if you exceed the max heap size. Probably there is way of starting Rstudio with ""java -Xmx32000m"" or however much ram you need.",1521804074.0
PM_ur_good_deeds,"Without knowing any details, it's hard to say.",1521705084.0
MarijnBerg,"You could run multiple instances of R in parallel on subsets of your input if that's an option.

And how you handle the output also matters. No cbind or rbind in your results handling loop and the like.",1521707080.0
shujaa-g,"If you are running the function multiple times, you can run it in parallel. Otherwise you can profile the function and try top optimize it.",1521721187.0
BerryGuns,"If you click chapter 2 here is the data still missing?

http://worldhappiness.report/ed/2017/

Gave me what seemed a fairly thorough spreadsheet",1521709837.0
abandoningeden,try ipums.org ,1521718515.0
jkapow,"Are you looking for cross section or panel? World Values Survey has a short panel, but you might need to construct from individual respondents.

Send me a message with more details of what you're looking for. ",1522184255.0
shujaa-g,"It's because you are passing `data = hyp` inside `poly()`. It needs to be an argument to `lm`.

    lm(y~poly(x,2,data=hyp)) #bad
    lm(y~poly(x,2),data=hyp) # good
    lm(y ~ poly(x, 2), data = hyp) # better - use whitespace make code readable",1521667262.0
efrique,In the later ones you told `poly` where your variables are -- you're not telling `lm`,1521674156.0
anotherep,"1. Do you have a specific question? 

2. You need to include a sample of your data (`studyData`). Can't troubleshoot without it unless I take the time to make up my own.

3. Can't tell if this is a copy paste problem, but your variable names shouldn't have spaces (unless you refer to them flanked by single quotes). So `Walk In` should be `WalkIn`, `Walk.In`, `Walk.In`, or whatever

4. Just looking at the code, my guess is that if you are having a problem, it will be with the reactive environment of your app. 

",1521678857.0
overemotionalclam,"Not the most efficient way since you'd be loading all the data into memory, but there are cases when it could be useful: you could load the csv as usual with `read.csv()` and then use the dplyr package to slice and dice any which way you want. For example:

    data <- read.csv(""file"", header = TRUE)
    data <- data %>% filter(Col1 >= 40)",1521664808.0
,[deleted],1521663699.0
dtrillaa,"As a general rule, you want to use functions created by Hadley Wickham in the “tidyverse” (the filter function mentioned above is one of them). There are lots of different functions and packages for doing virtually the same thing, but the tidyverse functions are generally accepted as best practice when it comes to coding in R",1521687279.0
pina_koala,"I think you've jumped in the deep end before mastering the shallow part of the pool. Gradient descent isn't really a ""basic concept"". Yes it's used widely but this article basically goes from 0-60 here by promising that you can just do a little dance and make gradient descent happen. 

Based on your first question:

>Firstly why does the learning rate parameter make the results so volatile? If I increased it one significant figure the sign of the coefficients change and the results are just wrong

I think you would benefit from some more basic introductory concepts. If you are puzzled by the volatility introduced by changing the significant figures then this demonstrates to me that your foundational knowledge needs shoring up.",1521648609.0
,"Aren't the variance and mean of a Poisson distribution directly linked, so that the 95% CI can be defined analytically? Why is this an R problem at all?",1521620528.0
inmanenz,Why cant you create a loop and adjust maxdepth until you get your preferred number of variables?,1521642710.0
ReimannOne,"First of all, thanks for posting you question with code, a reproducible example, and error messages.  


It looks like `mapdist()` is already vectorized, so there's no need for the sapply function.  I got this to work by using

    mapdist(as.character(zip_code_data2$Zip2), '18466', mode = 'driving')

That will give you a data.frame as a result:

    from           to      m      km     miles seconds   minutes     hours
    1 19380 philadelphia  57007  57.007  35.42415    2652  44.20000 0.7366667
    2 08096 philadelphia  21366  21.366  13.27683    1397  23.28333 0.3880556
    3 06437 philadelphia 307841 307.841 191.29240   11750 195.83333 3.2638889
    4 19540 philadelphia  90137  90.137  56.01113    3786  63.10000 1.0516667
    5 08094 philadelphia  40619  40.619  25.24065    2102  35.03333 0.5838889

You can assign that a name, and subset or filter to get the specific entries that you need.


Sometimes google map api calls don't work from the computer you're using due to your ip address.  The api only allows 2500 calls per day, and if you share an ip with other people making the requests, you might get an error.  Try a VPN, open hotspot, or tether to a phone if this happens.  ",1521625341.0
PandaMomentum,"Just parking this here -- if, for some reason, one had to code distances for millions of ZIP Codes (and thus blowing up the Google API), and didn't have the budget to buy a GIS package with ZIP Code centroids, you can (very closely) approximate using [US Census ZCTA data](https://www.census.gov/geo/maps-data/data/gazetteer2017.html) and a simple [haversine distance function](https://www.movable-type.co.uk/scripts/latlong.html). ",1521653113.0
AlisonByTheC,"Since the API calls to Google are so expensive with the 2,500 cap, we get around the problem by using the haversine method to build a rainbow table first and then filter for postal codes with less than 80 miles of distance between each postal code pairings.  

From there we use either the google or Bing APIs to calculate the drive distances from each centroid to centroid (using the population centroid lat/lon).  

The method doesn’t need to be incredibly precise since the postal codes themselves are representing approximations of geography.  

I’m not a fan of bing for reverse lookups in addresses after it placed a few of my address lookups into ridiculous locations that were blatantly wrong when you looked at them on a map but it does a good job with coordinate to coordinate distance/time calculations.  
",1522151224.0
,Sexy title. ,1521588119.0
zdk,"One option is to convert suitable columns to an integer, rather than the default double precision floating point.

    mtcars %>% as.tibble %>% 
      mutate_if(function(x) all(x==as.integer(x)), as.integer)
    # A tibble: 32 x 11
         mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb
        <dbl> <int> <dbl> <int> <dbl> <dbl> <dbl> <int> <int> <int> <int>
     1  21.0     6   160   110  3.90  2.62  16.5     0     1     4     4
     2  21.0     6   160   110  3.90  2.88  17.0     0     1     4     4
     3  22.8     4   108    93  3.85  2.32  18.6     1     1     4     1
     4  21.4     6   258   110  3.08  3.22  19.4     1     0     3     1
     5  18.7     8   360   175  3.15  3.44  17.0     0     0     3     2
     6  18.1     6   225   105  2.76  3.46  20.2     1     0     3     1",1521558345.0
millsGT49,"There is an issue about this on the `pillar` (which handles all the printing for `tibble`) github page you can track: https://github.com/r-lib/pillar/issues/105

EDIT: Some more links

You can also view another discussion about significant figure rounding here: https://github.com/r-lib/pillar/issues/40

And can view all the `tibble` print formatting options by going to the `pillar-package` help page: `?pillar::pillar-package`",1521564371.0
shujaa-g,"There's a lot to dislike in the new-ish formatting in `print.tibble`. I wish they offered an easy option to turn it off (or, better yet, had the default be off and offered an easy option to turn it on). One way is just `print.tibble <- print.data.frame`, but I do like dims at the top and especially that any groups are called out. ",1521557266.0
d4rkride,"~~It seems as though the default behavior of as.tibble is to convert all ""numeric"" data types to double, which I feel is a safe assumption as numeric may contain doubles or ints, while ""integer"" will only contain ints.~~

If you want to avoid this behavior you need to convert your ~~""numeric""~~ columns that contain only integer data to class `integer` instead, as zdk shows.",1521562325.0
Deleetdk,"`rma()` is a random effects meta-analysis though. Maybe try simulating some data to see if your pattern is a consistent feature, or whether it depends on some special feature of your data. Generally speaking, so called integrative data analysis should be better than subgrouping and meta-analyzing it.

- https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2777640/

They write:

> IDA has the potential to provide substantial increases in statistical power for testing research hypotheses through the combination of multiple individual data sets. It is well known that most research applications within psychology are often chronically under-powered such that there is an unacceptably low probability that a given effect will be found if that effect truly exists in the population (Cohen, 1992; Maxwell, 2004). However, when multiple independent samples are combined, there is **often** a marked increase in power when testing the same hypotheses based upon the aggregated versus independent sample.

Their use of *often* makes it seem like increased precision is not a mathematical necessity but an empirical generalization. Interesting question though.",1521556589.0
regis_regum,"are you trying to find 'coverage' over your locations? IE, a small set of items, that are present in most stores?

could you try describing the problem like this: ""assume you've been given a set of stores, use this dataset to find all the common products amongst that set of stores"" ",1521516882.0
ReimannOne,"Make up some sample data.  

Make up what you want to end up with.

Post the sample data.

Post what you want the solution to look like.

We can work with that.",1521506160.0
Minowaman,Tidyr::gather %>% tidyr::separate %>% dplyr::group_by %>% dplyr::summarize,1521389386.0
Econ_dude,"If you could give an example of the original data and the outcome you’d like to have it would make it easier to understand.

I’m guessing what you need is to use str_sub from the package stringr and group_by and summarize from the package dplyr.",1521381967.0
rayray0313,"Run it in ezAnova, but transform it to long format. 
mod.ez <- ezANOVA(data = datalong,
                  dv = .(rating), 
                  wid = .(id),
                  within = .(repeated variable),
                  detailed = TRUE,
                  return_aov = TRUE)",1521468125.0
efrique,"A minimal reproducible example would help (i.e. if you set up a small data set so that we're all working with one thing rather than everyone who wants to show you having to *make their own data set*). 

Then, rather than link to an image of the sort of text you want to get out, paste the actual text in and use code formatting on it.

In this case, look closely at the help for `predict.lm`  (`?predict.lm`), which tells you the various arguments and what gets passed back. Try these:

    carsreg = lm(dist~speed,cars) # cars is built in to R, this should work as is
    carsnewdf = data.frame(speed=c(11.5,21))
    predict(carsreg,new=carsnewdf) # just the predictions
           1        2 
    27.64361 65.00149 
    
    predict(carsreg,new=carsnewdf,se.fit=TRUE)# give se's
    $fit
           1        2 
    27.64361 65.00149 
    
    $se.fit
           1        2 
    2.712315 3.185116 
    
    $df
    [1] 48
    
    $residual.scale
    [1] 15.37959
    
     pcs = predict(carsreg,new=carsnewdf,interval=""confidence"",se.fit=TRUE)
     pcs$fit
            fit      lwr      upr
     1 27.64361 22.19013 33.09708
     2 65.00149 58.59738 71.40559
    
     pp = predict(carsreg,new=carsnewdf,interval=""predict"")
     preds = cbind(pcs$fit,se=pcs$se.fit,p.lwr=pp[,2],p.upr=pp[,3])
     preds
           fit      lwr      upr       se     p.lwr    p.upr
    1 27.64361 22.19013 33.09708 2.712315 -3.756326 59.04354
    2 65.00149 58.59738 71.40559 3.185116 33.422574 96.58040
    
 (using the same form of assignment operator as you did) 

You can make any output you like, in any order you like with any column labels you like.",1521353717.0
infrequentaccismus,Remove the dot before the Tilde in the facet wrap. ,1521335865.0
singularperturbation,"Very interesting!  I like the attention paid to reproducibility, too - never heard of [checkpoint](https://mran.microsoft.com/package/checkpoint) before, but I have used [packrat](https://rstudio.github.io/packrat/) before, and I don't see it used in a lot of projects.

Reproducibility should be emphasized for DS more.  Either with Docker, 'bundler'-alike, or with something like snapshot.  I like the 'snapshot' solution because it looks pretty fire-and-forget once setup.",1521341467.0
grandzooby,"If you're using R, you could also use the gumbel package.

https://cran.r-project.org/web/packages/gumbel/

At a minimum, you could inspect the source of that package, or use it to draw a set of data to compare your code against.  Here's what I see:

	rgumbel <- function(n, alpha, dim=2, method=1)
	{
			#check args
			if(alpha < 1 && !is.numeric(alpha))
					stop(""invalid argument : alpha\n"")

			if(method == 1)
			{
					#generate dim*n exponential random variables
					exprand <- matrix( rexp(dim*n), c(n,dim))

					#stable random generation S(1/alpha,0,1,0 ; 0) cf. Nolan(2005) and Chambers(1977)
					unifpirand <- runif(n, 0, pi)
					exprand2 <- rexp(n)
					beta <- 1/alpha
					stablerand <- sin((1 - beta) *unifpirand)^((1 - beta)/beta) * (sin(beta * unifpirand)) / (sin(unifpirand))^(1/beta)
					stablerand <- stablerand /( exprand2^(alpha-1) )

					#apply the laplace transform
					unifrand <- invphigumbel(exprand/stablerand, alpha)
					if(sum(is.nan(unifrand))) cat(""warning NaN produced in rgumbel\n"")
			}
			if(method == 2)
			{
					v2 <- runif(n)
					T <- runif(n)
					#K function
					K <- function(t) t-t*log(t)/alpha
					#numerical inverse of K
					Kinv <- function(x) optimize( function(y) (x-K(y))^2 , c(0,1) )$minimum
					v1 <- sapply(T, Kinv)
					#inverse the joint distribution function
					unifrand <- matrix(0, n,2)
					unifrand[,1] <- invphigumbel( phigumbel(v1, alpha)*v2 , alpha)
					unifrand[,2] <- invphigumbel( phigumbel(v1, alpha)*(1-v2) , alpha)
			}

			return(unifrand)
	}

Here's how Python's Numpy implementation does it:

	double rk_gumbel(rk_state *state, double loc, double scale)
	{
		double U;

		U = 1.0 - rk_double(state);
		return loc - scale * log(-log(U));
	}


It appears rk_state from the ""random kit"" that delivered a uniform random variable.  This is helpful: https://stackoverflow.com/questions/39910050/how-to-view-the-source-code-of-numpy-random-exponential/39910139, which has the note (for the exponential): ""/* We use -log(1-U) since U is [0, 1) */""",1521432477.0
efrique,"Write U = exp[-exp(-{X-µ}/β)]

then rearrange to solve for X (giving a way to generate the Gumbel result). U is your uniform.

If you do the algebra carefully, everything works just fine.",1521353447.0
huessy,"Cool package/shiny app. It classified my account as a bot too, but I guess that's more on me.",1521389908.0
amoonand3balls,"I am thinking LDA can't handle that many missing values, so I gave it a hand using missForest() to impute the missing continuous data. Then I ran the lda() and it seems to have worked. For those interested, missForest() does a much better job of imputing both continuous and categorical data.

    ",1521324998.0
idno0001,"You can group parts of your pattern using parentheses and then make a backreference to the group using \\1:

    gsub(x = x, pattern = ""([0-9]{3})"", replacement = ""a\\1"")

You can have more groups within your pattern and make backreferences to them using \\1, \\2, etc. For example:

    gsub(x = c(""123ab"", ""23bc""), pattern = ""^([0-9]*)([a-z]+)$"", replacement = ""\\2\\1"")

swaps the numeric and alphabetic parts of the pattern.",1521245822.0
Darwinmate,Problem sounds really cool. Honestly without efrique's comment I wouldn't have a clue how to solve it. But it sounds very doable and I'm fairly good at R programming. Can you message me a small subset of the dataset? I'll have a go and see what I can come up wiht.,1521282093.0
fastrmastrblastr,"I couldn’t follow some of your problem but see if this is in the ballpark

https://cran.r-project.org/web/packages/Rglpk/Rglpk.pdf
",1521288694.0
efrique,"Sounds like you have a combinatorial optimization problem with some constraints -- close to some kind of variation on the knapsack problem or the bin-packing problem. 

There are pretty fast algorithms that can give near to optimal answers for some of those kinds of problems that might be adapted to whatever it is you're doing.

(I am not able to do this work with you but I figure you might find pointers to related material helpful nonetheless)

https://en.wikipedia.org/wiki/Bin_packing_problem

https://en.wikipedia.org/wiki/Knapsack_problem

",1521254554.0
zdk,"You are looking for a surface plot, perhaps?
www.r-bloggers.com/creating-surface-plots/amp/",1521258203.0
,"There also is `packrat` R package:

     install.packages(packrat:::fileDependencies(""script.R""))

Taken from [THIS THREAD](https://www.reddit.com/r/rstats/comments/84ejm1/r_users_what_was_something_simple_you_learned/dvpjwnr/)",1521231137.0
RememberToBackupData,A Python script to answer a question about R scripts :3,1521274899.0
zdk,"looks good, but you may want to scrub whitespace for the sake of completeness. Since `library (stats)` is a valid function call.",1521235952.0
danielw29,I am just getting started in Python and this was something I needed sometimes. If you have any ideas on how to improve it I'd appreciate it!,1521229116.0
,[deleted],1521251286.0
dtrillaa,"I generalized this code because I’m not by my computer and can’t try it myself.

I would create a function:

    Mtdays_df <- function(df) {
      (Body of all redundant code)
    }

And then I would pass all the data frames I to a list

    l <- list(mt1, mt2, ..., mt30)

Finally I would pass it into lapply function and pipe that to rbind

    Mtdays <- lapply(l, mtdays_df) %>% rbind()

And that should do it in much less code.",1521233305.0
,"Try an old-fashioned way: by reading a book.

My recommendation: **The Art of R Programming**",1521229906.0
Verisimilitude_Dude,"If by ""going deeper"", you already have some background with R, there are some options for advanced learning with R. 

[An introduction to statistical learning: With applications in R](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf) is a great resource for learning more intermediate stats while providing examples in R. As others have said, it is much easier to learn R when you actually have a project to do in R.

Hadley Wickham's [Advanced R](https://adv-r.hadley.nz/) book (which is written entirely in R!) is a great resource for learning the nitty-gritty R details. 

There are many other free books out there to help learn R but these are just two intermediate-level books that I recommend. ",1521232532.0
DataDouche,"Swirl is a great starting point, I recommend looking at [R for Data Science](http://r4ds.had.co.nz/). It's completely free and very resourceful. Also [this](https://paulvanderlaken.com/2017/08/10/r-resources-cheatsheets-tutorials-books/) website is a super great reference with a good collection of cheat sheets, books, guides, etc. ",1521230418.0
RememberToBackupData,"I learn new programming languages fastest when I can think of a small project that I personally want, one that isn’t related to work. You get a product you need, the program is small enough in scope that it has an attainable end, and you learn how to read the docs in a focused way to get the info you need from moment to moment. ",1521287642.0
masterofninja,"What have you used r for? My recommendation is think of a small project that is relevant to you and execute it in R. I've learned by doing, and that's been my key in learning what to do l.

however I think datacamp is worth having as well to give you proper teaching on concepts and new packages. Also get a good r reference book.",1521229679.0
dtrillaa,"I used data camp and I recommend it because it covers both programming in base R and the full tidyverse library. I did the R programming course and it covered everything from functional programming to object oriented programming in R.

If you don’t want to do a monthly subscription, purchase Hadley Wickams (author of the tidyverse) book [R for Data Science](https://www.amazon.com/Data-Science-Transform-Visualize-Model/dp/1491910399) ",1521232532.0
efrique,"Your title seems fine. 
",1521239376.0
,"> For learning R which method do you recommend?

Get working on a project that you are determined to complete.",1521558744.0
inmanenz,"LASSO is a good option, you should check out the GLMnet package. If you are not concerned about interpretability and only want to make predictions you could use PCA before fitting your model.",1521215027.0
efrique,"I strongly suggest you read Frank Harrell's book *Regression modelling strategies*, in particular what's in Chapter 4 of the first edition (I presume similar material is to be found in the second edition but I don't know where it might be; I gather there's a lot of changes).

There's a summary of some of the issues with it discussed here:

https://www.lexjansen.com/pnwsug/2008/DavidCassell-StoppingStepwise.pdf

",1521253249.0
ISuckBallz1337,"I'm not sure what model your building, but you could use Variable Clustering to group ""like"" variable and choose the most predictive of the group to reduce the number of potential inputs",1521222083.0
chocolateandcoffee,You might also want to try the [variable importance function](https://www.r-bloggers.com/variable-importance-plot-and-variable-selection/) in the randomForest or caret packages to determine which variables are the most useful in model building.,1521219085.0
Reggaepocalypse,"Wpuld transforming the scale of the variables in your model help?  Log transform, for example",1521229324.0
MonkGyatso,You can bypass this by scraping Reddit instead. In almost all cases if you put .json at the end of the URL it'll report back a JSON formatted page response. Something like this: https://www.reddit.com/r/rstats/comments/84ktme/reddit_scraping_using_redditextractor.json,1521141701.0
efrique,"1. Do you mean 

 Dimitris N. Politis and Joseph P. Romano (1994),  
 ""Large Sample Confidence Regions Based on Subsamples under Minimal Assumptions"",  
 *Ann. Statist*. Vol. 22, No. 4, pp2031-2050.

 or something else? 

2. > I am thinking just linking to QuantExchange is easer than rewriting the problem here.

 Oh, certainly it is! In a similar vein I am thinking that just not clicking the link is easier than flicking back and forth to try to figure out what you're asking about.
",1521071552.0
RememberToBackupData,"The timestamp is wrong.

    library(lubridate)
    as_datetime(1380291194000)
    [1] ""45709-09-08 12:33:20 UTC""

Removing the trailing zeroes fixes it:

    as_datetime(1380291194)
    [1] ""2013-09-27 14:13:14 UTC""

So my guess is that when the timestamp was stored in whatever record system it went into, it got zero-padded to 13 characters *on the right side of the number instead of the left*.

(Why you would want to zero-pad a UNIX timestamp is beyond me, though, since it's not a code or anything. It's the number of seconds elapsed since a particular time.)",1521064303.0
Im_int,"dplyr

Base R code isn't too difficult to write, but you have to spend double the time writing comments so you could read and modify the code in a few months. dplyr is intuitive.",1521045029.0
esotericish,"within dplyr and in RStudio you can pipe into View()

    df %>%
      filter %>%
      group_by %>%
      View()",1521047035.0
ryapric,"Honestly? Package development.

I work as a ""data engineer"" of sorts, and I used to have all of my code in one long script per client project (often 2,000+ lines each, even while using a ton of `magrittr`-piped blocks). It was an absolute nightmare to maintain, even after we learned about git.

The team at Rstudio's recent work has made package development into a *stupid* easy workflow, once you understand how R packages are structured. Obviously R packages are commonly full of functions or other general-purpose code, etc, but I have moved all of ETL work with R into packages because of all the productivity benefits that come with them. I have also ended up refactoring a lot of existing code into functions, though.

[Anyone who hasn't, should read this book by Hadley Wickham](http://r-pkgs.had.co.nz).",1521065781.0
,"Base R pipes!

    rnorm(100) ->.
      abs(.) ->.
      log(.) ->.
      density(.) ->.
      plot(.)

But seriously - that you can do multiple linear regressions with lm if you put all your dependant variables in a columns of a matrix:

    lm(Y ~ a + b, data=data)

will do a regression on every column of Y. And you will get back an object of class `mlm` (`m` for multiple). Who knew...

Illustration.

    mat <- data.matrix(iris[,1:4])
    fit <- lm(mat ~ Species, data=iris)

Will give you:

    > fit

    Call:
    lm(formula = mat ~ Species, data = iris)

    Coefficients:
                       Sepal.Length  Sepal.Width  Petal.Length  Petal.Width
    (Intercept)         5.006         3.428        1.462         0.246
    Speciesversicolor   0.930        -0.658        2.798         1.080
    Speciesvirginica    1.582        -0.454        4.090         1.780

    > class(fit)
    [1] ""mlm"" ""lm""

",1521046705.0
RememberToBackupData,"Shortcuts in RStudio:

`Ctrl + Shift +  R` makes a section header. This can be used to fold code in the editor.

`Ctrl + Shift + /` reflows a comment line to fit inside the margin, no need to add linebreaks manually.

`Alt + -` types the `<-` operator.

`Ctrl + Shift + M` types the `%>%` operator.",1521092175.0
shujaa-g,"Good code organization principles for scripts/projects that will be run frequently.

Most people know to load packages  at the top of a script (with `library`, not `require`, so you get a nice quick error if the package isn't available). But you should put other constants/configurations at the top of the script too so they're all together and in one place, easy to audit or modify. If there's a lot of them, break them out into a separate config file.

Break up large scripts into multiple files, and if they need to be run in an order, include that order in the name (and a separate script that does nothing but run them in the right order). 

Packages are pretty easy with `devtools` and `roxygen`, so if you have more than a small handful of functions, put it in a package.

Think carefully about adding dependencies. If you just need one function from a package, see if you can copy the code of that function rather than adding a dependency on the whole package.",1521048251.0
_nt2,"I go back as far as S-Plus days and definitely used R as of version 1.5.1 if not earlier. I used `_` as an assignment operator!

Reproducible research tools (`Sweave` and lately `Rmarkdown` and friends) always looked odd to me so I avoided them, until I actually started to use `Rmarkdown` regularly. I wish I had used them from the beginning.

I've also started using the `%$%` ""explosion"" operator from `magrittr`, mainly to allow for NSE and code completion for variable names when using non-tidyverse functions.",1521048371.0
Tarqon,"With Magrittr you can pipe directly into an anonymous function by wrapping it in parentheses. Lately I find myself doing something like this a lot:

    df %>% 
      # Do some preprocessing
      (function(x) {
        bind_rows(
          x %>%
            group_by(Grouping) %>% 
            summarise(n = n_distinct(stuff)),
          x %>%
            summarise(n = n_distinct(stuff)) %>% 
            mutate(Grouping = ""Total"")
        )
      }) %>% 
      {.} -> df_pivot

 This is just a simple example, but it's a great pattern to use when you have an operation where you'd like to use your original dataframe twice.   


Another trick that's amazing is training many models at once using list-columns and purrr::map(). A nice introduction to this technique can be found in [chapter 21 of R for data science](http://r4ds.had.co.nz/many-models.html).",1521049036.0
triple_dee,"Honestly, I wish I learned about R (what is a factor, how are data.frames vs. lists vs. matrices, how to extract information from lists, how are the apply functions different from each other?) and about basic programming (how to think about writing code, how to use stackoverflow/google) before being forced to use R for problem sets in college.  I think these are relatively simple things that I had to teach myself later on, when I was less overloaded by learning stats theory :|

My worst memories were struggling with base R plotting only to later on find that a lot of people use ggplot2. An extension of this would be I wish I knew dplyr and other tidyverse libraries.  I'm not great with these libraries, but I think they're easier to learn and have great resources.

I mainly use data.table at work, which I'd highly recommend to anyone that is working with a lot of data at one time.  A simple data.table tip is to use data.table joins instead of merge()--it's very fast!

edit: I checked the link in your original post and found a lot of my answers do overlap.  Added a data.table tip. ",1521053441.0
Elesday,"Use Rmarkdown to make your data analysis, benchmarks, tests and so on reproducible.
This way you don't end up having to recode something from scratch two months later because you forgot the result or because you wanna try another dataset.

Guess my next goal is to write down my results in order to avoid redoing the experiments again and again...",1521057189.0
firesquidwao,"you can do everything with data.table


",1521080805.0
quaternion,expand.grid,1521051554.0
pyl_time,"Maybe more relevant for people like me who frequently have to test other people's code, but if you have a piece of R code or a Shiny app you're trying to test on your system, install/load packrat and run:
`install.packages(packrat:::fileDependencies(""script.R""))`    
and it will install all necessary packages for that particular file. Super helpful to make sure you've got all the necessary dependencies installed!
",1521065205.0
guepier,"> You can wrap A LOT of commands inside `mutate` to make them work in the tidyverse.

There’s a slight, but crucial, misunderstanding in what you said:

In fact, you can wrap **all** commands inside `mutate` (to the extent that it makes sense). That is, in fact, its whole point: it’s a generically usable function, it isn’t restricted to a specific subset of functions. Everything else would be a substantial flaw in the dplyr API.",1521062126.0
mr_matzoball,"doParallel & foreach

Can save you hours of time over for loops/apply/purrr",1521074773.0
not_rico_suave,using groupby and summarise together.,1521048371.0
huessy,"That you can do weighted means and medians easily just by using rep().

    w.mean <- mean(rep(""data.array"", times = ""corresponding weights array))
    w.median <- mean(rep(""data.array"", times = ""corresponding.weights.array))

Before that I had was doing a whole lot of complicated for statements with capture.output(), grep(), as.numeric(as.character()), and so on. Felt silly for not putting the pieces together.

If anyone here works with Census data, this saved me hours and computing allocation on PUMS work.",1521062016.0
Ahzu,"- Aliases: assign()
Using aliases to customize an R envir, (check: ""~\Program Files\R\R-x.x.x\etc\Rprofile.site"") adding for instance a ""remove everything"" function: 
  .startup <- new.env() 
  assign(""del"", function() rm(list=ls(envir = .GlobalEnv),envir = .GlobalEnv) , env=.startup) 
  attach(.startup)

- x[][]
Practical when wandering into a dataset, using double [][] to subset a subset :
matrix[filter1,][filter2,]

- Regex: matrix[,grep()]
Using regular expressions to subset data, modify bunch of variables at once, time saver.

- tables::tabular()
In my opinion the best way to produce efficient and easy customizable tables including customized summary functions (weigthed statistics, customized confint...).
",1521068367.0
danielw29,"Identify. Just learned about it yesterday. :) 

https://stat.ethz.ch/R-manual/R-devel/library/graphics/html/identify.html",1521057254.0
Rylick,"Obviously dplyr, but also ""Y ~."".",1521064118.0
loluminium,doParallel and foreach,1521087338.0
esotericish,learn how to use dplyr's do() function! this has made so many things so much easier for me,1521078219.0
RememberToBackupData,"`magrittr` has some very useful pipes:

    library(magrittr)

    vec <- runif(100)
    vec %<>%
        log %T>% 
        hist

Which is equivalent to:

    vec <- runif(100)
    vec <- log(vec)
    hist(vec)

The example is kind of circuitous (you could of course skip to `vec <- log(runif(100))`), but it's a typical use case for a pipeline, where you want to visualise the results of some manipulation inline.

The `%T>%` pipe can also stack, so you can make multiple `%T>%` pipes and then continue with normal pipes and the variable will not store anything that isn't done with an assignment pipe.

    vec %<>%
        log %T>% 
        hist %T>%
        plot %>%
        mean

    vec
    [1] -1.055589",1521090382.0
spectrum_specter,"library(plyr)  
  
count(df$var)  

Is much more useful than  

levels(df$var)  

Because it gives you the levels of a variable & the count of level instances  

Basic, but just found out after six months of learning",1521127331.0
hetero-scedastic,"::: to access non-exported functions in a package.

Something I could have done with early on: a clear explanation of [[ ]]",1523417566.0
mbillion,"that the /\ are backwards
",1521084717.0
DrNoPants,python,1521043737.0
thelatemail,"    table(paste(head(x,-1),tail(x,-1)))

Or something similar should do it.",1520991847.0
ryapric,"Update: I have been made aware that the [rio](https://github.com/leeper/rio) package is a far better documented & designed R package that accompishes the same goal as readit, and more. The lesson here should be: practice better Google-fu before you reinvent the wheel...",1520986997.0
,"This is, of course, an awesome effort, having been subject to all the Bad Data Hygiene that one could imagine. 

How does this compare to [rio](https://github.com/leeper/rio)? ",1520985926.0
bprs07,"I tackled something like this in R today as well, but my approach was less about simulating the probability of a team winning a specific outcome as it was optimizing a bracket based on **teams' statistical chances of advancing** and **who the fans are picking**. The theory is you want to pick teams that have a higher probability of advancing than the fans are giving them credit for.

I used KenPom's tournament probabilities \([https://kenpom.com/blog/](https://kenpom.com/blog/)\) and manually entered the fans pick frequencies from CBS, who tells you what percentage of the time people are picking, say, Virginia to advance to the Elite 8.

I found that if you're truly trying to maximize expected value \(EV\) you tend to have a lot of upsets in early rounds, leaving you with what I call a ""Havoc Bracket."" While there's a lot of value to be had in picking Seton Hall to make the finals should it actually happen \(something like 0.1&#37; of brackets or lower have this prediction\), KenPom forecasts this outcome only 0.9&#37; of the time. There's a real need to find a balance between **value picks** and picks **that are still probable.**

To solve for this, I created a model where I use KenPom outcomes by matchup to simulate thousands of brackets, and for each bracket I calculate the **cumulative EV** \(the gap between what fans are picking and what KenPom picks\) and the **relative probability** \(how likely that bracket is compared to a bracket where all favorites win\).

With this method I generated only 1 bracket in my first test run of 1,000 simulated brackets that finished in the top 10&#37; in both cumulative EV and relative probability. This means it has both enough key differentiators but is still highly probable.

I won't list the outcome of every game, but these were the results from the Sweet 16 and beyond:

**South**

* Virginia \(1\) over Arizona \(4\)
* Cincinnati \(2\) over Tennessee \(3\)
* **Elite 8**: Cincinnati \(2\) over Virginia \(1\)

**West**

* Ohio State \(5\) over Missouri \(8\)
* UNC \(2\) over Michigan \(3\)
* **Elite 8**: UNC \(2\) over Ohio State \(5\)

**East**

* Villanova \(1\) over West Virginia \(5\)
* Texas Tech \(3\) over Purdue \(2\)
* **Elite 8**: Texas Tech \(3\) over Villanova \(1\)

**Midwest**

* Auburn \(4\) over Seton Hall \(8\)
* Duke \(2\) over ASU/Syracuse \(11\)
* **Elite 8**: Duke \(2\) over Auburn \(4\)

**Final 4**

* Cincinnati \(2\) over UNC \(2\)
* Duke \(2\) over Texas Tech \(3\)
* **Championship**: Duke \(2\) over Cincinnati \(2\)

My next steps are to wait until Wednesday evening to re\-collect the fan data, when more people have created their brackets, and run it over far more simulations.

There's definitely a sliding scale with this method, as you can always dial up the riskiness and pick a bracket with a higher projected EV, or you could dial down the riskiness and go with something more conventional but with less EV.",1520992688.0
Run_nerd,"I was just wondering if I could try to simulate March Madness in R! This looks really interesting, thanks for posting. ",1520972528.0
Atheriel,"Well, if you like the idea of working with Stan (which is just about the most general Bayesian package possible), you can use the **rstan** interface. To ease you into using Stan there is also the **rstanarm** package, which provides familar models and uses the traditional R formula interface. That's what I would recommend.",1520987383.0
questionquality,brms,1520964837.0
Tarqon,"Honestly, it takes less time to test this than to post about it on reddit...

    > 2.8434e-05
    [1] 2.8434e-05
    > 2.8434e-05 + 1
    [1] 1.000028

It works fine.
",1520964182.0
efrique,https://en.wikipedia.org/wiki/Scientific_notation#E-notation ,1521034242.0
shaggorama,http://data.stackexchange.com,1520958217.0
efrique,http://r4stats.com/articles/popularity/ ,1521032963.0
_nt2,"This is a statistics homework question and not a question about the R programming language.

But I'm a nice guy so what I'll say is that there are different elementary margin of error formulae for cases when you are trying to estimate a proportion (p with 1 - p = q as in the formula you've tried) as opposed to cases when you are estimating an average of something. So try the other formula from your textbook. ",1520954760.0
Sir_not_sir,"Install linux on the mac you have. My 2013 13"" runs Manjaro-i3 flawlessly as a single boot.",1520902799.0
SeveralBritishPeople,"Since you’re using RStudio, why not connect remotely to a GCP or AWS instance running rstudio server? You’ll get boatloads more performance (when you need it), a user experience very similar to local rstudio, and develop useful skills for industry or academia.

I’m mostly an AWS user, but GCP seems to have more beginner-friendly setup guides, including some from rstudio themselves: https://blog.rstudio.com/2017/08/18/google-cloud-platform/",1520910725.0
Atheriel,"It's very common to recommend used Thinkpads as personal Linux machines. (There's a very active community over on /r/thinkpad, too). You can get 2y-old retired enterprise machines on craigslist/ebay for $200-400 and they have excellent driver support on Linux. I tend to use Emacs/ESS, but I know my L450 runs RStudio just fine.",1520919751.0
efrique,"It really depends on what you're using it to do -- R can run on a phone, and for 90% of what I do even a tiny laptop is fast enough.

To offer advice that would relate to what you do (rather than what they do) people will need to how much of what are you doing, and how big your instances are. 

I've seen people run off to buy high end machines and pay for commercial versions of R to handle large amounts of memory for their ""big data"" problem, only to discover that all of their data and functions would run happily in ordinary R and their analyses would complete in perfectly reasonable time (usually a handful of seconds, a few things took a few minutes at worst) if they organized their work sensibly and then simply killed a few processes they didn't actually need to leave running. 

[They had asked  a guy for advice about handling big data, but they didn't specify to the guy they had spoken to how *big* that actually was. Not very big at all, as it turned out, and they wasted a lot of time and money solving a problem they didn't even have.]
",1520910575.0
agclx,"I am not comfortable using my notebook for ""heavy lifting"".  I have a desktop machine setup that I can leave on 24/7 and let it do the work (one of the institutes equipment).  The results I load on my notebook for visualisation.  It gets expensive if you want performance on a notebook. Plus you have a lot of issues when using it on the go (ressources sometimes not available and such).  It just is way more comfortable having a second machine and free the notebook for writing.",1520934444.0
spinur1848,"DigitalOcean.com with Docker and Rocker images. Full cloud and it costs pennies. Add in a postgresql container and you've got full database support.

As long as you've got reliable network and no special security concerns, a chrome book is all you need.",1523965356.0
Darwinmate,"you mean a package that interfaces with the reddit API? 

let me google that for you

https://cran.r-project.org/web/packages/RedditExtractoR/index.html",1520899618.0
TripKnot,"I didn't test this out but it should be close.  You'll first want to add a column to the dataset with the fill colors.  The ggplot code should have most of the other features you are looking for.  Adjust the break values in scale_x_continuous to change the start/end locations.

    library(dplyr)
    axin2_one <- axin2_one %>%
      mutate(color = ifelse(V3 > 0.95, ""red"", ifelse(V3 > 0.75, ""yellow"", ""grey"")))
    plot <- ggplot(axin2_one, aes(V2, V3, fill = color)) +
      geom_point() +
      geom_hline(yintercept = 0.4) +
      scale_x_continuous(name = ""10 KB Region"", breaks = c(4100000,4200000), labels = c(""Start"",""End"")) +
      ylab(""FST Value"")

Edit:  Above doesn't work... this does. Also eliminates need for the dplyr mutate command.  Its a little clunky though: [Plot](https://imgur.com/a/wJyd7)

    ggplot(axin2_one, aes(V2, V3)) +
      geom_point(aes(color = cut(V3, breaks = c(-Inf, 0.75, 0.95, Inf), labels = c(""grey"",""yellow"",""red"")))) +
      scale_color_identity() +
      geom_hline(yintercept = 0.4) +
      scale_x_continuous(name = ""10 KB Region"", breaks = c(4305000,4315000), labels = c(""Start"",""End"")) +
      ylab(""FST Value"") ",1520885760.0
mattindustries,"They are looking for the referer. Not sure how to do that with httr, but in curl it is

>curl 'http://www.snirh.gov.br/hidroweb/Estacao.asp?Codigo=64903000&CriaArq=true&TipoArq=1' -H 'Content-Type: application/x-www-form-urlencoded' -H 'Referer: http://www.snirh.gov.br/hidroweb/Estacao.asp?Codigo=64903000' --data 'cboTipoReg=9'",1520873042.0
fraud_93,Have you looked at the institution documentation? Almost all Brazilian organizations has an API usage for their resources.,1520880514.0
berf,"Whoever wrote that question didn't understand that ""-"" and ""*"" and ""^"" are also R functions, even though they can be called without parentheses.  For example,

    `-`(2,3)

For more of a hint, look at the formula for variance in the textbook.  It has sums in it does it not?",1520870013.0
suity1,"probably bad form to help with homework, but enough to get you started:

to compute the sample variance of a vector comprised of the integers between 1 and 10

    sum((1:10 - sum(1:10)/10)^2)/(10-1)
",1520870672.0
anotherjsanders,"Someone please answer this! It drives me mad. When would you ever want to just rerun one line of a multiline block?

 The only solution I've found is to always run multiline code from the markdown doc by pasting it into the console, rather than Ctrl+Enter-ing it - but this is really tedious. ",1520895841.0
spacecraftily,"TBH I'm not sure of this will fix it but worth a try.

The printing of each line of code from a script can be disabled from the top right corner of the editor. On my phone at the moment but it's something like a little drop-down that says ""echo code on source"" or something.

Turn that off and I'd guess the up arrow will work as you wish",1520864718.0
,"> Is there an easier way to select/ rerun earlier code executions in notebooks/markdown?

I guess I'm struggling to understand what is going on in your workflow where just rerunning the current code chunk (shift-ctrl-enter) isn't an option or isn't desirable.",1520869605.0
questionquality,Why isn't hypothesis() working for you?,1520843220.0
Hoelk,"A minimal, eye-friendly csv editor made with shiny, rhandsontable and readr. Shed is designed to quickly edit small (hundreds of rows) csv files and data.frames. It uses readr::write_excel_csv() as backend for writing files, and thus produces UTF-8 encoded csv files that are compatbile with MS Excel by default.

If you wanna help file bug reports and feature request, or design nicer css themes :)",1520772307.0
efrique,"That appears to attribute to the CLT something it doesn't actually say.

(This is a pretty common error, though.) 
 ",1520739691.0
noviceProgrammer1,Is there an advantage to this implementation  compared to running python in rmarkdown? ,1520685957.0
samclifford,"Confidence intervals are interval statistics for a population parameter. Are you trying to estimate a mean? Some other statistic? Never fear, you don't need normal data to compute the confidence interval for a mean. ",1520650019.0
Ader_anhilator,"Why not use quantile(df[,x], probs=c(0.025, 0.975))",1520650817.0
dtrillaa,"Edit: first use `hist(your_data, breaks=20)` to create more bins in your histogram so the first bin is split up a little bit.

Second, try and determine statistically what type of [distribution](http://blog.cloudera.com/blog/2015/12/common-probability-distributions-the-data-scientists-crib-sheet/) the data is. Then in r, you can use functions related to that distribution to create your confidence intervals. Here’s an example of how to do so for [log normal](https://www.rdocumentation.org/packages/stats/versions/3.4.3/topics/Lognormal) ",1520649176.0
AprimeAisI,"In the bootstrap, what function were you using to get the replicates. ",1520731341.0
seeellayewhy,"5 files, each with 36 sheets? Why don't you do something like loop through 1:36 combined with `read_excel`, with i in the `sheets` argument. You might could even package that into a function to do across each of the 5 files. If you don't need sheet names then you won't have to worry about them since the sheets argument can use indexing.  

I'm on mobile but if you need help I can jump on and try to write something up for you. ",1520646733.0
jimmyjimjimjimmy,"Readxl::excel_sheets(f) %>% map(~readxl::read_excel(f, .x)) %>% 
set_names(excel_sheets(f)

Something like that will work for one file where the file is f.  Writing while walking dog, your mileage may vary.

Requires tidyverse and readxl",1520647032.0
edimaudo,"You can use this as a start - https://www.datacamp.com/community/tutorials/r-tutorial-read-excel-into-r

Then possibly store the sheetnames in a vector and the loop over it to import.",1520633539.0
team_morty,"As a follow up question, is there a package/method  that allows you to import every sheet from an excel document into a list or something, or do I need to import each sheet by name?",1520646831.0
mattindustries,"The file is UTF-16, so append read.csv() with `fileEncoding=""UTF-16LE""`, or whatever read_csv() uses for file encoding.",1520565329.0
dmuney,"So I googled your fread error and stumbled upon this [stackexchange thread](https://stackoverflow.com/questions/22643372/embedded-nul-in-string-error-when-importing-csv-with-fread).  It looks like the fix for that is only on linux (I'm on windows), but I looked further down the comments and someone suggested this solution using read.csv and it worked for me:

    data <-  read.csv(""E:/R Programming resources/R Data Sets/LNFH_PIT_IS.csv"", fileEncoding=""UTF-16LE"")

Not sure what system you are running but that command finished almost instantaneously on my machine.",1520565516.0
chalaila,"If you look carefuly, the link you posted isn't your typical url, it's an ftp link (not an http). Probably that's why read.csv isn't working.

Doing a quick search tells me that RCurl might be your answer.",1520561960.0
,"read.csv(""file.csv"") should work if you've set your directory. If you're trying to read directly from the webpage, then that is beyond what I know. ",1520561818.0
RememberToBackupData,"You're overthinking it haha :3 If it's only aesthetic formatting that you're doing, you can define the theme as a variable and then `+` it in.

    mytheme <-
        theme_bw() +
        theme(panel.grid = element_blank())
    
    mtcars %>% 
        ggplot(aes(x = wt,y = mpg))+
            geom_point() +
            mytheme

But theme objects can't hold things that are dependent on the mapping of the chart, like `guides()` or `labs()`. If you want to change those uniquely for each plot, then best practice would be to create a function to make the plot for you.",1520540754.0
emiltb,"I am not sure about what ggplot internals is causing your piping approach to fail, but you might be able to work around it by adding your additional layers as a list instead of a function. This way you can easily reuse layers between several plots.

    library(tidyverse)
    
    plotformatter <- list(
      theme_bw(),
      scale_x_continuous(limits = c(3,4)),
      labs(x = ""My X label"", y = ""My Y label"")
    )
    
    mtcars %>% ggplot(aes(x=wt,y=mpg))+
      geom_point() +
      plotformatter
 
EDIT:
And if your usecase needs a function because you need to easily change some parameters, you could just make a function that returns a list. This is then still added using ""+"" and not the pipe.

    plotformatter <- function(x) {
      list(
      theme_bw(),
      scale_x_continuous(limits = c(x,4)),
      labs(x = ""My X label"", y = ""My Y label"")
    )
    }
    
    mtcars %>% ggplot(aes(x=wt,y=mpg))+
      geom_point() +
      plotformatter(2.5)",1520544741.0
,Have you tried using your function without the piping?,1520534301.0
RaggedBulleit,Curious what this gains over +theme,1520536376.0
shujaa-g,"As someone else pointed out, this is just an order of operations thing.

You can solve with braces:

		{mtcars %>%
			ggplot(aes(x=wt,y=mpg))+
		  geom_point()} %>% 
			plotformatter

Or assigning your plot to an object first:

		p = mtcars %>% ggplot(aes(x=wt,y=mpg))+
		  geom_point()
		
		p %>% plotformatter

In general, `%>%` takes precedence over `+`. You can find this info at `?Syntax`. As written, your code was passing `geom_point` to `plotformatter`, and then trying to add `ggplot(mtcars, aes(wt, mpg))` to the result.",1520605035.0
jacquespeeters,"Not the answer to your question but if you've the same formatting for your entire documents you can also set config for all ggplots.

```
library(hrbrthemes) theme_set(theme_ipsum(plot_margin = margin(5, 5, 5, 5)))
```",1521035396.0
bwcampbell,"Yeah, I’m a grad student...  don’t have $1680...",1520508688.0
,Might be a bit of a rude question in a thread like this but - why not donate to the *R foundation* instead?,1520529732.0
drunkferret,Man I love Hadley and all but let's cut that down to like 10 cents/hr and I'll be a strong maybe. I use it 3-4 times a week for 4 or more hours for work.,1520515986.0
EuGENE87,"I follow the spirit of the campaign, but to be fair it doesn't seem very attractive to somebody that lives outside the USA. By the way, only donations in USD? No BTC address?",1520516196.0
erlo,"""1,000,000 will get you on day of brainstorming with me, anywhere in the world"" .... :|",1520550559.0
RememberToBackupData,Gee there’s a $25k donation :0,1520505799.0
,I'm going to see if I can get my company to donate...,1520522545.0
seitgeist,Yeah but what if I was forced to use R for 300 hours?,1520548536.0
BetterGhost,"Catchy title, but I don’t have thousands to donate.",1520575196.0
xiaodai,$2000!!!,1520553126.0
amiba45,"Now pay the users back $1 for every hour of frustration from bugs and quirks in R & (too many ""incomplete"") packages...",1520537906.0
questionquality,"R is fundamentally a language made for and by statisticians. In the development, lots of choices were optimized for scripting statistics, which make it awkward to use for other purposes. The strength of R is the package ecosystem, and afaik there aren't any good general purpose web stack packages.

But for specific purposes, there is Shiny as you say. I'm not sure if you're aware, but it has a (GPLv3 licence)[https://github.com/rstudio/shiny/blob/master/LICENSE], which basically means if you build an web-application on shiny but don't distribute the program (ie the only thing you ""give out"" is access to the web-application), you don't need to distribute the source either. ",1520452197.0
_Wintermute,"As it's already been said, it's the not the fact that R is missing a Django/Flask alternative, but rather R is pretty horrible for general purpose programming, largely for the same reasons it's pretty great at statistics/data-analysis.

I think if you're picking a single language for full-stack development, it's far easier to do the data-analysis in python/javascript/scala than it is to do everything else that's required in R.",1520454551.0
mattindustries,"R can work as ""full stack"", but it isn't great with concurrency. I usually write dashboards and have my R session act as a JSON API endpoint in a websocket connection. You can also add NodeJS as a handler between the two for some advanced websocket stuff with caching and routing websocket connections, but then it stops being full stack. ",1520491676.0
CohoCharlie,"I'm more of a python guy - pandas/matplotlib/seaborn make some really attractive graphs. 
 
I've experimented minimally with Shiny, but does anyone know if you can use it as a stats API? Set up endpoints to pass data from whatever front end you like to Shiny/R then get back what you need. Due to constraints put on me at work I'm running a node/angular front end with a flask/pandas API soley dedicated to crunching numbers. Would something similar be feasible with Shiny?",1520470143.0
edimaudo,You can take a look at R Studio server.,1520633415.0
Tarqon,"https://github.com/thomasp85/fiery/blob/master/README.md These people are working in making it easier to run a web stack in R.

Overall though I'd agree that languages each have their own strength. Ones that are ideal for running web servers on top of generally have robust stability and multithreading.",1520454381.0
Randybones,"An interaction between a continuous and categorical variable is created in exactly the same way as a categorical/categorical interaction: multiply them together. You can think about this in the simplest case with dependent Y and predictors {X (cat), Z(cont)} as two models mashed together, one when X = 0 and the other when X = 1:

Overall: Y = B0 + B1X + B2Z + B3XZ

When X = 0: 
Y = B0 + ~~B1X~~ + B2Z + ~~B3XZ~~
or Y = B0 + B2Z

When X = 1:
Y = B0 + B1 + B2Z + B3Z
or Y = (B0 + B1) + (B2 + B3)Z

Therefore the interaction coefficient (B3) can be interpreted as ""the amount the slope on Z changes if X = 1 vs. when X = 0"" and B2 is the baseline slope of Z (when X = 0).

Subgrouping is similar to allowing interaction on all terms with your grouping variable. Modelling as interaction allows you to specify which variables should have different coefficients by group and which should be the same. 

Be careful of modelling interactions where there is no biological rationale. If you search long enough, you will find something significant... doesn't mean its real",1520452301.0
Randybones,"You need to specify values for par1 and par2 and replace the generic functions function_one and function_two with actual functions for this to work. e.g.

    n <- 10
    mat <- matrix(ncol = 2, nrow = n)
    par1 <- 5
    par2 <- 10

    function_one <- function(num1,num2){
        return(num1+num2)
    }
    function_two <- function(num1,num2){
        return(num1*num2)
    }

    for(i in 1:n){
      var1 <- function_one(i,par1)
      var2 <- function_two(i,par2)
      mat[i,] <- c(var1,var2)
    }

    print(mat)

Which should give:

            [,1] [,2]
     [1,]    6   10
     [2,]    7   20
     [3,]    8   30
     [4,]    9   40
     [5,]   10   50
     [6,]   11   60
     [7,]   12   70
     [8,]   13   80
     [9,]   14   90
    [10,]   15  100

Also note that because of the way R handles vector operations, you can do this same operation without for loops, e.g.:

    cbind(function_one(seq(1,10),5),function_two(seq(1,10),10))",1520452798.0
The_Sodomeister,"You have to create the functions called function_one and function_two. 

""mat[i,] <- "" stores the results in row i, across all columns.",1520443400.0
ran88dom99,you need to take a course in computer programming. or at least a tutorial in R.,1520447145.0
_nt2,"1. Either you have an IT department or you don't.
2. If you don't have an IT department, then install LaTeX and use it to render a PDF of your book. 
3. If you do have an IT department, and they are helpful, you could ask them for help and/or permission with installing LaTeX and go back to item 2. above.
4. If you have an IT department, and they are helpful, you could also ask them for help in how to make web pages available for only some people to see (some sort of Intranet or firewall stuff.)
5. If you have an IT department and they are not helpful then I'm not sure what you'd be able to do.",1520450662.0
circulus_one,"Is this what you are after? 

https://datacritics.com/2018/02/28/melt-your-data-for-fast-visuals-with-your-dataset-in-r/?utm_campaign=News&utm_medium=Community&utm_source=DataCamp.com",1520449306.0
Deleetdk,"They are called [splines](https://en.wikipedia.org/wiki/Spline_interpolation), in your case of degree 1 or linear splines instead of the usual cubic (3rd degree) splines. Discussed in:

- http://www-bcf.usc.edu/~gareth/ISL/
- http://biostat.mc.vanderbilt.edu/wiki/Main/RmS

Quick Googling finds a demonstration of an R package to fit them. This package also supports setting the knots yourself.

- https://cran.r-project.org/web/packages/lspline/vignettes/lspline.html",1520436111.0
IPredictAReddit,"You mentioned elsewhere that you already know the breakpoints (the vertical dotted lines). 

The simplest way is to interact your time variable with a categorical variable for each of those segments and the targeted/nontargeted binary. You'll have to be careful understanding which is the ""omitted"" category, but you'll get 5+1 coefficients corresponding to the 6 different slopes you're looking for.

First, create a categorical (let's call it **regime**) for each of the 3 segments based on the date (before first dotted line, after first dotted line but before second dotted line, after second dotted line).

Second     
    `lm(y ~ date*as.factor(regime)*as.factor(targeted), data=df)`

Third, interpret carefully (using all effects to build your baseline slope).",1520437226.0
slammaster,"I remembered it from my Master's as ""hockey stick"" regression, but after some Googling I think what you want is ""segmented"" or ""piecewise"" regression

I've never done it in practice, but these links below should be a good start

https://en.wikipedia.org/wiki/Segmented_regression  
https://stats.stackexchange.com/questions/18468/how-to-do-piecewise-linear-regression-with-multiple-unknown-knots  
https://www.r-bloggers.com/r-for-ecologists-putting-together-a-piecewise-regression/",1520432971.0
ran88dom99,Look up Friedman's MARS. R has packages for this for sure. It will determine segments for you. ,1520441304.0
LifeInDrag9,"Are there any reasons or is there a rationale for separating by those breakpoints? 

An alternative to stepwise regression would be non-linear regression or generalized additive models (GAMs). The mgcv package is fantastic for GAMs. It will add ""smooths"" [seen s(variable) in model syntax] to your predictor (which looks like time) and integrate the nonlinearity in the data.

For example:

example.GAM <- gam(readmission.rate ~ s(time), 
                                 family = ""gaussian"",
                                 data = your.data,
                                 method = ""ML"")

If you want to analyze both groups (targeted conditions and nontargeted conditions) separately, I would give GAMs on each dataset a shot. If there are reasons to have those breakpoints (exogenous factors), then stepwise regression (splitting the data by the dates for those breakpoints, then running separate linear models for each of the three subset datasets) would be fine.",1520436517.0
metagloria,"In the most generic terms, say you have outcome Y, predictor X, and a known cutpoint k at which you want a bend in the line. 

Define X2 as (X-k)*(X>k). So it will be 0 left of k, and will increase from 0 after k. This ensures continuity. Then regress Y on X and X2.",1520436603.0
sohaibhasan1,"You should be able to just fit a plain old linear model that includes flags for whether or not time is in those regions. If there are other features in the model, you can add interaction terms to account for the relationships between being in the time period and that feature.",1520436929.0
qryCosmos,"Assuming there's a rationale for the breaks (e.g. policy regime with known dates, etc.), an easy and intuitive path (from the perspective of interpretation) might be to include a set of regressors that flag each period of time.

In other words, if we have P1, P2, and P3 as variables indicating ""period 1"", ""period 2"", ""period 3"": 

* P1 would take a value of 1 during the period Oct 2007-Apr 2010, (zero elsewhere),
* P2 would take a value of 1 during the period May 2010 - Oct 2012, and
* P3 would take a value of 1 during the period Nov 2012 - Apr 2015.

If you do include all three, be sure to remember to drop the 'constant' from the regression. ",1520441040.0
PEG-8000,"There are good responses here, this is indeed a segmented regression problem. One other option is to turn it into a changepoint problem by taking the first derivative of the data and then looking for changes in level. This can be done with the r packages changepoint and wbs among others. Wbs is very fast and user friendly and produces good results with the default settings; changepoint has many more options. These packages will give you the positions of the changepoints, and then you can fit lines between them on your raw data.",1520450346.0
aelendel,"What are you trying to test?

This looks like time series where each point is dependent on the previous, so certain assumptions from just applying a linear regression won’t apply—or, at least, won’t answer the question you want to answer.",1520470724.0
berf,"For this you have to [use the source, Luke](http://catb.org/jargon/html/U/UTSL.html).

    ets

shows you the source for this function.  Find out where it is emitting this and why.  If it goes to C or C++ or Fortran, then you will have to download source and read that.",1520541339.0
,"Resisting the urge to copy-paste the analysis is the right response. Before you know it, there will be another 3 added to your list - trust me. I recommend building a function out of the analysis as the first step. 

Then you can either loop around that function or use an apply statement with that function -- or, just copy paste the function call if that makes it more readable. Since it is boiled down to one or two function calls, the copy-pasting is more tolerable as major changes will happen in your function definition. 

> Advice/suggestions?

> setwd(""C:/Users/jaewook/Documents/anal/"")

Are you on a work computer? I might suggest not naming a subfolder 'anal'.",1520387144.0
Alex_Pan,"I don't see a reason why you need to put your analyses in a loop. There is nothing wrong with copy-pasting parts of your analysis, especially if you're only doing 3 data sets.


A simple solution would just be to wrap your entire analysis in a loop:

	data(iris)
	data(cars)
	data(airquality)

	datasets <- list(iris, cars, airquality)

	for (i in 1:length(datasets)){
		dataset <- data.frame(datasets[i])
		
		# Stick your analysis in the loop. eg.
		print(colMeans(dataset[1]))
	}


P.S. it is better to use relative file paths rather than absolute paths. If you share your code or work on it from more than one computer, it is easier with relative paths.",1520381212.0
The_Sodomeister,You can define your whole algorithm as a single function. This would make it easy to stick in a for loop or an lapply statement or whatever you want. You could also source the function from a separate script if you wanted to be really organized.,1520386729.0
Tarqon,The 'recipes' package might be helpful for this.,1520441253.0
afatsumcha,"You're looking for the ""map"" family of functions, if you'd like to apply a function to a list of something. [This should be a helpful chapter, good luck.](http://r4ds.had.co.nz/lists.html)

*edit: [Here’s a cheat sheet, too](https://github.com/rstudio/cheatsheets/blob/master/purrr.pdf)",1520384606.0
,"A for loop may make it appear in a simpler way that is more easily understood -- but efficiency wise, I think you have probably the best code given the structure of your list. I might argue that since all your data.frames are the same it may make sense to just rbind or dplyr::bind_rows all together and use dplyr functions group() and summarize() to accomplish what you are after (I think). That being said, lots of ways to skin a cat.",1520375939.0
deadcaribou,"library(""dplyr"")
select_if(dat, !is.numeric)",1520370675.0
,"Sure -- here you are:

    col <- dat[,!unlist(lapply(dat,is.numeric))]

Let me know if you have any questions about it or need it broken down for you.",1520369867.0
efrique,"here's one way.

First make some data (you should have done this for us):

    > x <- data.frame(a=c(""a"",""b"",""w""),b=1:3,d=c(TRUE,FALSE,TRUE))
    > x
      a b     d
    1 a 1  TRUE
    2 b 2 FALSE
    3 w 3  TRUE

Now

    > x[!sapply(x,is.numeric)]
      a     d
    1 a  TRUE
    2 b FALSE
    3 w  TRUE

There are numerous other ways to do it, but that one's pretty easy",1520381733.0
tdawry,"FWIW the author of this course, Dean Attali, is probably the foremost expert on Shiny, outside of the Shiny developers.


This is a good second step, after completing [Building Web Applications in R with Shiny](https://www.datacamp.com/courses/building-web-applications-in-r-with-shiny) by Mine Cetinkaya-Rundel, easily one of the best R educators around.",1520397010.0
UWillAlwaysBALoser,"Looks like you are running a GLM, not a LM. Try this:

    models <- paste(""A ~"", colnames(d)[-1])
    fits <- lapply(models, function(m) {
         glm(m, data=d, family = binomial())
    })
    summaries <- lapply(fits, summary)
    confints <- lapply(fits, confints)

",1520347055.0
arlinconio,"You can define a function that does the LM and takes the covariate as argument, then *lapply* it over the list of covariates.",1520346037.0
informaticsdude,"This will only help you with dealing with all of the outputs from those regressions... but I think you'd benefit greatly from reading up on David Robinson's [broom package](http://varianceexplained.org/r/broom-intro/). 

",1520351316.0
enilkcals,Consider learning how to use [`purrr`](https://emoriebeck.github.io/R-tutorials/purrr/).,1520415213.0
efrique,"1. you can't fit a logistic regression with `lm` (try it and ACTUALLY READ THE WARNING MESSAGE telling you as much; it doesn't do what you want); you use `glm` for that. 

2. Why are you doing univariate regressions? if more than one variable has an impact on the response, the marginal relationships are not likely to be useful (and indeed [may actively mislead you](https://en.wikipedia.org/wiki/Simpson's_paradox))

3. This is the kind of task R is designed to do. If you write a function to do the glm you can use lapply, e.g. something like

        fit1glm <- function(i) glm(A~d[,i],data=d,family=binomial(link='logit'))   
        fits <- lapply(2:7,fit1glm)  
        summaries <- lapply(fits,summary)  
        confints <- lapply(fits,confint)  
    
This is a bit rough -- there are a bunch of other ways to do it. If it's something you will need to do more than once you'd write a more generic function that this one.
 ",1520383194.0
Trauma,First abstract your regression into a function that takes the variable as the input. Then apply that function across a vector of the variables.,1520346831.0
arlinconio,"Let me first confirm I understand the structure of your data, based on your description:

    cityexp <- data.frame(city = c(""London"", ""London"", ""London"", ""Paris"", ""Paris"", ""Shanghai"", ""Shanghai"", ""Tokyo"", ""Tokyo"", ""Tokyo""),
                          t = c(1, 2, 3, 1, 2, 1, 2, 1, 2, 3),
                          exp = c(100, 120, 140, 150, 130, 700, 800, 600, 650, 750))
    cg <- data.frame(c = c(""London"", ""Paris"", ""Tokyo"", ""Shanghai""),
                     g = c(""Europe"", ""Europe"", ""Asia"", ""Asia""))
    cityexp
    cg
See if these look ok.

Then use dplyr. If you haven't encountered it before, read up on it, it makes life a lot easier when working with R.

I'm assuming the group means you're after are also time-variant.

    library(dplyr)
    groupedexp <- cg %>% 
      inner_join(cityexp, by = c(""c"" = ""city"")) %>% 
      group_by(g, t) %>% 
      summarise(groupexp = mean(exp))
    cityexp %>% 
      inner_join(cg, by = c(""city"" = ""c"")) %>% 
      inner_join(groupedexp, by = c(""g"" = ""g"", ""t"" = ""t"")) %>% 
      select(city, t, exp, groupexp)

Does anybody know if it's possible Markdown directly from a notebook, picking up code and results, and paste into a Reddit comment?",1520345745.0
Help_Quanted,PM’d. Would be happy to help.,1520332420.0
LiesLies,Also PM'd!,1520349786.0
bubbles212,"Looks like some posters already got back to you about the one on one help, but I just wanted to mention that you might find the dplyr package (part of the ""tidyverse"" collection of packages) useful for the first task you described. It has functions that work like SQL style joins for merging data sets with common variables.",1520353429.0
NotAnotherDecoy,"Here's what's being asked for, as I understand it:

Use the regression formula you developed pre-step 4) on your ""Test"" dataset to predict the y-response values expected based on the x-values in your ""test"" dataset. Then calculate the difference (or ""residuals"") between the predicted y (from the ""Train"" dataset regression) and the measured values (from the ""Test"" dataset) to estimate an average % predictive accuracy for your model. Repeat this procedure so that you analyze both a logistics and probit regression model on your ""Train"" dataset, compare the outputs between predicted and observed y values as described above in each case, and then compare the % accuracy across the model types to determine which provides the most accurate predictions. ",1520310369.0
MrLegilimens,So you understand 1-3 but can’t understand 4?,1520317270.0
The_Sodomeister,"They are the columns which will be included in the subset. You are not including all of the collumns from the original data, only the ones listed.",1520290957.0
phobonym,"I'm a bit confused by your title aaaaand is that a screenshot from a tutorial or something or are you presenting a problem of your own?

Anyways:
In order to drop the variables described in the text you could go with:

    training.data.raw[-c(1, 5, 12)]

The numbers are the column indexes, the minus makes these columns be dropped.

Or using dplyr you can use the variable names:

    library(dplyr)    
    select(training.data.raw, -cabin, -Ticket, -PassengerId)

Finally, if you insist on using subset you should leave out the indexes of the columns you want to drop, in the screenshot the index values do not correspond with the columns that should be kept. The correct command would look like that:

    subset(training.data.raw, select(c(2, 3, 4, 6, 7, 8, 9, 10, 11))

I'd recommend using dplyr's select. It's the most readable solution and you don't need to count the columns, which is an error prone process.

",1520296817.0
efrique,"From the help on `subset`:

>  `select: expression, indicating columns to select from a data frame.`

i.e. `c(2,3,5,6,7,8,10,12))` says ""keep the variables in columns 2,3,5,6,7,8,10,12""... which is the same as saying ""leave out variables in columns 1,4,9 and 11""


The numbers of the named variables to leave out don't seem to correspond to the order in the image though. You'll have to sort that out because as it stands it doesn't look right. ",1520305889.0
IPredictAReddit,"Any advantage of using gputools versus gpuR, especially given the installation process difficulty? I know gpuR uses OpenCL, which might not leverage all of the available power that CUDA can, but just getting gpuR to install on a VM was enough of a headache...",1520303552.0
shujaa-g,"As a side note, `:` is a shortcut for the sequence `seq` command. The columns you want don't follow a pattern, so metagloria's suggestion is best. But if you wanted, say, every 3rd column, you could use `data[, seq(from = 1, to = 12, by = 2)]`. You can try it out in your console first to see what you'll get: `seq(from = 1, to = 12, by = 2)`.",1520277442.0
metagloria,"If you're intending to pluck columns of the matrix ""data"" you need a comma in there: pairs(data[,1:10]).

Colon just means to include every integer between the two provided. 

c() allows you to make arbitrary vectors, so in your case  

    pairs(data[,c(2:4,5,6:10)]

would do the trick.",1520275533.0
coffeecoffeecoffeee,"data[, a:b] means ""Give me all integers between a and b, inclusive on both.""  If you wanted those columns, you could do something like data[, c(2:4, 7)].  It's nesting vector concatenation, which is allowed in R.",1520294265.0
efrique,"These links don't directly answer your questions but might add a little useful background (it's hard to guess what you are familiar or unfamiliar with from just a few words):


https://stats.stackexchange.com/questions/205301/lay-persons-explanation-of-sheather-jones-method-for-bandwidth-selection

https://stats.stackexchange.com/questions/33918/is-there-an-optimal-bandwidth-for-a-kernel-density-estimator-of-derivatives

https://stats.stackexchange.com/questions/43766/what-practical-application-is-there-for-the-asymptotic-mean-integrated-squared-e

",1520293969.0
mmoores,"The spline itself is already a type of average. If you want to compute the average of the average exactly, that is defined as a finite integral of the spline function over time. This is available in closed form, but will require a bit of algebra.

On the other hand, you can get a rough approximation by discretisation (turning the integral into a sum). Just call the R function predict, passing the 4 days as the new X values. Then take the average of those 4 numbers.

You can make this more accurate by passing in more X values that are closer together in time (every 12 hours, or 8 hours, or so on). This will converge to the same answer as the exact integral as the number of values increases.",1520288182.0
spinur1848,"Make sure you've got dates on everything, data and logs. Plan to repeat the downloads at a later time and compare subsequent downloads for differences.

Your raw data is your source of truth. You need to know if it changes underneath you.",1520257349.0
phobonym,"This is an interesting question. I failed to find comprehensive resources on that topic as well. Gonna share some of my practices.

- From my experience SQLite databases get corrupted often, when the API/source fails. Using PostgreSQL works great.
- Whenever one batch is done I would check the database for missing or failed entries, collect their IDs and rerun the data collection for those IDs. Storing the ID as a separate column in the database table is useful for that, of course.
- Some tutorials use lapply or purrr's map for the data collection, from my experience for-loops can be a bit more helpful, especially when dealing with sources that are failing often, since lapply et al. lose all requests that where successful up unto the failing one. With for loops only the failing request is lost and you can restart the loop starting with the ID of the request that failed. I usually do this part manually, but of course this could/should be automated with some error handling code.",1520298181.0
yeezypeasy,"Best way to do this in the plot is to fit a smooth line for the relationship of your outcome variable with time for each group. If you have data with outcome y, time, and group the ggplot code would be:

ggplot(data, aes(x = time, y = y, color = group)) + geom_point() + geom_smooth()

Probably what you have now, but with the geom_smooth() function.
",1520211780.0
LiesLies,Bruh.,1520230201.0
DemonKingWart,"You're not using sapply correctly, it does not apply the function to each row of a dataframe. 

If you want the two columns, you can just do crimes[, c(""latitude"", ""longitude"")].",1520204633.0
throughthekeyhole,"What kind of output are you expecting? With

    getLongLat(crimes)

you'll get a single vector... but I doubt if that's what you want? Probably more something like:

    cbind(crimes[[""longitude""]],crimes[[""latitude""]])

This way you'll get 2 columns. One with latitude, and one with longitude. No need for sapply, though if for some reason you need the data to be very wide, you can still pop the t() on.

To add some more detail after re-reading your post... the issue is not crimes[[""longitude""]]. That's fine. It's the incorrect usage of sapply. Drop the sapply and use t(getLongLat(crimes)), and you'll get output, though not in a very nice form.",1520204547.0
too_many_splines,"To simply get the `crimes` data.frame with just your longLat columns you needn't deal with iterating and attempted row-wise operations. It is as simple as...

    crimes.loc <- crimes[, c(""longitude"", ""latitude"")]

Now as for why your code isn't working, sapply was iterating across the *columns* of your `crime` table instead of the *rows*, as you had assumed. So `x` in the `getLongLat` was a numeric vector, not a single row data.frame.

This would work instead...

    crimes.loc=t(apply(crimes, 1, getLongLat))

But obviously, the first solution is much better.",1520204469.0
Ader_anhilator,For the kaggle competition? ,1520201324.0
spectrum_specter,"I would recommend grouping the players into the teams first before predicting, so the team vs team outcome is predicted instead of player vs player",1520205123.0
fang_xianfu,"If all you want to do is compute correlations, you can compute a correlation matrix of every variable in a dataframe with every other variable in the data frame with the function `cor`. After that you just manipulate the results to get the detailed answers you're looking for - for example, picking out the column for the variable you're interested in and sorting it, or applying a visualisation.

This is a super basic statistical approach, though. Age and experience are both correlated with income, but also with each other, so which has the most explanatory power? Or is it that none of those factors explains it and it's some other, hidden covariate?",1520115506.0
shujaa-g,"There's probably not a (common) R package to do it because it sounds very dubious statistically.

",1520179649.0
too_many_splines,Honestly this sounds like an awful idea. ,1520198908.0
PM_ur_good_deeds,"So I guess the data consists of a population's creedences for believing the Greens will win, yes?

If that's the case, you can find the number of people having answered <10% with

~~sum(poll[poll < 0.10])~~
    
    sum(poll < 0.10)

Try to adjust this line to find the number of people having answered >18%.",1520092922.0
fang_xianfu,"No. How severe the problem will be depends on your link function. From Wikipedia:

> While the OLS point estimator remains unbiased, it is not ""best"" in the sense of having minimum mean square error, and the OLS variance estimator does not provide a consistent estimate of the variance of the OLS estimates.

> For any non-linear model (for instance Logit and Probit models), however, heteroscedasticity has more severe consequences: the maximum likelihood estimates of the parameters will be biased (in an unknown direction), as well as inconsistent (unless the likelihood function is modified to correctly take into account the precise form of heteroscedasticity).[5] As pointed out by Greene, “simply computing a robust covariance matrix for an otherwise inconsistent estimator does not give it redemption.”[6]",1520097057.0
Atheriel,"The way I tend to think about employing the various standard error adjustments (robust, cluster robust, bootstrap, etc) is that they use a model of how the noise term is structured to inform more accurate standard errors, and need to be reasoned about and justified in the same way that your OLS model itself is.

In practice that your standard errors are larger using these methods and that perhaps may make you feel more confident about the statistical significance of your point estimates (if indeed they remain significant). But keep in mind they are still simply a model of the variance -- they do not ""eliminate"" anything in your data generating process, and no choice of standard error will affect your point estimates from OLS itself, which are themselves simply the result of fitting a model.

Robust standard errors are usually introduced to solve a toy problem (heteroskedacity, narrowly defined) with a linear model specification. But they don't mean much if your model specification doesn't have this precise problem -- and on real data, knowing the appropriateness of your model specification is a hard problem.",1520100007.0
occamsphasor,You may be interested in quantile regression to better deal with heteroscedasticity...,1520129774.0
Slabs,With A.,1520204115.0
questionquality,"This is more of a basic programming problem, really. Think about the logic you want your program to follow. First, unrar a rar file. Then look through the files you got, parse the filenames and filter the ones you want. Then you can load each of those interesting csv files as a data frame and add the information you parsed from the filename. Now repeat for all rar files, and combine all the data frames you got into one. Aaaand bingo. 

 
For each of those steps, google. Unrar might be the trickiest one, since it depends on which OS you're on. On linux I'd just do something like `system('unrar FILENAME')`. Others are more common R issues that you'll find lots of others have struggled with. `list.files()` to, well, list the files. The `stringr` package to parse the filenames. `rbind()` to combine dataframes into one. Etc.",1520075664.0
Ucspe,"Check out the dplyr library, specifically the dplyr::lag function (and mutate to create new column)",1520037367.0
questionquality,"I can see why you're having problems with mutate (and would have the same problem with apply etc) - each value depends on the previous value.  
  
A way to do it, provided you don't have too much data, is a simple for loop.

    # set the first value
    data[1,""warming_total""] = .8
    # now for row 2 and onwards
    for (i in 1:nrow(data)) {
        data[i,""warming_total""] = data[i-1,""warming_total""] + mu * (data[i,""Warming_rad_forcing""] - data[i-1,""warming_total""]
    }
",1520076392.0
TKirby422,"Can someone please enlighten me? Which add-ins was Hadley using in [this video](https://www.youtube.com/watch?v=go5Au01Jrvs&lc=UgwlGrQDSSD0t1nkJ854AaABAg)?

My Windows version of RStudio doesn't work like that.

I can't page through the data like Hadley did, and I don't see green checks and red x's when I install a package.",1520019910.0
this_time_for_good,"You're looking at some type of clustering unless you're looking to simply draw those spaces rather than compute them. Check out these two posts on K-means clustering in R: 

1. [UC Business Analytics R Programming Guide ](https://uc-r.github.io/kmeans_clustering)
2. [Cluster Analysis in R Simplified and Enhanced - STHDA](http://www.sthda.com/english/articles/25-cluster-analysis-in-r-practical-guide/106-cluster-analysis-in-r-simplified-and-enhanced/)

Those both have example code for producing plots with areas defined for each cluster. The UC Business Analytics R Programming Guide link also shows how to control the number of clusters. Let me know if you have any trouble getting it to work!

*** Edit: Just saw your comment on ggplot2 - you might have to branch out, but I would guess the packages used in my links are built on ggplot2. 
",1520009514.0
Amishjohnthomas,I would like to do so using ggplot FYI,1520002313.0
demyelinated,Best accomplished using convex hulls https://stats.stackexchange.com/questions/22805/how-to-draw-neat-polygons-around-scatterplot-regions-in-ggplot2,1520009019.0
Drewdledoo,"Good read. 

As a biologist who uses R to analyze biological/genomic data (read: someone not trained in CS), I sometimes have a hard time finding the balance between “minimize your dependencies when possible” and “don’t reinvent the wheel”.",1519955333.0
dkesh,Doesn't help that the most popular ecosystem of packages in R breaks compatibility constantly,1519968152.0
zdk,I feel like suggested packages are underutilized as well. Why not isolate dependencies from within the one function that needs a specific package?,1520014148.0
Darwinmate,How were the confidence intervals calculated? ,1519953509.0
infrequentaccismus,"This is an interesting question. The confidence intervals are all univariate right?  And the clustering is multivariate?  If so, then the distance should be calculated to the nearest end of the confidence interval. It shouldn’t take too much custom code to do this yourself. I’ll look around a it myself and see what I can find. ",1519961196.0
Ucspe,"Definitely interested, Visual Studio with R Tools does this by default in the variable explorer",1519951280.0
,I’m doing OK with excel <-> R interactions. I use two functions that copy df to clipboard to be easily pasted into excel and paste from excel as df in R. ,1519999331.0
_nt2,"Do you need more than what already works?

    openxlsx::write.xlsx(df, ""df.xlsx"")
    shell.exec(""df.xlsx"")

I'm not seeing the value of creating a graphical interface for this.
",1520004803.0
jimmyjimjimjimmy,"Definitely interested too.

My current workflow is a poorly written writeExcel function in a personal package. It spits a df into a csv file, saves it to my downloads folder (that's my junk drawer) with the filename yyyy-mm-dd-hms, then opens the csv with Excel.

I've meant to modify it to write straight to xlsx, freeze the first row, and add some logic to guess number formats or pct formats.",1519957510.0
arlinconio,"Readxl package is really good at importing from sheets and exporting into sheets in an xlsx, but I don't know of any way to copy a data frame as an array into the clipboard. It would be really useful.",1520001184.0
dudeasaurusrex,"If you want to open the excel file you've written from R, you can use the `system()` function. 

    library(readr)
    
    write_excel_csv(iris,path=""path/to/folder/iris.csv"")
    
    system(""open 'path/to/folder/iris.csv'"") # for osx
    
    system(""excel.exe 'path/to/folder/iris.csv'"") # for windows

You could even wrap all of it into one function:

    openinexcel = function(df,path){
      require(readr)
      write_excel_csv(df,path)
      
      os = Sys.info()[[""sysname""]]
      if(os==""Windows""){
        system(paste0(""excel.exe "", path))
      }else{
        system(paste0(""open "", path))
      }
    } ",1520023095.0
Drewdledoo,"If you are trying to plot a dendrogram using a distance matrix, why not just use `plot(hclust(as.dist(test)))`?

If you could edit your post with either the full data you are using or a small sample of it, it would be a lot easier to see what you are trying to accomplish.",1519950405.0
Deto,!!!  This is huge.  Can't wait to try it out on neovim.,1519926437.0
,what is the advantage of using this versus ESS in Emacs?,1519949374.0
maltiv,"You can access the input with input$text_input. 

In order to make your tibble react to changes, you need to use a reactive expression, e.g df = reactive({expression to generate your filtered tibble}).",1519931105.0
VincentStaples,"ggplot(df, aes(x=x, y=y))+    
stat_summary(fun.data = mean_cl_normal, geom = ""errorbar"")
",1519902567.0
samclifford,Using `broom::tidy()` with `conf.int=T` on an object with class lm will return a tidy data frame that you can feed into ggplot2 and plot with the `geom_pointrange()` geometry to show the estimates and lower and upper bounds of the confidence intervals. ,1519940682.0
The_Sodomeister,"Do you want like a parallel coordinates plot? Or what do you want the plot to look like, in terms of x and y?",1519932943.0
Darwinmate,"How are you reading in the file?

I just read in using `read.csv` and it formats all correctly. Either you're using an old version of R or something else is happening such as how you're reading/handling that file. Where does the `?` get produced, in the reading or the output? ",1519886618.0
Drewdledoo,Can I ask why you need to read emojis in from that file? There are a few packages out there that enable the usage of emojis in R — more detail about your use case could point you to a package that’s already been made!,1519883713.0
guepier,"This isn’t an R problem, it’s a problem with the library you’re using (and it’s unclear how you’re using that anyway, since the package seems to be for working with JSON data, not pseudo-CSV).

As you’ve noted yourself these two strings simply don’t correspond at all (and it isn’t just the `'�'`^(1) that’s the problem: `'\x9F'` ≠ `'\u009F'`!). At any rate, if you just read the textual data you will end up with a data frame with rows looking like this:

      Description     Native                Bytes               R-encoding
    1    DOG FACE \U0001f436 \\xF0\\x9F\\x90\\xB6 <ed><a0><bd><ed><b0><b6>

And the “Native” column contains your correct UTF-8 byte sequence. Yay:

    > message(x$Native[1])
    🐶

You can easily obtain the above just with the following code:

    x = readr::read_delim('https://raw.githubusercontent.com/today-is-a-good-day/emojis/master/emDict.csv', delim = ';')

(Or using `read.table` from base R.)

---

^(1) Incidentally, � (U+FFFD) is the [Unicode replacement character](https://www.fileformat.info/info/unicode/char/fffd/index.htm). You’re usually getting this when a character isn’t representable in the current encoding or font. It’s completely unclear why it was inserted here.",1519913415.0
Sa-lads,Could you not replace the '\u00' s with '\x' s drop the question mark and then do a partial string search. ,1519879841.0
WigglyHypersurface,I've had this problem with Twitter data in R. See this stack overflow https://stackoverflow.com/questions/37999896/twitter-emoji-encoding-problems-with-twitter-and-r,1519947313.0
Drewdledoo,"Hmm, interesting. Mine actually displays the emoji. Are you on Windows? I’m on Mac. I’ll dig into it a bit more, but not right now. 

Update if you figure something out!",1519953143.0
Vickerspower,"Is ‘treatment’ in your study the diet they were fed and each tank is fed a different diet? If that is the case then your tank and treatment variables are essentially the same thing, just a different way of it being coded. In that circumstance the random variable of tank would control for all the variation between your treatments and you would see no effect of treatment. You’d probably get a warning message in that case telling you that you have probabilities of 1 occurring.

You say each tank had a very large number of fish in them. I assume the sample of 50 was randomised each time? If you had repeated measures on the same specific fish then fish ID would be ideal as a random variable. 
If you random sampled then you are running the risk of stochastic chance in your sampling driving differences that are seen and that’s not possible to control for as far as I know. That being said, it’s highly unlikely with your sampling size.",1519859741.0
intothelionsden,Why is the treatment * observation interaction meaningful? ,1519883469.0
EuGENE87,"I think you are right in your model. If the diet is something that is different (on purpose) between tanks, AND you expect different effects between the levels of that factor to be the same no matter how times you repeat the experiment (eg more food more weight). Then, it is a fixed factor.

In the case of the tanks, it *could* have an effect as they are different environments and could have differences in someway that you dont control (different exposition to sunlight). The point is that for random variables you cannot define a set of known levels, and you cannot predict how will be their behaviour if you repeat the experiment.",1519889764.0
The_Old_Wise_One,"I have never used the package, but it should definitely not take that long to complete. Comparing means should take seconds. ",1519865555.0
hummingbirdz,Try one of the other mcmc packages.  I believe there is a good one called mcmc or mcmc_pack or something like that. If you are just dabbling a basic algorithm is probably the place to start.,1519867762.0
SataMaxx,"First of all, don't use loops, use `map`. You also don't need to include **magrittr**, the pipe is in the **tidyverse**, as well as string manipulation functions from **stringr** that I'll be using here.

Also, **rvest** has a function to extract html tables, `html_table`.

Thus you can do:

    str_c(patriot_url, team_list) %>% # drop the %s in patriot_url, btw
      map(~read_html(.) %>%
              html_table %>%
              .[[1]] %>% #extract only table from the built list of tables
              setNames(c(""Rk"", ""Season"", ""Conf"", ""Overall W"", ""Overall L"", ""Overall WL%"", ""Conference W"", ""Conference L"", ""Conference W-L%"", ""SRS"", ""SOS"", ""PTS"", ""PTS2"", ""AP Pre"", ""AP High"", ""AP Final"", ""NCAA Tournament"", ""Coach(es)"")) %>% # html_table should catch the column names, but the tables are not correctly formed in the html
            filter(!Rk == ""Rk"")) %>% # filter the repeated ""header"" lines
    bind_rows -> data # assemble the table",1519848857.0
too_many_splines,"Your final statement should look like this:

    patriot <- vector(""list"", length(team_list))
    for(team in team_list) {
        patriot_url <- sprintf(patriot_url, team)
        patriot[[team]] <- read_html(patriot_url) %>%
            html_nodes(""td[data-stat]"") %>% 
            html_text() %>%
            str_trim %>%
            matrix(ncol = 17, byrow = T) %>% 
            as.data.frame
        Sys.sleep(1)
    }
    patriot_combined <- do.call(""rbind"", patriot)",1519848032.0
,"It all depends on the context of the problem you are working on. R understands specific locations in a matrix in the form y[i,j], where i is the row and j is the column. 

For instance, let's say we have two matrices as below:

    y <- matrix(rnorm(25),nrow=5, ncol=5) #A 5x5 matrix with randomly generated values
    z <- matrix(0,nrow=5,ncol=5) #A 5x5 matrix with all zero values

You can simply pick a specific row of y and assign it to a specific row of z and vice versa. This is achieved as follows:

    z[1,] = y[3,] #Replace row 1 in z with row 3 from y
    z[2,] = y[5,] #Replace row 2 in z with row 5 from y

    and so on....

This idea can be extended to loops, but how to go about it depends on how you are selecting the rows. Hope this helps as a start.",1519837304.0
hummingbirdz,"You should not store so many matrices freely in the global environment. 

When you create the matrices place them as slices of a 3D array.  This is much more efficient and makes it easy to find matrix j.

Once you have them in a 3D array you can then use regular R assignment. i.e bigarray[c(rows I want),,j]

Rule of R code #45: if you're using get and assign, then stop and re-evaluate.",1519867558.0
infrequentaccismus,"What are your five variables and their data types?  It is likely that you can put two variables on your x and y axes, two variables in a facet_grid() and use a binary variable to color by. But a lot depends on what the types are. ",1519799895.0
dudeasaurusrex,One quick and dirty way that might give you a better idea of what you want to do is to use the `ggpairs()` function from the GGally package. It has some nice defaults.,1520019585.0
thedukeofedinblargh,"I'm guessing your real problem here is that you're confused about what's an observation and what's a variable.  If you're willing to share what these things actually stand for, you'll probably get much better help.

That said, here's my best guess at what you're trying to do.  As you'll see, I basically reshape the file to treat the number in the column name as a new version related to the observation, at which point it's pretty easy to do the case_when's.  If you needed to, you could then use spread to spread your X's back out into X1, X2, etc.

    library(tidyverse)

    df = tribble( # make the dataframe
       ~'A1', ~'A2', ~'A3', ~'A4', ~'B1', ~'B2', ~'B3', ~'B4', ~'C1',     ~'C2', ~'C3', ~'C4'
       ,120 ,100 ,NA ,110 ,1 ,1 ,NA ,1 ,NA ,NA ,NA ,NA
       ,100 ,NA ,115 ,NA ,NA ,NA ,NA ,NA ,'Y' ,'N' ,'Y' ,'N'
    ) %>%
      rowid_to_column(var = 'id') # add an id variable to your rows

    gather(df, key = 'key', value = 'value', -id) %>%
      separate(key, sep = 1, into = c('col_letter','col_version')) %>%
      spread(col_letter, value) %>%
      mutate(X = if_else(A == 100
                          & (B == 1
                             | C == 'Y'
                          ) , 1,0)
      ) 

Then again, I'm pretty confused by the question, so if nothing else, I made the tibble for others to work with.",1519796907.0
giziti,"Are they selling software it just having you solve problems? There is no real quandary with internal use, the relevant parts of open source come into play when you start selling software. ",1519798522.0
fang_xianfu,"This is ubiquitous for anyone who creates anything in their work. Art, code, product designs, whatever. Part of what you are selling in exchange for your salary is the intellectual property rights in what you create. That's just life. If you try to take things with you when you leave, you will be in breach of your contract.

This does mean that by default, you can't contribute back to open source projects with code you write on the clock. You don't own the IP rights so you can't give anyone else a license to them or have the right to publish them.

You should also check what the contract says about intellectual property you produce on your own time. There are lots of variations of these terms but your company may demand certain rights in your side projects while they employ you.

Finally, if there are things you've produced already that you want to protect, make sure their existence is publicly documented, for example on github. If you can't prove you made it prior to your employment you may have problems.

There are no R-specific matters with any of this though. You'll just have to decide for yourself if it's worth it.",1519798475.0
Crypt0Nihilist,"Unless they're selling software, all it's really saying is that they own everything that you do. It's not like you can't get another job and they sue you because you wrote

    library(dplyr)

in code for them and your new company. What you can't do is use your river algae prediction and visualisation script at your next company because you're agreeing here that it is theirs, not yours.",1519829973.0
spinur1848,"This is where open source licences help. If you develop something on top of technologies that are covered by the GPL, then those technologies much be covered by the GPL (or a compatible open source licence) as well. 

Your employer can retain the copyright and they can even dual licence the code with something more restrictive if they want to. They must still release the source code with the original licence.

Its probably best to have a chat with them first about expectations. If they really want exclusive economic rights to anything you produce for them, you will pretty much be restricted from using anything covered by the GPL. ",1519774070.0
don_draper97,"What are the ways you've tried? The standard code for ANOVA would look something like this:

fit1 <- lm(ammonia ~ distance)

fit2 <- lm(nitrate ~ distance)

anova(fit1)

anova(fit2

Which you could then do for each site. One method I would use to check would be to make a boxplot of what you're trying to test. Was it easy? If not, then you may have to do some additional data cleaning to make the data more appropriate for ANOVA.



Edit: If this wasn't at all helpful to what you're attempting to do, check out this page on ANOVA tests in R. https://www.statmethods.net/stats/anova.html
",1519762824.0
VincentStaples,summary(aov(df$Outcome ~ df$Group)),1519764623.0
shoneone,"If distance is continuous then I suspect a linear regression would be appropriate, and you would determine whether the slope is greater or less than expected. If distance is a discrete factor, not continuous, ANOVA would confirm whether there is a distinction between the different distances. Is there a way to divide the distances into two or three groups, near the source and far? Then ANOVA between the two distances might be simplest. 
",1519781318.0
RememberToBackupData,"Did you find [this tutorial](http://marcoghislanzoni.com/blog/2014/09/01/pivot-tables-r-dplyr/)?

Exploratory also [has a pivot table function](https://blog.exploratory.io/a-quick-introduction-to-pivot-command-in-exploratory-8390512372ea) that plays nice with tidy data, if that's the direction you want to go. You can install their package without using Exploratory itself.",1519711404.0
wolfesmc11,"If each line is unique then you can do this:

df %>% 
    group_by(Year, Age, SiteName) %>%
    summarise(n=n()) %>%
    ungroup() %>%
    spread(SiteName, n) %>%
    arrange(Year, Age) %>%
    mutate(GrandTotal=BON+ICL)

Does that answer your Q?
    ",1519711667.0
Darwinmate,"provide dummy data and required output so we can help. also format your code properly. 

What error are you getting? what package are y

`spread` converts long to wide not what you think it is.",1519702899.0
efrique,"1. The function in the normal distribution of R, `wilcox.test` (which performs either a *Wilcoxon-Mann-Whitney* test or a *Wilcoxon signed rank* test) has a lower case first letter (and *case matters in R*, don't type uppercase unless you mean it). If you're really using a function from elsewhere that does have an upper case first letter, *you have to tell us what package it's from*. There might well be a bunch of functions with the same name, potentially with different ways to call them.

2. The two sample Wilcoxon-Mann-Whitney test *is not a comparison of medians*. It can reject the null when the sample medians are identical!

3. The easiest way to do it with `wilcox.test` would be to use the formula interface: `wilcox.test(response~group)` (where ""`response`"" is the name of the variable you're comparing and ""`group`"" is the binary group indicator)
",1519691373.0
tomQ11,median(isocornoil4hrctrl.dat$p.greg),1519682305.0
anthracene,Use str_split() to split the string at the parenthesis into two new variables.,1519679344.0
RememberToBackupData,Use tidyr's `separate()` function to split into columns based on a delimiter.,1519718511.0
sohaibhasan1,Any R script can be run in its entirety from command line by using the Rscript command along with the filepath. ,1519619892.0
efrique,"If you're using plain R (rather than say running via R-Studio), check out the second and third menu items in the ""File"" menu. You can also do it from the command line of an OS rather than from within R. (This is all in the copious documentation that comes with R)

",1519622751.0
danielw29,"If you are new to R I'd recommend RStudio. When you open it you can create a new file by clicking on the white sheet on the top left corner. It will present you with a couple of options. Rscript is probably the closest to a do file. You can write in the script and send each line to the console with Ctrl-Enter. You don't have to highlight the line or anything just put the cursor in the line (I think in stata you have to highlight. ;)) You can run multiple lines but highlighting. Also check out Rnotebook. It's awesome because code, output and text can all be combined in the same document. Similar to jupyter notebook. ",1519673383.0
not_rico_suave,"Do your coding in R Studio. It has both the command line and a ""do-file"" script.",1519676447.0
MrLegilimens,Who types things not into a script first?,1519627876.0
Tarqon,"Subset your x vector with the same indices you're using for your C matrix.

N.b. Using row indexes like this is error-prone and not recommended.",1519637185.0
Resquid,Screenshots of code. Really?,1519626800.0
ohheyitsdeejay,"Maybe I’m misunderstanding, but subset would give you two different dataframes based on the conditions of the function, not two new variables. ",1519612995.0
efrique,"The *first* code example at the bottom of the help on `subset` seems to be directly relevant to your problem, with only the teeniest bit of effort. 
",1519623143.0
SabbyPlays,"msports <- subset(NeopilQ, sex == “m”) 
View(msports) 

And similar for females ",1519616236.0
LostMasterpiece,"Where am I going wrong now? 

df3<-subset(df2, sex==""m"", select= -neur, -extr, -open, -agre, -cons, -iq1, -iq2, -height, -weight)

Trying to get rid of the extra columns


Error in `[.data.frame`(x, r, vars, drop = drop) : 
  object 'extr' not found

extr is an object though",1519617280.0
AnInquiringMind,"Subsetting does not quite work that way. You need to create a boolean vector and then use that vector to subset.

Try:

Dataset[Dataset$Status==""Control"",]

EDIT: as a sidenote, you may want to try using dplyr for your process. You can chain simple operations on groups (like control and disease) quite easily.",1519597261.0
samclifford,"You can either use the `subset()` function and do `healthy <- subset(Dataset, Status == ""Control"")` or look at using a logical vector as /u/AnInquiringMind suggests. I agree with their comment about learning to use dplyr for manipulating data frames.",1519598643.0
tikeshe,A boxplot would work fine. Potentially look into violin plots too. ,1519595146.0
efrique,"With so few data points I'd want to see every one of them; a boxplot is fine but I'd put the data over the top, either as a jittered stripchart or a centered swarm plot or something along those lines.

https://i.stack.imgur.com/ue0jI.png
",1519597400.0
infrequentaccismus,"http://adv-r.had.co.nz/Style.html

Indent each geom one tab more than your call to ggplot. ",1519571578.0
RememberToBackupData,"Your IDE (assuming you're using RStudio) deals with that automatically, no? Otherwise you can just inset all of the subsequent lines.


    # VERY exaggerated example:
    
    ggplot(data = filename, aes(x = xvar, 
                                y = yvar)
          ) + 
        geom_point() +
        geom_jitter()
    ",1519542471.0
Ambivalent_Warya,"If you're using RStudio, I think the editor will automatically indent after you press ""+"", then ""SPACE"", then ""ENTER"". It then breaks to a new line with the indentation already done for you.

I could be wrong, i haven't fired up RStudio in ages, but I recall it being fairly intuitive. ",1519542650.0
,I like to use RStudio’s reformat code and reindent code. Makes everything look nice. ,1520000251.0
deanat78,"You should look into this alternative that came out last year, it's worked great for me and is very simple https://ficonsulting.github.io/RInno",1519540430.0
xiaodai,I am the original author of the blog that the blog you referenced referenced. I can look into it for you for a fee. Please do pm if you require my services.,1519531148.0
JoshLemonLyman,"Assuming you have a variable ""school"" (e.g., my school, other1, other2, other3, other 4), you can use size = school in your aes() and then do a manual scale to make them whatever size you want. So this would let you set the size of ""my school"" to 2 and the others to .5:

ggplot(df, aes(time, enrollment, size = school))+  
geom_line()+  
scale_size_manual(values = c(2, .5, .5, .5, .5)) 

You can also manually set (e.g.,) linetype and color this way... Making the others dotted but yours solid, or making the others light colors but yours bright...

ggplot(df, aes(time, enrollment, size = school, color = school, linetype = school))+  
geom_line()+  
scale_size_manual(values = c(2, .5, .5, .5, .5))+   
scale_color_manual(values = c(""red"", ""light grey"", ""dark grey"", ""black"", ""azure""))+   
scale_size_manual(values = c(""solid"", rep(""dashed"", 4)))  

In order to consolidate legends if you do manual values (or just to set the name/labels in the legend), just add ""name"" and ""labels"" to the scale manual function like:  
 
scale_color_manual(values = c(""red"", ""light grey"", ""dark grey"", ""black"", ""azure""),  
name = ""School"", labels = c(""My school"", ""School 1"", ""School 2"", ""School 3"", ""School 4""))    

*(corrected my parentheses)

And if you do multiple manual inputs, you'll have to make sure the name/labels are the same across each one (e.g., in both size and color) so it will show one legend displaying both size and color, not one legend for size, and another for color. ",1519507855.0
infrequentaccismus,My favorite way to emphasize one line is to add a column that is true for the category you want to emphasize and false for all others. Then set alpha = category in aes().  This way it will be more prominent and the others will be faded. ,1519505809.0
SeveralBritishPeople,"What do you mean by “more defined”? Depending what you want, aesthetics like size, alpha, and color can control line width, transparency, and color. You could also plot all the schools with the same settings and overlay another geom_line with just the school you’re highlighting to ensure its layered on top of the others. ",1519503392.0
infrequentaccismus,"An alternative to my other post.  I used data from : https://nces.ed.gov/edfin/longitudinal/.

    library(tidyverse)

    read_csv(""Year,Park City,Provo,Rich,Salt Lake
    1990,4716,3274,4851,3820
    1991,5024,3242,4706,3607
    1992,5407,3878,5492,4309
    1993,5530,3600,5945,4523
    1994,5999,4089,6103,4654
    1995,6284,4272,6294,5160
    1996,6519,4629,7421,5473
    1997,7653,5294,7797,5746
    1998,8257,5596,8941,5975
    1999,8147,5761,8502,6344
    2000,8891,5660,8541,6778
    2001,9311,6424,10027,6983
    2002,9955,6552,10803,7094"",
    col_names = T,
    skip = 0
    ) %>% 
      gather(District, Funding, -Year) %>% 
      mutate(Year = as.integer(Year),
             Funding = Funding / 1000,
             my_district = ifelse(District == ""Park City"", T, F)
             ) %>% 
      ggplot(aes(Year, Funding, group = District, alpha = my_district, linetype = my_district)) +
        geom_line() +
        scale_alpha_manual(guide = F, values = c(.6,1)) +
        scale_linetype_manual(guide = F, values = c(""dotted"", ""solid"")) +
        scale_x_continuous(name = element_blank(), breaks = 1990:2002, minor_breaks = 1990:2002) +
        scale_y_continuous(name = ""Funding ($M)"", breaks = seq(0,12,5), minor_breaks = 0:12, limits = c(0,12), expand = c(0,0))",1519586035.0
giziti,"Collaboration with git and github (presumably a private repository) is a great idea, though the collaboration part has a small learning curve associated with it, especially when it comes to merging differing branches. However, it's much better than dealing with Dropbox and Google Drive or doing anything that requires manual merging. Plus, this is a perfect opportunity to learn the tool.

Here's a great, mostly painless, introduction: http://happygitwithr.com/",1519505606.0
too_many_splines,This sounds dangerous. You expect users to enter their credentials which you will use in plain-text to connect to the db?,1519502752.0
CastorpH,"This might work

page <- read_html(url) 

page %>%
 html_nodes(""li"") %>%
 html_attr(name = ""data-messagescore"")

page %>%
 html_nodes(""li"") %>%
 html_attr(name = ""data-thumbusers"")
",1519491295.0
zdk,"Without actually running your code (you did not provide any data) I'm guessing that `class(glm1)` is a character vector.

For example try this:

    if (c('one', 'two') == 'one') print('foo')

Replace the condition with `inherits(glm1, 'try-catch')`.",1519426372.0
rampje,Could you post some code to show what you are doing? Converting factors to character may improve performance.,1519423043.0
infrequentaccismus,"Mutate a new column in the second dataframe as a value rounded to the level of the column in the  first. Then group by that column, summarize the values, and join with the first. ",1519425286.0
FactoidHunter,"dplyr::left_join(df1, df2)",1519448021.0
efrique,"> Apologies for the formatting.

What you need to do is indent your output by 4 spaces.",1519455291.0
CJL_LoL,"If the data is quite large I'd recommend using the data table package. There's a fairly simple to use function called merge, or more complex and speedy ways to merge if it's still too slow. Give me a shout if you need more info and I'll boot up my pc",1519476024.0
the_real_ch3,If by Business Objects you mean SAP the answer is most likely no there’s not. SAP is pretty well known for refusing to play nice with others,1519403129.0
Eleventhousand,Just access the databases on which the universe sits on top of.  RODC package.,1519409301.0
SecretAgentZeroNine,"Preface: I think you're using terminologies that are not exactly universal, or at least widely known outside of your job.

Python is a great bridge for this kind of stuff, but without knowing the details, we can only give you vague ideas.

Whatever product that's storing the data, check it's online blog and see if there's an API you can have R tap into.",1519401367.0
fangnp,"I think the construction of the data frame is confusing.

In a dataframe, you should have one variable per column and let observations on individuals be the rows.

In your dataframe, you have two rows for ""Bill"" and ""Bob"". Similarly, you don't have to put the string for the columns and row names in the data set. In fact, this may make it more complicated to do what you want to do; use the colnames() and rownames() functions to label your rows and columns after the construction of the dataframe.

At this point in time, I don't know if the Bobs or Bills are supposed to be distinctly different Bobs and Bills or if they're the same Bobs and Bills.

After you make a viable data frame, you can use the filter() function in dplyr to select columns and take their means. ",1519409595.0
efrique,"Tips:

- Indent code 4 spaces, like so

        A <- c(""Names"", ""Bob"", ""Bob"", ""Bill"", ""Bill"") 
        B <- c(""C1"", 2, 3, 1, 3) 
        C <- c(""C2"", 4, 3, 1, 3) 
        D <- c(""C3"", 4, 1, 1, 3) 
        E <- data.frame(A, B, C, D)
    
        F <- c(""Names"", ""Bob"", ""Bill"") 
        G <- c(""C"", 3, 2) 
        H <- data.frame(F, G)

- If possible avoid mixing character and numeric in a single column, because it forces your numbers to be character. This will cause you difficulty

- if you realize it needn't be like this, don't make it like this. Let's try to solve the problem you have rather than try to fix this approach.",1519410980.0
Oddy-7,"I didn't quite get what you are trying to achieve. In your first dataframe E, are you aware that you are working with your column names (Names, C1, C2, C3) as first data entry? 

edit: Just read your last sentence. Nevermind the last question.

Still, what are you trying to achieve? Calculating column means?",1519398423.0
EdinDevon,"I find this kind of thing much easier in data.table.

Where I think you'd want something like

     dt[,.(mean(c1,c2,c3)),by=name]",1519412148.0
Tarqon,"It depends on how large your data set is. I'd use regular expressions to construct some examples. If your dataset is small you can simply extend that to all the values, otherwise try training a classifier. Perhaps you can adapt a part-of-speech model to your problem or something similar.",1519401234.0
itsmeFrick,"http://eriqande.github.io/rep-res-web/lectures/making-maps-with-R.html

I would recommend using ggplot2 instead of maps package.",1519408297.0
_nt2,"This is a homework question (and a very easy one.) 

This is a probability question.

This subreddit is about the R programming language. ",1519335415.0
efrique,This has nothing to do with R,1519337563.0
mattindustries,"Drop the e, make it Ric.

    R
    in
    console",1519345188.0
thefringthing,"rtist  

rtwork  

rtifact  

rtichoke",1519347820.0
pitnips,ryce?,1519336815.0
efrique,archon,1519337724.0
xiaodai,shuidao,1519341476.0
metagloria,rMatey,1519359569.0
_nt2,rAndy3k,1519397688.0
cumin_clove,"If your data is at all dense spatially, I’d recommend a GAM. Simon Wood (author of mgcv package) has an excellent book with nice examples in R.",1519332732.0
SWAYYqq,"[This](https://training.fws.gov/courses/references/tutorials/geospatial/CSP7304/documents/PointPatterTutorial.pdf) should help you with any modelling of spatial point patterns. 

Generally, I'd recommend you look into [Geographically Weighted Regression](https://rstudio-pubs-static.s3.amazonaws.com/44975_0342ec49f925426fa16ebcdc28210118.html) (GWR) and into [Gaussian Processes](http://www.maths.bath.ac.uk/~jjf23/brinla/gpreg.html). ",1519379680.0
itsmeFrick,"Also check out SPDEP package

https://www.rdocumentation.org/packages/spdep/versions/0.7-4/topics/lagsarlm

I can help you through this one if needed.",1519408070.0
samclifford,"What is the question, in plain language, that you're trying to answer? ",1519306743.0
millsGT49,On mobile but google “Tidyeval” and there are a ton of resources that should help. ,1519262238.0
ryapric,[Give this a watch](https://youtu.be/nERXS3ssntw),1519267598.0
oreo_fanboy,Tidy Eval is a way of life!,1519267984.0
erudite_eros,"I've had success adding dplyrs version of escape chars which is ""!!!""

If you have exactly one grouping variable: 

    test <- function(data, grouping_var) {
        res <- data %>% group_by(!!!grouping_var) %>% ...
    }

If your function takes a varying number of vars try this...

    test_mult <-function(data, ...) #... are the grouping vars {
        group_by_vars = quos(...)
        res <- data %>% group_by(!!!group_by_vars) %>% ...
    }",1519276085.0
zdk,"How about 


    test <- function(df, c) df[,as.character(substitute(c))]
    iris %>% group_by(Species) %>% do(test(., Petal.Width))

",1519262208.0
tdawry,"This may help you in the future.

[So you’ve been asked to make a reprex](https://www.jessemaegan.com/post/so-you-ve-been-asked-to-make-a-reprex.html)
",1519269792.0
nobadchainsmokers,You can get zip code lat/long from the `zipcode` package and plot with `ggmap` and `ggplot2`. Then merge it with the data you have. Remember to consider leading zeros.,1519236075.0
msidari,"I’m sure there’s a much better way but I’ve done it as follows before:

a <- rnorm(N, mean, SD)

b <- a + rnorm(N, 0, SD*.5)

Essentially, you can manipulate the correlation between a and b by manipulating the difference in their standard deviations. The smaller you make b’s SD relative to a’s SD, the stronger the correlation. So .5 is just an example but you can play with it until you find the desired value for your correlation. 

Again, it’s not ideal, but it is very simple so at least you won’t need to learn any new code :)",1519207083.0
efrique,">  is it possible to create a longitudinal dataset that also has an assumed correlation between the variables? 

You have a known collection of means and variances and correlations and want to generate from a multivariate normal distribution?

Yes, so as long as your correlations form a valid correlation matrix, you can generate data from such information.
",1519223920.0
xiaodai,Google copula r,1519282313.0
Alex_Pan,"This is something close to what you are looking for. To my knowledge, there is no easy way to do it. I've basically made a boxplot, hard-coded coordinates for a white box to cover half of it, and then plotted the points on top it.

	library(ggplot2)
	data(iris)



	ggplot(iris) +
		theme_bw() +
		theme(line = element_blank()) +
		geom_boxplot(aes(x = Species, y = Sepal.Length),
					 alpha = 1, size = 0.75, width = 0.25) +
		annotate(""rect"", xmin = 1, xmax = 1.5, ymin = 0, ymax = 8, alpha = 1, fill = 'white') +
		annotate(""rect"", xmin = 2, xmax = 2.5, ymin = 0, ymax = 8, alpha = 1, fill = 'white') +
		annotate(""rect"", xmin = 3, xmax = 3.5, ymin = 0, ymax = 8, alpha = 1, fill = 'white') +
		geom_point(aes(x = as.numeric(Species) + 0.1, y = Sepal.Length),
				   alpha = 0.5, position = position_jitter(width = 0.1))


Might I suggest something simpler? (your preference)

	ggplot(iris) +
		theme_bw() +
		aes(x = Species, y = Sepal.Length) +
		geom_point(alpha = 0.5, position = position_jitter(width = 0.1)) +
		geom_boxplot(alpha = 0, size = 0.75, width = 0.25)

Edit: Violin plot (great point from comment below)

	ggplot(iris) +
		theme_bw() +
		aes(x = Species, y = Sepal.Length) +
		geom_violin(alpha = 0, size = 0.75, width = 0.5) +
		geom_boxplot(alpha = 0, size = 0.75, width = 0.1)
",1519185607.0
mattindustries,"No idea, but have you looked into pirate plots? It might work for what you are looking for. ",1519184573.0
tronj,Plotly has a side by side version I Iike better than overlayed. But it's not the half boxplot style if that matters to you. ,1519186268.0
miniocz,It think it could by possible in ggplot by plotting boxplot without whiskers and outliers  + plotting jitter and shift one to the left and the other to the right. Error bars would have to be plotted separately. This is theory as I do not have time now to try it :/,1519196413.0
jahutch2,"You should be able to accomplish this in ggplot using position_dodge() for the boxplots and position_jitterdodge() for the points. I don't think you need to use any white boxes, but I'm on mobile so can't say for sure.",1519219489.0
,Curious about this code but in base R! That'd be creative. ,1519190017.0
saikjuan,"Hey, I have a question related to this graph. I clearly see that the left side is the boxplot, and as to what I read, the right side is sort of a scatter plot... but of which vars?

I'm kind of confused on what the right side means.",1519225330.0
D-Juice,Answered in this Twitter thread about the exact same plot: https://twitter.com/helenajambor/status/965929984366927872,1519226176.0
,"Thanks for making this -- I used it just the other day and it works perfectly, with the low overhead that I want from a logger.",1519175433.0
ryapric,"I forgot to submit a URL field along with package submission, but [you can view the README here](https://github.com/ryapric/loggit).",1519165783.0
Bayesbayer,"    library(tidyverse)
    
    subcode <- function(code, length)
      ifelse(nchar(code) >= length, 
        substr(code, 1, length), 
        NA)
    
    data_frame(code = as.character(c(1, 11, 1101, 1102))) %>% 
      mutate(
        lvl1 = subcode(code, 1),
        lvl2 = subcode(code, 2),
        lvl3 = subcode(code, 3),
        lvl4 = subcode(code, 4))
   
    # A tibble: 4 x 5
      code  lvl1  lvl2  lvl3  lvl4 
      <chr> <chr> <chr> <chr> <chr>
    1 1     1     NA    NA    NA   
    2 11    1     11    NA    NA   
    3 1101  1     11    110   1101 
    4 1102  1     11    110   1102 ",1519168035.0
Sir_not_sir,Looks like you've fucked yourself.,1519159976.0
tomQ11,"sum (grepl ('5', dataframe_name $type)) should work for what you want ",1519159406.0
Im_int,"You can try `freq <- as.data.frame(table(df$type))` which will create a frequency table. Then you can view this new dataframe directly or use something like `freq[which(freq$type=='1a'),]` to get info about specific codes. ",1519166130.0
Alex_Pan,"Here is a tidyverse answer that may resemble English more:


    library(tidyr)

    v1 <- c(1, 2, 3, 4, 5)
    type <- c('1a', '2b', '5c', '5a', '5c')

    df <- data.frame(v1, type)

    # separate is a function from tidyr that splits `type` into `letter` and `number` based on 
    # position 1 of the variable
    new_df <- separate(df, col = type, into = c('number', 'letter'), sep = 1)

Output: 

      v1 number letter
    1  1      1      a
    2  2      2      b
    3  3      5      c
    4  4      5      a
    5  5      5      c

Then you can do any of the normal data frame stuff. e.g.

    sum(newdf$number == 5)

Output:

    [1] 3
",1519171650.0
dm319,Have you managed to read the file in?  Do you know how the columns are separated - commas or tabs?,1519168301.0
Orange_jb,"Taking more CS courses on there rn. First time taking online courses for me. I think it varies greatly by instructor, so just reading reviews and looking for the best course is definitely your best bet. And make sure you buy when it’s around $10 (I think they’re having it still now for Presidents’ Day?) bc they run a ton of sales so it’s not worth paying too much. ",1519156342.0
Ruffie1234,I would so recommend you to me. It’s usually only 10 bucks at most so even if things aren’t perfect you’re going to learn quite a bit from just 10 bucks. I have found several instructors that I really like so I buy anything dealing with that area of topic from them and go through it all.,1519167949.0
Revanchist95,"Great as intro, not good for deeper stuff. I took some ML courses in Python and R, and it got me familiar enough with the languages and the ML libraries in each language that I can start doing things with books or by myself. Coursera/EdX is better it you want to delve into the more standard theory/math stuff. ",1519171034.0
standard_error,"The simplest way would be to estimate a single OLS regression of the form 

    y = a + b * x + c * over50 + d * over50 x + e, 

where over50 is an indicator variable equal to 1 for x > 50, and zero otherwise. The c coefficient gives the difference in intercept, and the d coefficient gives the difference in slope.",1519145977.0
efrique,"While it's straightforward to test for a slope change at a known breakpoint k by adding a predictor (x-k)_+ (corresponding a linear spline basis), you have a problem:

> Notice how the slope changes at point 50. 

plus

> What is the best way to determine if the difference in slope and/or intercept is significant at p<0.05 before/after time point 50?

is clearly

https://en.wikipedia.org/wiki/Testing_hypotheses_suggested_by_the_data

as such, the usual p-values will be meaningless.
",1519169039.0
Tarqon,Piecewise linear regression seems suited for this. Check out the 'segmented' package.,1519296170.0
The_Sodomeister,"> If so, is there a p-value associated with it to find out if it is significant?

I believe coefficients in an ARIMA are normally distributed. Therefore, your 95% significance level would be 1.96*s.e. 

In this case, 2.0269 > 1.96*0.1633, so you would report significance.

> What does it mean if a plot of my data appears to have a trend, and auto.arima produces an ARIMA(3,1,2) model, but there is no drift? Is there no trend, or do I need to rerun the model but with pdq values of (3,0,2), ie with no differencing, to determine if there is a trend?

The way I see it: if you find that d=1 is appropriate, that is already evidence of a trend.

To quantify the trend, I would include a time index on the regression and let R fit a coefficient to it. 

That might be what ""drift"" is doing. I've never used the auto.arima function, only regular arima model-building. (I'm not a fan of automated model selection)",1519147907.0
fonzy6,"Not enough details but should be instantaneous unless coded wrong. Also, please don’t use loops to add 2 new columns to a dataset",1519129732.0
standard_error,"What are you looping over, and what are you doing in the loop? Unless that part is heavy, I would expect this to be more or less instantaneous, especially if you use the data.table package.",1519128032.0
sharkinwolvesclothin,"Multiplying a column by another column and having the result in a third column should be instant, but doesn't require recursion. Did you mean something else? If not, you're probably running an exponential number of calculations over what's required. ",1519141092.0
MrLegilimens,You really shouldn't use for and if loops unless absolutely necessary. There are many other ways to get the same result in R. ,1519135056.0
natched,"If you don't care about the order, then R pretty much does this by default if you give data.frame a list of strings, since it converts them to a factor which is really a list of integers with one integer for each unique string.

    df = data.frame(Name=c(""joe"", ""linda"", ""joe"", ""frank"", ""frank"", ""fred"", ""joe""))
    df$ID = as.integer(df$Name)
    df

The other way is to use the 'match' function which assigns integers in the order they appear in the list (like in your example):

    df = data.frame(Name=c(""joe"", ""linda"", ""joe"", ""frank"", ""frank"", ""fred"", ""joe""), stringsAsFactors=FALSE)
    df$ID = match(df$Name, unique(df$Name))
    df

",1519088884.0
efrique,"1. avoid calling an object `names`; `names` is a primitive function in R.

2. what is the purpose of calling `matrix` inside `data.frame`, rather than something like 

   data.frame(pers.name=c('joe','joe','liz', 'liz', 'liz', 'luke'), pers.ID=c(1,1,2,2,2,3))

",1519101101.0
mf_L,"Hi all! I wrote the above, linked article to demonstrate using R to web-scrape crypto prices, perform a few calculations, and then export it all into excel, using the [""rvest""](https://cran.r-project.org/web/packages/rvest/rvest.pdf) package and a few R base functions. The article itself is a bit verbose (assuming minimal prior R knowledge); if you'd just like to look at the code, I've posted it here on [GitHub](https://github.com/MattLunkes/crypto-quick-scrape). Thanks for reading! ",1519075897.0
Darwinmate,"Nice job. It's great you posted a github page with the complete script.

But I'm really wondering, why did you think it was appropriate to insert emoticons into this post?",1519082465.0
odeleongt,"Your best bet would be to look for a vignette, which would show some use cases. Looked for a vignette for `ri`, and only found one for [`ri2`](https://cran.r-project.org/web/packages/ri2/vignettes/ri2_vignette.html), a package that claims to be a successor to `ri`.",1519046712.0
kokakapo,https://tbradley1013.github.io/2018/02/01/pca-in-a-tidy-verse-framework/,1519021904.0
natched,Help file for the 'prcomp' function.,1519006499.0
timtamtoucan,Dudi.pca in the ade4 package. ,1519030943.0
infrequentaccismus,"I have learned that if I need to explain to someone else the impact IVs have on a response variable, the best tactic is usually to convert the response variable into into a binary. For example, “mediocre”, “good”, and “great” might get converted into “neutral” and “positive”.  If not, then getting variable importance from a tree model and sign from a separate univariate plots can be a good solution (though it doesn’t quantify the effect size of the various relationships).  I am watching this to see if someone else has found a great way to communicate ordinal regression results to lay persons. ",1519005582.0
Hoelk,"I just finished a new version of my package **tatoo** a few days ago. It's designed to ease the life of people who have to produce long excel ""reports"" (workbooks with lots of tables). See the [package vignette](https://cran.r-project.org/web/packages/tatoo/vignettes/tatoo.html) for more info.

The new version subdivides the resulting workbooks into named regions (column names, caption, table body, ...) and comes with an apply like syntax to apply formatting to said named regions (see [second vignette](https://cran.r-project.org/web/packages/tatoo/vignettes/named_regions.html)).

Tatoo is a built on top of the awesome [openxlsx](https://cran.r-project.org/web/packages/openxlsx/index.html), and knowledge of that package is definitely useful, though not required.",1518969026.0
suity1,"great package!

apologies if i'm missing something obvious--to what does the name refer?",1518985989.0
I_Like_Smarties_2,"> How do I calculate VaR with Monte Carlo simulation?

If this is the thrust of your question then i'll go out on a limb here and say it might be easier to implement a stochastic model/function to simulate x paths which you could then use to compute your VAR. However, I'm not totally sure exactly what your driving at, since I'm not familiar with GARCH models. 


 ",1518971869.0
jbuckets89,I need a bit of help understanding your question.  Are you looking to calculate VaR at each point in your time series and compare that with your model results at a given point ?,1518964553.0
RememberToBackupData,"Out of curiosity, what class are you doing? These are unusual questions haha. ",1518953994.0
FlyingApple31,"here is a thought to begin the first one

diag<-function(x){
y<-NULL
for( i in 1:ncol(x)){y<-c(y, x[i, i])}
return(y)}",1518948556.0
Darwinmate,"Have you done any recursion? The second can be easily solved by applying recursion but can be long winded if ""manually"" applied.

stolen from an SO post and modified to meet your request:

    longestCollatz <- function(n) {
      
      collatz <- function(n, acc=c()) {
        if(n==1) return(c(acc, 1));
        collatz(ifelse(n%%2==0, n/2, 3*n +1), c(acc, n))} 
      
      seqnum <- collatz(n)
      max <- (seqnum)
      len <- length(seqnum)
      return(list(seqnum, max, len))}


",1518954482.0
Darwinmate,"Right, the error is coming from `chartSeries` would have helped if you specified that because it can also be confused with `getSymbols` not finding `BTC-EUR`. It's having trouble dealing with the `-` in the name. Use backticks to solve this issue:

    chartSeries(`BTC-EUR`)",1519083123.0
kazi1,Use easybuild to compile it using the Intel toolchain. Don't do it yourself.,1518874719.0
iamdelf,"Intel MKL will run some stuff in parallel silently which can be good if your programs don't explicitly use multiple cores.  If you are already writing programs that will use multiple cores, it can be very painful.  See http://blog.revolutionanalytics.com/2015/10/edge-cases-in-using-the-intel-mkl-and-parallel-programming.html ",1518882551.0
bluestorm21,"Those options seem fine as you've specified them. Honestly, you'll get the most out of R in an HPC with additional packages like Rmpi. I don't think there are many build options that are HPC specific.",1518885879.0
BirthDeath,"You may have trouble installing certain compiled packages with the intel C/C++ libraries (rugarch comes to mind as one).  I was able to get around most of these by adding the cxx flag -wd308.

I experienced similar issues to the other poster with regard to MKL running silently in parallel which can be really problematic on a cluster as it seems to use all available cores by default.  I typically include something like this in the beginning of my scripts which controls the number of threads/cores used in compiled code.

    library(RhpcBLASctl)
    
    get_num_cores()
    get_num_procs()
    
    blas_get_num_procs()
    blas_set_num_threads(3)
            ",1518887696.0
thedukeofedinblargh,"I assume the others' code works to actually turn your bar blue.  You don't have to use ggplot.

To address the ""so lost"" part, ggplot is an [entirely different plotting system that uses the ggplot2 package](http://ggplot2.tidyverse.org).  It's generally considered the best all-around approach to graphing in the R ecosystem.  If you intend to continue to use R, it's worth [learning it sooner than later.](http://r4ds.had.co.nz/data-visualisation.html)",1518848138.0
efrique,"You need to show what you're doing with a reproducible example.

to get steel blue bars, try

       barplot(VADeaths[,1],col=""steelblue"")

(but using whatever data you like)",1518851143.0
chandaliergalaxy,"Modifying the first example from `?barplot`:

    tN <- table(Ni <- stats::rpois(100, lambda = 5))
    r <- barplot(tN, col = ""steelblue"")
    
You will generally have better luck with ""how do I...""? type questions over on stackoverflow.",1518843905.0
Snotaphilious,"It'd be helpful to see more of your code. Without it, I'm just guessing. Still, maybe try starting with this: 

    require(grDevices)

And then add the 

     col = 'steelblue'

within barplot().


Also, use ?barplot, and check out the examples at the bottom. Lots of them are using colors.",1518844054.0
sowenga,"What type of model is it estimating in those instances? I think ARIMA(0, 1, 0) is basically a random walk model—the differenced original time series is random white noise, for which the mean forecast would be the last value.",1518813696.0
chalaila,"Did you run any tests beforehand? Maybe ARIMA is not a model that can fit your data well.

You can start by looking into the ACF and APF of the time series.",1518817721.0
afranko22,"The thing we noticed with auto arima, if it can't find a good model it will default to arima(0,0,0) which will because similar to a flat line on the chart.  Essentially saying next month will be equal to this month because there is no model fit",1518808046.0
Taeligniafuge,"Can you share the dataset I don’t understand what you are describing and what deduplicating every variable would do for you? 

What are they duplicates of and why do they need removing?

Did you create the duplicates during the join? Deduplicate before joining?

Will help to see the data. ",1518794742.0
HMG-CoAReductase,"Without seeing how your data looks like, I'm going to guess that you probably want to focus on the merge instead of post hoc filtering.",1518881961.0
GildedFuchs,"Going to echo the sentiment that you likely have an issue before / with the join. I drunkenly hacked some code together to solve your general case. But once you move past a trivial data set I think you'll also agree that stepping back from your problem statement and thinking a different way will be more helpful. Hundreds of rows and columns - how is it useful to know that row x is different from row y at columns a, b, c? You're going to end up over complicating your solution to work with these differences in any meaningful way. 

If you're doing something non standard, I could possibly see a use for this in a few cases. I'm guessing you aren't - message me if you're doing something more exotic and I'll see if I can help. ",1518927847.0
odeleongt,"If you didn't have duplicates in `name` before the `left_join`, probably the issue comes from the join itself as others have mentioned.

Anyway, I think what you're trying to do would help you figure that out too so here it goes:


    mydata %>%
      # lets avoid filtering groupwise, can be slow if there are many groups
      filter(name %in% name[duplicate(name)])  %>%
      # now gather everything except name
      gather(key = variable, value = value, -name) %>%
      # add unique identifiers to each value
      group_by(name, variable) %>%
      mutate(
        col = paste0(""row_"", seq(1, n())
      ) %>%
      ungroup() %>%
      # put repeated rows side by side for each name and variable
      spread(key = col, value = value) %>%
      # and filter those that are different
      filter(row_1 != row_2)


You could run into issues if there are some names with more than 2 rows though.

* edit: code block formating",1519047598.0
Cronormo,"You told ggplot to use the same fill color for all groups, you gotta change the fill to be an aesthetic based on a variable. Also, you can chain the ggplot layers all in one go to make your code more compact.

In this case, the fill of the boxes should be based on teeth type, and there are 5 different types (so 5 fills, although only 2 distinct colors will be used):

    library(ggplot2)

    teeth <- read.csv(""run run.csv"", header=F)
    teeth$type <- factor(teeth$V1, levels = c(""Unicuspid in Unicuspid Mouth"", ""Bicuspid in Bicuspid Mouth"", ""Uni Uni in Mixed Mouth"", ""Bi Bi in Mixed Mouth"", ""Uni Bi in Mixed Mouth"", ""Bi Uni in Mixed Mouth""))

    fill <- c(""#1f78b4"", ""#1f78b4"", ""#33a02c"", ""#33a02c"", ""#33a02c"")
    line <- ""#1F3552""

    plot <- ggplot(data=teeth, aes(x=type, y=teeth$V2, fill = type)) +      #set fill to be based on type
            geom_boxplot(colour = line, alpha = 0.7) +
            scale_x_discrete(name=""Mouth Type"") +
            scale_y_discrete(name=""Average Number of Days\n to Replace Tooth"",breaks=seq(0,80,10),limits=c(0,90)) +
            scale_fill_manual(values = fill) +                               #Choose values to use for fill manually
            ggtitle(""Number of Days to Replace Teeth in Different Mouths"")  +
            theme_bw()


Basically what I did was tell ggplot2 that each group in the variable type should have a different fill. Then I told ggplot to use a manual scale for the fills, using the colors in the `fill` vector. Btw, ggplot2 chooses fills in the same order as the factor levels by default, but you can use named vectors to specify them in a different order.

Also: if you google ""ggplot2 fill boxplot"" the first link is http://www.sthda.com/english/wiki/ggplot2-box-plot-quick-start-guide-r-software-and-data-visualization, which has a topic called ""Change box plot colors by groups"". I'm not saying this to be rude but google has gotten me 99.9% of everything I needed so far in terms of R, you just gotta use the right keywords

Edit: I had messed it up at first, should be good now",1518791072.0
klo99,try facet_grid() http://ggplot2.tidyverse.org/reference/facet_grid.html,1518791028.0
sempervent,"Here is a link to shiny application that you can use with this service: https://sempervent.shinyapps.io/PlexMovieRatings/.

Here is the github for the data and connection: https://github.com/sempervent/PlexMovieRatings

This is my first shiny application for pleasure, so I welcome any feedback.",1518762516.0
Darwinmate,"You should read the documentation in the `ROlogit` package for more information on how to use the functions

https://cran.r-project.org/web/packages/ROlogit/ROlogit.pdf

for ROlogit fuction:

    yvar string. Name of outcome variable.
    evar string (or vector of strings). Name of exposure(s).
    cfdr string (or vector of strings). Names of confounder(s). Default is NULL.
    emod string (or vector of strings). Name of effect modifier(s). Default is NULL.
    svar string. Name of stratum variable. Use NULL to fit model without stratification.
    dat data.frame. Contains all the variables needed for the analysis.

",1518756462.0
MrLegilimens,Why ordered logit? That’s typically enough variation to just claim continuous and go with a linear regression,1518780383.0
zdk,spls for sparse PLS and caret for PLS discriminant analysis.,1518749121.0
detner,"PLS regression is not the same as PLS-SEM.
As you mention SmartPLS, I assume you’re interested in structural equation modeling - SEM. There are two approaches to this: CB-SEM and PLS-SEM. Most common R packages for both types are:
- plspm
- semPLS
- lavaan
- sem

I’m sure there’s more, but out of my head those are popping out. It’s also possible to define the structural model and draw it using pathdiagram package.",1518973664.0
Darwinmate,"This is possible, but your code is going to get ugly very quickly. Reconsider how you're selecting the different rows from the csv file. There is probably a better cleaner, efficient and readable way, for example using `filter` in the `dplyr` package to filter specific rows. Another possible solution is using `map` from the `purrr` package. But it sounds like you are filtering, so use `dplyr`. I used this method in a for loop, saving the output of a function into a separate df and performing more computation on these dfs. With my current knowledge, I would use the `purrr` package for this. 10 lines of code condensed into 2 lines using `purrr`

If you are adamant then this is how, use the `assign` function like so:

    assign(name, data.frame(mydata[1:12,])

My spiel is probably excessive but I don't want you going down the ugly path I took. From your example, it sounds like you're slicing your dataset, not really filtering or subsetting. If this is the case, checkout `slice` again from the `dplyr` package. ",1518743973.0
mattindustries,"You might be better served with lists of dataframes. 

    > df.list <- list()
    > for(group in paste0(""group"",1:10)) {
    + 	df.list[[group]] <- data.frame(replicate(5,sample(0:1,10,rep=TRUE)))
    + }
    > names(df.list)
     [1] ""group1""  ""group2""  ""group3""  ""group4""  ""group5""  ""group6""  ""group7""  ""group8""  ""group9"" 
    [10] ""group10""
    > df.list$group1
       X1 X2 X3 X4 X5
    1   0  1  1  0  1
    2   1  1  1  0  1
    3   1  0  0  0  1
    4   0  0  1  1  0
    5   0  0  1  0  1
    6   1  1  0  0  1
    7   1  0  0  0  1
    8   0  0  0  1  1
    9   0  1  1  0  1
    10  0  1  0  1  1",1518748665.0
dysregulation,"I am on mobile, but the ""assign"" function would be one solution as far as I remember. https://stat.ethz.ch/R-manual/R-devel/library/base/html/assign.html",1518743435.0
efrique,"both the second last line and the last line assign the value of the RHS expression to a variable called ""name"". You seem to think ""name = "" does two different things in adjacent lines. It doesn't.

Check out `?assign`",1518791993.0
Kickuchiyo,"This is just a shot in the dark, but some survival models may adjust confidence intervals for study's where subjects ""exit"" after a certain amount of time. If these subjects exit the study before failure, then the estimates would be low-biased. Could be that you're using a package that's adjusting the intervals somehow, possibly using a skewed distribution. I could definitely be wrong about this though",1518740983.0
motts,"You're pitching a rather complex idea and it sounds like you have limited experience in R. Here's some stuff that should get you started.

**mapping**

There are a couple packages out there to visualize spatial data. The one you will want to use will depend on the format of your data and your visualization goals. Here's some starting points
https://www.computerworld.com/article/3175623/data-analytics/mapping-in-r-just-got-a-whole-lot-easier.html

http://spatial.ly/r/

**animating**

You can export any plot you make to a pdf or image format (png, jpg) and make a gif outside of R. Within R there are a couple different packages, I think the best is [animation](https://cran.r-project.org/web/packages/animation/index.html)

",1518730071.0
goodgameplebs,You may want to consider a Bayesian model with that expectation reflected in your prior.,1518661757.0
basejester,"Why do you think it's going up (e.g., in more stores so expecting more sales)?  Include that factor in the model.  ",1518663490.0
zdk,you can add data to an existing plot using ?points.,1518648974.0
Darwinmate,"I can help but I'm lost as to what you want your x and y axis to be for the plot. 

As the number of sites increases so does the richness and this makes perfect sense. So for the individual lines which are for each habitat, what would the x and y axis be? you can't have them as x = habitat and y = richness group by habitat because that doesn't make sense. It's the NUMBER of habitats analysed not the habitat site number. So in your all plot, it should be Number of Sites as your X axis.",1518651802.0
shaqerd,We aren't doing your homework assignment. Damn bro,1518660976.0
Darwinmate,Sorry down voting because it's a bunch of questions with very little background information or what you're working with or what you want to achieve. ,1518658132.0
Darwinmate,"I can't give you specific reasons, but my intuition when I've run into something similar is that the packages are computing differently because of either a default variable that's been specified for you, or because the underlying computation is different. I'm especially suspicious of a package describing itself as ""easy"". It's making assumptions for you, and if this is the case you need to find out how SPSS and the packages are computing the EMMs. 

Something to note about the `ezANOVA` function is this warning:

>Prior to running (though after obtaining running ANCOVA regressions as described in the details
section), dv is collapsed to a mean for each cell defined by the combination of wid and any variables
supplied to within and/or between and/or diff. Users are warned that while convenient when used
properly, this automatic collapsing can lead to inconsistencies if the pre-collapsed data are unbalanced
(with respect to cells in the full design) and only the partial design is supplied to ezANOVA.
When this is the case, use within_full to specify the full design to ensure proper automatic collapsing.

A similar warning is given for `ezStats`. You specify `within_full` var for `ezANOVA` but not for `ezStats`. I wonder if this is the difference you are seeing?",1518744952.0
wolfesmc11,Damn well done,1518659973.0
Darwinmate,"Have you looked for any packages that help with cyclic populations?
For example:
https://cran.r-project.org/web/packages/popdemo/index.html

and the paper

http://onlinelibrary.wiley.com/store/10.1111/j.2041-210X.2012.00222.x/asset/j.2041-210X.2012.00222.x.pdf?v=1&t=jdmxn6tm&s=c89efb1e69b1c75ca2137bb702d3d8f7f8ad9958",1518604325.0
Surge_attack,"The answer to 1,2, and part of 3 is already in your question: summary

Othe part of 3: I'm sure you mean confint, not cinfint, this one is very straightforward if you use the correct function call

4: The function to retrieve the residuals of an lm object is resid. All you have to do is plot them after this.

5: The predict function can do this part",1518559084.0
Surge_attack,"I would personally say that when I encounter a new package/function I go straight to the manual. I check out the arguments and what they return, and how the functions interact. So, yes, I would say it is entirely possible to learn everything from the manuals. But for the basics I would use the various tutorials, videos available online, at least until you are comfortable with the workings of base R.

I'm not sure I understand your second question. R has many packages that deal with dataviz, one of the more prominent is ggplot and it's various offshoots. R was designed with statistics in mind so I would say R does an amazing job ""creating graphics and pulling data"".

The difference between SQL and R is not subtle at all. SQL is a query language designed to create databases and edit and retrieve the stored data in these databases. R is multi-paradigm programming language with a focus on statitical applications. Since R is mainly about data it is quite easy to interface SQL in R (and a lot of othet databases for that matter). I personally like RSQLite, but other packages exist. I would say that knowing both is important in data anlaysis and rather then being directly transferable they are better described as harmonious. ",1518560604.0
thedukeofedinblargh,"Personally, I recommend [skipping the ""official"" manual](http://varianceexplained.org/r/teach-tidyverse/) and reading this instead: http://r4ds.had.co.nz .  

As for SQL, the logic of data manipulation (rectangles of numbers, joins, filters, etc.) is similar regardless of language (or even standalone programs like SPSS).  Learning to manipulate data in one language will help in nearly any language.  That said, SQL is just really helpful to know, so it [wouldn't hurt to learn it at some point](https://community.modeanalytics.com/sql/tutorial/introduction-to-sql/), too.",1518762473.0
huxleyan,"Just wanted to tell you /u/deanat78 that your tutorial on getting RStudio Server and Shiny on Digital Ocean worked perfectly. 

[Here's a link for those interested](https://deanattali.com/2015/05/09/setup-rstudio-shiny-server-digital-ocean/)

And this looks like a terrific addition to Shiny. Keep it up!",1518553259.0
Loco_Mosquito,"Hey man, off-topic but thanks for [this] (https://github.com/daattali/advanced-shiny/tree/master/multiple-pages), I found it really useful!",1518556094.0
kenderpl,sweetAlert was already available in shinyWidgets.,1518553685.0
hyzerline,I can't wait to try this.,1518636134.0
northernwildling,check out the broom package and knitr,1518544141.0
Statman12,"Would something like this work for your purposes?

    data(trees)
    
    sum_stats <- data.frame( 
     Mean = apply( trees, 2, FUN=""mean"" ) , 
      SD  = apply( trees, 2, FUN=""sd"" )
    )
    
    sum_stats
",1518543629.0
,as.dataframe() ? ,1518540853.0
mfogs1,"I've had a lot of luck using the stargazer package for this. Take a look: 

https://www.jakeruss.com/cheatsheets/stargazer/

I think it defaults to outputting Latex code, but it's great for making pretty tables of summary stats or regression output.",1518543821.0
DrHampants,"I'm a bit of an r noob, but it seems to me like broom::tidy() is the function you're looking for. Someone can correct me if I'm wrong, but I think this would work (using R Studio):

    install.packages(""broom"")
    library(broom)
    data(trees)
    s <- tidy(trees)
    View(s)

It gives a lot of output, but you can trim it to the columns you want.
",1518546116.0
grasshoppermouse,https://github.com/ropenscilabs/skimr,1518580508.0
Darwinmate,"This looks like a markov chain graph. Checkout the packages `markovchain` and `ggm` package.

The `plotGraph` function is kinda cool as it lets you manipulate the graphs. ",1518499111.0
Darwinmate,"Are you loading the library? You need to do `library(""maps"")` to load the package into the current environment. 

Wait a sec, are you sure `map_data` is from `maps` package? Because it's not in there. It's in the `ggplot2` package though.

The warning you are getting is normal because you haven't setup a dir for it to install into, so it uses the default. Use `.libPaths` if you want to specify a specific dir. But I wouldkeep it as is for now.

P.S Stop beating yourself up. Mistakes happen, everyone is constantly learning. I've made this mistake before. I've made idiot mistakes before, multiple times. ",1518499512.0
Surge_attack,"My first remark is to point out that the red line seems to be the fit as suggested by your legend, thus you might not want to remove it.

After reviewing the reference manual it seems that denscomp takes a list argument thus excluding fit1 from the argument should delete the plot. It also seems denscomp is used for GOF, which looking at your plot seems to suggest a poor GOF. Maybe explore other fits.

I am not familar with this package at all and have not used it, only looked at the manual. I hope I was able to help a little bit though.",1518497303.0
LiesLies,"Try reducing the bin size on your histogram or binning your theoretical distribution similarly to the histogram.

This has happened to me before and the issue was comparing density functions with binned discrete observations.",1518981766.0
dtrillaa,"Few different things I think it could be, but I haven’t checked it myself and could be way off the mark. 1st check the distribution of the y outcomes. If 90% or so are class 2, then the model may just train to classify everything as 2. You can resample your training and test sets to have a higher percentage of 0s and 1s and it’ll train better.

If the distribution is fine, play around with your learning rate and your number of hidden layers/ nodes. I worked with the “red wine quality” data set and if I remember correctly the best model I found was one hidden layer/ 3 hidden nodes. However when I did it, I just used the quality rankings as is. Finding the right learning rate is important too because it uses gradient decent, if the learning rate is too large or too small, it’s possible to find the incorrect local minimum/ maximum.

Finally, if num.round is how many iterations you use to optimize your model, you should try and increase that as well. Try increasing it to at least 100 (very possibly more) and see if your model improves that way. I typically start with 500 and work from there.

I hope this helps in someway, if it doesn’t, I can try running it and seeing if I can pinpoint the issue.

But unfortunately, optimizing neural nets can be a lot of brute force optimizing layers/ nodes, learning rate, iterations, etc.",1518516995.0
coffeecoffeecoffeee,"This was an awesome talk.  It's rare that I see a deep learning talk that's not sensationalized, but JJ did a killer job.",1518471324.0
scottmmjackson,"Full day 2 stream is available here:

https://youtu.be/Ol1FjFR2IMU

For the folks who clicked on comments because they saw ML:

2:01:54-ish

Michael Quinn and Sina Chavoshi from Google talk on TF/CloudML/BigQuery.

2:19:24

Javier Luraschi from RStudio talks about TFDeploy, deploying TF models to RStudio Connect, and KerasJS

2:40:53-ish

Kevin Kuo from RStudio talks about SparkML machine learning pipelines in Sparklyr.

2:59:14

Ali Zaidi talks about reinforcement learning and Minecraft 

Day 1: https://youtu.be/ogy7rHWlsQ8

",1518497397.0
MarcelChafouin,"I'm currently into *Deep learning with R*, by JJ Allaire and François Chollet.

The book is excellent, I know nothing in Deep learning and even Mahine learning in general, and it provides both an introduction for newbies and an in-depth presentation of many concepts. I'm going to read it from start to finish. Highly recommended.",1518530773.0
iconoclaus,great talk — very approachable and honest.  i particularly like that he de-hypes things and yet gives a very good overview of what is available.  good times ahead for the R community.,1518502237.0
11111000000B,"How about this package ;)? https://cran.r-project.org/web/packages/readability/index.html
",1518466711.0
The_Old_Wise_One,"You probably have hidden .Rhistory and .Rproject files in your directories. If so, R automatically reads these in when you open it. I would look into finding and deleting those.",1518447408.0
Elesday,"Uninstalling R only delete the R interpreter, it won’t delete any of your scripts.
I’m the same way that uninstalling Microsoft Office doesn’t delete any document file associated with it.

And the “only for it to open with...” makes me wonder if you’re not talking about Rstudio instead of R ?",1518440650.0
Darwinmate,"You've got a typo, it's `nrow` not `nrowS`.

The arguments you are passing to the function don't make sense. You're passing a character to the function `order` that want's a variable. 

    MultiVarBins(School,""Performance"",""Ethnicity"",""Gender"",""Attendence"")

I think I know what you're trying to do. Your code should be this:

    dat <- read.csv(""FILEPATH"")
    MultiVarBins <- function(data_file, bin_no, col_name1, col_name2, col_name3, col_name4) { 
      df_sorted <- data_file[order(data_file$col_name1, data_file$col_name2, data_file$col_name3, data_file$col_name4) , ]
      total_rows_in_file <- nrow(df_sorted) 
      df_sorted$Bin <- rep(1:bin_no, len = total_rows_in_file) 
      return(df_sorted) }

But now I get your error! It's probably how you're calling the variables from the dataframe. Looks like the easiest solution for this error is to replace `$` with `[[]]`:

    df_sorted <- data_file[order(data_file[[col_name1]], data_file[[col_name2]], data_file[[col_name3]], data_file[[col_name4]]) , ]
    
and now it works.",1518423777.0
BetterGhost,"I’m on mobile so can’t see your Excel file. Take a look at your Ethnicity column. R could be treating that as a list or factors. You probably want to convert it to a character vector. If I’m right, this should take care of it for you:

    School$Ethnicity <- as.character(School$Ethnicity)",1518406471.0
dtrillaa,"Look into the purrr package through the tidy verse. If you have a function that works on one column, you can simply use


    map_df(df, your_function, ...(other arguments))

And it will apply the function to each column. The map functions are similar to the lapply family, but they can work on more than just lists and vectors",1518462106.0
Darwinmate,"WHAT issues are you running into? How can you help you if you don't help us. 

WHat does your data look like?",1518416615.0
mattindustries,[Does this one help you more?](http://databiomics.com/how-to-create-a-fast-and-easy-heatmap-with-ggplot2/),1518409761.0
singularperturbation,"Could do

    length(unique(your_vec)) == 1

for testing if all the values are the same.",1518398624.0
Darwinmate,"A few things:

1. It wouldbe great if you could post some data we can play with.
2. Format your code correctly please (see ""formatting help"" below the reply box).
3. Don't try to recreate the wheel! Someone else has probably written a package/function for this. In this case checkout the `zoo` package or the `TTR` package for calculating a rolling mean.
4. Checkout the `dplyr` package, specifically the `group_by` and `mutate` functions. These will help you with grouping the exposures and then applying whatever function onto them. Also in the same function is the awesome `filter`function that will allow you to select specific values.

If you provide some example data (say 10 rows + 2 vars) and what you're expected output then I can have a bash at it. Currently, I'm having trouble understanding exactly what you're after.",1518392703.0
reddit_tothe_rescue,"It sounds like you have two problems you’re trying to solve (operating across many spreadsheets and doing a rolling average thing). I would recommend you solve the first problem first, by bringing all those spreadsheets together in a data.frame or data.table. If you need to keep track of what data came from which spreadsheet, just makes a new variable to label observations. 

Then, the rolling average can probably be done with zoo, TTR or data.table as the other commenter suggests. You could also just write a loop. It would be slow, but might have the flexibility you need here. ",1518396224.0
abecker93,"I'm unsure of what you mean by rolling average of three points, but you can use the index to indicate what you want using a for loop and an if/else statement and then use the else to calculate your rolling mean when the group ends and start the next group. It can get really complicated especially when you're trying to track several changes in things, but I find for/if/then/while logic to be really legible (albeit slow).

something like this:

    #initialize loop variables
    location <- index[1] 
    roll <- data1$varA[index[1]]
    roll_mean <- numeric(0)
    
    for(i in 2:length(index)){
        #check to see if the current indexed item is in the same group as the previous one
        if(location+1 == index[i]){
            roll <- c(roll, data1$varA[index[i]])
        }
        #when it isn't, create the rolling mean and reset 'roll'
        else{
            #conduct your rolling mean here
            roll_mean <- c(roll_mean, SOMEFUNCTION(roll)
            roll <- data1$varA[index[i]]
        }
        #log the location of the current point being tracked
        location <- index[i]
    
    }

As you could probably guess, SOMEFUNCTION needs to be replaced with something that conducts a rolling mean in the fashion you want.",1518396448.0
Im_int,"Try adapting the following code: 

    #install(tidyverse, zoo)
    library(tibble, dplyr, zoo)

    # create sample data with one argument and three variables
    data2 <- tibble(x=1:20, a=16:35, b=6:25, c=11:30) 

    # make rolling mean columns for each variable
    data2 <- 
      data2 %>% 
      select (a,b,c) %>% # select columns to work with
      mutate_all(funs(rollav=rollmean(., 3, na.pad=TRUE, align=""right""))) # create rolling mean columns

    # get max for each rolling mean column
    data2 %>% 
      select(a_rollav, b_rollav, c_rollav) %>% # select columns to work with
      filter_all(all_vars(!is.na(.))) %>% # filter out NA data (otherwise the next step will fail)
      summarize_all(max) # get max for each column

Instead of the sample data, supply your actual data as follows: `data2 <- data1[index, ] %>% as.tibble`",1518397955.0
unnamedn00b,"The [missforest](https://cran.r-project.org/web/packages/missForest/missForest.pdf) package might also be of interest. Here is the [paper](https://academic.oup.com/bioinformatics/article/28/1/112/219101) that the package is based on.

Edit: You can find an example on page 5 of the package guide that I linked to.",1518376436.0
spraynard,Check out the mice package for imputation. ,1518371483.0
spinur1848,"It usually tries to allocate RAM for the resulting object first before doing anything and gives you an error if it cant find enough RAM to allocate.

How do you know you're actually maxed out? The system measures may not be entirely accurate because R always does copy on write and then reclaims overwritten memory with a garbage collection when needed.

It won't execute a command unless it has enough RAM for the calculation. So if you aren't getting an error in R then it found what it needed (this may have come from swap space).

For these kinds of tasks that involve selective counting and simple grouped aggregations, have a look at dplyr and databases. Your available RAM won't be a limiting factor anymore if you do the heavy lifting in the database.

",1518372006.0
suity1,"a `tidy` approach:

    library(tidyverse)
    
    chart %>% 
      group_by(Batch) %>% 
      sample_n(20) 
    ",1518368041.0
berf,"Why a data frame?   A list would be much better.  Then you could use lapply and friends.

    ldata <- split(data, batch)
    sdata <- lapply(ldata, sample, size = 25)

This assumes you want *sampling without replacement* so each component of sdata is still a random sample from the population.  Just of smaller size.

AFAIK there is no way to use your R object chart to do this easily.  Data frames are for fitting models.  They get in the way as soon as things become complicated.

Edit:  And you keep going using lapply and friends.

    smean <- sapply(sdata, mean)
    ssd <- sapply(sdata, sd)

and so forth.",1518363995.0
,"Here is my approach using base R's sample function. Sample the index of the rows using the sample() function and select only the rows in the index.
If you don't want replacement, turn the toggle to FALSE. 

    data <- rnorm(n = 1000, mean = 50, sd = 1)
    batch <- rep(x = 1:10)
    chart <- data.frame(Data = data, Batch = batch)
    rand_ind <- sample(1:nrow(chart),size=25, replace = TRUE)
    rand_chart <- chart[rand_ind,]

Disregard. See below.",1518376297.0
djkaffe123,"There is properly a better way, but here:

    SampledData<-NULL
    for (i in 1:length(batch)){
      SampledData<-c(SampledData,sample(subset(chart$Data,chart$Batch==i),25))
    }",1518363759.0
longshortdogsFTW,Are you using RMarkdown to generate the reports? You could use tabbed sections?,1518368082.0
efrique,"Are you assuming a model like E(y|x) = a + b x (i.e. where there can be measurement error in the observed y but not in the observed x) or are you assuming a model like  E(x|y) = a + b y ,  ... or something else? 
",1518350107.0
Juko007,"Would this work for your purpose?

    test<- data.frame(A=c(1:7),
                      B=as.numeric(c(""0.5"",""0.7"", ""1"", NA,""5"",""5.5"",""7"")))
    
    #plotting data
    plot(test$A,test$B)
    
    # linear regression
    Reg <- lm(B~A,test)
    
    # plotting linear regression
    abline(Reg)
    
    # plotting intersection of y value with regression line
    y_val <-  0.95 # insert desired value here
    abline(h = y_val ,col = ""red"")
    
    # finding the x-value that results in the specified predicted y value
    x_val <- solve(Reg$coefficients[2],(y_val-Reg$coefficients[1]))
    
    # plotting the x-value
    abline(v = x_val, col=""blue"")    


[Output](https://ibb.co/dEgOWS)",1518358229.0
Darwinmate,"Please format your code correctly. Click ""formatting help"" below the reply box to find out how. The comments are being converted to headings because of markdown. Would be great if you could also provide some data that's in `Fleiss93cont`.

    library(meta) 
    library(tidyverse)
    
    data(Fleiss93cont)
    
    #Add some (fictitious) grouping variables:
    Fleiss93cont$age <- c(55, 65, 55, 65, 55) 
    Fleiss93cont$region <- c(""Europe"", ""Europe"", ""Asia"", ""Asia"", ""Europe"")
    Fleiss93cont$rob <- c(""Low-risk"", ""High-risk"", ""Low-risk"", ""High-risk"", ""Unclear-risk"")
    
    attach(Fleiss93cont)
    
    #Main meta-analysis
    meta1 <- metacont(n.e, mean.e, sd.e, n.c, mean.c, sd.c, data = Fleiss93cont, sm = ""MD"")
    
    #Function to extract the sub group data
    createSubData <- function(subG) { 
    subData <- update(meta1, byvar = subG) %>% 
    with(., data.frame(sub = bylab, name = bylevs, TE = TE.random.w, seTE = seTE.random.w, k = k.w, het = I2.w*100)) 
    return(subData) 
    }
    
    #testing the function
    subList <- c(""region"", ""rob"", ""age"")
    
    subDataList <- sapply(subList, createSubData)
    
    mainsubGroupData <- do.call(rbind, subDataList)

",1518330810.0
,[removed],1518331896.0
shaqerd,What have you tried already? ,1518320029.0
dankwormhole,"Thanks for providing the example data.  That really helps.

Why not join the two data.frames together. Your key is the Sample ID field.  Do you know how to join 2 tables together?",1518327771.0
webbed_feets,"The Stata manuals are amazing. It's the best documented software I've ever seen. I would really, really recommend using those.",1518748506.0
savage_geek,These Stata [cheatsheets](https://geocenter.github.io/StataTraining/portfolio/01_resource/) from Github might help. Good luck!,1518283993.0
Darwinmate,But why?,1518324090.0
suity1,"this should get you started, only a little hacky

    library(tidyverse)
    library(rvest)
    library(magrittr)

    recruiting_rankings <- read_html(""https://247sports.com/Season/2018-Football/CompositeTeamRankings?Conference=ACC"")

       recruiting_rankings %>%
          html_nodes("".list-data"") %>%
          html_text() %>% 
          str_trim %>% 
          str_split(""   "") %>% 
          unlist %>% 
          extract(-1) %>% 
          matrix(ncol = 6, byrow = T) %>% 
          as.data.frame %>% 
          modify(
            ~str_trim(.)
          )
    
returns

                   V1        V2   V3    V4    V5         V6
    1         Clemson Total: 17 5: 5  4: 7  3: 5 Avg: 93.45
    2           Miami Total: 23 5: 1 4: 14  3: 8 Avg: 90.99
    3   Florida State Total: 21 5: 0 4: 13  3: 8 Avg: 90.91
    4  North Carolina Total: 21 5: 0  4: 6 3: 15 Avg: 87.66
    5   Virginia Tech Total: 26 5: 0  4: 7 3: 19 Avg: 87.46
    6      N.C. State Total: 24 5: 0  4: 6 3: 17 Avg: 86.58
    7      Louisville Total: 24 5: 0  4: 4 3: 20 Avg: 87.08
    8      Pittsburgh Total: 21 5: 0  4: 1 3: 20 Avg: 85.84
    9        Syracuse Total: 19 5: 0  4: 1 3: 18 Avg: 85.58
    10   Georgia Tech Total: 21 5: 0  4: 2 3: 19 Avg: 84.76
    11    Wake Forest Total: 22 5: 0  4: 0 3: 22 Avg: 84.41
    12           Duke Total: 16 5: 0  4: 1 3: 15 Avg: 85.74
    13       Virginia Total: 21 5: 0  4: 0 3: 21 Avg: 84.39
    14 Boston College Total: 22 5: 0  4: 1 3: 19 Avg: 83.65",1518219317.0
samclifford,What are you using to scrape? And does it only need to be a one off as the data are static in one table or is this something you want to be able to rerun as the data updates? ,1518215209.0
ichikaren,"after reading your SO question, I came with with an idea of composing it, not making a single function to do it. 

    library(tidyverse)
    library(magrittr)

Here you specify the function you wanna do,  which is replace NA with 0. 

    na_to_0 <- function(x) {
      if_else(is.na(x), 0, x)
    }

but since you only wanna do that function to the pop that you choose and variable that you wanna iterate, specify them too.

    pop_names_you_choose <- c(""Spades"", ""Clubs"")
    
    variables_you_wanna_iterate <- c(""V1"", ""V2"")


slice your data up, filtered them according to your pop. 
for the slices that doesn't meet the criteria, just save them. 

    df_notSpades_notClubs <- df %>% 
      filter(!is_in(pop, pop_names_you_choose))


and for those that meet, perform the function at the specified variables.

    df_Spades_Clubs_filled <- df %>% 
      filter(is_in(pop, pop_names_you_choose)) %>% 
      mutate_at(.vars = variables_you_wanna_iterate, .funs = na_to_0)


finally, union them back.

    union(df_notSpades_notClubs, df_Spades_Clubs_filled)


EDIT : edit the formatting.",1518482459.0
Darwinmate,Are you scraping the same link in the script ? Because I am getting a 404 error. So the script is running but it's returning nothing thus giving you that warning. ,1518142215.0
samclifford,"Use gather on the data frame and facet wrap by the new variable so you have a bar plot for calories in and out and another bar plot underneath it for weight. Trying to jam these on the same sclet of axes is surely going to be a mess.

Hadley Wickham has spoken out against two y axis plots, so it's no surprise this is difficult. ",1518126691.0
Darwinmate,"What sort of random distribution are you after?

The code for `circuleUnif()` is fairly simple. Use `View` function to inspect the code. It has this line:

    th <- stats::runif(n, 0, 2 * pi)

`runif` is creating a uniform distribution from 0 to 2* pi. If you find a function that generates the type of distribution you want, replace `runif` with it. For example:
circleR <- function (n, r = 1) 
        {
          if (!is.numeric(n) || length(n) != 1 || n < 0) {
            stop(""n should be a nonnegative integer"")
          }
          if (!is.numeric(r) || length(r) != 1 || r < 0) {
            stop(""r should be a nonnegative number"")
          }
          th <- sample(size = n, (0.1:2*pi), replace = TRUE)
          x1 <- r * cos(th)
          x2 <- r * sin(th)
          X <- cbind(x1, x2)
          return(X)
        }

The usage of `sample` could be completely wrong for your task at hand. `sample` does have the option of including probability weights via the option `prob = ` so you may want to look into it.

Rereading your post, maybe you want a randomly sampled radius? If so, then change the code for the value of `r` in `circleUnif` to allow for a vector of randomly created radiuses, then use randomly use them in the calculation. Another another option, is to create a random distribution of radiuses and use them in a loop inside `circleUnif`.

Hope that helps somehow",1518144232.0
efrique,"1. Why are you writing the length of part of NY over the top of NY? If you run that you'll lose the information that was in NY (and change it to a vector with one element). Running it twice will give you exactly that error.

2. Re-read in your data and do `str(NY)` before you execute that line. (And then try it again after. Is that really what you want?)
",1518126671.0
freedomakkupati,"You can use object[[""element""]] instead of object$element. ",1518114321.0
syrphus,">**Use stable instead of absolute paths.** For instance, `here::here(""data"",""raw-data.csv"")` loads the raw-data.csv-file from the data folder in your project directory. If you are not using the here package yet, you are honestly missing out! Alternatively you can use `fs::path_home()`. `normalizePath()` will make paths work on both windows and mac. Use `basename` instead of `strsplit` to get name of file from a path.

What's the advantage of using `here()` over something like `file.path()` relative a project directory?
",1518101131.0
efrique,">  My response variable is plant cover (percent)

This cannot actually be Gaussian, but it's possible that for some such situations a Gaussian might be a reasonable approximation. You may have an issue with nonconstant variance even then.

A common choice for continuous proportion data like this would be a beta model (which is not strictly a GLM). Search on *beta regression*. This requires no exact 0 or 1 values (but if that's in your data there's always 0-1 inflated beta regression).

You may be able to approximate your data well enough using a quasi-binomial GLM (particularly if you're not getting values close to 0 and 1).
",1518080900.0
grasshoppermouse,"Often, it is good to model proportions (percents) using a glm and the binomial family. Make sure your outcome variable is coded ranging from 0 to 1, not 0 to 100:

https://stats.idre.ucla.edu/stata/faq/how-does-one-do-regression-when-the-dependent-variable-is-a-proportion/

Also, in R you will need supply the weights argument, which is the number of cases that each proportion is based on.",1518056919.0
sohaibhasan1,"Use gsub with a regex to strip out any characters you don't want. For instance, the following will remove anything non-alphanumeric:

cleanString = gsub(""[^[:alnum:] ]"", """", dirtyString)

Loop this over all your problematic strings, whether they are lists or character vectors or columns in a data frame.",1518067677.0
ReimannOne,"Would something like this work?

 `xx_transformed <- transformation_function(xx[xx!=0])`
",1518059395.0
agclx,"There are helper function that turn normal functions into vectorized ones.  I purposely wrote the function with regular if:

    ff <- function(x) {
       if (x==0) return(x)
       else return (x^2+1)}

The vectorised variant is obtained via `ff_vec <- Vectorize(ff)` (you can also use other functions from the `apply` family if you prefer those).

Only issue is when you call `ff_vec` it will output just a plain list.  But it is easy to fix. Call it like this `xx[] <- ff_vec(xx)`

I suppose it will be a bit faster if you can express the function itself in vectorised functions as the other answers give examples. However there are cases where this just isn't possible, and it takes usually a serious rewrite of your function for which one sometimes just doesn't have time.",1518086325.0
murgs,"As the others pointed out, you might be able to use subsetting. In general vectorization with conditional logic: use the ifelse function",1518083037.0
samclifford,It's a binomial. Response is number of marks out of 20. Logistic link. ,1518065407.0
samclifford,Make a data frame with all your rules in it and do a join? ,1518042887.0
Darwinmate,"Have you tried to use `mutate_if` or `mutate_all`?

Also would be great to provide fake but decipherable data and you're expected output. ",1518046831.0
IkeEis,Rule one for R fact data....every row is a single observation or data point. Your life will be immensely easier if you remember that cardinal rule for initial fact data in R.,1518038657.0
efrique,"You set up a data frame like this:

    ""S"" 0   5    
    ""M"" 0   9 
    ""L"" 0  13
    ""S"" 1   30  
    ""M"" 1   22  
    ""L"" 1  10
    ""S"" 0   17  
    ""M"" 0  33  
    ""L"" 0  20
    ""S"" 1  18 
    ""M"" 1  31
    ""L"" 1   15

You name the columns as whatever they should be called

You make the first two columns factors (by default the first would be already)

without names for the response or the 0/1 factor it's hard to explain what to do ... why wouldn't you say?

you fit a main effects model with something like

     lm(response ~ binaryfactor + size, data=mydata)

you fit a full two-way anova with interaction using

 
    lm(response ~ binaryfactor*size, data=mydata)

and then call summary on either.

also see the `aov` and `anova` functions",1518041273.0
CabSauce,What have you tried?,1518038609.0
grandzooby,"I'm assuming this properly generates your data table (commas are decimal points?):

    mydata <- matrix(c(5.17,9.33,13.22,30.18,22.31,10.15),ncol=3,byrow=TRUE)
    colnames(mydata) <- c(""S"",""M"",""L"")
    rownames(mydata) <- c(""0"",""1"")
    mydata <- as.table(mydata)
    mydata

          S     M     L
    0  5.17  9.33 13.22
    1 30.18 22.31 10.15    

In that case, you can convert that data into a dataframe using:
    
    library(reshape)
    melt(mydata)

       X1 X2 value
    1  0  S  5.17
    2  1  S 30.18
    3  0  M  9.33
    4  1  M 22.31
    5  0  L 13.22
    6  1  L 10.15

This will get each observation on its own row, making it much easier to do the analysis you may want to do.",1518142337.0
SeveralBritishPeople,You could always just exponentiate the parameters inside your metropfun. Then MH can take steps anywhere on the real line but you always have a positive parameter inside. That’s how Stan and similar packages deal with hard boundaries. Just remember to also exponentiate the draws for those parameters when analyzing the output. ,1518075730.0
shaggorama,"rpart is building a decision tree, which is basically a way to automatically divide your feature space into lots of boxes and returning predictions based on which box your data falls in. Logistic regression constructs a single separating hyperplane, i.e. divides the space into just two boxes. 

Decision trees are much more flexible and therefore have higher potential to overfit (high variance, low bias). This is often addressed by building ensembles of high bias ""weak learner"" trees, i.e. random forests.",1518033746.0
shujaa-g,"Logistic regression is a GLM, which is an extension of linear models. rpart uses classification trees, which are nothing like linear models. Maybe you should read one of the `rpart` vignettes, or look for other introductory material. [Even just the first few pages of their Intro Vignette](https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf) should help.",1518028361.0
Digging_For_Ostrich,"As far as I understand it, they are completely different models. Using logistic regression is one model, recursive partitioning is another, and so results will differ on predictive models.",1518026257.0
blahblahblahblah8,"They are different algorithms. Different algorithms will give you different results. 

You absolutely should not use accuracy to measure your model performance. Models with lower accuracy can have higher predictive power. This is because accuracy is not admissible bc it maximizes a degenerate loss function. 
",1518045406.0
roadrussian,"Did you lock the seed? Otherwise split is random, which with smaller data sets can alter outcome a little. Also, as the other dude said, look at the algorithm, and how the weights are distributed. ",1518027175.0
RememberToBackupData,So what's the problem?,1517980302.0
infrequentaccismus,To be honest Rstudio is pretty freaking awesome. I’m not sure I’ve ever met anyone who doesn’t prefer it apart from occasionally running r or calling an r script form the command line. What is your elevator pitch for rice?,1517979201.0
fencelizard,Just started running some R over ssh so this looks really useful. ,1517979535.0
keepitsalty,"I fire up rice all the time when I don't want to deal with the large and cumbersome screens of Rstudio. Its awesome, definitely the best R CLI out there. i just wish the auto complete was more nuanced. ",1517980347.0
Deto,I usually run R over ssh with vim+tmux so I'll definitely give this a try,1517983829.0
jarandaf,"I would definitely use R from the command line if plotting performance were the same as within RStudio. Don't know why, but X11 performance isn't the same, which is very painful (e.g. when interactively plotting montecarlo trajectories). Thank you for sharing!",1517994570.0
MeanMrMustard92,I use your sendcode package for Sublime with R/Python all the time. This looks great. Thanks for the great packages. ,1518043585.0
,"If you want automatic width setting and colors without installing rice -
 try R libraries `setwidth` and `colorout`",1518013367.0
mtelesha,"Just use Jupyter it runs natively with R. Whole reason why I Python only speaks of the python kernel. Jupyter now supports 40 languages. What does Rice do that Jupyter doesn't.

http://jupyter.org/",1518020818.0
DataWave47,TIL some people don't like RStudio,1518023272.0
statguy,Guess the title should say 'Can't use rstudio try rice' ,1517985779.0
Petzval,How do you write an .R script with rice?,1517986684.0
Nesatam,"I have been using rice for the past few weeks and with vim it has been wonderful in a tiling window manager or over ssh in tmux.

Though my main issue with it is it's lack of support for GNU Readline as the vim mode doesn't work. That is the main reason why i might go back to vanilla.

It would also be interesting to see it render certain object directly in terminal  like tables or images

The ability to switch between R and bash is pretty awesome too though i wish the bash session would source my bashrc file 

Thanks for all the work ! ",1518298405.0
dkesh,"Why not have one data frame with three columns: Date, Precipitation, and City?",1517960101.0
blozenge,"Blimey, that is difficult to read! A pastebin would be easier, but you should at least turn off spellcheck next time.


The script loads a csv file of data (17 variables, ~8000 cases) and displays some descriptives of the variables (with str).


Next it fits a [linear regression model](https://en.wikipedia.org/wiki/Linear_regression) to the data predicting price from HD size in GB. It displays the coefficients of the linear model and then saves a copy of it to an object called linearMod.


The predict command uses the linear model to make a prediction for what the price should be with a HD size of 500 and 1000Gb.
The (abbreviated) syntax for predict is 
    predict(modelobject, newdata)
where newdata is the values of your predictors you want to make a prediction for.


The script then goes on to do this two more times with slightly different versions of the model with more predictors (now a multiple regression). Both times the model is saved as lmMod. The first iteration had HD Size and battery life, the second iteration has HD size, battery life and RAM size.


The predict command in each case is then used to generate some predictions given the model for specific configurations e.g. 2000 GB HD, 3 hours battery life.


I don't think this on its own can tell you much because there's not enough information to suggest the model is a valid way to determine price. Particularly worth thinking about is whether the relationship between price and component quality is constant over the range of the data. The difference in price between a 500 Gb HDD and a 1000 Gb HDD is not likely to be the same as that between a 3.5 Tb and a 4Tb, but in effect this is assumed to be true by the model.


I would guess the take away from the final predict statement is that (given the model), it's reasonable to expect to pay 1317 - 1088 = 229 more to get an 'upgrade' which gives an additional 500GB HDD, and 2 GB more RAM at the cost of 1 hour battery life. However the model residuals (or difference between the predicted price and the actual price for the input data) runs between -95 and +67 so without better regression diagnostics buyer beware.",1517960821.0
Darwinmate,"JFC. A screen cap of a code and output that's been posted in word.

Blozenge is a better man than I. ",1518047375.0
Darwinmate,"Give us some example data to work with. In:

    if(x %in% y) {ifelse(length(values)>1, ""values"", ""value"")}

What is being returned (""values"" and ""value""), a character? a logical?  ",1517961532.0
Darwinmate,SO post is gone.... so I dont know,1518047283.0
wilderecon,"I've worked with leaflet and found it pretty easy, although the map interface part was pretty simple. No experience with the other one.",1517936518.0
huxleyan,"I like leaflet because you can build interactivity with datatable using the crosstalk package.

Plus leaflet has a lot of documentation.",1517953004.0
newjrm,I've done this by scraping Weather Underground's historical pages. The daily numbers are in a table so it's pretty straightforward to scrape. The URL format is like this- *https://www.wunderground.com/history/airport/ric/2015/8/1/CustomHistory.html?dayend=31&monthend=7&yearend=2016&req_city=&req_state=&req_statename=&reqdb.zip=&reqdb.magic=&reqdb.wmo=*. At a glance there are a few similar packages out there [1](https://github.com/mpiccirilli/weatheR) [2](https://cran.r-project.org/web/packages/rwunderground/rwunderground.pdf),1517936098.0
midianite_rambler,"I'm curious about this too. In my city there are rain gauges which have records going back a few decades. I have often thought of doing some exploratory analysis with them. I think a general system for gathering weather data would be really great. Of course there is no one size fits all approach, but maybe we can assemble a library of functions that each work for a different source of data. Just thinking out loud here.",1517948679.0
Laerphon,"This is a bit lazy and clunky, but will work in your application:

    library(dplyr)
    summary_frame <- test_data %>% select(-ITI) %>% group_by(Cue) %>% summarize_all(mean)
    output_frame <- setNames(as.data.frame(t(summary_frame[,-1])), summary_frame$Cue)
    output_frame # Displaying result
",1517865134.0
efrique,"If your data frame was called `a`, you could do this:

    aggregate(a[,3:5],list(Cue=a$Cue),mean)
      Cue        a        b        c
    1   0 0.679780 0.510710 0.317230
    2   4 0.209805 0.331270 0.357665
    3   7 0.309140 0.134280 0.248870
    4  12 0.442505 0.588225 0.304565
    5  22 0.292965 0.511780 0.308835
    
Of course you can manipulate further as required.",1517871455.0
efrique,Are there always the same number of rows per subject? ,1517866699.0
samclifford,"Do you care about the intermediate object with the means inserted as rows? Because if not then I would avoid trying to create it and it's weird structure altogether and head straight for the final object.

Using the dplyr and tidyr packages you'd gather the a, b, c columns (so `gather(data, key, value, - c(ITI, Cue)`) then `group_by(Cue, key)` and summarise the result with `summarise(mean=mean(value))`. Then use `spread(Cue, mean)` to undo the gather from before. ",1517902408.0
_westernmagic,`x - 1`? Or [`dplyr::case_when()`] (https://rdrr.io/cran/dplyr/man/case_when.html)?,1517852147.0
moosejock,"Males =1 & Females = 2:  
ifelse (df$sex = 1, 0, 1)  
Now Males = 0 & Females = 1  ",1517855430.0
efrique,Why the heck would you not just subtract 1 from the variable?,1517867025.0
manwithoutaguitar,"dplyr::recode()

http://dplyr.tidyverse.org/reference/recode.html",1517856064.0
DemiGodSuperNaked,">df$newsex <- rep(3, length(df)) *# defining new column*

>df$newsex[df$sex==""2""] <- 1 *# if old column is a 2, put a 1 in the new one*

>df$newsex[df$sex==""1""] <- 0 *# if old column is a 1, put a 0 in the new one*

The presence or not of """""""" depends on if the original variable is numeric or factor.",1517868870.0
Brighteye,"I prefer: 

  df$newvar<- plyr::mapvalues(df$var, from = c(1,2), to = c(0,1))",1517863769.0
Darwinmate,"From the help file on `scatter3d`:

https://www.rdocumentation.org/packages/car/versions/2.1-6/topics/scatter3d

>`scatter3d` does not return a useful value; it is used for its side-effect of creating a 3D scatterplot. `Identify3d` returns the labels of the identified points.

So I don't think it's possible. Use a different package or maybe use `scatterplot3d` function. see

http://www.sthda.com/english/wiki/scatterplot3d-3d-graphics-r-software-and-data-visualization

and this vignettes:

https://cran.r-project.org/web/packages/scatterplot3d/vignettes/s3d.pdf",1517912852.0
Drewdledoo,"Your parentheses are misplaced. Currently everything is wrapped within the `ggplot` call, whereas it should be separated out. Try this:

    ggplot(df, aes(x=exposure, y=outcome)) + 
      geom_jitter(size = 2, alpha=0.4, position = position_jitter(height=0.02)) + 
      stat_smooth(method = ""loess"", colour = ""blue"", size = 1.5)

To be clear, your code was structured like this (note parentheses marked with `^`:
       
    ggplot(df, aes() + geom_*() + stat_*())
          ^                               ^ # All wrapped in ggplot

Whereas the two marked parentheses should have closed after the `aes()` call, like this:

    ggplot(df, aes()) + geom_*() + stat_*()
          ^         ^                       # Self-enclosed ggplot(), geom_*(), and stat_*()

edit: formatting",1517847637.0
Drewdledoo,"The `ggplot` code you posted is missing a `)` at the end of the `geom_jitter` call. Could that be it?

edit: Yeah, your parentheses are misplaced throughout. [See my other comment for a correction](https://www.reddit.com/r/rstats/comments/7vfo6c/ggplot_2_error_non_numeric_argument_to_binary/dtrwfv4/).
",1517847542.0
scottmmjackson,"Would you mind opening an issue?

https://github.com/tidyverse/dplyr/issues",1517845114.0
ColorsMayInTimeFade,Can you the code you're running?,1517885413.0
wytz,"When R knits a document, it does so in a new environment. 
It ignores the variables and datasets you have loaded in your console, where you are testing your chunks and loose commands. 

I assume this gap2007 dataset is a standard R-dataset? Try loading it explicitly in your markdown file (with the same command you did in your console), and run your code again. 
If your chunk works, the knitting process should work as well. ",1517816410.0
manwithoutaguitar,Did you try to load your dataframe in a ```｛r｝call before this code?,1517816296.0
scottmmjackson,Have you looked into https://github.com/tidyverse/forcats ?,1517811919.0
refined_compete_reg,"Try ifthan loop. Something like: 

Ifthan(is.na(beer.var.1)==T, beer.var.2, NA)

You could nest more options in if there are more conditions you are looking to replace. Search for ""nested ifthan loop in R "" for more info.",1517824759.0
infrequentaccismus,Knn imputation will do this ,1517847377.0
MrLegilimens,Just download swirl.,1517833564.0
sandalguy89,"I'm still a beginner myself, but working through r for data science is helping me get a good grasp at what I need to get stuff done.

Pm me if you want to chat sometime about it.",1517798744.0
wittja01,"Have you found anyone to help you yet? I wouldn't say I'm an expert, but I am pretty confident using r for data analysis, data manipulation/cleaning, plotting, etc. PM me if you want to chat.",1517800845.0
Darwinmate,"lovely username, made me laugh. 

Another one putting my handup, I'm in OZ so you're gonna have to PM me. Good luck getting help",1517802234.0
LiesLies,"Looks like there was a lot of willingness to help! 

There might be an opportunity to connect students and tutors going forward.",1517852317.0
crackednut,"Try kaggle.com and data.world ... If you're looking to practice on existing datasets, you'll find plenty on these two sites.",1517774071.0
spinur1848,"Here you go:

https://aact.ctti-clinicaltrials.org/connect

Have fun.",1517786387.0
SonicBoom16,Are you looking for *data* to practice with? Or databases?,1517782376.0
efrique,"> Would I be wrong in assuming its an exponential distribution?

Clearly wrong, since the values are discrete (integer); the exponential is continuous (the discrete equivalent would be the geometric). How do the values arise? What are they?


",1517781747.0
WayOfTheMantisShrimp,"It looks like a pretty close fit, but the ""steps"" that you can see on the QQ plot and eCDF suggests that this may be discrete data.

Exponential distributions are continuous, but a [geometric distribution](https://en.wikipedia.org/wiki/Geometric_distribution) is the discrete version. I'm not familiar with the package you mentioned, but you might try checking if you can specify fitting discrete distributions, or force it to attempt a geometric model.

Depending on what you are modelling, exponential might be entirely appropriate, but in other cases it would not make sense to use. If you want to discuss the context of the data/application, we might be able to advise you how to go about things.",1517819603.0
keepitsalty,Ask and ye shall recieve,1517694444.0
Drewdledoo,"I support this motion, with the caveat that in principle, RStudio Community is supposed to be for things that do *not* belong on StackOverflow. ",1517636501.0
hodos_ano_kato,I like this idea,1517634200.0
shujaa-g,"Searching the help `?deldir` for ""area"":

> **Value**
> 
> A list (of class `deldir`), invisible if `plotit=TRUE`, with components:
>
> ...
>
> `summary` a data frame with 9, 10 or 11 columns... The columns are: 
> 
> - ...
> 
> - `dir.area` (the area of the Dirichlet tile surrounding the point)
> 
> - ...

The `dir.area` column of the `summary` data frame is what you want:

    library(""deldir"")
    set.seed(47)
    n = 10
    x = runif(n)
    y = runif(n)
    
    dd = deldir(x, y)
    dd$summary
    #           x        y n.tri del.area  del.wts n.tside nbpt dir.area  dir.wts
    # 1  0.976962 0.138798     3 0.017828 0.049289       2    2 0.061740 0.081624
    # 2  0.373916 0.701987     2 0.026559 0.073428       3    2 0.082360 0.108886
    # 3  0.761502 0.162194     4 0.050045 0.138361       4    2 0.086250 0.114028
    # 4  0.822492 0.599307     3 0.023926 0.066149       3    2 0.092684 0.122535
    # 5  0.573544 0.506036     7 0.095317 0.263525       7    0 0.080882 0.106931
    # 6  0.691412 0.901974     3 0.033922 0.093785       3    2 0.119010 0.157339
    # 7  0.389062 0.400503     3 0.027488 0.075996       4    2 0.064058 0.084689
    # 8  0.468946 0.030945     2 0.007203 0.019915       2    2 0.046687 0.061723
    # 9  0.543310 0.071358     5 0.039499 0.109203       4    2 0.055114 0.072865
    # 10 0.924892 0.468317     4 0.039914 0.110350       4    2 0.067604 0.089378

",1517632905.0
dkesh,"Lines starting with four spaces are treated like code.  So:

    for (i in 1:max(raw.data$array_number)) { # run loop for each array
  
      print(i)
      data.temp <- raw.data[raw.data$array_number == i, ] # extract all detections for array i
       if(nrow(data.temp) == 0) next # skip iteration if there are no songs in data.temp
  
       mics <- miclist[miclist$Array == i, ] # extract mic coordinates for detection
       lights <- lightlist[lightlist$array==i, ] 
       speakers<-speakerlist[speakerlist$array==i, ]#add index to specify within row
       coordinates_light <- cbind(lightlist$gps_eastern_light[i], lightlist$gps_northern_light[i]) #ditto-add index here 
       point_light <- SpatialPointsDataFrame(coordinates_light) #
       coordinates_speaker <- cbind(speakerlist$gps_eastern_speaker, speakerlist$gps_northern_speaker) # 
       point_speaker <- SpatialPointsDataFrame(coordinates_speaker) #  
       poly <- as.matrix(mics[, 5:4]) # convert xy coordinates to matrix
       poly <- rbind(poly, poly[1,]) # add first row of poly matrix to end of matrix to create polygon
       poly <- Polygon(poly) # specify as polygon
       poly <- Polygons(list(poly),1) # add wrapper to polygon class
       poly <- SpatialPolygons(list(poly)) # add wrapper??
       poly.centre <- gCentroid(poly)
       #	poly <- gBuffer(poly, width = buffer, joinStyle=""ROUND"") # add buffer",1517611622.0
midianite_rambler,"No need to choose between different models. Let your forecast = weighted sum of forecasts from different models, weighted by the probability of the models given the data or some reasonable approximation thereof (such as 1/n where n = number of models).  This is a Bayesian approach by the way.",1517593288.0
mr-datascientist,"Cross validate on the training set, estimate rsme on the test set, but use tidyposterior to estimate credible intervals of performance to see if one model is practically better than the others.  ",1517671427.0
The_Sodomeister,"You can adjust the training/testing split to see if that edges one method over the other, or maybe choose the model which has better accuracy on the training data (assuming that all of your measurements have been calculated on the testing data).",1517611275.0
Fugalysis,Cross validate,1517617446.0
nkk36,"No experience with doing this myself in Shiny, but I believe it's possible. Here's the link where I read about something similar:

https://deanattali.com/2015/06/14/mimicking-google-form-shiny/",1517802668.0
agclx,"Tables in pdfs can be very tricky.  Depends how they were created.  So you will need to try which method works for you.  

Immediately I fount [pdftools](https://ropensci.org/blog/2016/03/01/pdftools-and-jeroen/) and [tabulizer](https://github.com/ropensci/tabulizer). They would be least problematic - if they work for you.  

Then there is also [pdftables](https://cran.r-project.org/web/packages/pdftables/vignettes/convert_pdf_tables.html) - however this one uploads your data to a company, you may have issues with that.  

Then there are semi-manual methods where you convert pdfs into a file type that is easier to handle in R. If you have Acrobat Pro it has the option to save your document into office format - and work from there.  I found it can find tables where other tools give up. There is some more software that can import pdfs - like Office - that also can import pdfs - but I was not satisfied last time.",1517556136.0
Darwinmate,"hhah that's great. 

but how do you bold in cat function?",1517553105.0
OnlyDeanCanLayEggs,That second picture looks like a bald koala.,1517586017.0
jazzlw,"Neat!
You should put this on r/dataisbeautiful ",1517579548.0
Runner1928,"Yeah, arules can return very few results based on the parameters you choose. See http://mhahsler.github.io/arules/ for more details. Support and confidence are important.",1517530327.0
LiesLies,"I'm not familiar with arules, but you may want to look into techniques for class balancing, especially up- and down-sampling. 

If your most interesting class only occurs 7% of the time, it makes sense that a simple model would converge on the idea of ""regardless of inputs, most of the time, this event doesn't happen"". It's akin to looking at a regression problem and just giving the training set mean as the predictions for all new observations.",1517592720.0
DemiGodSuperNaked,"Do I assume that you don't have sas? I mean, that is the code for *creating* that dataset in sas, if you do so, then you could save it as a sas data file and read it with foreing: https://cran.r-project.org/web/packages/foreign/foreign.pdf
I'm sorry if this is not helping you.",1517869652.0
infrequentaccismus,"It looks like there is a package called “klaR” on CRAN.  Pay close attention to the capitalization in you code and make sure it matches. Run:

     install.packages(“klaR”)
      library(klaR)",1517506955.0
DeuceWallaces,"Pretty nice, but I would prefer the addition of data.table.",1517513636.0
efrique,"You're not really explaining what you're doing in the question.

Is this univariate? [If so are you trying to estimate location (just use the median) or both location and scale (mean absolute deviation from median)?]
",1517445194.0
renshinihengaoxing,"Example 3, 1

> library(quantmod)

> library(VGAM)

> load(""06Data.RData"")

> Q <- 140000

> datax <- diff(Z1)

> minusll <- function(loc, scl) {
+ -sum(VGAM::dlaplace(x=datax,
+ location=loc, scale=scl, log=T) +)
+}

> mle.ex1 <- mle(minuslogl=minusll,
+   start=list(loc=0, scl=0.1),
+ method=""CG"", nobs=length(datax) +)

This is the example I can't seem to use/understand
",1517443703.0
ReimannOne,"use the `data = ` in your call to geom_line()

    autoplot(forecast_model) + geom_line(aes(y = observed_data, x = something), data = original_data)",1517444921.0
UbiquitinatedKarma,"Seems like that boxplot will only make sense with the salary bins on the X-axis, and for that I think you want `boxplot(Age~Salary)`.

If you want to move beyond base R you could do this in ggplot with something like:

`ggplot(df, aes(x=Salary, y=Age)) + geom_boxplot() + geom_jitter(aes(color=Age))`",1517445174.0
efrique,"If there's only two levels of salary and only a few levels of age, boxplots doesn't really make sense. You want one continuous (or at least 'interval' variable, and preferably one with a lot of different values) and one categorical",1517445884.0
Crypt0Nihilist,"    boxplot(Salary~Age) 
perhaps?",1517442744.0
infrequentaccismus,"I recommend a barplot instead.  Stacked or dodged.  Set the fill color as the age group.  Make the count, mean or some other other aggregate of interest as the y scale. Set the income as the x variable. Good luck!!",1517452479.0
lolercakesmcgee,"Sorry I can’t read your link right now, but the heatmap.2 function should allow you to define the breaks and specific colors for each break. 

I use stackoverflow for all these sorts of questions or otherwise just look in the help for the function. ",1517457615.0
Hefeystus,"On mobile so no reproducible example here but,

norm <- function (x){ return ((x-min(x))/(max(x)-min(x))}

is a standard way to normalize data. Normalize your vector of data and then multiply each value by 100/mean to have the average be 100. 

I'm assuming you have a basic working knowledge of R, I can give more detailed instructions if otherwise.",1517434632.0
dspace,You can use scale(x) to normalize and apply whatever transformation is appropriate (eg 100 + scale(x)*10 ). Particularly nice if your metrics are normally distributed.,1517435498.0
lakenp,Code here: http://rpubs.com/lakenp/pixel_worldmap,1517428139.0
iconoclaus,"start by using functional apply family of functions over for loops. then, parallelize your functional loops using the `parallel` package. it can parallelize across cores or across machines. google around for examples of each.",1517543748.0
Darwinmate,"I think this SO answers this question:
https://stackoverflow.com/questions/37514603/hyper-parameter-tuning-using-pure-ranger-package-in-r",1517374632.0
,Final plots would make for good /r/dataisbeautiful submissions.  ,1517419975.0
Evanescent_contrail,Awesome. That's what I come to see.,1517361339.0
pina_koala,Nice work.,1517414442.0
VincentStaples,"Get into the habit of working with subsets of your data. Alternatively, you can just remove NAs in the tests/plots directly.

Inefficient but easy to interpret approach:  
df.Reading <- subset(df, !is.na(Reading))  
df.Reading.and.Attention <- subset(df.Reading, !is.na(Attention))",1517343676.0
ReimannOne,You have a lot of questions without providing either sample data or expected outcomes.  That might not be so bad if there were at least some acknowledgement for us doing your homework for you.,1517338899.0
Sarlock,Check out the Blogdown documentation. Seems perfect for your needs. ,1517339260.0
too_many_splines,"What happens if a group has an unequal nonzero number of counts in different Item fields? For example:

|Group | ItemA| ItemB| ItemC| ItemD|
|:-----|-----:|-----:|-----:|-----:|
|A001  |     1|     0|     0|     2|

Does the transformed dataframe look like this?

|Group | ItemA| ItemB| ItemC| ItemD|
|:-----|-----:|-----:|-----:|-----:|
|A001_1  |     1|     0|     0|     1|
|A001_2  |     0|     0|     0|     1|

Or this?

|Group | ItemA| ItemB| ItemC| ItemD|
|:-----|-----:|-----:|-----:|-----:|
|A001_1  |     0|     0|     0|     1|
|A001_2  |     1|     0|     0|     1|",1517328664.0
Aemon12,"Are the latency scores following a growth curve in your hypothesis?
Then:

Latency ~ day + genotype + genotype:day + (1+day | subject)

Where day is a quantitative variable (1, 2,..) and you want to test whether genotype:day has a significant effect.

I assume here that subjects are under some daily treatment and you want to know if genotype moderates  the daily growth.",1517274269.0
thedukeofedinblargh,"I'd also add that if you expect to use mixed effects models in the future, [this book is both well written](https://www.amazon.com/Analysis-Regression-Multilevel-Hierarchical-Models/dp/052168689X) and all the techniques are illustrated in R using lme4.  Since it works whole examples, you also learn about various helper functions for extracting useful bits out of the giant model objects (which saves a ton of time).",1517282843.0
craoloro,"Much can depend on the distribution of your latency variable, and it appears it can perhaps be defined as discrete or count data (e.g, 0,1,2,3,4, etc.). Whether there are zeros in your data, and how many or how few could also impact the integrity of your analysis. I would try a poisson family first, yet a negative binomial will likely fit your data better. I would recommend glmmTMB: https://cran.r-project.org/web/packages/glmmTMB/index.html for mixed effect models. The vignettes may be useful, and search for GLMM Bolker - he seems a whiz in ecology and mixed models. Note, coefficient estimates would be in log form; hence you'd need to exponentiate 'em.",1517260364.0
_nt2,"`data` is the name of a function in `R`, so don't call your data `data`. Anyway, here is a slightly different dataset (first row has 33 but not 1 and second row has 1 but not 33) and a possible solution using `rowSums`:

    library(tidyverse)

    foo <- read_table2(""A1 A2 A3 A4 A5 A6
                      6 2 45 35 33 38
                      5 1 23 34 58 47
                      18 26 78 15 5 6"")

    foo %>% 
      filter(rowSums(. == 1) > 0 | rowSums(. == 33) > 0)
    #> # A tibble: 2 x 6
    #>      A1    A2    A3    A4    A5    A6
    #>   <int> <int> <int> <int> <int> <int>
    #> 1     6     2    45    35    33    38
    #> 2     5     1    23    34    58    47


",1517238827.0
malkav183,"Filter looks each column separately so you either need to write code with a lot of or statements or some other route. If you know you will only need two values then you can just:
DF[apply(DF,1,function(x){1%in%x |33%in%x})]
Sorry for bad formatting writing from mobile.",1517238284.0
ropty,"filter_all might be what you are looking for:

        library(tidyverse)
        
        foo <- read_table2(""A1 A2 A3 A4 A5 A6
                          6 2 45 35 33 38
                          5 1 23 34 58 47
                          18 26 78 15 5 6"")
    
    > foo %>% filter_all( any_vars(.==33|.==1)) 
    # A tibble: 2 x 6
         A1    A2    A3    A4    A5    A6
      <int> <int> <int> <int> <int> <int>
    1     6     2    45    35    33    38
    2     5     1    23    34    58    47
        
  It works because filter_all looks at all the columns and applies its function to each. ",1517278664.0
dtrillaa,"Not to beat a dead horse, but this combines your use of a vector and %in% with the use of filter_all. I believe this is as ""tidy"" as you can get the code, because now you can filter on any vector of values.
    

    library(tidyverse)

    foo <- read_table2(""A1 A2 A3 A4 A5 A6 
                                 6 2 45 35 33 38 
                                 5 1 23 34 58 47 
                                 18 26 78 15 5 6"")

    bar <- c(1, 33)

    foo %>%
      filter_all(any_vars(. %in% bar))

EDIT: Alternatively, if you do this more than once, here's the function form.

    library(tidyverse)

    foo <- read_table2(""A1 A2 A3 A4 A5 A6
                   6 2 45 35 33 38
                   5 1 23 34 58 47
                   18 26 78 15 5 6"")

    bar <- c(1, 33)

    filter_rows <- function (df, vector) {
      filter_all(df, any_vars(. %in% vector))
    }

    filter_rows(foo, bar)",1517327511.0
dm319,"just adjusting /u/_nt2's answer a bit so you can keep your vector of matches:

    library(tidyverse)
    
    foo <- read_table2(""A1 A2 A3 A4 A5 A6
    									 6 2 45 35 33 38
    									 5 1 23 34 58 47
    									 18 26 78 15 5 6"")
    
    bar <- c(""1"",""33"")
    
    matches <- matrix(unlist(foo) %in% bar, nrow = dim(foo)[1])
    
    matched_rows <- rowSums(matches) > 0
    
    foo[matched_rows,]",1517245384.0
natched,"You can do it more simply by setting the 'size' parameter of rbinom to your 'n' and then dividing by that. This eliminates the for loop.

    rbinom(number_of_samples_to_take, size=n, prob=p) / n

And one thing missing from your work is calculating the variance of your distribution and seeing if it is what it should be.",1517191847.0
beren323,"That graph is VERY normal. Not just a little normal, a little deviation on the ends actually would happen even if you draw a sample directly from the normal distribution. Try d <- rnorm(10000,0,1);qqnorm(d);qqline(d).

From the theoretical side, a statistic doesn't really have just a single number, it actually has a distribution of values. The values f1,f2,f3,... form a distribution.",1517195390.0
efrique,"> Simulate f = X/n , show that the proportion follows the normal distribution 

It doesn't. However, for large n the distribution will be very close to a normal distribution (as long at the population proportion isn't too close to 0 or 1).

> The plots follow the norm line pretty closely, so it seems ( from this ) that the statement was true

This would suggest that the deviation from normality was small, not that it actually was normally distributed. 

[Indeed, a well-chosen test could easily reveal that it wasn't normally distributed.]
",1517231848.0
mikethechampion,"Clarifying question: you mean you have two separate panels that have different individuals (or whatever the unit of observation is), but otherwise same independent variables? Sort of like you administered the same survey every year but switched whom you are asking it to in the middle?",1517163686.0
Payne77,nobody knows about that?,1517257753.0
lawkee,"For anyone interested in this topic, modelr::formulas, modelr::fit_with, and purrr::map(~broom::tidy(.x) ) do a pretty good job too.",1517139098.0
,"In the code:

    if(visualize == T){

Why not simply

    if(visualize){",1517136684.0
efrique,"Your use of ""true"" and ""false"" could be clearer.

Note that the F-test for variances is strictly only valid when the populations are both normal (and it's fairly sensitive to that assumption). Your significance level won't be what you chose otherwise. Of course, if the point is to investigate the *impact* of such assumption-violations, this is fine.

What you should see if you use normally distributed data (but you need to do a much larger number of simulations):

- if the null hypothesis is true, you should nevertheless reject a proportion of time equal to your significance level (in some situations, that significance level is an upper bound rather than an equality, but it looks like you aren't in any of those situations)

- when the null hypothesis is false you should reject more often than the significance level (since this is not a biased test -- some common tests can be), but it won't be 100%. If the variances are close together and the sample sizes are small it may not be a very high rejection rate.

---

Curious why you're calculating variance ""by hand"" rather than using the `var` function. I'd also suggest considering using `replicate` (passing back just the statistic), so you can do more than one test at a time; also see `var.test`. 
",1517089886.0
TripKnot,[r-bloggers](https://www.r-bloggers.com/) may be a good place to find small projects with code snippets that you can play around with and reproduce.  ,1517089944.0
,The good old github.com is always a good soure for interesting R source codes,1517096182.0
RebelSaul,"I second github, but here's a generic souce of things: 
https://www.datamentor.io/r-programming/examples",1517108670.0
ryapric,"It depends on what your use case would be; if you want to learn how people wrote *packages*, then I'd recommend starting by reading Hadley Wickham's R Packages (sorry, on mobile) to understand package structure. Then, look at the source code of the packages you use most often. You may be surprised how many of them you could even help improve!",1517110364.0
don_draper97,Kaggle is a great resource due to the wealth of submissions and the ability to see different codes and packages put into use in real-world situations. ,1517154433.0
klo99,Stackoverflow is a good source as well,1517167556.0
rforjournalists,"Specifically I would like to add GBP signs to the legend values (i.e. £1,050,000,000 and £10,750,000,000) and change the font of the text. I tried using the scales and extrafont packages but couldn't find any joy. Also the addMapLegend() function in rworldmap didn't seem to work, I got this error message:

    Error in image.default(iy, ix, t(iz), xaxt = ""n"", yaxt = ""n"", xlab = """",  : 
    'x' and 'y' values must be finite and non-missing
    In addition: Warning messages:
    1: In min(x) : no non-missing arguments to min; returning Inf
    2: In max(x) : no non-missing arguments to max; returning -Inf

    Source data:

                                  country       total Code
    1                             Saudi Arabia 10750979451  SAU
    2                 United States of America  8893167709  USA
    3                                   Israel  8289736928  ISR
    4                     United Arab Emirates  7282975953  ARE
    5                                    China  4951121818  CHN
    6                                   France  4229626503  FRA
    7                                   Taiwan  2989593430  TWN
    8                                     Oman  2496732723  OMN
    9                              South Korea  2324709823  KOR
    10                                Malaysia  2299054565  MYS
    11                                    Iran  1883563151  IRN
    12 Hong Kong Special Administrative Region  1595659149  HKG
    13                                  Norway  1314111294  NOR
    14                                   Italy  1281783411  ITA
    15                                   Japan  1239764726  JPN
    16                                   India  1238778523  IND
    17                                  Russia  1180164207  RUS
    18                               Singapore  1102687933  SGP
    19                                  Turkey  1084617310  TUR
    20                                 Austria  1084255034  AUT
    21                            South Africa  1050001852  ZAF",1517055902.0
fasnoosh,"Didnt get a chance to run it in R, but it may be caused by the ldf-leftOver+1:ldf part. Try using parentheses to lock in exactly what you’re trying to do, so you aren’t relying on order of operations",1517030849.0
efrique,"You seem to want

    (ldf-leftOver+1):ldf

rather than what you asked for, which corresponds to

     ldf-leftOver+(1:ldf)

watch out for operator precedence/ order of operations.

Also see 8.1.3 in https://www.burns-stat.com/pages/Tutor/R_inferno.pdf",1517041000.0
BeeeefSupreme,"The quote does nothing to distinguish data engineering from data science, it's just chock full of buzzwords.",1516987387.0
efrique,"
How is the author defining ""data engineering""? 
",1517006362.0
kzbigboss07,"[Kimball’s Datawarehouse Toolkit](https://www.amazon.com/dp/1118530802/ref=cm_sw_r_cp_api_Ig5AAbYNKDDER)  is a solid place to start with the foundations of data engineering. 

My career rides the line between scientist and engineer.  The distinction is the engineer needs to build a data platform for the scientists/analysts to stand on. Without thinking about how to best model the data at rest (data lake or database), you put a tax on the scientists/analysts to do a bunch of transformation work before they can get to experiments and analyses. 

Kimball walks through the basics of building a data warehouse that involves how to preserve source data and some ideas to build various aggregations or other logical layers on top. I like that each chapter starts with a business concept (point of sales capture, supply chain, ect...) then walks through challenges on how to solve them when it comes to warehousing the data. ",1516999442.0
lstmemery,"I haven't been able to find many resources, unfortunately. The field is very new and the technologies change from vendor to vendor. A good vendor-agnostic introduction is Designing Data-Intensive Applications by Martin Kleppmann.",1517011330.0
,[deleted],1516990516.0
DarthBane9955,"For n=21 you have a total of 22 different solutions: 0 - 21. Because 22 is even you can exactly split this in halves.
On the other side for n=20 you have 21 different solutions. Because 21 is uneven you cant split it in halves.
As you may see: for n=18 and x= 0:9 it will work.",1516991956.0
john_ensley,it might be getting cut off because the margin isn't big enough. try changing par('mar') like they do [in this article](https://www.r-bloggers.com/setting-graph-margins-in-r-using-the-par-function-and-lots-of-cow-milk/amp/).,1516975261.0
efrique,"This sounds like basic class homework. You should explain what you're doing

1.             
             
              var3 <- var1 * var2

2.           
             
              var2/100



You need to read a basic document on how to use R. There are hundreds of good ones around, including on the CRAN website.



",1516933621.0
where_is_the_mustard,"Although this does seem really cool, why would I ever want to do this? serious question.",1516943131.0
Thoxic,"1) Look into the filter() function (dplyr package, I think).
2) Look into the arrange() function (also dplyr, I think).
(sort() could perhaps also work)
3) see 2)
Do note, that if 'subjects' is a factor (and not a string)
ordering would probably follow factor levels.
Converting it to string (as.character()) could help then.",1516914784.0
Tarqon,"I just wanted to add that in response to your first question, the most primitive way to do this is

    dataframe[dataframe$sex == ""female"",]

Dplyr's filter function is a convenient wrapper around this.

The same applies to ordering:

    dataframe[order(dataframe$mass),]",1516971765.0
questionquality,"While knowing how to do these things in R is nice and useful, it sounds like your first objective is to learn some basic statistics. Any online course claiming to be able to teach you basic statistics or machine learning in R should do to get you started.",1516883896.0
infrequentaccismus,"One hint to get you started: the traditional teaching method would be to show that on can measure a difference between means for each gender using a t-test. More advanced tests might show a difference in means between several levels of race (ie more than two races).  Ultimately these correlations do not answer the question: “is there racial/ gender discrimination?”  You can get much closer by running a linear model where you can “control” for things like education, age, experience, etc. I’m the end, the only way to prove a causal relationship would be with a controlled experimental trial (ie, you take a set of males and females who are exactly the same, raise them in exactly the same conditions, give them the same education, etc and see if their pay is different).  Since this “gold standard” of causal proof is not likely possible or feasible, we are left with doing our best to control for possible “confounding variables”.  Have fun learning about this... you’ll be glad you did!",1516889351.0
MrLegilimens,"T.test() aov(lm()), and lm().  But if you’re New i don’t understand how you’re going to solve anything if you can’t even tell what it’s asking.",1516880850.0
spinur1848,"Well, your homework has asked for the answer to be coded in Stata. R does many things but it does not speak Stata (as far as I know).",1516878887.0
IpwndGoliath,"Not the answer you're looking for, but..

You could probably do your 2450 tests by running nested for loops which paste the X and Y variable names into the function calls. Or I'm sure there's a way to do it using vectors and the map/apply functions. ",1516852013.0
UbiquitinatedKarma,"Generally you really don't want to manually edit datasets. One of the reasons for using R is to get away from that sort of thing and get more into the habit of programmatically, reproducibly making these types of adjustments. Specifically I would suggest looking into using the tidyr and dplyr packages (both part of tidyverse) to clean your data and do some manipulations.

Chapters 5 and 12 in this book should help you get started : http://r4ds.had.co.nz/",1516841568.0
coip,"You should be able to manually edit any dataset in any version of R via the `edit()` or `fix()` commands. For example, enter the following into the console:

    df <- mtcars #creates a data frame called 'df' from 'mtcars' dataset
    edit(df) #allows you to manually edit the 'df' data frame in a new pop-up window",1516890285.0
nobadchainsmokers,"I know its kind of late but a big reason to not manually edit is for reproducibility. Every time you want to rerun your code, you'll have to manually edit whatever you did again. The better option may be to script your edits. This way others (and yourself) will be able to see every edit that's happening. It will be reproducible. And every time its run, the script will handle the edit which will always be faster than a human without making an errors.",1516894084.0
efrique,You need to ask the developers of R studio why they would decide include or not include a particular feature.,1516847716.0
JackOneill,"Assuming the missing values are NA:

`V3 <- ifelse(is.na(V2), V1, V2)`

And if you haven't attached the data (they're still part of a data frame rather than separate vectors), you'll need to specify like this:

`DAT$V3 <- ifelse(is.na(DAT$V2), DAT$V1, DAT$V2)`",1516839393.0
MachupoVirus,"V3<-V2

for(j in 1:length(V2)){

if(is.na(V2[j])){

V3[j]<-V1[j]
}
}",1516839653.0
guepier,"An [opinionated API](https://stackoverflow.com/q/802050/1968) is an API that predominantly supports *one way* of doing things, rather than providing maximum flexibility to accommodate different workflows. The first API that I can remember where this was a major selling point was the [Ruby on Rails](https://en.wikipedia.org/wiki/Ruby_on_Rails) web framework. Before RoR, web frameworks gave very general, fairly low-level tools to build websites. RoR did pretty much the opposite.

Opinionated APIs have advantages and disadvantages: An opinionated API forces users to conform to a certain style, and is generally inflexible: if there’s a scenario that the developer of the API didn’t incorporate, it’s often impossible or complicated to solve this use-case with that API. For instance, the tidyverse works very well on “tidy data” — that’s what it’s for, after all! But it does not support working on wide tables/matrices very well at all.

Opinionated APIs are often more robust in the sense that they are more restrictive: they are hard to use wrongly by accident. They also typically allow the users to write a lot less code to achieve a goal.

For these reasons it has become a lot more fashionable to write opinionated frameworks: they are generally small in scope, do one thing, and do it well, and are easier to maintain and use than large, low-level, general-purpose frameworks.

From an engineering standpoint, opinionated frameworks often represent the second or third generation of frameworks that solve a given problem: in the first generation of tools, people are still figuring out the best way of solving problems, and therefore require low-level, flexible tools. Once experience has been collected, developers can hone in on a handful of principles that are useful, and encapsulate these into one (opinionated) API.",1516877134.0
,"Just a guess but it probably means it is designed according to a specific opinion of how things should be done. Examples: ""the grammar of graphics"" for ggplot, ""tidy data format"" for tidyr.",1516837501.0
oggesjolin,"You could use unnest:

library(tidyverse)  
  
data %>%  
    unnest(column3 = strsplit(column3, "",""))  

Works nicely if you have an unknown number of values in the list..",1516827715.0
jemne_perliva,"Try [separate](http://tidyr.tidyverse.org/reference/separate.html) and [gather](http://tidyr.tidyverse.org/reference/gather.html) from tidyr package.

It could look like this:  

    mydata %>%  
      separate(col = column3, into = c(""col3a"", ""col3b"", ""col3c""), sep = "","") %>%  
      gather(key = ""column_temp"", value = ""column3"", col3a:col3c) %>%  
      select(-column_temp) %>%  
      na.omit()",1516820680.0
edimaudo,It depends on the functionality you need.  It is pretty good since it contains some of the main machine learning libraries.,1516806856.0
too_many_splines,Scala and spark work beautifully together. ,1516810386.0
shujaa-g,"If you think you'll do deep learning, I'd learn Python. It's closer to the Deep Learning, while still being nicely high-level. For example, RStudio's interface to TensforFlow goes via Keras, which is written in Python. So you're essentially going from R to Python to get to TensorFlow, and you're potentially limited by the first R to Python step.",1516809899.0
guepier,"It’s extremely confusing that this article keeps talking about a vector called `my_list` that is *not a list*! The semantics for `[` and `[[` differ between lists and vectors.

Getting these details right is *especially* important in the context of teaching, i.e. in this blog post.",1516817114.0
SeveralBritishPeople,"You could just divide the matrix by its sum, ie `x<-x/sum(x)`. 

I can’t get formatting on my phone right with underscores, so swap in your matrix variable name for x. ",1516763566.0
efrique,"Your code doesn't run! You should make sure that what you post runs.

What you need is `my_matrix <- my_matrix/sum(my_matrix)`
",1516765450.0
fusionet24,"Have you tried debuging and stepping thorough your code?

add browser() above your URL. Then Isolate your issue to which line of code. From there step into the code and work out what is happening. 

I've never had much look with Rjava. 

Edit:
[This is a basic start] (https://www.rplumber.io/docs/tips-tricks.html#debugging) from the documentation.

Also consider building logs, maybe implementing one of the [good logging packages for R] (https://awesome-r.com/#awesome-r-logging) or like me build a custom package for our internal Plumber APIs. It will save you a lot of effort if your API becomes complicated. 


",1516811773.0
guepier,"You can simplify your code quite a bit by using `lapply` instead of a `for` loop:

    m = function (...) {
      as.character(lapply(lazy_dots(...), `[[`, 'expr'))
    }

But instead of {lazyeval}, consider using {rlang}, which effectively deprecates the former:

    m = function (...) {
        vapply(quos(...), quo_name, character(1))
    }

Alternatively, here’s a way of doing this in base R:

    m = function (...) {
        as.character(substitute(list(...))[-1])
    }",1516746939.0
fukalite,"Hi /u/Jon-Osterman,

 If you could please post a code snippet of the code you're trying to run and the output of

    str(yourdataset)
then we will have a better idea of what you're trying to do and how we can help.
",1516735278.0
Jon-Osterman,"for anyone who sees this in the future, if you get ""c50 code called exit with value 1"" and none of the results online (stackoverflow et al) are able to help you, try checking if the strings in your variables are too long! if you're able to find a code to match them, use that code instead.",1516811530.0
headfullofradio,"Perfect timing, will be needing this within the next week.",1516731163.0
WayOfTheMantisShrimp,"This topic has nothing to do with R, and not much to do with statistics either. Also, because you didn't give any details about what your goal is with this ranking, it's not an interesting or easy to answer math problem either.

For the sake of being constructive, consider these options:

1) Add up all the variables, take the sum/average as your final rank. This gives equal weighing to all variables and all scores.  
2) Add the squares of the variables. This gives stronger weight to highest scores, even if other variables for the same observation see much lower scores.  
3) Add the square-roots of the variables. This ranking will favour observations that score consistently high across the board, even if they are not top in any one field.",1516640623.0
NoFascistAgreements,"You can also try a PCA, and use the first component as your index, similar to 'asset indices' which you may be familiar with as an economist.",1516720472.0
john_ensley,"Using `dplyr`:

    mutate_at(df1, vars(-date), funs(approx(date, ., df2$date)$y))

That assumes you want to pass every column of `df1` except `df1$date` to `approx()`. You can select the columns in other ways depending on your needs: instead of `vars(-date)`, `vars(V1:Vn)` or `vars(starts_with('V'))` would also work in your example data, but may be more appropriate in the context of your actual data.",1516650373.0
lakenp,"Here's my list of R learning materials, including many free books and courses: https://paulvanderlaken.com/2017/08/10/r-resources-cheatsheets-tutorials-books/",1516634764.0
klo99,"I often recommend to Ph.D. students in biomedical sciences to start with ""R for Data Science"" by Hadley. ",1516630851.0
DeuceWallaces,"Books are nice. I have 100s in PDF for stats and ecology. On the other hand, if you know what you actually want to accomplish you might get much farther much faster by simply googling it. 

""R genetics package'
""forum R repeated measure drug trial""
""R pairwise allele test""
""R hierarchical hospital patient doctor""

Etc., you will likely find tons of blogs and detailed stack exchange posts that will have you off and running.

",1516630028.0
ychinenov,"take a look at ""Introduction to statistical learning"". I believe it is also available online for free.",1516621494.0
audbro,"I've used quite a few books and plenty of free materials, but ""Statistics: An Introduction Using R "" by Michael Crawley has probably been my most used text.",1516643517.0
efrique,"> But apparently great statistics material.

It has quite a few errors of various kinds. I'd approach it with caution. ",1516676281.0
hetero-scedastic,"I really like Julian Faraways books ""Linear models in R"" and ""Extending the Linear Model with R"". Many more advanced techniques take linear models as a starting point.

Venables and Ripley's ""Modern Applied Statistics with S"" (also covers R) is popular.

However don't jump into advanced statistics without a bucketload of visualization and exploration. See Hadley Wickham's books for that.",1518906558.0
,"How about:

  y <- subset(df,condition1>5 | condition2>10 | condition3>15,select=crit)
",1516556953.0
slammaster,"There are probably more efficient ways to do this overall, but to turn a list of vectors into a single vector just use unlist(y) ",1516547424.0
Runner1928,The dplyr library will help with this sort of problem too. I now prefer it over most uses of subset().,1516550534.0
Hoelk,"you probably just want to do

    df[df$condition1 > 5 | df$condition2 > 10 | df$condition3 > 15"", ]$crit

`subset(df, ...your conditons...)$crit` instead of the `select` argument should also work

the problem with your code is that `subset(df,condition1>5,select=crit)` returns a data frame, while you want a vector",1516568758.0
paerb,"You are looking for [MiniCRAN](https://cran.r-project.org/web/packages/miniCRAN/index.html)!

[This vignette](https://cran.r-project.org/web/packages/miniCRAN/vignettes/miniCRAN-dependency-graph.html) covers exactly what you describe.",1516500759.0
TripKnot,"Edit:  Sorry, didn't see that you had no internet connection... eh I'm not sure the best way to proceed then.

Start R from the command line `R`

From an R prompt enter:

`install.packages(""x.ent"")`

This will install x.ent and all dependencies. 

 If you want to install multiple packages at once use the `c()` as in:

`install.packages(c(""x.ent"",""ggplot2""))`

You can also do this from within RStudio under Tools -> Install Packages and then enter multiple packages separated by a space.
",1516495112.0
ron_leflore,"Microsoft has a version of R called Microsoft open R that they package with other stuff for machine learning.  It has some advantages compared to installing from Cran.

They have instructions on offline installation here.

https://docs.microsoft.com/en-us/machine-learning-server/install/machine-learning-server-linux-offline
",1516568831.0
rpietro,might want to look at https://cran.r-project.org/web/packages/pacman/vignettes/Introduction_to_pacman.html - will take care of all dependencies,1516590023.0
fdren,"I’m an R developer in a production environment. I’ve also set up RStudio Server and shiner Server for personal use.

PM me and I’ll help you out on a google hangout if you need it.",1516712718.0
infrequentaccismus,"Try this:

    library(dplyr)
    df1 %>% 
      left_join(df2, by = ""Unit"") %>% 
      filter(between(Date, DateIn, DateOut)) %>% 
      group_by(Unit) %>% 
      summarise(Num_Total = sum(Num))",1516511701.0
JaceComix,"Based on your example code, I think this is what you're looking for:  

    df2$want <- apply(df2, 1, function(x) sum(df1$Num[df1$Date >= x['DateIn'] & df1$Date <= x['DateOut'] & df1$Unit == x['Unit']]))  
I'm assuming you wanted the date range to be inclusive, but if that's not the case, you can just change the logical ops.",1516510605.0
samclifford,"Are you saying you want something that looks like what ggplot2 does with `facet_wrap()` without learning ggplot2? So each data set gets its own axes but the scale is common across them all? You can use `par(mfrow=c(1,3))` to tell R to set a plotting matrix with one row and three columns. Then you run the plot code for each graph (calling `plot()`will move to the next cell in the plotting matrix) but set the y limits manually and use the max max and min min from the three data sets. ",1516485164.0
efrique,"If you have the x-axis values repeating three times that's really three plots side by side, just really close together",1516491831.0
Science_and_Sports,"Here's what I use to send my daily reports.  It's a combination of rmarkdown::render and the mailR package.  The basic idea is that I have an R script that pulls in my data, manipulates it, and builds my graphs.  Then I have an Rmd file that just calls my graphs and has titles and such.  At the bottom of my R file is where I render the Rmd file which creates a nice looking html file with just headers and graphs, and send the email.

Here's some relevant code, with a few things edited out for privacy sake since this is work data.

        
    rmarkdown::render(input = 'DailyToHTML_Launch.Rmd', output_file = sprintf('./DailyReports/LaunchDaily_%s.html', 
                                                                      format(today, format = ""%m-%d-%y"")))

The sprintf function is just so I can get today's date in the name of the html file (I have a variable called today that gets the date from Sys.Date())


    from <- ""emailname@place.com""
    to <- ""emailname@place.com""
    cc <- ""emailname@place.com""
    bcc <- ""emailname@place.com""
    subject <- ""Daily Report""
    fileName <- sprintf('./DailyReports/LaunchDaily_%s.html', format(today, format = ""%m-%d-%y""))
    body <- ""whatever you want here""
    
    send.mail(from = from,
                  to = to,
                  cc = cc,
                  bcc = bcc,
                  subject = subject,
                  attach.files = fileName,
                  html = T,
                  inline = T,
                  body = body,
                  smtp = list(host.name = smtpServer, port = 25),
                  authenticate = F,
                  send = T
        )

You may have to mess with the host.name and port variables - I don't remember where I got those from or if I needed to do anything about that.


One note from setting this up and tweaking it a lot:

I have a variable early in my code called stopVar which I set to STOP if any of the data is missing from that day or something is messed up.  Then before I send the email, I have an if then statement looking at that variable and if it is STOP, I simply send an email to myself with the subject ""Data not ready - must send manually"".  That way no errors are getting sent out to people.

Let me know if you have more questions and I can try to answer sometime this weekend.  Good luck!
    ",1516461239.0
lemur78,Watching... ,1516445405.0
TeslaIsAdorable,"rmarkdown::render + the emailR package? I have Rscripts that run automatically, so as long as you can trigger it once you can automate it.",1516451886.0
enilkcals,"If you were on a GNU/Linux system I would recommend setting up a [cron](https://en.wikipedia.org/wiki/Cron) using `Rscript` to call a short script that knits your file and then uses `emailR` /u/TeslaIsAdorable suggests.

I'm not sure how to scheduled such tasks under M$-Win as I've not used the OS in over 12 years!",1516457299.0
UbiquitinatedKarma,"Have you tried either of the email packages discussed here :
https://stackoverflow.com/questions/23412265/how-do-you-send-email-from-r  ? ",1516449674.0
fang_xianfu,"In my view:

#Questions

All questions should come with a complete and minimal reproducible example.

With R, that means you need to provide a minimal sample of your data that still shows the problem. If you can't provide your data, you should make up a dataset instead.

Then post the minimal amount of code that can be run on the sample data in a completely fresh R session and cause the problem you're having. If you're asking a ""how do I?"" question, show as far as you've managed to get. You should know it can be run in a fresh R session by testing it.

These two steps are important because 90% of the time when you do them, you'll find the problem yourself. By cutting out code and data that aren't relevant to the problem, you'll find what it is.

Finally, post a dataset that shows where you want to end up. After whatever magic code has been run on the data you provided, what do you want to see? This is important because no matter how you try to describe things, it will always be ambiguous. Simple words like ""by"", ""group"", ""add"", ""link"" etc have specific contextual meanings, and it's hard to get everyone on the same page. Example data is totally unambiguous.

Finally, explain why you need the data to be that way. This is useful because the answer can sometimes be a change in your approach.

If this sounds like a lot of time investment to ask a simple question, well, it is. But you're investing time so that it will be easy to help you. The easier it is to help you, the more and better responses you'll get, which saves time in the long run.

#Answers

In exchange, answers should provide a minimal set of code that, when run on the example data, result in the provided end data, and an explanation of what the code does. If there is more than one possible solution, the answer might include a comparison of them, or an explanation of why they weren't explored. If the answer implies a bug in your code or a problem in your way of thinking, it should also explain what was going wrong in your examples and how your mindset needs to change.

If that sounds like a lot of time to invest in answering a simple question, well, it is. So please respect the strangers upon whom you are relying for help enough to spend time asking a quality question.",1516478776.0
efrique,I think a phone is not a suitable device for working on any but the most trivial of code. I have occasionally used R on a phone but I wouldn't try to debug something substantial enough for the formatting to matter much on one. ,1516435792.0
agclx,"In almost all cases I find it necessary to actually run the code. At least as courtesy to not post typos. That makes also answering on a phone near impossible.

I've seen some sites provide online evaluation of R code - like [datacamp-light](https://github.com/datacamp/datacamp-light). I could see such a representation useful. However I doubt it will be useful to many questions.",1516443541.0
fonzy6,"Check out the sweep package. 

http://www.business-science.io/code-tools/2017/07/09/sweep-0-1-0.html",1516455424.0
MeanMrMustard92,"Pooled OLS is usually done as an illustrative exercise before showing panel estimates. It just means you run a regular OLS regression on all your data ignoring the time components entirely (i.e. not a panel model  with year-FEs etc). So regular `lm` should suffice. 

As an aside, this table is awful to read: it stacks 3 regressions in each column and it is easy to mix up the dummy variable for Female with the F statistic. 

",1516401085.0
Iron_Rod_Stewart,"Without the analyses section, it's hard for me to know what he did exactly.

But for panel data (as in longitudinal data) then I assume you aren't just interested in static income, but also _change in income over time_. (E. g., whether income increases faster for taller people.) For that I would do growth curve modeling. To do that you would just include time as one of the regression variables.",1516400785.0
10101010101111,The `offset` function only makes sense inside a formula context. What are you trying to predict? If you have another dataframe with the same “shape” as `mydata` you can simply pass that to the `newdata` argument of `predict`,1516355425.0
blahblahblahblah8,"Offset is a function which only has meaning inside the modeling function, because it is a parameter of that function which controls how a predictor is treated. The predictor in this case is year. You have instructed your modeling function to do something special with the variable year. This must mean that there is a variable called year in the dataset you ran the modeling function on. If you look at the first few rows you will see it. If you want to get predictions in a new dataset, then that dataset also needs to have the variable year, along with any other variables in the model. You don't pass offset() to  ""data.frame""  because data.frame is just creating a data structure. The predict() function will handle that for you. 

All of that said it sounds like an introductory R course like an online tutorial would be useful to you.

Edit: if you want to get the fitted values then either extract them from the mode object or predict using the original data set used to build the model.",1516377980.0
RisFunr,"As you state, dataset two has multiple CODE records. For any code with multiple dates, which record do you wish to join to?

Once you've decided this, you can transform Dataset 2 to keep only the desired records. Then you can use a left join between Dataset 1 and dataset 2",1516355715.0
kokakapo,"Doing some form of join sounds like the best idea. 

But how do you want the problem of the non-unique date codes resolved? Do you want to use the first date as it appears in the dataframe? Do you want to use the mid-point of the two dates? Do you want to use the latest date?",1516351496.0
spraynard,inner_join will remove the rows that don’t have a matching value,1516338752.0
efrique,">  What are you thoughts/suggestions?

Likely almost everything you want to do is readily available, though the way you achieve it will be different, and you may not get the information in exactly the way you'd be used to (though it's easy to write a backend on some other function to take what is returned and present it in any way you like, if it hasn't already been done). 

That is, with a little effort you should find R quite suitable.

I'd suggest:

1. Work on learning some basic R. The ordinary distribution of R lets you do regression, time series, GLMs and much more, so there's plenty to get going with. 

 There are introductory documents, videos, books, courses, blogs and all manner of fora from which to get information. Once you have some basic familiarity (at the very least, enough to run a regression and get the output, look at a few plots):

2. Make a list of the main kinds of analysis you want to be able to do for your project, plus any diagnostic displays or other calculations you want

3. Start with the Task View for econometrics:

 https://cran.r-project.org/web/views/Econometrics.html

 and locate what you can of your list from step 2. there. This will tell you the what the main packages are that you'll need to add to the normal R distribution (this is generally trivial) to get R to do what you need.

4. Investigate further what R can do for econometrics, e.g. by starting with google searches like *R for econometrics*, which brings up all manner of useful links.
",1516324850.0
mbillion,"https://cran.r-project.org/doc/contrib/Farnsworth-EconometricsInR.pdf

",1516319117.0
savage_geek,"If you were planning to use EViews for time-series estimation (ARIMA,VAR), you can use this [simple guide](https://a-little-book-of-r-for-time-series.readthedocs.io/en/latest/) to get you started. It takes take you from installation to some simple analysis and plots in a few hours. 

If your analysis is more involved and theory-intensive, Diebold's complete course notes are available [here](http://www.ssc.upenn.edu/~fdiebold/Teaching706/TimeSeriesEconometrics.pdf). It might be overkill but it's a fantastic reference.

Good luck!",1516355634.0
danielw29,What field of econometrics are you working in? I think it would be good to get some suggestions for packages based on the field. For data manipulation definitely look at tidyverse and data.table. Another open source alternative to eviews would be Gretl (Gnu Regression Econometrics and Timeseries Library?),1516344428.0
too_many_splines,"`stringr::str_extract(strings, paste(state.name, collapse = ""|""))`",1516319211.0
MachupoVirus,"state<-array(NA)

library(stringr) #You may have to install this package first

for(j in 1:length(state.names)){
if(str_detect(strings, state.names[j])){
state[j]<-state.names[j]
}
}

I think this should work based on my understanding of the question, let me know if it doesn't ",1516318229.0
sohaibhasan1,"You need the expand.grid function: 

a=1:12
b=letters[1:6]

df=expand.grid(nums=a, lett=b)
df$desired_col = paste0(df$nums, df$lett)",1516246919.0
tommy_oak,I would suggest to use the mutate() function in the dplyer package. The help page is also very clear.,1516280814.0
mrohdubs,Mutate() and paste() functions should get you where you need to go. ,1516290021.0
pina_koala,"This question is better suited for /r/statistics so you will get a better answer over there. 

Your problem isn't about programming in R, it's about handling small sample sizes.",1516235908.0
samclifford,"There are packages in R that can handle what are called mixed effects models which allow you to account for group level effects (community, in your case). I agree with /u/pina_koala that /r/statistics would be better, or you could try searching online for mixed effect models.",1516236562.0
agclx,"A simple solution would be to use the `weights` parameter of `lm` and adjust the weights.

It's easy to adjust such that the effect each community on the error `sum(w*e^2)` is the same.  However as the other posters note to make it correct it is not so simple.",1516297119.0
efrique,"one way (of many):

`as.numeric(ordered(mydf$mood,levels=c(""bad"",""meh"",""good"",""rad"")))-2`
",1516190309.0
samclifford,You could use dplyr's `case_when` function and put the scheme in that way. Or you could build a table with the text labels in one column and the corresponding numbers in another and use the `inner_join` function to retain both text and numeric labels. ,1516185117.0
Thoxic,"I also support using case_when, although I will suggest recode too.",1516194479.0
Iam_Voldemort_AMA,"library(dplyr)

data2=data%>% mutate_if(is.character, as.numeric)

Maybe this?",1516183797.0
Deleetdk,"Most of base R and the associated packages is very old and compatible with S that's even older. Hence, lack of any real standardization of names. Note also that functions use a mix of camelCase (`base::colMeans`), snake_case (`base::seq_along`) and even dot.case (`base::which.min`).

You can search package names using e.g. `ls(""package:base"") %>% str_subset(""\\."")` which finds all functions in **base** with dots (relies on **stringr**).",1516153953.0
guepier,"While it’s true what /u/Deleetdk is saying about inconsistent naming, there’s a specific reason why these parameter names are in all uppercase:

**It’s to prevent parameter name clashes** (to some extent)^(1). Note that all these functions have a `...` parameter, and accept arbitrary, named arguments. For instance, I could use `lapply` as follows:

   
    lapply(data_sets, lm, formula = y ~ x, x = TRUE)

This computes linear models using the same formula to a list of data sets, and specifies that the `x = TRUE` parameter should be set when calling `lm`. It’s equivalent to calling

    list(lm(data_sets[[1]], formula = y ~ x, x = TRUE), …and so on…)

But this only works because neither `formula` nor `x` are a formal parameter of the `lapply` function. Now look at `lapply`’s first parameter name: luckily, it’s `X`, not `x`. Otherwise the above wouldn’t have worked.

But as noted, R is terribly inconsistent (`sapply` has as `simplify` parameter; almost certainly accidentally: `mapply`’s equivalent parameter is called `SIMPLIFY` in all caps). And even without inconsistency, this measure is far from perfect in avoiding name clashes.

Modern packages use more convoluted parameter names in similar situations to avoid name clashes: dplyr uses parameters that start with a dot (`.`). Other packages use altogether invalid names, e.g. ones starting with an underscore (which is invalid in R).

---

^(1) `switch` is a special case since it’s a primitive function and different parameter rules apply, but it follows the same naming rule; ironically consistently.",1516210210.0
blahblahblahblah8,Arguments is a perfectly good word. I don't personally know of any difference in what the two words imply but perhaps there is one. ,1516207968.0
blahblahblahblah8,"A significant portion of the time, uppercase parameters are used to denote R expressions and functions. However, base R is not consistent in its syntax and doesn't try to be, as an earlier commenter pointed out.",1516199405.0
Darwinmate,"Nice job, but I feel like you're reinventing the wheel. I personally use regexr.com. I don't remember R regex being substantially or any different from ""standard"" regex. ",1516165649.0
too_many_splines,"`sapply` is basically just `lapply` wrapped in `simplify2array`. Your anonymous function returns a numeric vector of length 5 through every iteration through elements of `x`.  `simplify2array` simplifies this list of vectors into a matrix.

check out `lapply(x, function(d) { y * abs(d - x) })` for how your output looks before `sapply` attempts to simplify it with `simplify2array`",1516137159.0
efrique,"Your posted code doesn't run.

It fails at this line:

      loss.1<-sapply(x, function(d) d-x))

(for an obvious reason). While it's easy to fix, it implies this wasn't actually identical to the code you were running, which is kind of the point of posting code - we shouldn't be in the position guessing what you actually did.

A piece of advice (for anyone posting code): when you post code, paste it back into a clean session, and check what you posted actually runs. 

---

in effect what `sapply` is doing with `d-x`  is calculating `d[i] - x` for each `i` and collecting those results up and (because its sapply) putting them together into a matrix. So in the `loss.1` code when you pass it x, you don't get a zero vector, you 
get the equivalent of `outer(x,x,""-"")`

You could see this most easily by passing your function something of a different size than x. Look at:

     sapply(c(1,2), function(d) d-x)",1516139451.0
DangerousPie,"You could try using the country flag unicode characters: https://emojipedia.org/flags/

Not sure if that will work, but worth a shot!",1516135318.0
BluesTime,"Yes, I suggest the ggflags package (non-CRAN). I've used it successfully to accomplish what you've linked. E.g. Plot I've generated: https://imgur.com/a/FzM6H

Do note you'll have to use the countrycode package to convert your country names to 'iso2c' format which is what ggflags uses to identify which countries you want. Here's code to generate the sample chart above:

    data.frame %>%   
      ggplot(aes(country)) +
      geom_bar(aes(y = ..count..)) +
      geom_flag(y = -24, aes(country = code), size = 15) +
      facet_wrap(~ gate, scales = ""free_x"") +
      theme_tufte() +
      theme(
        axis.title.y = element_blank(),
        axis.title.x = element_blank(),
        axis.text.y = element_blank(),
        panel.spacing = unit(1.5, ""lines""),
        plot.margin = unit(c(1, 1, 1.5, 1.2), ""cm""),
        axis.line = element_line(),
        axis.line.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.x = element_text(family = """")
      ) +
      scale_y_continuous(expand = c(0.25, .01), breaks = seq(0,60, 20), limits=c(-20,68)) +
      coord_flip()",1516143845.0
Er4zor,"Well, you could just create the package skeleton (with RStudio it only takes File → New Project → New Directory → New Package), move your scripts to the newly created `R/` subfolder, build the package (Build → Install and Restart) and you're good to go!

The skeleton automatically exports every function found in the sources.

Then, of course, you need to add documentation, fill in the `DESCRIPTION` and so on. But it is a breeze with RStudio. You don't even need to mess with `.Rd` files, you just document functions using `roxygen2` syntax, directly above them (Code → Insert Function Skeleton).

More resources [here](http://r-pkgs.had.co.nz/).
",1516120956.0
RaggedBulleit,Found an answer: looks like web.mit.edu/insong/www/pdf/rpackage_instructions.pdf covers it and my Google Fu was weak.,1516115708.0
adhi-,No because Hadley is a true homie. I have the book but use the site anyways. ,1516122971.0
lakenp,"They are nearly identical. Try out the online version for free and, if you like it, which you'll do, buy the book to incentivize behavior such as Hadley's.",1516124056.0
SmallPrimitive,"It's rather the other way around. The online version is still being updated, so if there are differences the online version will always be up to speed. ",1516127271.0
SecretAgentZeroNine,You can ask him directly on Twitter. He'll respond if he doesn't get pissed off.,1516115488.0
jacktast1c,"I have the book and am like about half way through it. So far the only difference I have noticed are some of the figures are incorrect in the print version. 

However I didn’t always have both side by side and may have missed a thing or two. ",1516116690.0
mr_sideburnz,"I would say that, if there are any updates, check online version ",1516132805.0
2strokes4lyfe,"I’m just now learning geocomputation in R, but it sounds like the rgdal, rgeos, and sp packages may be useful in this situation. Wish I could help more. ",1516125843.0
zdk,"I am imagining converting this to a graph problem where properties are nodes n and edges e exist if the distance between two properties is less than some threshold.

Then your problem reduces to finding all k-node induced subgraphs. This is not an easy problem, but here is software for one approach used in biologocal networks for motor detection. http://theinf1.informatik.uni-jena.de/motifs/",1516155674.0
Tarqon,"Really interesting question. In the ['performance' chapter of Advanced R](http://adv-r.had.co.nz/Performance.html) Hadley talks about how R could be made an order of magnitude faster, but that the capacity of the core language development team is very limited.

I'm really not sure what would be a large enough change to warrant a version bump, that's also likely to be achieved within a few years to be honest.",1516151156.0
efrique,"> partipate in the development process

Isn't that partly what the dev mailing list is for? ",1516069464.0
hairynip,"Thanks /u/hadley 

Being able to see a professional work was nice.  I'm pretty much self-taught for programming & stats and it's nice to see my general workflow for a basic eda like this isn't far off from someone that does this sort of thing for a living.  Though it takes me a bit longer.",1516032566.0
2strokes4lyfe,"I swear it looks like he's typing `mean = nean(f1units)` at the 9:13 mark. Does anyone else see that? What the hell is nean!?

*EDIT:* It's at line 53 and 57 of his code.",1516042671.0
hswerdfe,"Virtually, his entire workflow is in the top left panel, I felt boxed in just watching him. So, much wasted space in the other 3 panels. That being said, it clearly is working for him.",1516074926.0
mrohdubs,What class is he referring to?  I would love to take it if it’s a MooC or something like. ,1516074013.0
2strokes4lyfe,That’s awesome! Fcuk yeah!,1516064162.0
ashwinmalshe,"Awesome find!
",1516025795.0
dastram,Thanks for posting. I am just learning to create nice charts and planned using animation and interactive charts. This is exactly what I needed. (And it isn't plotly),1515974321.0
Thaufas,"Great write-up! I don't frequently animate using R, but when I have, I didn't use any R packages to help with the animation aspects. Rather, I generated all of the intermediate images myself and used ImageMagick to convert the individual images to a single gif. The example you showed makes me want to explore animation more.",1515988573.0
Ryo-N7,"Hi, i'm Ryo-N7. thanks for posting my blog here, i hadn't thought of doing that! if anyone has any questions or comments let me know ! ",1516047174.0
iconoclaus,"Hi, I'm not familiar with PLS regression. However, SmartPLS is for composite path modeling.  Its probably not what you are looking for unless you are dealing with composites/factor models.  You might look into the pls package on R instead:

https://cran.r-project.org/web/packages/pls/vignettes/pls-manual.pdf",1515933272.0
aelendel,Why are you doing PLS?,1515954703.0
biledemon85,"Hi, thanks for this! I was trying to use GNURoot from that other thread but had no joy getting a more recent version of R on it. Dependency issues where certain versions weren't available for Debian ""Jessie"".

I'd just like to point out that it might have been helpful to also let the reader know that they can install the latest R after they've started Arch with ""startarch"" with a simple: 
 
    pacman -S r  

Then they can start loading up with R packages as they need.

Edit: And while I'm here, for the newbies like me, you'll need to use pacman to install gcc and make at a minimum to allow R to compile the packages.
",1516225209.0
Laerphon,"    data$column_Y <- ifelse(data$column_X < 36, 1, 2)

Note if you have missing values in column X, you'll get NA in column Y (which is probably what you want, anyway), but good for a beginner to know.",1515882990.0
Thoxic,You could also look into the 'case_when()' function (I think it's in the dplyr package).,1515883904.0
efrique,"You should post a small reproducible example so people don't have to construct examples to show you. I'll focus on 0/1 rather than 1/2 but it's easy to convert if you need to

So let's make some data:

    mydata <- data.frame(X=c(42L, 30L, 37L, 37L, 33L, 50L, 50L, 26L, 21L, 26L))


Try

    mydata$Y <- mydata$X>35 

so that mydata$Y contains a group indicator (it will be TRUE/FALSE but that will be interpreted as 1/0 any time it's used as if it were a number, so you could add 1 to it to get 1/2).

You can just use that TRUE/FALSE indicator in almost anything you need to show the groups. This is the ""R"" way to do it.

You can also do:

    mydata$Y <- ifelse(mydata$X>35, 1, 0)

(substituting any values you like in place of 1 and 0. You can also do:

    mydata$Y <- cut(mydata$X,breaks=c(-Inf,35.5,Inf),labels=c(""<36"",""36+""))

Sure, that's not actually returning a 0/1 variable but you can use it in places where you need one -- it's a factor so it will work as the group indicator for regression or t-tests or whatever.
",1515890645.0
thedukeofedinblargh,"The problem is that you're trying to give your reader over a thousand pieces of information (several hundred coefficients, plus their significance).  Thus, the big question you need to ask is why are you giving them that information:

* If you're giving them the actual numeric values as part of your due diligence/transparency, then just give them the giant table.  It's a correlation table, people know how to use it, and you've done your duty.    

* If you just want them to understand the big picture, then it's up to you to tell the story.  You could do some sort of heatmap as in your second example, omit the correlation coefficients entirely (i.e., color-only) and just accept that any single cell is going to be tiny.  Then, you use words to describe to your reader why they see large splotches of red where they do (""the strong correlations in the lower left reflect statistically signficant the relationships between XYZ sorts of items both cross sectionally and year-to-year (r's .38 - .62; all p's < .001)"").  

* Do you really need to tell them everything?  If you had the time and skills (and was appropriate for the audience), this might be  a good use for some sort of data reduction(PCA, factor analysis, etc.).  One really simple data reduction here is to ask whether the 16 variables asked one year apart really need to be included.  If there are strong year to year correlations, perhaps you can just omit one time point and note that the pattern of correlations is extremely similar for the other time point.  ",1515903166.0
dm319,The best way I have found is to find the strongest correlations and then plot these as a force directed graph.  Form a correlation matrix and use two thresholds to find the correlations of interest.  Then use igraph to plot it.,1515863683.0
abstrusiosity,"For a compact, organized, view try the `heatmap` command.  I doesn't, by default, show significance, but it give a clear picture of what's related to what. ",1515877244.0
RaggedBulleit,"Try ggpairs. It's customizable, but right of it usually does a decent job",1515884057.0
kordof,"You could try corrplot. I've used it in the past.
https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html",1515902112.0
mbillion,"theres a ton of them

http://www.sthda.com/english/wiki/r-built-in-data-sets",1516319410.0
SeveralBritishPeople,"Easiest would be to make a monitoring script that checks every 10 seconds or whatever and increases priorities of those processes, and have it start as a daemon at startup. That’ll keep working when you update RStudio. 

But more importantly, what do you feel like you’d improve by increasing the process priority?",1515859468.0
infrequentaccismus,"Try table() and prop.table(), like this:

    table(df$Person, df$Reason)
    table(df$Person, df$Reason) %>% prop.table


It appears to be doing the same thing, unless I am missing something?  Table and prop.table have been shown to be blazing fast in tests.

    library(rbenchmark)

    benchmark(table = table(df$Person, df$Reason),
          prop_table = table(df$Person, df$Reason) %>% prop.table,
          table_dplyr = df %>% 
                            select(Person, Reason) %>% 
                            gather(key, value, -Person) %>% 
                            group_by(Person, key, value) %>% 
                            summarise(n=n()) %>% 
                            spread(value, n, fill=0L) %>% 
                            ungroup() %>% 
                            select(-key)
          )

Results-
prop_table: 0.039
table: 0.028
table_dplyr: 2.264 

The dplyr chain you used is 75x slower.  :)",1515818366.0
LittleNonsuchII,"In these scenarios I use eval(parse(text=“code I want to execute”))

Am i a Bad Person?",1515836564.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/statistics] [NBA Analytics:](https://www.reddit.com/r/statistics/comments/7ptjzo/nba_analytics/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1515725187.0
werdiser,"NBA analytics twitter is a really good place to start for seeing examples. Definitely bookmark Nylon Calculus, following their writers is a good start.",1515882944.0
Prof_Chaos22,See first reply here https://stackoverflow.com/questions/23519224/extract-r-square-value-with-r-in-linear-models,1515722486.0
soloaus,You should take a look at the [broom](https://cran.r-project.org/web/packages/broom/vignettes/broom.html) package. It has a lot of handy functions for extracting many model attributes for different types of models in addition to lm.,1516138693.0
too_many_splines,"I really hope I'm not just doing your homework.

The simplest way is to think of the death-rate as Bernouilli random variables. You need to find sufficient evidence that the death-rate of smaller armies is smaller than the death-rate of larger armies. Construct a test statistic which is asymptotically normal under the null hypothesis that the true death-rate parameter of the smaller size army is equal to the death-rate parameter of the larger size army. 

Calculate the the following test statistic:
(p\_(small army) - p\_(large army) ) / (p\_(pooled)(1-p\_(pooled))(n\_(small army) + n\_(large_army))^-1 )

Where p\_(pooled) is the total number of deaths suffered by both armies divided by the total number of soldiers in both armies.

If there is a very low probability that values as small or smaller than your test statistic occur under a standard normal distribution (given some exogenous level of confidence), you can conclude that the death-rate in smaller armies is smaller than the death-rate in larger armies.",1515721729.0
Twentyone21pilots,"Since it can't be fixed by a log change, why not try using a BoxCox test and an adf test to try and detrend the data and then use a normal distribution approach?

By limits, do you mean the min and max?",1515785687.0
_nt2,"The ""line"" in your plot is piece-wise linear - in other words it connects the dots between the points. 

So *technically* the derivative of the line would be undefined at each of your `time` points and would be a flat line equal to:

    diff(score)/diff(time)

between `time` points. Here's a picture (but without the left- and right-most horizontal lines): 

    plot(stepfun(x=time[-c(1,length(time))], y=diff(score)/diff(time)), verticals=FALSE, do.points=FALSE, xlim=c(time[1], time[length(time)]))

I'm guessing this is a technically correct answer that is not at all what you are really looking for.

But it's hard to guess what you actually want. Perhaps you want to get a smoothed estimate of the relationship between `time` and `score` and then find a (possibly numerical) derivative of that estimate? There is no unique way to answer that.

",1515697710.0
efrique,"If your data are observed without error, then you want *numerical differentiation*.

https://en.wikipedia.org/wiki/Numerical_differentiation

In particular, *symmetric differences* would get about as close as you'd hope (a forward difference or a backward difference will each give you a derivative estimate that's shifted in time -- and which has a higher order of error in it). That is, if you had equispaced data, you'd use

dy(t)/dt ≈ (y[t+1]-y[t-1]) / (2h)

where *h* would be the time gap between observations, if it were constant.

Apart from the first two observations, the gap between consecutive points in your data is consistently just over 1. That should work fine, but your inter-observation gap (""*h*"") is not close enough to constant, so you will want a divided difference.

https://en.wikipedia.org/wiki/Divided_differences

So doing ""symmetric""\* divided differences, we get:

    dy <- diff(score,lag=2)
    dt <- diff(time,lag=2)
    plot(dy/dt~time[2:(length(time)-1)])
 
\*(They're not quite symmetric now, but aside from the first one, are close enough for present purposes)


I'd be inclined to drop the first dy/dt there, because of the weird first gap, but it in this case it's similar enough to the other numerical derivatives that probably won't make enough difference to matter much. Aside from that one, there's a small residual amount of off-centeredness in each symmetric difference due to the varying time-gap but it will be small in impact relative to the noise in the scores.

If we look at the resulting plot, we can see that there's a lot of noise in the derivative -- if you expect that to look smooth, you should probably be smoothing the original series and then taking a numerical derivative of a smoothed fit. However, you can probably do more or less okay by smoothing that numerical estimate of the derivative -- but don't use a form of smoothing that shifts your derivative-approximation forward or back in time.

If your recorded *times* have noise, it's a bit more complicated.
",1515713606.0
ReimannOne,"The tidyverse answer is the easiest to read and write, the subset answer is a good-to-know answer from base packages, and this is just plain bracketted subsetting:

    df_over_25 <- df[df$age > 25,]

    # Or men over 30 & women over 28:
    df_m30_w28 <- df[df$sex == M & df$age > 30 | df$sex == F & df$age > 28,]

That should work.  If it doesn't somebody will come along and correct me.",1515676555.0
Thoxic,"Like Absjalon implied, look into the filter function in the dplyr package. You can provide multiple combined arguments.
You should also search for the 'data wrangling cheat sheet' online. It has a lot of interesting data manipulation pointers.",1515674292.0
jbuckets89,"df<-subset(df,df$sex==""male"" & df$age >= 21)
df<-subset(df, df$sex==""female"" & df$age >= 18)
",1515673980.0
Absjalon,"library (tidyverse)

dplyr:: filter (dataframe, age > x )
",1515673786.0
efrique,"If your matrix is in `X`, and the ones labelled 0-9 are all the columns and in that order `X[,c(10:2,1:10)]` should do it. You should not have the same column names twice though.

Let's set up some data:

    set.seed(83278)
    x <- round(matrix(rnorm(50),nr=5),1)
    colnames(x) <- as.character(0:9)
    x
            0    1    2    3    4    5    6    7    8    9
    [1,] -0.9  1.1  2.5  0.6 -0.7  2.0 -0.8  0.4 -0.4  1.1
    [2,]  0.4 -0.5  1.6 -0.5  0.1  2.8 -0.8 -1.0 -0.2  0.1
    [3,]  0.2  1.4 -0.9 -1.0 -0.2 -1.2  1.6  0.9 -1.2 -0.4
    [4,]  0.5  1.5 -2.3 -1.5 -0.5 -0.7  0.1 -1.0  0.1 -0.3
    [5,] -0.8  0.4 -1.3 -3.0  1.2 -0.6  0.6  0.2  1.5 -0.2

Now let's see what my suggestion does:

    x[,c(10:2,1:10)]
            9    8    7    6    5    4    3    2    1    0    1    2    3    4    5
    [1,]  1.1 -0.4  0.4 -0.8  2.0 -0.7  0.6  2.5  1.1 -0.9  1.1  2.5  0.6 -0.7  2.0
    [2,]  0.1 -0.2 -1.0 -0.8  2.8  0.1 -0.5  1.6 -0.5  0.4 -0.5  1.6 -0.5  0.1  2.8
    [3,] -0.4 -1.2  0.9  1.6 -1.2 -0.2 -1.0 -0.9  1.4  0.2  1.4 -0.9 -1.0 -0.2 -1.2
    [4,] -0.3  0.1 -1.0  0.1 -0.7 -0.5 -1.5 -2.3  1.5  0.5  1.5 -2.3 -1.5 -0.5 -0.7
    [5,] -0.2  1.5  0.2  0.6 -0.6  1.2 -3.0 -1.3  0.4 -0.8  0.4 -1.3 -3.0  1.2 -0.6
            6    7    8    9
    [1,] -0.8  0.4 -0.4  1.1
    [2,] -0.8 -1.0 -0.2  0.1
    [3,]  1.6  0.9 -1.2 -0.4
    [4,]  0.1 -1.0  0.1 -0.3
    [5,]  0.6  0.2  1.5 -0.2
    
",1515622406.0
DrPineapple21,"`lme4` does not currently support p-values by default anymore. You can see alternatives for obtaining them in the `pvalues` section of the `lme4` [documentation](https://cran.r-project.org/web/packages/lme4/lme4.pdf). Alternatively, and my personal recommendation, is to use `lmer` from the `lmerTest` package, which provides estimated p-values by default.",1515618065.0
trilober,"Seconding all the other points -- [here](https://mindingthebrain.blogspot.com/2014/02/three-ways-to-get-parameter-specific-p.html) is a good explanation of why p-values are non-trival to compute for mixed effect models.

Also check out the [sjPlot](https://strengejacke.wordpress.com/sjplot-r-package/) package, with the function sjt.lmer() which can estimate p-values.

Editing to add the the treatise from one of the authors of lmer [here](https://stat.ethz.ch/pipermail/r-help/2006-May/094765.html).",1515801045.0
,[deleted],1515619084.0
Digging_For_Ostrich,"I enjoyed this as someone who has no idea about the rules of baccarat, thanks Paul!",1515604642.0
MrLegilimens,"This is well done! I think a fun, if not useless graph, would be to show how the players are doing over time. I know it’s randomly generated but it would be interesting to see those peaks and then the crashes as the strategies fail, to end with being $X lower than what you started. 

Great write up.",1515675589.0
lakenp,For those primarily interested in the code: https://github.com/paulvanderlaken/2018_casino/blob/master/PuntoBanco.R,1515617015.0
zonination,"Are you thinking of ...?

    library(GGally)
    ggpairs(data)

If the thing is slow, you can always try (in base R)

    pairs(data, upper.panel=panel.cor, diag.panel=panel.hist)",1515603638.0
master_innovator,"Psych package, corr.test(“data frame”)",1515604493.0
itsmeFrick,"I'm a fan of the corrplot and corrgram packages

http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram

https://www.statmethods.net/advgraphs/correlograms.html",1515607136.0
caleblareau,"Assuming that you want the p-value for the correlation (which you can get from the standard `cor` function when applied to a matrix), you can compute the p-value using this function:

    #Compute p-value from correlation given a sample size
    rhoToP <- function(rho, n = 28){
      t <- abs(rho / sqrt((1-rho)^2/(n-2)))
      2*pt(t, n-1, lower=FALSE)
    }


See [bottom of this post for the form of the statistic](https://stats.stackexchange.com/questions/153937/finding-p-value-in-pearson-correlation-in-r)",1515610204.0
scrample2401,"try rcorr() from the hmisc package, or chart.correlation from performanceAnalytics

see https://www.r-bloggers.com/more-on-exploring-correlations-in-r/",1515606406.0
ReimannOne,"It probably could be, but I would recommend `case_when()` and `mutate()`.

    
    library(dplyr)
    # Toy data
    df <- data.frame(Subject = 1:10, 
        reading = sample(60:100, 10, replace = T), 
        attention = sample(20:60, 10, replace = T))
    
    df %>% mutate(group = case_when(
            reading > 80 & attention > 40 ~ 1,
            reading > 80 & attention < 40 ~ 2, 
            reading < 80 & attention > 40 ~ 3,
            reading < 80 & attention < 40 ~ 4))

    
[case_when](https://www.rdocumentation.org/packages/dplyr/versions/0.7.3/topics/case_when) help page.",1515601405.0
itsmeFrick,"
df$group<-ifelse(df$reading >80 & df$attention >40 , 1,
 
                  ifelse(df$reading >80 & df$attention <40, 2,

                    ifelse(df$reading <80 & df$attention >40, 3,

                       ifelse(df$reading <80 & df$attention <40,4 , 999))))
                                    ",1515607892.0
balanaicker,I am not sure but check out [cut\(\)](https://www.rdocumentation.org/packages/base/versions/3.4.3/topics/cut) function as well. You might be able to cut two columns and concatenate the results.,1515624710.0
Atheriel,"This behaviour makes sense to me, personally.  Factor levels represent metadata, so it may well be important to know that there are no data in a particular bin. Silently dropping factor levels would be dropping relevant metadata.",1515563262.0
natched,"Dropping unused levels requires recoding some, possibly all, the values for the factor. From a computational standpoint, this could be expensive, and it's better to make that time usage optional than automatic.",1515533077.0
shujaa-g,"This is mostly speculation, but back in the day when `factor`s were created memory was tight. It's much more efficient to store a short key (level number and level names) and a long vector of integers (the levels). This is why factors are the way they are, and part of the reason `stringsAsFactors = TRUE` is the unfortunate default.

Now let's talk about subsetting a data frame. This is a super-common operation and pretty simple - there's some index that tells us what rows to keep, and you get rid of the rest. Doesn't matter what you're subsetting, that's the process. Because it's such a common operation, it should be efficient.

What you're proposing is taking the optimized subset operation and add to it the following steps:

- check if any column are factors
- if so, go check all the levels to make sure they each have at least one occurrence
- if any are missing, recode the entire vector omitting the missing levels

And also remember that R came out of S, which was created in 1976. The original `factor` code was probably written in the early 80's, if not before.

---

Another reason, is that due to the way they are stored internally, adding new levels to a factor is a pain. Try `c(factor(c('a', 'b')), 'c')`, or even worse `c(factor(c('a', 'b')), factor('c'))`. The famous ""split apply combine"" paradigm for data processing and analysis relies on splitting apart data, doing something to each piece, and recombining it. If unused factor levels were dropped automatically, you're adding a lot of overhead to both the ""split"" and the ""combine"" steps to make sure they're treated correctly. If dropping levels was automatic, I think you'd have a lot more questions about ""where did my factor levels go!"" than you have questions about how to drop unused levels.

I also don't have a lot of sympathy for all the ""posts and questions online regarding how to drop unused levels"". Maybe I'm getting old, but just a *little* reading the manual gets you there, the third line of *Examples* at `?factor` shows how to do this:

    (f. <- factor(ff))  # drops the levels that do not occur

And for several years now the `droplevels()` function is available too.",1515536095.0
efrique,"I think retaining the levels *should* be the default behavior -- knowing all the possible values (including them showing in 0 in a tabulate) is usually what I want -- and it's very easy to deal with dropping unused levels in several different ways (e.g. by calling `factor` on it again).
",1515580910.0
guepier,"> One of the annoying things about writing R functions is that […] the arguments are immutable.

Uhm, no. Other functional programming languages do just fine with immutable arguments, and even non-functional languages have a strong trend towards encouraging immutability. *Immutability is an advantage*, not a disadvantage. Don’t circumvent it.

Nothing against using R6 here *per se* but you can absolutely write a clean implementation without it. Here’s an example usage that doesn’t use R6, is fully immutable, and yet, I argue, cleaner than the R6 solution proposed in the article:

    train = training(iterations = 999, trace = 1E3, learn_rate = 1E-4)
    model = neural_net(Species ~ ., iris, hidden_layers = 5, train)
    results = predict(model, newdata)

In fact, having a separate `train` method (as in the article) makes no sense since an untrained neural network isn’t useful. One of the rules of good software engineering is that objects should not exist in an uninitialised state because they might accidentally be used incorrectly: what happens if I call `predict` on an untrained neural net from the article? Nothing good.

Furthermore, the above code conforms to the existing `stats::predict` interface in R rather than reinventing its own incompatible API. It can thus be plugged straight into existing analysis code.",1515587467.0
yordanivanov92,Love it!,1515573106.0
Darwinmate,"This is a data cleaning issue. Why arent you fixing the incorrectly entered data before merging? 

I think regex should be able to identify any observations that arent in a specific date format. 

Unless your goal is to merge and then clean...",1515480868.0
tenurestudent,"You'll have to test, verify each file before the merge. Some can be automated, if I know the file structure is consistent, is.numeric(df$v1) can be used. If your clients are consistent with how their inconsistencies, you can look for those patterns and reformat using the date functions. I've used XLConnect, mainly so I can format my output as well. ",1515496735.0
Thoxic,"If you are using read_excel (from the readxl package), you could set column types (col_types = ...) manually. ",1515522017.0
samclifford,You could add an intermediate step to your processing that runs `readr::parse_*` functions on the columns to convert them to the correct type.,1515470907.0
greybey,"You could try the openxlsx package. Its ""read.xlsx()"" function has a ""detectDates = TRUE"" argument that might work for this scenario.",1515470728.0
catdci,"Couldn't find anything for lagsarlm objects, but created a handwritten function. The model used is copy and pasted directly from [here](https://www.rdocumentation.org/packages/spdep/versions/0.7-4/topics/lagsarlm).

    library(spdep)
    data(oldcol)  # load data
    COL.lag.eig <- lagsarlm(CRIME ~ INC + HOVAL, data = COL.OLD, nb2listw(COL.nb, style=""W""), method=""eigen"", quiet=FALSE)  # make model
    summary(COL.lag.eig)

    library(ggplot2)
    mod.dwplot <- function(model, intercept = FALSE, alpha = 0.05) {
        if(intercept == FALSE) {
            vars <- colnames(model$X)[-1]  # get coefficient names
            coefs <- unname(model$coefficients)[-1]  # obtain coefficients
            conf.ints <- confint(model)[-c(1, 2),]  # calculate confidence intervals, #  except for rho and (Intercept)
            # make df
            plot_df <- data.frame(vars, coefs, ci.lb = conf.ints[, 1], ci.ub = conf.ints[, 2])
        } else {
            vars <- colnames(model$X)  # get coefficient names
            coefs <- unname(model$coefficients)  # obtain coefficients
            conf.ints <- confint(model)[-1,]  # calculate CIs except for rho

            # make df
            plot_df <- data.frame(vars, coefs, ci.lb = conf.ints[, 1], ci.ub = conf.ints[, 2])
        }

        # make plot
        ggplot(plot_df, aes(y = vars)) +
        geom_point(aes(x = coefs)) +  # place dot at value of coefficient
        geom_segment(aes(x = ci.lb, xend = ci.ub, y = vars, yend = vars)) + # draw segment to span the CI for each coefficient
        labs(x = NULL, y = NULL, 
                title = paste(""BW Plot of Coefficients and "", 100 * (1 - alpha), ""% CIs"", sep = """"))
    }
    mod.dwplot(COL.lag.eig) # makes plot w/o intercept, w/ 95% CIs by default
    mod.dwplot(COL.lag.eig, intercept = TRUE, alpha = 0.05)  # include the int., make 90% CIs

Let me know if you need changes or clarification.",1515451783.0
standard_error,"Nice comparison, thank you! Both RDS and fst have optional compression (on by default in RDS; off by default in fst). It would be nice to see both with and without compression. 

I'd also love to see some benchmarks with larger file sizes (say 10 times as many rows) - my impression is that both fread and fst really show their edge in such use cases.

Finally, you might want to specify what type of hard drive you are using for the tests. I have noticed that the difference between SSD and mechanical drives can be large, and in fact sometimes affects the choice between compressed and uncompressed files for speed (compression increases processor time, but decreases write time, which will be more important on a mechanical drive).",1515438232.0
_nt2,I'm pretty good with most things `R` but I've never even considered the idea of storing raw data in a more efficient format. So thank you very much for showing me the possibilities. This will help me every day.,1515438400.0
Hoelk,"Interesting statistic, I was surprised that feather is so much faster than `.rds`. It would have been interesting if you would have also compared the other compression algorithms available for `saveRDS()` via the `compress` parameter. Filesize and save/load time can vary quite noticeably between those.",1515429710.0
CadeOCarimbo,"If you want some feedback, you should have shown the plot outputs of microbenchmark() ",1515436433.0
redouad,"Thanks for the feedback! I've added a few options for compress = FALSE in saveRDS, as well as compress = 0 and compress = 100 in write_fst.

Overall write_fst and read_fst seem to offer the greatest flexibility for binary formats, with a custom compression value anywhere between 0 and 100, but very fast loading times whether compressed or not.",1515581418.0
grasshoppermouse,"Not quite sure what you're asking, but if you like the theme, cowplot has `theme_cowplot`. 

As an aside, check out `patchwork` for a different way to combine several ggplots into a single plot:

https://github.com/thomasp85/patchwork",1515592684.0
questionquality,"Depending on your project and your model, you might be able to use some of the modelling functions in R to construct a design matrix and a data matrix and pass those on to stan. But then again, it might be even easier to just use [brms](https://github.com/paul-buerkner/brms).  
  
Edit: If you really want to learn to do it in stancode yourself, you could still have brms build a simple model with factors and look at the stancode it generates.",1515420071.0
Darwinmate,"First of all, can you post a link to the shanghai smog index? Or give some data to recreate the error?

Have you googled the  error message? Sounds like the error is indicating that you have duplicates. ",1515374865.0
mlcyo,Check for duplicates using which(duplicated(X)) where X is your zoo object. It'll return an index of where duplicates are (if any). Hope this helps!,1515401051.0
geocompR,"Try using read.file:

    f <- read.file(""/path/to/file.txt"", header=FALSE)

The strsplit:

    parsed <- strsplit(f, "","")

And then construct the matrix in whatever fashion you need in an lapply:

    matrix_list <- lapply(parsed, function(x){
        x <- as.numeric(strsplit(as.character(x),"""")[[1]]) #breaks apart the numbers into a numeric vector with single-digit values
        matrix(x, ncol=20, nrow=20, byrow=TRUE)
    })

Now you have a list with all of your matrices. You could also skip the matrix() call in the lapply and have it return ""as.numeric(strsplit(x,"""")[[1]])"" to return a numeric vector for each series of matrix data.",1515371044.0
_nt2,"/u/geocompR gives an answer (which requires the `psych` package) which assumes your file consists of one very, very long row with values separated by `, `, which, to be fair, is what we get when we cut and paste your example text above.

If your file is actually 1000 rows with a comma and space at the end of each row, then you can treat it as a fixed width file format an read the file using `read.fwf`, which takes care of all the splitting for you. Then you have a data.frame which you have to deal with. The following works:

    f <- read.fwf(fname, widths = rep(1, 20*20))
    m <- lapply(1:nrow(f), function(i) matrix(unlist(f[i,]), 20, 20, byrow=TRUE))

",1515425421.0
too_many_splines,"Pre-allocate your memory before entering the loop. This can have dramatic performance increases.

Replace line 7 of your gist with `datalist <-  vector(""list"", 3000)`

If you are still unsure over the speed, just write a print statement in the loop to track it's progress. That should give you a bit more insight into what's happening. Is the average scrape time per page increasing? Is it a few atypical pages which are taking a while to get through? These are some things you could easily answer with simple logging. ",1515361902.0
ossicones,"You may have better luck using RSelenium. That allows you to control an actual browser window from your code, rather than “flying blind” so to speak, as you do in rvest. ",1515378930.0
modestwitness,"Off-topic comment: Consider saving each page in your computer instead of downloading again and again. You will save your own time and the bandwidth of the site you are scraping.

I've done a bunch of scraping in R. Typically I download and save the page and do scraping afterwards outside a loop. Speeds up everything (e.g. parallel processing instead of loops when extracting data), minimizes bandwidth usage and makes everything reproducible.",1515922591.0
MachupoVirus,"When you use lmer(), you will notice an argument called na.action, by default, it uses na.omit. This means that any observations with incomplete data are disregarded in the model. I would assume that your original model was using partial observations since there was no NA's included yet, but with the default argument for na.action in lmer, once you had included NA's, these observations were removed. ",1515359193.0
truncatedusern,"It doesn't look like this post ever got any love, but I wanted to let you know that I appreciate your work (as both a former GraphTV user and as an R fan).  I will continue using this if you keep it up.",1529276949.0
itsmeFrick,"df$seasons <- revalue(df$month, seasons)

seems to have worked",1515280298.0
MachupoVirus,"I would use recode in the ""car"" package for this",1515365048.0
tuturuatu,Surprised Visual Basic is that high.,1515298135.0
AllezCannes,Forgot to add from January 2017 to January 2018.,1515268516.0
_Wintermute,Bear in mind TIOBE ranking is absolute nonsense.,1515344863.0
vaguely_specific1,"What happened to cause the jump?  I know there were improvements related to deep learning/ neural networks, R studio has been improved, and I think tidyverse got some upgrades, but 8 places?  Did several language get mass abandoned or something?",1515299545.0
shujaa-g,"Sure. `paste0` combines to things with no spaces. If you have names that aren't unique, the easy way to make them unique is to paste 1, 2, 3, ... on them.

    names(your_data) = paste0(names(your_data), 1:ncol(your_data))",1515244834.0
efrique,">  was wondering if you can have multiple interaction terms in a linear regression?

Yes.

>  if all 4 of the X variables are continuous in the above model with 2 interactions, is that OK?

Yes. They're not always as easy to interpret, but you can validly have continuous-continuous interactions in a statistical model, and you can certainly fit them in R.

> if you wanted to include a 3-way interaction between 3 independent continuous variables in a linear regression, say X1 \* X2 \* X3,

Strictly speaking, in R's formula notation, the interaction term itself is X1:X2:X3, while the expression X1\*X2\*X3 stands for the model terms X1 + X2 + X2 + X1:X2 + X1:X3 + X2:X3 + X1:X2:X3

See also `?formula` and also section 11.1 (page numbered 51) of the R-intro manual (in R, you can easily find it by going to the menu bar at the top and choosing  `Help -> Manuals (in pdf) -> An introduction to R`)",1515205427.0
MrLegilimens,"Adding things into the model does not “in theory make things more accurate”. That’s why we should only, i don’t even want to say trust, acknowledge our adjusted r^2s. If they don’t make theoretical sense to include in the model, then you’re just p hunting and hacking.",1515236055.0
brianverk,"The impact of interactions depend on whether or not it helps better explain the response. The code would be 
y ~ x1*x2 + x3*x4
y ~ x1*x2*x3 + x4

Do know that multiple interactions could lead to overfitting and this only make it seem as if the interaction model is better (especially with outliers). Make sure you conduct a thorough exploratory analysis and do some visualizations of the fitted values as well. 

Edit looks like the asterisk is cut out in mobile reddit but there should be one in between all of the X’s ",1515199126.0
navidshrimpo,"Try every combination. To avoid overfitting, use cross validation. You can then find the best subset.",1515239204.0
dtrillaa,I knew the basic pipe %>% but I had no clue about the others. Thank you ,1515215402.0
zreeon,Update your version of R and try again,1515181232.0
,[deleted],1515181819.0
Trauma,"Not sure about immediately, but you could write a script to periodically ping a page, compare the return to what it was before?",1515175236.0
13ass13ass,"I heard about a project that tried to use machine learning to predict when to ping a page to look for new data. It seemed pretty cool. The idea is to start pinging at some arbitrary rate and then estimate how often the data is updated, then use that estimate to make a new ping rate.",1515192894.0
ING_Chile,I don't know but I'm interested :),1515174786.0
biledemon85,"Unless the website does some sort of push notification when they update the page, you're going to have to do some periodic polling of the page.",1515264956.0
balanaicker,"R for Data Science by Garrett Grolemund and Hadley Wickham

http://r4ds.had.co.nz/",1515190905.0
spectrum_specter,Maybe try this: https://www.stat.ubc.ca/~jenny/STAT545A/block05_getNumbersOut.html,1515165807.0
fivearmoctopus,Datacamp.com does some Basic courses with build in practise tool which helped me quite a bit :),1515171855.0
guepier,"For your specific problem, there’s the function [`readRDS`](https://www.rdocumentation.org/packages/base/versions/3.4.3/topics/readRDS) to read data from an RDS file in R:

    data = readRDS(""yourfile.rds"")

Whoever gave you the file should probably have informed you how to access it. After that, I second /u/fivearmoctopus’ advice: the DataCamp courses are excellent for getting a quick start in R. I can particularly recommend [David Robinson’s introductory course](https://www.datacamp.com/courses/introduction-to-the-tidyverse).",1515172759.0
gordo_c_123,"Try Swirl, I just started it the other day and it's awesome. It provides step-by-step instructions for different topics inside R studio. It's free and all courses are open source. You'll get Swirl and 3rd party built courses. Check it out: http://swirlstats.com/
",1515467263.0
dastram,"If you can handle some other program like excel or spss, you could send it to me and I convert it to a file-format you can use.

else the other link posted could help you importing it.",1515166359.0
agclx,"Can you be a bit more specific what‘s in the file and what you intend to do with it?
",1515217935.0
Gullinkambi,"Under the first graph you'll see ""By default, the whiskers span the 95% confidence interval."" A confidence interval is a measurement of uncertainty of the sampling method. It is probably worth reading up more on confidence intervals as they are frequently misinterpreted, so rather than an ELI5 I'll drop these links:

http://stattrek.com/estimation/confidence-interval.aspx

https://en.wikipedia.org/wiki/Confidence_interval",1515096365.0
grasshoppermouse,"The dot is the value of the regression coefficient for that variable. The whiskers are the 95% confidence intervals for that value.

These plots are especially useful if:

(1) you have standardized your predictor variables, so that the coefficients are standardized betas. You can then compare the coefficients and 95% CI's of the variables in the model.

(2) you are comparing different regression models that each have the same outcome (independent) variable, and share some predictor variables in common. You can then see how the coefficients shift when including or excluding particular variables.

For example, the first plot in that link is for this model:

    m1 <- lm(mpg ~ wt + cyl + disp + gear, data = mtcars)
    summary(m1)

    Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
    (Intercept) 43.539847   4.860059   8.959 1.42e-09 ***
    wt          -3.792867   1.081819  -3.506  0.00161 ** 
    cyl         -1.784296   0.613889  -2.907  0.00722 ** 
    disp         0.006944   0.012007   0.578  0.56782    
    gear        -0.490445   0.790285  -0.621  0.54007    

Notice that the Estimates of each variable correspond to each dot.",1515100253.0
navidshrimpo,"There are two concepts here that both need to be understood first, and then they can be understood together:

 * Confidence Intervals

 * Coefficient Estimates

It sounds like you already have a rough understanding of what a confidence interval is, but if not, Google is rich with help here. Important to this context though is that a confidence interval is a range of values around an *estimate*. Most simple examples of confidence intervals are around sample means, sample proportions, etc. It's an estimate because it's a sample, and not the full population that you were measuring. In other words, you measure something and want to know how frequently you would get similar results if you were to repeatedly resample.

What if instead of measuring a summary statistic like a sample mean, you were training a regression model on a sample? A regression model estimates what the model coefficient values should be for your sample. Again, this is an estimate. The exact concept applies--how confident are you in your estimate?

To interpret this, if you have a positive coefficient estimate for a model parameter, then you assume that increases your response variable however it makes sense, logistic, linear, whatever. However if your confidence interval for that coefficient estimate is really large, and it dips into negative, then maybe if you were to take some more samples, or your sample size were to increase, you'd learn that the effect of that parameter is actually negative.

Hopefully this helps you understand how these intervals can be used to better understand your model.
",1515176801.0
guepier,"Nice share. That said, there are lots of more versatile/easier solutions  for integrating the R console and Vim:

* [Nvim-R](https://github.com/jalvesaq/Nvim-R) is an integrated development environment for R in Vim. It essentially does everything.

* [Iron](https://github.com/BurningEther/iron.nvim) is a more general and lightweight plugin for sending text between Vim and another terminal.

For more discussion, visit this comment thread: https://www.reddit.com/r/rstats/comments/7j1dtf/what_is_a_good_ide_other_than_rstudio/dr2x9wc/",1515098102.0
a_statistician,"Try \vspace{3mm} (you may have to change where this falls in relation to the break). [Here](https://www.sharelatex.com/learn/Line_breaks_and_blank_spaces#!#Vertical_blank_spaces) is some reading on line breaks and spacing in latex. 

Given that you're using text on top of a picture, you may want to try some other options that would be more reliable, like editing the picture within R. If it's an R plot, that's probably fairly easy - all of the plotting packages that I know of have ways to add text overlays. If it's an image of some sort, you could try the [`magick` package](https://cran.r-project.org/web/packages/magick/vignettes/intro.html), which will also allow you to edit pictures using R commands. ",1515097366.0
Laerphon,"Assuming you're in a standard text environment and mode (you're really overlaying text over an image), you can do manual vertical spacing using \vspace{x} where x is the height of the break (e.g. \vspace{20 mm} for a 20mm break). If that works in the particular case you're concerned with, you will want to google around about more complex uses of \vpsace and \hspace.",1515097183.0
VisuelleData,"Personally, I would recommend just putting text directly on the plot while making it. There are many different ways of annotating plots.
 
 
 
 
If you're using ggplot, then you can use the annotate() function.  
 
 
 
If you're using anything else, you can draw_text() from cowplot. With this command you can easily draw anywhere on your plot and not have to worry about clipping. ",1515147610.0
Babahoyo,"In general, no. This is not how latex works really. To be honest I would suggest editing the picture in MS paint or something and then including it using \includegraphics",1515094894.0
samclifford,It'll be tricky. Did they base their answer off how many hours a week they work and then divide by the number of days they work or did they think of a typical work day and just report that? You may not know! Do you know how many days a week they work? ,1515078006.0
ReimannOne,What does the data look like and what rows and columns do you have?,1515079017.0
byronhout,"I primarily use R as well, but I'm currently reading through Clean Code and I am learning a lot. Like many R users, I don't have much formal training in programming so I'm finding it very useful to learn how to think like a programmer. I'd definitely still recommend the book. ",1515042650.0
whatifurwrong,"In addition to all the advice given here, I would say:

1. Pick a style guide and follow it. If you don't like that style guide, follow a different one. But follow a certain style guide. Try to understand why those rules are being set and when you can break them.
2. Break your code into functions so that each function does one thing and one thing only.
3. Write test cases for your functions.

These have been most helpful for me to develop human-readable code. Also, if possible, get someone to review your code as you develop. Don't dump the whole thing but try to get incremental reviews done by someone else. Questions from the reviewer will help you to understand where you code might be lacking (for the reviewer to understand better). It is important to do this in small increments and not in big chunks.",1515053428.0
,"You might enjoy a classic: ""The pragmatic programmer"".",1515059600.0
guepier,"Unfortunately most of the other answers implicitly confuse *style* and *structure*. Style is just a superficial facet of clean code. Much more important is the underlying structure, and no style checker helps here, and few style guides touch on it. Unfortunately I think these answers are misleading because they imply that clean code is only skin-deep. And that’s a big mistake.

There are a few canonical resources when it comes to writing clean code. Bob Martin’s *Clean Code* is indeed one of them, although your friend is right about the audience: *Clean Code* is mostly about agile, test-driven development in OOP languages (and assumes a fair level of understanding in languages such as Java). Likewise, Steve McConnell’s *Code Complete* is heavily geared towards OOP and procedural programming, for people with knowledge in such languages.

The best (or at least most famous) resource for learning how to write clean code in functional languages is probably still Abelson & Sussman’s [*Structure and Interpretation of Computer Programs* (SICP)](https://mitpress.mit.edu/sicp/). It’s generally recommend reading but be warned, it’s long and although it *will* make you a better programmer in any language, there are no direct references to R.

For R specifically, amongst the best resources are probably Hadley Wickham’s [*Advanced R*](http://adv-r.had.co.nz/) and [*R for data science*](http://r4ds.had.co.nz/) (with Garrett Grolemund). But neither of these specifically focuses on clean code.",1515068716.0
pagan_sinus,"I really liked the first 4 chapters (~60 pages) of clean code by Robert c Martin. The book is for java, but the first 4 chapters apply to all my code. 

All these chapters really cover is using better names and using more functions (instead of more comments). The examples really drove things home for me though, and the lessons learned have stuck with me and helped me improve my code in all languages I use (R, python, bash). ",1515073332.0
AllezCannes,"Not really answering your question, but the formatR package will clean up your code to the style preferred by the R Studio people.",1515045198.0
,"There's a new package called ""styler"" that has just been released as well",1515046342.0
Elesday,"Not a book, but this may be a good start : [https://google.github.io/styleguide/Rguide.xml](https://google.github.io/styleguide/Rguide.xml)",1515032089.0
solostman,I found this very helpful: http://style.tidyverse.org,1515044334.0
VincentStaples,Is there any real need to go beyond a phi coefficient corrplot (or 2 if you want to separate out P and S)? Are you looking for a dyad ranking or just looking at the associations?,1515018081.0
The_Sodomeister,"Cluster analysis works as long as you can construct a distance or similarity matrix. There is no inherent need for numeric data.

For example, construct a similarity matrix according to how often pairs of colors occur together. Bam, you're set for spectral clustering.

You can go even farther by scoring them according to similarity-of-ranking, e.g. assign 3 points to choices 1 & 2, but only 2 points to choices 1 & 3.",1515018848.0
The_Sodomeister,"In slight opposition to the other poster, coefficient significance is not the end-of-story factor in deciding what variables are important. This is *especially* true for smaller effect sizes, where you may not statistically reject the null due to small sample sizes or magnitudes. Utilize domain knowledge: if a variable makes sense as a predictor, don't exclude it without good reason.

If you have a large amount of data, comparing train/test data is probably a better way of choosing a model than relying strictly on coefficient significance.",1515018676.0
chocolateandcoffee,"As long as you look at the ~~descriptive~~ validation statistics and verify that the two variable model is better in most ways, I'd say yes. A simple model almost always performs better, especially because you won't tend to overfit.",1515017421.0
ReimannOne,"You probably didn't tell your function to print anything.  That's usually not a problem.  

Try running your function like this:

    test_object <- myfunction()
    test_object

myfunction() will do whatever it does, and put the results into an object you've named test_object.  Take a look at test object by typing test_object at the console prompt to see what you've got.

Alternatively, if you just want to see what myfunction() returns every time you run it, you'll have to tell it to return something at the end of the function.

    myfunction <- function(x){
      a <- rnorm(x, mean = 37)
      return(a)
    } ",1514994811.0
efrique,"The R-way to do it is to do calculations and then show them is usually pass back an object (a vector, a data frame, a list) and display the object after you call it. 

    result <- my.calculation(inputdata)
    result

or

   summary(result)

(if you write a specific `summary.resulttype` function. These would often use things like `print` or `cat`)

Sometimes - especially with little functions that do some kind of one off thing - you want the function that did the calculation to print the result itself. That's easy enough. e.g. 

     mysummary <- function(x) {
       c(mean=mean(x),sd=sd(x))
     }

This will calculate and print the mean and sd of its argument; in this case I build a vector whose elements are named and use the default print function of a vector to actually show it.

Try it:

    a <- c(1,3,6)
    mysummary(a)

        mean       sd 
    3.333333 2.516611 

",1515024545.0
sellorm,"Not a programming guide, but a concise guide to the R landscape as a whole. There's a post about the motivation and so on here - http://blog.sellorm.com/2018/01/01/field-guide-to-the-r-ecosystem/",1514894969.0
patriotto,"> The CRAN website currently contains more than 12,000 packages for R. 

wow! thanks for writing this ",1514898952.0
eric_he,"Great write up, thanks!",1514762201.0
k3ithk,I can only select from 4 teams? And why is there a photo of the app just above the actual app itself? ,1514752218.0
Gorons,"What are you specifying for the dbConnect host parameter? IIRC localhost won't work, you have to use ""127.0.0.1"".",1514743737.0
hdgdtegdb,"I don't use Windows Linux Subsystem, but I do use a combination of Linux and Windows servers, so probably a similar situation.

If you are just reading data from MySQL into R, RODBC or RJDBC works pretty much as fast as RMySQL.  For compatibility, I now only use RODBC.  You'll need the drivers for Windows and/or Linux, from here: https://dev.mysql.com/downloads/connector/odbc/

You'll just need to know the IP/Host and Port of the machine/virtual machine where the MySQL server is hosted.

If you are *writing* data from R to MySQL, RMySQL is an order or magnitude faster than ODBC, because it first dumps the data to CSV and then bulk inserts into a table (at least in Windows; might use sockets in Linux).  

If you just need to read data, I suggest switching to standard RODBC.  If you need to write, perhaps somebody else can advise if there is a faster way to insert using RODBC, as it's not a problem I've ever needed to solve.",1514733128.0
fang_xianfu,"Are you knitting, or are you using R Notebooks in RStudio (which is what that thread is about). Assuming the latter, which version of RStudio do you have installed?",1514681985.0
Cosi1125,"As counterintuitive as it sounds, the best way to define the observer is the insertUI() block (before the last expression, obviously). You are sure that the observer is registered at the same time the new element is added to the UI.",1514719884.0
too_many_splines,"Rather than a loop, I'd keep track of the selector of each new created button with `reactiveValues`. This value can be used to trigger your `observeEvent` call.

Trivial example:

    library(shiny)
    shinyApp(
        ui = fluidPage( actionButton(""add"", ""Add UI"") ),
        server = function(input, output, session) {
            react_val <- reactiveValues(current = ""add"")
            
            observeEvent(input[[react_val$current]], {
                old_button <- react_val$current
                new_button <- ifelse(old_button == ""add"", ""1"", as.character( as.numeric(old_button) + 1 ))
                insertUI(
                    selector = paste0(""#"", old_button),
                    where    = ""afterEnd"",
                    ui       = actionButton(new_button, paste(""Spawned from:"", old_button))
                )
                react_val$current <- new_button
            })
        }
    )",1514849496.0
samclifford,The mgcv package has splines with a periodic basis. You can do a tensor product of a periodic basis for theta and a non periodic basis in r. ,1514597822.0
AllezCannes,"For gifs, use gganimate: http://www.ggplot2-exts.org/gganimate.html

For interactive charts, use ggiraph: http://www.ggplot2-exts.org/ggiraph.html

If by interactive, you mean ""let the end user specify what data to show on the chart"", you'll need to look into shiny: https://shiny.rstudio.com/",1514591834.0
Kickuchiyo,http://www.ggplot2-exts.org/gganimate.html,1514591840.0
Owz182,"I use the packages ‘animation’ and ‘magick’. Magick in particular has a great vignette, but animation allows has a really good function that calls to Imagemagic or graphicsmagic. ",1514610871.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/applewatch] [Advice \/ comments \/ help on scripts to analyse Apple Health data](https://www.reddit.com/r/AppleWatch/comments/7mxu4z/advice_comments_help_on_scripts_to_analyse_apple/)

- [/r/applewatchfitness] [Advice \/ comments \/ help on scripts to analyse Apple Health data](https://www.reddit.com/r/AppleWatchFitness/comments/7mxwde/advice_comments_help_on_scripts_to_analyse_apple/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1514589508.0
biledemon85,"Apt-get installs an old version of R, can't get packages like dplyr to install as a result. 

Going to try this tutorial to get latest version of R installed: 
https://www.digitalocean.com/community/tutorials/how-to-install-r-on-ubuntu-16-04-2",1514638237.0
balanaicker,Check out termux [https://termux.com/](https://termux.com/) as well there is a custom repository [https://github.com/its-pointless/gcc_termux](https://github.com/its-pointless/gcc_termux) with R.,1514937409.0
wipeoutout,"For the random factor, the format is (1|participants/(B*C)). Note that lmer won’t give you a significant value automatically.",1514579784.0
paerb,I find that R Notebooks make this type of presentation much easier on the presenter. ,1514513089.0
infrequentaccismus,"Im not totally sure what you are trying to do, but this is exactly what you asked for:

    c(mod1,mod2) %>% unlist %>% as.character

If you are not literally asking for a vector of the elements of each list contained in each model object, let me know a bit more about what you are shooting for and I can try to help",1514421227.0
shujaa-g,"Use `mget` to create you list so that it is named. [Here's the corresponding Stack Overflow thread](https://stackoverflow.com/a/30765678/903061).

     some_models = mget(c(""mod1"", ""mod2""))
",1514494263.0
_Wintermute,"You have two options

1. Have miceadds as an import then, if you're using devtools/roxygen then you can use `#' @importFrom miceadds round2`, though this is still going to drag in the entire `miceadds` package and all of its dependencies on install.

2. Just copy the code from [miceadds::round2](https://github.com/alexanderrobitzsch/miceadds/blob/master/R/round2.R) into your package somewhere and give credit to the original authors. This will only work as it looks like the function is self-contained.",1514402773.0
_nt2,"As it happens this was asked just a few weeks ago:

https://www.reddit.com/r/rstats/comments/7j3r9n/when_creating_an_r_package_what_is_the/

In your case a best practice would be to put `miceadds` (package name only) in the `DESCRIPTION` file and call `miceadds::round2` in your code explicitly.",1514468129.0
coffeecoffeecoffeee,This seems like a very useful new package.  It adds Pandas DataFrame-like time index functionality to tibbles so you can perform operations like filtering and summarizing using time intervals.,1514394999.0
infrequentaccismus,"This is pretty awesome. It does what xts does, but in a more tidyverse-like way!  Awesome!!",1514419142.0
adhi-,This is why I fucking love R. *swoon*,1514429583.0
MP940,"Agree with the rest of the posts, this is really fucking awesome.",1514519113.0
Gumeo,"What are you looking at? This is a grid of size 10 by 10. Each patch represents weights from an input image of a handwritten digit to a hidden unit in a neural network. I was inspired by [3blue1brown](https://www.youtube.com/watch?v=aircAruvnKk&vl=en) when he went through explaining deep learning and wanted to do it myself, to see what was happening inside the network. Each patch matches against an incoming digit and then generates a number based on the similarity. These numbers are further aggregated to make a prediction on what the network sees. These kind of models are used to read cheques.

I am writing a small [blog series](https://gumeo.github.io/post/part-2-deep-learning-with-closures-in-r/) about neural networks, mostly for myself to learn. I was interested in how neural networks learn and made these gifs.",1514329732.0
csjpsoft,"I'm about to go to bed, so I cannot be of immediate help, but could you fill in some details for others?  Here are some questions that occur to me, but I'm still just a data analytics student, so please forgive me if they're not appropriate.

How many observations?  (I assume 1000 errors is a high percentage.)  Did you divide the data into training data and test data, and in what proportion?  How many predictors?  Are your predictors correlated with your categorical response variable?  Did you run ANOVA to see whether the response levels reflect differences in (at least) the means of the predictors?  Did you perform logistic/binomial regression?  Did you cross-validate?  Would a different cut-off point for choosing the class give you fewer errors?  Might you get better results with polynomials of the predictors?  ",1514268474.0
AllezCannes,"I would follow the advice of this answer on SO: https://stackoverflow.com/a/24122775/5221626

As far as hosting data on a website, you'll need to use something like shinyapps.io or use your own linux server with a shiny server license.",1514181539.0
epijim,"Shiny could do it - but you have lots of potential solutions, including a scripted cron job. I would go with the latter if doing it myself. E.g make a website with blogdown or normal rmarkdown, then just schedule it to run then redeploy every 15 mins. or just use r to make and deploy the data, and use a total different tool for the website. 

As another alternative - At my workplace we have RStudio Connect - and it seems perfect for this. You can upload shiny apps - or Rmarkdown, and if you upload Rmarkdown you can schedule when it should recompile. I dont know how this will work with the free tiers available to the public though...",1514197998.0
tukey,"Great post, Merry-----Christmas!",1514176643.0
team_morty,"From a mathematical standpoint it doesn't matter, but choosing your intercept well can help frame your research question.  If you're looking at racial disparities in America, it would probably make more sense to choose white or black as the intercept- aka the category to which everything else is being compared to.

Here's a demonstration why using fake data:  which headline sounds easier to understand? 

1)  Black americans are 60% more likely to die of cancer than asians, compared to white americans who are 30% more likely to die of cancer compared to asians.

2)  White americans are 40% more likely to die of cancer than black americans.

In this case, the primary result is cancer among white or black americans so asians is not the best choice for a reference category.  If the conclusion was focused on asians, then maybe it would be more relevant.",1514140973.0
navidshrimpo,"Use White because it is the default race.

Half joking.

Statistically there is no difference. Regarding interpretation, use whichever is most intuitive. If you're in a predominantly Caucasian majority region, like the US, and that is also the majority audience of your work, it would be a reasonable choice. If you were in China, then Chinese would probably make most sense.

My $0.02",1514133958.0
anonemouse2010,"I'd hesitate to choose parameters based on the data... I always give the advice of choosing things based on which is easiest to interpret, if there is no choice go with the default for your program.",1514133277.0
efrique,"The question has essentially nothing to do with logistic regression; it has to do with how you set up factor predictors for many kinds of modelling -- it would apply equally to ordinary regression for example.
 ",1514185252.0
odeleongt,You could embed it in a Rmarkdown presentation. (Hint: iframe containing the rendered html flexdashboard),1514088860.0
ashwinmalshe,I don’t think it’s possible,1514079517.0
ThomasSpeidel,"Flex dashboard is just HTML5. So you could search embedding HTML5 into PowerPoint. However, these are completely different beasts. I seriously doubt you can do much there. In the past, I have taken screenshots and pasted in powerpoint.
I believe you can produce PowerPoint presentation now in r markdown though they would not have a dashboard look.",1514129894.0
arbelt,"Actually I think the `showtext` package is the most seamless solution for this most of the time (from what I remember there's also a branch of `extrafont` or one of its dependencies that supports otf).

But `showtext` is definitely worth a look if you haven't checked it out. It converts the font to shapes in the output, so you don't need to worry about the fonts themselves embedding correctly, and most of the time I'd say this is the better solution for making sure your output is viewed as intended. The only use case I've experienced where this clearly isn't right is sending to post processing that needs access to the text, but then you'd handle fonts there anyways. A bonus is that you can use Google fonts directly without installing on your system.",1514080389.0
samclifford,"If there's a variable you subset the data frame on, you may want to look into split() and then purrr::map() as a way to run the same analysis technique on the subsets. This integrates well into the tidyverse approach and I prefer it to lapply().

You could pipe chain together the split, a map of spread to convert the subset to wide format, your analysis technique, a map of tidy() or whatever summarising command is used to return the values you want and then bind_rows() to get everything back into  single data frame.

Also consider using faceting within ggplot2 if you want to replicate the same plot style over and again. ",1513943635.0
cyril1991,"Honestly just look into the data.table package. It handles very large amount of data and grouping data is also quite easy (you use a syntax like data.table[rows to select on, variables and function of variables like mean/sd/min/max, how to group things]). To get the mean of observation1 and  group size grouped by ID and JN

    data.table[, .(meanVal= mean(observation1), count=.N), by=list(« ID »,  « JN »)]",1513953347.0
fonzy6,Check out nested data sets using tidyr. That may help.,1513958573.0
shaggorama, You're speaking in very broad terms here. It would help a lot iuf you could provide a concrete minimal example to demonstrate the pain points you are hoping to address. ,1513943316.0
michaelquinn32,"Use dplyr::group_by to define the subsets. Write your subset analysis in a function that takes a subset data frame as an argument. Then iterate through everything using dplyr do.

http://dplyr.tidyverse.org/reference/do.html",1513956653.0
TwoTacoTuesdays,"So, two solutions:

A, instead of saving multiple copies of your data in various forms, define a couple functions that do the analyses you repeat often (e.g., a function that takes a given ID as an argument and finds the peak values) and use those in your plots to save keystrokes and time,

or

B, list columns! I don't see this used all that often, but it's one of those things that comes in handy in a few specific situations. You can have a dataframe where columns contain not just a single piece of data, but whole other dataframes, lists, or anything. In this example, each row of column ""c"" isn't just a single number but ten numbers.

    list_column_test <- data_frame(a=1:10,
                                   b=letters[1:10],
                                   c=list(sample(1:100, size=10)))

[This is a great tutorial around list columns.](https://jennybc.github.io/purrr-tutorial/ls13_list-columns.html) It may be worth looking into making a single dataframe that holds all of your other dataframes inside of it, with the other columns in that master control dataframe acting as an index to find what your need. It will keep your global environment neat and tidy, and it will also make finding specific pieces of data easier.",1514005594.0
RaggedBulleit,"You didn't assign them anywhere, you ran some calculations that probably printed to your screen

New_df <- in front of your pipe will assign the summary somewhere",1513900923.0
grasshoppermouse,A gotcha to watch out for: the `plyr` package has a `summarise` function that can mask the `summarise` function in `dplyr`. ,1513953548.0
Cronormo,"Yea, just create a dummy variable categorizing which bin from the histogram that specific point falls in, then tell ggplot to categorize it according to that:

    library(ggplot2)
    
    #Random values to test
    Df <- data.frame(""value"" = rnorm(100,4,2)) 
    
    #Cut our values into 5 categories (bin number)
    Df$bins <- cut(Df$value, breaks = 5, labels = seq(1:5))
    
    #Plot a histogram, where colors are assinged based on the bin variable
    ggplot(Df) +
      geom_histogram(aes(x = value, fill = bins), bins = 5)

EDIT: ggplot2 doesn't simply ""cut"" the data vector, it performs some other operations on it. I managed to do it using the ggplot2 functions (see https://github.com/tidyverse/ggplot2/blob/master/R/bin.R). You have to define all functions up to `bin_breaks_bins`, then use that function, providing minimum and maximum values as well as number of bins. The output is a list with a `$breaks` component, that you can use as an argument on the `cut`function to get the same breaks as ggplot2. Not as simple as I originally thought but still doable. 

EDIT2: Just realized you can just define your own binwith instead of trying to figure out how ggplot2 goes it. Should be much easier",1513884657.0
Digging_For_Ostrich,"Could you not use an aesthetic on the geom_histogram, and set it based on some dummy variable in the data?

e.g. geom_histogram(data, aes(fill=colorVar))",1513882407.0
TroyHernandez,"Is anybody watching this?  How?  I hear major interference.  This is unwatchable.

I also hear H2O is ""proprietary garbage"".  I've not messed with it, because why when I've got `glmnet`, `randomforest`, `mxnet`, etc?",1513861219.0
samclifford,Join the two data frames together with dplyr::inner_join and then either dplyr::arrange by population and then use forcats::fct_inorder or jump to forcats::fct_reorder after joining and tell it you want to sort on population. ,1513770709.0
shujaa-g,"I would suggest just using a linear model. (Call it an ANOVA if you want, since all your predictors are categorical.) Something like

    lm(dep_delay ~ carrier:origin - 1, data = ..your filtered data..)

You can then sort the coefficients by airport and then by airline. Plot them with their confidence intervals.

I don't like your CDF idea because ""what is the probability that a flight is on-time or early"" seems less useful than ""what is the expected value of the delay"". Personally, I don't mind much if a flight is 5 minutes late, so summary metrics like mean, median, trimmed mean, etc. seem like they offer more value. And while we're on that note, as much as delayed departures can be annoying, maybe what really matters is arrival delays?

**If** you really want to discard all that information and reduce on-time-ness to a binary yes or no, I still don't see the point of using a CDF to estimate the the probability of being on time instead of just calculating the proportion of flights that are on time. Seems like complexity for complexity's sake.

From the start, I think it's a badly posed question. The question itself presupposes that airline lateness at a given airport depends on time of day. Lateness by airline and by airport (and their interaction) makes sense from a common-sense perspective, but do you have reason to assume that the answer will differ based on before or after noon? If not, why are you throwing away half (actually more like 60%) of your data by only looking at morning scheduled departures? 
",1513731708.0
Dgarciarieckhof23,"Introduction to R by datacamp
Introduction to R in Edx
R courses in coursera

But just pick a book like this one from Hadley http://r4ds.had.co.nz

I hope this help you in some way ",1513729406.0
shujaa-g,The [R tag wiki](https://stackoverflow.com/tags/r/info) on Stack Overflow has a nice list of R resources.,1513791988.0
grasshoppermouse,"As u/_nt2 said, `lapply` would work. This code assumes you want to recode all columns:

    df2 <- data.frame(lapply(df, function(x) as.numeric(x == 1)))

Edit: if you wanted to use a `for` loop:

    for (i in 1:50){
        df[i] <- as.numeric(df[i] == 1)
    }",1513732439.0
efrique,"`var1:var50` won't work and the syntax for accessing columns of your data frame is wrong; there are several ways to do that if it's the only way you have of identifying those columns, but I presume you have a handle on which ones they are and can set that up into a variable. If so:

Assuming if `idx` contained the columns of the variables that you wanted to change, you could do 

    df[,idx] <- 2-df[,idx]

if it was all columns, you could do:

    df <- 2-df",1513823067.0
AllezCannes,"To answer your question directly, you were getting close in your last attempt. Just remove the `$` from `df$[i]`.

Remember that `df$var` means the same as `df[[""var""]]`. `df[i]`means take the *i*th variable in the dataframe and treat it as its own dataframe (as opposed to a vector). If you want to treat it as a fector, then you need to do `df[, i]`.

You already received a base R answer (`lapply`) and a `dplyr` answer (`mutate_at`), so I'll add another way using `purrr` (which is from the same author as `dplyr`):

    purrr::map_df(df, ~ ifelse(. == 2, 0, .))

",1513824322.0
ReimannOne,"Use apply().  On mobile, so no telling how this will come out:

    apply(df, 2, function(x) ifelse(x==2, 0, 1))

Might work, I can't test it right now.",1513720651.0
_nt2,"Many ways to do this which largely depend on the structure of the dataframe.

If your variables `var1` up to `var50` are all beside each other, you can use `dplyr::mutate_at` as follows:

    library(dplyr)
    df <- data.frame(replicate(50, sample(1:2, 10, replace = TRUE)))
    names(df) <- c(paste0(""var"", 1:50, sep=""""))

    df2 <- df %>% 
      mutate_at(vars(var1:var50), funs(as.numeric(. == 1)))  # FIXED as per /u/AllezCannes correction

This is closest to what you had envisaged working. There are many other ways to do this.",1513721522.0
_nt2,"**Use `mapply` to apply a function to rows of a data frame when the function arguments are the values in each row.**

    DF$Score <- mapply(grepl, pattern = DF$substring,  x = DF$reference)


Performance comparison on a 1000 row data.frame.

    library(microbenchmark)
    DF <- dplyr::data_frame(substring = sample(LETTERS, size = 1000, replace = TRUE), 
                     reference = replicate(1000, paste0(sample(LETTERS, 2), collapse="""")),
                     Score = NA)

    microbenchmark(
      for(i in 1:nrow(DF)) { DF$Score[i] <- grepl(DF$substring[i],DF$reference[i], fixed=TRUE) },
      DF$Score <- mapply(grepl, DF$substring, DF$reference)
    )

Results:

    Unit: milliseconds
                                                                                                   expr        min         lq      mean     median          uq      max neval
     for (i in 1:nrow(DF)) {     DF$Score[i] <- grepl(DF$substring[i], DF$reference[i], fixed = TRUE) } 236.841996 243.673311 258.85204 246.512336  269.483839 512.3068   100
                                                              mapply(grepl, DF$substring, DF$reference)   4.262248   4.424526   4.63872   4.568922    4.806158   5.4372   100

So `mapply` is about 60 times faster on my computer.

Edited to add: `mapply` is slightly faster than `stringr::str_detect` suggested by /u/ReimannOne below, which was also a good answer. Perhaps `str_detect` would be faster in a more complex example.

But `mapply` is more general and is a good tool for the chest.
",1513706199.0
ReimannOne,"
    library(stringr)
    df$Score <- str_detect(df$a, df$b)

Should give you df$Score with TRUE or FALSE, pretty much the same thing.",1513703894.0
_dm3ll3n_,Make sure you're declaring `DF$Score` before the loop.,1513702750.0
Cronormo,"hm...your code worked fine for me (although I don't have acess to your data). Might it just be the number of loops thats wrong? try:

     for(i in 1:nrow(DF)){
          DF$Score[i] <- grepl(DF$substring[i],DF$reference[i], fixed=TRUE)}

Remember that grepl checks if your reference contains your substring, not if they are completely equal (might be or not be relevant).
",1513702294.0
shujaa-g,"For real speed, use `stringi::stri_detect_fixed` directly. It's more than 10x faster than the `mapply` solution:

	library(microbenchmark)
	library(stringi)
	microbenchmark(
	  mapply_unfix = {DF$map_unfix <- mapply(grepl, DF$substring, DF$reference, fixed = FALSE)}, 
	  mapply_fix = {DF$map_fix <- mapply(grepl, DF$substring, DF$reference, fixed = TRUE)}, 
	  stri = {DF$stri = stri_detect_fixed(DF$reference, DF$substring)}
	)
	# Unit: microseconds
	#          expr      min       lq      mean    median        uq       max neval
	#  mapply_unfix 7006.813 7316.523 9154.8968 7721.9240 10234.619 23650.861   100
	#    mapply_fix 3915.146 4163.578 5202.4653 4312.0940  5961.466 11313.171   100
	#          stri  328.426  387.893  474.3883  445.5485   495.657   924.301   100",1514056783.0
dtrillaa,"Great video for someone relatively new to R, thank you",1513751864.0
shujaa-g,"I don't know that ""most common speaker"" qualifies as a subtle insight. And the distribution of the first letter of speaker's names seems neither interesting nor subtle.",1513871096.0
seitgeist,Cute,1513671266.0
infrequentaccismus,It looks like you are just trying to Facet the plots by group and arrange the facets according to the mean of the factor level. If this is correct I can send you some sample code. Do you have a simplified dataset you can share?,1513646013.0
infrequentaccismus,"    library(dplyr)

    #Create a dataframe with 20 different cars from mtcars, populate points from logistic distribution
    data_frame(x = rep(1:100, 20),
                 cars = mtcars %>% 
                    rownames_to_column %>% 
                    select(1) %>% 
                    head(20) %>% 
                    unlist %>% 
                    unname %>% 
                    rep(100) %>% 
                    sort,
                y = rlogis(100) %>% 
                    sort %>% 
                    rep(20)  
           ) %>% 
      # Convert cars to a factor
      mutate(cars = factor(cars)
         ) %>% 
      # Plot points, faceted by cars
      ggplot(aes(x,y, color = cars)) + 
        geom_point() +
        facet_grid(.~fctrs)",1513648921.0
docdc,Very clever! I'll bite.  Why does R have an audio library?,1513638241.0
BurkeyAcademy,"[I made a quick YouTube video of it](https://youtu.be/iPVhTim2c6Q) just in case anyone wants to play it, but isn't at their workstation at the moment! If I have time, I might compose another song and post...",1513693087.0
BurkeyAcademy,"OK, one last song I composed in R: [Let it Snow!](https://youtu.be/67vdM9SEiCw) 
Code link in video description. ☺",1513710949.0
hohohomer,"FWIW, I work in IT for an org that has Shiny apps. We host them internally using https://www.shinyproxy.io/. With ShinyProxy, each Shiny is hosted inside of a dedicated Docker container.",1513560211.0
TeslaIsAdorable,"We use a shiny server behind the corporate firewall. Right now we're using the free edition, but eventually we may spring for the paid version. The server is on a virtual machine running Ubuntu... It set up the machine and then handed the keys over to me.",1513563787.0
The_Sodomeister,Lol SJSU represent. This is Joey,1513477350.0
nobadchainsmokers,How about the [github vignettes](https://github.com/cran/gridExtra/tree/master/vignettes)?,1513449592.0
infrequentaccismus,Check out the ggplot tutorials on datacamp... the advanced in (third tutorial) explains how to use these packages. ,1513442261.0
efrique,"Even a small change in bin origin and/or bin width can make a difference to the appearance of a histogram (in particular circumstances, sometimes dramatically so), and the algorithms that choose bins automatically tend to prefer round numbers meaning that the bins won't scale exactly the same as the data, so standardizing will generally give you a somewhat different-looking histogram. 
",1513410030.0
sohaibhasan1,You should be able to do this by encoding your time variable as a factor. You could then use geom_vline or geom_segment to make a small mark the signify the breakpoint.,1513391123.0
zabulon_,"My understanding is discontinuous axes are not possible in ggplot and not written into the package.

Looks like you can use base R and package plottrix. See here for some suggestions:
https://stackoverflow.com/questions/7194688/using-ggplot2-can-i-insert-a-break-in-the-axis",1513386747.0
samclifford,"If the question is literally about four points, use a table. If you're wanting a more generic answer about breaks in time series, are the measurements from two different campaigns? If so, facet by campaign number. ",1513393185.0
nobadchainsmokers,"Base package's `agrep` matches on Levenshtein distance. I use `fuzzyjoin` package for string dist and regex matching (https://cran.r-project.org/web/packages/fuzzyjoin/fuzzyjoin.pdf). 

[The github](https://github.com/dgrtwo/fuzzyjoin).",1513372477.0
roadrussian,"First model should include control variables, the second model should throw in the independent variable. To be able to say shit about your model you should include Odds ratio, p value(paper show them as stars) and coefficients(coeff). P value and coeff you already have, the others you still have to calculate. Dont forget that if ya have 2 classes (male/female) you use binary logit, if more than 2 ordinal logistic or nominal logistic. 

Here is a paper doing the shit that you need to do, look at the pretty graphs: http://www.sciencedirect.com/science/article/pii/S0048733317301038

If you are a real high rolla you could add nagelkerke R or logistic spesific R2 to say samething about how well your model fits. 

Finally, just look at the graph of the paper i sent, and look at what data went where. 

Here some more shit for you: 


If you are still stuck send me an message with an email and i will send you a lecture explaining all this so well a mentally retarted fish could bang out a research paper in a week. 
",1513329035.0
shujaa-g,"The `stargazer` package can be useful for creating prettified tables for this. It will include all the relevant info by default, outputs to LaTeX, HTML, or ASCII. [See their vignette for examples](https://cran.r-project.org/web/packages/stargazer/vignettes/stargazer.pdf).
",1513354754.0
zorp_,Two by two grid with a graphic and a table for each variable.,1513320106.0
bwcampbell,"Look into the texreg package, you'd specifically be interested in the plotting function (plotreg I think), screenreg, and if you're using latex, texreg. ",1513337231.0
spiderdaynightlive,"You have the right idea with making a parameter table of your top model. For categorical factors, you will need to indicate which category was the reference (for you, genderFemale looks like it was the reference). Check out Grueber 2011 for examples. Will update with link when off mobile",1513347830.0
Gumeo,[Here](https://imgur.com/V3l01mn) is a gif showing how the weights in the first layer change when training over two epochs. I'll outline in the future posts how I created this as well! Hope you guys enjoy this! Feedback much appreciated.,1513329135.0
infrequentaccismus,Sys.sleep() or profvis::pause(),1513352702.0
blahblahblahblah8,Load dplyr last. Packages are attached to the search path in order of loading: last in first searched. ,1513214391.0
mjskay,"Besides making sure you load `dplyr` last as /u/blahblahblahblah8 suggested, you can also use the [import](https://github.com/smbache/import) package to load specific functions as desired, e.g.:

    import::from(some_package, some_function)

 I sometimes use this when I just want one or two functions from a package and don't want it to clobber the rest of my namespace.",1513223510.0
tenurestudent,"You can also explicitly call a function dplyr::select()

So you could do 
select <- dplyr::select()
To ensure you use a given function regardless of library order",1513236870.0
exolon128,"Here's a way it that could work - I wasn't clear on what sort of error you are dealing with (one from the API or an R error)

I've adapted your idea for generating errors to make things easier

    # Function to generate an error
    error_gen <- function(x){
      if (x == 1) { stop(""Error: Try Again"") }
      else if (x == 2) { stop(""Error: Stop for loop"") }
      else ""Success""
    }
 
This is how to wrap your code in a try catch:

    error_handler <- function(){
      tryCatch({
        # Simulate code with possible error
        error_gen(sample(1:3, 1, prob = c(0.4, 0.2, 0.4)))
      }, error = function(e){
        # Catch and return the error
        e
      })
    }
    
This is how you can put it all together

Note: 'out$message' will only work if you have an R error

    ids <- c(""1"", ""3"", ""7"", ""9"")
    numbers <- as.list(rep(NA, length(ids)))
    
    for(i in seq_along(ids)){
      # Set criteria for while loop
      success <- FALSE
      while (!success) {
        # Try to run the code -
        out <- error_handler()
        # Check the output
        if(!inherits(out, ""error"")){
          # If not error, update list...
          numbers[i] <- out
          # ...and exit
          break
        } else if (out$message == ""Error: Stop for loop"") {
          # Handle API key error
          stop(sprintf(""Fix API key and restart with id %s"", ids[i]))
        }
      }
    }
    numbers
",1513206642.0
hodos_ano_kato,"This might actually be a better post for rstudio [commuity](https://community.rstudio.com/) since there is a Shiny specific discussion board and sometimes Joe, Winston, or Dean will answer questions!",1513185849.0
NoFascistAgreements,"It's hard to tell without your actual dataset and knowing what everything means, but my instinct would be that the intercept refers to eggs when mosquito species (whatever is not latens) feeds on host (whatever is not Macaque) when Temperature is 0 and Mosquito size is 0 and Blood Volume is 0.",1513187159.0
VincentStaples,"This might make it more intuitive.  

ggCaterpillar <- function(re, QQ=TRUE, likeDotplot=TRUE) {
  require(ggplot2)
  f <- function(x) {
    pv   <- attr(x, ""postVar"")
    cols <- 1:(dim(pv)[1])
    se   <- unlist(lapply(cols, function(i) sqrt(pv[i, i, ])))
    ord  <- unlist(lapply(x, order)) + rep((0:(ncol(x) - 1)) * nrow(x), each=nrow(x))
    pDf  <- data.frame(y=unlist(x)[ord],
                       ci=1.96*se[ord],
                       nQQ=rep(qnorm(ppoints(nrow(x))), ncol(x)),
                       ID=factor(rep(rownames(x), ncol(x))[ord], levels=rownames(x)[ord]),
                       ind=gl(ncol(x), nrow(x), labels=names(x)))
if(QQ) {  ## normal QQ-plot
      p <- ggplot(pDf, aes(nQQ, y))
      p <- p + facet_wrap(~ ind, scales=""free"")
      p <- p + xlab(""Standard normal quantiles"") + ylab(""Random effect quantiles"")
    } else {  ## caterpillar dotplot
      p <- ggplot(pDf, aes(ID, y)) + coord_flip()
      if(likeDotplot) {  ## imitate dotplot() -> same scales for random effects
        p <- p + facet_wrap(~ ind)
      } else {           ## different scales for random effects
        p <- p + facet_grid(ind ~ ., scales=""free_y"")
      }
      p <- p + xlab("""") + ylab(""Random effects"") + ggtitle(""TITLEHERE"")
}
    
    p <- p + theme(legend.position=""none"", plot.title = element_text(size = 10))
    p <- p + geom_hline(yintercept=0)
    p <- p + geom_errorbar(aes(ymin=y-ci, ymax=y+ci), width=0.1, colour=""black"")
    p <- p + geom_point(size=1.25, colour=""blue"") 
    return(p)
  }
  
  lapply(re, f)
}



summary(model)  
coef(model)  
ranef(model)    
ggCaterpillar(ranef(x, condVar=TRUE), QQ=F, likeDotplot = T)",1513204797.0
ihowson,"Hi all -- I'm the developer behind rdrr.io. Happy to answer any questions you might have!
",1513251629.0
enilkcals,Can't imagine I'll ever spend hours typing in commands from a mobile browser but one solution for those who have wanted to run R on their phone/tablet.,1513178758.0
questionquality,"The ""first column with numbers"" you're talking about isn't actually considered a column by R. You can check this with `yourdata[,1]` (it will return the first column, country). Instead, it's the row names attribute which is used for printing sometimes, and can be modified with something like 

    row.names(yourdata) = yourdata$`Country Name`",1513197542.0
USADeutsch,"Additionally, here is the code that I have used.

model.fiml <- 'claimed ~ INTERNATIONAL + suicide + propextent + individual + nperps'

fit.fiml <- sem(model = model.fiml, data=new.data, missing='fiml', estimator = 'mlr')

model.fiml2<- 'compclaim ~ INTERNATIONAL + suicide + propextent + individual + nperps'

fit.fiml2 <- sem(model = model.fiml2, data=new.data, missing='fiml', estimator = 'mlr')

star.1 <- stargazer(as.data.frame(fit.fiml), as.data.frame(fit.fiml2),
                    title=""Title: Regression Results"",
                    align=TRUE,
                    type = ""html"",
                    style = ""ajs"")",1513160462.0
TotesMessenger,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/rstudio] [How to make a (nice) table comparing two lavaan glm models?](https://www.reddit.com/r/RStudio/comments/7jidl4/how_to_make_a_nice_table_comparing_two_lavaan_glm/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1513161205.0
trngoon,"Try all these and see what they do:

Get all info about dataset:
https://www.statmethods.net/input/contents.html


Descriptive statistics:
https://www.statmethods.net/stats/descriptives.html  

Then try some of this stuff:
http://r4ds.had.co.nz/exploratory-data-analysis.html",1513138542.0
zorp_,Google library(nycflights13) and run the tutorial.,1513136987.0
catdci,"Your approach is fine, but could be done differently by using ifelse() instead of cut() along with ggplot2:

    data(""trees"")
    trees$big <- ifelse(trees$Height <= 75, ""Little"", ""Big"")
    trees$seq <- seq_along(trees$big)
    library(ggplot2)
    ggplot(trees, aes(x = seq, y = Volume)) +
      geom_point(aes(colour = big, shape = big))

If you wanted to add Girth on the x axis:

    ggplot(trees, aes(x = Girth, y = Volume)) +
      geom_point(aes(colour = big, shape = big))",1513118333.0
fonzy6,Check out the odbc package.,1513114441.0
Crypt0Nihilist,"Not tried to SSH. RODBC worked for me fine in the past. I remember having some kind of difficulty and had to use the 32 bit version of R, but that may well have been database specific.",1513118537.0
datatitian,"Was going to tell you how to do with DBI and JDBC, but searching for the syntax, I found this brand new article on the odbc package from RStudio

https://support.rstudio.com/hc/en-us/articles/214510788-Setting-up-R-to-connect-to-SQL-Server-",1513130165.0
mr_matzoball,Not SSH but you can connect using rpostgresql/rmysql packages provided you have the proper keys. I keep mine in an encrypted yaml file. Look up the dbconnect function ,1513131667.0
SecretAgentZeroNine,"Apparently my function's calculations are off

chi.test.stat <- function(df){
  one <- NULL
  two <- NULL
  for(i in 1:nrow(df)){
    one[i] <- round((df[i,""event.times.observed""]-df[i,""expected.frequencies""])^2,2)
    two[i] <- round(one[i]/df[i,""expected.frequencies""],2)
  }
  print(sum(two))
 }",1513092865.0
_nt2,"Remember to put 4 spaces before lines of code you post. Here is your code:

    chi.test.stat <- function(df) {        
        one <- NULL
        two <- NULL 
        for (i in 1:nrow(df)) {
          one[i] <- round((df[i, ""event.times.observed""] - df[i, ""expected.frequencies""])^2, 2) 
          two[i] <- round(one[i] / df[i, ""expected.frequencies""], 2)
        } 
        print(sum(two))
    }

Numerical analysis note: Never, ever, ever round interim calculations. That's the biggest problem. Get rid of the `round` code and you'll be fine (although I didn't run your code - I only read it and it looks correct otherwise.)

A programming note. Use `return(sum(two))` and not `print`. Your function doesn't actually return a value but produces a ""side-effect"" instead.

`R` has lots of ways to avoid writing your own loops. Ask if you want to learn how.
",1513094609.0
JigglypuffIRL,Looks like someone hasn't been going to class,1513039515.0
RememberToBackupData,"So what do you need help with in particular? Based on this write-up, your teacher is comprehensive and has probably covered what you need to know.",1513039367.0
spectrum_specter,"Start off with subsetting the data (like the teacher suggest). You can use the subset command or dplyr. Then, use a modeling method that you likely learned in class to predict the Y variable, the overall rating. Look to minimize the error rate while maximizing the R^2 without overfitting and including correlated variables. You will also need to remove outliers and correct the data (ex. heteroskedasticity) before you begin building your model. ",1513046772.0
blahblahblahblah8,You want us to help you do your job interview? You know you signed an NDA right?,1513052912.0
lionbutt_iii,"Hi Jack! That is one doozy of an error message. For assignments in R it's commonplace to use <-

It's tough to get used to, but once you've got that down the rest is smooth sailing.",1513053758.0
VincentStaples,"Amusing snark aside, I guess start here?  

df.total <- read_csv(""FILEPATH/airbnbData.csv"")  
df <- subset(df.total, attribute.to.subset.on == ""relevant.component"")  
x <- lm(rating ~ attribute1 + attribute2 + attribute3, data=df)  
summary(x)  
plot(x)  
predict()...",1513067069.0
The_Sodomeister,"You can make bgame a vector:

bgame <- rep(0,15)

for (i in 1:15) {

bgame[i] <- todayDataS[i,]

}",1513029013.0
grandzooby,"This `bgame[i]s <- ` will generate an error because R doesn't know what to do with that `s`.  Try just `bgame[i] <- ` or 'bgames[i] <-`.

But you'll also need to create `bgame[]` ahead of time so that the value can be stored in it.  Maybe like this:

    > bgames <- rep(0,10)
    > bgames[3] <- 3
    > bgames
     [1] 0 0 3 0 0 0 0 0 0 0

Though maybe it makes more sense to change your todayDataS into a dataframe and just deal with it row by row rather than creating new variables.",1513029368.0
_nt2,"This is a good short book for getting into building your own packages: http://r-pkgs.had.co.nz/

Your questions are dealt with here: http://r-pkgs.had.co.nz/namespace.html

It's not an easy chapter, but I think it's worth it to go through carefully.

In your case, you are using only one function from one package. The book recommends:

> If you are using just a few functions from another package, my recommendation is to note the package name in the Imports: field of the DESCRIPTION file and call the function(s) explicitly using ::, e.g., pkg::fun().

If you follow this advice it means you'll need to know a few other things about building packages, which you may or may not at this point, put it's all in the book.

The book does not recommend using `library` or `require` in a package.",1513015958.0
netskink,In the description file put a imports: somepkg,1513089965.0
freudianslip05,"If I'm understanding correctly, you'll need to restructure your data set so that your dependent variable is all in one column, instead of spread over multiple columns by plant species.  This would result in one column of plant cover and one column of species.  Check out the gather function from the dplyr package to do this.  ",1513024164.0
enilkcals,"I've used [Emacs](https://www.gnu.org/software/emacs/) + [Emacs Speaks Statistics](http://ess.r-project.org/) for years, ESS is these days hosted on the r-project.org site.",1513010381.0
guepier,"Just yesterday, /u/hjkl_ornah posted an article on /r/Rlanguage, describing [how to turn Vim into an R IDE](
https://medium.com/@kadek/turning-vim-into-an-r-ide-cd9602e8c217).

I am using a very similar setup (Nvim-R + YouCompleteMe) and I can highly recommend it: it just works, and it comes with all of the power of Vim.",1512990755.0
_Shipwreck,"You can also try Sublimetext (or Atom), you need to set it up though, it doesn’t work out of the box. ",1513001391.0
amstell,Try deleting your .RData file in your home directory.  I was having similar problems before and reinstalling RStudio doesn't do anything if you don't clean out the directory it lives in. Hope this helps.,1513056434.0
brbecker,"Are you sure it's an RStudio issue? Can you quickly open just R via the terminal with no GUI?

I was puzzled while helping my colleague since her RStudio and R were hanging when opening. We reinstalled everything 2x before I realized her R opened instantly without hanging when I called R in the non-default directory. Turns out she had huge R data files in ~/ dir that would always attempt to load on R or RStudio being initialized (like waiting 5 minutes and they werent done loading, maybe swap was being written to?)

Anyways I think it's unlikely the problem would be RStudio or ide based.",1513021866.0
informaticsdude,"Are you using git? When i installed git on my Rstudio, the default indexing was for my entire C: Drive, which brought the program to a standstill. I haven't solved the git issue yet, but Rstudio went back to normal speeds after I disabled it.",1513006252.0
coip,"> What is a good IDE other than Rstudio?

I quite like **R Tools for Visual Studio**. 

I see you say you have looked into it already, so maybe it wasn't your cup of tea, but if you have any questions on it, let me know. It's a pretty robust IDE with some convenient features (like instantly exporting to Excel at the press of a button, and also has support for RStudio keyboard shortcuts and R Projects).",1513011213.0
Evanescent_contrail,"You can split the client and server roles. Run R as a server, and connect in to it from R-Studio. ",1513022777.0
RaoOfPhysics,"This is probably overkill, but have you considered running RStudio using Docker? See https://www.rocker-project.org/

If you have Docker installed, you need to type the following in your terminal:

```
$ docker run --rm -p 8787:8787 rocker/rstudio
```

Here's some info on how to mount local volumes: https://www.rocker-project.org/use/shared_volumes/",1512995191.0
IamGrabear,Add it to https://github.com/grabear/awesome-rshiny,1513002475.0
ballzoffury,It might be time to buy a new laptop ;),1513037093.0
tacothecat,I am calling all my simulations simies from now on,1513054071.0
nimreth,"Hey! Great job! Off topic, I wonder what project won the first prize? ",1513082023.0
CadeOCarimbo,Fucking nice job using Keras for images distance calculations. ,1513016705.0
,"This is a great article. Would like to see more of this here.

Quick question: does it mean `nrow()` will always be slower than `dim()`? If so I have some refactoring to do. Ever since I learned about `nrow` and `ncol` I stopped using `dim`.",1512956279.0
ReimannOne,"This is a great article that gets a little 'under the hood' of R.  


I'm intermediate at best in my understanding of R. I've have always heard it was slow but, good at what it does.  Occasionally coming across explanations like this really help in speeding things up, and understanding what's going on.



This is the kind of thing I really like to see in /r/rstats, thanks for the post.",1512955424.0
normee,"After reading this post, I found this section of [Advanced R](http://adv-r.had.co.nz/memory.html#modification) useful for more examples of modifying in place.",1512958647.0
Hoelk,R still manages to come up with some super weird behavior that surprises me,1512968142.0
navidshrimpo,Sick painting.,1512958903.0
inkhorn82,"See the below link :

https://cran.r-project.org/web/packages/kableExtra/vignettes/use_kable_in_shiny.html",1512911376.0
questionquality,"I'm guessing a little bit about what exactly you want to do:

* You want to split the strings in X1 into two, X2 and X3, 
* X2 and X3 should each be half of X1, but when it's an odd length, X2 should get the extra character.

Is that right? If so, here a solution. You might need to `install.packages`

    library(tidyverse)
    
    # example data
    df = data.frame(x1 = c(""105103"", 
                           ""110130"", 
                           ""5572"",
                           ""10599"",
                           ""512"",
                           ""03""))
    
    df %>%
        # make sure x1 is a character vector
        mutate(x1 = as.character(x1), 
               # for each x1, calculate the length
               l = nchar(x1),
               # for each x1, take the first half of the string
               x2 = str_sub(x1, 1, ceiling(l/2)),
               # for each x1, take the second half of the string
               x3 = str_sub(x1, 1 + ceiling(l/2), l))

    # results
          x1 l  x2  x3
    1 105103 6 105 103
    2 110130 6 110 130
    3   5572 4  55  72
    4  10599 5 105  99
    5    512 3  51   2
    6     03 2   0   3",1512910398.0
pina_koala,"Instead of parsing X1, you're just splitting it and hoping for the best. If you write a script to dissect the scores you will be in a better position. ",1512952761.0
oggesjolin,"shouldn't it be df$Length (with a capital ""L"") to match the ""Length"" column? Further, why should 105 be split into 10, and 5, while 512 is split into 5 and 12?",1512908374.0
oggesjolin,"I don't see how you could solve inconsistency in splitting a 5 length string into 2+3 in some cases and 3+2 in others... but maybe below could work for you?

r <- data.frame(X1 = c(""105103"",""110130"",""10599"",""87105"",""5572"",""2115"",""105"",""512"",""43"",""03"",""20""))  
library(tidyverse)  
d <- r %>%   
  mutate(X2 = substr(X1, 1, round(str_length(X1) / 2, 0)),  
         X3 = substr(X1, round(str_length(X1) / 2, 0) + 1,   
                     str_length(X1)))  
",1512909469.0
pina_koala,"Row 5:    512 3  51   2

Should read 512 3 5 12

Are these aggregated baseball scores or something? What kind of data are we looking at?",1513040078.0
castro_for_prez,"Ill never forget my lecturer warning us against  p-ing over our journal papers.   There's another good article here which I found yesterday: https://www.ncbi.nlm.nih.gov/m/pubmed/27209009/  Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations. (full text is available free).",1512865759.0
AllezCannes,It disappoints me that articles such as these don't also mention Bayesian statistics as an alternative to the usage of p-values.,1512885760.0
samclifford,I need to share this with my undergrads. ,1512882404.0
deanat78,Removed for not being related to R. Thanks for reports,1512941571.0
castro_for_prez,Polynomial regression is okay but you are still losing a lot of information - the data has steady drops that don't appear to be just random variation.   This would be an ideal data set to look into splines. It looks a bit daunting at first but it is essentially just combining multiple polynomials across the outcome variable. Plenty of online tutorials - have a try.,1512865249.0
wilderecon,It would be helpful if you could your comment so that the code is readable. Probably this means adding an extra line between each statement.,1512860941.0
curseflewrightbyyou,"You need to get the second dataset into the same format as the first. You can do that in R using the tidyr package's gather function:

    new_dataset2 <- gather(dataset2, school, hours)

You'll then need to merge or join the two datasets with something like merge or left_join in dplyr. I'd suggest left_join to keep everything in the tidyverse:

    joined <- left_join(dataset1, new_dataset2, by = c(""school""))

This will create a data frame with three columns, school, hours.x, hours.y. You can then use mutate to combine these columns and select the columns you want into keep in your final data frame:

    final_dataset <- joined %>% 
        mutate(hours = hours.x + hours.y) %>%
        select(school, hours)
",1512852121.0
duffix,You could [reshape the second data frame](http://www.milanor.net/blog/reshape-data-r-tidyr-vs-reshape2/) and then [merge](https://www.statmethods.net/management/merging.html) using rbind().,1512849093.0
mikehonchoconchu,I've never encountered such difficult statistical problems and I am a professional statistic.,1512847491.0
efrique,"> The assumptions of linearity 

That's what the first plot is about. The mean residual (think about taking the mean in narrow vertical strips)  should not be related to the fitted value. [However, the assumption is a bit more general than
what that plot shows you -- it should normally be linear in each of the predictors.]

> Checking the assumption of normality of residuals

That's your second plot, but you can't really interpret it when the first plot isn't right. (It's actually an assumption about the errors rather than the residuals.)

See [here](https://stats.stackexchange.com/questions/101274/how-to-interpret-a-qq-plot) for an explanation about reading these plots.
> The X variables and residuals are uncorrelated

This is not an assumption, it's a mathematical fact, a consequence of the way the fitting is done. The assumption is actually about x-variables and the error term. 

> The assumption of constant variance in residuals (homoscedasticity)

You can assess it to some extent from the first plot (you can see if spread of residuals is related to the mean fitted value).

",1512857075.0
curseflewrightbyyou,"For the residuals vs. fitted values plot you want kind of a shapeless/random distribution around the horizontal zero line. Your zero line has a little hiccup in it, but not something to be very concerned about I️ don’t think.

A normal QQ plot would have all of data points on or near the straight line which indicates the data is normally distributed. Your qq indicates some skew in your data.

Depending on how large your dataset is these plots can be sort of worthless. Don’t over think these too much. 

This site is pretty helpful - https://onlinecourses.science.psu.edu/stat501/node/36",1512856274.0
castro_for_prez,"There is an excellent overview here which gives visual examples of residual plots ""passing"" and ""failing"" the various assumptions.  http://data.library.virginia.edu/diagnostic-plots/",1512864560.0
neuro99,"You an use get0, which will give NULL when the object does not exist. rbind accepts NULL objects:

    df1 <- data.frame(1:2)
    exists(""df1"")
    [1] TRUE
    exists(""df2"")
    [1] FALSE
    rbind(get0(""df1""),get0(""df2""))
      X1.2
    1    1
    2    2",1512834313.0
john_ensley,"Store the data frames in a list called `x` (or whatever) and then do `do.call('rbind', x)`. Just like `rbind()`, his will give you an error if the column names of all the data frames are not identical.",1512775936.0
infrequentaccismus,This is trivial if you use a dataframe. Simply dplyr::bind_rows() the entire dataframe of all new rows you want to append. ,1512792200.0
_nt2,"So I'm guessing you want to run some code on a data.frame that is a combination of other data.frames that will always have names from a predictable set of names.

This is a bit of a fragile way of doing things. It might not be a good idea to have things depend only on the names of other things.

First things first...I want to make sure you are **not** in the (actually terrible, but sadly default in RStudio) habit of saving the `R` workspace when you quit a session. 

Because if you are saving the workspace, and then your workflow depends on the current names of things in the workspace, then what you're suggesting might be a bad idea. Because there is no easy way of knowing if, say `dfg3` is some `dfg3` from two weeks ago you don't care about anymore, or it's actually today's `dfg3`.

So in what follows I'm assuming every time you run this code you start with a fresh workspace and doing something like:

    # ...code to create/import/whatever some data.frames with names like dfg[0-9]*

    newtable <- rbind(dfg1, dfg2, dfg3, dfg4, dfg5, dfg6, dfg7, dfg8, dfg9, dfg10, dfg11, dfg12, dfg13, dfg14, dfg15)

and here you run into trouble. I'm also assuming you might have a bunch of other objects in the workspace with other names **none of which are of the form `dfg[0-9]*`. So I'll make up such a workspace:

    dfg1 <- dfg2 <- dfg3 <- data.frame() # all empty, but it doesn't matter for demonstration
    foo <- function() {}
    bar <- 42

Here's what you can do. First, you want to get all the currently existing data.frames with names 'dfg[0-9]*'. You can get there *names* like this:

    ls(pattern = 'dfg[0-9]*')

But what you want is a *list* of objects with those names. The function `mget` is designed to do that exactly. 

    mget(ls(pattern = 'dfg[0-9]*'))

Now you want to 'rbind' this list of data.frames. For this you use:

    dfg_all <- do.call(rbind, mget(ls(pattern = 'dfg[0-9]*')))

Now, what if you have some non-data.frame objects whose names also follow the pattern 'dfg[0-9]*'? That would cause a problem. If you're in RStudio, hit 'CTRL'-'SHIFT'-'F10' now to start a new session, and we'll create this new workspace:

    dfg1 <- dfg2 <- dfg3 <- data.frame()
    
    dfg0 <- function() { 42 }

Then the code `dfg_all <- do.call(rbind, mget(ls(pattern = 'dfg[0-9]*')))` returns a cryptic error, because `dfg0` isn't a data.frame. So for an extra bit of safety in the code we can make sure we're only trying to combine data.frames:

    dfg <- mget(ls(pattern = 'dfg[0-9]*'))
    dfg <- dfg[sapply(dfg, is.data.frame)]
    dfg_all <- do.call(rbind, dfg)







",1512831862.0
adhi-,"you should tell us what your target variable is, to start.

> categoraical and variables on a 0-1 range

if it's just binary, yea you could use logistic regression on that. i'm assuming you're trying to predict churn aka `left`. in that case, just make sure you convert it from an int to a factor or better yet `True` and `False`.

and please learn about the value of [reproducible examples](https://www.tidyverse.org/help/), makes it much easier to get the help you need.",1512756023.0
Caswe,"Have you had a look at Kable? https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html

It should be easy enough to do there.",1512746516.0
madmongoose1,"At the end of each iteration you could try to delete (rm()) any large objects created within the loop. After the rm() command, call gc() to help R free up memory.

Do you save any data calculated / generated within the loop? If so, using lapply or pre allocating a list to store said results in will probably speed things up. ",1512744533.0
Sanguine_Abeyance,"does generating the personalized plots take a while? or are they fast?

",1512741475.0
Rhenor,"Another solution that's a little more elegant than using rm - put the contents of your for loop in a function with one parameter. Repeatedly call that function using i as your parameter.

One the function call is complete, the objects are automatically removed.",1512809791.0
Econ_dude,"It's difficult to tell exactly what's going without seeing your code. I can think of a few things that could help.

Replace the for loop with lapply. This might be a case where it speeds things up. If you google lapply and for loop you will find plenty of information..

Use a canvas to generate your plots. See https://ikashnitsky.github.io/2017/ggplot2-microbenchmark/.

Make sure that you are using the latest version of R.",1512743056.0
StevieHoovs,Implementing advice in other comments should help. You can also try profiling your code to identify exactly where the bottlenecks are occurring.,1512764589.0
blahblahblahblah8,"Can you post your code with a reproducible example? 

It's extremely unlikely that a manual call to garbage collector is needed. It's more likely that you are accumulating data in the global environment and approaching a memory limit (so rm would work but refactoring your code to be functional and then vectorizing would be better), or that later jobs just take longer than earlier ones.",1512778801.0
mr_matzoball,"Check out the purrr package. 

If you're feeling adventurous, you can use parallel processing using foreach",1513137738.0
john_ensley,"No idea what you mean by ""naive model"", but RMSE is the square root of the mean of the squared differences between some predicted values and the actual values. So something like `sqrt(mean((predicted - actual)^2))`.",1512776074.0
Crypt0Nihilist,"Hi, in case that's Chelsea, I'd suggest adding a touch more to your video. Your site is really engaging and a video example in R is a great idea, but you should cater for people who come across the video on its own. Simply give a one sentence introduction to MANOVA and each of the test stats and refer people back to your website (but only give the address once in the video).",1512735623.0
too_many_splines,"Find the selector corresponding to the html table. Then use `rvest` APIs to scrape the data.

    library(rvest)
    dta <- read_html(""https://www.nbastuffer.com/2017-2018-nba-team-stats/"") %>% 
        html_node(""#tablepress-9"") %>% 
        html_table()",1512670733.0
CapaneusPrime,"It looks like JavaScript generated table. You should look into using RSelenium. I gave a workshop on web scraping in R and threw all my code up on my GitHub. I'll PM the link to you, but consider it 'as-is.'

Also, read through this for a good guide to setting things up.
https://cran.r-project.org/web/packages/RSelenium/vignettes/RSelenium-docker.html",1512672139.0
strange_r_things,"I have just finished a course at my university on structural equation modeling - my instructor used *lavaan*, also for mediation analysis.

There seems to be some [documentation on mediation analysis](http://lavaan.ugent.be/tutorial/mediation.html), too.",1512668807.0
blozenge,"Which one is the best? I think the answer will usually be 'it depends' unless you want to get more specific about what exactly you're doing.

I've found the [mediation](https://cran.r-project.org/web/packages/mediation/mediation.pdf) package clear and easy to use. It didn't raise any hackles with my reviewers and is pretty flexible, it claims to handle: *""linear regression models, generalized linear models, ordered response models, generalized additive models, quantile regression models, parametric duration models, or multilevel models""*",1512691919.0
justneurostuff,"Are you the author? I might have access to a better and broader news database to study your research question with, and have already created a framework for scraping through it. Maybe we could collaborate?",1512669225.0
sissyslayer45,"This is really cool, thanks for sharing.",1512676118.0
slammaster,"Do you have an issue with missing data? It's sometimes the case that, after your missing values are dropped, you only have a single factor level left.

",1512646515.0
msjgriffiths,"Readability and composability.

That says, use whatever you prefer. Data.table is excellent.

No need to be frustrated.",1512617223.0
ejoran,"The only tidyverse packages I use are readxl and purrr. I find data.table much easier to read and write, and the focus on speed makes a huge difference for ""medium"" data. I also prefer the minimal ""i, j, by"" approach over tidyverse's ""separate verbs for everything"" design. 

The fact that dplyr allows you to use the same interface with different backends is nice, but I'm not convinced that producing, say, SQL code through a dplyr middleman is even a good idea.

That said, I think tidyverse has pushed the R world towards a more consistent set of conventions, which is a hugely positive thing.",1512627379.0
zorp_,If you are in production and are working with clean data it likely isn't a huge return on investment. If code is secondary for you and you may not have as much control over data and need to work fast. It's nice.,1512623015.0
keepitsalty,The idea is that often times the main bottleneck in coding is the time it takes to develop the code and not the speed at which the code runs. The idea behind the tidyverse is that it provides a standard and consistency to the R language and makes writing scripts a lot easier and more intuitive. ,1512627204.0
mtelesha,I find it easier and more functional. I learned Racket and purr and now my code is much cleaner and functional. ,1512622009.0
GoblinoidToad,"The pipes aren't necessary, they just emphasize the fact that you're stringing together a chain of functions. 

What's great about the tidyverse is how much easier it makes data cleaning. ",1512625028.0
guepier,"> I find … base R to be faster

That’s surprising. Because base R is *objectively slower* for most dplyr and readr operations at least.

As for “more intuitive”, it entirely depends on what you’re used to. But once you’ve worked with SQL or other relational data abstractions, you’ll recognise the same patterns that dplyr/tidyr exposes, and it becomes a lot more intuitive.

As for the other tidyverse packages, they’re simply generally more consistently designed, stricter APIs of existing functions. Which means that they are easier to learn, easier to use correctly and, most importantly, [harder to use incorrectly](http://programmer.97things.oreilly.com/wiki/index.php/Make_Interfaces_Easy_to_Use_Correctly_and_Hard_to_Use_Incorrectly) (than the respective base R functions). There’s a broad consensus in software engineering about what constitutes a good API. The tidyverse scores highly in this regard.",1512641309.0
shujaa-g,"I love `dplyr`, and I mostly use `data.table` now. They each have their place.

I learned dplyr first, and when I need to do *a lot* of data manipulation, switching between different grouping levels, lots of cleaning, I find developing in dplyr faster than developing in data.table.

For example, at my old job, I was combining US Census data with spreadsheets from other government agencies and our own data from SQL - lots of reshaping, renaming, separating, and aggregating, maybe doing 10 things at  state level, then a bunch at the county level, etc. And the code needed to be run once per year or once per quarter. I didn't need the speed of `data.table`, and I found it really easy to work one step at a time in `dplyr`. 

Dt's update-by-reference was frustrating because if I had a bug I would need to re-run everything from the beginning to reset, with `dplyr` I just wouldn't assign the change until it looked right.

Nowadays I deal with bigger data that is usually cleaner. My production code runs weekly or daily, so the speed of `data.table` is nice, and I typically need 10-50 lines of data manipulation instead of 200-500 from my multiple-messy-Excel-file days. 

In teaching, most of my students have taken to `dplyr` *very quickly*, but I've had mixed results with `data.table`. So my opinion is that `dplyr` has a friendlier learning curve.

For the rest of the ""tidyverse"" I have mixed feelings. `ggplot2` is of course wonderful. I like `lubridate` and `stringr` a lot. I use `tidyr` mostly for `separate` and `unnest` which I find handy, but I prefer `melt`/`dcast` to `spread`/`gather`. I rarely use `purrr` because I get by just fine with lapply/mapply/Map for lists, and I don't nest too deep. `readr` ain't got nothing on `fread`. `readxl`, `jsonlite`, `rvest` are all great; I've never got around to trying `forcats`, `glue`, `blob`. I'm annoyed at `rlang` because I was just getting proficient at `lazyeval` before everything swtiched over to `rlang`. I doubt I'll take the time to learn it well.",1512671285.0
standard_error,"I use data.table for most of my data preparation, because I work with somewhat large data sets. However, when I have a set of results that I want to present in a table or figure, I usually do the required transformations with the tidyverse tools. I find that the chain structure is very helpful when I'm figuring out where I need to go step by step (I know that piping isn't strictly part of the tidyverse, and that you can chain with data.table, but the verb structure of the tidyverse is more intuitive to me for this kind of work).",1512632300.0
GildedFuchs,"I really appreciate all of the good conversation and valid points that have been made. For correction, I know that dplyr is faster than base R, but I find it harder to understand. One of the major advantages of data.table is the SQL like interpretation of the code. The data folks at my job balk at R and think it's a toy until I show them how seamless we can work in R. 

I agree that the tidyverse is probably easier to pick up (but honestly, before I got into stats I was a humanities phd student). It's all about solving problems, which is what we do. I use some of the tidy packages (stringr, mostly). I'm not offended by how people code, which was a comment, but I have a hard time with mutate and melt syntax - they're words without meaning beyond R. If I want to transform data, I want it to be explicit in the code without feeling like a member of the X-Men. 

Regarding performance, if I really need it I just write that but in Julia. I've had to do it like twice. 

Anywho, I'm alone as an R user at work and it's great to hear different opinions! ",1512701682.0
toot_ma_boot,"why are your frustrated by someone else, a stranger to you, using free software?

pretend you didn't see it! do whatever you were doing beforehand!",1512696398.0
toot_ma_boot,lol. awful.,1512532655.0
Darwinmate,"Read this:
http://www.sthda.com/english/wiki/two-way-anova-test-in-r

I think it gives a good explanation of what a two-way anova is doing. 

The results of distance or depth are NOT the same as 1way anova of the same variable.",1512536794.0
MrLegilimens,"I don’t understand why you’d look at the 1 way anova. The 2 way did exactly what you wanted. You want to do post hoc stuff on the 2 way now. The one way does you no good. Of course the p value would change, you’re controlling for more things.",1512558385.0
crescal,"Vectorised operation should be much faster.

Y1 <- data$X1/rowSums(data)",1512520439.0
GildedFuchs,"Vectorized operations are fast in R, but I wish the notion that loops are so much slower and bad would die. OP had 8 cols and 1k rows - it doesn't matter. I routinely work with 10+ million rows with upwards of 50 columns. Guess what? Loops are totally viable, and they're easier to grok. Depending on what you're writing, it makes sense to use a loop. The apply family is lovely, but don't cut off your nose to spite your face. 

Write a loop, it RARELY makes a substantial difference at OP's scale ",1512615513.0
blahblahblahblah8,Use the scale function,1512533973.0
castro_for_prez,"R2 is not a great way of evaluating a model, neither for predictive modelling nor causal reasoning. I would suggest considering why you are modelling the data, and from there investigate better alternatives. I can't suggest any particular measures because it entirely depends on what you're trying to do.  
  
But as the other poster says, it essentially is down to your data. If the variables are unrelated then of course you cannot make a useful model, no matter what modelling tricks you may try.",1512519033.0
the_odds_hacker,"My advice would be to take a step back from modeling and look at more data. 

The PerformanceAnalytics package has some great visualizations for looking at several variables at the same time. 

You also might want to move from lm() to the caret package. Far more control for how the model is trained and which algorithm is used",1512518633.0
,[deleted],1512519683.0
millsGT49,"I think that writing your own functions in R is a great way to learn the language. For example one of the first functions I wrote is a wrapper around `table` that always returned `NA` values if there were any

    tableNA <- function(...) table(..., useNA = ""always"")

To do this I had to learn what the `...` meant and how to pass multiple arguments to a function. 

Now, if your functions are simple wrappers around existing code or rewriting some basic functionality to be presented in a different format that's fine but like you said there are reasons R's base functions work they way they do. Maybe those reasons don't make sense for you but if you are re-writing base functions and not adding any new functionality you should probably make sure you are doing it for a good reason. And btw just learning is a good reason, nothing wrong with that. 

And in regards to your P.S. yes I think doing data analysis using functional programming definitely makes you a better programer and more able to switch languages for different purposes. ",1512487591.0
efrique,"> Is this frowned upon? Is this going from R user to R programmer?

If you're calculating things for a job and you worked for me I would definitely frown upon calculating things readily obtained from existing widely used functions in the usual distribution of R.

Why?

1. The chances you will make a mistake are much higher than the chance there's a mistake in the corresponding function in base R

2. Even if you're so good you make mistakes incredibly rarely, you're spending valuable time repeating work that someone already did.

If you're doing it just as practice to learn R, it's fine.

> My codes typically consist of for loops and if statements

I almost never use for loops and if statements. I tend to use vector-functions more (replicate, ifelse, etc) -- not because they're faster, but because they're usually more concise and more explanatory of the idea of what is being done -- so easier to follow the overall program logic across, and to find mistakes in later, or to change to do something similar.

On occasion for loops and if statements are better. Whatever you use, try to write for the poor person that has to fix your code a year from now (which will probably be you... future-you will appreciate code that's easy to understand).

> What are your thoughts?

It depends on *why* you're doing it. If you're trying to learn how to code more generally (how to write things in any of a variety of languages), I don't see a big issue. If you're trying to learn how to write *R* that will be like people tend to write R, it doesn't seem like it's teaching you the habits that will serve you well most often.
",1512512315.0
,[deleted],1512493567.0
mattindustries,Next step is to put your custom functions in a package. ,1512502256.0
AllezCannes,"> Is this frowned upon?

You do whatever you want, but the strength of R is that there are so many contributors out there that have put together amazing packages, the question would be why would you wish to re-invent the wheel? What do you need that those packages don't already provide that would require building those custom functions from scratch?

> I know this kind of goes against R's vectorization, so I expect some push back. What are your thoughts?

It's possible that your code is less efficient and less accurate than others.",1512534752.0
GildedFuchs,"I'm going to glom on and express my displeasure with the tidyverse paradigm. One of the things I like about base R (and data.table) is nesting my functions - piping is awkward and doesn't help reading code. With functions, you start from the inside and work out, it's mathematically intuitive. The tidyverse is backasswards ( I'm willing to change my mind in the gave of a favorite argument). 

I also use loops when it makes sense. During a code review someone said it's poor practice to initialize a matrix for the output of a function, because you can vectorize it. That's true, but it's more efficient to initialize the object first. 

I write a lot of custom functions for my job that aren't wrappers, and I have to write for efficiency and speed without blowing my RAM out. YMMV of course, but your code should reflect the problem you're solving. ",1512615141.0
agclx,"Generally it is good and necessary to write ones own functions.  Those standard functions only take one so far, and especially base R looks archaic nowadays.  [tidyverse](https://www.tidyverse.org/learn/) is all the rage now and makes programs clearer - and often shorter.

However these stock functions were seen by many eyes and thoroughly tested.  It is a lot of work to do this for own functions.  So of you need guaranties go for stock.

For loops are not bad but in my experience they often get unnecessary verbose and can become hard to maintain. ",1512502088.0
wilderecon,Shinyapps.io has free hosting with some limitations and can be published to directly from rstudio. I've hit some snags before due to package dependencies but in general it's good.,1512469436.0
a_statistician,"You could set up a linux server on AWS (or a home server) and set up Shiny, then configure your app. It's more work, but it also shows a different skillset that is often important in smaller companies - the ability to interface with IT and do some of your own support work.

Otherwise, shinyapps.io is probably your best bet.",1512481213.0
DaedalusPuddlejumper,"You could do something like:

if(is.null(arg)){return(null)}

There are also functions that let you check whether parameters exist but I forget what they're called.

",1512448392.0
Hoelk,"Terminates with an error:

    function(arg)  print(arg)     

Optional argument:

    function(arg) if (!missing(arg)) print(arg)     

Optional Argument v2 (like in your question):

    function(arg = NULL) if (!is.null(arg)) print(arg)    

I prefer the last option (your way). This way you can already see in the function signature which arguments are optional an which ware not.

",1512495479.0
fasnoosh,Here’s Google’s style guide: https://google.github.io/styleguide/Rguide.xml,1512533429.0
jsgrova,"If somebody commented here, your comment is gone FYI",1518810566.0
normee,I've used the [`sandwich`](https://cran.r-project.org/web/packages/sandwich/vignettes/sandwich-CL.pdf) package for cluster robust SEs.,1512533421.0
icecreamgainz,Check out the package jtools. The vignette explains how to produce the standard errors and replicate Stata results.,1526793522.0
grandzooby,"What you're talking about is ""forecasting"".  A great book to start with is Forecasting, Principles and Practice:  https://www.otexts.org/fpp

The online version is freely available and uses R for the examples.  It's probably one of the most approachable books on the subject.

And while I hate the series title, you could learn a lot from ""Econometrics for Dummies"".  It doesn't have R examples, but covers the concepts pretty well.

The ideas are a lot like doing normal regressions, except that you no longer can assume that the samples are independent.  If I measure the height and weight of different students, you can assume that the height or weight of one has very little effect on another.  But if you measure the demand for a product today, there's a much stronger chance that it's correlated with the demand yesterday.  So the tools you find in forecasting and time-series analysis are about working around that limitation while trying to identify and reconstruct the various trends and cycles in the data.",1512421561.0
RaggedBulleit,"I prefer using the full sheets from rstudio or whoever created them.  Just as easy to understand, but more breadth.",1512438227.0
kazi1,"You need to create a `~/.Rprofile` file with your user-specific config (only goes in your account, so no worries about admin permissions):

Here's an example that points at the RStudio CRAN:

    local({
      r <- getOption(""repos"")
      r[""CRAN""] <- ""https://cran.rstudio.com/""
      options(repos = r)
    })",1512419228.0
,[deleted],1512501083.0
nicknle,You can use jsonlite package to convert an rlist to json output. Look at jsonlite::toJSON() function. ,1512435066.0
Nschnock,"I have the script for calculations, how to make the server script ? The shiny part (the one that that render the json result into a webpage when called from a url with parameters ?)",1512448406.0
SeveralBritishPeople,"If you don’t have or don’t care about special handling of gaps (leave/return) then this is a perfect place to use `tidyr::spread()`.

If you have gaps that you want to do things with, grouping by person, ordering by year, then using `c(1,diff(year)) != 1` will flag gaps other than exactly one year. Ex. `cumsum` on that column in the grouped/ordered data will index contiguous blocks. ",1512340417.0
mLalush,"    library(dplyr)
    library(tidyr)
    
    df <- data.frame(year = rep(c(1, 2, 3), each=5), 
               person=c(""a"", ""b"", ""c"", ""d"", ""e"", ""a"", ""b"", ""c"", ""g"", ""h"", ""a"", ""b"", ""g"", ""h"", ""i""))  
    
    df$enrolled <- rep(1, nrow(df))
    
    df <- df %>%
      spread(person, enrolled) %>%
      gather(year)
    
    colnames(df)[2:3] <- c(""person"", ""enrolled"")
    df[is.na(df$enrolled), ""enrolled""] <- 0
    
    df <- df %>% 
      group_by(person) %>%
      mutate(dummy = ifelse(lag(enrolled) == 0, yes = 0, no = 1),
             dummy = coalesce(dummy, enrolled))
    
    print(df, n = 30)",1512353174.0
thunderdome,"Parsimoniously just means that the dependency doesn't cause lots of inconvienience for little gain. Dplyr is a pretty general tool and if anything will make your data transformation code easier to read and maintain than using base R, plus most people already probably have it installed. Not that big of a deal. 

Things I would avoid:

* Using obscure/hard to find packages.
* Using packages that themselves depend on very specific versions of R or other packages.
* Using edge-case functionality that could change in future versions.
* Using a package for a single function or single time use that is not hard to implement on your own. ",1512333165.0
,"Having fewer dependencies will be more convenient for yourself. Less headache about somebody changing something. Easier to maintain. Less requirements for your users.

If you are going to use dplyr (no reason you shouldn't) you can take additional steps in making sure the user is hidden from your choices as much as possible. If you want your package to be usable outside tidyverse consider these points:

* Don't drag the whole tidyverse along if you only need a few functions
* Don't load the library, instead call it's functions with dplyr::function
* Don't assume your users will know how dplyr works
* Don't return objects that are related to dplyr like ""tibbles""",1512335311.0
efrique,"As a general principle, the fewer dependencies, the fewer things that will break your package the next time they update (and the fewer interactions between packages you have no control over).

That said, you needn't go out of your way to avoid a few widely used packages -- even though that does add some risk -- often the benefit is worth the modest extra costs and risks.

If you can *easily* avoid adding dependency for a package, it's worth considering, but with something like dplyr, if it's particularly useful for your package I wouldn't worry overly much.

Of course there's tradeoffs with every such decision; you have to figure out where the line goes for you.",1512335653.0
shujaa-g,"I love `dplyr`. I've also had packages break when `dplyr` moved from `lazyeval` to `rlang` as the NSE back-end. And I've had to do other maintenance with other `dplyr` changes that I wouldn't have had to do if I just use `base`. 

It's up to you whether you prefer the stability and reliability of a `base` dependency or the convenient but still changing`dplyr` dependency. For me, if there's lots of grouped Split/Apply/Combine style work, I'm happy to use `dplyr`. But for simple subsetting, adding columns, etc., I'll use base syntax.",1512412457.0
kitedart,Can you make the example clearer? Is 'd' an example of the data you are looping over or something else?,1512286683.0
efrique,"Indent your code 4 spaces. (click ""formatting help"" while editing)

If you see the code tool `<>` above your edit window, select all your code and click it",1512296893.0
abecker93,"Yeah, that sounds about right. The Bartlett test indicates a difference between the groups-- I can see without crunching any numbers that zone 2 is going to be significantly different than zone 1, and possibly different than zone 3.

That being said, your sample size is wicked small. I'd imagine that with an increased sample size, you might begin to see normality and more clear differences between the zones. If possible, collect more data (double at least).",1512276127.0
efrique,"> NOT normally distributed / non-parametric.

""non-parametric"" does NOT mean ""not normally distributed""

> doing a Bartlett test of homogeneity of variances,

But that's highly sensitive to the assumption of normality.

",1512296774.0
Deto,"In a way, the only way to compare the interpreters themselves is to use non-vectorized code, otherwise you're comparing whatever C++/Fortran is being called under the hood.  Though this leads to the question of why it's meaningful to just compare the interpreters of languages if people would never likely implement your algorithm using them solely.  And even looking at the numbers, I wouldn't consider the differences they're finding between the R/Python performance very significant (and I'm saying this as a Python fan).",1512282347.0
eaclv,"I did some tests when the paper was first published and found that replacing the inner for loop with a while loop would cut the execution time by more than half (812.528 seconds to 345.515 seconds). In more recent versions of R I seem to recall that it no longer made a difference, it would always take the same time regardless of the type of loop. Anyway, in this useR talk they claim that using the new R compiler results in a 10x speed-up for this particular program, which is quite a lot if it's true: https://youtu.be/pdU0y-wGkOk?t=7m26s
",1512314141.0
efrique,"> the R code contains a triple-nested for loop

Yikes. There's almost certainly a better way to organize the calculation.


",1512296478.0
everetr,"R doesn't know where `var1` and `var3` are coming from.  You're missing one thing:

`testdata$var1[which(testdata$var3 == ""name5"")] <- 31`

and

`testdata$var1[6] <- 31`

You could also use `ifelse()`:

`testdata$var1 <- ifelse(testdata$var3 == ""name5"", 31, testdata$var1)`

This is how you'd do it in base R. A more intuitive set of tools comes from the `dplyr` package. 

`library('dplyr')`

`testdata <- mutate(testdata, var1 = ifelse(var3 == ""name5"", 31, var1)`)

`dplyr` and other packages in the `tidyverse` set of packages help immensely with data cleaning tasks and make R easier to use. I highly recommend [this chapter](http://r4ds.had.co.nz/transform.html) if you're serious about getting into R.",1512268633.0
ReimannOne,"The second one should work fine.

The first `var1[which(var3== ""name5"")] <- 31` should change var1's 5th slot from 30 to 31.


You might think you're going to see a difference in testdata, but you won't.  After creating the testdata data frame, changing var1, var2, or var3 won't change testdata.
To change testdata,  you'll need to change testdata directly.  
  
    testdata[rownumber, column] <- new_data",1512269088.0
toot_ma_boot,"interesting problem that i also encountered when i set aside stata and started working in R a decade (or so) ago. your challenge is to set aside the most fundamental design choice inside Stata--that it can only have one data frame in memory at once. this makes Stata code super super concise--everything you refer to which isn't a function name has to be a variable in the one data frame in memory.

so, R having multiple tables in memory is a huge advantage--for doing joins between tables, storing the results of a model prediction as a whole new table, storing bootstrap simulations, etc etc, but to start with, remembering to do `testdata$var1` for assignments will take a little getting used to.",1512326188.0
_Wintermute,"If your tables are separated by blank lines then it's pretty easy with awk.

e.g, if your file of multiple matrices is called `multiple_matrices.txt`:

    command = ""awk -v RS= '{print > (\""table_\"" NR "".txt\"")}' multiple_matrices.txt""
    system(command)
    lapply(list.files(pattern=""table_*"", read.table)

This will return a list of your matrices.",1512255720.0
Honeabee,This is going to be great in my dissertation about magikarp. ,1512261166.0
gaybearswr4th,Frantically trying to come up with a reason to use these themes right now,1512237553.0
vga896,Why is this a thing and how can I convince my supervisors to let me use these colors,1512268033.0
clbustos,Not all heroes wear capes. Thanks!,1512276124.0
coffeecoffeecoffeee,"This is an amazing package that I'm shocked I hadn't heard about until today.  You copy data, let's say from a Wikipedia table.  You then use a special keyboard shortcut to copy the data as a tibble or vector declaration in RStudio.  This is going to be a huge timesaver.",1512157918.0
samclifford,I work with Miles! There's have been a number of times that I've pulled a table off Wikipedia or a government website when there's no cleaner way to get the data. ,1512175413.0
too_many_splines,Yes! No more wasting time writing one-off web scraping scripts ! ,1512228065.0
slammaster,"Look at expand.grid, something like expand.grid(a=1:3,b=1:3,c=1:3) will give you all the combinations as a data frame, then you can work from there ",1512152271.0
slammaster,"Yeah I've switched to always using which() in these situations.  It's been so long I can't remember why, but I think this was the reason that == by itself didn't handle missing values well.

Plus the more ({[ you fit in a single line the better your code is, right?
",1512147235.0
SirDigbyChknSiezure,"Really useful, thanks",1512142174.0
,Thanks for the post. Really well written.,1512151208.0
GetTheeAShrubbery,"That's an awesome article. I love the end when it says that the way you'd do something once, is rarely the way you'd do it 1000x",1512677968.0
clbustos,"The symbol for covariance is ~~ . So:

    inq11~~inq12

If you want to set the covariance for a specific value, just add  <value>* , like

    inq11~~1*inq12
",1512102616.0
